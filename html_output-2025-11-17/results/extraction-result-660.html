<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-660 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-660</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-660</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-8e9088c102b3714ae4e5cac7ced93a59804bfc7c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8e9088c102b3714ae4e5cac7ced93a59804bfc7c" target="_blank">RewardBench: Evaluating Reward Models for Language Modeling</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The RewardBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries and presents many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.</p>
                <p><strong>Paper Abstract:</strong> Reward models (RMs) are at the crux of successfully using RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we present RewardBench, a benchmark dataset and code-base for evaluation. The RewardBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e660.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e660.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero sampling temperature during RM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recommended inference-time control to reduce variability when using generative or DPO-trained models as reward models; the paper sets or recommends temperature=0 for RM evaluation to lower variance from sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various reward models (classifier and DPO models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP (LM alignment, RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluating reward models (RMs) on prompt-chosen-rejected trios using the ReWARDBench benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>sampling temperature during inference (stochastic token sampling vs deterministic decoding), sampling randomness of generative models used as judges</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Set temperature to 0 at evaluation time (deterministic decoding) to substantially reduce variance of RM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The authors state that evaluation noise is small but recommend setting temperature=0 for RM evaluation because it yields substantially lower variance than sampling-based settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e660.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e660.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO reference model effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference model availability and correctness for Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies that DPO-trained models require a documented, correct reference model at inference for computing implicit rewards, and that omitting or using the wrong reference model substantially reduces RM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DPO-trained chat/reward models (multiple named DPO models: e.g., allenai/tulu-2-dpo-70b, zephyr variants, Mixtral, Qwen chat models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP (RLHF and DPO reward computation)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Comparing RM evaluation using the trained reference model vs reference-free or wrong-reference inference to quantify impact on RM scores</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>absence of the reference model or use of a different/wrong reference model during reward computation, undocumented reference model in released DPO checkpoints, normalization choices (partition function Z and length-sum effects), base model differences</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy/score on ReWARDBench (Avg score) and Delta between full-reference and reference-free inference (points) reported per model; also per-section deltas (Chat Hard, Safety, Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 7: examples — mistralai/Mixtral-8x7B-Instruct-v0.1: Avg 82.2, Ref.Free 64.2, Delta -18.0 (Chat Hard -6.4, Safety -28.5, Reason -35.3); allenai/tulu-2-dpo-13b: Avg 78.8, Ref.Free 62.9, Delta -15.9 (Chat Hard -10.3, Safety -19.0, Reason -36.5); HuggingFaceH4/zephyr-7b-alpha: Avg 78.6, Ref.Free 65.6, Delta -13.0 (Chat Hard -10.9, Safety -10.5, Reason -31.0).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Score differences (Delta) between evaluations using the documented reference model vs reference-free inference (point changes in ReWARDBench score and per-section score changes).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Omitting or using a wrong reference model reduced DPO RM performance by between about 4.5 and 18.0 points in average ReWARDBench score across the reported models (see Table 7), with particularly large drops on Reasoning and Safety sections (e.g. up to ~36-point drops on 'Reason' for some models).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Many released DPO models do not clearly document the reference model used in training; using a different reference or a reference-free approximation changes the reward calculation and leads to large performance degradation, complicating benchmarking and replication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Require and distribute the exact reference model used for DPO training; compute policy and reference probabilities sequentially when GPU memory is constrained; avoid reference-free shortcuts unless normalized; document reference/model provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified in Table 7: having the correct reference model preserves published RM performance, while reference-free evaluation produced deltas of -4.5 to -18.0 points in average score and large per-section drops (examples above).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 DPO models compared in the referenced Table 7 (per-model one reference vs reference-free evaluation comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The presence and correct use of the reference model for DPO inference is crucial: omitting or substituting the reference model reduces RM evaluation scores substantially (up to ~18 points average, and much larger losses on some sections like Reasoning), showing a reproducibility failure mode if reference models are undocumented or unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e660.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e660.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subset variance (Chat Hard/Safety/Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High variability and low ceilings across dataset subsets (Chat Hard, Safety, Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that different ReWARDBench subsets show widely differing ceilings and high variance in RM performance — some subsets reach near 100% accuracy while others are near random or widely spread across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Many reward models (classifier-based RMs and DPO models evaluated on ReWARDBench)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP (evaluation and benchmarking of RMs)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measuring RM accuracy on structured subsets (Chat, Chat Hard, Safety, Reasoning, Prior Sets) of ReWARDBench to analyze performance variability across tasks and prompts</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>dataset difficulty (adversarial/near-miss prompts), out-of-distribution examples, inter-annotator disagreement in original datasets, base model scaling and architecture differences, preference-data composition and filtering (e.g., UltraFeedback filtering), subset-specific challenges (precise token-level differences in reasoning/code), refusal propensity policies</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Per-subset accuracy (percentage wins) and reported score ranges across models; ceilings and observed ranges reported in tables rather than formal variance statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported ranges: Reasoning section ranges from ~35% up to ~97% accuracy across models; Chat Hard and Safety subsets show many models around random baseline (50%) up to high-performing models (~76.8 for top Chat Hard). Many subsets show low ceilings or high variance (qualitative and tabulated per-model accuracies).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Heterogeneous dataset difficulty and lack of uniform test sets (new datasets without test splits), potential data contamination, and ambiguous annotation noise (inter-annotator disagreement) reduce the ability to compare and reproduce results across works.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Design curated, structured subsets with human-verified chosen/rejected pairs; control confounders like length (AlpacaEval Length subset) to reduce proxies; weight subsets for final aggregated score to balance aspects; manual validation of semi-automatically collected pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Evaluated over ~75-80 models across the benchmark; per-model single evaluation on the benchmark subsets (no multiple-run variance statistics reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RM performance is highly dataset-dependent: some subsets are solved reliably by many models (near 100% accuracy) while others (notably Chat Hard and some Safety subsets) exhibit low ceilings and high variance, indicating that benchmarking must use diverse, carefully curated subsets to reveal true RM limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e660.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e660.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward distribution & length bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-model reward score distributions and length-bias effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that per-prompt reward distributions across RMs are typically non-Gaussian and that length correlations exist (length bias), which can confound RM performance and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Classifier RMs and DPO models evaluated in ReWARDBench (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP (reward modeling and evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Analyzing score distributions assigned by different RMs across benchmark prompts and testing for length bias between chosen and rejected completions</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>reward output scaling and shape differences across models (non-Gaussian, not centered at 0), correlation between output length and assigned reward, normalization choices in DPO reward computation (length-sum of log probabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Descriptive analysis of score distributions (per-prompt distributions shown in appendix figures) and examination of length distributions in dataset subsets; no standard deviation/variance numbers reported for distributions in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Authors observe that few RMs have Gaussian score distributions, fewer are centered near 0, and none tested produce centered Gaussians; they cite prior work showing strong correlation between length and reward and design AlpacaEval Length subset to control length confound.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Length bias and differing output distributions across RMs can make cross-model comparisons and replication fragile if not controlled or normalized.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Construct evaluation subsets controlling for response length (e.g., chosen responses similar length or shorter than rejected responses), analyze and report reward distributions, and pursue future work on identifying preferred RM output distributions for downstream RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reward score distributions vary in shape and center across RMs and length correlates with reward; the benchmark controls for length in some subsets to reduce this confound, but establishing standardized RM output distributions remains an open task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e660.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e660.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error bars / seeds missing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of reported error bars and seed-based variance analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper explicitly notes that they did not report error bars over random seeds for RM evaluation and that only a small amount of variability could arise in evaluation; they flag reporting error bars as N/A in the checklist but recommend deterministic evaluation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RewardBench evaluations across many RMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP (benchmarking and experimental methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Benchmark evaluation of many RMs with single-run per-model scoring (no multiple-seed experimental repeats reported)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>random seeds, stochastic training and sampling procedures, small evaluation-time variability if sampling not deterministic</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>No error bars reported and no multiple-run seed analysis means the magnitude of run-to-run variability in RM evaluation is not quantified, limiting reproducibility claims about subtle score differences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Recommend deterministic evaluation (temperature=0), release code and evaluation artifacts (datasets, text-score pairs) to facilitate replication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper acknowledges absence of seed-based error bars for evaluation (not reported) and recommends deterministic inference and artifact release to reduce evaluation variance and aid reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e660.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e660.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Private models / unreproducible leaderboard entries</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-weight / private model entries and reproducibility limits on leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that many leaderboard entries correspond to private models from providers; those are not reproducible and thus are excluded from the paper's analyses, highlighting a reproducibility limitation in public benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Closed-weight generative LLMs and reward models hosted by private providers (e.g., proprietary GPT/Gemini/Claude variants referenced in leaderboards)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP (benchmarking and evaluation transparency)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Public leaderboard aggregation and evaluation of reward models where some noted top scores originate from private models inaccessible for independent replication</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>closed weights, undocumented training details, API-based model versions and updates, private evaluation procedures</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Scores from private models cannot be reproduced by the community due to absent weights/documentation; API versioning and provider updates also hamper reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Only include publicly reproducible models in paper analyses; release code, datasets, and text-score pairs; provide an online leaderboard but note which entries are not reproducible and exclude them from paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper excludes private-model leaderboard scores from the paper because they are not reproducible and emphasizes that ReWARDBench and released artifacts enable community replication for open models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RewardBench: Evaluating Reward Models for Language Modeling', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>A long way to go: Investigating length correlations in rlhf <em>(Rating: 2)</em></li>
                <li>The trickle-down impact of reward (in-) consistency on rlhf <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models <em>(Rating: 1)</em></li>
                <li>UltraFeedback: Boosting Language Models with High-quality Feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-660",
    "paper_id": "paper-8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Temperature=0",
            "name_full": "Zero sampling temperature during RM evaluation",
            "brief_description": "A recommended inference-time control to reduce variability when using generative or DPO-trained models as reward models; the paper sets or recommends temperature=0 for RM evaluation to lower variance from sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various reward models (classifier and DPO models)",
            "model_size": null,
            "scientific_domain": "Machine learning / NLP (LM alignment, RLHF)",
            "experimental_task": "Evaluating reward models (RMs) on prompt-chosen-rejected trios using the ReWARDBench benchmark",
            "variability_sources": "sampling temperature during inference (stochastic token sampling vs deterministic decoding), sampling randomness of generative models used as judges",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": null,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": null,
            "mitigation_methods": "Set temperature to 0 at evaluation time (deterministic decoding) to substantially reduce variance of RM scores.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "The authors state that evaluation noise is small but recommend setting temperature=0 for RM evaluation because it yields substantially lower variance than sampling-based settings.",
            "uuid": "e660.0",
            "source_info": {
                "paper_title": "RewardBench: Evaluating Reward Models for Language Modeling",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "DPO reference model effect",
            "name_full": "Reference model availability and correctness for Direct Preference Optimization (DPO)",
            "brief_description": "The paper identifies that DPO-trained models require a documented, correct reference model at inference for computing implicit rewards, and that omitting or using the wrong reference model substantially reduces RM performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DPO-trained chat/reward models (multiple named DPO models: e.g., allenai/tulu-2-dpo-70b, zephyr variants, Mixtral, Qwen chat models)",
            "model_size": null,
            "scientific_domain": "Machine learning / NLP (RLHF and DPO reward computation)",
            "experimental_task": "Comparing RM evaluation using the trained reference model vs reference-free or wrong-reference inference to quantify impact on RM scores",
            "variability_sources": "absence of the reference model or use of a different/wrong reference model during reward computation, undocumented reference model in released DPO checkpoints, normalization choices (partition function Z and length-sum effects), base model differences",
            "variability_measured": true,
            "variability_metrics": "Accuracy/score on ReWARDBench (Avg score) and Delta between full-reference and reference-free inference (points) reported per model; also per-section deltas (Chat Hard, Safety, Reasoning)",
            "variability_results": "Table 7: examples — mistralai/Mixtral-8x7B-Instruct-v0.1: Avg 82.2, Ref.Free 64.2, Delta -18.0 (Chat Hard -6.4, Safety -28.5, Reason -35.3); allenai/tulu-2-dpo-13b: Avg 78.8, Ref.Free 62.9, Delta -15.9 (Chat Hard -10.3, Safety -19.0, Reason -36.5); HuggingFaceH4/zephyr-7b-alpha: Avg 78.6, Ref.Free 65.6, Delta -13.0 (Chat Hard -10.9, Safety -10.5, Reason -31.0).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Score differences (Delta) between evaluations using the documented reference model vs reference-free inference (point changes in ReWARDBench score and per-section score changes).",
            "reproducibility_results": "Omitting or using a wrong reference model reduced DPO RM performance by between about 4.5 and 18.0 points in average ReWARDBench score across the reported models (see Table 7), with particularly large drops on Reasoning and Safety sections (e.g. up to ~36-point drops on 'Reason' for some models).",
            "reproducibility_challenges": "Many released DPO models do not clearly document the reference model used in training; using a different reference or a reference-free approximation changes the reward calculation and leads to large performance degradation, complicating benchmarking and replication.",
            "mitigation_methods": "Require and distribute the exact reference model used for DPO training; compute policy and reference probabilities sequentially when GPU memory is constrained; avoid reference-free shortcuts unless normalized; document reference/model provenance.",
            "mitigation_effectiveness": "Quantified in Table 7: having the correct reference model preserves published RM performance, while reference-free evaluation produced deltas of -4.5 to -18.0 points in average score and large per-section drops (examples above).",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 DPO models compared in the referenced Table 7 (per-model one reference vs reference-free evaluation comparisons)",
            "key_findings": "The presence and correct use of the reference model for DPO inference is crucial: omitting or substituting the reference model reduces RM evaluation scores substantially (up to ~18 points average, and much larger losses on some sections like Reasoning), showing a reproducibility failure mode if reference models are undocumented or unavailable.",
            "uuid": "e660.1",
            "source_info": {
                "paper_title": "RewardBench: Evaluating Reward Models for Language Modeling",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Subset variance (Chat Hard/Safety/Reasoning)",
            "name_full": "High variability and low ceilings across dataset subsets (Chat Hard, Safety, Reasoning)",
            "brief_description": "The paper documents that different ReWARDBench subsets show widely differing ceilings and high variance in RM performance — some subsets reach near 100% accuracy while others are near random or widely spread across models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Many reward models (classifier-based RMs and DPO models evaluated on ReWARDBench)",
            "model_size": null,
            "scientific_domain": "Machine learning / NLP (evaluation and benchmarking of RMs)",
            "experimental_task": "Measuring RM accuracy on structured subsets (Chat, Chat Hard, Safety, Reasoning, Prior Sets) of ReWARDBench to analyze performance variability across tasks and prompts",
            "variability_sources": "dataset difficulty (adversarial/near-miss prompts), out-of-distribution examples, inter-annotator disagreement in original datasets, base model scaling and architecture differences, preference-data composition and filtering (e.g., UltraFeedback filtering), subset-specific challenges (precise token-level differences in reasoning/code), refusal propensity policies",
            "variability_measured": false,
            "variability_metrics": "Per-subset accuracy (percentage wins) and reported score ranges across models; ceilings and observed ranges reported in tables rather than formal variance statistics.",
            "variability_results": "Reported ranges: Reasoning section ranges from ~35% up to ~97% accuracy across models; Chat Hard and Safety subsets show many models around random baseline (50%) up to high-performing models (~76.8 for top Chat Hard). Many subsets show low ceilings or high variance (qualitative and tabulated per-model accuracies).",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Heterogeneous dataset difficulty and lack of uniform test sets (new datasets without test splits), potential data contamination, and ambiguous annotation noise (inter-annotator disagreement) reduce the ability to compare and reproduce results across works.",
            "mitigation_methods": "Design curated, structured subsets with human-verified chosen/rejected pairs; control confounders like length (AlpacaEval Length subset) to reduce proxies; weight subsets for final aggregated score to balance aspects; manual validation of semi-automatically collected pairs.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Evaluated over ~75-80 models across the benchmark; per-model single evaluation on the benchmark subsets (no multiple-run variance statistics reported).",
            "key_findings": "RM performance is highly dataset-dependent: some subsets are solved reliably by many models (near 100% accuracy) while others (notably Chat Hard and some Safety subsets) exhibit low ceilings and high variance, indicating that benchmarking must use diverse, carefully curated subsets to reveal true RM limitations.",
            "uuid": "e660.2",
            "source_info": {
                "paper_title": "RewardBench: Evaluating Reward Models for Language Modeling",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Reward distribution & length bias",
            "name_full": "Per-model reward score distributions and length-bias effects",
            "brief_description": "The paper reports that per-prompt reward distributions across RMs are typically non-Gaussian and that length correlations exist (length bias), which can confound RM performance and evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Classifier RMs and DPO models evaluated in ReWARDBench (various)",
            "model_size": null,
            "scientific_domain": "Machine learning / NLP (reward modeling and evaluation)",
            "experimental_task": "Analyzing score distributions assigned by different RMs across benchmark prompts and testing for length bias between chosen and rejected completions",
            "variability_sources": "reward output scaling and shape differences across models (non-Gaussian, not centered at 0), correlation between output length and assigned reward, normalization choices in DPO reward computation (length-sum of log probabilities)",
            "variability_measured": false,
            "variability_metrics": "Descriptive analysis of score distributions (per-prompt distributions shown in appendix figures) and examination of length distributions in dataset subsets; no standard deviation/variance numbers reported for distributions in main text.",
            "variability_results": "Authors observe that few RMs have Gaussian score distributions, fewer are centered near 0, and none tested produce centered Gaussians; they cite prior work showing strong correlation between length and reward and design AlpacaEval Length subset to control length confound.",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Length bias and differing output distributions across RMs can make cross-model comparisons and replication fragile if not controlled or normalized.",
            "mitigation_methods": "Construct evaluation subsets controlling for response length (e.g., chosen responses similar length or shorter than rejected responses), analyze and report reward distributions, and pursue future work on identifying preferred RM output distributions for downstream RL training.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Reward score distributions vary in shape and center across RMs and length correlates with reward; the benchmark controls for length in some subsets to reduce this confound, but establishing standardized RM output distributions remains an open task.",
            "uuid": "e660.3",
            "source_info": {
                "paper_title": "RewardBench: Evaluating Reward Models for Language Modeling",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Error bars / seeds missing",
            "name_full": "Lack of reported error bars and seed-based variance analysis",
            "brief_description": "The paper explicitly notes that they did not report error bars over random seeds for RM evaluation and that only a small amount of variability could arise in evaluation; they flag reporting error bars as N/A in the checklist but recommend deterministic evaluation settings.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "RewardBench evaluations across many RMs",
            "model_size": null,
            "scientific_domain": "Machine learning / NLP (benchmarking and experimental methodology)",
            "experimental_task": "Benchmark evaluation of many RMs with single-run per-model scoring (no multiple-seed experimental repeats reported)",
            "variability_sources": "random seeds, stochastic training and sampling procedures, small evaluation-time variability if sampling not deterministic",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "No error bars reported and no multiple-run seed analysis means the magnitude of run-to-run variability in RM evaluation is not quantified, limiting reproducibility claims about subtle score differences.",
            "mitigation_methods": "Recommend deterministic evaluation (temperature=0), release code and evaluation artifacts (datasets, text-score pairs) to facilitate replication.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The paper acknowledges absence of seed-based error bars for evaluation (not reported) and recommends deterministic inference and artifact release to reduce evaluation variance and aid reproducibility.",
            "uuid": "e660.4",
            "source_info": {
                "paper_title": "RewardBench: Evaluating Reward Models for Language Modeling",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Private models / unreproducible leaderboard entries",
            "name_full": "Closed-weight / private model entries and reproducibility limits on leaderboards",
            "brief_description": "The paper notes that many leaderboard entries correspond to private models from providers; those are not reproducible and thus are excluded from the paper's analyses, highlighting a reproducibility limitation in public benchmarking.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Closed-weight generative LLMs and reward models hosted by private providers (e.g., proprietary GPT/Gemini/Claude variants referenced in leaderboards)",
            "model_size": null,
            "scientific_domain": "Machine learning / NLP (benchmarking and evaluation transparency)",
            "experimental_task": "Public leaderboard aggregation and evaluation of reward models where some noted top scores originate from private models inaccessible for independent replication",
            "variability_sources": "closed weights, undocumented training details, API-based model versions and updates, private evaluation procedures",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Scores from private models cannot be reproduced by the community due to absent weights/documentation; API versioning and provider updates also hamper reproducibility.",
            "mitigation_methods": "Only include publicly reproducible models in paper analyses; release code, datasets, and text-score pairs; provide an online leaderboard but note which entries are not reproducible and exclude them from paper tables.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The paper excludes private-model leaderboard scores from the paper because they are not reproducible and emphasizes that ReWARDBench and released artifacts enable community replication for open models.",
            "uuid": "e660.5",
            "source_info": {
                "paper_title": "RewardBench: Evaluating Reward Models for Language Modeling",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2
        },
        {
            "paper_title": "A long way to go: Investigating length correlations in rlhf",
            "rating": 2
        },
        {
            "paper_title": "The trickle-down impact of reward (in-) consistency on rlhf",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
            "rating": 1
        },
        {
            "paper_title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
            "rating": 1
        }
    ],
    "cost": 0.0192325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Reward Models for Language Modeling</h1>
<p>Nathan Lambert ${ }^{\text {® }}$ Valentina Pyatkin ${ }^{\alpha \beta}$ Jacob Morrison ${ }^{\text {® }}$<br>LJ Miranda ${ }^{\text {® }}$ Bill Yuchen Lin ${ }^{\text {® }}$ Khyathi Chandu ${ }^{\text {® }}$ Nouha Dziri ${ }^{\text {® }}$<br>Sachin Kumar ${ }^{\text {® }}$ Tom Zick $^{\text {® }}$ Yejin Choi ${ }^{\alpha \beta}$ Noah A. Smith ${ }^{\alpha \beta}$ Hannaneh Hajishirzi ${ }^{\alpha \beta}$<br>${ }^{\text {® }}$ Allen Institute for Artificial Intelligence<br>${ }^{\beta}$ University of Washington ${ }^{\text {® }}$ Berkman Klein Center, Harvard Law<br>contact: nathanl@allenai.org</p>
<h4>Abstract</h4>
<p>Reward models (RMs) are at the crux of successfully using RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we present ReWARDBench, a benchmark dataset and code-base for evaluation. The ReWARDBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the ReWARDBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\Delta$</th>
<th style="text-align: center;">Leaderboard</th>
<th style="text-align: left;">https://hf.co/spaces/allenai/reward-bench</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: center;">Code</td>
<td style="text-align: left;">https://github.com/allenai/reward-bench</td>
</tr>
<tr>
<td style="text-align: left;">$\Delta$</td>
<td style="text-align: center;">Dataset</td>
<td style="text-align: left;">https://hf.co/datasets/allenai/reward-bench</td>
</tr>
</tbody>
</table>
<h2>1 Introduction</h2>
<p>Reinforcement learning from human feedback (RLHF) is a necessary but opaque tool underlying the success of popular language models (LMs) such as OpenAI's ChatGPT (Schulman et al., 2022) and Anthropic's Claude (Bai et al., 2022a). The prevalence of RLHF stems from its efficacy at circumventing one of the greatest difficulties in integrating human preferences into language models: specifying an explicit reward (Christiano et al., 2017). Reward models (RMs) are central to this process. They are created by copying the original language model and training it on labeled preference data, producing a model that can predict whether one piece of text is likely to be preferred over another. A reinforcement learning optimizer then uses this reward model signal to update the</p>
<p>parameters of the original model, improving performance on a variety of tasks (Ouyang et al., 2022; Touvron et al., 2023).</p>
<p>While the post-RLHF model (known as the policy) and even the pretrained model are extensively documented and evaluated, the basic properties of the RLHF process like the RMs receive far less attention. Recent work on training reward models (Zhu et al., 2023a; Jiang et al., 2023c) has begun to fill this gap, but utilizes validation sets from previous RLHF training processes, such as Anthropic's Helpful and Harmless data (Bai et al., 2022a) or OpenAI's Learning to Summarize (Stiennon et al., 2020), which are known to have ceilings on accuracy between 60 and $70 \%$ due to inter-annotator disagreement (Wang et al., 2024). Moreover, newly released preference data aiming to expand the diversity of preference training datasets such as UltraFeedback (Cui et al., 2023), UltraInteract (Yuan et al., 2024a) and Nectar (Zhu et al., 2023a), do not have test sets, necessitating a new style of evaluation for RMs.</p>
<p>We begin to rectify the lack of evaluation methods by introducing REWARDBENCH, the first toolkit for benchmarking reward models. RLHF is a broadly applicable process used to enhance specific capabilities of LMs such as safety (Dai et al., 2023) or reasoning (Lightman et al., 2023; Havrilla et al., 2024a) as well as general capabilities such as instruction following (Ouyang et al., 2022) or "steerability" (Askell et al., 2021; Bai et al., 2022a). Thorough evaluations of RMs will also cover these categories. In this work, we curate data to create structured comparisons across a variety of reward model properties. Each sample is formatted as a prompt with a human-verified chosen and rejected completion. We design subsets so as to vary in difficulty and coverage. Some subsets are solved by small RMs, reaching $100 \%$ accuracy, but others are harder to differentiate and still have state-of-the-art performance around $75 \%$, with many models around the random baseline.</p>
<p>We aim to map the current landscape of openly available reward models via a leaderboard for REWARDBENCH. We have evaluated over 80 models, such those trained as classifiers, including UltraRM (Cui et al., 2023), Starling (Zhu et al., 2023a), PairRM (Jiang et al., 2023c), SteamSHP (Ethayarajh et al., 2022), models from Reward rAnked FineTuning (RAFT) (Dong et al., 2023), and others. We also evaluate popular chat models trained with Direct Policy Optimization (DPO) (Rafailov et al., 2023), for example, Zephyr- $\beta$ (Tunstall et al., 2023), Qwen-Chat (Bai et al., 2023), StableLM (Bellagente et al., 2024), and Tülu 2 (Ivison et al., 2023) to ground recent debates on RLHF methods and showcase specific datasets where they fall short.</p>
<p>With these models, we compare scaling, test reasoning capabilities, highlight three buckets of refusal behavior, and share more details on the inner workings of RMs. The accompanying code-base provides a common inference stack for many variations of models and we release many text-score pairs to analyze their performance. With ReWARDBench, we:</p>
<ol>
<li>Release a common framework for evaluating the many different architectures of reward models, along with tools for visualization, training, and other analysis. We also release all data used in the evaluation, composed of text-score pairs for all inputs, to enable further data analysis on the properties of reward models. ${ }^{1}$</li>
<li>Illustrate the differences between DPO and classifier-based reward models across a variety of datasets. DPO models, while more plentiful due to the method's simplicity, fail to generalize to popular preference data test sets and present a higher variance in performance.</li>
<li>Chart the landscape of current state-of-the-art reward models. We showcase the scaling laws, the propensity to refuse (or not), the reasoning capabilities, and more for popular RMs.</li>
<li>Show the limitations of existing preference data test sets for evaluating these models, showcasing common pitfalls of RMs on subtle, but challenging instruction pairs (e.g. intentionally modified rejected responses, which superficially look high quality but answer the wrong prompt).</li>
</ol>
<h1>2 Related Works</h1>
<p>Reinforcement Learning from Human Feedback Using Reinforcement Learning to align language models with human feedback or preferences (Christiano et al., 2017; Ziegler et al., 2019) has led to improved chat models such as ChatGPT (Schulman et al., 2022) and Llama2 (Touvron et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Summary of the dataset used in RewardBench. Note: Adver. is short for Adverserial.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Subset</th>
<th>N</th>
<th>Short Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Chat <br> 358 total</td>
<td>AlpacaEval Easy</td>
<td>100</td>
<td>GPT4-Turbo vs. Alpaca 7bB from <em>Li et al. (2023b)</em></td>
</tr>
<tr>
<td></td>
<td>AlpacaEval Length</td>
<td>95</td>
<td>Llama 2 Chat 70B vs. Guanaco 13B completions</td>
</tr>
<tr>
<td></td>
<td>AlpacaEval Hard</td>
<td>95</td>
<td>Tulu 2 DPO 70B vs. Davinici003 completions</td>
</tr>
<tr>
<td></td>
<td>MT Bench Easy</td>
<td>28</td>
<td>MT Bench ratings 10s vs. 1s from <em>Zheng et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td>MT Bench Medium</td>
<td>40</td>
<td>MT Bench completions rated 9s vs. 2-5s</td>
</tr>
<tr>
<td>Chat Hard <br> 456 total</td>
<td>MT Bench Hard</td>
<td>37</td>
<td>MT Bench completions rated 7-8s vs. 5-6</td>
</tr>
<tr>
<td></td>
<td>LLMBar Natural</td>
<td>100</td>
<td>LLMBar chat comparisons from <em>Zeng et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td>LLMBar Adver. Neighbor</td>
<td>134</td>
<td>LLMBar challenge comparisons via similar prompts</td>
</tr>
<tr>
<td></td>
<td>LLMBar Adver. GPTInst</td>
<td>92</td>
<td>LLMBar comparisons via GPT4 similar prompts</td>
</tr>
<tr>
<td></td>
<td>LLMBar Adver. GPTOut</td>
<td>47</td>
<td>LLMBar comparisons via GPT4 unhelpful response</td>
</tr>
<tr>
<td></td>
<td>LLMBar Adver. Manual</td>
<td>46</td>
<td>LLMBar manually curated challenge completions</td>
</tr>
<tr>
<td>Safety <br> 740 total</td>
<td>Refusals Dangerous</td>
<td>100</td>
<td>Preferring refusal to elicit dangerous responses</td>
</tr>
<tr>
<td></td>
<td>Refusals Offensive</td>
<td>100</td>
<td>Preferring refusal to elicit offensive responses</td>
</tr>
<tr>
<td></td>
<td>XSTest Should Refuse</td>
<td>154</td>
<td>Prompts that should be refused <em>Röttger et al. (2023)</em></td>
</tr>
<tr>
<td></td>
<td>XSTest Should Respond</td>
<td>250</td>
<td>Preferring responses to queries with trigger words</td>
</tr>
<tr>
<td></td>
<td>Do Not Answer</td>
<td>136</td>
<td>Questions that LLMs should refuse <em>(Wang et al., 2023)</em></td>
</tr>
<tr>
<td>Reasoning <br> 1431 total</td>
<td>PRM Math</td>
<td>447</td>
<td>Human vs. buggy LLM answers <em>(Lightman et al., 2023)</em></td>
</tr>
<tr>
<td></td>
<td>HumanEvalPack CPP</td>
<td>164</td>
<td>Correct CPP vs. buggy code <em>(Muennighoff et al., 2023)</em></td>
</tr>
<tr>
<td></td>
<td>HumanEvalPack Go</td>
<td>164</td>
<td>Correct Go code vs. buggy code</td>
</tr>
<tr>
<td></td>
<td>HumanEvalPack Javascript</td>
<td>164</td>
<td>Correct Javascript code vs. buggy code</td>
</tr>
<tr>
<td></td>
<td>HumanEvalPack Java</td>
<td>164</td>
<td>Correct Java code vs. buggy code</td>
</tr>
<tr>
<td></td>
<td>HumanEvalPack Python</td>
<td>164</td>
<td>Correct Python code vs. buggy code</td>
</tr>
<tr>
<td></td>
<td>HumanEvalPack Rust</td>
<td>164</td>
<td>Correct Rust code vs. buggy code</td>
</tr>
<tr>
<td>Prior Sets <br> 17.2k total</td>
<td>Anthropic Helpful</td>
<td>6192</td>
<td>Helpful split from test set of <em>Bai et al. (2022a)</em></td>
</tr>
<tr>
<td></td>
<td>Anthropic HHH</td>
<td>221</td>
<td>HHH validation data <em>(Askell et al., 2021)</em></td>
</tr>
<tr>
<td></td>
<td>SHP</td>
<td>1741</td>
<td>Partial test set from <em>Ethayarajh et al. (2022)</em></td>
</tr>
<tr>
<td></td>
<td>Summarize</td>
<td>9000</td>
<td>Test set from <em>Stiennon et al. (2020)</em></td>
</tr>
</tbody>
</table>
<p>2023). Incorporating human feedback into models in this way has been used to improve summarization <em>(Stiennon et al., 2020; Wu et al., 2021)</em>, question answering <em>(Nakano et al., 2021)</em>, image models <em>(Lee et al., 2023)</em> and instruction following in general <em>(Ouyang et al., 2022)</em>.</p>
<p>RLHF often focuses on aspects of preference, where aspects could be more general concepts like <em>helpfulness</em> or <em>harmlessness</em> <em>(Bai et al., 2022a)</em>, or more fine-grained ones <em>(Wu et al., 2023)</em>, among others. In general, RLHF involves training a reward model on preference data collected from crowdworkers <em>(Wang et al., 2024)</em> (or from LM selected responses <em>(Bai et al., 2022b)</em>). Given a reward model, a policy can be learned using RL algorithms like PPO <em>(Schulman et al., 2017)</em>, which has been shown to work well for language policies <em>(Ramamurthy et al., 2022)</em>. Another option is to directly optimize a policy with chosen and rejected pairs, using DPO <em>(Rafailov et al., 2023)</em>. Some reward modeling extensions include process reward models <em>(Luo et al., 2023; Lightman et al., 2023)</em> and step-wise reward models <em>(Havrilla et al., 2024b)</em>, which are primarily used for reasoning tasks.</p>
<p>Reward Model &amp; RLHF Evaluation Preference tuned models can be evaluated using downstream evaluations, for example using AlpacaFarm <em>(Dubois et al., 2024)</em>, where LMs are used to simulate human preferences by comparing a model generated output with that of a reference model. The reported metric is the win-rate of the model over the reference model. Similarly, MT-Bench <em>(Zheng et al., 2023)</em>, evaluates chatbots on multi-turn conversations that are judged by LMs as proxy for human judgments and Chatbot Arena <em>(Zheng et al., 2023)</em> crowdsources the preferences between two different model outputs. These types of setups only indirectly evaluate the reward model. Other works, directly analyze the reward model, such as <em>Singhal et al. (2023)</em>, who found a strong correlation between output length and rewards by looking at the training dynamics of RMs. Another analysis looked at reward inconsistencies, by creating a benchmark of contrasting instructions <em>(Shen et al., 2023)</em>. <em>Clymer et al. (2023)</em> study reward model performance under distribution shift.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Prompts to test capabilities
Figure 1: The scoring method of the REWARDBENCH evaluation suite. Each prompt is accompanied by a chosen and rejected completion which are independently rated by a reward model.</p>
<h1>3 Background</h1>
<p>Reward Modeling The first step of training a reward model, and therefore doing RLHF, is collecting preference data from a group of human labelers. Individuals are presented with prompts, $x$, akin to a question or task, and asked to choose between a set of completions, $y_{i}$, answering the request. The most common case is for only two completions to be shown with measurement of preference, such as win-loss-tie or a Likert scale indicating the magnitude of preference between completions (Bai et al., 2022a), though other methods for labeling exist, such as ranking in a batch of 4+ answers (Ouyang et al., 2022). The resulting data is transformed into a set of prompt-chosenrejected trios, where the chosen completion is preferred over the rejected completion for training. Training a reward model involves training a classifier to predict the human preference probability, $p^{*}$, between two answers, as modeled by a Bradley-Terry model (Bradley and Terry, 1952):</p>
<p>$$
p^{<em>}\left(y_{1}&gt;y_{x} \mid x\right)=\frac{\exp \left(r^{</em>}\left(x, y_{1}\right)\right)}{\exp \left(r^{<em>}\left(x, y_{1}\right)\right)+\exp \left(r^{</em>}\left(x, y_{2}\right)\right)}
$$</p>
<p>Then, estimate the parameters of the RM by optimizing the maximum likelihood loss as follows: $\mathcal{L}(\theta, \mathcal{D})=\mathbb{E}<em _chosen="{chosen" _text="\text">{\left(x, y</em>\right)$.}}, y_{\text {rejected }}\right)-\mathcal{D}}[\log \left(1+e^{r_{y}\left(x, y_{\text {rejected }}\right)-r_{y}\left(x, y_{\text {chosen }}\right)}\right)]$.For language models, the RM is often implemented by appending a linear layer to predict one logit or removing the final decoding layers and replacing them with a linear layer. At inference time, a trained reward model returns a scalar, such that $P\left(y_{1}&gt;y_{2} \mid x\right) \propto \mathrm{e}^{r\left(x, y_{1}\right)}$ (which intuitively is the probability that the completion would be a preferred response, but is trained indirectly via the pairwise loss). Thus, a win between completions $y_{1}$ and $y_{2}$ is achieved when $r\left(x, y_{1}\right)&gt;r\left(x, y_{2</p>
<p>Direct Preference Optimization Direct Preference Optimization solves the RLHF problem without needing to learn a separate reward model. It achieves this by reparameterizing the preferencebased reward function using only the policy models (Rafailov et al., 2023) The implicit reward used in DPO is a function of the policy model probabilities (i.e. the model being trained), $\pi(y \mid x)$, a regularization constant, $\beta$, the base model probabilities, $\pi_{\text {ref }}(y \mid x)$, and a partition function $Z(x)$ :</p>
<p>$$
r(x, y)=\beta \log \frac{\pi(y \mid x)}{\pi_{\text {ref }}(y \mid x)}+\beta \log Z(x)
$$</p>
<p>Given two completions to a prompt, we compare the rewards $r\left(x, y_{1}\right)$ and $r\left(x, y_{2}\right)$ as follows, where the score is computed via the log ratios of $\pi: \log \frac{\pi\left(y_{1} \mid x\right)}{\pi_{\text {ref }}\left(y_{1} \mid x\right)}&gt;\log \frac{\pi\left(y_{2} \mid x\right)}{\pi_{\text {ref }}\left(y_{2} \mid x\right)}$.</p>
<h2>4 The ReWARDBench Benchmark</h2>
<p>In this section, we detail the design philosophy and construction of the evaluation dataset. The dataset is designed to provide a broad set of basic evaluations for reward models, covering chat, instruction following, coding, safety, and other important metrics for fine-tuned language models. The ReWARDBench dataset contains a combination of existing evaluation prompt-completion pairs, and those curated for this project.</p>
<p>A good reward function, and therefore a good RM broadly, is one that stably assigns credit to the classes of good or bad content.Given one verified answer that is better than another for factual or clear qualitative reasons (e.g. typos), a good reward model will choose the correct one $100 \%$ of the time. To evaluate this, each datapoint consists of a prompt and two completions, chosen and rejected. For each prompt, the score of the reward model is computed. The prompt is then categorized as a win if the score of the prompt with the verified chosen completion is higher than that of the verified rejected completion, as shown in Fig. 1. Finally, we report accuracy for each subset as the percentage of wins. For all the section scores of ReWARDBench (e.g. Chat or Safety) except Prior Sets, the average score is weighted per-prompt in the requisite subsets.</p>
<h1>4.1 ReWARDBench Dataset</h1>
<p>The benchmark is broken down into five sections from different subsets - the first four compose the ReWARDBench dataset described in this section. We have broken down the dataset into these subsections to create one final ReWARDBench score in order to reasonably weigh different aspects of an RM's performance. The RewardBench dataset is released under the ODC-BY license ${ }^{2}$ and the code is released under Apache $2.0^{3}$. The summary of the dataset is shown in Tab. 1 (see appendix F for full details) At a high level, the subsets consist of the following:</p>
<ol>
<li>Chat: Testing a reward model's basic ability to distinguish a thorough and correct chat response in open-ended generation. Prompts and chosen, rejected pairs are selected from AlpacaEval (Li et al., 2023b) and MT Bench (Zheng et al., 2023), two popular open-ended chat evaluation tools.</li>
<li>Chat Hard: Testing a reward model's abilities to understand trick questions and subtly different instruction responses. Prompts and chosen, rejected pairs are selected from MT Bench examples with similar ratings and adversarial data specifically for fooling LLM-as-a-judge tools from LLMBar's evaluation set (Zeng et al., 2023) (reformatted for RMs).</li>
<li>Safety: Testing the models' tendencies to refuse dangerous content and to avoid incorrect refusals to similar trigger words. Prompts and chosen, rejected pairs are selected from custom versions of the datasets XSTest (Röttger et al., 2023), Do-Not-Answer (Wang et al., 2023), and examples from an in-development refusals dataset at AI2, where the chosen response is a refusal and the rejected is harmful text of either dangerous or offensive nature.</li>
<li>Reasoning: Evaluating the models code and reasoning abilities. Code prompts are created by reformatting HumanEvalPack examples with correct code as chosen and rejected as one with bugs (Muennighoff et al., 2023). Reasoning prompts pair reference answers with incorrect model generations from the PRM800k dataset (Lightman et al., 2023).</li>
<li>Prior Sets ${ }^{5}$ : For consistency with recent work on training reward models, we average performance over test sets from existing preference datasets. We use the Anthropic Helpful split (Bai et al., 2022a) (the only multi-turn data), the Anthropic HHH subset of BIG-Bench (Askell et al., 2021), a curated subset of the test set from the Stanford Human Preferences (SHP) Dataset (Ethayarajh et al., 2022), and OpenAI's Learning to Summarize Dataset (Stiennon et al., 2020).</li>
</ol>
<h3>4.2 ReWARDBench Scoring</h3>
<p>REWARDBENCH is scored via accuracy. For each prompt-chosen-rejected trio, we infer the score the RM assigns for the prompt-chosen and prompt-rejected pairs then assign a true classification label when the chosen score is higher than rejected, as highlighted in Fig. 1. Details on computing scores for classifiers and DPO models is in Sec. 3. Given the binary classification task, a random model achieves a result of $50 \%$. In order to create a representative, single evaluation score, we perform a mixture of averaging across results. For the sections detailed in Sec. 4.1 except for Reasoning, we perform per-prompt weighted averaging across the subsets to get the normalized section scores. For example, in Chat we take a weighted average of the AlpacaEval and MT Bench sets based on the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Top-20 open models on ReWARDBench. Evaluating many RMs shows that there is still large variance in RM training and potential for future improvement across the more challenging instruction and reasoning tasks. Icons refer to model types: Sequence Classifier ( $\boxtimes$ ), Direct Preference Optimization ( $\boxtimes$ ), Custom Classifier ( $\$$ ), Generative Model ( $\boxtimes$ ), and a random model ( $\square$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reward Model</th>
<th style="text-align: right;">Score</th>
<th style="text-align: right;">Chat</th>
<th style="text-align: right;">Chat <br> Hard</th>
<th style="text-align: right;">Safety</th>
<th style="text-align: right;">Reason</th>
<th style="text-align: right;">Prior <br> Sets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RLHFlow/ArmoRM-Llama3-8B-v0.1</td>
<td style="text-align: right;">$\mathbf{8 9 . 0}$</td>
<td style="text-align: right;">96.9</td>
<td style="text-align: right;">$\mathbf{7 6 . 8}$</td>
<td style="text-align: right;">$\mathbf{9 2 . 2}$</td>
<td style="text-align: right;">$\mathbf{9 7 . 3}$</td>
<td style="text-align: right;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">RLHFlow/pair-preference-model-LLaMA3-8B</td>
<td style="text-align: right;">85.7</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">65.8</td>
<td style="text-align: right;">89.7</td>
<td style="text-align: right;">94.7</td>
<td style="text-align: right;">74.6</td>
</tr>
<tr>
<td style="text-align: left;">sfairXC/FsfairX-LLaMA3-RM-v0.1</td>
<td style="text-align: right;">83.6</td>
<td style="text-align: right;">$\mathbf{9 9 . 4}$</td>
<td style="text-align: right;">65.1</td>
<td style="text-align: right;">87.8</td>
<td style="text-align: right;">86.4</td>
<td style="text-align: right;">74.9</td>
</tr>
<tr>
<td style="text-align: left;">openbmb/Eurus-RM-7b</td>
<td style="text-align: right;">81.6</td>
<td style="text-align: right;">98.0</td>
<td style="text-align: right;">65.6</td>
<td style="text-align: right;">81.2</td>
<td style="text-align: right;">86.3</td>
<td style="text-align: right;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">Nexusflow/Starling-RM-34B</td>
<td style="text-align: right;">81.4</td>
<td style="text-align: right;">96.9</td>
<td style="text-align: right;">57.2</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">88.5</td>
<td style="text-align: right;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">weqweasdas/RM-Mistral-7B</td>
<td style="text-align: right;">79.3</td>
<td style="text-align: right;">96.9</td>
<td style="text-align: right;">58.1</td>
<td style="text-align: right;">87.1</td>
<td style="text-align: right;">77.0</td>
<td style="text-align: right;">$\mathbf{7 5 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">hendrydong/Mistral-RM-for-RAFT-GSHF-v0</td>
<td style="text-align: right;">78.7</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">57.9</td>
<td style="text-align: right;">86.3</td>
<td style="text-align: right;">74.3</td>
<td style="text-align: right;">75.1</td>
</tr>
<tr>
<td style="text-align: left;">stabilityai/stablelm-2-12b-chat</td>
<td style="text-align: right;">77.4</td>
<td style="text-align: right;">96.6</td>
<td style="text-align: right;">55.5</td>
<td style="text-align: right;">82.6</td>
<td style="text-align: right;">89.4</td>
<td style="text-align: right;">48.4</td>
</tr>
<tr>
<td style="text-align: left;">Ray2333/reward-model-Mistral-7B-instruct...</td>
<td style="text-align: right;">76.9</td>
<td style="text-align: right;">97.8</td>
<td style="text-align: right;">50.7</td>
<td style="text-align: right;">86.7</td>
<td style="text-align: right;">73.9</td>
<td style="text-align: right;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">allenai/tulu-2-dpo-70b</td>
<td style="text-align: right;">76.1</td>
<td style="text-align: right;">97.5</td>
<td style="text-align: right;">60.5</td>
<td style="text-align: right;">83.9</td>
<td style="text-align: right;">74.1</td>
<td style="text-align: right;">52.8</td>
</tr>
<tr>
<td style="text-align: left;">meta-llama/Meta-Llama-3-70B-Instruct</td>
<td style="text-align: right;">75.4</td>
<td style="text-align: right;">97.6</td>
<td style="text-align: right;">58.9</td>
<td style="text-align: right;">69.2</td>
<td style="text-align: right;">78.5</td>
<td style="text-align: right;">70.4</td>
</tr>
<tr>
<td style="text-align: left;">prometheus-eval/prometheus-8x7b-v2.0</td>
<td style="text-align: right;">75.3</td>
<td style="text-align: right;">93.0</td>
<td style="text-align: right;">47.1</td>
<td style="text-align: right;">83.5</td>
<td style="text-align: right;">77.4</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">NousResearch/Nous-Hermes-2-Mistral-7B-DPO</td>
<td style="text-align: right;">74.8</td>
<td style="text-align: right;">92.2</td>
<td style="text-align: right;">60.5</td>
<td style="text-align: right;">82.3</td>
<td style="text-align: right;">73.8</td>
<td style="text-align: right;">55.5</td>
</tr>
<tr>
<td style="text-align: left;">mistralai/Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: right;">74.7</td>
<td style="text-align: right;">95.0</td>
<td style="text-align: right;">64.0</td>
<td style="text-align: right;">73.4</td>
<td style="text-align: right;">78.7</td>
<td style="text-align: right;">50.3</td>
</tr>
<tr>
<td style="text-align: left;">upstage/SOLAR-10.7B-Instruct-v1.0</td>
<td style="text-align: right;">74.0</td>
<td style="text-align: right;">81.6</td>
<td style="text-align: right;">68.6</td>
<td style="text-align: right;">85.5</td>
<td style="text-align: right;">72.5</td>
<td style="text-align: right;">49.5</td>
</tr>
<tr>
<td style="text-align: left;">HuggingFaceH4/zephyr-7b-alpha</td>
<td style="text-align: right;">73.4</td>
<td style="text-align: right;">91.6</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">74.3</td>
<td style="text-align: right;">75.1</td>
<td style="text-align: right;">53.5</td>
</tr>
<tr>
<td style="text-align: left;">allenai/tulu-2-dpo-13b</td>
<td style="text-align: right;">73.4</td>
<td style="text-align: right;">95.8</td>
<td style="text-align: right;">58.3</td>
<td style="text-align: right;">78.2</td>
<td style="text-align: right;">73.2</td>
<td style="text-align: right;">49.5</td>
</tr>
<tr>
<td style="text-align: left;">0-hero/Matter-0.1-7B-boost-DPO-preview</td>
<td style="text-align: right;">73.4</td>
<td style="text-align: right;">91.1</td>
<td style="text-align: right;">61.0</td>
<td style="text-align: right;">66.3</td>
<td style="text-align: right;">83.9</td>
<td style="text-align: right;">55.7</td>
</tr>
<tr>
<td style="text-align: left;">prometheus-eval/prometheus-7b-v2.0</td>
<td style="text-align: right;">72.4</td>
<td style="text-align: right;">85.5</td>
<td style="text-align: right;">49.1</td>
<td style="text-align: right;">78.7</td>
<td style="text-align: right;">76.5</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">HuggingFaceH4/starchat2-15b-v0.1</td>
<td style="text-align: right;">72.1</td>
<td style="text-align: right;">93.9</td>
<td style="text-align: right;">55.5</td>
<td style="text-align: right;">65.8</td>
<td style="text-align: right;">81.6</td>
<td style="text-align: right;">55.2</td>
</tr>
</tbody>
</table>
<p>number of prompts. For Reasoning, we increase the weight of the PRM-Math subset so code and math abilities are weighed equally in the final number. For Prior Sets, we take an unweighted average over the subsets due to the large disparity in dataset sizes. Once all subsets weighted averages are achieved, the final ReWARDBench score is the weighted average across the section scores (Prior Sets at 0.5 weight).</p>
<h1>5 Evaluation Results</h1>
<p>REWARDBENCH includes evaluation of many public reward models, ranging in parameter count from 400 million (PairRM) to 70 billion (Tülu 2), trained as classifiers or with DPO (when the reference model is available). In this section, we detail the core findings of ReWARDBench , and more results are available in Appendix E. In particular, we study the state-of-the-art reward models (Tab. 2), results of similar-size models at 7B (Tab. 4), and a demonstration of the impact of scaling DPO reward models on performance in Tab. 3. We further study the limits of current reward models (Section 5.2) and prior test sets (Section 5.3).</p>
<h3>5.1 Comparing State-of-the-art Reward Models</h3>
<p>Tab. 2 shows the results for the top 20 models across different model sizes and types. Large models and those trained on Llama 3 are the only models capable of high performance on the Chat Hard and Reasoning sections, with the model ArmoRM-Llama3-8B-v0.1 (89) being state-of-the-art. Across different base models, scale is a crucial property, with Starling-RM-34B (81.4) trained on Yi 34B and Tulu-2-DPO-70B (76.1) on Llama 2 being top models. The best open-weight models for LLM-as-a-judge are Meta-Llama-3-70B-Instruct (75.4) and prometheus-8x7b-v2.0 (75.3) (Kim et al., 2024), though they still fall well below classifier-based RMs. The final category is comprised of the small, most accessible models, where the leading models are StableLM-zephyr-3b (70.6) and oasst-rm-2.1-pythia-1.4b-epoch-2.5 (69.5), but there is substantial room for progress.</p>
<p>Table 3: RewardBench results for two model groups, Tülu and Qwen-Chat, with a broad range of model sizes with fixed datasets, showcasing the scaling performance of DPO reward models. Scaling reward models, at least those trained with DPO, shows clear improvements in performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reward Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Chat</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prior</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sets</td>
</tr>
<tr>
<td style="text-align: left;">allenai/tulu-2-dpo-70b</td>
<td style="text-align: center;">$\mathbf{7 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 1}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">allenai/tulu-2-dpo-13b</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: left;">allenai/tulu-2-dpo-7b</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">$\mathbf{9 7 . 5}$</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">47.7</td>
</tr>
<tr>
<td style="text-align: left;">Qwen/Qwen1.5-72B-Chat</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: left;">Qwen/Qwen1.5-14B-Chat</td>
<td style="text-align: center;">$\mathbf{6 9 . 8}$</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">$\mathbf{7 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 3}$</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">Qwen/Qwen1.5-7B-Chat</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">42.9</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparing 7B class models. Top shows some Zephyr-style fine-tuned models (Tunstall et al., 2023), showcasing the variance across base models and implementation. Bottom is other top 7B models, trained with various methods and datasets. Icons refer to model types: Sequence Classifier ( $\boxtimes$ ), Custom Classifier ( $\mathscr{H}$ ), or DPO ( $\boxtimes$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reward Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Chat</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prior</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sets</td>
</tr>
<tr>
<td style="text-align: center;">(1) HuggingFaceH4/zephyr-7b-alpha</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">(1) HuggingFaceH4/zephyr-7b-beta</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: center;">(1) allenai/tulu-2-dpo-7b</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">47.7</td>
</tr>
<tr>
<td style="text-align: center;">(1) allenai/OLMo-7B-Instruct</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">(1) HuggingFaceH4/zephyr-7b-gemma-v0.1</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">$\mathscr{H}$ RLHFlow/ArmoRM-Llama3-8B-v0.1</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;">$\mathscr{H}$ RLHFlow/pair-preference-model-LLaMA3-8B</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">74.6</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ sfairXC/FsfairX-LLaMA3-RM-v0.1</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ openbmb/Eurus-RM-7b</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ weqweasdas/RM-Mistral-7B</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">75.3</td>
</tr>
</tbody>
</table>
<p>The Impacts of Different Base Models In our evaluation there are multiple models trained either with the same or very similar fine-tuning approaches on different base models. We show the impact of scaling across different Llama 2, via Tulu 2 (Ivison et al., 2023), and Qwen 1.5 versions in Tab. 3. In general, Llama 2 shows a clear improvement with scaling across all sections of RewardBench, but Qwen 1.5 shows less monotonic improvement, likely due to out of distribution generalization challenges. Tab. 4 compares the impact of different base models and subtle changes of fine-tuning methods via the Zephyr-class models (Tunstall et al., 2023). Each of these models are fine-tuned on the UltraFeedback dataset via DPO as the final stage, with different base models and instructiontuning before. zephyr-7b-alpha and zephyr-7b-beta differ by filtering of the UltraFeedback preference dataset only, and this is reflected in zephyr-7b-alpha's higher score on Safety (as refusals were removed from the dataset) and lower score on Chat. tulu-2-dpo-7b highlights the difference from the Mistral 7B to the Llama 2 7B base models and a different supervised fine-tuning dataset pre DPO, as regressions on Chat Hard and Reasoning, but improvements on Safety.</p>
<p>Different Shapes of Reward Functions The per-prompt scores demonstrate the different magnitudes and distributions of rewards assigned to each reward model over the RewardBench evaluation dataset. Results shown in Appendix E.1, such as Fig. 7, show these distributions for some RMs trained as a classifier. Few RMs are Gaussian in their scores across the RewardBench datasets, fewer RMs are centered around 0 reward, and none we tested centered Gaussians. Future work should identify a preferred RM output distribution for downstream RL training.</p>
<h1>5.2 Limits of Current Reward Models</h1>
<p>Current reward models can solve some subsets of RewardBench reliably, approaching $100 \%$ accuracy, but many subsets experience a combination of low ceilings on performance or high variance of performance. The subsets with low ceilings, mostly in the Chat Hard and Reasoning sections</p>
<p>Table 5: Different categories of performance on Chat Hard, where only a few models obtain strong results (top). Middle shows where some of the top overall reward models land on the subset and bottom shows how some average-overall RMs struggling on this section (performing worse than random). Icons refer to model types: Sequence Classifier ( $\boxtimes$ ), DPO ( $\Theta$ ), and random ( $\Theta$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MTBench</th>
<th style="text-align: center;">LLMBar</th>
<th style="text-align: center;">LLMBar Adversarial</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reward Model</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Natural</td>
<td style="text-align: center;">Neighbor</td>
<td style="text-align: center;">GPTInst</td>
<td style="text-align: center;">GPTOut</td>
<td style="text-align: center;">Manual</td>
</tr>
<tr>
<td style="text-align: left;">$\pi$ RLHFlow/ArmoRM-Llama3-8B-v0.1</td>
<td style="text-align: center;">$\mathbf{7 6 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">$\mathbf{7 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ Qwen/Qwen1.5-14B-Chat</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">$\mathbf{8 3 . 6}$</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ upstage/SOLAR-10.7B-Instruct-v1.0</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">67.4</td>
</tr>
<tr>
<td style="text-align: left;">$\boxtimes$ openbmb/UltraRM-13b</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ allenai/tulu-2-dpo-13b</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">$\boxtimes$ berkeley-nest/Starling-RM-34B</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ HuggingFaceH4/zephyr-7b-gemma-v0.1</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: left;">$\boxtimes$ IDEA-CCNL/Ziya-LLaMA-7B-Reward</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: left;">$\boxtimes$ berkeley-nest/Starling-RM-7B-alpha</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">28.3</td>
</tr>
</tbody>
</table>
<p>Table 6: A subset of results for the Safety category grouped by behavior type. Top: Example reward models that tend to correctly prefer refusals of sensitive prompts and prefer responding to prompts with potential trigger words. Middle: Example reward models that have a propensity to choose a refusal for every request, including those that should be responded to. Bottom: Example reward models that have a propensity to choose a compliance to every request, even those that should be refused. Model types: Sequence Classifier ( $\boxtimes$ ), Custom Classifier ( $\pi$ ), and DPO ( $\Theta$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Refusals</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSTest Should</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Do Not <br> Answer</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reward Model</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Dang.</td>
<td style="text-align: center;">Offen.</td>
<td style="text-align: center;">Refuse</td>
<td style="text-align: center;">Respond</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\pi$ RLHFlow/ArmoRM-Llama3-8B-v0.1</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\boxtimes$ Nexusflow/Starling-RM-34B</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ allenai/tulu-2-dpo-70b</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ stabilityai/stablelm-2-12b-chat</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ Qwen/Qwen1.5-14B-Chat</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\boxtimes$ IDEA-CCNL/Ziya-LLaMA-7B-Reward</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\boxtimes$ openbmb/UltraRM-13b</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\checkmark$ HuggingFaceH4/zephyr-7b-gemma-v0.1</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>indicate areas where preference datasets and reward modeling methods can be extended to improve performance, and subsets with high variability, such as many of the Safety subsets, indicate areas where best practices can be converged upon.</p>
<p>Evaluating across Chat Hard Categories Tab. 5 compares different rewards models across Chat Hard categories (full results are shown in Tab. 11). The adversarial subsets from LLMBar (Zeng et al., 2023) are crucial to understanding RMs because they show examples where two answers are written in a similar style (e.g. the same GPT-4 model version), but with slightly different subjects. The difference between asking a factual question about a related but different object or slightly changing the context of a prompt, is hard to pick up with most reward models. The Chat Hard section (and to some extent Reasoning) is largely correlated with final performance, but some DPO models excel at it and not overall - even Qwen Chat and others with low average performance overall. The models scoring highly largely are trained on recent base models and preference datasets, showcasing recent progress on RM training.</p>
<p>Evaluating across Reasoning Categories The Reasoning section of ReWARDBench has the widest, smooth variation in performance - e.g. models populate many levels, from $35 \%$ accuracy (well below random) all the way to $97 \%$ accuracy. The reasoning data largely relies on code examples where just one or two tokens are different between the chosen and rejected samples, showcasing precise classification abilities of the best RMs. Full reasoning results are included in Tab. 13.</p>
<p>Evaluating across Safety Metrics Tab. 6 (full results in Tab. 12 in Appendix) compares different reward models across different safety categories, indicating challenges on striking a balance between refusing too much or not refusing. Models, such as UltraRM-13b and zephyr-7b-gemma-v0.1 show how a model focused on helpfulness without a strong notion of safety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest Should Respond. Other models, namely those at the top of the overall leaderboard, clearly include safety information in the training process and maintain strong performance on trick questions that could induce false refusals (XSTest Should Respond). Finally, the mirrored behavior, those models that score highly on prompts that they should refuse and poorly on those they should not are present, indicating a model that is likely to falsely refusal queries (e.g. the Qwen chat models). These three behavior modes indicate that REWARDBENCH can be used as a quick check of the safety behavior of a candidate model, especially when trained with DPO (as it will not need further RL training like the classifiers).</p>
<h1>5.3 Limitations of Prior Test Sets</h1>
<p>Many popular models trained with RLHF use new preference datasets such as UltraFeedback (Cui et al., 2023) or Nectar (Zhu et al., 2023a), which don't have publicly available validation sets. Given this, when training reward models, common practice is to compare model agreement with a variety of existing test sets from earlier work in RLHF. Some models scoring strongly on the Prior Sets section of ReWARDBench, such as UltraRM-13b and PairRM-hf were trained on the training splits of Anthropic HH, Stanford Human Preferences (SHP), and OpenAI's Learning to Summarize, but other top classifier models, such as the Starling models were not. Combining this with the very low average score of DPO models on these test sets indicates that substantial research is needed to understand the full limitations of these previous datasets. Full results are detailed in Tab. 14.</p>
<h2>6 Conclusion</h2>
<p>We present ReWARDBench, and show the variety of performance characteristics of current reward models in order to improve understanding of RLHF. While we covered a variety of topics important to alignment of LMs, a crucial next step is needed to correlate performance in REWARDBENCH to RLHF usefulness. Initial experiments with ranking RMs with best-of-N sampling and downstream training with PPO are underway. We have taken a first step to understanding which values are embedded in the RLHF training across many base models and preference datasets. The toolkit we have released can easily be expanded include custom data to specifically audit a certain property of the RLHF process. Scores of RMs from private LM providers are on the public leaderboard, but are not in the paper because they are not reproducible. ReWARDBENCH is one of many tools which will help us understand the science of whose and what values are embedded in our language models.</p>
<h1>Acknowledgements</h1>
<p>The authors would like to thank Thomas Gilbert for early discussions that helped motivate this project. Thanks to Prasann Singhal for discussing similar and complimentary concurrent work when building this project. Thanks to Hamish Ivision for helping with the math data filtering code. Thanks to Matt Latzke for help with the logo and design artifacts.</p>
<h2>References</h2>
<p>Marwa Abdulhai, Gregory Serapio-Garcia, Clément Crepy, Daria Valter, John Canny, and Natasha Jaques. Moral foundations of large language models. arXiv preprint arXiv:2310.15337, 2023.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv preprint arXiv:2212.08073, 2022b.</p>
<p>Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable LM 2 1.6B Technical Report. arXiv preprint arXiv:2402.17834, 2024.</p>
<p>Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952. ISSN 00063444. URL http:// www.jstor.org/stable/2334029.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017.</p>
<p>Joshua Clymer, Garrett Baker, Rohan Subramani, and Sam Wang. Generalization analogies (genies): A testbed for generalizing ai oversight to hard-to-measure domains. arXiv preprint arXiv:2311.07723, 2023.</p>
<p>Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting Language Models with High-quality Feedback. arXiv preprint arXiv:2310.01377, 2023.</p>
<p>Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314, 2023.</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. arXiv preprint arXiv:2304.06767, 2023.</p>
<p>Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring the representation of subjective global opinions in language models. arXiv preprint arXiv:2306.16388, 2023.</p>
<p>Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\mathcal{V}$ usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5988-6008. PMLR, 17-23 Jul 2022.</p>
<p>Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024a.</p>
<p>Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Railneau. Glore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint arXiv:2402.10963, 2024b.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. NeurIPS, 2021.</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a Changing Climate: Enhancing LM Adaptation with Tülu 2. arXiv preprint arXiv:2311.10702, 2023.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a.</p>
<p>Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. Tigerscore: Towards building explainable metric for all text generation tasks. ArXiv, abs/2310.00752, 2023b. URL https://api.semanticscholar.org/CorpusID:263334281.</p>
<p>Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise comparison and generative fusion. In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL 2023), 2023c.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023.</p>
<p>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024.</p>
<p>Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. The history and risks of reinforcement learning and human feedback. arXiv e-prints, pages arXiv-2310, 2023.</p>
<p>Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023.</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023a.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An Automatic Evaluator of Instructionfollowing Models. https://github.com/tatsu-lab/alpaca_eval, 2023b.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's Verify Step by Step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.</p>
<p>Maximilian Mozes, Jessica Hoffmann, Katrin Tomanek, Muhamed Kouate, Nithum Thain, Ann Yuan, Tolga Bolukbasi, and Lucas Dixon. Towards agile text classifiers for everyone, 2023.</p>
<p>Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction Tuning Code Large Language Models. arXiv preprint arXiv:2308.07124, 2023.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Andrew Ng and Michael Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems, 14, 2001 .</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.</p>
<p>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022. URL https://arxiv.org/abs/2210. 01241 .</p>
<p>Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. arXiv preprint arXiv:2308.01263, 2023.</p>
<p>Michael J. Ryan, William Held, and Diyi Yang. Unintended impacts of llm alignment on global representation, 2024.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>John Schulman, Barret Zoph, Christina Kim, and more. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt/, 2022. Accessed: 2023-02-12.</p>
<p>Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, and Dong Yu. The trickle-down impact of reward (in-) consistency on rlhf. arXiv preprint arXiv:2309.16155, 2023.</p>
<p>Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct Distillation of LM Alignment. arXiv preprint arXiv:2310.16944, 2023.</p>
<p>Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Secrets of rlhf in large language models part ii: Reward modeling, 2024.</p>
<p>Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv preprint arXiv:2308.13387, 2023.</p>
<p>Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023.</p>
<p>Sierra Wyllie, Ilia Shumailov, and Nicolas Papernot. Fairness feedback loops: Training on synthetic data amplifies bias, 2024.</p>
<p>Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024a.</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024b.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating Large Language Models at Evaluating Instruction Following. arXiv preprint arXiv:2310.07641, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7B: Improving LLM Helpfulness \&amp; Harmlessness with RLAIF, November 2023a. URL https://starling.cs. berkeley.edu/.</p>
<p>Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631, 2023b.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>Checklist</h1>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See section Appendix A.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See section Appendix A.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]</li>
<li>If you ran experiments (e.g. for benchmarks)...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Available on first page, and also here: https://github.com/allenai/reward-bench.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? $[\mathrm{N} / \mathrm{A}]$
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] There is a small amount of variability that could come when evaluating reward models, though the temperature should be set to 0 and have substantially lower variance than training experiments.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix C.</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] Primarily in Sec. 4.1, we clearly cite all the datasets we built upon in this work. The code is almost entirely new, but in-line comments exist on GitHub, e.g. for the source code of models for inference.
(b) Did you mention the license of the assets? [Yes] See Section 4.1 for datasets, which are all permissively licensed. The code copied was either released with no license (e.g. in a model card) or with a license that does not require noting it (Apache / MIT).
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We have included a substantial amount of assets via URL (of which, all should be in the main text. For example, the Leaderboard ${ }^{5}$ is only useful as an online artifact. Other artifacts such as the full results from evaluation and the evaluation datasets themselves are linked externally.
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] The data was either generated by an LLM, by the team, or from previously released narrow benchmarks.
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] It is low risk, but discussed in Appendix A, particularly for the Safety section of the benchmark.</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] Though, the authors did have explicit instructions for data collection, which are detailed in Appendix. I. We did not use any additional crowdsourcing.
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? $[\mathrm{N} / \mathrm{A}]$</li>
</ol>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Appendices</h1>
<p>A Limitations \&amp; Broader Impacts ..... 16
B Discussions ..... 16
C Compute Usage ..... 18
D Codebase Discussion ..... 18
E Additional Results ..... 18
E. 1 Subset Distributions ..... 28
E. 2 Model Reward Distributions ..... 28
F Dataset Details ..... 29
G Discussion on Prior Test Sets ..... 34
H Dataset Characteristics ..... 34
H. 1 Source of chosen and rejected completions ..... 34
H. 2 Investigating length bias ..... 34
I Data processing notes ..... 35
I. 1 Data processing instructions ..... 35
I. 2 MT Bench filtering ..... 36
I. 3 AlpacaEval filtering ..... 38
I. 4 Refusals data ..... 38
I. 5 XSTest filtering ..... 38</p>
<h1>A Limitations \&amp; Broader Impacts</h1>
<p>Limitations The RewardBench benchmark is limited by a couple of factors. First, we lack human preference data and instead, except for specific subsets, have to rely on semi-automatic ways of obtaining chosen-rejected pairs, which we then manually validate. We also note that the formats in certain domains, such as the reasoning domain, might potentially include spurious correlations leading to possible biases in humans and models. Another unresolved question is whether and how the benchmark results correlate with downstream training. Lastly, there might be a chance of possible data contamination, in cases where models are (wrongly) directly trained on alpacaeval or MTBench data.</p>
<p>Broader Impacts This work does expose potentially offensive and or sensitive text to users through the rejected samples of the Safety section of the benchmark. Therefore users should use this data at their own risk. Given the preexisting prompts from other benchmarks, we are not worried about eliciting personally identifiable information.</p>
<h2>B Discussions</h2>
<p>Evaluating Length Bias Given the results showing length bias in RLHF and reward models (Singhal et al., 2023), we designed ReWARDBench so that the chosen responses are either a similar length or shorter than the rejected responses. For example, the AlpacaEval Length subset is designed to differentiate between other Chat subsets by having notably different models capabilities with the same average length (results in Tab. 10). In this case, the results are lower than other easy chat subsets, but $90 \%$ plus accuracy is achieved by over 10 models - far above random for most models. Though, more detailed statistical tests are needed to fully understand this, as this only tests the reward models' abilities to discern information without the help of length as a proxy. More details on the length distributions of REWARDBench are found in Appendix H.2.</p>
<p>DPO Models vs Classifiers Since DPO-trained LLMs are implicit reward models largely used for their generative abilities, the question of how they compare to RMs trained as classifiers is unstudied. There are currently more DPO models released to the public, partially due to DPO requiring notably fewer computational resources among other factors such as existing implementations and relevant datasets. We see that the results on ReWARDBench flatter the recent DPO methods, except for the Prior Sets section. For how the DPO reward is computed, see Sec. 3.
The same inference code of popular DPO training implementations can easily be used for evaluation as an RM by not propagating gradients through the models. The simplest implementations requires more GPU memory to run evaluation of DPO-trained models given the two models needed to compute the reward, but this can be avoided by computing the probabilities over the policy and base models sequentially. Though, some of the released DPO models do not clearly document which reference model is used in training (e.g. if it is a base model or a model obtained via supervised fine-tuning), which can result in unclear benchmarking. ${ }^{6}$ When a reference model is unavailable or compute is constrained, an alternative approach in such cases would be to obtain a reference free reward: $\pi\left(y_{1} \mid x\right)&gt;\pi\left(y_{2} \mid x\right)$, which could be normalized using different approaches. Without normalization, the loss has a length penalty by summing over probabilities of each token which are all negative numbers. We will explore the impacts of reference free inference in future work.
We also experimentedwith using the "wrong" reference model, i.e. a similar but different base model, and found that this reduced the DPO trained RM performance to similar levels as the random baseline.</p>
<p>There is still a lot that is unknown about the best practices of training RMs: trained with DPO they are regularized by KL distance, but the classifiers are not. Additionally, a common practice for training RMs via classification is to train for 1 epoch (Ouyang et al., 2022), while DPO models are usually trained for more than 1 epoch (Tunstall et al., 2023; Ivison et al., 2023). Other future work ideas therefore include analyzing the role of the training hyperparameters in DPO training and RM</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 7: Comparing 10 DPO performance with and without the reference model. The DPO models show clear reductions in performance without the required reference model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reward Model</th>
<th style="text-align: center;">Avg</th>
<th style="text-align: center;">Ref. <br> Free</th>
<th style="text-align: center;">Delta</th>
<th style="text-align: center;">Chat <br> Hard</th>
<th style="text-align: center;">Chat <br> Safety</th>
<th style="text-align: center;">Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">mistralai/Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">-18.0</td>
<td style="text-align: center;">-6.4</td>
<td style="text-align: center;">-28.5</td>
<td style="text-align: center;">-35.3</td>
</tr>
<tr>
<td style="text-align: left;">allenai/tulu-2-dpo-13b</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">-15.9</td>
<td style="text-align: center;">-10.3</td>
<td style="text-align: center;">-19.0</td>
<td style="text-align: center;">-36.5</td>
</tr>
<tr>
<td style="text-align: left;">HuggingFaceH4/zephyr-7b-alpha</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">-13.0</td>
<td style="text-align: center;">-10.9</td>
<td style="text-align: center;">-10.5</td>
<td style="text-align: center;">-31.0</td>
</tr>
<tr>
<td style="text-align: left;">NousResearch/Nous-Hermes-2-Mistral-7B-DPO</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">-15.6</td>
<td style="text-align: center;">-6.1</td>
<td style="text-align: center;">-21.2</td>
<td style="text-align: center;">-48.7</td>
</tr>
<tr>
<td style="text-align: left;">allenai/tulu-2-dpo-7b</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">-14.8</td>
<td style="text-align: center;">-12.0</td>
<td style="text-align: center;">-20.9</td>
<td style="text-align: center;">-32.1</td>
</tr>
<tr>
<td style="text-align: left;">HuggingFaceH4/zephyr-7b-beta</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">-10.9</td>
<td style="text-align: center;">-9.2</td>
<td style="text-align: center;">-16.6</td>
<td style="text-align: center;">-18.3</td>
</tr>
<tr>
<td style="text-align: left;">stabilityai/stablelm-zephyr-3b</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">-13.6</td>
<td style="text-align: center;">-1.7</td>
<td style="text-align: center;">-22.0</td>
<td style="text-align: center;">-34.0</td>
</tr>
<tr>
<td style="text-align: left;">0-hero/Matter-0.1-7B-DPO-preview</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">-13.1</td>
<td style="text-align: center;">-5.9</td>
<td style="text-align: center;">-23.3</td>
<td style="text-align: center;">-23.1</td>
</tr>
<tr>
<td style="text-align: left;">Qwen/Qwen1.5-72B-Chat</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">-8.1</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">-30.7</td>
<td style="text-align: center;">-26.8</td>
</tr>
<tr>
<td style="text-align: left;">Qwen/Qwen1.5-14B-Chat</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">-6.6</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">-29.1</td>
<td style="text-align: center;">-30.6</td>
</tr>
<tr>
<td style="text-align: left;">Qwen/Qwen1.5-7B-Chat</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">-4.5</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">-29.9</td>
<td style="text-align: center;">-27.9</td>
</tr>
<tr>
<td style="text-align: left;">HuggingFaceH4/zephyr-7b-gemma-v0.1</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">-7.9</td>
<td style="text-align: center;">-11.5</td>
<td style="text-align: center;">-15.9</td>
<td style="text-align: center;">-9.8</td>
</tr>
<tr>
<td style="text-align: left;">stabilityai/stablelm-2-zephyr-1_6b</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">-10.0</td>
<td style="text-align: center;">-16.2</td>
<td style="text-align: center;">-9.7</td>
<td style="text-align: center;">-16.9</td>
</tr>
<tr>
<td style="text-align: left;">allenai/OLMo-7B-Instruct</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">-9.8</td>
<td style="text-align: center;">-6.1</td>
<td style="text-align: center;">-13.7</td>
<td style="text-align: center;">-25.3</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparing state of the art generative LLMs. Models with weights available are denoted with $[\mathbf{O}]$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reward Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Chat</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Safety</th>
<th style="text-align: center;">Reason</th>
<th style="text-align: center;">Prior <br> Sets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">google/gemini-1.5-pro-0514</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">openai/gpt-4-0125-preview</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">openai/gpt-4-turbo-2024-04-09</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">73.6</td>
</tr>
<tr>
<td style="text-align: left;">openai/gpt-4o-2024-05-13</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: left;">openai/gpt-4o-2024-05-13</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: left;">google/gemini-1.5-pro-0514</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">69.4</td>
</tr>
<tr>
<td style="text-align: left;">Anthropic/claude-3-opus-20240229</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">[O] meta-llama/Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: left;">[O] prometheus-eval/prometheus-8x7b-v2.0</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Anthropic/claude-3-sonnet-20240229</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">69.6</td>
</tr>
<tr>
<td style="text-align: left;">Anthropic/claude-3-haiku-20240307</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">66.3</td>
</tr>
<tr>
<td style="text-align: left;">[O] prometheus-eval/prometheus-7b-v2.0</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">[O] CohereForAI/c4ai-command-r-plus</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">69.2</td>
</tr>
</tbody>
</table>
<p>classification performance (such as Beta KL regularization on generated text, number of training epochs, etc.).</p>
<p>Generative Reward Modeling An alternate to classifier based reward models, which are discriminative ( Ng and Jordan, 2001), is to use generations from a language model to create a judgement between two answers (Zheng et al., 2023) ${ }^{7}$. Given LLM-as-a-judge's prevalent use for evaluation, recent works have emerged using LLMs as feedback mechanisms very similar to reward models. Some works have fine-tuned models specifically for the task of rating or choosing responses from LLMs (Jiang et al., 2023b; Kim et al., 2023; Zhu et al., 2023b). Others use the policy LM itself as a generative reward model via prompting it to behave as a judge (Yuan et al., 2024b; Li et al., 2023a). While similar to the reward computation of DPO models, this mode of score calculation often involves specific prompting per-model and more computation per sample, such as explaining reasoning before or after the score. Results are shown in Tab. 8 where there is a substantial variation among existing open and closed models. Note, the best classifier RMs outperform the best generative reward models.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Values Represented in Reward Models Reward models inhabit an important normative role in the RLHF process being the primary artifact where human preferences or values are encoded in the final policy. The RewardBench infrastructure enables asking basic questions when studying reward models such as whose or which values are embedded as the sense of reward (Lambert et al., 2023). Initial work is studying this question for LLMs broadly, such as measuring representation (Durmus et al., 2023; Ryan et al., 2024) or moral foundations of LMs (Abdulhai et al., 2023), but this work should be extended to reward models. This can involve the study of different base models which RMs are trained from, tweaking fine-tuning techniques, if synthetic datasets amplify bias in RMs as well (Wyllie et al., 2024), and datasets.</p>
<p>Safety In or After RLHF An emerging trend in LLMs is the shift from chat systems being only a model to being a system of models, with small models used as classifiers for tasks such as safety (Mozes et al., 2023). If some LLMs or RMs are designed to be used with additional safety classifiers after the fact, evaluating them on RewardBench may not be a fair comparison. For systems such as this, each classifier for a specific task should be evaluated on the sections it controls. The most common area where this is handled is safety, where a small reward model can be used to permit or block all outputs from a larger generating model.</p>
<h1>C Compute Usage</h1>
<p>This work primarily evaluates models on NVIDIA A100 GPUs hosted by Cirrascale ${ }^{8}$. Each model, of which we evaluated 75 , takes about 12 hours to run on 16 bit quantization. Re-running the entire evaluation suite of RewardBench would take approximately 1000 A100 hours to complete.</p>
<h2>D Codebase Discussion</h2>
<p>Additional data is included in the code-base, but not included in the evaluation score due to noisy results or lack of clear use instructions (e.g. could be easy for unintentional test-set contamination). In this vein, results on SafeRLHF (Dai et al., 2023) data and MT Bench labels ${ }^{9}$ (from humans and GPT-4) are supported within the methodology, but not included in this analysis.</p>
<h2>E Additional Results</h2>
<p>Table 9 shows the full results for the first reward models we collected in this work. In addition, Tables 10-14 provides the performance breakdown per category.</p>
<p>Table 9: Leaderboard results in RewardBench. Icons refer to model types: Sequence Classifier ( $\square$ ), Direct Preference Optimization ( $\square$ ), Custom Classifier ( $\$$ ), Generative Model ( $\%$ ), and a random model ( $\square$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">RLHFlow/ArmoRM-Llama3-8B-v0.1</th>
<th style="text-align: left;">89.0</th>
<th style="text-align: left;">96.9</th>
<th style="text-align: left;">76.8</th>
<th style="text-align: left;">92.2</th>
<th style="text-align: left;">97.3</th>
<th style="text-align: left;">74.3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">google/gemini-1.5-pro-0514</td>
<td style="text-align: left;">88.1</td>
<td style="text-align: left;">92.3</td>
<td style="text-align: left;">80.6</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">92.0</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">RLHFlow/pair-preference-model-LLaMA3-8B</td>
<td style="text-align: left;">85.7</td>
<td style="text-align: left;">98.3</td>
<td style="text-align: left;">65.8</td>
<td style="text-align: left;">89.7</td>
<td style="text-align: left;">94.7</td>
<td style="text-align: left;">74.6</td>
</tr>
<tr>
<td style="text-align: left;">openai/gpt-4-0125-preview</td>
<td style="text-align: left;">84.3</td>
<td style="text-align: left;">95.3</td>
<td style="text-align: left;">74.3</td>
<td style="text-align: left;">87.2</td>
<td style="text-align: left;">86.9</td>
<td style="text-align: left;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">openai/gpt-4-turbo-2024-04-09</td>
<td style="text-align: left;">83.9</td>
<td style="text-align: left;">95.3</td>
<td style="text-align: left;">75.4</td>
<td style="text-align: left;">87.1</td>
<td style="text-align: left;">82.7</td>
<td style="text-align: left;">73.6</td>
</tr>
<tr>
<td style="text-align: left;">sfairXC/FsfairX-LLaMA3-RM-v0.1</td>
<td style="text-align: left;">83.6</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">65.1</td>
<td style="text-align: left;">87.8</td>
<td style="text-align: left;">86.4</td>
<td style="text-align: left;">74.9</td>
</tr>
<tr>
<td style="text-align: left;">openai/gpt-4o-2024-05-13</td>
<td style="text-align: left;">83.3</td>
<td style="text-align: left;">96.6</td>
<td style="text-align: left;">70.4</td>
<td style="text-align: left;">86.7</td>
<td style="text-align: left;">84.9</td>
<td style="text-align: left;">72.6</td>
</tr>
<tr>
<td style="text-align: left;">openbmb/Eurus-RM-7b</td>
<td style="text-align: left;">81.6</td>
<td style="text-align: left;">98.0</td>
<td style="text-align: left;">65.6</td>
<td style="text-align: left;">81.2</td>
<td style="text-align: left;">86.3</td>
<td style="text-align: left;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">Nexusflow/Starling-RM-34B</td>
<td style="text-align: left;">81.4</td>
<td style="text-align: left;">96.9</td>
<td style="text-align: left;">57.2</td>
<td style="text-align: left;">88.2</td>
<td style="text-align: left;">88.5</td>
<td style="text-align: left;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">Anthropic/claude-3-opus-20240229</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">94.7</td>
<td style="text-align: left;">60.3</td>
<td style="text-align: left;">89.1</td>
<td style="text-align: left;">78.7</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">weqweasdas/RM-Mistral-7B</td>
<td style="text-align: left;">79.3</td>
<td style="text-align: left;">96.9</td>
<td style="text-align: left;">58.1</td>
<td style="text-align: left;">87.1</td>
<td style="text-align: left;">77.0</td>
<td style="text-align: left;">75.3</td>
</tr>
<tr>
<td style="text-align: left;">hendrydong/Mistral-RM-for-RAFT-GSHF-v0</td>
<td style="text-align: left;">78.7</td>
<td style="text-align: left;">98.3</td>
<td style="text-align: left;">57.9</td>
<td style="text-align: left;">86.3</td>
<td style="text-align: left;">74.3</td>
<td style="text-align: left;">75.1</td>
</tr>
<tr>
<td style="text-align: left;">stabilityai/stablelm-2-12b-chat</td>
<td style="text-align: left;">77.4</td>
<td style="text-align: left;">96.6</td>
<td style="text-align: left;">55.5</td>
<td style="text-align: left;">82.6</td>
<td style="text-align: left;">89.4</td>
<td style="text-align: left;">48.4</td>
</tr>
</tbody>
</table>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reward Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Chat</th>
<th style="text-align: center;">Chat <br> Hard</th>
<th style="text-align: center;">Safety</th>
<th style="text-align: center;">Reason</th>
<th style="text-align: center;">Prior <br> Sets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Ray2333/reward-model-Mistral-7B-instruct-Unified...</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;">allenai/tulu-2-dpo-70b</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">52.8</td>
</tr>
<tr>
<td style="text-align: center;">PoLL/gpt-3.5-turbo-0125_claude-3-sonnet-20240229...</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama/Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: center;">prometheus-eval/prometheus-8x7b-v2.0</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Anthropic/claude-3-sonnet-20240229</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">69.6</td>
</tr>
<tr>
<td style="text-align: center;">NousResearch/Nous-Hermes-2-Mistral-7B-DPO</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: center;">mistralai/Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">50.3</td>
</tr>
<tr>
<td style="text-align: center;">upstage/SOLAR-10.7B-Instruct-v1.0</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: center;">Anthropic/claude-3-haiku-20240307</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">66.3</td>
</tr>
<tr>
<td style="text-align: center;">HuggingFaceH4/zephyr-7b-alpha</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">allenai/tulu-2-dpo-13b</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: center;">0-hero/Matter-0.1-7B-boost-DPO-preview</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">55.7</td>
</tr>
<tr>
<td style="text-align: center;">prometheus-eval/prometheus-7b-v2.0</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HuggingFaceH4/starchat2-15b-v0.1</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">55.2</td>
</tr>
<tr>
<td style="text-align: center;">HuggingFaceH4/zephyr-7b-beta</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: center;">allenai/tulu-2-dpo-7b</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">47.7</td>
</tr>
<tr>
<td style="text-align: center;">jondurbin/bagel-dpo-34b-v0.5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: center;">berkeley-nest/Starling-RM-7B-alpha</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">67.9</td>
</tr>
<tr>
<td style="text-align: center;">NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;">0-hero/Matter-0.1-7B-DPO-preview</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">stabilityai/stablelm-zephyr-3b</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">50.7</td>
</tr>
<tr>
<td style="text-align: center;">Qwen/Qwen1.5-14B-Chat</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: center;">CohereForAI/c4ai-command-r-plus</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: center;">OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">65.3</td>
</tr>
<tr>
<td style="text-align: center;">Qwen/Qwen1.5-7B-Chat</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">42.9</td>
</tr>
<tr>
<td style="text-align: center;">weqweasdas/RM-Gemma-7B</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: center;">openbmb/Eurus-7b-kto</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">52.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen/Qwen1.5-72B-Chat</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: center;">openbmb/UltraRM-13b</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: center;">weqweasdas/RM-Gemma-7B-4096</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: center;">mightbe/Better-PairRM</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">72.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen/Qwen1.5-MoE-A2.7B-Chat</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">45.4</td>
</tr>
<tr>
<td style="text-align: center;">RLHFlow/RewardModel-Mistral-7B-for-DPA-v1</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: center;">allenai/OLMo-7B-Instruct</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">HuggingFaceH4/zephyr-7b-gemma-v0.1</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">openbmb/MiniCPM-2B-dpo-fp32</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">49.6</td>
</tr>
<tr>
<td style="text-align: center;">stabilityai/stablelm-2-zephyr-1.6b</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: center;">openai/gpt-3.5-turbo-0125</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama/Meta-Llama-3-8B-Instruct</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">60.8</td>
</tr>
<tr>
<td style="text-align: center;">weqweasdas/RM-Gemma-2B</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: center;">stabilityai/stable-code-instruct-3b</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: center;">IDEA-CCNL/Ziya-LLaMA-7B-Reward</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: center;">OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen/Qwen1.5-1.8B-Chat</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">PKU-Alignment/beaver-7b-v1.0-cost</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">Ilm-blender/PairRM-hf</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">69.6</td>
</tr>
<tr>
<td style="text-align: center;">ContextualAI/archangel_sft-kto_llama30b</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">58.6</td>
</tr>
<tr>
<td style="text-align: center;">ContextualAI/archangel_sft-kto_llama13b</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: center;">ContextualAI/archangel_sft-dpo_llama30b</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen/Qwen1.5-4B-Chat</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">44.7</td>
</tr>
<tr>
<td style="text-align: center;">Qwen/Qwen1.5-0.5B-Chat</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">46.3</td>
</tr>
<tr>
<td style="text-align: center;">ContextualAI/archangel_sft-kto_pythia6-9b</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;">OpenAssistant/reward-model-deberta-v3-large-v2</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">58.4</td>
</tr>
<tr>
<td style="text-align: center;">ContextualAI/archangel_sft-kto_pythia1-4b</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: center;">ContextualAI/archangel_sft-kto_pythia2-8b</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">55.7</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Reward Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Chat</th>
<th style="text-align: center;">Chat <br> Hard</th>
<th style="text-align: center;">Safety</th>
<th style="text-align: center;">Reason</th>
<th style="text-align: center;">Prior <br> Sets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-dpo_llama13b</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">56.6</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-kto_llama7b</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-dpo_pythia2-8b</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">55.0</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-dpo_llama7b</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">55.4</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-dpo_pythia6-9b</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-dpo_pythia1-4b</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">54.3</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ random</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-kto_pythia12-0b</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">55.0</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ ContextualAI/archangel_sft-dpo_pythia12-0b</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">53.0</td>
</tr>
<tr>
<td style="text-align: left;">$\nabla$ stanfordnlp/SteamSHP-flan-t5-xl</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">65.0</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ weqweasdas/hh_rlhf_rm_open_llama_3b</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">65.6</td>
</tr>
<tr>
<td style="text-align: left;">$\nabla$ stanfordnlp/SteamSHP-flan-t5-large</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">62.7</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ PKU-Alignment/beaver-7b-v1.0-reward</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">59.9</td>
</tr>
</tbody>
</table>
<p>Table 10: RewardBench results for the Chat category. Icons refer to model types: Sequence Classifier ( $\boxtimes$ ), Direct Preference Optimization ( $\boxtimes$ ), Custom Classifier ( $\nabla$ ), Generative Model ( $\boxtimes$ ), and a random model ( $\boxtimes$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reward Model</th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;">AlpacaEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MT Bench</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Medium</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ sfairXC/FsfairX-LLaMA3-RM-v0.1</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\nabla$ RLHFlow/pair-preference-model-LLaMA3-8B</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ hendrydong/Mistral-RM-for-RAFT-GSHF-v0</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ berkeley-nest/Starling-RM-7B-alpha</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">92.5</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ openbmb/Eurus-RM-7b</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ Ray2333/reward-model-Mistral-7B-instruct-Unified...</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ meta-llama/Meta-Llama-3-70B-Instruct</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ allenai/tulu-2-dpo-70b</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ allenai/tulu-2-dpo-7b</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ Nexusflow/Starling-RM-34B</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ weqweasdas/RM-Gemma-7B</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ weqweasdas/RM-Mistral-7B</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\nabla$ RLHFlow/ArmoRM-Llama3-8B-v0.1</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ stabilityai/stablelm-2-zephyr-1_6b</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ openai/gpt-4o-2024-05-13</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ stabilityai/stablelm-2-12b-chat</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ openbmb/UltraRM-13b</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ HuggingFaceH4/zephyr-7b-gemma-v0.1</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ allenai/tulu-2-dpo-13b</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: center;">$\nabla$ mightbe/Better-PairRM</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ PoLL/gpt-3.5-turbo-0125_claude-3-sonnet-20240229...</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ HuggingFaceH4/zephyr-7b-beta</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ openbmb/Eurus-7b-kto</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ openai/gpt-4-0125-preview</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ openai/gpt-4-turbo-2024-04-09</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ CohereForAI/c4ai-command-r-plus</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ mistralai/Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ weqweasdas/RM-Gemma-7B-4096</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ Anthropic/claude-3-opus-20240229</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ weqweasdas/RM-Gemma-2B</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ HuggingFaceH4/starchat2-15b-v0.1</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ jondurbin/bagel-dpo-34b-v0.5</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">$\boxtimes$ Anthropic/claude-3-sonnet-20240229</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">95.0</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>26</th>
<th>prometheus-eval/prometheus-8x7b-v2.0</th>
<th>93.0</th>
<th>96.0</th>
<th>87.4</th>
<th>92.6</th>
<th>92.9</th>
<th>100.0</th>
</tr>
</thead>
<tbody>
<tr>
<td>26</td>
<td>Anthropic/claude-3-haiku-20240307</td>
<td>92.7</td>
<td>99.0</td>
<td>80.0</td>
<td>100.0</td>
<td>92.9</td>
<td>90.0</td>
</tr>
<tr>
<td>26</td>
<td>OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1</td>
<td>92.5</td>
<td>97.0</td>
<td>91.6</td>
<td>98.9</td>
<td>82.1</td>
<td>75.0</td>
</tr>
<tr>
<td>26</td>
<td>google/gemini-1.5-pro-0514</td>
<td>92.3</td>
<td>95.0</td>
<td>84.2</td>
<td>93.7</td>
<td>98.2</td>
<td>97.5</td>
</tr>
<tr>
<td>26</td>
<td>NousResearch/Nous-Hermes-2-Mistral-7B-DPO</td>
<td>92.2</td>
<td>96.0</td>
<td>83.2</td>
<td>95.8</td>
<td>92.9</td>
<td>95.0</td>
</tr>
<tr>
<td>26</td>
<td>openai/gpt-3.5-turbo-0125</td>
<td>92.2</td>
<td>95.5</td>
<td>82.1</td>
<td>98.9</td>
<td>94.6</td>
<td>90.0</td>
</tr>
<tr>
<td>26</td>
<td>HuggingFaceH4/zephyr-7b-alpha</td>
<td>91.6</td>
<td>99.0</td>
<td>78.9</td>
<td>95.8</td>
<td>92.9</td>
<td>92.5</td>
</tr>
<tr>
<td>26</td>
<td>NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO</td>
<td>91.6</td>
<td>98.0</td>
<td>87.4</td>
<td>96.8</td>
<td>75.0</td>
<td>85.0</td>
</tr>
<tr>
<td>26</td>
<td>0-hero/Matter-0.1-7B-boost-DPO-preview</td>
<td>91.1</td>
<td>98.0</td>
<td>88.4</td>
<td>90.5</td>
<td>89.3</td>
<td>82.5</td>
</tr>
<tr>
<td>26</td>
<td>llm-blender/PairRM-hf</td>
<td>90.2</td>
<td>96.0</td>
<td>75.8</td>
<td>97.9</td>
<td>92.9</td>
<td>90.0</td>
</tr>
<tr>
<td>26</td>
<td>allenai/OLMo-7B-Instruct</td>
<td>89.7</td>
<td>90.0</td>
<td>91.6</td>
<td>92.6</td>
<td>85.7</td>
<td>80.0</td>
</tr>
<tr>
<td>26</td>
<td>0-hero/Matter-0.1-7B-DPO-preview</td>
<td>89.4</td>
<td>100.0</td>
<td>84.2</td>
<td>95.8</td>
<td>67.9</td>
<td>75.0</td>
</tr>
<tr>
<td>26</td>
<td>openbmb/MiniCPM-2B-dpo-fp32</td>
<td>89.1</td>
<td>95.0</td>
<td>92.6</td>
<td>88.4</td>
<td>85.7</td>
<td>70.0</td>
</tr>
<tr>
<td>26</td>
<td>OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5</td>
<td>88.5</td>
<td>95.0</td>
<td>78.9</td>
<td>93.7</td>
<td>85.7</td>
<td>85.0</td>
</tr>
<tr>
<td>26</td>
<td>RLHFlow/RewardModel-Mistral-7B-for-DPA-v1</td>
<td>88.0</td>
<td>91.0</td>
<td>73.7</td>
<td>95.8</td>
<td>89.3</td>
<td>95.0</td>
</tr>
<tr>
<td>26</td>
<td>IDEA-CCNL/Ziya-LLaMA-7B-Reward</td>
<td>86.9</td>
<td>85.0</td>
<td>84.2</td>
<td>92.6</td>
<td>92.9</td>
<td>80.0</td>
</tr>
<tr>
<td>26</td>
<td>stabilityai/stablelm-zephyr-3b</td>
<td>86.3</td>
<td>72.0</td>
<td>95.8</td>
<td>89.5</td>
<td>96.4</td>
<td>85.0</td>
</tr>
<tr>
<td>26</td>
<td>stanfordnlp/SteamSHP-flan-t5-large</td>
<td>85.8</td>
<td>94.0</td>
<td>72.6</td>
<td>97.9</td>
<td>75.0</td>
<td>75.0</td>
</tr>
<tr>
<td>26</td>
<td>prometheus-eval/prometheus-7b-v2.0</td>
<td>85.5</td>
<td>92.0</td>
<td>81.1</td>
<td>86.8</td>
<td>73.2</td>
<td>85.0</td>
</tr>
<tr>
<td>26</td>
<td>meta-llama/Meta-Llama-3-8B-Instruct</td>
<td>85.5</td>
<td>91.0</td>
<td>72.6</td>
<td>90.5</td>
<td>94.6</td>
<td>83.8</td>
</tr>
<tr>
<td>26</td>
<td>stanfordnlp/SteamSHP-flan-t5-xl</td>
<td>85.5</td>
<td>93.0</td>
<td>69.5</td>
<td>98.9</td>
<td>78.6</td>
<td>77.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-kto_llama30b</td>
<td>84.4</td>
<td>93.0</td>
<td>76.8</td>
<td>88.4</td>
<td>82.1</td>
<td>72.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-kto_llama13b</td>
<td>84.1</td>
<td>96.0</td>
<td>76.8</td>
<td>87.4</td>
<td>71.4</td>
<td>72.5</td>
</tr>
<tr>
<td>26</td>
<td>OpenAssistant/reward-model-deberta-v3-large-v2</td>
<td>83.2</td>
<td>99.0</td>
<td>41.1</td>
<td>96.8</td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td>26</td>
<td>PKU-Alignment/beaver-7b-v1.0-reward</td>
<td>81.8</td>
<td>98.0</td>
<td>63.2</td>
<td>100.0</td>
<td>67.9</td>
<td>52.5</td>
</tr>
<tr>
<td>26</td>
<td>weqweasdas/hh_rlhf_rm_open_llama_3b</td>
<td>81.8</td>
<td>95.0</td>
<td>64.2</td>
<td>96.8</td>
<td>64.3</td>
<td>67.5</td>
</tr>
<tr>
<td>26</td>
<td>upstage/SOLAR-10.7B-Instruct-v1.0</td>
<td>81.6</td>
<td>92.0</td>
<td>74.7</td>
<td>75.8</td>
<td>89.3</td>
<td>80.0</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-dpo_pythia2-8b</td>
<td>80.7</td>
<td>96.0</td>
<td>58.9</td>
<td>92.6</td>
<td>67.9</td>
<td>75.0</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-kto_pythia6-9b</td>
<td>77.7</td>
<td>88.0</td>
<td>64.2</td>
<td>90.5</td>
<td>57.1</td>
<td>67.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-kto_pythia2-8b</td>
<td>75.7</td>
<td>92.0</td>
<td>55.8</td>
<td>80.0</td>
<td>67.9</td>
<td>77.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-kto_pythia12-0b</td>
<td>74.9</td>
<td>79.0</td>
<td>69.5</td>
<td>82.1</td>
<td>67.9</td>
<td>65.0</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-dpo_pythia6-9b</td>
<td>74.9</td>
<td>89.0</td>
<td>58.9</td>
<td>87.4</td>
<td>57.1</td>
<td>60.0</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-MoE-A2.7B-Chat</td>
<td>72.9</td>
<td>77.0</td>
<td>82.1</td>
<td>58.9</td>
<td>60.7</td>
<td>82.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-dpo_llama13b</td>
<td>71.2</td>
<td>80.0</td>
<td>62.1</td>
<td>69.5</td>
<td>78.6</td>
<td>70.0</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-dpo_llama30b</td>
<td>69.3</td>
<td>78.0</td>
<td>61.1</td>
<td>74.7</td>
<td>67.9</td>
<td>55.0</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-kto_pythia1-4b</td>
<td>68.4</td>
<td>79.0</td>
<td>52.6</td>
<td>75.8</td>
<td>57.1</td>
<td>70.0</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-dpo_pythia12-0b</td>
<td>66.8</td>
<td>71.0</td>
<td>62.1</td>
<td>70.5</td>
<td>60.7</td>
<td>62.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-dpo_pythia1-4b</td>
<td>64.0</td>
<td>73.0</td>
<td>49.5</td>
<td>75.8</td>
<td>35.7</td>
<td>67.5</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-72B-Chat</td>
<td>62.3</td>
<td>73.0</td>
<td>70.5</td>
<td>38.9</td>
<td>60.7</td>
<td>72.5</td>
</tr>
<tr>
<td>26</td>
<td>PKU-Alignment/beaver-7b-v1.0-cost</td>
<td>61.7</td>
<td>43.0</td>
<td>67.4</td>
<td>74.7</td>
<td>57.1</td>
<td>67.5</td>
</tr>
<tr>
<td>26</td>
<td>stabilityai/stable-code-instruct-3b</td>
<td>57.8</td>
<td>27.0</td>
<td>81.1</td>
<td>57.9</td>
<td>75.0</td>
<td>67.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-dpo_llama7b</td>
<td>57.8</td>
<td>65.0</td>
<td>48.4</td>
<td>66.3</td>
<td>35.7</td>
<td>57.5</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-14B-Chat</td>
<td>57.3</td>
<td>64.0</td>
<td>70.5</td>
<td>32.6</td>
<td>60.7</td>
<td>65.0</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-1.8B-Chat</td>
<td>56.1</td>
<td>30.0</td>
<td>89.5</td>
<td>51.6</td>
<td>57.1</td>
<td>52.5</td>
</tr>
<tr>
<td>26</td>
<td>ContextualAI/archangel_sft-kto_llama7b</td>
<td>55.9</td>
<td>60.0</td>
<td>51.6</td>
<td>57.9</td>
<td>50.0</td>
<td>55.0</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-7B-Chat</td>
<td>53.6</td>
<td>50.0</td>
<td>73.7</td>
<td>32.6</td>
<td>57.1</td>
<td>62.5</td>
</tr>
<tr>
<td>26</td>
<td>random</td>
<td>50.0</td>
<td>50.0</td>
<td>50.0</td>
<td>50.0</td>
<td>50.0</td>
<td>50.0</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-4B-Chat</td>
<td>38.8</td>
<td>8.0</td>
<td>71.6</td>
<td>35.8</td>
<td>53.6</td>
<td>35.0</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-0.5B-Chat</td>
<td>35.5</td>
<td>9.0</td>
<td>65.3</td>
<td>25.3</td>
<td>57.1</td>
<td>40.0</td>
</tr>
<tr>
<td>26</td>
<td>Qwen/Qwen1.5-0.5B-Chat</td>
<td>35.5</td>
<td>9.0</td>
<td>65.3</td>
<td>25.3</td>
<td>57.1</td>
<td>40.0</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Per model batch size and settings include online: https://github.com/allenai/reward-bench/ blob/main/scripts/configs/eval_configs.yaml.
${ }^{9}$ https://huggingface.co/datasets/lmsys/mt_bench_human_judgments&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2121 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2121</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2121</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-280011325</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.21763v1.pdf" target="_blank">THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow.Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show ~60\% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured.This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology \textbf{H}istory \textbf{E}volution Tree), a computational framework that constructs such domain-specific evolution trees from scientific literature.THE-Tree employs a search algorithm to explore evolutionary paths. During its node expansion, it utilizes a novel"Think-Verbalize-Cite-Verify"process: an LLM proposes potential advancements and cites supporting literature. Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded.We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research.Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10\%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100\%.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2121.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2121.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>THE-Tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Technology History Evolution Tree</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational framework that constructs domain-specific, causally-linked evolution trees from scientific literature to serve as a verifiable historical substrate for evaluating new scientific ideas; it combines LLM-guided search with retrieval-augmented NLI validation and expert curation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>THE-Tree</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs technology evolution trees where nodes are papers and edges are semantically meaningful evolutionary/causal links; automated construction uses SGT-MCTS for path search and TVCV for node expansion, with RA-NLI to validate each proposed link; intended to ground and verify AI- or human-generated scientific propositions by situating them in explicit, verifiable historical paths.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature (demonstrated primarily in AI/Computer Science with examples across Biomedicine and Materials Science)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is primarily literature- and inference-based: THE-Tree proposes evolutionary links via LLM-guided SGT-MCTS and TVCV; each proposed edge is assigned an attribution reward Rattr computed by RA-NLI (retrieval of relevant text passages + fine-tuned NLI inference + LLM fallback). Constructed trees and link labels are quantitatively compared against expert-refined ground-truth trees (88 topics) using node/edge recall, precision, F1; downstream validation tasks include graph-completion and future-path prediction benchmarks and augmentation experiments (e.g., NeurIPS paper evaluation). Seed surveys and expert review are used to initialize and refine ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (literature-grounded computational validation using retrieval + textual NLI; not physics/experiment simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper argues literature-based RA-NLI validation is sufficient to substantially reduce hallucinations and provide high-fidelity verification for tracing idea lineage in literature-centric domains; nevertheless it explicitly states this is not a universal replacement for domain-specific experimental validation (e.g., wet-lab or physical experiments) and relies on expert curation as a gold-standard proxy for tree correctness in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Quantitative results: MCTS-generated trees vs expert ground truth: node recall 0.84, precision 0.67, F1 0.75; edge recall 0.78, precision 0.64, F1 0.70. Downstream improvements reported: graph-completion hit@1 improved by ~8–14% over citation graphs; future-path prediction hit@1 improved by ~10%; augmentation of LLM paper-evaluation models improved recognition of important papers substantially (paper reports up to ~100% relative improvement in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical/bench experiments were performed; validation is computational and empirical using held-out literature data, an expert-curated ground-truth of THE-Trees, and large-scale fact-verification benchmarks (71k verifications over 27k papers). Paper notes the lack of a universal gold standard and uses domain-expert refined trees as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared THE-Tree to traditional citation-only graphs on graph-completion and future prediction tasks (THE-Tree outperforms citation graphs across Hit@k, MRR, and rank metrics). Compared RA-NLI-enhanced verification to many LLM baselines (see RA-NLI entry). Also compared LLM-alone vs THE-Tree-augmented LLMs on NeurIPS acceptance/status prediction (augmentation improved accuracy and ability to identify high-impact papers).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No single catastrophic failure case reported, but limitations include: MCTS-generated trees are imperfect and require expert refinement; LLM-only verifiers show high fact-missing and hallucination rates (~60% in some settings); THE-Tree's effectiveness depends on coverage of source literature (English bias, missing patents/reports), concept granularity choices, and yearly temporal resolution—these can lead to missed or incorrect links.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>RA-NLI achieved the best fact-consistency metrics on the 71k fact-verification benchmark, enabling THE-Tree to produce trees that better recover expert-validated nodes/edges and improve downstream tasks: graph completion, future-path prediction, and augmented LLM paper evaluation (notable case studies include improved DeepReviewer-14b performance on NeurIPS 2024 data and more accurate identification of Oral/Spotlight papers).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>MCTS-generated trees were compared to expert-refined THE-Trees (human curated) as ground truth; metrics described above quantify agreement. THE-Tree also compared performance against citation graphs and LLM-only baselines on downstream tasks, using established benchmarks (NeurIPS accept/status; graph-completion/future-prediction metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper releases a dataset (88 THE-Trees, 71k fact verifications) and describes architecture/training details; expert-refined ground truth construction is documented. No independent external replication reported within the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Discussed qualitatively: human/manual verification is prohibitively slow; THE-Tree’s automated process substantially reduces manual effort but incurs significant computational cost due to large-scale LLM usage and graph processing. Exact monetary/time costs are not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper emphasizes that domain norms vary: literature-based entailment and expert review are acceptable norms for tracing idea lineage in humanities/CS/AI; many scientific domains (e.g., experimental biology, chemistry, materials) still require physical experiments or wet-lab verification for claims about real-world behavior; THE-Tree is presented as complementary to, not a substitute for, domain-specific experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uses normalized scores (e.g., Rattr in [0,1]), importance scores S(v), DPO scores for generation reward, and temporal coherence penalties; uncertain/neutral cases in NLI are handled by a secondary LLM evaluator. The paper suggests quantitative scoring is used throughout but does not present formal Bayesian uncertainty propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Identified limitations include coverage and language bias of source data, subjectivity in concept granularity, annual temporal resolution coarseness, complexity of validating scientific evolution without universal gold standards, and significant computational demand of LLM-guided construction.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines automated retrieval + NLI + LLM fallback (RA-NLI) for link validation, LLM-guided MCTS search and node generation (TVCV), and human expert refinement to produce final ground-truth trees; seeds construction from curated surveys to reduce noise.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2121.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2121.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RA-NLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Natural Language Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validation module that assesses whether a proposed evolutionary/causal relationship between two literature nodes is supported by retrieved textual evidence, combining embedding-based retrieval, a fine-tuned scientific NLI model (fNLI), and an LLM-based adjudicator for ambiguous cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RA-NLI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Per-edge verifier: retrieves contextual passages from cited documents via embedding-based search, applies a fine-tuned DeBERTa NLI model to classify entailment/contradiction/neutral and produce a normalized score, and uses a large LLM (Qwen2.5-72B-Instruct) to re-evaluate neutral/ambiguous cases; outputs an attribution reward Rattr used to accept/reject and score evolutionary links.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Literature-based verification across scientific domains (demonstrated on AI/CS literature; applicable to other domains where factual claims are textually documented)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Procedure: (1) retrieve evidence passages using embedding-based retrieval; (2) pass premise (retrieved evidence) and hypothesis (citing claim / node description) to fine-tuned DeBERTa NLI classifier producing entailment/contradiction/neutral probabilities; (3) normalize NLI output to [0,1]; (4) if label is neutral or ambiguous, call LLM evaluator (Qwen2.5-72B-Instruct) to adjudicate; (5) compute Rattr = α * NLI_score + (1-α) * LLMeval_score with α=0.7. Used both as a reward signal in SGT-MCTS and as an automatic verification filter in TVCV.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (textual entailment fidelity depends on retrieval quality and fNLI model suitability; considered high-fidelity for literature entailment when evidence is retrievable and text aligns well)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper demonstrates RA-NLI is highly effective for literature-level validation (claim: substantially reduces fact-missing rates vs LLM-only). Authors caution that RA-NLI sufficiency is limited to claims documented and evidenced in text; experimental claims requiring domain-specific measurements may require additional empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>On a 71k fact-verification dataset derived from 27k papers, RA-NLI achieved accuracy 95.60% with a fact-missing rate of 4.75% (reported as best among compared methods).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Validated purely on a large computational fact-verification benchmark built from citation sentences paired with evidence from source papers; no physical experiments. Compared against multiple baseline LLMs with and without factual supplementation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Direct quantitative comparison in Table 2: RA-NLI (4.75% fact-missing, 95.60% accuracy) outperformed a set of LLM baselines (examples: GPT-4o had 58.19% fact-missing, 60.18% accuracy; Qwen2.5 58.29% missing, 53.95% accuracy; DeepSeek-R1 and DeepReviewer models varied), and factual-supplement variants improved some LLMs but generally did not match RA-NLI's combined low missing/high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Neutral/ambiguous cases require LLM fallback and can remain uncertain; performance depends on quality of retrieved context (if retrieval misses supporting text, RA-NLI cannot validate claims); does not validate non-textual experimental results (e.g., instrument readings) beyond what is described in text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>High success on the 71k fact-verification benchmark with top accuracy and lowest fact-missing rate among compared systems; used successfully to filter and score evolutionary links in THE-Tree construction, improving downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>The 71k dataset serves as the ground-truth testbed for RA-NLI; RA-NLI outputs were compared against labelled fact–evidence pairs derived from original texts.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Dataset and methodology described and intended for release; paper does not report independent external replications beyond its internal experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Requires embedding retrieval, NLI inference, and in ambiguous cases expensive LLM calls (Qwen2.5-72B), implying non-trivial compute cost; exact wall-clock or monetary costs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Appropriate and accepted for literature-level claim verification; authors emphasize that in domains where empirical measurement is the norm (e.g., experimental chemistry/biology), text-based entailment is necessary but not sufficient to fully validate experimental claims.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Returns normalized entailment probabilities; combines NLI and LLMeval via weighted average (α=0.7); uses explicit neutral label handling and LLM-based secondary adjudication to manage uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Sensitive to retrieval quality and to the expressiveness of textual evidence; neutral/ambiguous cases require costly LLM fallback; cannot validate claims that have no textual evidence or that require raw experimental/measurement data beyond reported text.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Hybrid computational pipeline: embedding-based retrieval (information retrieval) + fine-tuned DeBERTa NLI (fNLI) + LLM adjudication (Qwen2.5) for ambiguous cases; scores combined by weighted formula for final Rattr.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2121.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2121.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TVCV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Think-Verbalize-Cite-Verify</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A four-step LLM-driven node-expansion methodology used within THE-Tree construction: generate candidate technological advancements (Think), summarize them (Verbalize), ground them in literature (Cite), and validate proposed relationships using RA-NLI (Verify).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Think-Verbalize-Cite-Verify (TVCV)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each node expansion, an LLM proposes candidate next-step ideas (Think), condenses them into concise propositions (Verbalize), retrieves or cites supporting literature to ground the proposition (Cite), and then invokes RA-NLI to verify the logical/causal coherence of the proposed parent→child link (Verify). The Verify score contributes to the attribution reward Rattr used by SGT-MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General literature-driven scientific idea generation and validation (demonstrated in AI/CS domains)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Uses LLM generation plus retrieval: LLMs propose and summarize candidate advances, retrieve supporting papers or concepts, and RA-NLI performs per-link textual validation. The Verify step ensures links are temporally coherent and textually supported; ambiguous NLI outputs trigger LLM reconsideration or rejection.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (not a simulation; fidelity depends on LLM generation quality and RA-NLI retrieval/NLI fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Effective for reducing LLM hallucination in proposing literature-grounded evolutionary steps and improving downstream prediction tasks; the paper recommends initializing from human-validated surveys to reduce noise, and notes that domain-specific experimental validation may still be required for empirical claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No standalone accuracy metric reported specifically for TVCV alone; as part of the integrated pipeline it contributed to overall MCTS-generated tree metrics (node/edge recall & precision above) and downstream task gains (graph-completion hit@1 +8–14%, future prediction ~+10%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Validation is computational: TVCV's Verify step is implemented and evaluated via RA-NLI over the 71k fact-verification set and within the MCTS pipeline compared to expert-refined trees; physical experiments not involved.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared implicitly against LLM-only generation pipelines and citation-only networks via downstream evaluations; TVCV+RA-NLI pipeline notably outperformed LLM-only and citation-only baselines in reconstructing and predicting evolution paths.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Standalone LLM Think/Verbalize steps are prone to hallucination and domain-knowledge gaps (paper cites up to ~60% unawareness of relevant papers for LLM-only verification). The Verify step mitigates but does not eliminate such failures, particularly when retrieval misses evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>When combined with RA-NLI and SGT-MCTS, TVCV enabled construction of trees that matched expert-refined trajectories reasonably well and improved downstream tasks (see THE-Tree and RA-NLI success metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Component-level outputs are evaluated as part of MCTS-generated trees against expert-refined THE-Trees and in the large fact-verification benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Methodology and algorithmic description provided; no external replication reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>The Verify step requires retrieval and NLI/Large-LLM calls per candidate link, increasing computational cost relative to LLM-only generation; exact costs not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Authors recommend seeding TVCV with human-validated surveys to adhere to community norms for historical reconstruction; for empirical claims, TVCV's literature verification is necessary but may be insufficient without experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uses Rattr scores from RA-NLI, DPO and TemporalCoherence metrics for generation reward; ambiguous NLI outputs are explicitly flagged and re-evaluated by LLM, providing a mechanism to surface uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Depends on LLM capability to surface relevant literature and on retrieval quality; high computational overhead; neutral NLI cases reduce automated decisiveness; not a substitute for non-textual experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines LLM generation with retrieval-augmented NLI verification (RA-NLI) and uses expert-curated surveys as starting points; Verify step fuses automated inference and LLM adjudication.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2121.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2121.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGT-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Guided Temporal Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-guided Monte Carlo Tree Search variant that constructs likely chronological evolutionary paths through literature by optimizing a composite reward combining node importance, generation coherence, and attribution validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SGT-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Search algorithm used to explore and assemble candidate evolution paths: nodes are proposed/expanded via TVCV; selection uses a modified UCT (SGT-UCT) that includes LLM-derived priority and temporal coherence; rewards combine S(v) (importance), Rgen (DPO-based generation coherence + temporal penalties) and Rattr (RA-NLI link validation).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Literature mining and knowledge-graph construction for scientific domains (demonstrated in AI/CS and related fields)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation of candidate paths is baked into the search objective via Rattr from RA-NLI and Rgen coherence scores; the algorithm is evaluated by reconstructing expert-refined trees (node/edge recall, precision, F1) and by downstream prediction tasks (graph completion and future-path prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (algorithmic search fidelity depends on LLM guidance, reward design, and RA-NLI reliability; not a physics simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper demonstrates SGT-MCTS guided by TVCV+RA-NLI yields high recall/precision in reconstructing expert trees and better predictive performance than citation-only baselines; authors note it is sufficient for literature-tracing goals but must be combined with expert curation for final authoritative ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reconstruction metrics for MCTS-generated trees: node recall 0.84, precision 0.67, F1 0.75; edge recall 0.78, precision 0.64, F1 0.70. Downstream performance gains (see THE-Tree entry) attributed to SGT-MCTS + TVCV + RA-NLI pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>SGT-MCTS behavior and outputs were validated computationally by comparing generated trees to expert-curated ground truth and by measuring downstream task performance; no physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared performance of THE-Tree (built via SGT-MCTS pipeline) against citation-only graphs on graph-completion and future-prediction tasks—SGT-MCTS-derived THE-Trees performed substantially better across metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Generated trees are not perfect and require expert refinement; specific false-positive/false-negative edge cases are summarized in aggregate metrics (precision < recall in some cases), highlighting areas requiring human input.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Produced trees with high node/edge recall and improved downstream predictive capabilities compared to citation networks; aided in better LLM-based paper evaluation when used to augment reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Direct comparison against expert-refined THE-Trees across 88 topics; used as quantitative ground-truth benchmark for reconstruction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Algorithmic formulas, hyperparameters (e.g., S(v) composition, SGT-UCT), and architecture details (node embeddings, GNN layers) are included; no external reproduction reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Search requires many LLM calls for node expansion and RA-NLI verification, making it computationally intensive; the paper flags computational cost as a limitation but does not provide exact runtime/cost figures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>SGT-MCTS is presented as a computational method to recover literature-based evolutionary structure; for domains where experimental verification is normative, literature-only search must be complemented with empirical checks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Search uses empirical mean rewards Q(v)/N(v), exploration terms, and LLMpriority * TemporalCoherence modifiers; reward-sensitive exploration and growing exploration coefficient c(t) incorporate observed reward statistics to manage uncertainty during search.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Performance depends on quality of node importance scoring (S(v)), LLM guidance, retrieval quality for RA-NLI, and the completeness of the underlying literature corpus; requires downstream human curation to reach gold-standard quality.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>SGT-MCTS integrates algorithmic search with LLM-guided proposals (TVCV), RA-NLI verification for edges, and human expert refinement to finalize ground-truth trees.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CycleResearcher: Improving Automated Research via Automated Review <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Validating ai-generated code with live programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2121",
    "paper_id": "paper-280011325",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "THE-Tree",
            "name_full": "Technology History Evolution Tree",
            "brief_description": "A computational framework that constructs domain-specific, causally-linked evolution trees from scientific literature to serve as a verifiable historical substrate for evaluating new scientific ideas; it combines LLM-guided search with retrieval-augmented NLI validation and expert curation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "THE-Tree",
            "system_description": "Constructs technology evolution trees where nodes are papers and edges are semantically meaningful evolutionary/causal links; automated construction uses SGT-MCTS for path search and TVCV for node expansion, with RA-NLI to validate each proposed link; intended to ground and verify AI- or human-generated scientific propositions by situating them in explicit, verifiable historical paths.",
            "scientific_domain": "General scientific literature (demonstrated primarily in AI/Computer Science with examples across Biomedicine and Materials Science)",
            "validation_type": "hybrid",
            "validation_description": "Validation is primarily literature- and inference-based: THE-Tree proposes evolutionary links via LLM-guided SGT-MCTS and TVCV; each proposed edge is assigned an attribution reward Rattr computed by RA-NLI (retrieval of relevant text passages + fine-tuned NLI inference + LLM fallback). Constructed trees and link labels are quantitatively compared against expert-refined ground-truth trees (88 topics) using node/edge recall, precision, F1; downstream validation tasks include graph-completion and future-path prediction benchmarks and augmentation experiments (e.g., NeurIPS paper evaluation). Seed surveys and expert review are used to initialize and refine ground truth.",
            "simulation_fidelity": "N/A (literature-grounded computational validation using retrieval + textual NLI; not physics/experiment simulation)",
            "validation_sufficiency": "Paper argues literature-based RA-NLI validation is sufficient to substantially reduce hallucinations and provide high-fidelity verification for tracing idea lineage in literature-centric domains; nevertheless it explicitly states this is not a universal replacement for domain-specific experimental validation (e.g., wet-lab or physical experiments) and relies on expert curation as a gold-standard proxy for tree correctness in evaluations.",
            "validation_accuracy": "Quantitative results: MCTS-generated trees vs expert ground truth: node recall 0.84, precision 0.67, F1 0.75; edge recall 0.78, precision 0.64, F1 0.70. Downstream improvements reported: graph-completion hit@1 improved by ~8–14% over citation graphs; future-path prediction hit@1 improved by ~10%; augmentation of LLM paper-evaluation models improved recognition of important papers substantially (paper reports up to ~100% relative improvement in some settings).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical/bench experiments were performed; validation is computational and empirical using held-out literature data, an expert-curated ground-truth of THE-Trees, and large-scale fact-verification benchmarks (71k verifications over 27k papers). Paper notes the lack of a universal gold standard and uses domain-expert refined trees as ground truth.",
            "validation_comparison": "Compared THE-Tree to traditional citation-only graphs on graph-completion and future prediction tasks (THE-Tree outperforms citation graphs across Hit@k, MRR, and rank metrics). Compared RA-NLI-enhanced verification to many LLM baselines (see RA-NLI entry). Also compared LLM-alone vs THE-Tree-augmented LLMs on NeurIPS acceptance/status prediction (augmentation improved accuracy and ability to identify high-impact papers).",
            "validation_failures": "No single catastrophic failure case reported, but limitations include: MCTS-generated trees are imperfect and require expert refinement; LLM-only verifiers show high fact-missing and hallucination rates (~60% in some settings); THE-Tree's effectiveness depends on coverage of source literature (English bias, missing patents/reports), concept granularity choices, and yearly temporal resolution—these can lead to missed or incorrect links.",
            "validation_success_cases": "RA-NLI achieved the best fact-consistency metrics on the 71k fact-verification benchmark, enabling THE-Tree to produce trees that better recover expert-validated nodes/edges and improve downstream tasks: graph completion, future-path prediction, and augmented LLM paper evaluation (notable case studies include improved DeepReviewer-14b performance on NeurIPS 2024 data and more accurate identification of Oral/Spotlight papers).",
            "ground_truth_comparison": "MCTS-generated trees were compared to expert-refined THE-Trees (human curated) as ground truth; metrics described above quantify agreement. THE-Tree also compared performance against citation graphs and LLM-only baselines on downstream tasks, using established benchmarks (NeurIPS accept/status; graph-completion/future-prediction metrics).",
            "reproducibility_replication": "The paper releases a dataset (88 THE-Trees, 71k fact verifications) and describes architecture/training details; expert-refined ground truth construction is documented. No independent external replication reported within the paper.",
            "validation_cost_time": "Discussed qualitatively: human/manual verification is prohibitively slow; THE-Tree’s automated process substantially reduces manual effort but incurs significant computational cost due to large-scale LLM usage and graph processing. Exact monetary/time costs are not quantified.",
            "domain_validation_norms": "Paper emphasizes that domain norms vary: literature-based entailment and expert review are acceptable norms for tracing idea lineage in humanities/CS/AI; many scientific domains (e.g., experimental biology, chemistry, materials) still require physical experiments or wet-lab verification for claims about real-world behavior; THE-Tree is presented as complementary to, not a substitute for, domain-specific experimental validation.",
            "uncertainty_quantification": "Uses normalized scores (e.g., Rattr in [0,1]), importance scores S(v), DPO scores for generation reward, and temporal coherence penalties; uncertain/neutral cases in NLI are handled by a secondary LLM evaluator. The paper suggests quantitative scoring is used throughout but does not present formal Bayesian uncertainty propagation.",
            "validation_limitations": "Identified limitations include coverage and language bias of source data, subjectivity in concept granularity, annual temporal resolution coarseness, complexity of validating scientific evolution without universal gold standards, and significant computational demand of LLM-guided construction.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines automated retrieval + NLI + LLM fallback (RA-NLI) for link validation, LLM-guided MCTS search and node generation (TVCV), and human expert refinement to produce final ground-truth trees; seeds construction from curated surveys to reduce noise.",
            "uuid": "e2121.0"
        },
        {
            "name_short": "RA-NLI",
            "name_full": "Retrieval-Augmented Natural Language Inference",
            "brief_description": "A validation module that assesses whether a proposed evolutionary/causal relationship between two literature nodes is supported by retrieved textual evidence, combining embedding-based retrieval, a fine-tuned scientific NLI model (fNLI), and an LLM-based adjudicator for ambiguous cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RA-NLI",
            "system_description": "Per-edge verifier: retrieves contextual passages from cited documents via embedding-based search, applies a fine-tuned DeBERTa NLI model to classify entailment/contradiction/neutral and produce a normalized score, and uses a large LLM (Qwen2.5-72B-Instruct) to re-evaluate neutral/ambiguous cases; outputs an attribution reward Rattr used to accept/reject and score evolutionary links.",
            "scientific_domain": "Literature-based verification across scientific domains (demonstrated on AI/CS literature; applicable to other domains where factual claims are textually documented)",
            "validation_type": "computational_proof",
            "validation_description": "Procedure: (1) retrieve evidence passages using embedding-based retrieval; (2) pass premise (retrieved evidence) and hypothesis (citing claim / node description) to fine-tuned DeBERTa NLI classifier producing entailment/contradiction/neutral probabilities; (3) normalize NLI output to [0,1]; (4) if label is neutral or ambiguous, call LLM evaluator (Qwen2.5-72B-Instruct) to adjudicate; (5) compute Rattr = α * NLI_score + (1-α) * LLMeval_score with α=0.7. Used both as a reward signal in SGT-MCTS and as an automatic verification filter in TVCV.",
            "simulation_fidelity": "N/A (textual entailment fidelity depends on retrieval quality and fNLI model suitability; considered high-fidelity for literature entailment when evidence is retrievable and text aligns well)",
            "validation_sufficiency": "Paper demonstrates RA-NLI is highly effective for literature-level validation (claim: substantially reduces fact-missing rates vs LLM-only). Authors caution that RA-NLI sufficiency is limited to claims documented and evidenced in text; experimental claims requiring domain-specific measurements may require additional empirical validation.",
            "validation_accuracy": "On a 71k fact-verification dataset derived from 27k papers, RA-NLI achieved accuracy 95.60% with a fact-missing rate of 4.75% (reported as best among compared methods).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Validated purely on a large computational fact-verification benchmark built from citation sentences paired with evidence from source papers; no physical experiments. Compared against multiple baseline LLMs with and without factual supplementation.",
            "validation_comparison": "Direct quantitative comparison in Table 2: RA-NLI (4.75% fact-missing, 95.60% accuracy) outperformed a set of LLM baselines (examples: GPT-4o had 58.19% fact-missing, 60.18% accuracy; Qwen2.5 58.29% missing, 53.95% accuracy; DeepSeek-R1 and DeepReviewer models varied), and factual-supplement variants improved some LLMs but generally did not match RA-NLI's combined low missing/high accuracy.",
            "validation_failures": "Neutral/ambiguous cases require LLM fallback and can remain uncertain; performance depends on quality of retrieved context (if retrieval misses supporting text, RA-NLI cannot validate claims); does not validate non-textual experimental results (e.g., instrument readings) beyond what is described in text.",
            "validation_success_cases": "High success on the 71k fact-verification benchmark with top accuracy and lowest fact-missing rate among compared systems; used successfully to filter and score evolutionary links in THE-Tree construction, improving downstream tasks.",
            "ground_truth_comparison": "The 71k dataset serves as the ground-truth testbed for RA-NLI; RA-NLI outputs were compared against labelled fact–evidence pairs derived from original texts.",
            "reproducibility_replication": "Dataset and methodology described and intended for release; paper does not report independent external replications beyond its internal experiments.",
            "validation_cost_time": "Requires embedding retrieval, NLI inference, and in ambiguous cases expensive LLM calls (Qwen2.5-72B), implying non-trivial compute cost; exact wall-clock or monetary costs not provided.",
            "domain_validation_norms": "Appropriate and accepted for literature-level claim verification; authors emphasize that in domains where empirical measurement is the norm (e.g., experimental chemistry/biology), text-based entailment is necessary but not sufficient to fully validate experimental claims.",
            "uncertainty_quantification": "Returns normalized entailment probabilities; combines NLI and LLMeval via weighted average (α=0.7); uses explicit neutral label handling and LLM-based secondary adjudication to manage uncertainty.",
            "validation_limitations": "Sensitive to retrieval quality and to the expressiveness of textual evidence; neutral/ambiguous cases require costly LLM fallback; cannot validate claims that have no textual evidence or that require raw experimental/measurement data beyond reported text.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Hybrid computational pipeline: embedding-based retrieval (information retrieval) + fine-tuned DeBERTa NLI (fNLI) + LLM adjudication (Qwen2.5) for ambiguous cases; scores combined by weighted formula for final Rattr.",
            "uuid": "e2121.1"
        },
        {
            "name_short": "TVCV",
            "name_full": "Think-Verbalize-Cite-Verify",
            "brief_description": "A four-step LLM-driven node-expansion methodology used within THE-Tree construction: generate candidate technological advancements (Think), summarize them (Verbalize), ground them in literature (Cite), and validate proposed relationships using RA-NLI (Verify).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Think-Verbalize-Cite-Verify (TVCV)",
            "system_description": "At each node expansion, an LLM proposes candidate next-step ideas (Think), condenses them into concise propositions (Verbalize), retrieves or cites supporting literature to ground the proposition (Cite), and then invokes RA-NLI to verify the logical/causal coherence of the proposed parent→child link (Verify). The Verify score contributes to the attribution reward Rattr used by SGT-MCTS.",
            "scientific_domain": "General literature-driven scientific idea generation and validation (demonstrated in AI/CS domains)",
            "validation_type": "hybrid",
            "validation_description": "Uses LLM generation plus retrieval: LLMs propose and summarize candidate advances, retrieve supporting papers or concepts, and RA-NLI performs per-link textual validation. The Verify step ensures links are temporally coherent and textually supported; ambiguous NLI outputs trigger LLM reconsideration or rejection.",
            "simulation_fidelity": "N/A (not a simulation; fidelity depends on LLM generation quality and RA-NLI retrieval/NLI fidelity)",
            "validation_sufficiency": "Effective for reducing LLM hallucination in proposing literature-grounded evolutionary steps and improving downstream prediction tasks; the paper recommends initializing from human-validated surveys to reduce noise, and notes that domain-specific experimental validation may still be required for empirical claims.",
            "validation_accuracy": "No standalone accuracy metric reported specifically for TVCV alone; as part of the integrated pipeline it contributed to overall MCTS-generated tree metrics (node/edge recall & precision above) and downstream task gains (graph-completion hit@1 +8–14%, future prediction ~+10%).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Validation is computational: TVCV's Verify step is implemented and evaluated via RA-NLI over the 71k fact-verification set and within the MCTS pipeline compared to expert-refined trees; physical experiments not involved.",
            "validation_comparison": "Compared implicitly against LLM-only generation pipelines and citation-only networks via downstream evaluations; TVCV+RA-NLI pipeline notably outperformed LLM-only and citation-only baselines in reconstructing and predicting evolution paths.",
            "validation_failures": "Standalone LLM Think/Verbalize steps are prone to hallucination and domain-knowledge gaps (paper cites up to ~60% unawareness of relevant papers for LLM-only verification). The Verify step mitigates but does not eliminate such failures, particularly when retrieval misses evidence.",
            "validation_success_cases": "When combined with RA-NLI and SGT-MCTS, TVCV enabled construction of trees that matched expert-refined trajectories reasonably well and improved downstream tasks (see THE-Tree and RA-NLI success metrics).",
            "ground_truth_comparison": "Component-level outputs are evaluated as part of MCTS-generated trees against expert-refined THE-Trees and in the large fact-verification benchmark.",
            "reproducibility_replication": "Methodology and algorithmic description provided; no external replication reported in the paper.",
            "validation_cost_time": "The Verify step requires retrieval and NLI/Large-LLM calls per candidate link, increasing computational cost relative to LLM-only generation; exact costs not quantified.",
            "domain_validation_norms": "Authors recommend seeding TVCV with human-validated surveys to adhere to community norms for historical reconstruction; for empirical claims, TVCV's literature verification is necessary but may be insufficient without experimental data.",
            "uncertainty_quantification": "Uses Rattr scores from RA-NLI, DPO and TemporalCoherence metrics for generation reward; ambiguous NLI outputs are explicitly flagged and re-evaluated by LLM, providing a mechanism to surface uncertainty.",
            "validation_limitations": "Depends on LLM capability to surface relevant literature and on retrieval quality; high computational overhead; neutral NLI cases reduce automated decisiveness; not a substitute for non-textual experimental validation.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines LLM generation with retrieval-augmented NLI verification (RA-NLI) and uses expert-curated surveys as starting points; Verify step fuses automated inference and LLM adjudication.",
            "uuid": "e2121.2"
        },
        {
            "name_short": "SGT-MCTS",
            "name_full": "Self-Guided Temporal Monte Carlo Tree Search",
            "brief_description": "An LLM-guided Monte Carlo Tree Search variant that constructs likely chronological evolutionary paths through literature by optimizing a composite reward combining node importance, generation coherence, and attribution validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SGT-MCTS",
            "system_description": "Search algorithm used to explore and assemble candidate evolution paths: nodes are proposed/expanded via TVCV; selection uses a modified UCT (SGT-UCT) that includes LLM-derived priority and temporal coherence; rewards combine S(v) (importance), Rgen (DPO-based generation coherence + temporal penalties) and Rattr (RA-NLI link validation).",
            "scientific_domain": "Literature mining and knowledge-graph construction for scientific domains (demonstrated in AI/CS and related fields)",
            "validation_type": "computational_proof",
            "validation_description": "Validation of candidate paths is baked into the search objective via Rattr from RA-NLI and Rgen coherence scores; the algorithm is evaluated by reconstructing expert-refined trees (node/edge recall, precision, F1) and by downstream prediction tasks (graph completion and future-path prediction).",
            "simulation_fidelity": "N/A (algorithmic search fidelity depends on LLM guidance, reward design, and RA-NLI reliability; not a physics simulation)",
            "validation_sufficiency": "Paper demonstrates SGT-MCTS guided by TVCV+RA-NLI yields high recall/precision in reconstructing expert trees and better predictive performance than citation-only baselines; authors note it is sufficient for literature-tracing goals but must be combined with expert curation for final authoritative ground truth.",
            "validation_accuracy": "Reconstruction metrics for MCTS-generated trees: node recall 0.84, precision 0.67, F1 0.75; edge recall 0.78, precision 0.64, F1 0.70. Downstream performance gains (see THE-Tree entry) attributed to SGT-MCTS + TVCV + RA-NLI pipeline.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "SGT-MCTS behavior and outputs were validated computationally by comparing generated trees to expert-curated ground truth and by measuring downstream task performance; no physical experiments.",
            "validation_comparison": "Compared performance of THE-Tree (built via SGT-MCTS pipeline) against citation-only graphs on graph-completion and future-prediction tasks—SGT-MCTS-derived THE-Trees performed substantially better across metrics.",
            "validation_failures": "Generated trees are not perfect and require expert refinement; specific false-positive/false-negative edge cases are summarized in aggregate metrics (precision &lt; recall in some cases), highlighting areas requiring human input.",
            "validation_success_cases": "Produced trees with high node/edge recall and improved downstream predictive capabilities compared to citation networks; aided in better LLM-based paper evaluation when used to augment reviewers.",
            "ground_truth_comparison": "Direct comparison against expert-refined THE-Trees across 88 topics; used as quantitative ground-truth benchmark for reconstruction metrics.",
            "reproducibility_replication": "Algorithmic formulas, hyperparameters (e.g., S(v) composition, SGT-UCT), and architecture details (node embeddings, GNN layers) are included; no external reproduction reported in the paper.",
            "validation_cost_time": "Search requires many LLM calls for node expansion and RA-NLI verification, making it computationally intensive; the paper flags computational cost as a limitation but does not provide exact runtime/cost figures.",
            "domain_validation_norms": "SGT-MCTS is presented as a computational method to recover literature-based evolutionary structure; for domains where experimental verification is normative, literature-only search must be complemented with empirical checks.",
            "uncertainty_quantification": "Search uses empirical mean rewards Q(v)/N(v), exploration terms, and LLMpriority * TemporalCoherence modifiers; reward-sensitive exploration and growing exploration coefficient c(t) incorporate observed reward statistics to manage uncertainty during search.",
            "validation_limitations": "Performance depends on quality of node importance scoring (S(v)), LLM guidance, retrieval quality for RA-NLI, and the completeness of the underlying literature corpus; requires downstream human curation to reach gold-standard quality.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "SGT-MCTS integrates algorithmic search with LLM-guided proposals (TVCV), RA-NLI verification for edges, and human expert refinement to finalize ground-truth trees.",
            "uuid": "e2121.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
            "rating": 2
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2
        },
        {
            "paper_title": "Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Validating ai-generated code with live programming",
            "rating": 1
        }
    ],
    "cost": 0.02080825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?
26 Jun 2025</p>
<p>Xin Wang 
Westlake University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Jiyao Liu 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Innovation Institute</p>
<p>Yulong Xiao 
Fudan University</p>
<p>Junzhi Ning 
Shanghai Artificial Intelligence Laboratory</p>
<p>Fuzhou University</p>
<p>Lihao Liu 
Shanghai Artificial Intelligence Laboratory</p>
<p>Junjun He 
Shanghai Artificial Intelligence Laboratory</p>
<p>Botian Shi shibotian@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Innovation Institute</p>
<p>Kaicheng Yu 
Westlake University</p>
<p>Shanghai Innovation Institute</p>
<p>Imperial College London Homepage Code</p>
<p>THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?
26 Jun 202566C5A59A3AFE29CA78407D661F6C391AarXiv:2506.21763v1[cs.AI]
Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow.Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show 60% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured.This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this, we introduce THE-Tree (Technology History Evolution Tree), a computational framework that constructs such domainspecific evolution trees from scientific literature.THE-Tree employs a search algorithm to explore evolutionary paths.During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify" process: an LLM proposes potential advancements and cites supporting literature.Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded.We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research.Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8% to 14% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100%.By constructing explicit, verifiable pathways of scientific progression, THE-Tree provides a robust historical foundation for evaluating new hypotheses (human or AI-generated) and enables a computable science history, fostering evidence-based AI-driven scientific discovery.</p>
<p>Introduction</p>
<p>Automating scientific discovery has been a long-standing goal [22,18].The recent rise of Large Language Models (LLMs) offers new avenues, with applications from hypothesis generation [35,5,3] to simulating autonomous AI scientists [8,6].However, a critical bottleneck remains: the effective evaluation and validation of scientific ideas, whether AI or human-generated.Current idea validation approaches face several critical challenges.First, manual verification, while ideal, is prohibitively time-consuming [28].Second, automated validation using LLMs [17,3] exhibits multiple limitations: (1) potential for hallucination and incomplete domain knowledge (our findings show 60% unawareness of relevant papers in specific domains), (2) susceptibility to superficial textual features, often highly rating plausible but erroneous propositions [28,35], and (3) inheritance of biases when trained on human review data (e.g., CycleReviewer [34]).Third, existing knowledge representation methods are inadequate, as (1) citation networks [7,16] contain noise and lack explicit causal links, and (2) narrative surveys remain unstructured.These limitations fundamentally stem from the absence of structured, causally linked historical data, hindering reliable AI-driven idea validation.These challenges stem from a critical absence of structured, causally linked historical data, hindering reliable AI-driven validation of ideas.</p>
<p>To address this, we propose leveraging the authentic patterns and causal evolutionary pathways from scientific history for more reliable assessment.We introduce THE-Tree (Technology History Evolution Tree), a computational framework to construct structured, verifiable, domain-specific technology evolution trees from scientific literature (illustrated in Figure 1).THE-Tree builds a topic's evolution by representing individual papers as nodes and the inferential relationships between these papers, specific to the topic, as edges.This aims to provide a solid factual basis and clear historical context for evaluating new hypotheses.THE-Tree utilizes a Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) and a novel Think-Verbalize-Cite-Verify (TVCV) methodology for node expansion.This process prompts an LLM to generate potential evolutionary steps (Think), summarize them concisely (Verbalize), ground them in specific supporting literature (Cite), and critically, validate the proposed relationship (Verify).Crucially, the 'Verify' step employs a Retrieval-Augmented Natural Language Inference (RA-NLI) mechanism to assess the causal and logical coherence of proposed relationships based on cited evidence, ensuring semantic soundness and fidelity of the identified evolutionary relationships.Tree construction often starts from human-validated knowledge like scientific surveys, providing a reliable starting point.</p>
<p>We demonstrate THE-Tree's efficacy by constructing trees for 88 distinct topics across relevant scientific domains.Their quality, validated by automated metrics and human assessment, confirms their effectiveness in reconstructing meaningful and accurate technological trajectories.Downstream tasks, including future node prediction and graph completion, showcase THE-Tree's potential to enhance AI-assisted scientific reasoning by anticipating subsequent developments and highlight its superiority over traditional citation networks.For instance, in graph completion, it outperforms traditional citation networks (e.g., on hit@1 across all tested models); for future node and trajectory prediction, it improves hit@1 by nearly 10%; and in paper evaluation, it enhances the ability of other models to assess important papers by almost 100%.In summary, our main contributions are:</p>
<p>• A novel computational framework, THE-Tree, incorporating the TVCV methodology with LLMguided SGT-MCTS to construct and validate verifiable technology evolution trees from scientific literature, addressing the lack of structured, causal historical data for scientific evaluation.</p>
<p>• A RA-NLI mechanism within TVCV for rigorous validation of the logical and causal coherence of evolutionary relationships, ensuring tree fidelity.</p>
<p>• The construction and validation of THE-Trees dataset, comprising 88 technology evolution trees across AI domains and a benchmark dataset of 71k fact verification evaluations from 27k scientific papers, with extensive experiments demonstrating superior performance in downstream tasks.</p>
<p>• A structured, verifiable foundation based on historical evolution patterns for evaluating scientific ideas (both human and AI-generated) and supporting grounded AI-driven discovery processes, enabling systematic assessment of scientific progress.</p>
<p>Related Work</p>
<p>Our work intersects with several research areas, primarily AI for scientific discovery, scholarly knowledge representation, and the evaluation of scientific novelty.</p>
<p>AI for Scientific Discovery and Evaluation.The ambition to automate scientific discovery using AI has gained significant momentum with the rise of LLMs [19,20].Current applications range from hypothesis generation [35,5,3] to simulating autonomous AI agents for specific scientific tasks [8,6].A persistent challenge, as highlighted in our Introduction, is the rigorous evaluation of the novelty and feasibility of AI-generated outputs.Manual verification remains a bottleneck [13].While some emerging efforts aim to automate aspects of this evaluation, they often focus on simulating existing human processes [17,24,34].For instance, CycleResearcher [34] employs an LLM within an automated research-review loop to mimic peer review by predicting scores and providing feedback.</p>
<p>Although valuable for replicating current assessment workflows, such approaches primarily model established paradigms and may not provide the deep.Our work complements these efforts by focusing on constructing the underlying historical structure itself, offering a fact-based pathway for evaluation grounded in demonstrable scientific lineage.</p>
<p>Scholarly Knowledge Representation and Analysis.Representing and analyzing the vast body of scientific literature has long been a goal.Traditional bibliometric methods, including citation analysis [14,29] and co-word analysis [10], offer insights into publication impact and thematic trends.Science mapping tools like VOSviewer [32] and CiteSpace [11] provide valuable visualizations of research landscapes.However, as noted in our Introduction, these approaches face limitations.Citation networks are often noisy and fail to capture the explicit causal or logical dependencies signifying true intellectual inheritance [7], making them a "poor substrate for tracing idea lineage".Co-word analysis identifies term co-occurrence but not necessarily causal links.While useful for broad overviews, these methods generally lack the granularity and causal structure needed for deep reasoning about technological evolution or predictive analysis of research trajectories.</p>
<p>Scholarly Knowledge Graphs.More recently, large-scale scholarly knowledge graphs, such as the Microsoft Academic Knowledge Graph [33] and AMiner [30], have emerged, integrating diverse metadata.Knowledge graph construction techniques [2] have also been applied to scientific literature.While these graphs offer rich resources, they often focus on entity relationships (e.g., author collaborations, affiliations) or represent relatively static snapshots of knowledge domains.They typically do not explicitly model the dynamic, temporal, and causal evolutionary pathways of scientific ideas -how one concept or technology directly enables or influences the next.Capturing this validated, directed evolution is precisely the gap THE-Tree aims to fill.</p>
<p>THE-Tree as Scientific Verifier</p>
<p>THE-Tree: A Structured Representation of Scientific Evolution</p>
<p>The pursuit of scientific discovery is increasingly aided by AI, yet verifying the novelty and validity of numerous AI-generated or human-conceived hypotheses presents a significant bottleneck, as highlighted in our Abstract and Introduction.Traditional methods like citation networks lack the necessary semantic depth (i.e., the relationship between nodes is often limited to a simple "cite" declaration), offering only noisy and superficial links (e.g., perfunctory citations, negative citations, or citations to general background rather than specific conceptual building blocks), while LLMs as standalone verifiers can hallucinate or miss crucial domain knowledge (our experiments indicate that large models may exhibit factual omissions or fabrications in nearly 60% of cases when used for direct verification).This underscores the urgent need for a structured, verifiable, and causally-linked representation of scientific evolution.</p>
<p>To address this challenge, we introduce the THE-Tree (Technology History Evolution Tree).In a THE-Tree, each scientific paper is conceptualized as a node.Each node encapsulates rich metadata crucial for understanding the paper's context, contribution, and significance.This information typically includes the paper's title, abstract, authors, publication venue, and publication year.Furthermore, each node v is associated with a computationally derived importance score S v , reflecting its relevance and impact within the specific domain, as will be detailed in Section 3.3.1.</p>
<p>The edges in a THE-Tree represent the historical, inferential, and evolutionary relationships between these paper nodes (see Figure 3).</p>
<p>Unlike traditional citation networks, where an edge might merely indicate a citation without specifying the nature of the relationship, edges in THE-Tree are imbued with deeper semantic meaning.They are constructed to signify how one paper (or the ideas and technologies presented therein) causally contributes to, logically enables, or provides an essential foundation for the advancements detailed in a subsequent, connected paper.This distinction is critical.While traditional citation links are valuable, they are often too simplistic or noisy (e.g., perfunctory citations, negative citations, or citations to general background rather than specific conceptual building blocks) to accurately map the nuanced, multi-step evolution of scientific ideas.Our methodology, further detailed in Section 3.3.1 (referring to SGT-MCTS and TVCV parts), therefore focuses on identifying and establishing edges that reflect substantive intellectual lineage and direct technological dependence, aiming to filter out superficial connections and capture the true pathways of innovation.THE-Tree thus facilitates a more evidence-based approach to scientific idea validation.</p>
<p>The primary purpose of constructing and utilizing THE-Trees is to provide a structured, verifiable, and causally-linked historical tapestry of scientific evolution for specific domains.This detailed, graph-based representation serves as a robust knowledge scaffold.It allows new scientific propositions-whether generated by humans or AI-to be situated within an explicit, evidence-backed evolutionary context.By tracing connections and analyzing pathways within the THE-Tree, we can more rigorously assess a new idea's novelty (i.e., does it genuinely extend or diverge from known paths?), its factual consistency with established knowledge, and its potential impact, thereby addressing the limitations of standalone LLM evaluators and often unstructured narrative surveys.THE-Tree thus facilitates a more evidence-based approach to scientific idea validation.</p>
<p>Leveraging THE-Tree for Scientific Idea Verification</p>
<p>Once a THE-Tree, with its richly annotated nodes and semantically meaningful edges, is constructed for a specific scientific domain, it serves as a powerful instrument for the verification and contextualization of new scientific ideas or papers.We propose a straightforward yet effective methodology to utilize THE-Tree for this purpose, enabling the retrospection of relevant historical evolutionary paths for a given input scientific paper, P in , defined by its title T in and abstract A in .This approach provides critical context by situating new research within established knowledge frameworks, thereby aiding in the assessment of its novelty and potential contribution.</p>
<p>The core steps of this verification and retrospection process are as follows:</p>
<ol>
<li>Initialization: This methodology assumes access to:</li>
</ol>
<p>• A collection of pre-computed THE-Trees {G k }, where each G k = (V k , E k ) corresponds to a specific scientific topic T opic k .Nodes v ∈ V k represent scientific papers with attributes such as publication year Y v , title T v , abstract A v , and an importance score S v , as defined in Section 3.1.Edges e ∈ E k signify directed evolutionary relationships, capturing inferential and developmental dependencies.</p>
<p>• A Large Language Model (LLM) for semantic tasks such as similarity assessment and topic matching.They can be ranked based on various criteria, such as the semantic similarity score Sim(P in , v * term ) of their terminal nodes to the input paper P in , the cumulative importance of nodes in the path, or the overall coherence of the path.The top N P unique paths are selected and formatted for presentation.This presentation details the sequence of papers, their key attributes (title, year, summary of contribution derived from abstract/node info), and the nature of the connecting evolutionary relationships.</p>
<p>This methodology provides a simple yet powerful way to leverage the structured knowledge within THE-Trees to verify a new scientific idea by exploring its historical context and connections to established research.The retrieved paths can highlight the foundations upon which P in builds, identify potentially overlooked prior art, or help assess its incremental novelty versus a more radical departure from existing trajectories.Users can adapt this general approach based on specific analytical needs, such as modifying the LLM prompts, similarity thresholds, path selection heuristics, or the depth of retrospection.</p>
<p>Automated Construction of THE-Tree</p>
<p>The manual construction of comprehensive and accurate THE-Trees for diverse scientific domains would be a prohibitively laborious task.Therefore, we develop a computational framework for the automated construction of THE-Trees from scientific literature.Our approach formulates this construction as an optimization problem: the goal is to identify and assemble evolutionary paths through the literature that maximize a composite reward.This reward is designed to reflect the significance of the constituent papers (nodes) and the logical coherence and evidential support of the evolutionary steps (edges) they represent:
max Path v∈Path S(v) + R gen + R attr ,(1)
where S(v) is the importance score of a node (paper) v (see Section 3.3.1 for how S(v) is determined), R gen is the generation process reward reflecting the coherence of the path (how well a new node continues an existing path), and R attr is the attribution process reward validating the evidential support for the link (edge) between connected nodes.This objective is pursued using a Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) algorithm, as detailed in the pipeline stages below.</p>
<p>THE-Tree Construction Pipeline Overview</p>
<p>The construction of a THE-Tree involves several key stages, from initial data preparation to the iterative refinement of the tree structure.Figure 3 provides a schematic overview of this pipeline.Dataset Construction: Initialization and Foundation.The journey of constructing a THE-Tree begins with scientific surveys.These documents are invaluable as they encapsulate rich historical and inferential information about scientific progression, making them an ideal starting point for tracing technological evolution.However, directly utilizing raw surveys presents challenges due to their predominantly unstructured narrative format, lacking the explicit relational information needed for a computational model.To address this, our first crucial step is to transform these surveys into a structured dataset.We strategically select surveys from diverse time periods for each topic to mitigate recency bias and capture a broader, less noisy historical perspective.The core output of this stage is an interconnected dataset comprising survey documents, identified paper nodes (from citations within the surveys), concept nodes (core ideas extracted from paragraphs), and their explicit relationships.This structured foundation overcomes the limitations of raw narratives and noisy citation networks, enabling the detailed construction of technology evolution histories.Further details on this data construction pipeline are elaborated in Appendix A.2.</p>
<p>Core Component: Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS).With the structured dataset (Section 3.3.1)as our bedrock, we employ a Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) [9] algorithm to navigate the vast space of potential technological evolutionary paths and construct the THE-Tree.SGT-MCTS iteratively builds the tree by strategically selecting nodes for expansion, simulating potential future paths, and backpropagating rewards to update the value of explored paths (see Algorithm 1).The search is guided by the composite reward function introduced earlier (Equation 1: S(v) + R gen + R attr ), which balances node importance with path coherence and link validity.</p>
<p>Central to the SGT-MCTS guidance are the node importance assessment and generation process reward:
LLM-Enhanced Node Importance Assessment (S(v)):
The importance of a paper (node v) is a weighted combination of its structural significance within the citation graph and its semantic relevance as assessed by an LLM:
S(v) = γ • S graph (v) + (1 − γ) • S LLM (v)
, where γ is a weighting factor.S graph (v) combines multiple centrality measures (PageRank, citation count, Degree, Betweenness, Eigenvector Centrality) with dynamic weighting.S LLM (v) is obtained by prompting an LLM to assess the paper's importance within the specified topic.</p>
<p>Generation Process Reward (R gen ): This reward encourages the formation of coherent and temporally sound evolutionary paths.It is defined for a node v given a preceding path P prev : R gen (v|P prev ) = DPO(v|P prev ) • TemporalCoherence(v|P prev ).The DPO score [27], approximated by an LLM, evaluates how well node v continues the trajectory of P prev .The Temporal Coherence term penalizes achronological or large time gaps.The SGT-MCTS UCT formula is enhanced with LLM guidance and temporal coherence:</p>
<p>The selection of nodes during the SGT-MCTS process is guided by an Upper Confidence Bound 1 applied to Trees (UCT) formula [21].Our SGT-UCT variant, presented below, incorporates LLM guidance and temporal coherence to refine this selection:
SGT-UCT(v) = Q(v) N (v) + c • ln N (p) N (v) + λ • LLMpriority(v) • TempCoherence(v|Pprev)(2)
In this formula (Equation 2), Q(v)/N (v) is the exploitation component (empirical mean reward of node v).</p>
<p>The term c • ln N (p)/N (v) is the exploration component, encouraging visits to less explored nodes (N (p) is parent visit count, N (v) is current node visit count).LLMpriority(v) injects semantic guidance from an LLM.The entire expression is modulated by TempCoherence(v|Pprev) to enforce chronological path consistency.Constants c and λ are balancing hyperparameters.</p>
<p>We also implement reward-sensitive exploration where the exploration coefficient c(t) increases with the average reward, encouraging broader exploration in promising regions.</p>
<p>Node Expansion Method: Think-Verbalize-Cite-Verify (TVCV) Methodology.Node expansion within SGT-MCTS, the process of adding new paper nodes and establishing connections (edges with rich semantic meaning as discussed in Section 3.1), is performed using our novel Think-Verbalize-Cite-Verify (TVCV) methodology (see Algorithm 2).This process leverages an LLM to systematically generate, ground, and validate new nodes (potential technological advancements) and their links within the tree:</p>
<p>• Think: The LLM generates candidate technological advancements or scientific contributions that could logically follow from the current path history and domain knowledge.</p>
<p>• Verbalize: The LLM summarizes these generated ideas into concise statements or propositions that represent potential new nodes.</p>
<p>• Cite: For each summarized proposition, the LLM retrieves or identifies specific supporting scientific literature (i.e., existing paper nodes from the dataset or newly found papers) that grounds the proposed advancement, thereby proposing a potential link between an existing paper node and a new one.</p>
<p>• Verify: The proposed relationship (edge) between the current path's terminal node and the newly cited paper node, along with the relevance of the new node itself, is rigorously validated for logical consistency, causal coherence, and temporal soundness.This crucial step ensures the factual and logical soundness of the link, and is performed by the RA-NLI mechanism described in Section 3.3.1.1, is performed by our Retrieval-Augmented Natural Language Inference (RA-NLI) mechanism.RA-NLI is designed to rigorously assess the causal and logical coherence of the evolutionary link (edge) proposed between a parent node (e.g., vparent) and a newly cited child node (v),
Rattr(vparent → v) = RA-NLI(sparent, sv)(3)
This mechanism (illustrated in Figure 4) integrates embedding-based retrieval to fetch relevant contextual passages from the cited documents, followed by fine-grained Natural Language Inference using a specialized model (fNLI) to determine if the textual description of the child node (sv) is entailed, contradicted by, or neutral with respect to the parent node's description (sparent) and supporting evidence.An LLM-based verifier (fLLM) further refines ambiguous cases.This ensures semantic soundness and high fidelity for the identified evolutionary relationships, forming the backbone of the THE-Tree's verifiability.</p>
<p>Experiments and Results</p>
<p>For detailed information on dataset construction, statistics, and the definitions of evaluation metrics, please refer to Appendix B.1 and Appendix B.3 respectively.</p>
<p>THE-Tree Verifier for Scientific Evaluation</p>
<p>Further experiments are conducted to investigate whether THE-Tree can enhance the verification capabilities of Large Language Models (LLMs) for evaluating scientific claims and research contributions.We employ a straightforward methodology, leveraging the structural and semantic information within THE-Trees to augment LLM-based assessments.This approach is detailed in Appendix C.These investigations focus on two primary scenarios:</p>
<p>• Assessing Paper Acceptance with Factual Grounding: We evaluate whether the factual basis provided by THE-Tree can assist in determining if a paper merits acceptance.To avoid potential data contamination, as our THE-Tree might include previously published conference papers, the experiments involving THE-Tree augmentation were conducted exclusively on submissions to NeurIPS 2024.The core idea is to assess if grounding a paper's claims and contributions within the historical and causal context of a THE-Tree correlates with acceptance decisions.• Identifying High-Quality Papers Among Accepted Submissions: For papers that are accepted, we further investigate if THE-Tree can aid in distinguishing truly high-impact or high-quality research from other accepted works.This involves analyzing whether deeper integration or stronger alignment with the evolutionary trajectories and validated knowledge within THE-Tree can serve as an indicator of superior quality among the pool of accepted papers.The experimental results from the NeurIPS 2024 dataset for THE-Tree augmented evaluations are presented in Table 1.These findings indicate that by providing the scientific evolutionary context, THE-Tree significantly enhances the LLM's capability to determine paper acceptance.This augmentation notably improves the model's ability to reject low-quality submissions, rather than indiscriminately accepting them.Furthermore, THE-Tree augmentation proves highly effective in identifying high-impact papers , Orals, Spotlights.This enhancement substantially boosts the LLM's recognition capability for such papers, in some instances nearly doubling the accuracy or pushing it towards perfect identification in specific high-impact categories.For a detailed analysis and case study on the enhancement provided by THE-Tree, particularly for the DeepReviewer-14b model, please refer to Appendix C.1.In our RA-NLI-based validation system, we compared its accuracy and fact consistency against several state-of-theart models, both with and without factual supplementation.</p>
<p>THE-Tree</p>
<p>As shown in Table 2, our method demonstrates significantly lower fact-missing rates and the highest overall accuracy.</p>
<p>To evaluate the fact missing rate and NLI validation capability of our RA-NLI system, we conducted experiments on a dataset of 71k fact verifications covering 27k papers.This dataset was extracted from scientific papers and includes fact titles along with their corresponding evidence content from the original text.We compared RA-NLI's accuracy and fact consistency against several state-of-the-art models, both with and without factual supplementation.As shown in Table 2, our method demonstrates significantly lower fact-missing rates and the highest overall accuracy.</p>
<p>The MCTS-generated THE-Trees are compared against this ground truth using several quantitative metrics; these metrics are detailed in Appendix B.3.Table 3 summarizes these quantitative results, demonstrating that the MCTS-generated THE-Trees achieve strong performance in recalling entities and relations validated by experts, with reasonable precision and F1-scores.The average time difference in years for entity reconstruction is also comparable.</p>
<p>Structural and Semantic Properties: Graph Completion</p>
<p>We evaluated THE-Tree's ability to represent scientific knowledge structures via a graph completion task (Table 4).By using the graph completion method from [23], this task involved predicting missing evolutionary entities within our THE-Tree by masking entities from a year.The entities and relations from before that year, as historical information, were then used to predict the masked entities and their relations.This process benchmarked whether our THE-Tree could capture richer structural and semantic information compared to traditional citation-based networks.As detailed in Table 4, THE-Tree robustly outperformed traditional citation graphs, particularly in models with larger sizes, such as Qwen2.5-72b.This was evidenced by consistently higher prediction accuracy across metrics from Hit@1 through Hit@5, along with significantly improved mean reciprocal ranks (MRR) and notably lower median and mean ranks (MR).These results highlight THE-Tree's superior efficacy in modelling latent knowledge structures between entities.</p>
<p>Model</p>
<p>Hit@1 (↑) Hit@2 (↑) Hit@3 (↑) Hit@4 (↑) Hit@5 (↑) MR (↓) MRR (↑) MedianRank (↓)</p>
<p>Graph built with Traditional Citations Relations</p>
<p>Qwen2. 5</p>
<p>Future Path and Trajectory Prediction</p>
<p>To assess THE-Tree's proficiency in capturing scientific evolutionary dynamics, we conducted a future path prediction task.The experiment aims to validate THE-Tree's depth in modeling historical evolution and its capability to forecast 'reasonable next steps' in research trajectories, rather than claiming precise scientific discovery.Performance is evaluated using standard metrics (see Appendix B.3).The task involves predicting future entities (e.g., papers or concepts) and the semantic relations that lead to them, given a THE-Tree constructed with data up to year Y .This forecasting is benchmarked by comparing our semantically-enriched THE-Tree against a traditional citation-only graph, with results presented in Table 5.The citation-only graph shows limited predictive power (Hit@1 ≈10-18%, MR ≈ 4.5), underscoring the inadequacy of relying solely on citation topology for foresight.In contrast, THE-Tree, with its rich, context-aware semantic relations, demonstrates markedly superior performance.It achieves substantial gains, increasing Hit@3 and Hit@5 by approximately 5-10 percentage points, and significantly reduces both Mean Rank (MR) and Median Rank for entity and relation predictions alike.This consistent improvement across metrics highlights that the semantic signals encoded by THE-Tree, not just its structural density, are pivotal for sharper anticipation of research trajectory extensions.These findings validate the significant value of THE-Tree's semantic knowledge for scientific foresight and path discovery tasks.Model Graph Entity Relation Hit@1 (↑) Hit@3 (↑) Hit@5 (↑) (↓) MedianRank (↓) Hit@1 (↑) Hit@3 (↑) Hit@5 (↑) MR (↓) MedianRank (↓) Qwen2.5</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we introduced the Technology Evolution Tree (THE-Tree), a novel computational framework for constructing structured, verifiable, and causally-linked representations of scientific and technological evolution from literature.THE-Tree addresses critical challenges in evaluating scientific ideas, particularly those generated by AI, by moving beyond traditional bibliometrics and ungrounded LLM evaluations.</p>
<p>Our core contributions include: 1) A robust methodology integrating SGT-MCTS with TVCV and RA-NLI for verifiable tree construction.2) A novel dataset of 88 THE-Trees covering diverse AI topics.3) Demonstrated utility through quantitative evaluations, qualitative studies, and downstream tasks, showing THE-Treeś effectiveness in representing scientific knowledge and predicting its trajectory.</p>
<p>While THE-Tree offers a significant advancement, key limitations that guide our future work include:</p>
<p>• Data Coverage and Bias: Reliance on primarily English-language peer-reviewed literature.</p>
<p>• Granularity and Scope: Subjectivity in concept definition and a yearly temporal resolution for analysis.</p>
<p>• Validation Complexity: Inherent complexity of validating scientific evolution without a universal "gold standard".</p>
<p>• Computational Cost: Significant computational demands from extensive LLM usage and large-scale graph processing.</p>
<p>Future work will focus on addressing these limitations and expanding THE-Treeś capabilities.Key directions include:</p>
<p>• Enhancing Data Coverage and Representation: Broadening data sources (e.g., non-English literature, patents, technical reports), refining temporal granularity, and improving representation of diverse research outcomes (e.g., negative results).</p>
<p>• Improving Validation and Reasoning: Developing advanced automated validation techniques (e.g., incorporating causal inference), quantifying uncertainty in evolutionary links, and enhancing reasoning capabilities, particularly for understanding underexplored or abandoned research paths.</p>
<p>• Expanding Applications and Usability: Creating user-friendly interactive visualization tools, integrating THE-Tree insights into scientific workflows, refining predictive models, and developing cross-disciplinary analysis tools.</p>
<p>By providing a structured, dynamic, and causally validated map of scientific progress, THE-Tree offers a more reliable foundation for understanding scientific evolution, evaluating new ideas, and guiding future research.We believe THE-Tree represents a significant step towards a more transparent, evidence-based, and computationally-assisted approach to navigating and shaping the future of science and technology.The processing pipeline applied to these curated surveys involves the following key steps designed to extract structured information:</p>
<p>A.2 Detailed Dataset Construction from Surveys for THE-Trees</p>
<ol>
<li>
<p>Document Processing and Metadata Extraction: Each survey document (typically PDF) is parsed.Metadata for the survey itself is extracted.Crucially, its reference list is parsed to create initial 'paper nodes' for each cited work, populated with available metadata (title, authors, year, etc.).The survey text is segmented into paragraphs and sentences.</p>
</li>
<li>
<p>Sentence-Citation Pairing: Sentences containing citations within the survey text are systematically identified.For sentences in the paper that originally contain citations, we do not directly use them as citation sentences; instead, we perform a series of post-processing steps on their factual content, including the removal of invalid facts.For each processed sentence, its textual content is extracted and explicitly linked to the corresponding cited paper node(s) (identified via citation markers like ( [12]).This step generates numerous '<citing sentence, cited paper>' pairs.These pairs form the basis for two critical downstream tasks: (a) creating a dataset for subsequent causal relationship validation (e.g., using NLI to assess if the citing sentence is supported by the cited paper's content); and (b) creating a dataset for evaluating the model's ability to accurately extract citation context.</p>
</li>
</ol>
<p>Paragraph-Level Concept Graph Construction:</p>
<p>To capture the substantive content of scientific contributions beyond simple entity mentions and address the limitations of relying solely on named entities for tracing the evolution of <em>ideas</em>, we construct a paragraph-level concept knowledge graph.</p>
<p>Traditional entity-based KGs can face challenges in citation alignment for our task: (a) Granularity Mismatch: A citation often supports a broader claim or methodology within a sentence/paragraph, not just a specific entity mentioned nearby.(b) Synonymy/Paraphrasing: The core scientific concept might be described using different terminology or entities across papers (or even within the same paper), potentially breaking entity-based links.(c) Implicit Concepts and Alignment Omission: Important ideas (e.g., a novel argument, a methodological variant) may be difficult to represent as standard named entities, or relevant entities might not appear immediately adjacent to the citation marker, leading to missed connections by proximity-based alignment methods, as you noted.</p>
<p>To overcome these issues, we employ NLP techniques (e.g., keyphrase extraction, relation extraction, or summarisation) to extract core scientific/technical concepts from each paragraph.A 'concept' here refers to a core idea, method, or finding, often represented as a phrase or concise statement, rather than just an isolated named entity.A 'concept node' is created for each extracted concept.Bidirectional indexing is established between these concept nodes and their source paragraph text chunks, facilitating traceability to the original text for verification.Crucially, if a concept is derived from a sentence that contains a citation, this 'concept node' is linked to the 'paper node(s)' referenced by that sentence.This approach allows us to directly connect the core <em>ideas</em> expressed in the literature to their claimed sources (cited papers), better capturing intellectual lineage even when specific entity mentions vary or are absent.</p>
<p>This pipeline yields a structured dataset comprising surveys, cited papers (with metadata), concepts, sentences, paragraphs, and explicit links representing relationships such as '<citing sentence, cited paper>', '<concept, paragraph>', and '<concept, cited paper>'.This rich dataset covers content related to up to 27k papers, with as many as 71k entries used for factual verification evaluation.It forms the foundation for the subsequent construction of detailed technology evolution histories using the THE-Tree framework (integrating SGT-MCTS and TVCV).</p>
<p>A.3 Model Architecture Details</p>
<p>Construction Model Implementation: Our technology tree construction model uses a multi-layer graph neural network with the following architecture:</p>
<p>• Node embedding layer: 768-dimensional vectors initialised from SciBERT [4] • Graph attention layers: 3 layers with 8 attention heads each</p>
<p>• Temporal encoding: Sinusoidal position encoding based on publication year</p>
<p>• Edge type encoding: Learned embeddings for different relationship types</p>
<p>Training was performed using Adam optimizer with a learning rate of 0.0001 and batch size of 32.We used early stopping with a patience of 10 epochs based on validation loss.</p>
<p>NLI Model Implementation:</p>
<p>To assess the textual entailment within scientific citations in RA-NLI, we employ a fine-tuned DeBERTa model specifically adapted for inference tasks in the scientific domain.The model categorizes the relationship between a citation claim (hypothesis) and its associated source content (premise) into one of the following three classes:</p>
<p>• Entailment: The premise logically supports or implies the claim.</p>
<p>• Contradiction: The premise directly contradicts the claim.</p>
<p>• Neutral: There is insufficient evidence to determine a clear inferential relationship.</p>
<p>In cases where the NLI model outputs a neutral label-indicating ambiguity or lack of strong inferential evidence-we introduce a secondary validation step using a large language model (Qwen2.5-72B-Instruct).This LLM-based evaluator conducts a more nuanced assessment by considering broader contextual and semantic factors, further determining whether the cited relationship constitutes a direct citation, a paraphrase, or no meaningful connection.</p>
<p>Our validation process can be formalized as follows:</p>
<p>For a given pair of technology nodes (vi, vj) where vi is claimed to influence vj, we compute:
Rattr(vi → vj) = α • NLI(si, sj) + (1 − α) • LLMeval(si, sj, C)
where si and sj are the textual descriptions of nodes vi and vj respectively, C represents the retrieved context from the literature, and α is a weighting parameter determined empirically (set to 0.7 in our experiments).</p>
<p>The NLI function returns a normalised score in the range [0, 1] based on the entailment probability, while LLMeval returns a similar score based on the LLM's assessment of the citation validity.</p>
<p>A. 4 The data structure of THE-tree B Supplementary Details for Experiments</p>
<p>B.1 Dataset Construction and Statistics</p>
<p>Following the methodology described in Section 3, we constructed a dataset comprising 88 THE-Trees.These trees cover distinct technological topics primarily within core AI and its applications across diverse scientific domains (e.g., Computer Science, Biomedicine, Materials Science).The 88 THE-Trees represent validated evolutionary trajectories within this broader knowledge space.</p>
<p>B.2 Expert-Refined Ground Truth Construction Methodology</p>
<p>Our primary benchmark consists of THE-Trees meticulously curated by domain experts.This process involved two main stages: 1) Initial Tree Generation by MCTS: Our self-guided temporal Monte Carlo Tree Search (MCTS) algorithm, incorporating the Think-Verbalize-Cite-Verify (TVCV) methodology with Retrieval-Augmented Natural Language Inference (RA-NLI), first generated initial THE-Trees for each topic.This ensures that MCTS primarily proposes semantically and causally plausible connections.2) Expert Refinement and  Avg/Topic values computed using harmonic mean.</p>
<p>Augmentation: Domain experts then reviewed these MCTS-generated trees, performing comprehensive modifications.This included validating, correcting, or removing paths and nodes; augmenting trees with crucial missing links, milestone papers, or overlooked developmental trajectories; and ensuring overall semantic coherence, causal validity, and accurate representation of the field's historical evolution.The resulting expert-curated THE-Trees form the ground truth dataset used for validation as described in the main experimental sections.</p>
<p>B.3 Evaluation Metrics Definitions</p>
<p>We employ a comprehensive evaluation framework to assess both the quality of constructed THE-Trees and their performance on downstream tasks.The primary quantitative metrics used for comparing MCTS-generated THE-Trees against ground truth, and for other evaluations, are defined below:</p>
<p>• • Metrics for Future Node Prediction and Graph Completion (e.g., Hits@k, MR, MRR): These standard link prediction metrics are used as described in the main text when evaluating future node prediction (Section 4.3) and graph completion (Section 4.2.2).Their standard definitions apply.• Overall Accuracy Metrics in NeurIPS Paper Evaluation (Total% in Table 1): The experiment table includes two "Total%" overall accuracy metrics, calculated as explained below:</p>
<p>-"Total%" in the "Accuracy of accept and reject" section: This metric is a weighted average of the model's accuracy in correctly predicting acceptances and rejections, based on the actual acceptance and rejection rates for that year.The formula is:</p>
<p>Total%accept/reject = P (Actual Accept) × Accuracy(Predicted Accept|Actual Accept) +P (Actual Reject) × Accuracy(Predicted Reject|Actual Reject)</p>
<p>Where P (Actual Accept) and P (Actual Reject) represent the actual proportion of accepted and rejected papers in that year's dataset, respectively.Accuracy(Predicted Accept|Actual Accept) is the accuracy of the model in predicting a paper as accepted, given it was actually accepted (corresponding to the "Acc%" column in the table).Accuracy(Predicted Reject|Actual Reject) is the accuracy of the model in predicting a paper as rejected, given it was actually rejected (corresponding to the "Rej%" column in the table).-"Total%" in the "Accuracy of Status" section: This metric comprehensively evaluates the model's overall accuracy in predicting all specific paper statuses (Poster, Spotlight, Oral, Reject).</p>
<p>As per your description, its formula is: is the accuracy of the model in predicting a paper's status as s, given its actual status was s (corresponding to the "Poster%", "Spot%", "Oral%" columns in the table, respectively).Accuracy(Predicted Reject|Actual Reject) typically refers to the accuracy of the model in correctly predicting a paper as rejected, given it was actually rejected (e.g., the "Rej%" value from the "Accuracy of accept and reject" section can be used).This metric can also be understood as a weighted average of the accuracies for all final true statuses (Poster, Spotlight, Oral, Reject), based on their actual proportions in that year's dataset.Further detailed analysis of the DeepReviewer-14b model from the NeurIPS 2024 paper evaluation task (see Table 1 in the main text) provides a clear illustration of THE-Tree's impact.Figure 7 specifically highlights the performance differences when DeepReviewer-14b is augmented with THE-Tree versus its standalone performance.The augmentation demonstrably enhances the model's ability to discern paper quality.Notably, effectiveness in rejecting papers that do not meet the acceptance criteria, thus reducing the likelihood of erroneously endorsing lower-quality work.
Total%status = P (Actual Accept)× s∈{Post.,
Figure 8 delves into the prediction distribution for papers whose ground truth status was Oral or Spotlight.When THE-Tree augmentation is applied, the predictions made by DeepReviewer-14b for these high-impact papers shift more decisively towards categories indicating higher quality (e.g., predicting them as Oral or Spotlight with greater confidence or frequency).This contrasts with the standalone model, which may exhibit a more dispersed or less accurate prediction pattern for these important papers.This case study underscores THE-Tree's -Decision: accept -Rating: oral -Reason: "The paper introduces a novel approach to solving the many-electron Schrödinger equation using a learnable Pfaffian-based neural wave function.This is a significant departure from traditional methods that rely on Slater determinants and non-learnable orbital selection, addressing a key limitation in the field of computational quantum chemistry.-Status: Oral</p>
<p>• LLM Evaluation (without THE-Tree):</p>
<p>-Decision: accept -Rating: poster -Reason: "The paper introduces a novel approach to learning generalized wave functions for manyelectron systems by utilizing Pfaffians instead of Slater determinants, addressing the challenge of enforcing permutation antisymmetry in a fully learnable manner.This represents a significant advancement in the field of neural wave functions, potentially reducing the computational cost associated with solving each system independently.The empirical results demonstrate that the proposed 'neural Pfaffian' achieves chemical accuracy across various systems and outperforms the CCSD(T) CBS reference energies on the TinyMol dataset.The claim of reducing energy errors compared to previous generalized neural wave functions by up to an order of magnitude, if substantiated, indicates a substantial improvement in accuracy.The method's ability to generalize across molecules without constraints on electronic spin configurations or molecular structure further enhances its potential impact.However, the paper's clarity could be improved by providing more details on the specific architecture of the overparametrized neural network and the implementation of the Pfaffian.Additionally, a more detailed comparison with existing methods, including a discussion of their limitations and how the proposed approach overcomes them, would strengthen the paper's contribution.The significance of this work lies in its potential to revolutionize the way we approach quantum chemistry calculations, making them more efficient and accurate.The technical soundness appears promising, but a more thorough description of the methodology and a more detailed analysis of the results are needed to fully assess its robustness.Overall, the paper presents a compelling idea with strong empirical support, suggesting that it would be a valuable addition to the conference."</p>
<p>C.2.3 Comparative Analysis</p>
<p>The comparison between the two evaluation scenarios for the "Neural Pfaffians" paper, which had a ground truth status of Oral, reveals the positive impact of THE-Tree augmentation.</p>
<p>• Rating Accuracy: The LLM augmented with THE-Tree correctly predicted the paper's status as "oral", aligning with the ground truth.In contrast, the standalone LLM, while still deciding to "accept" the paper, rated it as "poster", underestimating its eventual impact tier.</p>
<p>• Identification of Novelty and Impact: The reasoning provided by the THE-Tree augmented LLM, despite noting that "the provided historical evolution paths primarily focus on general deep learning advancements," still identified that "the core idea of using Pfaffians in this context appears to be a novel contribution."This suggests that even with somewhat general historical context, THE-Tree helped the LLM anchor the paper's specific contribution within an evolutionary landscape, allowing it to better discern the novelty.It also highlights the "high impact potential" based on the claimed error reduction.The standalone LLM also recognized the significance ("potential to revolutionize"), but its reasoning carried more reservations regarding the need for "a more thorough description of the methodology and a more detailed analysis of the results ... to fully assess its robustness."While these are valid points for any review, the overall tone and the resulting "poster" rating suggest a slightly diminished confidence in the paper's immediate high-tier impact compared to the augmented evaluation.</p>
<p>• Confidence in Assessment: THE-Tree augmentation appears to provide the LLM with additional contextual evidence that strengthens its assessment.By situating the paper's contributions within a (even broadly) relevant historical trajectory, the LLM can form a more nuanced understanding of its specific advancements and their potential significance.This leads to an evaluation ("oral") that more closely mirrors the expert consensus reflected in the ground truth.The non-augmented LLM, lacking this explicit historical grounding, relies more heavily on the paper's intrinsic claims and presentation, which, while positive, led to a more conservative (and in this case, less accurate) impact assessment.</p>
<p>This case study demonstrates that THE-Tree enhancement can guide LLMs to more accurately assess the novelty and impact of scientific work, leading to evaluations that better align with expert judgments, particularly for high-quality papers.-Decision: accept -Rating: oral -Reason: "The paper introduces a novel perspective on Graph Contrastive Learning (GCL) by identifying a common mechanism, termed 'representation scattering,' that underlies the effectiveness of various GCL frameworks.This unifying concept is a significant contribution, offering a deeper understanding of why different GCL approaches work.The proposed Scattering Graph Representation Learning (SGRL) framework, which incorporates a center-away strategy to enhance representation diversity and a topology-based constraint to prevent excessive scattering, appears to be a technically sound and innovative approach.The paper's motivation is clear, and the proposed method is well-articulated.The experimental results, though not detailed in the provided text, are claimed to demonstrate the efficacy and superiority of SGRL over existing GCL methods, which, if substantiated, would indicate a significant impact on the field.The paper effectively bridges the gap between different GCL paradigms by highlighting their shared mechanism, and the proposed SGRL framework builds upon this understanding to improve performance.The technical evolution paths provided highlight the progression of GNNs and contrastive learning separately, and this paper attempts to synthesize these advancements in the context of GCL, which is a valuable contribution."-Decision: accept -Rating: poster -Reason: "The paper introduces a novel perspective on Graph Contrastive Learning (GCL) by identifying a common mechanism, termed 'representation scattering,' across different GCL frameworks.This unifying concept is a significant contribution, offering a deeper understanding of why these methods work.The proposed Scattering Graph Representation Learning (SGRL) framework, which explicitly leverages this mechanism with a center-away strategy and a topology-based constraint, demonstrates a clear technical innovation.The paper's claim of SGRL's efficacy and superiority is supported by evaluations on benchmark datasets, although the specific details of these evaluations are not provided in the abstract.The clarity of the abstract is good, outlining the problem, the proposed solution, and the key results.The potential impact of this work is substantial, as it not only provides a theoretical insight into GCL but also proposes a practical framework that could advance the field of graph representation learning."</p>
<p>C.3.3 Comparative Analysis</p>
<p>The distinct outcomes for the "Representation Scattering" paper highlight how THE-Tree enables a deeper, more contextual evaluation.</p>
<p>• Rating Accuracy: The THE-Tree augmented LLM correctly identified the paper as "oral," matching the ground truth.The standalone LLM, while positive, assigned a "poster" rating, failing to capture its full impact.</p>
<p>• Contextual Understanding of Contribution: Both evaluations acknowledge the novel "representation scattering" concept.However, the reasoning from the augmented LLM is more insightful.It explicitly references the historical context provided by THE-Tree, stating, "The technical evolution paths provided highlight the progression of GNNs and contrastive learning separately, and this paper attempts to synthesize these advancements..." This demonstrates that the LLM used the evolutionary context to understand how the paper unified two distinct research threads, a key factor in its high impact.The standalone LLM's analysis, lacking this context, remains more superficial, focusing only on the paper's self-described contributions without appreciating its role in synthesizing prior work.</p>
<p>• Assessment Confidence and Nuance: The augmented evaluation confidently points to the paper's value as a "synthesis of advancements."The standalone LLM, while acknowledging the "substantial" potential impact, gives a more standard review focused on the abstract's contents.The ability to place the work within its historical and technical lineage allowed the augmented LLM to make a more decisive and accurate judgment, mirroring the expert consensus of an "oral" presentation.</p>
<p>This case study further validates that by providing verifiable, historical context, THE-Tree empowers LLMs to move beyond surface-level text analysis and perform evaluations that are more aligned with nuanced, expert-level scientific assessment.</p>
<p>Figure 1 :
1
Figure 1: Overview of THE-Tree.(1) Limitations of existing data structures (publication databases, citation networks) for scientific idea generation versus THE-Tree's approach of constructing historical reasoning connections by screening important scientific history nodes and building pathways between them.(2) Methods for verifying LLM-generated ideas: implicit LLM-based cycle-review, human evaluation, and explicit THE-Tree-enhanced LLM verification.(3) Comparison of verification methods, highlighting issues like fact missing in LLM-only approaches, high cost in human evaluation, and THE-Tree's use of verifiable historical facts and scientific reasoning trajectories.(4) Performance improvements with THE-Tree in different tasks</p>
<p>Figure 2 :
2
Figure 2: A simple way to explicitly use THE-tree</p>
<p>Figure 3 :
3
Figure 3: THE-Tree construction overview.(a) Extracting chunks and references from surveys to build an initial concept graph structure; (b) Generating a structured knowledge tree using the SGT-MCTS algorithm, guided by the TVCV methodology for node expansion and RA-NLI for relationship validation.</p>
<p>Figure 4 :
4
Figure 4: Overview of RA-NLI process.This figure illustrates the RA-NLI process, including citation existence verification, document retrieval, and semantic relation assessment using NLI and LLM, which forms the core of the Verify step in TVCV and the R attr calculation.Relationship Validation: Retrieval-Augmented Natural Language Inference (RA-NLI) Mechanism.The critical Verify step of the TVCV methodology (Section 3.3.1),and the basis for the attribution process reward (Rattr) in Equation1, is performed by our Retrieval-Augmented Natural Language Inference (RA-NLI) mechanism.RA-NLI is designed to rigorously assess the causal and logical coherence of the evolutionary link (edge) proposed between a parent node (e.g., vparent) and a newly cited child node (v),</p>
<p>Figure 5 :
5
Figure 5: Document processing and metadata extraction pipeline applied to curated surveys for THE-Tree dataset construction.</p>
<p>Figure 6 :
6
Figure 6: The data structure of THE-tree</p>
<p>•</p>
<p>Node Recall: The proportion of nodes from the expert-refined ground truth trees successfully identified by the MCTS process.Node Recall = |MCTS-identified Nodes ∩ Ground Truth Nodes| |Total Nodes in Ground Truth| .• Node Precision: The proportion of nodes in MCTS-generated trees that are present in the expertrefined ground truth.Node Precision = |MCTS-identified Nodes ∩ Ground Truth Nodes| |Total Nodes in MCTS Tree| .• Edge Recall: The proportion of evolutionary connections from the expert-refined ground truth trees successfully identified by the MCTS process.Edge Recall = |MCTS-identified Edges ∩ Ground Truth Edges| |Total Edges in Ground Truth| .Edge Precision: The proportion of evolutionary connections in MCTS-generated trees that are present in the expert-refined ground truth.Edge Precision = |MCTS-identified Edges ∩ Ground Truth Edges| |Total Edges in MCTS Tree| .• F1 Score: The harmonic mean of precision and recall, calculated separately for nodes and edges.F1 = 2 × Precision×Recall Precision+Recall .• Average Temporal Interval: Given that our MCTS ensures chronological validity (parent node year ≤ child node year), this metric calculates the mean time difference in publication years between directly connected parent (vp) and child (vc) nodes: AvgInterval = 1 |E| (vp,vc)∈E (Year(vc) − Year(vp)).It characterizes the typical evolutionary pace captured.</p>
<p>C</p>
<p>Utilizing THE-Tree for Historical Path Retrospection C.1 Case Study: THE-Tree Augmentation Impact on DeepReviewer-14b for NeurIPS Paper Evaluation</p>
<p>Figure 7 :
7
Figure 7: Performance comparison of DeepReviewer-14b with and without THE-Tree augmentation on identifying high-quality papers and rejecting low-quality submissions in NeurIPS 2024 evaluation.</p>
<p>Figure 8 :
8
Figure 8: Prediction distribution of DeepReviewer-14b (with and without THE-Tree) for papers with ground truth status of Oral or Spotlight in NeurIPS 2024 evaluation.role as a powerful enhancement for LLM-based scientific evaluation.By providing structured historical context and verifiable evolutionary pathways, THE-Tree equips models like DeepReviewer-14b with a more robust foundation for assessing scientific contributions, leading to more accurate identification of impactful research and more reliable filtering of less meritorious submissions.</p>
<p>C. 3
3
Detailed Case Study: LLM Evaluation of "Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering" with and without THE-Tree Augmentation To further demonstrate the nuanced evaluation capabilities enabled by THE-Tree, this case study examines the LLM's assessment of the paper "Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering."The ground truth for this paper was also Oral.C.3.1 Case 1: LLM Evaluation with THE-Tree Augmentation • Paper Title: Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering • Original Output (Ground Truth):-Status: Oral• LLM Evaluation (with THE-Tree):</p>
<p>C. 3 . 2
32
Case 2: LLM Evaluation without THE-Tree Augmentation • Paper Title: Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering • Original Output (Ground Truth): -Status: Oral • LLM Evaluation (without THE-Tree):</p>
<p>3 .
3
Candidate Path Origination in Relevant THE-Trees: For each identified topic T opic k ∈ T Pin and its corresponding THE-Tree G k : (a) Identify a candidate set of M k recent papers {v k,j } ⊂ V k from G k based on publication year Y v k,j (e.g., papers published within a certain recent time window or up to the year of P in if known).(b) Calculate a semantic similarity score Sim(P in , v k,j ) ∈ [0, 1] between the input paper P in (using its title and abstract) and each candidate v k,j (using its stored title and abstract) using an LLM or other embedding-based techniques.(c) Select the top N S papers from these candidates, denoted S k = {v * Path Aggregation and Presentation: All generated paths {P ath k,l } are collected.
k,1 , . . . , v  *  k,N S }, that k,l ) ≥ θ sim , where θ sim is a predefined minimum exhibit a similarity score Sim(P in , v  *  similarity threshold. These papers v  *  k,l serve as terminal nodes (or "entry points") forbackward path tracing within the THE-Tree, representing the most closely relatedestablished works to P in .4. Historical Path Retrospection: For each selected terminal paper v  *  term ∈ S k in graph G k :• A historical path P ath k,l = (v 1 , v 2 , . . . , v n = v  *  term ) is constructed by iteratively selecting predecessors. This path traces the lineage of v  *  term backward through theTHE-Tree.• Starting from v curr = v  *  term , the predecessor v  *  pred is chosen from the set of allpredecessors P red(v curr ) (i.e., nodes from which an edge points to v curr ) according tothe lexicographical optimization that prioritizes strong evolutionary links and historicalrelevance:v  *  pred = argmin v∈P red(vcurr),Yv≤Yv curr((Y vcurr − Y v ), −S v )This heuristic prioritizes the predecessor with the smallest non-negative time difference(Y vcurr − Y v ), reflecting direct chronological succession, and secondarily, the one withthe highest importance score S v , indicating significant contributions. The use of nodeattributes Y v (year) and S v (importance) from the THE-Tree structure is central here.• Traversal continues backward until no valid predecessor (respecting chronologicalorder and edge semantics) is found, a predefined maximum path length is exceeded, ora root node of the THE-Tree (a node with no predecessors in the context of this path'shistory) is reached. The resulting path is then presented in chronological order fromthe earliest paper to v  *  term .5.</p>
<p>Table 1 :
1
Impact of THE-Tree Augmentation on LLM-based NeurIPS Paper Evaluation.The table compares the performance of various LLMs in predicting paper acceptance/rejection and status (Poster, Spotlight, Oral) for NeurIPS 2023 and NeurIPS 2024.For the NeurIPS 2024 dataset, results are presented with and without THE-Tree augmentation to demonstrate its effect on evaluation accuracy.Performance is evaluated using standard metrics (see Appendix B.3).
ModelNeurIPS 2023NeurIPS 2024Accuracy of accept and rejectAccuracy of StatusAccuracy of accept and rejectAccuracy of StatusAcc% Rej%Total%Poster% Spot% Oral% Total Acc% Rej%Total%Poster% Spot% Oral% Total%Qwen2.5-72b-instruct99.480.5226.343.5910003.3899.63025.701.2410002.42Deep-Reviewer-14b93.70 16.0636.3273.6531.82029.59 92.61 18.2837.4574.0322.776.9431.03Deep-Reviewer-7b83.94 18.7535.7657.4918.18027.54 86.79 19.7837.0760.4219.05028.9GPT4-O99.482.5927.8830.5490.91010.95 99.632.2426.0229.4686.96010.26Claude-3.5-Sonnet99.750.5226.426.275544.163.3899.750.3827.2810.9077.9618.574.59Deepseek-R199.573.4228.5154.4952.66016.27 100.02.0026.0354.1353.31015.00With THE-tree AugmentationQwen2.5-72b-instruct_tree-------99.840.3726.031.7697.3802.76Deep-Reviewer-14b_tree-------89.93 22.3939.8263.234.5534.7232.08Deep-Reviewer-7b_tree-------76.12 63.6966.905724.992.3360.84GPT4-O_tree-------99.662.6627.6934.45722.611.41Claude-3.5-Sonnet_tree-------71.46 36.5745.5728.7238.1336.5734.81Deepseek-R1_tree-------99.573.4228.2356.4956.662.616.68</p>
<p>Table 2 :
2
Comparison of Methods for Technol-
Quality Validation4.2.1 Effectiveness of Technology Trajectory ReconstructionTo validate THE-Tree's effectiveness in reconstructingmeaningful, accurate technology development trajecto-ries, we conducted comprehensive quantitative evaluationsagainst expert-refined ground truth. The methodology forogy Tree Relationship Verificationconstructing this expert-refined ground truth dataset is de-MethodFact Missing Rate (%) Accuracy (%)tailed in Appendix B.2.Claude 3.5 Sonnet [1]47.9360.22GPT-4o [26]58.1960.18DeepSeek R1 [15]48.1676.40Qwen 2.5-72B [31]58.2953.95DeepReviewer-7B [36]40.8893.95DeepReviewer-14B [36]68.8476.41LLaMA 3.1 [25]42.2960.40Factual SupplementClaude 3.5 Sonnet w/ Fact-65.34GPT-4o w/ Fact-69.40DeepSeek R1 w/ Fact-81.60Qwen 2.5-72B w/ Fact-66.80DeepReviewer-7B w/ Fact-95.38DeepReviewer-14B w/ Fact-82.09LLaMA 3.1 w/ Fact-65.35RA-NLI (Ours)4.7595.60</p>
<p>Table 3 :
3
Quantitative Comparison of THE-Tree Reconstruction Quality (MCTS-generated) against Expert-Refined Ground Truth.The methodology for constructing this expert-refined ground truth dataset is detailed in Appendix B.2. Metrics assess the ability to reconstruct entities (papers) and their evolutionary relations across the 88 topics.
EntityRelationMethodRecall PrecisionF1Avg_Time_Diff Recall PrecisionF1Expert1.001.001.002.931.001.001.00THE-Tree0.840.670.753.080.780.640.70</p>
<p>Table 4 :
4
Comparison</p>
<p>of Graph Completion Performance Between THE-Tree and Traditional Citation Graphs.Detailed definitions of the evaluation metrics (Hit@k, MR, MedianRank, MRR) can be found in Appendix B.3.</p>
<p>Table 5 :
5
Comparison of THE-Tree and Citation Graph on Future Path Prediction.Metrics reported for Entity and Relation predictions (see the definition of metrics in Appendix B.3).</p>
<p>Table 6 :
6
Enhanced Statistical Summary with Standardization
MetricTotalAvg/Topic Avg/THE-tree Avg/human selectProcessed topics88---Paper nodes35,392402.18103.1446.49Paper edges140,6161597.91255.57204.67
Note: All metrics calculated over 1950-2023 temporal scope.</p>
<p>Spot., Oral} (P (Actual is s|Actual Accept) × Accuracy(Predicted is s|Actual is s)) Actual is s|Actual Accept) represents the proportion of papers whose actual status is s (Poster, Spotlight, or Oral) among all accepted papers for that year.Accuracy(Predicted is s|Actual is s)
+ P (Actual Reject) × Accuracy(Predicted Reject|Actual Reject)whereP (Actual Accept)andP (Actual Reject)areasdefinedabove.P (</p>
<p>The use of Pfaffians to enforce permutation antisymmetry without constraints on spin configurations or molecular structure represents a notable innovation.The empirical results, demonstrating chemical accuracy on a range of systems and outperforming the CCSD(T) CBS reference on the TinyMol dataset, suggest a substantial improvement over existing generalized neural wave function methods.The paper's claim of reducing energy errors by up to an order of magnitude compared to previous methods, if substantiated, indicates a high impact potential for the field.However, the abstract lacks detailed information on the specific neural network architecture and training procedure, which would be crucial for a full assessment of the technical soundness.The claim of a single neural Pfaffian calculating both ground and ionization energies warrants further scrutiny regarding the methodology used for ionization energy calculations."
C.2.2 Case 2: LLM Evaluation without THE-Tree Augmentation• Paper Title: Neural Pfaffians: Solving Many Many-Electron Schrödinger Equations• Original Output (Ground Truth):
THE-Tree improves DeepReviewer-14b's capacity to correctly identify high-quality submissions, such as those ultimately designated as Oral or Spotlight presentations. Concurrently, it significantly boosts the model's
A Technical Appendices and Supplementary MaterialTechnical appendices with additional results, figures, graphs and proofs may be submitted with the paper submission before the full submission deadline (see above), or as a separate PDF in the ZIP file below before the supplementary material deadline.There is no page limit for the technical appendices.A.1 Data Collection and ProcessingWe collected data from multiple sources, including Web of Science, Scopus, arXiv, IEEE Xplore, and PubMed.The data processing pipeline consisted of the following steps:1. Metadata extraction: We extracted titles, abstracts, authors, venues, and publication dates using custom parsers for each data source.2. Citation network construction: We built a directed graph where nodes represent papers and edges represent citation relationships.3. Text preprocessing: We applied standard NLP preprocessing techniques, including tokenisation, stopword removal, and lemmatisation.4. Entity recognition: We used a combination of dictionary-based and machine learning approaches to identify technical terms and concepts.5. Temporal alignment: We aligned papers along a timeline, accounting for publication delays and citation patterns.To evaluate the accuracy of our metadata extraction component, we measured its precision on two commonly used formatting styles: 98.29% for IEEE format and 97.30% for APA format.These high precision scores demonstrate the robustness of our system in handling different citation conventions.
. Anthropic, 2024Claude 3.5 sonnet</p>
<p>Towards a knowledge graph for science. Sören Auer, Markus Stocker, Proceedings of the 11th International Conference on Semantic Systems. the 11th International Conference on Semantic Systems2018</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>What do citation counts measure? a review of studies on citing behavior. Lutz Bornmann, Hans-Dieter Daniel, Journal of documentation. 6412008</p>
<p>Sam Andres M Bran, Philippe Schilter, Delfosse, Anica Tobias, Apeksha Ivanova, Yoojin Thakkar, Philippe Jung, Alain C Schwaller, Kevin Maik Vaucher, Chang, arXiv:2304.05376Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>A survey of monte carlo tree search methods. Edward Cameron B Browne, Daniel Powley, Simon M Whitehouse, Peter I Lucas, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>Mapping the dynamics of science and technology: Sociology of science in the real world. John Michel Callon, Arie Law, Rip, Sociology of science in the real world. Macmillan London1983</p>
<p>CiteSpace II: Detecting and visualizing emerging trends and transient patterns in scientific literature. Chaomei Chen, Journal of the American society for information science and technology. 5732006</p>
<p>To cite, or not to cite? detecting citation contexts in text. Michael Färber, Alexander Thiemann, Adam Jatowt, Advances in Information Retrieval: 40th European Conference on IR Research. Grenoble, FranceSpringer2018. March 26-29. 2018. 201840</p>
<p>Validating ai-generated code with live programming. Kasra Ferdowsi, Ruanqianqian Huang, Michael B James, Nadia Polikarpova, Sorin Lerner, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Citation indexing: Its theory and application in science, technology, and humanities. Eugene Garfield, 1979Wiley</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Hlm-cite: Hybrid language model workflow for text-based scientific citation prediction. Qianyue Hao, Jingyang Fan, Fengli Xu, Jian Yuan, Yong Li, arXiv:2410.091122024arXiv preprint</p>
<p>Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Mlagentbench, arXiv:2310.03302Evaluating language agents on machine learning experimentation. 2023arXiv preprint</p>
<p>A theory of universal artificial intelligence based on algorithmic complexity. Marcus Hutter, arXiv preprint cs/00040012000</p>
<p>Towards a universal theory of artificial intelligence based on algorithmic probability and sequential decisions. Marcus Hutter, Machine Learning: ECML 2001. Springer2001</p>
<p>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, Peter Clark, arXiv:2503.227082025arXiv preprint</p>
<p>Levente Kocsis and Csaba Szepesv ári. Bandit based monte-carlo planning. European conference on machine learning. Springer2006</p>
<p>Scientific discovery: Computational explorations of the creative processes. Pat Langley, 1987MIT press</p>
<p>Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, Jay Pujara, arXiv:2305.10613Temporal knowledge graph forecasting without knowledge using in-context learning. 2023arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Introducing llama 3.1: Our most capable models to date. A I Meta, 2024</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances Neural Information Processing Systems. 202336</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Co-citation in the scientific literature: A new measure of the relationship between two documents. Henry Small, Journal of the American Society for information Science. 2441973</p>
<p>ArnetMiner: extraction and mining of academic social networks. Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, Zhong Su, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining2008</p>
<p>Qwen2.5: A party of foundation models. Qwen Team, September 2024</p>
<p>Software survey: Vosviewer, a computer program for bibliometric mapping. Nees Jan, Van Eck, Ludo Waltman, Scientometrics. 8422010</p>
<p>Microsoft academic graph: When experts are not enough. Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Darrin Eide, Yuxiao Dong, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>CycleResearcher: Improving Automated Research via Automated Review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, International Conference on Learning Representations (ICLR). 2025</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
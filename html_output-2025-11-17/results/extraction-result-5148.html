<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5148 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5148</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5148</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-269757280</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.06690v1.pdf" target="_blank">DrugLLM: Open Large Language Model for Few-shot Molecule Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have made great strides in areas such as language processing and computer vision. Despite the emergence of diverse techniques to improve few-shot learning capacity, current LLMs fall short in handling the languages in biology and chemistry. For example, they are struggling to capture the relationship between molecule structure and pharmacochemical properties. Consequently, the few-shot learning capacity of small-molecule drug modification remains impeded. In this work, we introduced DrugLLM, a LLM tailored for drug design. During the training process, we employed Group-based Molecular Representation (GMR) to represent molecules, arranging them in sequences that reflect modifications aimed at enhancing specific molecular properties. DrugLLM learns how to modify molecules in drug discovery by predicting the next molecule based on past modifications. Extensive computational experiments demonstrate that DrugLLM can generate new molecules with expected properties based on limited examples, presenting a powerful few-shot molecule generation capacity.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5148.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5148.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugLLM: Open Large Language Model for Few-shot Molecule Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive large language model trained on sequences of molecule modifications represented with a Group-based Molecular Representation (GMR) to perform few-shot and zero-shot molecule generation and optimization for drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoregressive Transformer decoder (LLaMA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Custom corpus constructed from ZINC and ChEMBL molecular/tabular data; authors report collecting over 25 million modification paragraphs and on the order of hundreds of millions to billions of molecule records (paper reports both ~200 million and ~2 billion molecules in different sections); training paragraphs encode sequences of molecule modifications tied to a single property.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — few-shot and zero-shot small-molecule generation/optimization for physicochemical properties (LogP, solubility, synthetic accessibility, TPSA) and biological activities.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretraining as an autoregressive language model on paragraphs of molecule modification cases (in-context few-shot learning); supports K-shot in-context generation (up to 9 shots due to input length limits) and instruction-guided zero-shot generation; uses GMR tokenization and vocabulary expansion (BPE).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Group-based Molecular Representation (GMR) strings (decodable back to canonical SMILES); can produce molecules that are decoded to SMILES for property evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate (proportion of generated molecules adhering to example modification rules), molecular similarity to query, property changes/distributions (LogP, solubility, TPSA, synthetic accessibility) assessed via RDKit/Python scripts, kernel density estimates, UMAP visualization of chemical space, predicted biological activity using a message-passing neural network (predictor with Pearson r ≥ 0.75).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Training data: ZINC, ChEMBL. Baselines for comparison: JTVAE, VJTNN, MoLeR, random sampling; LLM baselines: ChatGLM, ChatGPT, GPT-4. Test tasks included held-out biological activities (10 activities removed from training) and held-out composite optimization tasks for zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DrugLLM substantially outperformed the tested baselines in few-shot molecule optimization: for physicochemical properties it achieved progressive improvements with shot number (reportedly up to ~75% accuracy/success for LogP few-shot tasks), maintained higher success rates at higher similarity constraints than baselines, and slightly outperformed a SMILES-based variant (DrugLLM-SMILES) when using GMR. For biological-activity optimization DrugLLM often achieved large margins over baselines (example: ~76% success for Rho-associated protein kinase 1). In zero-shot instruction-guided optimization DrugLLM outperformed ChatGPT/GPT-4/ChatGLM across several composite tasks (Table 3 reports DrugLLM success rates ranging from ~40% to ~61% depending on task).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to graph/latent generative baselines (JTVAE, VJTNN, MoLeR), which achieved roughly ~50% success (similar to random in few-shot tests), DrugLLM showed marked improvement in few-shot tasks. Against general-purpose LLMs (ChatGPT, GPT-4, ChatGLM) in zero-shot molecule optimization, DrugLLM obtained higher success rates; ChatGPT/GPT-4 could sometimes produce valid optimized molecules but with substantially lower success, while ChatGLM largely failed or produced duplicates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Input length / hardware limits restrict support to up to 9-shot examples; zero-shot optimization capabilities are currently limited (mainly elementary compositions of up to two known properties); GMR has difficulty representing a small subset of complex molecules and lacks full standardization leading to occasional encoding/decoding errors; reported training-data size descriptions contain inconsistent counts in the paper (authors report different large-scale totals in different sections).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5148.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5148.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT family conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose conversational large language model from OpenAI used here as a baseline for zero-shot and few-shot instruction-guided molecule optimization; in experiments it could sometimes understand instructions and generate molecule strings but with low success rates compared to DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large autoregressive transformer (conversational LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper; described as trained on thousands of billions of tokens</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>General web and curated corpora (paper: 'trained on thousands of billion tokens'); not trained specifically on GMR or molecular modification paragraphs in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Tested for zero-shot instruction-guided molecule optimization (drug-design style tasks) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting with task descriptions and few-shot examples (prompt engineering) via API; attempted to output SMILES strings for optimized molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Attempted SMILES strings (validity variable); no GMR support.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Zero-shot success rate on composite optimization tasks evaluated via RDKit scripts (same tests as DrugLLM); success rates reported per task (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Test tasks: 6,000 instruction set (6 optimization tasks × 1000 each) constructed by authors; evaluation used RDKit and property calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGPT produced valid outputs on some zero-shot molecule optimization tasks but with relatively low success rates compared to DrugLLM. Table 3 reports ChatGPT success rates per composite task: QED & FractionCSP3 11%, QED & TPSA 15%, QED & #Rotatable bonds 15%, LogP & FractionCSP3 30%, LogP & TPSA 19%, LogP & #Rotatable bonds 19%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Inferior zero-shot optimization performance to DrugLLM; generally better than ChatGLM which often failed, but substantially worse than DrugLLM which benefited from targeted molecular-modification pretraining and GMR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General LLM not specialized to molecular language; difficulty mapping natural-language instructions to precise chemical-structure modifications; produces fewer valid/optimized SMILES and lower success rates in zero-shot molecular optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5148.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5148.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art general-purpose LLM used as a baseline for zero-shot instruction-guided molecule optimization; able to understand instructions and in places generate valid molecular outputs but with mixed success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large autoregressive transformer (multimodal-capable LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper; described as trained on thousands of billion tokens (cited GPT-4 technical report).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>General large-scale web and proprietary corpora (not task-specific to GMR or molecular modification paragraphs).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Tested for zero-shot molecule optimization in drug-design style composite tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting / instruction with examples via API; attempted to generate SMILES directly.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Intended SMILES strings (validity variable); no GMR support.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Zero-shot success rate on the authors' six composite optimization tasks evaluated with RDKit/property calculators (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Authors' zero-shot test set of 6,000 instructions (6 tasks × 1000 each); RDKit-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 achieved moderate zero-shot success on some tasks but generally underperformed DrugLLM. Table 3 reports GPT-4 success rates: QED & FractionCSP3 20%, QED & TPSA 20%, QED & #Rotatable bonds 10%, LogP & FractionCSP3 40%, LogP & TPSA 43%, LogP & #Rotatable bonds 5%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Better than ChatGLM on some tasks and sometimes comparable to ChatGPT, but consistently outperformed by DrugLLM which was pretrained on molecular modification paragraphs and GMR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not specialized for molecular structure-property modification tasks; difficulty reliably producing valid and property-optimized SMILES under zero-shot instructions; variable success across different composite tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5148.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5148.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose LLM (Chinese bilingual model family) evaluated locally as a baseline for zero-shot molecule optimization; struggled to produce appropriate or novel optimized molecules in the authors' tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large language model (transformer-based conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper (authors used an officially released pre-trained model deployed locally).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>General corpora used by ChatGLM (not specified in this paper); not trained on GMR data in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Tested for zero-shot molecule optimization (drug-design tasks) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting with instructions and examples (local deployment of pre-trained model).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Attempted SMILES strings (in practice often produced duplicates of input molecules or invalid outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Zero-shot success rate on authors' composite tasks evaluated with RDKit; behavior qualitatively described (duplicated outputs common).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Authors' zero-shot test set (6 tasks × 1000 instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGLM performed poorly in zero-shot molecule optimization; many outputs duplicated the input (i.e., failed to modify molecules) and success rates were near zero in several tasks. Table 3 reports ChatGLM success rates: QED & FractionCSP3 0.0%, QED & TPSA 3.0%, QED & #Rotatable bonds 6.0%, LogP & FractionCSP3 3.0%, LogP & TPSA 4.0%, LogP & #Rotatable bonds 4.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Significantly worse than DrugLLM, ChatGPT, and GPT-4 for zero-shot molecular optimization in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tendency to output duplicates of the input molecule or invalid molecular strings; lacks targeted molecular pretraining and specialized molecular tokenization (GMR) used by DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5148.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5148.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (Large Language Model Meta AI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of open foundation LLMs whose architecture/parameters (7B) were adopted as the base backbone for DrugLLM; other LLaMA-based open models (Alpaca, Vicuna) were noted to be unable to generate valid SMILES in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (decoder-only) foundation model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Paper states DrugLLM adopts parameters of LLaMA 7B; LLaMA 7B referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper for LLaMA (standard LLaMA training corpus); in this work LLaMA architecture/parameters were adapted and further trained on molecular modification corpus to create DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Foundation architecture for DrugLLM (drug-design/molecule generation when adapted and retrained).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Mentioned as backbone; generic autoregressive generation when used as LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Not applicable for LLaMA itself in this paper; base models lacked GMR-specific tokenization and typically failed to generate valid SMILES without further molecular training.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mentioned qualitatively: LLaMA (and Alpaca, Vicuna) were unable to generate valid SMILES strings in the authors' zero-shot tests.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not directly evaluated as a standalone method in molecular tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLaMA architecture was used as the foundation for DrugLLM (authors adopted LLaMA parameters and modified token/vocabulary). Generic LLaMA-like models without molecular pretraining were reported as unable to generate valid SMILES in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Served as a backbone (as opposed to an off-the-shelf molecular generator); DrugLLM augmented LLaMA with GMR and domain-specific pretraining to achieve molecule-generation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Base LLaMA models lack molecular tokenization/representations (e.g., GMR) and therefore do not reliably produce valid molecular strings or learn structure-property modification rules without domain-specific pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5148.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5148.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based approach for molecular generation cited by the authors; described in the paper as using SMILES as its primary representation (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style autoregressive model (as described by cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Described by authors as using SMILES as representation; original DrugGPT paper would detail training corpora (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug design / ligand generation (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>SMILES-based autoregressive generation (as stated by the authors' citation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES (explicitly noted by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated in this work (mentioned only in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Mentioned as related prior work; original datasets not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an example of prior GPT-based molecular generation that uses SMILES; DrugLLM distinguishes itself by using GMR and training on modification-paragraph corpora for few-shot capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited to contrast SMILES-based LLM approaches with DrugLLM's GMR-based approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in detail in this paper (only mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5148.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5148.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT (Generative Pre-trained Transformer for biomedical text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical-domain generative pretrained transformer cited as prior LLM work in biological/medical fields; mentioned by authors as still following traditional natural-language pretraining strategies, unlike DrugLLM's molecule-modification pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based generative language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Biomedical text corpora (as described by original BioGPT work; not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical text generation/mining (mentioned in related work, not used here).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Standard language-model pretraining on biomedical corpora (not specialized to direct molecule generation in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Natural language (not molecule-specific representations in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated here; mentioned to contrast with DrugLLM's molecular focus.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not applicable within this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned to highlight prior biomedical LLMs which generate natural language, in contrast to DrugLLM which generates molecular representations and performs few-shot molecular optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used to argue that existing biomedical LLMs are not directly tailored to molecule structure-property generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As noted by authors, traditional biomedical LLM pretraining does not directly learn structure-effect relationships required for molecule optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5148.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5148.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca / Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca and Vicuna (instruction-tuned LLaMA-based chat models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open community instruction-tuned models built on LLaMA; cited as examples of general-purpose LLMs that in the authors' tests were unable to generate valid SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca; Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned transformer LLMs (LLaMA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Instruction-tuning corpora based on LLaMA outputs and community datasets (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General-purpose conversational/assistant tasks; mentioned in context of molecule-generation capability (failed to produce valid SMILES in tests).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction tuning and prompting (not effective for SMILES generation in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Intended natural language; not producing valid molecular SMILES in authors' zero-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitatively reported as unable to generate valid SMILES—no detailed numeric evaluation provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Not applied to molecular benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Authors report Alpaca and Vicuna (LLaMA instruction-tuned models) could not generate valid SMILES in their zero-shot molecular generation tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with DrugLLM (specialized molecular pretraining and GMR) which succeeded at generating valid, property-optimized molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General-purpose instruction-tuned models lack molecular tokenization/representations and domain-specific pretraining required to produce valid molecule strings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins <em>(Rating: 2)</em></li>
                <li>Biogpt: generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 2)</em></li>
                <li>Large-scale chemical language representations capture molecular structure and properties <em>(Rating: 2)</em></li>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 1)</em></li>
                <li>Hierarchical generation of molecular graphs using structural motifs <em>(Rating: 1)</em></li>
                <li>Learning to extend molecular scaffolds with structural motifs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5148",
    "paper_id": "paper-269757280",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "DrugLLM",
            "name_full": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
            "brief_description": "An autoregressive large language model trained on sequences of molecule modifications represented with a Group-based Molecular Representation (GMR) to perform few-shot and zero-shot molecule generation and optimization for drug discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DrugLLM",
            "model_type": "Autoregressive Transformer decoder (LLaMA-based)",
            "model_size": "7B parameters",
            "training_data": "Custom corpus constructed from ZINC and ChEMBL molecular/tabular data; authors report collecting over 25 million modification paragraphs and on the order of hundreds of millions to billions of molecule records (paper reports both ~200 million and ~2 billion molecules in different sections); training paragraphs encode sequences of molecule modifications tied to a single property.",
            "application_domain": "Drug discovery — few-shot and zero-shot small-molecule generation/optimization for physicochemical properties (LogP, solubility, synthetic accessibility, TPSA) and biological activities.",
            "generation_method": "Pretraining as an autoregressive language model on paragraphs of molecule modification cases (in-context few-shot learning); supports K-shot in-context generation (up to 9 shots due to input length limits) and instruction-guided zero-shot generation; uses GMR tokenization and vocabulary expansion (BPE).",
            "output_representation": "Group-based Molecular Representation (GMR) strings (decodable back to canonical SMILES); can produce molecules that are decoded to SMILES for property evaluation.",
            "evaluation_metrics": "Success rate (proportion of generated molecules adhering to example modification rules), molecular similarity to query, property changes/distributions (LogP, solubility, TPSA, synthetic accessibility) assessed via RDKit/Python scripts, kernel density estimates, UMAP visualization of chemical space, predicted biological activity using a message-passing neural network (predictor with Pearson r ≥ 0.75).",
            "benchmarks_or_datasets": "Training data: ZINC, ChEMBL. Baselines for comparison: JTVAE, VJTNN, MoLeR, random sampling; LLM baselines: ChatGLM, ChatGPT, GPT-4. Test tasks included held-out biological activities (10 activities removed from training) and held-out composite optimization tasks for zero-shot evaluation.",
            "results_summary": "DrugLLM substantially outperformed the tested baselines in few-shot molecule optimization: for physicochemical properties it achieved progressive improvements with shot number (reportedly up to ~75% accuracy/success for LogP few-shot tasks), maintained higher success rates at higher similarity constraints than baselines, and slightly outperformed a SMILES-based variant (DrugLLM-SMILES) when using GMR. For biological-activity optimization DrugLLM often achieved large margins over baselines (example: ~76% success for Rho-associated protein kinase 1). In zero-shot instruction-guided optimization DrugLLM outperformed ChatGPT/GPT-4/ChatGLM across several composite tasks (Table 3 reports DrugLLM success rates ranging from ~40% to ~61% depending on task).",
            "comparison_to_other_methods": "Compared to graph/latent generative baselines (JTVAE, VJTNN, MoLeR), which achieved roughly ~50% success (similar to random in few-shot tests), DrugLLM showed marked improvement in few-shot tasks. Against general-purpose LLMs (ChatGPT, GPT-4, ChatGLM) in zero-shot molecule optimization, DrugLLM obtained higher success rates; ChatGPT/GPT-4 could sometimes produce valid optimized molecules but with substantially lower success, while ChatGLM largely failed or produced duplicates.",
            "limitations_or_challenges": "Input length / hardware limits restrict support to up to 9-shot examples; zero-shot optimization capabilities are currently limited (mainly elementary compositions of up to two known properties); GMR has difficulty representing a small subset of complex molecules and lacks full standardization leading to occasional encoding/decoding errors; reported training-data size descriptions contain inconsistent counts in the paper (authors report different large-scale totals in different sections).",
            "uuid": "e5148.0",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (GPT family conversational model)",
            "brief_description": "A general-purpose conversational large language model from OpenAI used here as a baseline for zero-shot and few-shot instruction-guided molecule optimization; in experiments it could sometimes understand instructions and generate molecule strings but with low success rates compared to DrugLLM.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_type": "Large autoregressive transformer (conversational LLM)",
            "model_size": "Not specified in paper; described as trained on thousands of billions of tokens",
            "training_data": "General web and curated corpora (paper: 'trained on thousands of billion tokens'); not trained specifically on GMR or molecular modification paragraphs in this work.",
            "application_domain": "Tested for zero-shot instruction-guided molecule optimization (drug-design style tasks) in this paper.",
            "generation_method": "Prompting with task descriptions and few-shot examples (prompt engineering) via API; attempted to output SMILES strings for optimized molecules.",
            "output_representation": "Attempted SMILES strings (validity variable); no GMR support.",
            "evaluation_metrics": "Zero-shot success rate on composite optimization tasks evaluated via RDKit scripts (same tests as DrugLLM); success rates reported per task (see Table 3).",
            "benchmarks_or_datasets": "Test tasks: 6,000 instruction set (6 optimization tasks × 1000 each) constructed by authors; evaluation used RDKit and property calculators.",
            "results_summary": "ChatGPT produced valid outputs on some zero-shot molecule optimization tasks but with relatively low success rates compared to DrugLLM. Table 3 reports ChatGPT success rates per composite task: QED & FractionCSP3 11%, QED & TPSA 15%, QED & #Rotatable bonds 15%, LogP & FractionCSP3 30%, LogP & TPSA 19%, LogP & #Rotatable bonds 19%.",
            "comparison_to_other_methods": "Inferior zero-shot optimization performance to DrugLLM; generally better than ChatGLM which often failed, but substantially worse than DrugLLM which benefited from targeted molecular-modification pretraining and GMR.",
            "limitations_or_challenges": "General LLM not specialized to molecular language; difficulty mapping natural-language instructions to precise chemical-structure modifications; produces fewer valid/optimized SMILES and lower success rates in zero-shot molecular optimization tasks.",
            "uuid": "e5148.1",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "State-of-the-art general-purpose LLM used as a baseline for zero-shot instruction-guided molecule optimization; able to understand instructions and in places generate valid molecular outputs but with mixed success.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "Large autoregressive transformer (multimodal-capable LLM)",
            "model_size": "Not specified in paper; described as trained on thousands of billion tokens (cited GPT-4 technical report).",
            "training_data": "General large-scale web and proprietary corpora (not task-specific to GMR or molecular modification paragraphs).",
            "application_domain": "Tested for zero-shot molecule optimization in drug-design style composite tasks.",
            "generation_method": "Prompting / instruction with examples via API; attempted to generate SMILES directly.",
            "output_representation": "Intended SMILES strings (validity variable); no GMR support.",
            "evaluation_metrics": "Zero-shot success rate on the authors' six composite optimization tasks evaluated with RDKit/property calculators (Table 3).",
            "benchmarks_or_datasets": "Authors' zero-shot test set of 6,000 instructions (6 tasks × 1000 each); RDKit-based evaluation.",
            "results_summary": "GPT-4 achieved moderate zero-shot success on some tasks but generally underperformed DrugLLM. Table 3 reports GPT-4 success rates: QED & FractionCSP3 20%, QED & TPSA 20%, QED & #Rotatable bonds 10%, LogP & FractionCSP3 40%, LogP & TPSA 43%, LogP & #Rotatable bonds 5%.",
            "comparison_to_other_methods": "Better than ChatGLM on some tasks and sometimes comparable to ChatGPT, but consistently outperformed by DrugLLM which was pretrained on molecular modification paragraphs and GMR.",
            "limitations_or_challenges": "Not specialized for molecular structure-property modification tasks; difficulty reliably producing valid and property-optimized SMILES under zero-shot instructions; variable success across different composite tasks.",
            "uuid": "e5148.2",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGLM",
            "name_full": "ChatGLM",
            "brief_description": "A general-purpose LLM (Chinese bilingual model family) evaluated locally as a baseline for zero-shot molecule optimization; struggled to produce appropriate or novel optimized molecules in the authors' tests.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGLM",
            "model_type": "Large language model (transformer-based conversational model)",
            "model_size": "Not specified in paper (authors used an officially released pre-trained model deployed locally).",
            "training_data": "General corpora used by ChatGLM (not specified in this paper); not trained on GMR data in this work.",
            "application_domain": "Tested for zero-shot molecule optimization (drug-design tasks) in this paper.",
            "generation_method": "Prompting with instructions and examples (local deployment of pre-trained model).",
            "output_representation": "Attempted SMILES strings (in practice often produced duplicates of input molecules or invalid outputs).",
            "evaluation_metrics": "Zero-shot success rate on authors' composite tasks evaluated with RDKit; behavior qualitatively described (duplicated outputs common).",
            "benchmarks_or_datasets": "Authors' zero-shot test set (6 tasks × 1000 instructions).",
            "results_summary": "ChatGLM performed poorly in zero-shot molecule optimization; many outputs duplicated the input (i.e., failed to modify molecules) and success rates were near zero in several tasks. Table 3 reports ChatGLM success rates: QED & FractionCSP3 0.0%, QED & TPSA 3.0%, QED & #Rotatable bonds 6.0%, LogP & FractionCSP3 3.0%, LogP & TPSA 4.0%, LogP & #Rotatable bonds 4.0%.",
            "comparison_to_other_methods": "Significantly worse than DrugLLM, ChatGPT, and GPT-4 for zero-shot molecular optimization in these experiments.",
            "limitations_or_challenges": "Tendency to output duplicates of the input molecule or invalid molecular strings; lacks targeted molecular pretraining and specialized molecular tokenization (GMR) used by DrugLLM.",
            "uuid": "e5148.3",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA (Large Language Model Meta AI)",
            "brief_description": "A family of open foundation LLMs whose architecture/parameters (7B) were adopted as the base backbone for DrugLLM; other LLaMA-based open models (Alpaca, Vicuna) were noted to be unable to generate valid SMILES in this work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLaMA",
            "model_type": "Transformer (decoder-only) foundation model",
            "model_size": "Paper states DrugLLM adopts parameters of LLaMA 7B; LLaMA 7B referenced.",
            "training_data": "Not specified in this paper for LLaMA (standard LLaMA training corpus); in this work LLaMA architecture/parameters were adapted and further trained on molecular modification corpus to create DrugLLM.",
            "application_domain": "Foundation architecture for DrugLLM (drug-design/molecule generation when adapted and retrained).",
            "generation_method": "Mentioned as backbone; generic autoregressive generation when used as LLM.",
            "output_representation": "Not applicable for LLaMA itself in this paper; base models lacked GMR-specific tokenization and typically failed to generate valid SMILES without further molecular training.",
            "evaluation_metrics": "Mentioned qualitatively: LLaMA (and Alpaca, Vicuna) were unable to generate valid SMILES strings in the authors' zero-shot tests.",
            "benchmarks_or_datasets": "Not directly evaluated as a standalone method in molecular tasks in this paper.",
            "results_summary": "LLaMA architecture was used as the foundation for DrugLLM (authors adopted LLaMA parameters and modified token/vocabulary). Generic LLaMA-like models without molecular pretraining were reported as unable to generate valid SMILES in the authors' experiments.",
            "comparison_to_other_methods": "Served as a backbone (as opposed to an off-the-shelf molecular generator); DrugLLM augmented LLaMA with GMR and domain-specific pretraining to achieve molecule-generation capability.",
            "limitations_or_challenges": "Base LLaMA models lack molecular tokenization/representations (e.g., GMR) and therefore do not reliably produce valid molecular strings or learn structure-property modification rules without domain-specific pretraining.",
            "uuid": "e5148.4",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DrugGPT",
            "name_full": "DrugGPT",
            "brief_description": "A GPT-based approach for molecular generation cited by the authors; described in the paper as using SMILES as its primary representation (mentioned in related work).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DrugGPT",
            "model_type": "GPT-style autoregressive model (as described by cited work)",
            "model_size": "Not specified in this paper.",
            "training_data": "Described by authors as using SMILES as representation; original DrugGPT paper would detail training corpora (not specified here).",
            "application_domain": "Drug design / ligand generation (mentioned in related work).",
            "generation_method": "SMILES-based autoregressive generation (as stated by the authors' citation).",
            "output_representation": "SMILES (explicitly noted by the authors).",
            "evaluation_metrics": "Not evaluated in this work (mentioned only in related work).",
            "benchmarks_or_datasets": "Mentioned as related prior work; original datasets not specified in this paper.",
            "results_summary": "Mentioned as an example of prior GPT-based molecular generation that uses SMILES; DrugLLM distinguishes itself by using GMR and training on modification-paragraph corpora for few-shot capability.",
            "comparison_to_other_methods": "Cited to contrast SMILES-based LLM approaches with DrugLLM's GMR-based approach.",
            "limitations_or_challenges": "Not discussed in detail in this paper (only mentioned).",
            "uuid": "e5148.5",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT (Generative Pre-trained Transformer for biomedical text)",
            "brief_description": "A biomedical-domain generative pretrained transformer cited as prior LLM work in biological/medical fields; mentioned by authors as still following traditional natural-language pretraining strategies, unlike DrugLLM's molecule-modification pretraining.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BioGPT",
            "model_type": "Transformer-based generative language model",
            "model_size": "Not specified in this paper.",
            "training_data": "Biomedical text corpora (as described by original BioGPT work; not detailed here).",
            "application_domain": "Biomedical text generation/mining (mentioned in related work, not used here).",
            "generation_method": "Standard language-model pretraining on biomedical corpora (not specialized to direct molecule generation in this paper).",
            "output_representation": "Natural language (not molecule-specific representations in this paper).",
            "evaluation_metrics": "Not evaluated here; mentioned to contrast with DrugLLM's molecular focus.",
            "benchmarks_or_datasets": "Not applicable within this paper's experiments.",
            "results_summary": "Mentioned to highlight prior biomedical LLMs which generate natural language, in contrast to DrugLLM which generates molecular representations and performs few-shot molecular optimization.",
            "comparison_to_other_methods": "Used to argue that existing biomedical LLMs are not directly tailored to molecule structure-property generation tasks.",
            "limitations_or_challenges": "As noted by authors, traditional biomedical LLM pretraining does not directly learn structure-effect relationships required for molecule optimization.",
            "uuid": "e5148.6",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Alpaca / Vicuna",
            "name_full": "Alpaca and Vicuna (instruction-tuned LLaMA-based chat models)",
            "brief_description": "Open community instruction-tuned models built on LLaMA; cited as examples of general-purpose LLMs that in the authors' tests were unable to generate valid SMILES strings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Alpaca; Vicuna",
            "model_type": "Instruction-tuned transformer LLMs (LLaMA-based)",
            "model_size": "Not specified in this paper.",
            "training_data": "Instruction-tuning corpora based on LLaMA outputs and community datasets (not specified here).",
            "application_domain": "General-purpose conversational/assistant tasks; mentioned in context of molecule-generation capability (failed to produce valid SMILES in tests).",
            "generation_method": "Instruction tuning and prompting (not effective for SMILES generation in this work).",
            "output_representation": "Intended natural language; not producing valid molecular SMILES in authors' zero-shot experiments.",
            "evaluation_metrics": "Qualitatively reported as unable to generate valid SMILES—no detailed numeric evaluation provided in the paper.",
            "benchmarks_or_datasets": "Not applied to molecular benchmarks in this paper.",
            "results_summary": "Authors report Alpaca and Vicuna (LLaMA instruction-tuned models) could not generate valid SMILES in their zero-shot molecular generation tests.",
            "comparison_to_other_methods": "Contrasted with DrugLLM (specialized molecular pretraining and GMR) which succeeded at generating valid, property-optimized molecules.",
            "limitations_or_challenges": "General-purpose instruction-tuned models lack molecular tokenization/representations and domain-specific pretraining required to produce valid molecule strings.",
            "uuid": "e5148.7",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins",
            "rating": 2,
            "sanitized_title": "druggpt_a_gptbased_strategy_for_designing_potential_ligands_targeting_specific_proteins"
        },
        {
            "paper_title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
            "rating": 2,
            "sanitized_title": "biogpt_generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        },
        {
            "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
            "rating": 2,
            "sanitized_title": "largescale_chemical_language_representations_capture_molecular_structure_and_properties"
        },
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 1,
            "sanitized_title": "junction_tree_variational_autoencoder_for_molecular_graph_generation"
        },
        {
            "paper_title": "Hierarchical generation of molecular graphs using structural motifs",
            "rating": 1,
            "sanitized_title": "hierarchical_generation_of_molecular_graphs_using_structural_motifs"
        },
        {
            "paper_title": "Learning to extend molecular scaffolds with structural motifs",
            "rating": 1,
            "sanitized_title": "learning_to_extend_molecular_scaffolds_with_structural_motifs"
        }
    ],
    "cost": 0.016347749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DrugLLM: Open Large Language Model for Few-shot Molecule Generation
7 May 2024</p>
<p>Xianggen Liu 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Yan Guo 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Haoran Li 
Jin Liu 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Department of Anesthesiology
Laboratory of Anesthesia and Critical Care Medicine
Frontiers Science Center for Disease-related Molecular Network
National-Local Joint Engineering Research Centre of Translational Medicine of Anesthesiology
West China Hospital
Sichuan University
610041ChengduChina</p>
<p>Shudong Huang 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Information Technology Research Center
Beijing Academy of Agriculture and Forestry Sciences
100097BeijingChina</p>
<p>Bowen Ke 
Department of Anesthesiology
Laboratory of Anesthesia and Critical Care Medicine
Frontiers Science Center for Disease-related Molecular Network
National-Local Joint Engineering Research Centre of Translational Medicine of Anesthesiology
West China Hospital
Sichuan University
610041ChengduChina</p>
<p>Jiancheng Lv 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>DrugLLM: Open Large Language Model for Few-shot Molecule Generation
7 May 202443D72A47FAC47A99D5D0B57EC2745F6CarXiv:2405.06690v1[q-bio.BM]large language modelfew-shot learningmolecule generation
Large Language Models (LLMs) have made great strides in areas such as language processing and computer vision.Despite the emergence of diverse techniques to improve few-shot learning capacity, current LLMs fall short in handling the languages in biology and chemistry.For example, they are struggling to capture the relationship between molecule structure and pharmacochemical properties.Consequently, the few-shot learning capacity of small-molecule drug modification remains impeded.In this work, we introduced DrugLLM, a LLM tailored for drug design.During the training process, we employed Group-based Molecular Representation (GMR) to represent molecules, arranging them in sequences that reflect modifications aimed at enhancing specific molecular properties.DrugLLM learns how to modify molecules in drug discovery by predicting the next molecule based on past modifications.Extensive computational experiments demonstrate that DrugLLM can generate new molecules with expected properties based on limited examples, presenting a powerful few-shot molecule generation capacity.</p>
<p>Introduction</p>
<p>Small molecules play a critical role in drug discovery due to their ability to bind to specific biological targets and modulate their functions (Kovachka et al., 2024;Offensperger et al., 2024;Scott et al., 2016).In fact, over the past decade, according to the approval records of the U.S. Food and Drug Administration (FDA), small molecule drugs have accounted for 76% of the total number of drugs approved for the market (Brown and Wobst, 2021;Norsworthy et al., 2024).Small molecules are advantageous in drug discovery because they can be synthesized relatively easily and have good bioavailability, making them more likely to reach their intended targets in vivo (especially when passing through cell membranes) (Vargason et al., 2021).However, based on the current research technologies, it is very challenging to design a molecule with ideal properties, and it will consume a lot of resources and time.For example, in the drug development process, it takes 9 to 12 years and billions of dollars to find an effective drug (Adams and Brantner, 2006;Dickson and Gagnon, 2009).</p>
<p>The vastness of the search space for new molecules, with up to 10 60 synthetically creatable drug-like molecules, presents a significant challenge in drug design as chemists must navigate this immense space to identify molecules that interact with a biological target, and while modern techniques allow for the testing of over 10 6 molecules in a laboratory setting, larger experiments become prohibitively expensive (Segler et al., 2018).As such, computational tools are necessary to help narrow down the search space.Virtual screening is one such strategy used to identify promising molecules from millions of existing or billions of virtual molecules (Lyu et al., 2023).But highthroughput screening and virtual screening only consider known molecules that are synthetically accessible and fall short of producing novel molecules (Crunkhorn, 2022;Sadybekov et al., 2022).</p>
<p>As an alternative to exploring the huge molecule space, de novo drug design exhibits the remarkable ability to generate entirely novel and distinctive molecules (Ren et al., 2024;Tropsha et al., 2024).Traditional de novo drug design methods use molecular construction rules to generate new molecules based on the receptor structure (Gillet et al., 1995;Waszkowycz et al., 1994) or the ligand structure (Afantitis et al., 2011).Recently, deep learning and reinforcement learning techniques offer promising potential in de novo drug design due to their powerfutl approximation capacity (Blaschke et al., 2018;Li et al., 2018;Liu et al., 2020;Putin et al., 2018).In particular, Popova et al. (2018) integrates both generative and predictive neural networks to generate novel targeted chemical libraries in a trial-and-error manner.Jin et al. (2018) propose a junction treebased variational autoencoder to learn a continuous molecule space and generate new molecules by sampling from it.</p>
<p>Despite the development of various deep learning (DL)-based methods for de novo drug design, the field of few-shot molecule generation remains notably underexplored.Few-shot molecule generation aims to generate new molecules with expected properties given limited molecule examples.Most of the current de novo drug design approaches require thousands of data for learning (Korshunova et al., 2022).However, data scarcity is a prevalent issue in drug discovery due to the high costs associated with biological experimentation (Wang et al., 2023).Consequently, the ability to perform few-shot generation is of paramount importance for the advancement of de novo drug design techniques.</p>
<p>The large language models (LLMs) have achieved significant progress in natural language processing, especially in the few-shot learning problem (Ahmed and Devanbu, 2022).Despite the emergence of diverse LLMs (Chang et al., 2024), they fall short in handling the languages in biology and chemistry (Ross et al., 2022).For example, they are still struggling to capture the relationship between molecule structure and the corresponding properties.Therefore, how do we build an LLM that can accurately characterize the "structure-effect-metabolism-toxicity" relationships in molecules?</p>
<p>In this work, we presented DrugLLM, a large language model for drug design.In DrugLLM, molecules are represented using Group-based Molecular Representation (GMR), which is a novel type of molecular representation to address token abundance, cyclic complexity, and structural sensitivity inherent in SMILES (O'Boyle, 2012).GMR makes structural groups as units to build the topological structure of molecules and transform them into linear sequences.</p>
<p>Furthermore, we will provide an in-depth exposition of the training methodology employed by DrugLLM.The methodology organizes the modification sequences in accordance with specific molecular properties.Drawing an analogy, a case of molecule modification (a pair of molecules with similar structures) toward a certain property serves as a "sentence".Multiple cases of modification toward the identical property constitute a paragraph.By continuously predicting the next molecule based on the modification history, DrugLLM learns the intrinsic relationship between the molecular structure and the corresponding property.To the best of our knowledge, DrugLLM is the first large language model for few-shot molecule generation.</p>
<p>Results</p>
<p>The DrugLLM Framework</p>
<p>The focus of this work is to train a large language model that can capture the relationship between the molecule structure and the corresponding chemical and biological activities.Unlike ChatGPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023), which are trained on massive text data from the internet, and DrugGPT (Li et al., 2023)that uses SMILES as its representation, DrugLLM employs Group-based Molecular RepresSMILESentation (GMR) strings as its primary language representation.GMR leverages structural groups to depict molecular architectures, thereby effectively overcoming three principal challenges inherent in the application of SMILES notation within model processing contexts: (1) Token Abundance: In the SMILES format, each character is considered a separate token, which can result in an unwieldy number of tokens and subsequently consume considerable computational resources during training.(2) Cyclic Complexity: The representation of cyclic structures within molecules is particularly intricate in SMILES, which increases the difficulty of model training.(3) Structural Sensitivity: Even minor alterations in a molecule's structure can produce significant discrepancies in the corresponding SMILES representation.</p>
<p>As shown in Figure 1A, the GMR framework employs unique string identifiers to represent distinct structural groups.These identifiers are linked by numerical position data flanked by a slash.By employing GMR, the model can recognize molecular strings by treating structural groups as units, thereby reducing the number of input and output tokens.Additionally, GMR removes cyclic structures by merging them, simplifying the logic of molecular assembly and lowering the difficulty of model recognition.It also minimizes the discrepancies in SMILES strings that result from even small structural changes.</p>
<p>To train DrugLLM, we construct sentences and paragraphs composed of molecule modifications as training data (Figure 1B).Specifically, DrugLLM regards the modification between two molecules with similar structures as a sentence.A series of such modifications are viewed as a sequence of sentences that form a paragraph.Note that we impose the constraint that the molecular modifications in a paragraph should characterize the identical property.For instance, if the first three cases of molecule modifications describe the increase in the number of hydrogen bond acceptors, we expect all subsequent sentences in that paragraph to also discuss the increase in acceptor numbers.In this way, the contents of a paragraph are concentrated, making DrugLLM able to auto-regressively predict the next token based on the previous contexts.Moreover, each paragraph exhibits autonomy, encompassing a diverse array of molecular characteristics.The distinct paragraphs engage with unique molecular properties, necessitating that DrugLLM be endowed with the capability for in-context learning (a form of few-shot learning).</p>
<p>However, there are few related datasets available.In this work, we collect the tabular form of the molecule datasets from the ZINC database (Wu et al., 2018) and the ChEMBL platform (Davies et al., 2015;Mendez et al., 2019), and convert them into the corresponding sentences and paragraphs.In total, we collected over 25, 000, 000 modification paragraphs and 200, 000, 000 molecules to build the training dataset (Table 1).The dataset involves over 10,000 different molecular properties or activities, such as the count of hydrogen bond acceptors and the topological  Following recent work on the pre-training large language models, DrugLLM is based on the Transformer architecture (Vaswani et al., 2017).We adopt the parameters of LLama 7B (Touvron et al., 2023) and expand the vocabulary by introducing the frequently-used SMILES tokens.These tokens are divided by the byte pair encoding (BPE) algorithm (Sennrich et al., 2016).We train DrugLLM using the AdamW optimizer for six weeks on eight NVIDIA RTX 3090 GPUs, where it learns to generate the paragraphs from scratch.In the view of machine learning, the paragraph plays as a process of few-shot molecule generation.Therefore, the trained DrugLLM is able to directly perform few-shot molecule generation without further fine-tuning.</p>
<p>DrugLLM is a few-shot learner in molecule optimization toward physicochemical properties</p>
<p>Figure 2A illustrates our approach under the K-shot learning framework, where we provide the model with K pairs of example modifications and a benchmark molecule.The objective of the model is to generate a new molecule that not only maintains structural similarity to the benchmark molecule but also exhibits superior properties (either increased or decreased, as guided by the examples).Due to input token limitations, we restrict the number of molecular optimization examples to a maximum of nine pairs.To visually represent the structural similarity between the benchmark and generated molecules in the chemical space, we employ the Uniform Manifold Approximation and Projection (UMAP) method to create a chart (Figure 2B).There is a high degree of consistency between the distribution of the generated molecules (on the left) and the source molecules (on the right).This distributional similarity, coupled with the notable improvement in the LogP properties of the generated molecules, underscores the robust capability of the model to optimize the properties of the benchmark molecule.</p>
<p>To evaluate the capacity of DrugLLM in terms of few-shot molecule generation, we select four physicochemical properties that are not seen by DrugLLM as the test tasks, including the wateroctanol partition coefficient (LogP), solubility, synthetic accessibility, and topological polar surface area (TPSA).As these four molecular properties can be accurately estimated by machine learningbased scripts, they are widely used in the assessment of molecule generation models.For comparison, we take the junction tree-based variational auto-encoder (JTVAE) (Jin et al., 2018), the variational junction tree neural network (VJTNN) (Jin et al., 2020), and the scaffold-based molecule generator (MoLeR) (Maziarz et al., 2021).We also include a random generation control implemented by random sampling based on the latent space of JTVAE.The quality of the generated molecules was assessed based on their success rate and molecular similarity.The success rate represents the proportion of generated molecules that adhere to the rule of examples of modifications.To avoid the generation bias of the generators, the input contexts (i.e., the prompts of the language models) describe the increment or decrement of the property with a balanced proportion.Although these models are not initially designed for few-shot learning, they are state-of-the-art molecule generators in literature.</p>
<p>We first present the distributions of several key properties -LogP, solubility, synthetic accessibility, and TPSA -for both the source and generated data in Figure 3A.These distributions are visualized using Kernel Density Estimation (KDE).The significant numerical improvements of the model in optimizing these key properties are clearly demonstrated, further attesting to the effectiveness of the model in optimizing molecular properties.Next, as shown in Figure 3B, we report the performance of few-shot generation with respect to the LogP value.We note that the three baseline molecule generators, namely JTVAE, VJTNN, and MoLeR, just obtained a success rate of about 50%, which is similar to a random generation.In contrast, DrugLLM exhibits a progressive improvement in few-shot molecule generation, with the accuracy of the generated molecules increasing incrementally to 75% as the number of shots increases.Performance comparisons on molecular solubility, synthetic accessibility, and TPSA are similar and consistent.When it comes to similarity, it is typically more challenging to optimize a molecule with fewer modifications (i.e., higher similarity).Despite this, DrugLLM maintains a higher success rate even with increased generation similarity, underscoring its superior performance in the few-shot generation.Furthermore, we note that DrugLLM-GMR slightly outperforms DrugLLM-SMILES, highlighting the benefits of GMR in large model training.</p>
<p>DrugLLM is a few-shot learner in molecule optimization towards biological activities</p>
<p>Since DrugLLM shows impressive few-shot generation capacity in physicochemical properties, we next validate the effectiveness of DrugLLM in the biological activities of molecules, which is more challenging.The molecules produced by DrugLLM are usually novel and are not recorded in the ChEMBL database.Unlike the physicochemical properties mentioned above, the biological activities of molecules (e.g. the Ki value on streptokinase A) are difficult to estimate by chemical or physical rules.In addition, the lengthy time and high costs associated with wet-lab experiments hinder the large-scale evaluation of molecules.Instead, we leverage a message-passing neural network to predict biological activities.Specifically, before building the DrugLLM dataset through the ChEMBL database, we scan all biological activities and select the one that has relatively adequate samples (N ≥ 800) and could induce an accurate property predictor (Pearson r ≥ 0.75).Finally, 10 activities are obtained and excluded from the training data.These activities are used to test the optimization of the few-shot DrugLLM.As the Pearson correlation of the prediction of the predictor is greater than 0.75, it could correlate well with the real evaluation in statistics.</p>
<p>We observe that the three generator baselines fail to obtain meaningful improvement compared with the random generation (Table 2), indicating that these molecule generators still struggle to capture the modification rules underlying the limited examples.As for DrugLLM, it significantly outperforms the other baselines by a large margin in most of the test properties.In particular, DrugLLM can generate appropriate molecules that bind to Rho-associated protein kinase 1, with a success rate of 76%.Note that these test properties are not observable in the training of DrugLLM.These results demonstrate that DrugLLM is able to figure out the intrinsic rules of the molecule modifications given a limited number of examples for an unknown molecular property.</p>
<p>DrugLLM support instruction guided molecule optimization in a zero-shot manner</p>
<p>Previous experiments demonstrated that DrugLLM can accept multiple pairs (i.e., shots) of molecules as references to learn the modification rules and generate new molecules with the desired properties.In this section, we explore zero-shot molecule optimization, which involves generating modified molecules with better properties of interest according to natural language instruction without any specific training instances.In this experimental setting, we assume that when DrugLLM is trained on a large scale of properties and their compositions, it is capable of generalizing to optimize the molecules toward unseen compositions of properties.Therefore, we move six optimization tasks out of the DrugLLM training set in advance and leave them as test tasks.For example, all samples related to the optimization of quantitative estimation of drug similarity (QED) and topological polar surface area (TPSA) are in the test set, but the samples related to the optimization of each single property are in the training set.Based on this setting, we build the test set that contains 6, 000 instructions, each optimization task for 1000.The generated molecules are evaluated by the Python scripts via the RDKit library.</p>
<p>It is noteworthy that DrugLLM is one of the very few approaches that support zero-shot molecule optimization.Apart from ChatGPT (Brown et al., 2020), GPT-4(Achiam et al., 2023), and ChatGLM (Zeng et al., 2022), the other large language models, e.g., LLaMA (Touvron et al., 2023), Alpaca (Maeng et al., 2017), and Vicuna (Chiang et al., 2023), were unable to generate valid SMILES strings of molecules.Thus, we only compared the zero-shot molecule optimization capacity between DrugLLM and ChatGPT, GPT-4, and ChatGLM, all of which are trained on thousands of billion tokens.The challenge of zero-shot molecule optimization lies in two folds.On the one hand, the mapping between semantics and molecular property is hard to learn from the general corpus.On the other hand, the biological data related to the structure-property relationship is not sufficient due to the lengthy time and high costs associated with wet-lab experiments.As a result, we observed that ChatGLM struggled to provide appropriate molecules on all the zero-shot molecule optimization tasks (Table 3), with most generations outputting the duplicated molecules with the input ones.In addition, ChatGPT and GPT-4 were able to understand the instructions and optimize some of the given molecules.However, the success rate is relatively low.In terms of DrugLLM, it improves the optimization success rates by significant margins compared with the other LLM, indicating a better capacity in instruction understanding and molecule optimization.</p>
<p>Discussion</p>
<p>In this study, we introduce a computational task named few-shot molecule optimization, which is one of the few-shot generation problems.Given a molecule of interest, the task involves generating new molecules that adhere to the rules underlying the few modification examples.Although various few-shot learning tasks have been proposed and investigated (Ma et al., 2021;Stanley et al., 2021;Zhang et al., 2024), there are few studies that consider few-shot molecule generation.Few-shot molecule optimization requires the model to capture the abstract rules in a small number of examples and apply the rules to new molecules, requiring a comprehensive understanding of "structure-effect-metabolism-toxicity" relationships.As expected, the current methods struggle to accomplish few-shot molecule optimization, including ChatGPT and the other competitive molecule generators.In comparison, DrugLLM exhibits impressive performance, taking a solidified step toward general artificial intelligence in drug design.</p>
<p>DrugLLM is a large language model (LLM), built on a large number of textual data that span a wide variety of small molecules and biological domains.Recently, LLMs, such as ChatGPT (Brown et al., 2020), Alpaca (Maeng et al., 2017), and ChatGLM (Zeng et al., 2022), have amazing capabilities for general-purpose natural language generation.However, they are designed for general use and lack profound biological and pharmaceutical knowledge.We notice that there are also several LLMs for biological and medical fields, such as BioGPT (Luo et al., 2022) and DrugGPT (Li et al., 2023).However, these LLMs still follow traditional training strategies that learn to generate natural language as in the biomedical article.This raises an open question that how LLM understands the language of biology and chemistry and how to perform few-shot learning in this field.In this work, we propose a novel solution in which DrugLLM iteratively performs similar molecule modifications according to the context using GMR.Experiments demonstrated its exceptional effectiveness in a few-shot molecule optimization.</p>
<p>Despite the advantages of our method, this study has several limitations.Firstly, DrugLLM merely supports up to 9 shots of molecule modifications due to the limitation of the hardware.In such an input length, we have validated the few-shot learning capacity of DrugLLM.In the future, we will increase the input length (i.e., the number of shots) to achieve more impressive performance in drug design.Secondly, the zero-shot molecule optimization of DrugLLM is relatively elementary.The current DrugLLM is only capable of optimizing the molecules toward the composition of two known molecular properties.The zero-shot learning ability for arbitrary instructions is still lagging behind.Thirdly, the GMR currently in use has difficulty representing a small number of complex molecules under certain circumstances and lacks certain standardization measures.In the future, we will optimize for special cases of GMR representation and add standardization methods to reduce the small number of encoding errors that the model may generate.</p>
<p>In conclusion, this study presents the first attempt to build a large language model for fewshot molecule generation and optimization.Based on the tabular data related to molecular properties and biological activities, we build a large-scale textual corpus in the format of the sequences of molecule modifications.DrugLLM is trained to predict the next molecules based on historical modifications in an autoregressive manner.In extensive computational experiments, we observed that DrugLLM surpassed all the competitive methods (including GPT4) in optimizing new molecules in the few-shot learning setting on over 20 properties or biological activities.These results establish the substantial enhancement of efficacy facilitated by our proposed methodology in molecule generation and optimization, highlighting the potential of DrugLLM as a powerful computational tool in drug discovery.</p>
<p>METHOD DETAILS</p>
<p>Data collection and preparation</p>
<p>To train and analyze the DrugLLM model, we construct a large-scale dataset from the ZINC (Wu et al., 2018) and ChEMBL (Davies et al., 2015;Mendez et al., 2019) datasets.ZINC is a free database that contains more than 230 million purchasable compounds in ready-to-dock, 3D formats.We filter the druglike molecules from ZINC and obtain 4.5 million molecules.ChEMBL is a comprehensive repository for bioactive compounds with their properties.We gather bioactivity data from the ChEMBL database with the corresponding Web resource client.Following the preprocessing pipeline in (Stanley et al., 2021), we excluded all compounds that are not druglike molecules.A standard cleaning and canonicalization procedure was applied to filtered compounds.All of the molecules were represented by SMILES strings and labeled with certain properties.To facilitate property comparison between two molecules, we only consider property categories with real numbers.Therefore, we obtained a large-scale dataset that comprised thousands of tabular data, each table corresponding to hundreds of molecules measured by an identical property.</p>
<p>Based on the collected tabular data, we then transform them into meaningful textual sentences and paragraphs.In particular, we regard the modification between two molecules with similar structures as a sentence and multiple cases of molecular modifications as a paragraph.In the meantime, we stipulate that the molecular modifications in a paragraph should describe the same property changes.In other words, if the first two cases of molecule modifications indicate the increase of solubility, we would expect the rest sentences of this paragraph to be all about the solubility increase.The above stipulation was realized by a heuristic algorithm: given a pool of molecules with their property (in tabular form), we first clustered the molecules in terms of the molecular scaffolds with randomly selected clustering centers.If the similarity between a molecule and a center is greater than 0.6, the molecule is clustered at that center.The number of clustering centers would dynamically increase until all of the molecules in the pool are classified.</p>
<p>Apart from the modification of the molecule to a single property, we also consider the compositions of multiple properties, which are mainly involved in the simple molecular properties that can be calculated by Python scripts.For example, we include LogP, Topological polar surface area (TPSA), and their composition in the training set for model training.In total, we collect over 25 million modification paragraphs and 2 billion molecules to build the training dataset.The dataset involves over 10, 000 different molecular properties, activities, and compositions.In ad-dition to the SMILES molecule, we also added descriptions of the property optimizations in each paragraph to build the relationship between the molecule structures and the semantic meaning of the properties.</p>
<p>Group-based molecular representation (GMR)</p>
<p>The core of the GMR framework involves decomposing molecules into structural groups and noting the inter-group connections, facilitating the group-based reconstruction of SMILES strings.GMR begins by assigning a unique string identifier to each structural group, thereby constructing a comprehensive dictionary.When a molecule is processed within the GMR framework, its SMILES string is converted into an encoded string that encapsulates both the identity of the structural groups and their positional relationships within the molecule.For computational analyses or property evaluations that require the original SMILES string, the encoded string can be decoded by applying a decoding algorithm.In the specific implementation of the GMR framework, there are three key steps:</p>
<p>(1) Dictionary construction: Initially, we leverage the extensive molecular data resources available in the ChEMBL database.Using SMILES expressions, we extract information about the ring structures in the molecule, merge intersecting rings, and identify structural groups on the rings.For the nonring parts of the molecule, we employed a strategy of breaking all C-C bonds and treating the remaining molecular fragments as independent structural groups.This approach allows us to decompose all the groups of the molecule, assign a unique string identifier to each group, and construct a comprehensive dictionary.</p>
<p>(2) Molecular encoding: The encoding process, as depicted in Algorithm 1, is initiated by splitting the SMILES string of an individual molecule into several structural units.We systematically decompose the molecule, removing each structural group and verifying the connectivity of the molecule after each removal.If the molecule remains connected after removal of a structural group, we record the two atoms at the connection point.Since marking the connection point may cause changes in the atom index and affect the subsequent normalization process of the molecule, we use a breadth-first search algorithm to characterize the marked atoms and their adjacent area in detail.This forms a feature description of the atoms.After removing the mark and performing SMILES normalization on the structural group and molecular fragment after splitting, we use the previously obtained atom feature description to re-identify and record the atom index of these two atoms as connection location information.This process is repeated until the molecule is completely decomposed into a single structural group, at which point the molecular fragment serves as the starting point for encoding.Starting from the encoding starting point, we build the basis of the encoding string according to the corresponding string in the dictionary.Subsequently, we traverse the structural groups recorded during the removal process and their corresponding atom indices, using the "/" character to separate different location information.Gradually, we integrate the strings corresponding to the structural groups in the dictionary and the location information into the encoding string, eventually generating a complete and accurate molecular encoding result.</p>
<p>(3) Molecular decoding: The decoding process is essentially the reverse of the encoding process.We use the encoded molecular fragment as the starting point for splicing and gradually recombine each structural group in the correct position according to the connection information recorded in the encoding.This process is repeated until all structural groups are correctly spliced back, thereby restoring the original molecular SMILES.This ensures the integrity and reversibility of molecular information.</p>
<p>The implementations of DrugLLM</p>
<p>Similar to ChatGPT, the training objective of DrugLLM is to iteratively predict the next token of the paragraphs.Formally, a generated paragraph x is composed of the optimization description o and the molecule modifications m, given by
x = [o, m 1 , m 2 , • • • , m N ],(1)
where m n stands for the n-th case of the molecule modification.Essentially, DrugLLM is to approximate the probability
P(x t |x 1 , x 2 , • • • , x t−1 ) = DrugLLM(x 1 , x 2 , • • • , x t−1 ),(2)
where x t is a token in the paragraph x.</p>
<p>However, the testing objective of the few-shot molecule optimization is to predict the optimized molecules given the few shots of molecule modifications.That is
m g = DrugLLM(m 1 , m 2 , • • • , m K , m o ),(3)
where m g stands for the generated molecules of DrugLLM in the K shots input.m o represents the query molecule that needs to be optimized.Similarly, the testing objective of the zero-shot molecule optimization is to predict the optimized molecules given the descriptions of the optimization tasks, given by
m g = DrugLLM(o, m o ).(4)
We adopt the LLaMA model architecture as the basic backbone of our DrugLLM.Specifically, DrugLLM is a Transformer decoder with 32 layers and 32 attention heads.The hidden dimension is set to 4096 and the batch size is 64.As a result, DrugLLM has 7 billion parameters, all of which are updated during the pre-training.The training process employs the AdamW optimizer with a learning rate of 3 × 10 −5 , and we adopt a cosine annealing schedule to adjust the learning rate.</p>
<p>The implementations of the competitive baselines</p>
<p>There are very few methods that support few-shot molecule optimization or generation.In this work, we take the widely used and powerful molecule generators as baselines, including JTVAE, VJTNN, MoLeR, and a control model (random generation).For these methods, we used the released official codes and adapted them to perform few-shot optimization tasks.For JTVAE, which is designed to generate molecules from a pre-trained latent space, we follow the convention to optimize the molecules based on the released pre-trained JTVAE model.To accomplish the K-shot optimization, we leveraged the K modification cases as the training samples and used the trained JTVAE to generate the new molecules for evaluation.The evaluation procedures of VJTNN and MoLeR are similar to JTVAE except for the exclusion of the pre-trained model.The random generation model is implemented by random sampling based on the latent space of the pre-trained JTVAE.</p>
<p>We further incorporated advanced large language models as benchmarks, including Chat-GLM, ChatGPT, and GPT-4.These models require carefully constructed prompts to generate relevant output data.In crafting these prompts, we integrated the characteristic descriptions of the molecules to be optimized and instances of a few-shot optimization to ensure that the model can accurately understand the task requirements.For ChatGLM, we used the officially released code and pre-trained models, deployed and executed in a local server environment, to obtain the model prediction results.In contrast, for ChatGPT and GPT-4, we utilized their online service capabilities by transmitting the processed input data through their official API interfaces, thereby eliciting the corresponding model outputs from remote servers.</p>
<p>Figure 1 :
1
Figure 1: Schematic overview of the DrugLLM framework.A, Construction process.Group-based Molecular Representation (GMR) is constructed from molecular structure units.B, Training framework.DrugLLM is trained on molecular modifications, with each paragraph representing a unique attribute.Each paragraph is self-contained and represents multiple characteristics, with different paragraphs corresponding to different attributes.</p>
<p>Figure 2 :
2
Figure 2: Visualization of few-shot molecule optimization.A, The training and testing examples of fewshot molecule optimization.B, Chemical space navigation by transfer learning.The UMAP plot shows the distribution of 15000 molecules selected from the source space and their corresponding 15000 molecules in the generated space after LogP property optimization.Different values of LogP are represented by different colors.</p>
<p>Figure 3 :
3
Figure 3: The performance of few-shot molecule optimization in physiochemical properties.A, The distribution of the water-octanol partition coefficient (LogP), Solubility, Synthetic Accessibility, and Topological Polar Surface Area (TPSA) for the source data and the generated data, represented by Kernel Density Estimation (KDE).The KDE demonstrations of the source data and the generated data are displayed by blue solid lines and red solid lines, respectively.Histograms (hist) for the source data and the generated data are represented by blue bars and red bars, respectively.B, The performance of the generation methods in terms of the success rates and the generation similarities.</p>
<p>Table 1 :
1
Pre-training data of DrugLLM.The training data include ZINC and ChEMBL databases.Similar molecules are collected together to build the modification paragraphs.A modification paragraph contains multiple molecule modifications that aim to improve or decrease the same molecular properties.TPSA).Considering that the few-shot learning capability of machine learning models arises from their exposure to a sufficient variety of training tasks, a large scale of different paragraphs could enforce DrugLLM captures the intrinsic nature of molecule design in a few-shot fashion.
Dataset# of molecules # of paragraphs # of tasks Disk sizeZINC4.5M0.6M770780MChEMBL180.2M24M1010030Gpolar surface area (</p>
<p>Table 2 :
2
The performance of few-shot molecule optimization toward biological activities
Target</p>
<p>Table 3 :
3
The success rates (%) of individual methods in zero-shot molecule optimization.
MethodChatGLM ChatGPT GPT-4 DrugLLMQED &amp; FractionCSP30.020.110.200.40QED &amp; TPSA0.030.150.200.47QED &amp; # Rotatable bonds0.060.150.100.59LogP &amp; FractionCSP30.030.300.400.60LogP &amp; TPSA0.040.190.430.55LogP &amp; # Rotatable bonds0.040.190.050.61</p>
<p>← moleculeAtom + "/" + groupAtom + group
Algorithm 1 Molecular encoding algorithm1: procedure ENCODEMOLECULE(molecule)2:unprocessedMolecularGroups ← splitIntoGroups(molecule)3:groupConnections ← []4:while len(unprocessedMolecularGroups) &gt; 1 do5:remainingGroups ← ∅6:for currentGroup ∈ unprocessedMolecularGroups do7:molecule, isProcessed ← ProcessGroup(molecule, currentGroup, groupConnections)8:if not isProcessed then9:remainingGroups ← remainingGroups ∪ {currentGroup}10:end if11:end for12:unprocessedMolecularGroups ← remainingGroups13:end while14:initialGroup ← first(unprocessedMolecularGroups)15:moleculeEncoding ← getDictionaryString(initialGroup)16:for all connectionItem ∈ groupConnections do17:moleculeAtom ← connectionItem[molConnectedAtom]18:groupAtom ← connectionItem[groupConnectedAtom]19:group ← getDictionaryString(connectionItem[group])20:21:moleculeEncoding ← moleculeEncoding + connectionEncoding22:end for23:return moleculeEncoding24: end procedure
connectionEncoding</p>
<p>ACKNOWLEDGMENTSThis work was supported by the National Natural Science Foundation of China (No. 62206192); the Natural Science Foundation of Sichuan Province (No. 2023NS-36 FSC1408); the Science and Technology Major Project of Sichuan Province; and the Fundamental Research Funds for the Central Universities (No. 1082204112364).
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Estimating the cost of new drug development: is it really $802 million?. C P Adams, V V Brantner, Health Aff (Millwood). 2522006</p>
<p>Ligand-based virtual screening procedure for the prediction and the identification of novel β-amyloid aggregation inhibitors using kohonen maps and counterpropagation artificial neural networks. A Afantitis, G Melagraki, P A Koutentis, H Sarimveis, G Kollias, Eur. J. Med. Chem. 4622011</p>
<p>Few-shot training llms for project-specific code-summarization. T Ahmed, P Devanbu, Proc. IEEE/ACM Int. Conf. Autom. Softw. Eng. 2022</p>
<p>Application of generative autoencoder in de novo molecular design. T Blaschke, M Olivecrona, O Engkvist, J Bajorath, H Chen, Mol Inform. 371-217001232018</p>
<p>): trends and future directions. D G Brown, H J Wobst, J. Med. Chem. 6452021. 2010-2019A decade of fda-approved drugs</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Adv. Condens. Matter Phys. 332020</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Trans Intell Syst Technol. 1532024</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023</p>
<p>Screening ultra-large virtual libraries. S Crunkhorn, Nat. Rev. Drug Discov. 21952022</p>
<p>M Davies, M Nowotka, G Papadatos, N Dedman, A Gaulton, F Atkinson, L Bellis, J P Overington, Chembl web services: streamlining access to drug discovery data and utilities. 201543</p>
<p>The cost of new drug discovery and development. M Dickson, J P Gagnon, Discov Med. 4222009</p>
<p>Sprout, hippo and caesa: Tools for de novo structure generation and estimation of synthetic accessibility. V J Gillet, G Myatt, Z Zsoldos, A P Johnson, Perspect. Drug Discovery Des. 31995</p>
<p>Junction tree variational autoencoder for molecular graph generation, ICML. W Jin, R Barzilay, T Jaakkola, 2018</p>
<p>Hierarchical generation of molecular graphs using structural motifs, ICML. W Jin, R Barzilay, T Jaakkola, 2020</p>
<p>Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds. M Korshunova, N Huang, S Capuzzi, Commun Chem. 51292022</p>
<p>Small molecule approaches to targeting rna. S Kovachka, M Panosetti, B Grimaldi, S Azoulay, A Di Giorgio, M Duca, Nat. Rev. Chem. 2024</p>
<p>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins. Y Li, C Gao, X Song, X Wang, Y Xu, S Han, 2023</p>
<p>Multi-objective de novo drug design with conditional graph generative model. Y Li, L Zhang, Z Liu, J. Cheminformatics. 102018</p>
<p>A chance-constrained generative framework for sequence optimization, ICML. X Liu, Q Liu, S Song, J Peng, 2020</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Brief. Bioinformatics. 2364092022</p>
<p>Modeling the expansion of virtual screening libraries. J Lyu, J Irwin, B Shoichet, Nat Chem Biol. 192023</p>
<p>Few-shot learning creates predictive models of drug response that translate from high-throughput screens to individual patients. J Ma, S H Fong, Y Luo, C J Bakkenist, J P Shen, S Mourragui, L F Wessels, M Hafner, R Sharan, J Peng, Nat Cancer. 222021</p>
<p>Alpaca: Intermittent execution without checkpoints. K Maeng, A Colin, B Lucia, PACMPL. 12017</p>
<p>K Maziarz, H Jackson-Flux, P Cameron, arXiv:2103.03864Learning to extend molecular scaffolds with structural motifs. 2021arXiv preprint</p>
<p>Chembl: towards direct deposition of bioassay data. D Mendez, A Gaulton, A P Bento, J Chambers, M De Veij, E Félix, M P Magariños, J F Mosquera, P Mutowo, M Nowotka, Nucleic Acids Res. 47D12019</p>
<p>Fda approvals in 2023: biomarker-positive subsets, equipoise and verification of benefit. K J Norsworthy, R J Lee-Alonzo, R Pazdur, Nat Rev Clin Oncol. 2024</p>
<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells. F Offensperger, G Tin, M Duran-Frigola, E Hahn, S Dobner, C W Ende, J W Strohbach, A Rukavina, V Brennsteiner, K Ogilvie, Science. 384669458642024</p>
<p>Towards a universal smiles representation-a standard method to generate canonical smiles based on the inchi. N M O'boyle, J. Cheminformatics. 42012</p>
<p>Deep reinforcement learning for de novo drug design. M Popova, Sci. Adv. 478852018</p>
<p>Reinforced adversarial neural computer for de novo molecular design. E Putin, A Asadulaev, Y Ivanenkov, V Aladinskiy, B Sanchez-Lengeling, A Aspuru-Guzik, A Zhavoronkov, J Chem Inf Model. 5862018</p>
<p>A small-molecule tnik inhibitor targets fibrosis in preclinical and clinical models. F Ren, A Aliper, J Chen, H Zhao, S Rao, C Kuppe, I V Ozerov, M Zhang, K Witte, C Kruse, Nature Biotechnology. 2024</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nature Machine Intelligence. 4122022</p>
<p>Synthon-based ligand discovery in virtual libraries of over 11 billion compounds. A A Sadybekov, A V Sadybekov, Y Liu, C Iliopoulos-Tsoutsouvas, X.-P Huang, J Pickett, B Houser, N Patel, N K Tran, F Tong, Nature. 60178932022</p>
<p>Small molecules, big targets: drug discovery faces the protein-protein interaction challenge. D E Scott, A R Bayly, C Abell, J Skidmore, Nat. Rev. Drug Discov. 1582016</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. M H Segler, T Kogej, C Tyrchan, M P Waller, ACS Cent. Sci. 412018</p>
<p>R Sennrich, B Haddow, A Birch, Neural machine translation of rare words with subword units, ACL. 2016</p>
<p>FS-mol: A few-shot learning dataset of molecules. M Stanley, J F Bronskill, K Maziarz, H Misztela, J Lanini, M Segler, N Schneider, M Brockschmidt, 2021NeurIPS</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Integrating qsar modelling and deep learning in drug discovery: the emergence of deep qsar. A Tropsha, O Isayev, A Varnek, G Schneider, A Cherkasov, Nature Reviews Drug Discovery. 2322024</p>
<p>The evolution of commercial drug delivery technologies. A Vargason, A Anselmo, S Mitragotri, Nat Biomed Eng. 52021</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Adv. Condens. Matter Phys. 302017</p>
<p>Multitask joint strategies of self-supervised representation learning on biomedical networks for drug discovery. X Wang, Y Cheng, Y Yang, Nat Mach Intell. 52023</p>
<p>Pro_ligand: an approach to de novo molecular design. 2. design of novel molecules from molecular field analysis (mfa) models and pharmacophores. B Waszkowycz, D E Clark, D Frenkel, J Li, C W Murray, B Robson, D R Westhead, J. Med. Chem. 37231994</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chem. Sci. 922018</p>
<p>A Zeng, X Liu, Z Du, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprint</p>
<p>Human-level few-shot concept induction through minimax entropy learning. C Zhang, B Jia, Y Zhu, S.-C Zhu, Sci. Adv. 101624882024</p>            </div>
        </div>

    </div>
</body>
</html>
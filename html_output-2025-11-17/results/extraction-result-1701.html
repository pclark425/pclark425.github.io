<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1701 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1701</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1701</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-274788497</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.11948v3.pdf" target="_blank">OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews</a></p>
                <p><strong>Paper Abstract:</strong> We present OpenReviewer, an open-source system for generating high-quality peer reviews of machine learning and AI conference papers. At its core is Llama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned on 79,000 expert reviews from top conferences. Given a PDF paper submission and review template as input, OpenReviewer extracts the full text, including technical content like equations and tables, and generates a structured review following conference-specific guidelines. Our evaluation on 400 test papers shows that OpenReviewer produces considerably more critical and realistic reviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other LLMs tend toward overly positive assessments, OpenReviewer's recommendations closely match the distribution of human reviewer ratings. The system provides authors with rapid, constructive feedback to improve their manuscripts before submission, though it is not intended to replace human peer review. OpenReviewer is available as an online demo and open-source tool.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1701.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1701.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecommendationMatching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matching generated recommendation scores to human reviewer recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct numeric comparison between generated review recommendation scores (normalized to a 1–10 scale) and human reviewer recommendations using Exact Match frequency and average absolute error as agreement metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Likert-style rating comparison (numeric recommendation normalized to 1--10) with Exact Match and average absolute error</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>peer review recommendation (numerical rating in a review)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>ML/AI conference papers (ICLR, NeurIPS)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>overall recommendation score (accept/reject continuum normalized to 1–10)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Used existing human reviewer recommendations from conference reviews (treating them as ground truth); compared each generated recommendation to (a) whether it exactly matched any human reviewer's recommendation and (b) the absolute difference to the human reviewers' mean recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Conference peer reviewers from ICLR and NeurIPS (expert reviewers)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Exact Match percentage (EM %) and average absolute error (mean absolute difference to human reviewers' average recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>OpenReviewer EM=55.5%; Avg abs error=0.96 ± 0.85. Baselines: GPT-4o EM=23.8%, error=2.34 ± 1.17; Claude-3.5-Sonnet EM=15.5%, error=2.77 ± 1.27; Llama-3.1-8B-Instruct EM=14.0%, error=2.95 ± 1.19; Llama-3.1-70B-Instruct EM=11.5%, error=3.03 ± 1.34.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Model specialized and fine-tuned on large corpus of expert reviews and trained to follow review templates; structured, template-guided review generation; producing more critical (less positively biased) recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>General-purpose LLMs that tend to produce overly positive recommendations (positivity bias), lack of specialization on peer-review data, and mismatch in calibration of rating scales.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not reported for recommendation matching specifically (paper does not analyze dependence on paper complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Using standardized review templates and a normalized rating scale (1–10) is associated with better alignment; authors state template-guided review generation contributed to improved recommendation alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>400</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Direct proxy-to-human comparison performed (EM and avg error); no numeric comparison reported between LLM-human agreement and inter-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Yes — OpenReviewer (Llama-OpenReviewer-8B) was fully fine-tuned on ~79K high-confidence expert reviews from ICLR and NeurIPS and trained to follow review templates; other baseline LLMs were not fine-tuned on this review dataset for this study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>OpenReviewer (specialized, fine-tuned) matches human reviewer recommendations far better than general-purpose LLMs: EM 55.5% and avg error ~0.96 vs. baseline EMs 11.5–23.8% and errors 2.34–3.03. Baseline LLMs show a systematic positivity bias (higher average recommendation scores) which reduces alignment with human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Evaluation assumes closeness to human reviews equals quality (acknowledged as imperfect); dataset limited to ICLR/NeurIPS (2022+); human reviewer counts per paper not specified; test set limited to 400 papers; partial non-anonymized papers risk leakage; calibration limited to the fine-tuned model only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1701.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1701.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge pairwise preference evaluation using GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pairwise preference evaluation where an LLM (GPT-4o) acts as an automatic judge: given expert reviews and two candidate generated reviews (A and B), the judge compares alignment section-by-section and decides which generated review aligns better with expert reviews (A, B, or Tie).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (pairwise preference comparison between generated reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o (2024-11-20)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Pairwise comparison prompt: provide n expert reviews and two candidate reviews; instruct judge to evaluate how well each section of A and B matches expert reviews (excluding summary) and decide A/B/Tie; prompts and example outputs provided in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated peer reviews (full textual reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>ML/AI conference papers (ICLR, NeurIPS)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>section-wise alignment with human expert reviews (soundness, presentation, contribution, strengths, weaknesses, questions, numerical ratings), plus overall decision between A, B, or Tie</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No fresh human preference judgments were solicited for the pairwise arena; instead human expert reviews served as ground truth and GPT-4o judged which of two generated reviews aligned better with the human reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>ICLR/NeurIPS conference peer reviewers (expert reviewers) whose reviews were used as ground truth in the comparison</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Win rate (percentage of pairwise comparisons where a model's review was judged better than the baseline) as reported by the LLM judge</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>OpenReviewer win rates reported in figure: OpenReviewer wins against other LLMs with win rates ranging from ~60% (vs GPT-4o) to ~76% (vs Llama-3.1-70B-Instruct); exact per-baseline percentages provided in the paper's figure.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Specialized fine-tuning on peer-review data and adherence to structured review templates; reviews that are more critical and that align with human reviewer style and structure are favored by the LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Baseline general-purpose LLMs producing overly positive or less-structured reviews lead to lower judged alignment; lack of template conformity or missing section-level correspondence reduces judged alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not reported — the paper does not break down win rates by paper complexity or topic complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Structured prompts instructing section-wise comparison and provision of fixed review templates aids the LLM judge in making consistent decisions; the paper emphasizes that template-guided reviews improve judged alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>400</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>The LLM judge compares generated reviews against human reviews (proxy vs. human) indirectly; paper does not provide a numeric comparison of LLM-human agreement versus inter-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>The judge model (GPT-4o) was not reported as being calibrated on these human judgments; OpenReviewer (the evaluated proxy) was fine-tuned on ~79K expert reviews. The LLM judge was used off-the-shelf with carefully designed prompts (prompts shown in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using GPT-4o as an automatic judge, OpenReviewer is judged to produce reviews that align better with human expert reviews than baseline LLMs, with win rates of roughly 60%–76% across baselines; structured section-wise judging and template-following generation are important contributors to this outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM-as-a-judge results substitute for expensive human preference studies but are themselves imperfect — the paper notes human judgments would be more meaningful but are costly; using an LLM judge may inherit judge biases and is not equivalent to human evaluation; prompts and judge model choice can affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Alpacaeval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
                <li>Is LLMa reliable reviewer? a comprehensive evaluation ofLLM on automatic paper reviewing tasks <em>(Rating: 2)</em></li>
                <li>Can large language models provide useful feedback on research papers? A large-scale empirical analysis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1701",
    "paper_id": "paper-274788497",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "RecommendationMatching",
            "name_full": "Matching generated recommendation scores to human reviewer recommendations",
            "brief_description": "Direct numeric comparison between generated review recommendation scores (normalized to a 1–10 scale) and human reviewer recommendations using Exact Match frequency and average absolute error as agreement metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Likert-style rating comparison (numeric recommendation normalized to 1--10) with Exact Match and average absolute error",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "peer review recommendation (numerical rating in a review)",
            "artifact_domain": "ML/AI conference papers (ICLR, NeurIPS)",
            "evaluation_criteria": "overall recommendation score (accept/reject continuum normalized to 1–10)",
            "human_evaluation_setup": "Used existing human reviewer recommendations from conference reviews (treating them as ground truth); compared each generated recommendation to (a) whether it exactly matched any human reviewer's recommendation and (b) the absolute difference to the human reviewers' mean recommendation.",
            "human_expert_count": null,
            "human_expert_expertise": "Conference peer reviewers from ICLR and NeurIPS (expert reviewers)",
            "agreement_metric": "Exact Match percentage (EM %) and average absolute error (mean absolute difference to human reviewers' average recommendation)",
            "agreement_score": "OpenReviewer EM=55.5%; Avg abs error=0.96 ± 0.85. Baselines: GPT-4o EM=23.8%, error=2.34 ± 1.17; Claude-3.5-Sonnet EM=15.5%, error=2.77 ± 1.27; Llama-3.1-8B-Instruct EM=14.0%, error=2.95 ± 1.19; Llama-3.1-70B-Instruct EM=11.5%, error=3.03 ± 1.34.",
            "high_agreement_conditions": "Model specialized and fine-tuned on large corpus of expert reviews and trained to follow review templates; structured, template-guided review generation; producing more critical (less positively biased) recommendations.",
            "low_agreement_conditions": "General-purpose LLMs that tend to produce overly positive recommendations (positivity bias), lack of specialization on peer-review data, and mismatch in calibration of rating scales.",
            "artifact_complexity_effect": "Not reported for recommendation matching specifically (paper does not analyze dependence on paper complexity).",
            "criteria_clarity_effect": "Using standardized review templates and a normalized rating scale (1–10) is associated with better alignment; authors state template-guided review generation contributed to improved recommendation alignment.",
            "sample_size": "400",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Direct proxy-to-human comparison performed (EM and avg error); no numeric comparison reported between LLM-human agreement and inter-human agreement.",
            "calibration_or_training": "Yes — OpenReviewer (Llama-OpenReviewer-8B) was fully fine-tuned on ~79K high-confidence expert reviews from ICLR and NeurIPS and trained to follow review templates; other baseline LLMs were not fine-tuned on this review dataset for this study.",
            "key_findings": "OpenReviewer (specialized, fine-tuned) matches human reviewer recommendations far better than general-purpose LLMs: EM 55.5% and avg error ~0.96 vs. baseline EMs 11.5–23.8% and errors 2.34–3.03. Baseline LLMs show a systematic positivity bias (higher average recommendation scores) which reduces alignment with human reviewers.",
            "limitations_noted": "Evaluation assumes closeness to human reviews equals quality (acknowledged as imperfect); dataset limited to ICLR/NeurIPS (2022+); human reviewer counts per paper not specified; test set limited to 400 papers; partial non-anonymized papers risk leakage; calibration limited to the fine-tuned model only.",
            "uuid": "e1701.0",
            "source_info": {
                "paper_title": "OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "LLM-as-a-judge pairwise preference evaluation using GPT-4o",
            "brief_description": "A pairwise preference evaluation where an LLM (GPT-4o) acts as an automatic judge: given expert reviews and two candidate generated reviews (A and B), the judge compares alignment section-by-section and decides which generated review aligns better with expert reviews (A, B, or Tie).",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (pairwise preference comparison between generated reviews)",
            "llm_judge_model": "GPT-4o (2024-11-20)",
            "llm_judge_prompt_approach": "Pairwise comparison prompt: provide n expert reviews and two candidate reviews; instruct judge to evaluate how well each section of A and B matches expert reviews (excluding summary) and decide A/B/Tie; prompts and example outputs provided in appendix.",
            "artifact_type": "generated peer reviews (full textual reviews)",
            "artifact_domain": "ML/AI conference papers (ICLR, NeurIPS)",
            "evaluation_criteria": "section-wise alignment with human expert reviews (soundness, presentation, contribution, strengths, weaknesses, questions, numerical ratings), plus overall decision between A, B, or Tie",
            "human_evaluation_setup": "No fresh human preference judgments were solicited for the pairwise arena; instead human expert reviews served as ground truth and GPT-4o judged which of two generated reviews aligned better with the human reviews.",
            "human_expert_count": null,
            "human_expert_expertise": "ICLR/NeurIPS conference peer reviewers (expert reviewers) whose reviews were used as ground truth in the comparison",
            "agreement_metric": "Win rate (percentage of pairwise comparisons where a model's review was judged better than the baseline) as reported by the LLM judge",
            "agreement_score": "OpenReviewer win rates reported in figure: OpenReviewer wins against other LLMs with win rates ranging from ~60% (vs GPT-4o) to ~76% (vs Llama-3.1-70B-Instruct); exact per-baseline percentages provided in the paper's figure.",
            "high_agreement_conditions": "Specialized fine-tuning on peer-review data and adherence to structured review templates; reviews that are more critical and that align with human reviewer style and structure are favored by the LLM judge.",
            "low_agreement_conditions": "Baseline general-purpose LLMs producing overly positive or less-structured reviews lead to lower judged alignment; lack of template conformity or missing section-level correspondence reduces judged alignment.",
            "artifact_complexity_effect": "Not reported — the paper does not break down win rates by paper complexity or topic complexity.",
            "criteria_clarity_effect": "Structured prompts instructing section-wise comparison and provision of fixed review templates aids the LLM judge in making consistent decisions; the paper emphasizes that template-guided reviews improve judged alignment.",
            "sample_size": "400",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "The LLM judge compares generated reviews against human reviews (proxy vs. human) indirectly; paper does not provide a numeric comparison of LLM-human agreement versus inter-human agreement.",
            "calibration_or_training": "The judge model (GPT-4o) was not reported as being calibrated on these human judgments; OpenReviewer (the evaluated proxy) was fine-tuned on ~79K expert reviews. The LLM judge was used off-the-shelf with carefully designed prompts (prompts shown in appendix).",
            "key_findings": "Using GPT-4o as an automatic judge, OpenReviewer is judged to produce reviews that align better with human expert reviews than baseline LLMs, with win rates of roughly 60%–76% across baselines; structured section-wise judging and template-following generation are important contributors to this outcome.",
            "limitations_noted": "LLM-as-a-judge results substitute for expensive human preference studies but are themselves imperfect — the paper notes human judgments would be more meaningful but are costly; using an LLM judge may inherit judge biases and is not equivalent to human evaluation; prompts and judge model choice can affect outcomes.",
            "uuid": "e1701.1",
            "source_info": {
                "paper_title": "OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Alpacaeval: An automatic evaluator of instruction-following models",
            "rating": 2,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Is LLMa reliable reviewer? a comprehensive evaluation ofLLM on automatic paper reviewing tasks",
            "rating": 2,
            "sanitized_title": "is_llma_reliable_reviewer_a_comprehensive_evaluation_ofllm_on_automatic_paper_reviewing_tasks"
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
            "rating": 1,
            "sanitized_title": "can_large_language_models_provide_useful_feedback_on_research_papers_a_largescale_empirical_analysis"
        }
    ],
    "cost": 0.0115515,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews
18 Mar 2025</p>
<p>Maximilian Idahl idahl@l3s.de 
L3S Research Center Leibniz University Hannover</p>
<p>Zahra Ahmadi ahmadi.zahra@mh-hannover.de 
PLRI Medical Informatics Institute Hannover Medical School</p>
<p>OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews
18 Mar 2025EA8D4804988ADBE713C9F47DCE2BE360arXiv:2412.11948v3[cs.AI]
We present OpenReviewer, an open-source system for generating high-quality peer reviews of machine learning and AI conference papers.At its core is Llama-OpenReviewer-8B 1 , an 8B parameter language model specifically fine-tuned on 79,000 expert reviews from top conferences.Given a PDF paper submission and review template as input, OpenReviewer extracts the full text, including technical content like equations and tables, and generates a structured review following conference-specific guidelines.Our evaluation on 400 test papers shows that Open-Reviewer produces considerably more critical and realistic reviews compared to generalpurpose LLMs like GPT-4 and Claude-3.5.While other LLMs tend toward overly positive assessments, OpenReviewer's recommendations closely match the distribution of human reviewer ratings.The system provides authors with rapid, constructive feedback to improve their manuscripts before submission, though it is not intended to replace human peer review.OpenReviewer is available as an online demo 2 and open-source tool.</p>
<p>Introduction</p>
<p>The peer review process is fundamental to maintaining scientific rigor in academic research, particularly in fast-moving fields like machine learning (ML) and artificial intelligence (AI).As submission volumes to major conferences continue to surgewith top venues receiving over 10,000 submissions annually -the traditional peer review system faces challenges.The task load for reviewers consistently increases, while authors lack access to preliminary expert feedback with a quick turnaround that could help improve their work before submission.</p>
<p>Large language models (LLMs) have recently demonstrated remarkable capabilities in under-standing and generating academic content, suggesting their potential to assist in peer review (Kuznetsov et al., 2024).However, generating high-quality reviews presents unique challenges beyond general language understanding.A good reviewer has to comprehend technical content, including mathematics and empirical results, and evaluate the work's contributions, novelty, significance, and methodological soundness according to high, field-specific standards.</p>
<p>In this paper, we present OpenReviewer, an opensource system designed to generate human-like, high-quality reviews of machine learning and AI papers.At its core is Llama-OpenReviewer-8B, a specialized language model fine-tuned on a curated dataset of 79,000 reviews from top ML conferences.Unlike general-purpose LLMs, OpenReviewer is trained to follow standard review templates and guidelines, ensuring structured, critical feedback that aligns with conference reviewing practices.</p>
<p>Our main contributions are:</p>
<p>• A specialized long-context large language model for generating academic reviews, finetuned on a large dataset of expert reviews from top ML conferences.</p>
<p>• An open-source demo that combines stateof-the-art transformer-based PDF processing with our specialized model to generate comprehensive reviews from paper submissions.</p>
<p>• An evaluation demonstrating that OpenReviewer generates reviews that align considerably better with human expert reviews compared to state-of-the-art LLMs, including GPT-4o and Claude-3.5-Sonnet.We find that general-purpose LLMs are not critical enough and tend to give much more positive recommendations than human reviews.</p>
<p>While OpenReviewer is not intended to replace human peer reviews, it provides authors with a valuable tool for obtaining rapid, structured feedback before submission.Our evaluation shows that OpenReviewer's reviews closely match human reviewer judgments, suggesting its potential to help authors identify and address weaknesses in their manuscripts in the writing process.</p>
<p>Motivation</p>
<p>The motivation behind OpenReviewer is not to replace human peer reviews.Instead, we want to assist authors who face challenges in the presubmission phase.Without access to expert feedback before submission, they may overlook significant weaknesses in their manuscripts or fail to address potential reviewer concerns.This can result in unnecessary desk rejections or negative reviews that could have been avoided with earlier feedback.</p>
<p>While recent advances in large language models have shown promise in various tasks related to academic research, including paper summarization, understanding, and analysis, existing models often struggle to generate reviews that match the depth, specificity, critical perspective, and structure expected in academic peer review.General purpose LLMs may miss field-specific conventions, fail to properly evaluate technical contributions, or provide feedback that does not align with established reviewing practices.OpenReviewer addresses this gap, aiming to provide authors with valuable presubmission feedback that closely mirrors the standards and expectations of human peer reviewers.</p>
<p>OpenReviewer</p>
<p>Demo Interface</p>
<p>We host a demo of OpenReviewer on HuggingFace Spaces 3 .The interface, depicted in Figure 1, is built with the Gradio (Abid et al., 2019) library.It starts with a short description and some guideline text to help users navigate the application.The user first faces a file-upload dialogue, where they can upload a PDF file.Uploading a file automatically triggers a PDF to markdown conversion process.This process takes some time, as OpenReviewer uses transformer-based PDF processing models that run on a GPU.Once the markdown conversion finishes, the markdown paper text will be displayed in a corresponding text area.The user can edit the text to fix any conversion errors.Alternatively, if the user already has a markdown representation of their paper, they can avoid the markdown conversion by 3 huggingface.co/spacesdirectly pasting it into this text area.Below the text area for the paper text, there is an accordion element that optionally shows an editable review template.The reviews generated by OpenReviewer follow this template, and the user can choose to deviate from the default template if they desire a review with different sections, aspects, or rating scales.Once the markdown paper text field is populated, the user can click a "Generate Review" button to run Llama-OpenReviewer-8B.When clicked, a review is generated in streaming mode and printed on the fly, token-by-token, below the button.</p>
<p>The demo uses Huggingface Spaces ZeroGPU hardware4 , which allocates and releases GPU resources dynamically.While this lets us host the demo free of charge up to a particular usage quota, it can negatively impact its speed and snappiness.However, users can clone and run the application on their hardware at any time if desired.</p>
<p>Llama-OpenReviewer-8B</p>
<p>The core component powering OpenReviewer is Llama-OpenReviewer-8B, a large language model finetuned on a large dataset of paper-review pairs.This section describes the data collection, prompt design, and training details.</p>
<p>Training Data</p>
<p>From OpenReview 5 , we collected a dataset of 36K submitted papers and 141K reviews from the International Conference on Learning Representations (ICLR) and the Conference on Neural Information Processing Systems (NeurIPS), considering editions from 2022 onwards.We obtain each paper in PDF format by downloading the earliest revision possible.Later revisions are typically cameraready versions that already incorporate feedback from the reviews, rendering at least some parts of the reviews invalid.Unfortunately, the original double-blind submissions are no longer available for some venues; we obtain the non-anonymized version.We convert all papers from PDF to markdown using Marker 6 .This open-source PDF processing pipeline combines heuristics with multiple transformer models to extract text, apply optical character recognition (OCR) when necessary, detect page layouts, determine the reading order, clean and format text blocks, combine blocks of text, and post-process complete text.We chose 1 2 3 4</p>
<p>Figure 1: Annotated screenshot of the OpenReviewer demo hosted on Huggingface Spaces, with slightly modified layout.1) Dialogue for uploading a PDF file.2) Once the user uploads a file, this text field will be populated with the papers' full text in markdown format.The user can choose to edit the text to fix conversion errors.3) An accordion element to show and optionally edit the review template used for generation.4) Button to run the review generation and enabled once the paper text field is populated.When clicked, a review is generated in streaming mode and printed below on the fly.</p>
<p>Marker because its per-page accuracy improves upon Nougat (Blecher et al., 2024).It can also convert equations and tables accurately, which we deem essential for scientific papers.We discard any appendix content and only retain the full text of the main and reference sections.We filter papers and reviews by length, removing the top and bottom 1% quantile.Finally, we keep only high-confidence reviews.For each venue, we select a reviewer confidence threshold roughly equal to "Confident, but not absolutely certain".After filtering, approximately 79K reviews are remaining.</p>
<p>Prompt Design</p>
<p>OpenReviewer uses a system prompt that conditions the LLM on its reviewer role and defines a fixed set of reviewer guidelines inspired by the ICLR 2024 reviewer guide 7 .The system prompt specifies that the review must be written in markdown format and follow a specific review template, which is part of the input and differs across venues.The user prompt is minimalistic and only contains "Review the following paper:" followed by the full paper text.The verbatim prompts are shown in 7 iclr.cc/Conferences/2024/ReviewerGuideSection A.1.</p>
<p>Training</p>
<p>We full finetune Llama-3.1-8B-Instruct for three epochs with an effective batch size of 64 and a learning rate of 2 × 10 −5 , using bfloat16 precision.The maximum sequence length during finetuning is 128k tokens to accommodate long paper texts.Model training used the axolotl8 library using Deepspeed ZeRO-3 (Rajbhandari et al., 2020) for parallelization and Flash Attention V2 (Dao, 2024) and LIGER Kernel (Hsu et al., 2024) to reduce memory usage and increase throughput.The training process took approximately 34 hours using 64 NVIDIA A100 80GB GPUs.Refer to Section A.3 for all training hyperparameters.</p>
<p>Evaluation</p>
<p>Evaluating automatically generated paper reviews is challenging.There is no single objective measure of review quality since even human experts often disagree in their assessments.Additionally, evaluating free-form generated text presents a challenge because there are many valid ways to express the same content.Unlike tasks with clear right or wrong answers, free-form text can vary greatly in style, structure, word choice, and level of detail while still being equally valid or effective.For example, two reviewers could make the same core criticism about a paper's methodology but express it differently.One reviewer might be more direct and concise, while another could be more elaborate with detailed examples.This makes it difficult to develop automated metrics that can reliably assess the quality of generated review text.</p>
<p>Our evaluation approach for OpenReviewer is based on comparing generated reviews to human reviews.This approach assumes that similarity to human reviews equals quality, which may not always be accurate as the quality control for humanwritten reviews is limited.</p>
<p>To measure how well OpenReviewer reviews align with reviews from human reviewers, we conduct experiments using a test set of 400 heldout papers and their reviews from NeurIPS 2024 and ICLR 2025, the most recent venues in our dataset.We compare OpenReviewer to Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct(Dubey et al., 2024), Claude-3.5-Sonnet(Oct.22) from Anthropic, and GPT-4o (2024-11-20) from Ope-nAI.We generate one review for each paper in the test set using greedy decoding (temperature of 0).All LLMs are instructed with the same system and user prompts used by OpenReviewer.We use vLLM (Kwon et al., 2023) to serve the OpenReviewer and Llama models, and access Claude-3.5-Sonnetand GPT-4o via OpenRouter 9 .</p>
<p>Matching Recommendations</p>
<p>While the sections in a review vary based on the corresponding venue review template, the recommendation is a numerical rating that consistently exists for all reviews.Additionally, the recommendation can be expected to reflect the overall sentiment and arguments expressed throughout the review.To measure how well the recommendation of a generated review matches the recommendations of the human reviewers, we check whether it exactly matches one of the human reviewers' recommendations.Additionally, we measure the average absolute distance between the generated review's recommendation and the human reviewers' average recommendations.For this, we normalize the recommendation scores to a scale from 1 9 openrouter.ai/(strong reject) to 10 (strong accept).As shown in Table 1, the recommendations by OpenReviewer match the human reviewers much better than the other LLMs across both metrics.OpenReviewer matches at least one human reviewer for 55.5% of its generated reviews and has an average recommendation error of 0.96.In contrast, GPT4o matches at least one reviewer only in 23.8% of reviews with a higher average recommendation error of 2.34.When examining why such a large gap exists between OpenReviewer and all other LLMs, we find that the other LLMs usually give positive recommendations (Table 2).While OpenReviewer matches the human reviewers with an average recommendation of 5.4 out of 10, the baseline LLMs produce average recommendations of 6.9 and higher, topped by Llama-3.1-8B-Instruct with an average recommendation of 8.1, which would lead to an "accept" for most of the papers in the test dataset.Although a positive recommendation can make authors happy, it is less desirable if the authors seek critical feedback for their paper presubmission.Recommendation scores that accurately align with human reviewer judgments provide authors with realistic expectations about their manuscript's reception and help prevent the disappointment that might follow from overly optimistic preliminary reviews.</p>
<p>Review Arena</p>
<p>We run an arena-style preference evaluation with an LLM-as-a-judge setup to measure whether Open-Reviewer produces better reviews than the other LLMs.This is similar to MT bench (Zheng et al., Model Avg.Recommendation Llama-3.1-8B-Instruct8.1 ± 1.4 Llama-3.1-70B-Instruct6.9 ± 2.8 Claude-3.5-Sonnet7.6 ± 1.7 GPT-4o (2024-11-20) 7.7 ± 0.8 OpenReviewer 5.4 ± 1.1 Human Reviewers 5.4 ± 1.2 2023) and AlpacaEval (Li et al., 2023b), which use an LLM-as-a-judge to evaluate the quality of instruction-tuned language models and chatbots.</p>
<p>While human judgments could be more meaningful, they are expensive to obtain as, in our case, they would require annotators to be trained reviewers and study each paper in great detail.</p>
<p>Given a set of human "expert" reviews and two reviews, A and B, we ask GPT-4o (2024-11-20) to determine whether review A or B aligns better with the given expert reviews.Specifically, we first ask it to consider how well each section of A and B matches the corresponding section in the expert reviews and then to decide between A, B, or Tie.Refer to Section A.1 for the exact prompts and to Section A.2 for an example output of the LLM judge.Figure 2 visualizes the win rates of OpenReviewer against the other LLMs.According to the LLM judge, OpenReviewer wins against other LLMs for most papers, achieving win rates ranging from 60% against GPT4o to 76% against Llama-3.1-70B-Instruct.</p>
<p>0%</p>
<p>25% 50% 75% 100% GPT-4o (2024-11-20) Claude-3.5-SonnetLlama-3.1-70B-Inst.</p>
<p>Llama-3. Figure 2: Preference evaluation using GPT-4o as the annotator, judging which generated review aligns better with a set of human-written reviews.</p>
<p>Discussion</p>
<p>Our results demonstrate that OpenReviewer generates reviews that align considerably better with human expert judgments than general-purpose LLMs.</p>
<p>The key findings warrant several important discussions:</p>
<p>Review Quality and Criticism Level: A striking observation is that general-purpose LLMs consistently produce overly positive reviews, with average recommendations between 6.9 and 8.1 on a 10-point scale.In contrast, OpenReviewer's average rating of 5.4 matches the human reviewer distribution exactly.This suggests that specialized training on peer review data helps overcome the tendency of LLMs to be overly favorable -a critical feature for providing constructive feedback that can help improve papers.</p>
<p>Use Cases and Limitations: While OpenReviewer shows promise as a pre-submission feedback tool, it is important to emphasize that it is not intended to replace human peer review.The system can help authors identify potential weaknesses and prepare stronger submissions, but should be used alongside other forms of feedback.Additionally, the model's training data comes primarily from ML/AI venues, potentially limiting its effectiveness for other fields.</p>
<p>Ethical Considerations: The development of automated review systems raises important questions about maintaining review quality and preventing potential misuse.There is a risk that authors might try to "gam" such systems or that conferences might be tempted to use them to replace human reviewers.Clear guidelines about appropriate use cases and limitations are essential.</p>
<p>Technical Tradeoffs: Our choice of an 8B parameter model balanced performance with accessibility and computational requirements.While larger models might achieve better results, our evaluation suggests that specialized training on peer review data could be more important than the model scale.However, we believe scaling up the LLM powering OpenReviewer will further improve the generated reviews.</p>
<p>6 Related Work</p>
<p>AI-Assisted Peer Review</p>
<p>Prior work has explored various ways NLP can support the peer review process.Early approaches focused on reviewer-paper matching using text similarity and topic modeling (Charlin and Zemel, 2013;Wieting et al., 2019).More recent work has investigated automated analysis of review quality (Yuan et al., 2022), detection of biases in peer review (Manzoor and Shah, 2021), generation of meta-reviews (Li et al., 2023a), and manuscript revision (Kuznetsov et al., 2022).Re-viewRobot (Wang et al., 2020) predicts review scores and generates comments using knowledge graphs.However, most existing systems target isolated aspects rather than providing comprehensive reviewing support.Our work builds on these efforts while introducing an end-to-end system designed explicitly for generating complete peer reviews.Kuznetsov et al. (2024) present an up-todate overview of opportunities for leveraging natural language processing across the entire peer review process.</p>
<p>Review Generation with Large Language Models</p>
<p>With the emergence of powerful LLMs, several recent studies have explored their potential for automated review generation.</p>
<p>Datasets and Resources</p>
<p>Several datasets have been developed to study peer review, including PeerRead (Kang et al., 2018), which contains reviews from machine learning conferences, and NLPeer (Dycke et al., 2023), which focuses on computational linguistics venues.MO-PRD (Lin et al., 2023b) is a multi-disciplinary dataset for peer review.Our training data incorporates a larger and more recent collection of human reviews from top ML conferences.</p>
<p>Evaluation of Review Quality</p>
<p>Measuring review quality is an ongoing challenge in the field.Prior work has proposed various metrics, including the soundness of human reviews (Shah et al., 2018), helpfulness ratings (Wang and Shah, 2019), and structured quality instruments (van Rooyen et al., 1999).Zhou et al. ( 2024) evaluate LLM-generated reviews with a questionanswering task and find that they are weak in giving critical feedback.Our evaluation is designed explicitly to assess generated reviews automatically, and we do not use humans-in-a-loop.</p>
<p>Ethics and Bias in Peer Review</p>
<p>Research has highlighted various forms of bias in peer review, including institutional bias (Tomkins et al., 2017), gender bias (Hofstra et al., 2020), and strategic manipulation (Jecmen et al., 2024).</p>
<p>While automation through NLP offers potential solutions, it also raises new ethical concerns about fairness, transparency, and accountability (Rogers and Augenstein, 2020; Ye et al., 2024;Lin et al., 2023a).</p>
<p>Conclusion</p>
<p>We presented OpenReviewer, an open-source system for generating high-quality peer reviews of ML/AI papers.Through careful fine-tuning on expert reviews and evaluation against multiple baselines, we demonstrated that OpenReviewer produces more realistic and critical reviews than general-purpose LLMs.Our key contribution is showing that specialized training on peer review data can overcome the tendency of LLMs to generate overly positive assessments.</p>
<p>Looking ahead, several promising directions emerge:</p>
<p>• Expanding training data to cover more venues and domains.</p>
<p>• Incorporating related literature search and citation graph information to improve the assessment of novelty claims.</p>
<p>• Developing better automatic evaluation metrics for review quality.</p>
<p>• Creating interfaces for collaborative human-AI reviewing.</p>
<p>While OpenReviewer shows promise as a tool for generating preliminary feedback, it should complement rather than replace human peer review.We hope this work spurs further research into AIassisted academic reviewing while maintaining high standards of scholarly assessment.</p>
<p>Limitations</p>
<p>Our study faces several key limitations across data, technical, evaluation, and practical dimensions.Regarding data, our dataset is restricted to papers from 2022 onwards from only ICLR and NeurIPS conferences within the machine learning and AI domain.However, given rapid field advances, this temporal bound helps ensure contemporary relevance.The partial use of non-anonymized papers may also introduce information leakage concerns.Technical limitations include OpenReviewer's dependence on PDF-to-markdown conversion accuracy and its relatively modest 8B parameter size compared to larger models with potentially better document understanding capabilities.Our evaluation scope is constrained by a test set of only 400 papers due to commercial LLM usage costs, focuses primarily on automated metrics rather than detailed human analysis, and compares against a limited set of baseline models due to resource constraints.Practical limitations include performance constraints from running on limited hardware and the risk of giving authors false confidence through automated feedback alone.Additional challenges involve the bias toward certain conferences and review templates, the limited domain focus on machine learning and AI, the handling of figures and images, and the incorporation of relevant background knowledge from references -areas we continue to work on improving.</p>
<p>Ethics and Broader Impact Statement</p>
<p>OpenReviewer raises several important ethical considerations that warrant careful discussion.While our demo aims to assist authors with presubmission feedback, it could potentially be misused to automate the peer review process entirely, compromising scientific rigor.We strongly empha-size that OpenReviewer is designed to complement, not replace, human peer review.</p>
<p>There are also concerns about fairness and bias.Our training data comes primarily from top ML/AI conferences, which may encode existing biases in the field regarding what constitutes "good research".This could disadvantage work from underrepresented perspectives or methodological approaches.Additionally, researchers with access to better computational resources might gain an unfair advantage in preparing submissions.</p>
<p>On the positive side, OpenReviewer could democratize access to high-quality feedback, particularly benefiting researchers from institutions without large peer networks or those in resourceconstrained environments.Early feedback could help authors improve their work before submission, potentially leading to higher-quality publications and more efficient use of human reviewer time.</p>
<p>To promote responsible use, we:</p>
<p>• Explicitly state the system's limitations in the documentation.</p>
<p>• Make OpenReviewer and the LLM powering it openly available.</p>
<p>• Include clear disclaimers about appropriate use cases.</p>
<p>• Encourage further research into bias detection and mitigation.</p>
<p>We call on the research community to carefully consider the implications of automated review systems and work together to establish guidelines for their ethical deployment.</p>
<p>A Appendix</p>
<p>A.1 Prompts</p>
<p>The prompts used by OpenReviewer are shown in Figures 3 and 4. The prompts used for GPT-4o as a judge are shown in Figures 5 and 6.</p>
<p>A.2 Example Outputs</p>
<p>Figure 7 shows an example output of GPT-4o as a judge, comparing two reviews.</p>
<p>A.3 Training Hyperparameters</p>
<p>Figure 8 shows all training hyperparameters used to train Llama-OpenReviewer-8B with the axolotl library.</p>
<p>You are an expert reviewer for AI conferences.You follow best practices and review papers according to the reviewer guidelines.</p>
<p>Reviewer  You are an expert meta-reviewer for an AI conference.You will be provided with {n_expert_reviews} expert reviews and two additional reviews, review A and review B, all for the same paper.The expert reviews form a groundtruth of reviews.Your task is to determine whether review A or review B aligns better with the given expert reviews.All reviewers were instructed to write reviews with the following sections: {review_fields} Think about how well each section of the reviews matches the corresponding section in the expert reviews, except for the summary section.For sections requiring a numerical rating, determine how well the numerical rating matches the numerical ratings of the expert reviews.</p>
<p>All reviews are delimited with XML tags.Start your response with your thoughts about how well each section of Review A and Review B matches the corresponding section in the expert reviews.Then, provide your decision as either "Review ", "Review ", or "Ti".-Review A highlights the clarity of the paper, the theoretical rigor, and the motivation for the problem, which aligns with the expert reviews' emphasis on the novelty of the epigraph form, theoretical guarantees, and empirical results.However, Review A does not mention the gradient conflict resolution or the toy example, which were noted as strengths in the expert reviews. 5. <strong>Weaknesses</strong>: -Review A mentions the focus on tabular MDPs and computational expense, which align with the expert reviews' concerns about computational inefficiency and scalability.However, it does not address the assumptions (e.g., Assumption 2) or the need for more complex real-world applications, which were highlighted in the expert reviews.6. <strong>Questions</strong>: -The questions in Review A focus on extending the algorithm to continuous spaces, dealing with infinite uncertainty sets, and computational efficiency.These are relevant but do not fully align with the expert reviews, which also asked about the gap between transition kernels, assumptions, and scaling with state space size.7. <strong>Rating</strong>: -Rating: 6 -Matches two of the expert reviews, which rated the paper as 6.The other two expert reviews rated it as 8 and 3, so this rating is within the range of expert opinions.#### <strong>Review B</strong> 1. <strong>Soundness</strong>: -Rating: 4 -Matches one expert review that rated soundness as 4, but is higher than the other three expert reviews, which rated it as 3.This rating is slightly optimistic compared to the majority of expert reviews.2. <strong>Presentation</strong>: -Rating: 3 -Matches the expert reviews, which consistently rated presentation as 3.</p>
<ol>
<li><strong>Contribution</strong>: -Rating: 4 -Higher than the expert reviews, which consistently rated contribution as 3.This rating is more optimistic than the expert consensus.4. <strong>Strengths</strong>: -Review B emphasizes the novelty of the epigraph form, theoretical rigor, and empirical validation, which align well with the expert reviews.It also mentions the clarity in problem formulation and the generality of the approach, which are consistent with the expert reviews.However, it goes further by claiming the contribution is "substantial" and "impactful," which is more enthusiastic than the expert reviews. 5. <strong>Weaknesses</strong>: -Review B mentions the complexity of presentation, computational efficiency, limited experimental scope, and lack of comparison to alternative methods.These align partially with the expert reviews, which also noted computational inefficiency and scalability issues.However, Review B does not address the assumptions or the need for more complex real-world applications, which were key weaknesses in the expert reviews.6. <strong>Questions</strong>: -The questions in Review B focus on scalability, intuition for the epigraph form, scenarios where the Lagrangian approach might be preferable, and sensitivity to hyperparameters.These are relevant but do not fully align with the expert reviews, which also asked about assumptions, scaling with state space size, and the gap between transition kernels.7. <strong>Rating</strong>: -Rating: 8 -Matches one expert review that rated the paper as 8, but is higher than the other three expert reviews, which rated it as 6, 6, and 3.This rating is more optimistic than the majority of expert opinions.</li>
</ol>
<p>-### Decision Both reviews align with the expert reviews to some extent, but Review A aligns more closely overall: -Review A's numerical ratings for soundness, presentation, and contribution match the majority of expert reviews, while Review B's ratings for soundness and contribution are more optimistic.-Review A's strengths and weaknesses align more closely with the expert reviews, though it misses some details (e.g., assumptions).Review B is more enthusiastic and introduces additional points (e.g., generality, comparison to alternatives) that are not emphasized in the expert reviews.-Both reviews' questions are relevant but do not fully align with the expert reviews.<strong>Final Decision</strong>: <strong>Review A</strong> aligns better with the expert reviews.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: System prompt used by OpenReviewer.Fields in {} are placeholders.</p>
<p>Figure 5 :
5
Figure 5: System prompt for the LLM judge.Fields in {} are placeholders.</p>
<p>Figure 7 :
7
Figure 7: Example output from the LLM judge.Review A is generated by OpenReviewer, and Review B is from GPT-4o.</p>
<p>Figure 8 :
8
Figure 8: The axolotl configuration used to train Llama-OpenReviewer-8B, including all hyperparameters.</p>
<p>Table 1 :
1
Exact Match (EM) and average error for the recommendation in 400 test reviews generated with dif-
ModelEM (%) Avg. Error (%)Llama-3.1-8B-Instruct14.02.95 ± 1.19Llama-3.1-70B-Instruct11.53.03 ± 1.34Claude-3.5-Sonnet15.52.77 ± 1.27GPT-4o (2024-11-20)23.82.34 ± 1.17OpenReviewer55.50.96 ± 0.85
ferent LLMs and normalized to a scale from 1 (strong reject) to 10 (strong accept).EM measures how often the LLM's recommendation matches at least one of the human reviews.The average error is the average absolute difference between the LLM's recommendations and the human reviewers' average recommendation.The recommendations by OpenReviewer match the human reviewers much better than other state-of-the-art LLMs.</p>
<p>Table 2
2: Average of recommendations in reviews for400 test papers, generated with different LLMs and nor-malized to a scale from 1 (strong reject) to 10 (strongaccept). The recommendations found in reviews pro-duced by OpenReviewer are much more critical thanother LLMs, which tend to give a positive recommenda-tion most of the time.</p>
<p>Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,  Joseph E. Gonzalez, and Ion Stoica.2023.Judging llm-as-a-judge with mt-bench and chatbot arena.In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -16, 2023.
Ruiyang Zhou, Lu Chen, and Kai Yu. 2024. Is LLMa reliable reviewer? a comprehensive evaluation ofLLM on automatic paper reviewing tasks. In Pro-ceedings of the 2024 Joint International Conferenceon Computational Linguistics, Language Resourcesand Evaluation (LREC-COLING 2024), pages 9340-9351, Torino, Italia. ELRA and ICCL.</p>
<p>guidelines: 1. Read the paper: It's important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it.Be sure to give yourself sufficient time for this step.2.While reading, consider the following: -Objective of the work: What is the goal of the paper?Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding?A combination of these?Different objectives will require different considerations as to potential value and impact.-Strongpoints: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g.theoretically, algorithmically, etc.)? -Weak points: is it weak in any of the aspects listed in b.? -Be mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the community, even if it may not be very interesting for you.3.Answer four key questions for yourself, to make a recommendation to Accept or Reject: -What is the specific question and/or problem tackled by the paper?-Is the approach well motivated, including being well-placed in the literature?-Does the paper support the claims?This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.-What is the significance of the work?Does it contribute new knowledge and sufficient value to the community?Note, this does not necessarily require state-of-the-art results.Submissions bring value to the community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).4. Write your review including the following information: -Summarize what the paper claims to contribute.Be positive and constructive.-List strong and weak points of the paper.Be as comprehensive as possible.-Clearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.-Provide supporting arguments for your recommendation.-Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.-Provide additional feedback with the aim to improve the paper.Make it clear that these points are here to help, and not necessarily part of your decision assessment.Your write reviews in markdown format.Your reviews contain the following sections: # Review {review_fields} Your response must only contain the review in markdown format with sections as defined above.</p>
<p>Model: huggingface.co/maxidl/Llama-OpenReviewer-8B
Demo: huggingface.co/spaces/maxidl/openreviewer
huggingface.co/docs/hub/spaces-zerogpu
 openreview.net <br />
github.com/VikParuchuri/marker
 axolotl.ai <br />
AcknowledgmentsThis research was primarily supported by the Leibniz Young Investigator Grant program (project ARENA, LYIG-2023-01) of Leibniz University Hannover, funded by the Ministry of Science and Culture of Lower Saxony (MWK) (grant no.11-76251-114/2022).Additional computational resources were provided through the AI service center KISSKI (grant no.01IS22093C), funded by the German Federal Ministry of Education and Research (BMBF).
Gradio: Hassle-free sharing and testing of ML models in the wild. Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, James Y Zou, CoRR, abs/1906.025692019</p>
<p>Nougat: Neural optical understanding for academic documents. Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>The Toronto Paper Matching System: An automated paper-reviewer assignment system. Laurent Charlin, Richard Zemel, Journal of Machine Learning Research Workshop and Conference Proceedings. Atlanta, Georgia, USA201328Proceedings of the 30th International Conference on Machine Learning</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. Tri Dao, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Rozière, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, David Livshits, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Frank Radenovic, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Graeme Lewis Anderson, Grégoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel M Ibarra, Ishan Kloumann, Ivan Misra, Jana Evtimov, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Jia, 10.48550/ARXIV.2407.21783Jan Geffert,Kartikeya UpasaniJade Copet, Jaewon Lee; Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stoneand et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783</p>
<p>Nlpeer: A unified resource for the computational study of peer review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, 10.18653/V1/2023.ACL-LONG.277Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Limgen: Probing the llms for generating suggestive limitations of research papers. Abdur Rahman Bin, Mohammed Faizullah, Ashok Urlana, Rahul Mishra, 10.1007/978-3-031-70344-7_7Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2024. Vilnius, Lithuania; Berlin, HeidelbergSpringer-Verlag2024. September 9-13, 2024Proceedings, Part II</p>
<p>Reviewer2: Optimizing review generation through prompt generation. Zhaolin Gao, Kianté Brantley, Thorsten Joachims, arXiv:2402.108862024arXiv preprint</p>
<p>The diversity-innovation paradox in science. Bas Hofstra, V Vivek, Sebastian Kulkarni, Munoz-Najar, Bryan Galvez, Dan He, Daniel A Jurafsky, Mc-Farland, 10.1073/pnas.1915378117Proceedings of the National Academy of Sciences. 117172020</p>
<p>Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen, arXiv:2410.10989Liger kernel: Efficient triton kernels for llm training. 2024arXiv preprint</p>
<p>On the detection of reviewerauthor collusion rings from paper bidding. Steven Jecmen, B Nihar, Fei Shah, Leman Fang, Akoglu, 10.48550/ARXIV.2402.07860CoRR, abs/2402.078602024</p>
<p>A dataset of peer reviews (PeerRead): Collection, insights and NLP applications. Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz, 10.18653/v1/N18-1149Proceedings of the 2018 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics20181Long Papers</p>
<p>What can natural language processing. Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Margot Mausam, Aurélie Mieskes, Danish Névéol, Lizhen Pruthi, Roy Qu, Noah A Schwartz, Thamar Smith, Jingyan Solorio, Xiaodan Wang, Zhu, 10.48550/ARXIV.2405.06563CoRR, abs/2405.065632024Nihar B. Shah, and Iryna GurevychAnna Rogers</p>
<p>Revise and resubmit: An intertextual model of text-based collaboration in peer review. Ilia Kuznetsov, Jan Buchmann, Max Eichler, Iryna Gurevych, 10.1162/COLI_A_00455Comput. Linguistics. 4842022</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, 10.1145/3600006.3613165Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023. the 29th Symposium on Operating Systems Principles, SOSP 2023Koblenz, GermanyACM2023. October 23-26, 2023</p>
<p>Summarizing multiple documents with conversational structure for meta-review generation. Miao Li, Eduard Hovy, Jey Lau, 10.18653/v1/2023.findings-emnlp.472Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023a</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023b</p>
<p>Can large language models provide useful feedback on research papers? A large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Scott Daniel, Yian Smith, Daniel A Yin, James Mcfarland, Zou, 10.48550/ARXIV.2310.01783CoRR, abs/2310.017832023</p>
<p>Ethan Lin, Zhiyuan Peng, Yi Fang, arXiv:2409.16605Evaluating and enhancing large language models for novelty assessment in scholarly publications. 2024arXiv preprint</p>
<p>Automated scholarly paper review: Concepts, technologies, and challenges. Information fusion. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, 2023a98101830</p>
<p>MOPRD: A multidisciplinary open peer review dataset. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, 10.1007/S00521-023-08891-5Neural Comput. Appl. 35342023b</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, 10.48550/ARXIV.2306.00622CoRR, abs/2306.006222023</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 10.48550/ARXIV.2408.06292CoRR, abs/2408.062922024</p>
<p>Uncovering latent biases in text: Method and application to peer review. A Emaad, Nihar B Manzoor, Shah, 10.1609/AAAI.V35I6.16608Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence. AAAI Press2021. February 2-9, 20212021</p>
<p>Zero: memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, 10.18653/v1/2020.findings-emnlp.112Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisAtlanta, Georgia, USAOnline. Association for Computational Linguistics2020. November 9-19, 2020. 20202020Findings of the Association for Computational Linguistics: EMNLP 2020</p>
<p>Design and analysis of the NIPS 2016 review process. B Nihar, Behzad Shah, Krikamol Tabibian, Isabelle Muandet, Ulrike Guyon, Luxburg Von, J. Mach. Learn. Res. 19342018</p>
<p>Reviewer bias in single-versus doubleblind peer review. Andrew Tomkins, Min Zhang, William D Heavlin, 10.1073/PNAS.1707323114Proc. Natl. Acad. Sci. USA. Natl. Acad. Sci. USA2017114</p>
<p>Development of the review quality instrument (RQI) for assessing peer reviews of manuscripts. Susan Van Rooyen, Nick Black, Fiona Godlee, Journal of Clinical Epidemiology. 5271999</p>
<p>Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. Jingyan Wang, Nihar B Shah, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '19. the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '19Montreal, QC, Canada2019. May 13-17, 2019International Foundation for Autonomous Agents and Multiagent Systems</p>
<p>Reviewrobot: Explainable paper review generation based on knowledge synthesis. Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Ji Heng, Nazneen Fatema, Rajani , 10.18653/V1/2020.INLG-1.44Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational Linguistics2020. December 15-18, 20202020</p>
<p>Simple and effective paraphrastic similarity from parallel translations. John Wieting, Kevin Gimpel, Graham Neubig, Taylor Berg-Kirkpatrick, ACL. Florence, Italy2019</p>
<p>Rui Ye, Xianghe Pang, Jingyi Chai, Jiaao Chen, Zhenfei Yin, Zhen Xiang, Xiaowen Dong, Jing Shao, Siheng Chen, arXiv:2412.01708Are we there yet? revealing the risks of utilizing large language models in scholarly peer review. 2024arXiv preprint</p>
<p>Automated peer reviewing in paper SEA: Standardization, evaluation, and analysis. Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, Ren-Jing Cui, Chengcheng Han, Qiushi Sun, Zhiyong Wu, Yunshi Lan, Xiang Li, 10.18653/v1/2024.findings-emnlp.595Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Can we automate scientific reviewing?. Weizhe Yuan, Pengfei Liu, Graham Neubig, 10.1613/JAIR.1.12862J. Artif. Intell. Res. 752022</p>
<p>. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </p>            </div>
        </div>

    </div>
</body>
</html>
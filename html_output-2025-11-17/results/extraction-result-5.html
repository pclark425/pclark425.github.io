<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-1.html">extraction-schema-1</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games or interactive fiction, focusing on how they use memory to improve task performance, including types of memory used, memory representations, and performance comparisons with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-31b99032f20373a19043c85492bf717fc17c06f1</p>
                <p><strong>Cost:</strong> 0.002</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games or interactive fiction, focusing on how they use memory to improve task performance, including types of memory used, memory representations, and performance comparisons with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STARLING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>STARLING is an interactive environment that utilizes large language models (LLMs) to generate text-based games for training reinforcement learning agents, enhancing their performance and generalization capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>STARLING</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>STARLING is a text-based reinforcement learning agent that leverages LLMs like GPT-3 to generate training games, focusing on sequential decision-making and skill acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Interactive Fiction Games (e.g., Cooking Pasta)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The agent utilizes episodic memory to track previously learned skills and actions, allowing it to apply these skills in new game contexts effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>1.0 ± 0.0 (normalized score) on TWC Easy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>0.99 ± 0.0 (normalized score) on TWC Easy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits_summary</strong></td>
                            <td>The use of memory allows STARLING to better generalize skills across different tasks, improving its ability to avoid failure states and choose valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_challenges</strong></td>
                            <td>STARLING struggles with tasks requiring complex navigation due to the simplicity of pre-training games, which may not adequately prepare it for longer trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_training_method</strong></td>
                            <td>Reinforcement learning with pre-training on generated games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_training_method</strong></td>
                            <td>Memory is adapted through reinforcement learning, allowing the agent to recall and apply previously learned skills during gameplay.</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>The tasks involve multiple sub-tasks requiring a sequence of actions, with varying complexity based on the game environment (e.g., TWC, ScienceWorld, Zork1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text-based RL agents with commonsense knowledge: New challenges, environments and baselines <em>(Rating: 2)</em></li>
                <li>ScienceWorld: Is your agent smarter than a 5th grader? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5",
    "paper_id": "paper-31b99032f20373a19043c85492bf717fc17c06f1",
    "extraction_schema_id": "extraction-schema-1",
    "extracted_data": [
        {
            "name_short": "STARLING",
            "name_full": "Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
            "brief_description": "STARLING is an interactive environment that utilizes large language models (LLMs) to generate text-based games for training reinforcement learning agents, enhancing their performance and generalization capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "STARLING",
            "agent_description": "STARLING is a text-based reinforcement learning agent that leverages LLMs like GPT-3 to generate training games, focusing on sequential decision-making and skill acquisition.",
            "text_game_name": "Interactive Fiction Games (e.g., Cooking Pasta)",
            "memory_used": true,
            "memory_type": "Episodic memory",
            "memory_mechanism_description": "The agent utilizes episodic memory to track previously learned skills and actions, allowing it to apply these skills in new game contexts effectively.",
            "performance_with_memory": "1.0 ± 0.0 (normalized score) on TWC Easy",
            "performance_without_memory": "0.99 ± 0.0 (normalized score) on TWC Easy",
            "performance_comparison_reported": true,
            "memory_benefits_summary": "The use of memory allows STARLING to better generalize skills across different tasks, improving its ability to avoid failure states and choose valid actions.",
            "memory_limitations_or_challenges": "STARLING struggles with tasks requiring complex navigation due to the simplicity of pre-training games, which may not adequately prepare it for longer trajectories.",
            "agent_training_method": "Reinforcement learning with pre-training on generated games",
            "memory_training_method": "Memory is adapted through reinforcement learning, allowing the agent to recall and apply previously learned skills during gameplay.",
            "task_complexity_description": "The tasks involve multiple sub-tasks requiring a sequence of actions, with varying complexity based on the game environment (e.g., TWC, ScienceWorld, Zork1).",
            "uuid": "e5.0",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text-based RL agents with commonsense knowledge: New challenges, environments and baselines",
            "rating": 2
        },
        {
            "paper_title": "ScienceWorld: Is your agent smarter than a 5th grader?",
            "rating": 1
        }
    ],
    "cost": 0.0024726,
    "model_str": null
}</code></pre>
        </div>

    </div>
</body>
</html>
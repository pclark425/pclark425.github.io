<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-211 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-211</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-211</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-02f033482b8045c687316ef81ba7aaae9f0a2e1c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/02f033482b8045c687316ef81ba7aaae9f0a2e1c" target="_blank">DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering and applies DExperts to language detoxification and sentiment-controlled generation, where it outperform existing controllable generation methods on both automatic and human evaluations.</p>
                <p><strong>Paper Abstract:</strong> Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with “expert” LMs and/or “anti-expert” LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e211.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e211.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DExPERTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding-time Experts (DExPERTS) product-of-experts ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time method that steers a pretrained base LM by reweighting its token logits with logits from finetuned 'expert' and 'anti-expert' LMs via a product-of-experts formulation, thereby increasing probabilities of desired tokens and decreasing probabilities of undesired tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (base); also applied to GPT-3 Ada via API</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M (GPT-2 Large base); GPT-3 Ada size not specified (API)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Controlled text generation (toxicity avoidance and sentiment steering), i.e., manipulating token probabilities to produce continuations with desired attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Model-generated logits / probability distributions from finetuned expert and anti-expert LMs (i.e., external model 'opinions' provided at decoding time).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Finetuned small-to-medium LMs trained on annotated corpora (human-annotated toxic/nontoxic comments for toxicity; SST-5 movie reviews for sentiment).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (experts may align with or contradict the base LM's parametric priors depending on finetuning data).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline GPT-2 Large (no experts): avg. max. toxicity = 0.527, toxicity probability = 0.520 (probability of generating a toxic continuation at least once over 25 generations); output perplexity (fluency) = 25.45 (measured by GPT-2 XL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>DExPERTS (large experts) on GPT-2 base: avg. max. toxicity = 0.314, toxicity probability = 0.128; output perplexity = 32.41. When applied to GPT-3 Ada via API: avg. max. toxicity = 0.293, toxicity probability = 0.111 (Table 2). For sentiment steering, DExPERTS substantially increased percentage of positive continuations on neutral prompts (e.g., ~94% positive for positive steering with large experts on many prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>Positive effect on attribute control: adding expert/anti-expert evidence substantially decreased the model's probability of undesired continuations (toxicity) and increased probability of desired attributes (e.g., positive sentiment). Negative trade-offs observed: increased perplexity (reduced fluency) for some configurations and sizes; strength of effect controlled by α (larger |α| increases attribute control but tends to reduce fluency).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Product-of-experts reweighting: new token logits = base logits + α*(expert_logits - anti-expert_logits), equivalent to scaling base probabilities by (P_expert / P_antiexpert)^α; α controls influence. Truncation (top-k / top-p) is applied on the base LM to prevent experts from reintroducing low-probability tail tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directly ensembling expert and anti-expert probability outputs is an effective form of 'evidence' to steer large LMs: small finetuned experts (even ~650 toxic comments) suffice to meaningfully reduce toxicity; DExPERTS outperforms prior decoding-time methods on toxicity and sentiment control while preserving diversity; α provides a smooth, controllable tradeoff between attribute control and fluency; method works even with limited access to base model outputs (e.g., top-100 tokens from GPT-3 API).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e211.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e211.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DExPERTS (anti-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DExPERTS anti-expert-only ablation (reusing base LM as expert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where the base LM's logits are reused as the 'expert' and only an anti-expert is subtracted (formally substituting expert_logits = base_logits), yielding logits = (1+α)*base_logits - α*anti_logits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (base used as expert and base on which anti-expert applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Controlled generation via steering away from undesired attributes using only an anti-expert signal.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Model-generated anti-expert logits only (negative evidence indicating undesired tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Finetuned anti-expert LMs trained on annotated toxic or negative sentiment examples.</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed/contradictory (anti-expert intentionally contrasts base LM's preferences).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline GPT-2 Large: avg. max. toxicity = 0.527; toxicity probability = 0.520; output ppl = 25.45.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>DExPERTS (anti-only) on GPT-2: avg. max. toxicity = 0.352; toxicity probability = 0.191; output perplexity = 52.02 (notably worse fluency and diversity compared to full DExPERTS).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>Mixed: adding only anti-expert evidence reduced probability of toxic continuations (positive for detoxification) but substantially harmed fluency and diversity (negative), likely due to weaker contrast and overcorrection.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Without a specialized expert to provide positive contrast, subtracting anti-expert logits excessively redistributes probability away from tokens the base LM prefers, causing degraded fluency and token diversity while still lowering toxic-token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Anti-expert-only steering is a viable lightweight option (no non-toxic training data needed) and reduces toxicity more than some baselines, but at the cost of higher perplexity and lower diversity than full expert+anti-expert ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e211.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e211.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plug and Play Language Models (PPLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time control method that modifies a pretrained LM's hidden representations by applying gradients from an attribute classifier to steer generation toward a desired class.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Plug and play language models: A simple approach to controlled text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decoding-time controlled generation using classifier-gradient updates to the LM's hidden activations (tested for toxicity and sentiment steering).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Classifier-derived gradient signals (the classifier's judgments act as evidence to update hidden activations).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Attribute classifiers retrained on toxicity / sentiment labeled data (SST-5, Jigsaw toxic comments for toxicity).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>unknown/mixed (classifier judgments may align or conflict with base LM priors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline GPT-2 Large: avg. max. toxicity = 0.527; toxicity probability = 0.520; output ppl = 25.45.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>PPLM (toxicity steering) reported: avg. max. toxicity ≈ 0.520, toxicity probability ≈ 0.518; output perplexity ≈ 32.58. For sentiment steering, PPLM exhibited extremely poor fluency in some cases (reported output ppl up to 142.11), indicating occasional degenerate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>Weak-to-no improvement on toxicity metrics in this implementation, and highly negative effects on fluency (very large increases in perplexity) in some sentiment settings; evidence via gradients can push the model into low-probability / degenerate regions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Applying classifier gradients to hidden states can drive generation into regions the base LM assigns very low probability to, producing high perplexity (reduced fluency) and occasional degenerate outputs despite classifier-driven steering.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PPLM is computationally expensive and in these experiments showed limited detoxification benefits while sometimes producing extremely high perplexity outputs (degraded fluency), suggesting classifier-gradient evidence can have negative side-effects on model confidence and generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e211.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e211.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeDi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Discriminator (GeDi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time control approach that uses class-conditioned LMs to compute class probabilities for each next token via Bayes' rule and steer generation toward a target attribute.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GeDi: Generative discriminator guided sequence generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Class-conditioned GPT-2 Medium used as generative discriminator; steering applied to GPT-2 Large base</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Class-conditioned LM: GPT-2 Medium (size not explicitly given here); base GPT-2 Large: 774M</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Attribute-conditioned controlled generation (toxicity and sentiment), where class-conditioned LM outputs serve as evidence about token class-membership.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Class-conditioned LM outputs used to compute token-level class posterior probabilities (Bayesian evidence from generative discriminators).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Class-conditioned LMs trained on labeled corpora (IMDB for the provided models; authors also considered SST-5).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (class-conditioned LM provides evidence that can strongly contradict base LM preferences).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline GPT-2 Large: avg. max. toxicity = 0.527; toxicity probability = 0.520; output ppl = 25.45.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>GeDi (toxicity steering) on GPT-2: avg. max. toxicity = 0.363; toxicity probability = 0.217; output perplexity = 60.03 (substantially worse fluency but strong reduction in toxicity compared to baseline). For sentiment, GeDi often achieved strong control but at high perplexity (e.g., output ppl often much larger than base).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>Positive effect on attribute control (substantial reduction in undesired-class probability) but large negative effects on fluency and sometimes topicality/diversity due to the strong class signal reweighting probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Bayesian weighting using class-conditioned LM posteriors strongly boosts tokens indicative of the target class, but this can move probability mass away from fluent continuations as judged by an external fluency LM, yielding higher perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GeDi provides strong attribute steering (reduces toxic/sentiment-mismatched continuations) but often at the cost of very poor fluency (high perplexity) compared to base LM; direct ensembling of expert logits (DExPERTS) was found more effective overall in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e211.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e211.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perspective API</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perspective API toxicity classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercially deployed automatic toxicity detection tool used here as an evaluation 'oracle' to measure the probability of toxic generations; the paper notes systematic biases in such classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automatic toxicity evaluation of generated text (used to compute avg. max toxicity and empirical probability of toxic continuations).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Classifier-generated toxicity scores (used as evidence/labels about whether a generated continuation is toxic).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Automated classifier (Perspective API) trained by an external provider on labeled toxicity data.</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>unknown/misaligned (the API's operationalization of 'toxicity' can mismatch human judgments; known biases towards overestimating toxicity in text referencing minority identities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>As an evaluation signal, it can bias conclusions: the paper reports that automatic toxicity scores can be inaccurate and spuriously rely on features leading to overestimation of toxicity for certain groups; therefore using the API as sole evidence can lead to misleading assessments of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Classifier reliance on spurious lexical cues and dataset bias leads to systematic mis-estimation of toxicity (e.g., higher scores for mentions of minority identities), which affects measured probabilities of harmful content.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The authors caution against sole reliance on automated toxicity classifiers (like Perspective API) because their judgments serve as 'evidence' that may be biased; they therefore complement automated evaluation with human annotation to get a more reliable picture.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e211.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e211.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>α (alpha) weighting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Control strength hyperparameter α in DExPERTS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalar hyperparameter that scales the influence of expert and anti-expert logits in the ensemble (logits_new = base_logits + α*(expert_logits - anti_logits)), tuning the tradeoff between attribute control and fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DExPERTS ensemble operating on GPT-2 / GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Regulate the amount by which external 'evidence' (expert/anti-expert outputs) changes the base LM's token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Scalar multiplier applied to model-generated logits used as evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Internal hyperparameter (set by practitioner; values studied in paper range α ∈ [−3.4, 3.4] for sentiment and α ≈ 2.0 for toxicity experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>n/a (α modulates how much external evidence overrides or augments the base model's parametric priors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>α = 0 corresponds to no evidence influence and yields base LM performance (e.g., GPT-2 toxicity avg max = 0.527, ppl = 25.45).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Increasing |α| increases attribute control (e.g., lower toxicity / higher target sentiment) but also increases perplexity (reduced fluency); authors chose α = ±3.2 for sentiment (good tradeoff) and α = 2.0 for toxicity in many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>Monotonic control: positive in terms of stronger steering toward desired attribute; negative in terms of fluency as measured by perplexity — a smooth tradeoff observed in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Larger |α| applies stronger multiplicative scaling of the expert/anti-expert ratio, shifting probability mass more aggressively away from base LM priors and thus reducing base-model-assigned probability (confidence) for many tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>α provides a predictable, smooth control knob; practitioners can tune α to trade control strength versus fluency; beyond certain α values marginal gains in attribute control require large losses in fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Plug and play language models: A simple approach to controlled text generation. <em>(Rating: 2)</em></li>
                <li>GeDi: Generative discriminator guided sequence generation <em>(Rating: 2)</em></li>
                <li>FUDGE: Controlled text generation with future discriminators <em>(Rating: 2)</em></li>
                <li>RealToxicityPrompts: Evaluating neural toxic degeneration in language models <em>(Rating: 1)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-211",
    "paper_id": "paper-02f033482b8045c687316ef81ba7aaae9f0a2e1c",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "DExPERTS",
            "name_full": "Decoding-time Experts (DExPERTS) product-of-experts ensemble",
            "brief_description": "A decoding-time method that steers a pretrained base LM by reweighting its token logits with logits from finetuned 'expert' and 'anti-expert' LMs via a product-of-experts formulation, thereby increasing probabilities of desired tokens and decreasing probabilities of undesired tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (base); also applied to GPT-3 Ada via API",
            "model_size": "774M (GPT-2 Large base); GPT-3 Ada size not specified (API)",
            "task_description": "Controlled text generation (toxicity avoidance and sentiment steering), i.e., manipulating token probabilities to produce continuations with desired attributes.",
            "evidence_type": "Model-generated logits / probability distributions from finetuned expert and anti-expert LMs (i.e., external model 'opinions' provided at decoding time).",
            "evidence_source": "Finetuned small-to-medium LMs trained on annotated corpora (human-annotated toxic/nontoxic comments for toxicity; SST-5 movie reviews for sentiment).",
            "parametric_knowledge_alignment": "mixed (experts may align with or contradict the base LM's parametric priors depending on finetuning data).",
            "performance_without_evidence": "Baseline GPT-2 Large (no experts): avg. max. toxicity = 0.527, toxicity probability = 0.520 (probability of generating a toxic continuation at least once over 25 generations); output perplexity (fluency) = 25.45 (measured by GPT-2 XL).",
            "performance_with_evidence": "DExPERTS (large experts) on GPT-2 base: avg. max. toxicity = 0.314, toxicity probability = 0.128; output perplexity = 32.41. When applied to GPT-3 Ada via API: avg. max. toxicity = 0.293, toxicity probability = 0.111 (Table 2). For sentiment steering, DExPERTS substantially increased percentage of positive continuations on neutral prompts (e.g., ~94% positive for positive steering with large experts on many prompts).",
            "evidence_effect": "Positive effect on attribute control: adding expert/anti-expert evidence substantially decreased the model's probability of undesired continuations (toxicity) and increased probability of desired attributes (e.g., positive sentiment). Negative trade-offs observed: increased perplexity (reduced fluency) for some configurations and sizes; strength of effect controlled by α (larger |α| increases attribute control but tends to reduce fluency).",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Product-of-experts reweighting: new token logits = base logits + α*(expert_logits - anti-expert_logits), equivalent to scaling base probabilities by (P_expert / P_antiexpert)^α; α controls influence. Truncation (top-k / top-p) is applied on the base LM to prevent experts from reintroducing low-probability tail tokens.",
            "key_findings": "Directly ensembling expert and anti-expert probability outputs is an effective form of 'evidence' to steer large LMs: small finetuned experts (even ~650 toxic comments) suffice to meaningfully reduce toxicity; DExPERTS outperforms prior decoding-time methods on toxicity and sentiment control while preserving diversity; α provides a smooth, controllable tradeoff between attribute control and fluency; method works even with limited access to base model outputs (e.g., top-100 tokens from GPT-3 API).",
            "counterintuitive_behavior": true,
            "uuid": "e211.0",
            "source_info": {
                "paper_title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "DExPERTS (anti-only)",
            "name_full": "DExPERTS anti-expert-only ablation (reusing base LM as expert)",
            "brief_description": "An ablation where the base LM's logits are reused as the 'expert' and only an anti-expert is subtracted (formally substituting expert_logits = base_logits), yielding logits = (1+α)*base_logits - α*anti_logits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (base used as expert and base on which anti-expert applied)",
            "model_size": "774M",
            "task_description": "Controlled generation via steering away from undesired attributes using only an anti-expert signal.",
            "evidence_type": "Model-generated anti-expert logits only (negative evidence indicating undesired tokens).",
            "evidence_source": "Finetuned anti-expert LMs trained on annotated toxic or negative sentiment examples.",
            "parametric_knowledge_alignment": "mixed/contradictory (anti-expert intentionally contrasts base LM's preferences).",
            "performance_without_evidence": "Baseline GPT-2 Large: avg. max. toxicity = 0.527; toxicity probability = 0.520; output ppl = 25.45.",
            "performance_with_evidence": "DExPERTS (anti-only) on GPT-2: avg. max. toxicity = 0.352; toxicity probability = 0.191; output perplexity = 52.02 (notably worse fluency and diversity compared to full DExPERTS).",
            "evidence_effect": "Mixed: adding only anti-expert evidence reduced probability of toxic continuations (positive for detoxification) but substantially harmed fluency and diversity (negative), likely due to weaker contrast and overcorrection.",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Without a specialized expert to provide positive contrast, subtracting anti-expert logits excessively redistributes probability away from tokens the base LM prefers, causing degraded fluency and token diversity while still lowering toxic-token probability.",
            "key_findings": "Anti-expert-only steering is a viable lightweight option (no non-toxic training data needed) and reduces toxicity more than some baselines, but at the cost of higher perplexity and lower diversity than full expert+anti-expert ensembles.",
            "counterintuitive_behavior": true,
            "uuid": "e211.1",
            "source_info": {
                "paper_title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "PPLM",
            "name_full": "Plug and Play Language Models (PPLM)",
            "brief_description": "A decoding-time control method that modifies a pretrained LM's hidden representations by applying gradients from an attribute classifier to steer generation toward a desired class.",
            "citation_title": "Plug and play language models: A simple approach to controlled text generation.",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (base)",
            "model_size": "774M",
            "task_description": "Decoding-time controlled generation using classifier-gradient updates to the LM's hidden activations (tested for toxicity and sentiment steering).",
            "evidence_type": "Classifier-derived gradient signals (the classifier's judgments act as evidence to update hidden activations).",
            "evidence_source": "Attribute classifiers retrained on toxicity / sentiment labeled data (SST-5, Jigsaw toxic comments for toxicity).",
            "parametric_knowledge_alignment": "unknown/mixed (classifier judgments may align or conflict with base LM priors).",
            "performance_without_evidence": "Baseline GPT-2 Large: avg. max. toxicity = 0.527; toxicity probability = 0.520; output ppl = 25.45.",
            "performance_with_evidence": "PPLM (toxicity steering) reported: avg. max. toxicity ≈ 0.520, toxicity probability ≈ 0.518; output perplexity ≈ 32.58. For sentiment steering, PPLM exhibited extremely poor fluency in some cases (reported output ppl up to 142.11), indicating occasional degenerate outputs.",
            "evidence_effect": "Weak-to-no improvement on toxicity metrics in this implementation, and highly negative effects on fluency (very large increases in perplexity) in some sentiment settings; evidence via gradients can push the model into low-probability / degenerate regions.",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Applying classifier gradients to hidden states can drive generation into regions the base LM assigns very low probability to, producing high perplexity (reduced fluency) and occasional degenerate outputs despite classifier-driven steering.",
            "key_findings": "PPLM is computationally expensive and in these experiments showed limited detoxification benefits while sometimes producing extremely high perplexity outputs (degraded fluency), suggesting classifier-gradient evidence can have negative side-effects on model confidence and generation quality.",
            "counterintuitive_behavior": true,
            "uuid": "e211.2",
            "source_info": {
                "paper_title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "GeDi",
            "name_full": "Generative Discriminator (GeDi)",
            "brief_description": "A decoding-time control approach that uses class-conditioned LMs to compute class probabilities for each next token via Bayes' rule and steer generation toward a target attribute.",
            "citation_title": "GeDi: Generative discriminator guided sequence generation",
            "mention_or_use": "use",
            "model_name": "Class-conditioned GPT-2 Medium used as generative discriminator; steering applied to GPT-2 Large base",
            "model_size": "Class-conditioned LM: GPT-2 Medium (size not explicitly given here); base GPT-2 Large: 774M",
            "task_description": "Attribute-conditioned controlled generation (toxicity and sentiment), where class-conditioned LM outputs serve as evidence about token class-membership.",
            "evidence_type": "Class-conditioned LM outputs used to compute token-level class posterior probabilities (Bayesian evidence from generative discriminators).",
            "evidence_source": "Class-conditioned LMs trained on labeled corpora (IMDB for the provided models; authors also considered SST-5).",
            "parametric_knowledge_alignment": "mixed (class-conditioned LM provides evidence that can strongly contradict base LM preferences).",
            "performance_without_evidence": "Baseline GPT-2 Large: avg. max. toxicity = 0.527; toxicity probability = 0.520; output ppl = 25.45.",
            "performance_with_evidence": "GeDi (toxicity steering) on GPT-2: avg. max. toxicity = 0.363; toxicity probability = 0.217; output perplexity = 60.03 (substantially worse fluency but strong reduction in toxicity compared to baseline). For sentiment, GeDi often achieved strong control but at high perplexity (e.g., output ppl often much larger than base).",
            "evidence_effect": "Positive effect on attribute control (substantial reduction in undesired-class probability) but large negative effects on fluency and sometimes topicality/diversity due to the strong class signal reweighting probabilities.",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Bayesian weighting using class-conditioned LM posteriors strongly boosts tokens indicative of the target class, but this can move probability mass away from fluent continuations as judged by an external fluency LM, yielding higher perplexity.",
            "key_findings": "GeDi provides strong attribute steering (reduces toxic/sentiment-mismatched continuations) but often at the cost of very poor fluency (high perplexity) compared to base LM; direct ensembling of expert logits (DExPERTS) was found more effective overall in the paper's experiments.",
            "counterintuitive_behavior": true,
            "uuid": "e211.3",
            "source_info": {
                "paper_title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Perspective API",
            "name_full": "Perspective API toxicity classifier",
            "brief_description": "A commercially deployed automatic toxicity detection tool used here as an evaluation 'oracle' to measure the probability of toxic generations; the paper notes systematic biases in such classifiers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_description": "Automatic toxicity evaluation of generated text (used to compute avg. max toxicity and empirical probability of toxic continuations).",
            "evidence_type": "Classifier-generated toxicity scores (used as evidence/labels about whether a generated continuation is toxic).",
            "evidence_source": "Automated classifier (Perspective API) trained by an external provider on labeled toxicity data.",
            "parametric_knowledge_alignment": "unknown/misaligned (the API's operationalization of 'toxicity' can mismatch human judgments; known biases towards overestimating toxicity in text referencing minority identities).",
            "performance_without_evidence": null,
            "performance_with_evidence": null,
            "evidence_effect": "As an evaluation signal, it can bias conclusions: the paper reports that automatic toxicity scores can be inaccurate and spuriously rely on features leading to overestimation of toxicity for certain groups; therefore using the API as sole evidence can lead to misleading assessments of model outputs.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Classifier reliance on spurious lexical cues and dataset bias leads to systematic mis-estimation of toxicity (e.g., higher scores for mentions of minority identities), which affects measured probabilities of harmful content.",
            "key_findings": "The authors caution against sole reliance on automated toxicity classifiers (like Perspective API) because their judgments serve as 'evidence' that may be biased; they therefore complement automated evaluation with human annotation to get a more reliable picture.",
            "counterintuitive_behavior": true,
            "uuid": "e211.4",
            "source_info": {
                "paper_title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "α (alpha) weighting",
            "name_full": "Control strength hyperparameter α in DExPERTS",
            "brief_description": "A scalar hyperparameter that scales the influence of expert and anti-expert logits in the ensemble (logits_new = base_logits + α*(expert_logits - anti_logits)), tuning the tradeoff between attribute control and fluency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DExPERTS ensemble operating on GPT-2 / GPT-3",
            "model_size": null,
            "task_description": "Regulate the amount by which external 'evidence' (expert/anti-expert outputs) changes the base LM's token probabilities.",
            "evidence_type": "Scalar multiplier applied to model-generated logits used as evidence.",
            "evidence_source": "Internal hyperparameter (set by practitioner; values studied in paper range α ∈ [−3.4, 3.4] for sentiment and α ≈ 2.0 for toxicity experiments).",
            "parametric_knowledge_alignment": "n/a (α modulates how much external evidence overrides or augments the base model's parametric priors).",
            "performance_without_evidence": "α = 0 corresponds to no evidence influence and yields base LM performance (e.g., GPT-2 toxicity avg max = 0.527, ppl = 25.45).",
            "performance_with_evidence": "Increasing |α| increases attribute control (e.g., lower toxicity / higher target sentiment) but also increases perplexity (reduced fluency); authors chose α = ±3.2 for sentiment (good tradeoff) and α = 2.0 for toxicity in many experiments.",
            "evidence_effect": "Monotonic control: positive in terms of stronger steering toward desired attribute; negative in terms of fluency as measured by perplexity — a smooth tradeoff observed in experiments.",
            "evidence_decreased_confidence": true,
            "proposed_mechanism": "Larger |α| applies stronger multiplicative scaling of the expert/anti-expert ratio, shifting probability mass more aggressively away from base LM priors and thus reducing base-model-assigned probability (confidence) for many tokens.",
            "key_findings": "α provides a predictable, smooth control knob; practitioners can tune α to trade control strength versus fluency; beyond certain α values marginal gains in attribute control require large losses in fluency.",
            "counterintuitive_behavior": false,
            "uuid": "e211.5",
            "source_info": {
                "paper_title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Plug and play language models: A simple approach to controlled text generation.",
            "rating": 2
        },
        {
            "paper_title": "GeDi: Generative discriminator guided sequence generation",
            "rating": 2
        },
        {
            "paper_title": "FUDGE: Controlled text generation with future discriminators",
            "rating": 2
        },
        {
            "paper_title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "rating": 1
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 1
        }
    ],
    "cost": 0.01851825,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DExPERTS: Decoding-Time Controlled Text Generation with Experts and Anti-Experts</h1>
<p>Alisa Liu ${ }^{\circ}$ Maarten Sap ${ }^{\circ}$ Ximing Lu ${ }^{\circ}$ Swabha Swayamdipta ${ }^{\text {A }}$ Chandra Bhagavatula ${ }^{\text {A }}$ Noah A. Smith ${ }^{\circ}$ Yejin Choi ${ }^{\circ}$<br>${ }^{\circ}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{\text {A }}$ Allen Institute for Artificial Intelligence<br>alisaliu@cs.washington.edu</p>
<h4>Abstract</h4>
<p>Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExPERTS: Decoding-time Experts, a decodingtime method for controlled text generation that combines a pretrained language model with "expert" LMs and/or "anti-expert" LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts and unlikely by the anti-experts. We apply DExPERTS to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExPERTS operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.</p>
<h2>1 Introduction</h2>
<p>Controlling the output of pretrained language models (LMs) is crucial for achieving useful and safe language generation applications, such as nonoffensive sentence completion or friendly conversation generation (See et al., 2019; Sheng et al., 2020; Gehman et al., 2020). For example, a safe completion to the prompt "When she rejected his advance, he grabbed..." requires avoiding word choices that could lead to continuations with gender-based violence (e.g., "her"; Figure 1).</p>
<p>Without such steering, these language models risk generating mindless and offensive content (Sheng et al., 2019; Holtzman et al., 2020) which hinders their safe deployment (Brockman et al., 2020; Bender et al., 2021). Importantly, as the scale of pretrained LMs increases (e.g., 175B and 1.6 T parameters; Brown et al., 2020; Fedus et al.,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of DExPERTS, where a toxic LM acts as an "anti-expert" and a non-toxic LM acts as an "expert". In this toy example, given the prompt, "When she rejected his advance, he grabbed," the toxic LM assigns greater weight to "her" than "his", expressing subtle signals of toxicity that can be leveraged for effective attribute control. The difference in logits $\mathbf{z}^{+}-\mathbf{z}^{-}$ output by the expert and anti-expert represents the perturbations to make to the logits $\mathbf{z}$ of the pretrained "base" LM.</p>
<p>2021), finetuning or re-training approaches are becoming increasingly computationally infeasible for most researchers.</p>
<p>We propose DExPERTS, ${ }^{1}$ a decoding-time method for controlled text generation based on a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>product of experts (Hinton, 2002). Our method combines an out-of-the-box pretrained ("base") LM with "expert" LMs and/or "anti-expert" LMs, which model text with desirable and undesirable attributes, respectively. By generatively modeling text with particular attributes and directly combining the output distributions from each LM, DExPERTS leverages subtle signals expressible by language models for effective attribute control, without sacrificing generation fluency or diversity. Moreover, because it operates only on the output of the base LM, DExPERTS can steer with (anti-)experts of smaller size, even in cases where we do not have full access to the base model (e.g., GPT-3 through an API).</p>
<p>We first apply DExPERTS to the task of language detoxification (§3), by finetuning an expert and an anti-expert on public comments that are humanannotated for toxicity. Our experimental results show that DExPERTS can successfully avoid toxicity in language generation while preserving output fluency, outperforming existing detoxification methods on both automatic and human evaluations. Moreover, we find that DExPERTS continues to outperform baselines when employing only an antiexpert and re-using the base model as the expert, making it one of the only methods that can avoid toxicity without annotated examples of non-toxic content. In analysis, we also show that our method successfully avoids toxic degeneration while using just $\sim 650$ toxic comments, opening avenues for easily customizable anti-experts.</p>
<p>We then showcase the generalizability of DExPERTS by tackling the task of controlling the sentiment of LMs' output (§4). To this end, we combine a pretrained LM with (anti-)experts modeling positive and negative sentiment. As with language detoxification, DExPERTS outperforms existing sentiment steering methods on both automatic and human evaluations. Additionally, we show our method is especially effective in the adversarial setting of steering negative prompts toward positive continuations, and vice versa. Finally, we demonstrate a preliminary proof-of-concept using DExPERTS for stylistic rewriting (§5).</p>
<p>Our work demonstrates the effectiveness of tuning small LMs on text with desirable and undesirable properties for efficient and effective steering of larger pretrained LMs, and highlights the promise of decoding-time methods for controlled language generation.</p>
<h2>2 Experts and Anti-Experts for Controlled Generation</h2>
<p>Given input text as a prompt, the task of controlled text generation is to generate a continuation that flows naturally from the prompt while having the desired attribute (e.g., positive sentiment) but not an undesired one (e.g., toxicity).</p>
<p>Given a prompt $\boldsymbol{x}<em t="t">{&lt;t}$, the language model computes the logits for the $t$ th token, denoted $\mathbf{z}</em>$ :} \in \mathbb{R}^{|\mathcal{V}|}$, where $\mathcal{V}$ is the vocabulary. A probability distribution over the vocabulary is obtained by normalizing and exponentiating $\mathbf{z}_{t</p>
<p>$$
P\left(X_{t} \mid \boldsymbol{x}<em t="t">{&lt;t}\right)=\operatorname{softmax}\left(\mathbf{z}</em>\right)
$$</p>
<p>and the next token is generated by sampling $x_{t} \sim$ $P\left(X_{t} \mid \boldsymbol{x}_{&lt;t}\right)$.</p>
<h3>2.1 DExPERTS Formalization</h3>
<p>DEXPERTS operates on a pretrained language model $M$ by combining its predictions with an expert $M^{+}$, which models text with a desirable attribute, and an anti-expert $M^{-}$, which models text with an undesirable attribute. At time step $t$, we condition each language model $M, M^{+}$, and $M^{-}$on the prompt $\boldsymbol{x}<em t="t">{&lt;t}$ to obtain $\mathbf{z}</em>}, \mathbf{z<em t="t">{t}^{+}$, and $\mathbf{z}</em>$}^{-}$, respectively. The product-of-experts ensemble is given by: ${ }^{2</p>
<p>$$
\tilde{P}\left(X_{t} \mid \boldsymbol{x}<em t="t">{&lt;t}\right)=\operatorname{softmax}\left(\mathbf{z}</em>}+\alpha\left(\mathbf{z<em t="t">{t}^{+}-\mathbf{z}</em>\right)\right)
$$}^{-</p>
<p>where $\alpha$ is a hyperparameter that controls the amount of modification to $\mathbf{z}_{t}$, and can be interpreted as the strength of control over the base model. Equivalently,</p>
<p>$$
\tilde{P}\left(X_{t} \mid \boldsymbol{x}<em t="t">{&lt;t}\right) \propto P\left(X</em>} \mid \boldsymbol{x<em t="t">{&lt;t}\right)\left(\frac{P^{+}\left(X</em>} \mid \boldsymbol{x<em t="t">{&lt;t}\right)}{P^{-}\left(X</em>
$$} \mid \boldsymbol{x}_{&lt;t}\right)}\right)^{\alpha</p>
<p>Intuitively, a token will only have high probability if it has high probability under both $P$ and $P^{+}$, and low probability under $P^{-}$. We can interpret the ratio $\frac{P^{+}\left(X_{t} \mid \boldsymbol{x}<em t="t">{&lt;t}\right)}{P^{-}\left(X</em>$ as a scaling coefficient for each token, which is used to modify the original probability predicted for that token.} \mid \boldsymbol{x}_{&lt;t}\right)</p>
<h3>2.2 Sampling from DExPERTS</h3>
<p>Sampling fluent output from language models commonly requires truncating the unreliable tail of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the probability distribution, as in top- $k$ (Fan et al., 2018) or nucleus sampling (Holtzman et al., 2020). We adapt this intuition to our method by truncating the logits $\mathbf{z}$ output by the base model prior to combining with the experts. Formally, let $\mathcal{V}^{\prime} \subset \mathcal{V}$ denote the set of tokens that are a part of the top$k /$ top- $p$ vocabulary of the base LM at time step $t$. The truncated logits $\mathbf{z}^{\prime}$ are given by</p>
<p>$$
\mathbf{z}^{\prime}[v]= \begin{cases}\mathbf{z}[v] &amp; \text { if } v \in \mathcal{V}^{\prime} \ -\infty &amp; \text { otherwise }\end{cases}
$$</p>
<p>By substituting $\mathbf{z}$ with $\mathbf{z}^{\prime}$ in Equation 2, we have</p>
<p>$$
\tilde{P}^{\prime}\left(X_{t} \mid \boldsymbol{x}<em t="t">{&lt;t}\right)=\operatorname{softmax}\left(\mathbf{z}</em>}^{\prime}+\alpha\left(\mathbf{z<em t="t">{t}^{+}-\mathbf{z}</em>\right)\right)
$$}^{-</p>
<p>We obtain our next token $x_{t}$ via pure sampling from the probability distribution $\tilde{P}^{\prime}\left(X_{t} \mid \boldsymbol{x}_{&lt;t}\right)$, which has non-zero probability only on tokens in $\mathcal{V}^{\prime}$. In this way, adding in the (anti-)experts can be interpreted as modifying the probability distribution over the candidate tokens in $\mathcal{V}^{\prime}$, without any chance of reintroducing tokens $v \notin \mathcal{V}^{\prime}$ from the tail of the original probability distribution.</p>
<h2>3 Toxicity Avoidance</h2>
<p>Given that large pretrained LMs are at risk of producing toxic content (Sheng et al., 2019; Gehman et al., 2020), steering away from toxic "degeneration" is crucial for their safe deployment. Our approach uses an anti-expert that models overt toxicity, as well as an expert that is finetuned on nontoxic data from the same domain.</p>
<p>Note that while obtaining an LM that is truly free from social biases is impossible (Fiske, 1993; Lakoff, 1973), the "non-toxic" expert serves the purpose of modeling the same domain of comments as the toxic anti-expert, providing more effective contrast. Nonetheless, we provide an ablation using only a toxic anti-expert and show that it remains effective above all previous baselines.</p>
<h3>3.1 Method</h3>
<p>We use GPT-2 Large as our base LM. For our expert and anti-expert, we finetune several sizes of GPT-2 (Small, Medium, Large) on a dataset of humanannotated comments from the Jigsaw Unintended Bias in Toxicity Classification Kaggle challenge. ${ }^{3}$ We consider an example toxic if $\geqslant 50 \%$ of annotators marked it as toxic, and nontoxic if none of the annotators mark it as toxic. This toxic dataset</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>has $\sim 160 \mathrm{~K}$ comments, and the nontoxic dataset $\sim 1.4 \mathrm{M}$ comments. Note that our toxic dataset is human-annotated and out-of-domain with respect to the pretraining corpus (WebText for GPT-2).</p>
<p>We report results for $\alpha=2.0$, chosen after observing the tradeoff between detoxification and fluency, but show results for other values of $\alpha$ in Appendix D.</p>
<h3>3.2 Evaluation</h3>
<h3>3.2.1 Generation Prompts</h3>
<p>To evaluate the problem of toxic degeneration where a user might unexpectedly receive harmful output from a model, we use a random sample of 10 K nontoxic prompts from the RealToxicityPrompts dataset (Gehman et al., 2020).</p>
<h3>3.2.2 Baselines</h3>
<p>Domain-adaptive pretraining (DAPT; Gururangan et al., 2020) We further pretrain the base model on the non-toxic subset of OpenWebText. This dataset is obtained by scoring the full OpenWebText corpus with the toxicity classifier from Perspective API ${ }^{4}$ and keeping the least toxic 2 percent of documents, a corpus of about 150 K documents, or 63 M tokens, following the implementation of this baseline from Gehman et al. (2020).</p>
<p>Plug-and-play language models (PPLM; Dathathri et al., 2020) PPLM uses gradients from a toxicity classifier to update the LM's hidden representations. We retrain the classifier to be compatible with our larger base model size, on the same toxicity data used in the original paper. ${ }^{5}$ Due to the extreme computational expense of PPLM (runtimes are shown in Appendix A.4), we evaluate PPLM on a random subset of 1 K prompts.</p>
<p>Generative discriminators (GeDi; Krause et al., 2020) GeDi uses a class-conditioned LM to provide classification probabilities for all possible next tokens via Bayes' rule. We use the toxicity classconditioned LM released by the authors with the recommended generation hyperparameters.</p>
<p>DExPERTS (anti-only) We also explore an anti-expert-only ablation of DExPERTS, by reusing the base model as the expert. To be clear, we substitute $\mathbf{z}<em t="t">{t}^{+}=\mathbf{z}</em>$ in Equation 1, so that we have</p>
<p>$$
\tilde{P}\left(X_{t} \mid \boldsymbol{x}<em t="t">{&lt;t}\right)=\operatorname{softmax}\left((1+\alpha) \mathbf{z}</em>\right)
$$}-\alpha \mathbf{z}_{t}^{-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Toxicity ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency ( $\downarrow$ )</th>
<th style="text-align: center;">Diversity ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Avg. max. toxicity</td>
<td style="text-align: center;">Toxicity prob.</td>
<td style="text-align: center;">Output ppl.</td>
<td style="text-align: center;">Dist-1</td>
<td style="text-align: center;">Dist-2</td>
<td style="text-align: center;">Dist-3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">25.45</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: left;">PPLM (10\%)</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">32.58</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: left;">Non-toxic expert</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">40.61</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: left;">DAPT</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">31.21</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: left;">GeDi</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">60.03</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (anti-only)</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">52.02</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (small)</td>
<td style="text-align: center;">$\mathbf{0 . 3 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 8}$</td>
<td style="text-align: center;">38.20</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (medium)</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">32.51</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (large)</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">32.41</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.84</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of experiments in detoxifying generations from GPT-2. DExPERTS (size) indicates the size of the (anti-)experts. Fluency is measured as perplexity of generated output according to a larger GPT-2 model. Diversity is measured as the count of unique $n$-grams normalized by the length of text. Toxicity is measured as the average maximum toxicity over 25 generations and the empirical probability of generating toxic text at least once over 25 generations, as judged by Perspective API. All models are evaluated on a dataset of 10 K nontoxic prompts from RealToxicityPrompts (Gehman et al., 2020), except PPLM, which is evaluated on a subset of 1 K prompts, due to the greater computational expense.</p>
<p>We use the toxic anti-expert based on GPT-2 Large and the same hyperparameter value $\alpha=2.0$.</p>
<p>Non-Toxic Expert Finally, we consider generating directly from the non-toxic expert based on GPT-2 Large.</p>
<p>For all baselines, we use nucleus sampling (Holtzman et al., 2020) with $p=0.9$ to generate up to 20 tokens. Note that for our method, nucleus sampling is done as described in $\S 2$, by using the nucleus from the base LM. Other training and generation details (e.g., hyperparameters) are described in Appendix A.</p>
<h3>3.2.3 Automatic Evaluation</h3>
<p>We evaluate our generations for toxicity, fluency, and diversity. Following previous work (Gehman et al., 2020), we characterize generation toxicity using the toxicity score from Perspective API, along two axes: 1) the maximum toxicity over $k=25$ generations, and 2) the empirical probability of generating a continuation with toxicity $\geqslant 0.5$ at least once over $k=25$ generations. Generation fluency is measured by the mean perplexity of generated continuations according to a larger pretrained LM, GPT-2 XL. Generation diversity is measured using the mean number of distinct $n$-grams, normalized by the length of text (Li et al., 2016), among the 25 generations for each prompt. We report Dist-1, Dist-2, and Dist-3 scores for distinct uni-, bi-, and trigrams, respectively.</p>
<p>Results According to automatic metrics shown in Table 1, DExPERTS substantially outperforms
all existing baselines at detoxification. In particular, DExPERTS (medium, large) are among the most fluent controllable generation methods, while fully preserving output diversity compared to the base model. Moreover, the DExPERTS (anti-only) ablation continues to outperform baselines at detoxification, although with a loss in fluency and diversity that is likely due to the less effective contrast between the base model and anti-expert. We report the per-generation runtime of each method in Appendix A. 4 to demonstrate DExPERTS's efficiency compared to other decoding-time methods.</p>
<h3>3.2.4 Human Evaluation</h3>
<p>While automatic toxicity classifiers like Perspective API enable the kind of large-scale evaluation required for systematic comparison of methods, an abundance of work shows that their accuracy is far from ideal (Dixon et al., 2018; Sap et al., 2019; Davidson et al., 2019; Hutchinson et al., 2020) in part due to reliance on spurious features, which we discuss in $\S 8$. Therefore, we carry out a human evaluation on Amazon Mechanical Turk on 120 random prompts from the 10 K nontoxic subset. For each prompt, we compare four pairs of models: DExPERTS (large) versus GPT-2 Large, PPLM, DAPT, and GeDi. For each pair of models, we randomly sample two generations from each model. This results in a total of 120 prompts $\times 4_{\text {prompt }}^{\text {pairings }} \times 2_{\text {pairing }}^{\text {generations }}=960$ comparisons. Each comparison pair is rated by three Turkers, who select which of the two continuations is: (1) less toxic, (2) more fluent, and (3) more topical, i.e., whether the continuation is natural,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of human evaluation for detoxification. DEXPERTS is rated as less toxic more often than every baseline, and equally fluent compared to the base model, GPT-2.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Toxicity (↓)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Avg. max. toxicity</td>
<td>Toxicity prob.</td>
</tr>
<tr>
<td>GPT-3</td>
<td>0.525</td>
<td>0.515</td>
</tr>
<tr>
<td>DExPERTS (large)</td>
<td>0.293</td>
<td>0.111</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of experiments in detoxifying generations from GPT-3.</p>
<p>relevant, and follows logically from the prompt. A screenshot of the user interface is provided in Appendix C.</p>
<p><strong>Results</strong> According to human evaluations, DExPERTS is rated as less toxic more often than all baselines (Figure 2). In particular, it is rated equally fluent compared to GPT-2, yet less toxic than GPT-2 10% more often than the other way around. See Appendix E for examples of generations.</p>
<h3>3.3 Steering GPT-3</h3>
<p>We next use DExPERTS to steer GPT-3 Ada. Because the OpenAI API allows access to only the top 100 log probabilities at each time step, we can only modify and sample from the probability distribution over the top 100 tokens. Nonetheless, results in Table 2 show that DExPERTS effectively reduces toxicity from GPT-3 to about the same level as when operating on GPT-2. This demonstrates that DExPERTS requires only the output of the base model, and indeed, the (anti-)experts do not need to be built on the base model.</p>
<h3>3.4 Analysis: Dataset Size</h3>
<p>In practice, gathering large amounts of toxic data may be challenging, especially in applications where we would want to customize the anti-expert LM for differing notions of harmful language. To explore the limited data setting, we investigate the relationship between the dataset size used to train the (anti-)experts and its effectiveness at steering the base model. We finetune GPT-2 Large</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of DExPERTS when (anti-)experts are trained on differently-sized datasets and evaluated at different checkpoints, calculated on a subset of 1K prompts. For comparison, recall the avg. max. toxicity of GPT-2 is 0.527.</p>
<p>on five different dataset sizes of exactly 40,960, 204.8K, 1.024M, 5.12M, and 10.24M tokens; for each dataset size, we train the expert and antiexpert for one epoch with checkpoints at every fifth of an epoch. The performance of each ensemble, at every (anti-)expert checkpoint, is shown in Figure 3.</p>
<p>We can see that even with a dataset of 40,960 tokens (~650 comments) corresponding to &lt; 0.4% of the original toxic dataset, we substantially reduce toxicity from the base model to about the same level as our strongest baseline, GeDi. (On one GPU, this corresponds to ~3 minutes of finetuning.) Nonetheless, as the size of the finetuning dataset for (anti-)experts increases, the performance of DExPERTS increases as well.</p>
<h3>4 Sentiment-Controlled Generation</h3>
<p>As a second application we consider the well-studied task of controlling the polarity of text's sentiment (e.g., Li et al., 2018; Sudhakar et al., 2019), steering towards either positive or negative sentiment.</p>
<h4>4.1 Method</h4>
<p>We use the same pretrained model from §3, GPT-2 Large, as our base LM. We finetune GPT-2 (Small,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Target <br> Sentiment</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">\% Positive Sentiment</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency ( $\downarrow$ )</th>
<th style="text-align: center;">Diversity ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Positive prompts</td>
<td style="text-align: center;">Neutral prompts</td>
<td style="text-align: center;">Negative prompts</td>
<td style="text-align: center;">Output ppl.</td>
<td style="text-align: center;">Dist-1</td>
<td style="text-align: center;">Dist-2</td>
<td style="text-align: center;">Dist-3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (large)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.46</td>
<td style="text-align: center;">36.42</td>
<td style="text-align: center;">45.83</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (medium)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.31</td>
<td style="text-align: center;">33.20</td>
<td style="text-align: center;">43.19</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (small)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.57</td>
<td style="text-align: center;">31.64</td>
<td style="text-align: center;">42.08</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">Positive</td>
<td style="text-align: center;">GeDi</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">86.01</td>
<td style="text-align: center;">26.80</td>
<td style="text-align: center;">58.41</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Positive expert</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">79.83</td>
<td style="text-align: center;">43.80</td>
<td style="text-align: center;">64.32</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.24</td>
<td style="text-align: center;">14.17</td>
<td style="text-align: center;">30.52</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (anti-only)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60.72</td>
<td style="text-align: center;">4.43</td>
<td style="text-align: center;">46.00</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CTRL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">61.81</td>
<td style="text-align: center;">18.88</td>
<td style="text-align: center;">43.79</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PPLM (10\%)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">52.68</td>
<td style="text-align: center;">8.72</td>
<td style="text-align: center;">142.11</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">99.08</td>
<td style="text-align: center;">50.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">29.28</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PPLM (10\%)</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">39.05</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">181.78</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CTRL</td>
<td style="text-align: center;">79.05</td>
<td style="text-align: center;">37.63</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">35.94</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (anti-only)</td>
<td style="text-align: center;">93.75</td>
<td style="text-align: center;">34.05</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.23</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">Negative</td>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">87.43</td>
<td style="text-align: center;">33.28</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.86</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Negative expert</td>
<td style="text-align: center;">61.67</td>
<td style="text-align: center;">24.32</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">65.11</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GeDi</td>
<td style="text-align: center;">39.57</td>
<td style="text-align: center;">8.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">84.11</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (small)</td>
<td style="text-align: center;">45.25</td>
<td style="text-align: center;">3.85</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">39.92</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (medium)</td>
<td style="text-align: center;">40.21</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.47</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DExPERTS (large)</td>
<td style="text-align: center;">35.99</td>
<td style="text-align: center;">3.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45.91</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.83</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for experiments in sentiment-controlled generation. We consider three sets of prompts relative to the base LM: neutral prompts, which are equally likely to lead to positive and negative generations, as well as positive prompts and negative prompts, which lead to overwhelmingly positive and negative generations, respectively. Sentiment is measured as the mean percentage of positive generations of out of the 25 continuations for each prompt, according to HuggingFace's sentiment analysis classifier. Higher is better for positive steering (top); lower is better for negative steering (bottom).</p>
<p>Medium, Large) on a positive sentiment corpus for our positive LM, and on a negative sentiment corpus for our negative LM. We use Stanford Sentiment Treebank (SST-5; Socher et al., 2013), which contains movie reviews labeled by human raters for sentiment on a scale from 1 (very negative) to 5 (very positive). Our positive dataset contains "positive" and "very positive" reviews, and our negative dataset "negative" or "very negative" reviews. Each of these sentiment datasets has about 4 K reviews.</p>
<p>For ease of notation we consider the positive LM our expert and negative LM our anti-expert, and use $\alpha= \pm 3.2$ for steering in each direction. The tradeoff between fluency and sentiment control for many values of $\alpha$ is shown in $\S 4.3$.</p>
<h3>4.2 Evaluation</h3>
<h3>4.2.1 Generation Prompts</h3>
<p>In order to test our method's ability to control sentiment beyond the domain that the sentiment experts are trained on (movie reviews), we collect a dataset of 100 K naturally occurring prompts from the OpenWebText Corpus (OWT) (Gokaslan and Cohen, 2019). Details are outlined in Appendix B. We generate 25 continuations for each prompt from
the base LM, and score them using HuggingFace's sentiment analysis classifier (Wolf et al., 2020) trained on SST-5 movie reviews. Using these generations from the base LM, we build three datasets of prompts: (1) 5K "neutral" prompts, which lead to 12 or 13 positive continuations, (2) 2.5K "negative" prompts, which lead to 25 negative continuations, and (3) 2.5 K "positive" prompts, which lead to 24 or 25 positive continuations. We consider the negative and positive prompts adversarial settings, where the task is to steer toward the opposite sentiment of the prompt.</p>
<h3>4.2.2 Baselines</h3>
<p>We consider the same baselines as in $\S 3$, along with a new baseline (CTRL; Keskar et al., 2019).</p>
<p>DAPT Corresponding to our DAPT baseline in §3, we score all documents in OpenWebText with the HuggingFace sentiment classifier, and keep the most positive $2 \%$ and most negative $2 \%$ (according to the probability of the predicted label) to obtain the positive and negative corpora. We perform another round of pretraining on each corpus to obtain a positive LM and negative LM.</p>
<p>PPLM As with toxicity §3, we retrain the sentiment classifier for PPLM with a larger embedding size compatible with our base model. The training data used is SST-5. Again, we evaluate PPLM on only $10 \%$ of the prompts compared to other models, which are randomly selected: 500 neutral prompts, 250 positive prompts, and 250 negative prompts.</p>
<p>GeDi We use GeDi with the sentiment classconditioned LMs released by the original authors, which are trained on IMDB movie reviews (Maas et al., 2011). (We find that retraining it on SST-5 results in slightly reduced performance, as discussed in Appendix A.)</p>
<p>DEXPERTS (anti-only) To explore whether simply steering away from one sentiment will yield the opposite sentiment, we again explore an anti-expert-only version of DExPERTS. As in §3, we reuse the base model as the expert, and use only a negative anti-expert LM for positive steering, and only a positive anti-expert LM for negative steering. We use $\alpha= \pm 2.0$ for this setting.</p>
<p>Positive/Negative Experts Again, we consider decoding directly from the corresponding sentiment expert for positive and negative steering.</p>
<p>Conditional Transformer LM (CTRL; Keskar et al., 2019) To control the sentiment of generations from CTRL , we use the "Reviews" control code and append a rating of " 5.0 " for positive generations and a rating of " 1.0 " for negative generations. The sentiment training examples for CTRL came from Amazon reviews (McAuley et al., 2015).</p>
<p>As with toxicity experiments (§3), we use nucleus sampling with $p=0.9$, and include our training and generation details in Appendix A.</p>
<h3>4.2.3 Automatic Evaluation</h3>
<p>We evaluate our generations for the target sentiment, fluency, and diversity. To estimate sentiment, we use HuggingFace's sentiment analysis classifier, and report the mean percentage of generations per prompt (out of 25) which are labeled positive (the rest are negative). We evaluate fluency and diversity in the same ways as $\S 3$.</p>
<p>Results As shown in Table 3, DExPERTS greatly outperforms previous controllable generation methods (PPLM, CTRL, DAPT, GeDi) on both neutral prompts and adversarial prompts. The limited performance of CTRL suggests that the effectiveness of class-conditioned training on domain-specific
data is limited to the domain of that data; training on Amazon reviews does not allow generalization outside of the reviews domain. In a similar vein, while the positive and negative experts achieve decent performance (even performing the best on negative prompts), they do so at the expense of much higher output perplexity. This contrast shows two sides of the same coin: we observe that while CTRL acts like a standard language model on out-of-domain prompts (good fluency, poor control), the sentiment experts are highly specialized on movie reviews and tend to steer every generation toward movies (poor fluency, strong control). Meanwhile, DAPT is more effective while maintaining fluency, because its training domain is the same domain as the prompts domain (i.e., OWT), but its performance decreases substantially in the adversarial setting which requires more active steering. We observe that the poor fluency of PPLM is due to occasional generations with extremely high perplexity, suggesting cases of degenerate behavior. DExPERTS with only an anti-expert is mildly effective on neutral prompts (outperforming or matching the performance of CTRL and PPLM), but works very poorly in the adversarial setting, confirming our intuition that steering away from negative sentiment does not provide sufficiently strong guidance for positive sentiment.</p>
<h3>4.2.4 Human Evaluation</h3>
<p>For human evaluation, we randomly choose 30 neutral prompts, 30 positive prompts, and 30 negative prompts, and consider five pairs of models: DExPERTS versus GPT-2, CTRL, PPLM, DAPT, and GeDi. For each prompt and pairing of models, we sample two generations from each model for each steering direction considered. This results in a total of 120 prompts $\times 5_{\text {prompt }}^{\text {pairings }} \times 2^{\frac{\text { generations }}{\text { pairing }}}=1200$ pairs, each rated by 3 MTurk workers. We ask annotators to select which generation achieves the desired sentiment better, along with the fluency and topicality questions from §3.2.4.</p>
<p>Results As shown in Figure 4, DExPERTS is substantially more effective at steering toward positivity on negative prompts while achieving better topicality and better fluency compared to all other baselines, including GPT-2. In the opposite setting of steering toward negativity on positive prompts, the gap in sentiment control performance between DExPERTS and each of GPT-2, CTRL, DAPT, and PPLM is even more pronounced: DExPERTS is</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of human evaluation for steering toward positivity on negative prompts (left) and steering toward negativity on positive prompts (right). DExPERTS is substantially more effective at achieving the desired sentiment over every baseline.</p>
<p>rated better than its comparison 62–78% of the time. While GeDi achieves close to DExPERTS' performance in this setting, its topicality and fluency are much worse. The asymmetry, where negative steering appears easier than positive steering for DExPERTS, is reflected in automatic evaluation as well. We hypothesize that it is easier to derail a positive prompt with negativity than turn something negative into something positive; but to human readers, these negative continuations may be unexpected (a similar observation was made in previous work; Madotto et al., 2020). For the neutral prompts, we see similar trends as those in the automatic and the human adversarial evaluations. Due to space constraints, we include those in Appendix D.2.</p>
<h3>4.3 Analysis: Sentiment versus Fluency</h3>
<p>In practice, we may want different levels of sentiment control depending on the application (e.g., aggressively positive marketing pitches versus merely friendly chatbots). Figure 5 shows the relationship between output sentiment and fluency for different choices of α ∈ [−3.4, 3.4], conditioned on neutral prompts. The smooth tradeoff suggests that α can by adjusted by a practitioner or user, depending on their application. In our experiments, we pick α = ±3.2 because the curve becomes less steep, meaning that a greater cost in fluency does not re-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The relationship between output fluency and positivity for different values of α ∈ [−3.4, 3.4]. We choose α = ±3.2 in our experiments. Results are calculated on a subset of 1K neutral prompts.</p>
<p>turn as great of an increase in the desired sentiment. The tradeoff between output toxicity and fluency looks very similar for DExPERTS detoxification (§3), and is included in Appendix D.1.</p>
<h2>5 Stylistic Rewriting with DExPERTS</h2>
<p>As a preliminary exploration, we go beyond generating text continuations to apply DExPERTS to stylistic rewriting, i.e., rewriting a sentence in a target style while preserving as much content as possible. We replace the base model with a pretrained</p>
<p>autoencoder, BART (Lewis et al., 2020), and use GPT-2 Large sentiment (anti-)experts from $\S 4$ for steering. At each time step, the autoencoder base model conditions on both the input sequence and the generation-so-far, whereas the (anti-)experts condition on only the latter. As a proof of concept, we show some examples of input/output from this system in Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Input $\rightarrow$ Output Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">I love cats and seeing them play with yarn.</td>
</tr>
<tr>
<td style="text-align: left;">$\alpha=-4.0$</td>
</tr>
<tr>
<td style="text-align: left;">$\alpha=-3.5$</td>
</tr>
<tr>
<td style="text-align: left;">Oatmilk is tasty and good for the environment.</td>
</tr>
<tr>
<td style="text-align: left;">Great food but horrible staff and very very rude workers!</td>
</tr>
<tr>
<td style="text-align: left;">$\alpha=2.0$</td>
</tr>
<tr>
<td style="text-align: left;">$\alpha=2.0$ A very nice restaurant</td>
</tr>
</tbody>
</table>
<p>Table 4: Examples of input/output from a preliminary system that applies DExPERTS to stylistic rewriting. Recall $\alpha&gt;0$ indicates positive rewriting, and $\alpha&lt;0$ indicates negative rewriting.</p>
<p>This exploration suggests that more innovation is required to apply DExPERTS to stylistic rewriting, but it is a promising direction. We anticipate future work on the subject.</p>
<h2>6 Related Work</h2>
<p>The task of controlling the output of a language generation model has been widely studied by previous work (for a review, see Prabhumoye et al., 2020). Prior to using pretrained LMs as a backbone, most work used custom neural models trained for their respective downstream generation tasks, including emotion-aware text generation (Ghosh et al., 2017; Ficler and Goldberg, 2017), attribute-aware product review generation (Dong et al., 2017), and friendly or empathetic dialogue response generation (See et al., 2019; Rashkin et al., 2019).</p>
<p>Since pretrained LMs have shown impressive text generation ability (Radford et al., 2018, 2019), two directions have emerged to control their language generation: training approaches and decoding-time approaches. Training approaches include finetuning the pretrained LMs on datasets that contain the desired attributes (Gururangan et al., 2020) as well as creating a class-conditioned pretrained LM trained on text with specific attributes control code prefixes (Keskar et al., 2019). In contrast to our method, such approaches can only steer towards desired text attributes, they cannot steer away from them. Additionally, training approaches require significant computational resources, which may no longer be feasible with the size of more recent pretrained LMs (Brown et al., 2020; Fedus et al., 2021).</p>
<p>Decoding-time methods, a more lightweight approach, have been used controlling the attributes of generated text, as well as for improving its quality (Li et al., 2016; Holtzman et al., 2018; Welleck et al., 2020). PPLM (Dathathri et al., 2020) is a steering method that updates a pretrained model's hidden representations according to the gradient of a classifier with respect to the desired class. Unfortunately, this approach is computationally expensive, as shown in this and previous work (Gehman et al., 2020). Contemporaneous with our work, FUDGE (Yang and Klein, 2021) trains classifiers on partial sequences to predict whether an attribute will be satisfied in the future, and uses Bayesian factorization to obtain the attribute-conditioned probability distribution. GeDi (Krause et al., 2020) uses Bayes' rule similarly, but computes classification probabilities using the output of class-conditioned LMs rather than directly training a classifier. In contrast, our experiments show that directly ensembling LMs' probabilities as opposed to using them for estimating class probabilities is more effective at steering text generation.</p>
<h2>7 Conclusion</h2>
<p>We present DExPERTS, a method for controlled text generation that reweights the predictions of language models based on expert (and anti-expert) opinions. In experiments for two different tasks, detoxification and sentiment control, we show that our method is able to effectively steer the language model towards the desired generations, while preserving the fluency and diversity of generated text. As applications built on language models become ubiquitous, DExPERTS demonstrates promise in steering these models toward safe and user-friendly generations.</p>
<h2>Acknowledgments</h2>
<p>This research is supported in part by NSF (IIS1714566), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and Allen Institute for AI. We thank OpenAI, specifically Bianca Martin and Miles Brundage, for providing access to GPT-3 through the OpenAI API Academic Access Program. We also thank UW NLP, AI2 Mosaic, and the anonymous reviewers for helpful feedback.</p>
<h2>8 Broader Impact and Ethical Implications</h2>
<p>Our study is motivated by the potential harms of using pretrained language models (Bender et al., 2021), specifically their tendency to generate hateful, offensive, or toxic content (Sheng et al., 2020; Gehman et al., 2020). Part of our work requires automatically detecting toxicity in generated texts, for which we use the Perspective API. ${ }^{7}$ a commercially deployed toxicity detection tool. However, the mismatch between the construct of toxicity and its operationalization through an automatic classifier can cause biased or unintended model behavior (Jacobs and Wallach, 2021). Specifically, recent work has shown that such hate speech classifiers overestimate the prevalence of toxicity in text that contains a minority identity mention (Hutchinson et al., 2020; Dixon et al., 2018) or text written by racial minorities (Sap et al., 2019; Davidson et al., 2019), therefore having the real possibility of backfiring against its very aim of fairness and inclusive dialogue. To address this limitation, we also perform a human evaluation of toxicity, for which we obtained IRB approval and sought to pay our workers a fair wage ( US\$7-9/h).</p>
<p>We also acknowledge that any controllable detoxification method runs the risk of dual use (Pandya, 2019), specifically, this technology could be used to automatically generate hateful text (e.g., extremist texts; McGuffie and Newhouse, 2020). For a broader discussion of such risks, and of the risks of large pretrained LMs in general, please see Bender et al. (2021).</p>
<p>Nevertheless, toxicity in pretrained LMs is an unsolved issue (Sheng et al., 2019; Gehman et al., 2020). Therefore, we hope future work continues to better define and evaluate the presence of harmful language (e.g., Sap et al., 2020), and to develop systems for mitigating such language that can be personalized to users' diverse experiences with language (e.g., dealing with reclaimed slurs appropriately; Croom, 2013).</p>
<h2>References</h2>
<p>Emily Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ence on Fairness, Accountability, and Transparency (FAccT).</p>
<p>Steven Bird and Edward Loper. 2004. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions.</p>
<p>Greg Brockman, Mira Murati, and Peter Welinder. 2020. OpenAI API. Blog post.
T. Brown, B. Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, G. Krüger, T. Henighan, R. Child, Aditya Ramesh, D. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, E. Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, J. Clark, Christopher Berner, Sam McCandlish, A. Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS).</p>
<p>Adam M Croom. 2013. How to do things with slurs: Studies in the way of derogatory words. In Language \&amp; communication.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In Proceedings of the 2020 International Conference on Learning Representations (ICLR).</p>
<p>Thomas Davidson, Debasmita Bhattacharya, and Ingmar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online.</p>
<p>Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018. Measuring and mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES).</p>
<p>Li Dong, Shaohan Huang, Furu Wei, Mirella Lapata, Ming Zhou, and Ke Xu. 2017. Learning to generate product reviews from attributes. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv.</p>
<p>Jessica Ficler and Yoav Goldberg. 2017. Controlling linguistic style aspects in neural language generation. In Proceedings of the Workshop on Stylistic Variation.</p>
<p>Susan T Fiske. 1993. Controlling other people: the impact of power on stereotyping. American Psychologist.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics (EMNLP Findings).</p>
<p>Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, and Stefan Scherer. 2017. Affect-LM: A neural language model for customizable affective text generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus.</p>
<p>Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Geoffrey E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. In Neural Computation.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proceedings of the Eighth International Conference on Learning Representations (ICLR).</p>
<p>Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl. 2020. Social biases in NLP models as barriers for persons with disabilities. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Abigail Z. Jacobs and Hannah Wallach. 2021. Measurement and fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT).</p>
<p>Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. 2019. CTRL: A conditional transformer language model for controllable generation. arXiv.</p>
<p>Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Jory, Richard Socher, and Nazneen Fatema Rajani. 2020. GeDi: Generative discriminator guided sequence generation. arXiv.</p>
<p>Robin Lakoff. 1973. Language and woman's place. Language in Society.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Andrea Madotto, Etsuko Ishii, Zhaojiang Lin, Sumanth Dathathri, and Pascale Fung. 2020. Plug-and-play conversational models. In Findings of the Association for Computational Linguistics (EMNLP Findings).</p>
<p>Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR).</p>
<p>Kris McGuffie and Alex Newhouse. 2020. The radicalization risks of gpt-3 and advanced neural language models. arXiv.</p>
<p>Jayshree Pandya. 2019. The dual-use dilemma of artificial intelligence. Forbes Magazine.</p>
<p>Shrimai Prabhumoye, Alan W Black, and Ruslan Salakhutdinov. 2020. Exploring controllable text generation techniques. In Proceedings of the 28th International Conference on Computational Linguistics (COLING).</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Preprint.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Preprint.</p>
<p>Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic opendomain conversation models: A new benchmark and</p>
<p>dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The risk of racial bias in hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston. 2019. What makes a good conversation? how controllable attributes affect human judgments. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2020. Towards Controllable Biases in Language Generation. In Findings of the Association for Computational Linguistics (EMNLP Findings).</p>
<p>Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran. 2019. "Transforming" delete, retrieve, generate approach for controlled text style transfer. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In Proceedings of the Eighth International Conference on Learning Representations (ICLR).</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,</p>
<p>Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations.</p>
<p>Kevin Yang and Dan Klein. 2021. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<h2>Appendix Overview</h2>
<p>In this supplemental material, we provide additional information for producing the results of the paper and additional results.</p>
<h2>A Modeling Details</h2>
<h2>A. 1 Out of the Box Models</h2>
<p>We use HuggingFace Transformers (Wolf et al., 2020) versions of all pretrained models (aside from GPT-3), implemented in the PyTorch deep learning framework. For GPT-3, we use the Ada model which is accessed with the OpenAI API. ${ }^{8}$</p>
<h2>A. 2 Training Details</h2>
<p>All training is performed on a single NVIDIA Quadro 6000 GPU.</p>
<p>DExPERTS Hyperparameters for finetuning (anti-)experts for DExPERTS are given in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">model</td>
<td style="text-align: center;">GPT-2 (S/M/L)</td>
</tr>
<tr>
<td style="text-align: center;">number of parameters</td>
<td style="text-align: center;">124M / 355M / 774M</td>
</tr>
<tr>
<td style="text-align: center;">number of steps</td>
<td style="text-align: center;">1-3 epochs</td>
</tr>
<tr>
<td style="text-align: center;">effective batch size</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">block size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">learning rate optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-8$</td>
</tr>
<tr>
<td style="text-align: center;">Adam initial learning rate</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;">learning rate scheduler</td>
<td style="text-align: center;">linear with no warmup</td>
</tr>
<tr>
<td style="text-align: center;">weight decay</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 5: Hyperparameters for finetuning (anti-)experts for DExPERTS and continued pretraining in domainadaptive pretraining (DAPT). We finetune the sentiment (anti-)experts and all DAPT models for 3 epochs, and the toxicity (anti-)experts for one epoch.</p>
<p>The finetuning time for each model size is shown in Table 6.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Non-toxic</th>
<th style="text-align: center;">Toxic</th>
<th style="text-align: center;">Positive</th>
<th style="text-align: center;">Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">2h:45m</td>
<td style="text-align: center;">18m:01s</td>
<td style="text-align: center;">34 s</td>
<td style="text-align: center;">32 s</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">7h:06m</td>
<td style="text-align: center;">46m:52s</td>
<td style="text-align: center;">1 m :30s</td>
<td style="text-align: center;">1 m :24s</td>
</tr>
<tr>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">14h:35m</td>
<td style="text-align: center;">1h:37m</td>
<td style="text-align: center;">3 m :19s</td>
<td style="text-align: center;">3 m :01s</td>
</tr>
</tbody>
</table>
<p>Table 6: Finetuning time for (anti-)experts in DExPERTS, for each GPT-2 size used.</p>
<p>DAPT For our implementation of DAPT in sentiment experiments ( $\S 4$ ), we use HuggingFace's sentiment analysis classifier to filter documents from OpenWebText () for the most positive $2 \%$ and most negative $2 \%$ of documents. Because the classifier takes a maximum of 512 tokens as input text, we approximate the sentiment of a document with its first 510 tokens (a start and end token are added by the classifier). The hyperparameters for the additional phase of pretraining on the attribute data is given in Table 5.</p>
<p>PPLM For our implementation of PPLM in experiments, we retrain the toxicity and sentiment classifiers to be compatible with our base model GPT-2 (large), as the original paper used GPT-2 medium for experiments. We use the same training datasets and hyperparameters as in the original PPLM paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">embedding size</td>
<td style="text-align: center;">1280</td>
</tr>
<tr>
<td style="text-align: center;">number of steps</td>
<td style="text-align: center;">10 epochs</td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">64</td>
</tr>
</tbody>
</table>
<p>Table 7: Hyperparameters for training the attribute classifiers used for PPLM.</p>
<p>GeDi For toxicity and sentiment steering, we download the class-conditioned language models (based on GPT-2 Medium) made available by the original authors. As an experiment, we also align the finetuning data for the sentiment GeDis and the (anti-)experts used in DExperts by finetuning a new class-conditioned LM on SST-5 data (as opposed to IMDB used by in GeDi). We found slightly lower performance on sentiment control $(\sim 1-2 \%)$ across the settings, and therefore use the original class-conditioned LMs.</p>
<h2>A. 3 Dataset Details</h2>
<p>Details of datasets used for further pretraining in the DAPT baselines are given in Table 8, and those
for finetuning our experts and anti-experts are given in Table 9 and Table 10.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset size</th>
<th style="text-align: center;">Non-toxic</th>
<th style="text-align: center;">Positive</th>
<th style="text-align: center;">Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Tokens</td>
<td style="text-align: center;">$63,457,536$</td>
<td style="text-align: center;">$13,240,192$</td>
<td style="text-align: center;">$57,805,184$</td>
</tr>
<tr>
<td style="text-align: center;">Documents</td>
<td style="text-align: center;">$1,320,876$</td>
<td style="text-align: center;">264,837</td>
<td style="text-align: center;">$1,208,186$</td>
</tr>
</tbody>
</table>
<p>Table 8: Dataset details for subsets of OpenWebText used to obtain the DAPT models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset size</th>
<th style="text-align: center;">Non-toxic</th>
<th style="text-align: center;">Toxic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Tokens</td>
<td style="text-align: center;">$91,856,000$</td>
<td style="text-align: center;">$10,262,144$</td>
</tr>
<tr>
<td style="text-align: center;">Comments</td>
<td style="text-align: center;">$1,401,762$</td>
<td style="text-align: center;">159,782</td>
</tr>
</tbody>
</table>
<p>Table 9: Dataset details for toxicity (anti-)experts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset size</th>
<th style="text-align: center;">Positive</th>
<th style="text-align: center;">Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Tokens</td>
<td style="text-align: center;">116,480</td>
<td style="text-align: center;">108,800</td>
</tr>
<tr>
<td style="text-align: center;">Movie reviews</td>
<td style="text-align: center;">4,963</td>
<td style="text-align: center;">4,650</td>
</tr>
</tbody>
</table>
<p>Table 10: Dataset details for sentiment (anti-)experts.</p>
<h2>A. 4 Generation Details</h2>
<p>Generation hyperparameters shared among all methods are shown in Table 11. Hyperparameters for PPLM generation are shown in Table 12. Following the recommendation of the authors, we performed a hyperparameter search for step size over the values ${0.02,0.06,0.10,0.20,0.40}$, and for number of iterations over the values ${10,20,40,60}$, over a small sample of twenty nontoxic prompts. We picked step size 0.20 and 10 iterations, for the best tradeoff between toxicity reduction and output fluency. Due to the extreme computational expense of this method, we were not able to repeat the hyperparameter search for sentiment prompts.</p>
<p>Hyperparameters for GeDi generation are shown in Table 13.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">number of samples</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">top-p (sampling)</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">temperature</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">max length</td>
<td style="text-align: center;">20</td>
</tr>
</tbody>
</table>
<p>Table 11: Hyperparameters for generation with all models.</p>
<p>We compare the runtime for each controllable generation method used in $\S 3$ in Table 14, all on a single NVIDIA Quadro 6000 GPU.. We see that</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">temperature</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">number of iterations</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">step size</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;">gamma</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">GM-scale</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">KL-scale</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">repetition penalty</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">grad length</td>
<td style="text-align: center;">100000</td>
</tr>
<tr>
<td style="text-align: center;">horizon length</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">window length</td>
<td style="text-align: center;">none</td>
</tr>
</tbody>
</table>
<p>Table 12: Hyperparameters for generation with PPLM. A description of each hyperparameter can be found in (Dathathri et al., 2020)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">posterior weighting exponent $(\omega)$</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">filter $\mathrm{p}(1-\rho)$</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;">target $\mathrm{p}(\tau)$</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;">repetition penalty scale</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">repetition penalty</td>
<td style="text-align: center;">1.2</td>
</tr>
</tbody>
</table>
<p>Table 13: Hyperparameters for generation with GeDi. A description of each hyperparameter can be found in (Krause et al., 2020)</p>
<p>DEXPERTS takes 2 to 3 times the time as decoding directly from the base model, depending on the size of the (anti-)experts. When using the same model size for the guiding language model as in GeDi (GPT-2 Medium), DExPERTS is more efficient than GeDi, and both methods are $100 \times$ faster than PPLM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Generation time (sec)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2 / DAPT</td>
<td style="text-align: center;">0.094</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (small)</td>
<td style="text-align: center;">0.186</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (medium)</td>
<td style="text-align: center;">0.240</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (anti-only)</td>
<td style="text-align: center;">0.248</td>
</tr>
<tr>
<td style="text-align: left;">GeDi</td>
<td style="text-align: center;">0.276</td>
</tr>
<tr>
<td style="text-align: left;">DExPERTS (large)</td>
<td style="text-align: center;">0.334</td>
</tr>
<tr>
<td style="text-align: left;">PPLM</td>
<td style="text-align: center;">25.39</td>
</tr>
</tbody>
</table>
<p>Table 14: Generation time (in seconds) per continuation of maximum length 20 tokens for toxicity experiments in $\S 3$, all run on the same architecture for comparison.</p>
<h2>B Collection of Sentiment Prompts</h2>
<p>We build our prompts for sentiment experiments (§4) from the OpenWebText Corpus (Gokaslan and Cohen, 2019), a corpus of English web text scraped from outbound links on Reddit. We randomly sample 100 K documents from OpenWebText and tokenize each document into sentences. Following the
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: A histogram of the number of positive generations out of 25 from GPT-2, conditioned on our sentiment prompts dataset of 100 k naturally occurring prompts.
creation of RealToxicityPrompts (Gehman et al., 2020), we split each sentence into the prompt, consisting of the first half of tokens, and the continuation, consisting of the remaining tokens. We keep only prompts that are between 4 and 10 tokens long (inclusive). For all tokenization, we use the NLTK library (Bird and Loper, 2004). This results in 140 M prompts, from which we randomly sample 100 K prompts.</p>
<p>For each of the 100 K prompts, we generate 25 continuations from our base model, GPT-2 (large), and score the continuations for sentiment using the HuggingFace sentiment classifier described in $\S 4$. The distribution of prompts with $n \in[0,25]$ positive continuations out of 25 is shown in Figure 6. Interestingly, we observe that more prompts have more negative continuations than positive continuations than vice versa. Based on these generations, we create three sets of prompts as described in $\S 4$.</p>
<h2>C Human Evaluation</h2>
<p>Our interface for human evaluation is shown in Figure 7. For each category, the annotator is allowed to choose either one of the continuations, or rate the two options as equal.</p>
<h2>D Additional Results</h2>
<h2>D. 1 Toxicity Hyperparameter Control</h2>
<p>Figure 8 shows the relationship between output toxicity and fluency for different values of $\alpha$ in our method. The relationship is smooth, reflecting the corresponding figure for sentiment in $\S 4.3$.</p>
<h2>D. 2 Human Evaluation on Neutral Prompts</h2>
<p>Figure 9 shows the results of human evaluation on sentiment control conditioned on neutral prompts.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The interface on Amazon Mechanical Turk used for collecting human evaluation in $\S 3$. The interface for positive and negative sentiment evaluation in $\S 4$ is equivalent, except replacing "less toxic" with "more positive" and "more negative," respectively.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The relationship between output fluency and toxicity for different values of $\alpha \in[1.0,2.2]$, which controls the strength of control. Results are calculated on a subset of 1 K nontoxic prompts.</p>
<h1>E Generation Examples</h1>
<p>Examples of generations from each method are given in Table 15 for detoxification (§3), and Table 16 for sentiment control (§4).</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Results of human evaluation comparing different methods' ability to control sentiment on neutral prompts, as an addendum to $\S 4$.</p>
<p>Prompt: She made out with him, took sexy pictures in a photo booth, and watched him...</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GPT-2</th>
<th style="text-align: left;">having sex with a still-pregnant woman in the bathroom. She brought him over to the</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PPLM</td>
<td style="text-align: left;">kiss the smiling teachers who showed up to see her. $\$ Her is the story of Kimberly Tur</td>
</tr>
<tr>
<td style="text-align: left;">DAPT</td>
<td style="text-align: left;">do it - with his brothers, neighbors, female employees at the bank and other bank customers who'd</td>
</tr>
<tr>
<td style="text-align: left;">GeDi</td>
<td style="text-align: left;">admire her enormous breasts for the first time. She laughed when she found out Bruce kept them in to</td>
</tr>
<tr>
<td style="text-align: left;">DEXPERTS</td>
<td style="text-align: left;">go about his daily chores like a good girlfriend should. A neat piece of work who wanted her to</td>
</tr>
<tr>
<td style="text-align: left;">$(\alpha=1.0)$</td>
<td style="text-align: left;">play drums on the rooftop patio with his band while another time his girlfriend came over and woke him up</td>
</tr>
</tbody>
</table>
<p>Table 15: Examples of generated continuations using different detoxification methods, conditioned on a nontoxic prompt (see §3) "She made out with him, took sexy pictures in a photo booth, and watched him..."</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt: Trust in automation can only evolve from...</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">within the market itself. No higher education, no corporate jobs, no content factories or on-demand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Positive steering</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PPLM</td>
<td style="text-align: center;">that default belief, Petersson says, "What is the best technology and automation tools employed in the</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CTRL</td>
<td style="text-align: center;">a strong foundation of good human skills and technology." The author gives examples where he feels</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">this trust is</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">people, not from machines. This is why I believe education is the key to unlocking automation for</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">education</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GeDi</td>
<td style="text-align: center;">within but must also nurture and empower all of our employees to help shape the future.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DExPERTS</td>
<td style="text-align: center;">integrating knowledge with innovation. Today's start-ups bring unique, complementary perspectives to</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$(\alpha=2.0)$</td>
<td style="text-align: center;">transform our planet and</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DExPERTS</td>
<td style="text-align: center;">awareness and acceptance of the importance of automation to business success and sustainability of</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$(\alpha=3.2)$</td>
<td style="text-align: center;">our communities," says Weibel</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Negative steering</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PPLM</td>
<td style="text-align: center;">the first to the last step. If we are serious about having the right engine for the right problem</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CTRL</td>
<td style="text-align: center;">an idea to a reality. The future of AI will be built by humans with intelligent machines creating new tools</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">the bottom-up. But some more traditional forms of automation have started to become mainstream, and it</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GeDi</td>
<td style="text-align: center;">bad code to worse developers that don't know what they're doing as well. That's why your</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DExPERTS</td>
<td style="text-align: center;">a level of dependence. Automation cannot truly be trusted when it reaches a level of dependence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$(\alpha=-2.0)$</td>
<td style="text-align: center;">on security</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DExPERTS</td>
<td style="text-align: center;">bad thinking: automation will fail because its logic is incoherent and artificial and does not add any value</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 16: Examples of generated continuations using different methods for controlled text generation, conditioned on the "neutral" prompt (see §4) "Trust in automation can only evolve from..."</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://openai.com/api/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://github.com/conversationai/ perspectiveapi
${ }^{5}$ https://bit.ly/3yQiCIo&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
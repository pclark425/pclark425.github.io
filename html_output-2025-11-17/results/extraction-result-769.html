<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-769 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-769</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-769</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-204800714</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1910.09532v3.pdf" target="_blank">Building Dynamic Knowledge Graphs from Text-based Games</a></p>
                <p><strong>Paper Abstract:</strong> We are interested in learning how to update Knowledge Graphs (KG) from text. In this preliminary work, we propose a novel Sequence-to-Sequence (Seq2Seq) architecture to generate elementary KG operations. Furthermore, we introduce a new dataset for KG extraction built upon text-based game transitions (over 300k data points). We conduct experiments and discuss the results</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e769.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e769.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextWorld KG Seq2Seq Updater</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-Sequence Transformer Knowledge Graph Update Module (TextWorld KG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based Seq2Seq model that generates text-form graph update operations (add/delete RDF triples) to maintain an agent's belief KG from the previous belief graph and a new text observation; the graph encoder can be a GCN, R-GCN, or R-GCN with learned relation embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TextWorld KG Seq2Seq KG-Updater</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The module takes as input the previous belief graph G_belief_{t-1} (RDF triple KG), the previous action A_{t-1}, and the new observation O_t (concatenated text). It consists of: (1) a text encoder (embedding layer + stack of transformer blocks) producing h_O; (2) a graph encoder (variants: none baseline, GCN, R-GCN, R-GCN with learned relation label embeddings) producing h_G; (3) an attention-based representation aggregator that produces cross-conditioned representations h_GO and h_OG; and (4) a transformer-based decoder command generator with a pointer-softmax mechanism that outputs a token sequence encoding a sequence of update operations (add/delete(node1,node2,relation)). The model is trained with teacher forcing (NLL loss) and at test time can run autoregressively; the generated operations Δg_t are applied by an oracle Update function to produce G_belief_t.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (TextWorld KG dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based, deterministic game instances generated by TextWorld where the agent receives textual observations after actions. The agent only observes part of the full world state (partial observability); object names vary across games and can be multi-token, layouts and object placements vary, and many actions (e.g., 'go') yield observations that omit prior-location information, making state tracking and generalization challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>none (the experiments do not use external tools such as OpenIE or external map services; all KG construction/updating is learned end-to-end by the Seq2Seq model and internal GNN encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A symbolic/structured belief graph (G_belief_t) represented as RDF triples (nodes = entities/states/locations; edges = relations). The graph is encoded into distributed vectors via graph neural networks (GCN or R-GCN variants) and used by the decoder to produce update operations; the belief itself is stored as explicit graph structure (triples).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Belief updates are produced as sequences of text commands (add(node1,node2,relation) and delete(node1,node2,relation)). Given O_t and G_belief_{t-1}, the Seq2Seq generator outputs Δg_t; an oracle Update function applies Δg_t to G_belief_{t-1} to yield G_belief_t (adding nodes if missing, ignoring deletes of non-existent edges). Ground-truth operation orderings are fixed by rules (e.g., add before delete) during training.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Not implemented in this work — the paper focuses on a learned KG-update module (supervised Seq2Seq generation) rather than integrating a planner; intended to support downstream RL planners (graph-based RL) but no planning algorithm is evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The learned KG-update module can accurately generate add/delete operations when conditioned on the previous belief graph and new observation; models that encode relational structure (R-GCN) outperform simple GCNs, and R-GCNs with learned relation-label embeddings achieve significantly better free-run (cascading-error) performance. Conditioning on a belief graph improves handling of navigation ('go') actions where the immediate observation lacks prior-location information. Errors accumulate in free-run evaluation, and actions that consume multiple ingredients (prepare) are challenging because observations omit consumed constituents and require retrieving multiple facts from the belief graph.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building Dynamic Knowledge Graphs from Text-based Games', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e769.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e769.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ammanabrolu&Riedl KG-based agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-based RL agent using OpenIE and game-specific rules (Ammanabrolu and Riedl, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based deep reinforcement learning agent for text-adventure games that leverages an explicitly constructed knowledge graph; the KG is built using OpenIE extractions combined with hand-crafted game-specific rules to extract and maintain entities and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-based deep RL agent (Ammanabrolu & Riedl 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in the related work: the agent constructs a knowledge graph from text observations by applying OpenIE to extract entities/relations and using several game-specific heuristics/rules to build and update the KG; the resulting graph is used as structured memory to inform a graph-based deep RL policy for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-adventure games (text-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-based environments where the agent receives textual scene descriptions and must act to achieve goals; partial observability arises from limited sensorimotor access to rooms/objects and the need to remember prior observations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>OpenIE (Open Information Extraction) for entity and relation extraction; additionally, several domain/game-specific rule modules (heuristics) are used to post-process and maintain the KG.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured relation tuples/triples (extracted subject-relation-object triples and entity mentions), i.e., structured triplets derived from text.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>An explicit knowledge graph constructed from OpenIE extractions and game-specific rules; the KG serves as the agent's memory/belief about observed entities, locations, and states.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The agent ingests OpenIE extractions and applies game-specific rules to add/update nodes and edges in the KG; specifics of add/delete mechanics are not detailed here (only referenced), but the pipeline uses extraction outputs to populate and maintain the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Graph-based deep reinforcement learning (learned policy) that conditions on the constructed KG; the KG is used to inform action selection by the RL agent rather than performing explicit search-based planning within the paper's description here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Related work highlights that leveraging an explicit KG (constructed with OpenIE and rules) can improve performance in text games by providing structured memory; however, relying on an external extractor plus hand-crafted rules reduces generality and requires domain knowledge (not learned end-to-end).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building Dynamic Knowledge Graphs from Text-based Games', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e769.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e769.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Das et al. MRC-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic KG construction using Machine Reading Comprehension (Das et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that leverages a machine reading comprehension (MRC) mechanism to query text for entities and states, using attention to resolve aliasing and dynamically track entity states across short text passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Building dynamic knowledge graphs from text using machine reading comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MRC-conditioned KG builder (Das et al. 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses a machine reading comprehension module to query input text for entities and their states; attention mechanisms are used to handle aliased mentions and to dynamically update entity state representations, effectively building and maintaining a dynamic KG from local textual context.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Short textual passages / text-based environments (as applied to textual state-tracking tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable in the sense that each passage supplies limited information about entities and states; challenges include aliasing (multiple mentions/references of same entity) and the need to aggregate information over time to build a coherent KG.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>A machine reading comprehension (MRC) querying mechanism (treated as a module/tool to extract entity/state answers from passages).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Text spans / extracted entity-state answers and associated attention scores; outputs are effectively structured assertions about entity states that can be integrated into a KG.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A dynamic knowledge graph assembled from MRC extraction outputs and attention-guided entity linking; entity states are tracked over time via the KG representation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>MRC queries produce extracted entity/state mentions which are integrated into the dynamic KG; attention and alias-resolution mechanisms are used to match mentions to existing nodes and update their states.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Not primarily a planner; focused on information extraction and dynamic KG construction to support downstream tasks (could be used by planners), so planning approach is not specified in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MRC mechanisms can be effective tools to extract entity-state information from text and to handle aliasing through attention, enabling the construction of dynamic KGs that track entities over sequences of observations; such modules can serve as external extractors that feed structured information to agents operating under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building Dynamic Knowledge Graphs from Text-based Games', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Building dynamic knowledge graphs from text using machine reading comprehension. <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games. <em>(Rating: 2)</em></li>
                <li>Clutrr: A diagnostic benchmark for inductive reasoning from text. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-769",
    "paper_id": "paper-204800714",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "TextWorld KG Seq2Seq Updater",
            "name_full": "Sequence-to-Sequence Transformer Knowledge Graph Update Module (TextWorld KG)",
            "brief_description": "A transformer-based Seq2Seq model that generates text-form graph update operations (add/delete RDF triples) to maintain an agent's belief KG from the previous belief graph and a new text observation; the graph encoder can be a GCN, R-GCN, or R-GCN with learned relation embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TextWorld KG Seq2Seq KG-Updater",
            "agent_description": "The module takes as input the previous belief graph G_belief_{t-1} (RDF triple KG), the previous action A_{t-1}, and the new observation O_t (concatenated text). It consists of: (1) a text encoder (embedding layer + stack of transformer blocks) producing h_O; (2) a graph encoder (variants: none baseline, GCN, R-GCN, R-GCN with learned relation label embeddings) producing h_G; (3) an attention-based representation aggregator that produces cross-conditioned representations h_GO and h_OG; and (4) a transformer-based decoder command generator with a pointer-softmax mechanism that outputs a token sequence encoding a sequence of update operations (add/delete(node1,node2,relation)). The model is trained with teacher forcing (NLL loss) and at test time can run autoregressively; the generated operations Δg_t are applied by an oracle Update function to produce G_belief_t.",
            "environment_name": "TextWorld (TextWorld KG dataset)",
            "environment_description": "Text-based, deterministic game instances generated by TextWorld where the agent receives textual observations after actions. The agent only observes part of the full world state (partial observability); object names vary across games and can be multi-token, layouts and object placements vary, and many actions (e.g., 'go') yield observations that omit prior-location information, making state tracking and generalization challenging.",
            "is_partially_observable": true,
            "external_tools_used": "none (the experiments do not use external tools such as OpenIE or external map services; all KG construction/updating is learned end-to-end by the Seq2Seq model and internal GNN encoders)",
            "tool_output_types": null,
            "belief_state_mechanism": "A symbolic/structured belief graph (G_belief_t) represented as RDF triples (nodes = entities/states/locations; edges = relations). The graph is encoded into distributed vectors via graph neural networks (GCN or R-GCN variants) and used by the decoder to produce update operations; the belief itself is stored as explicit graph structure (triples).",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Belief updates are produced as sequences of text commands (add(node1,node2,relation) and delete(node1,node2,relation)). Given O_t and G_belief_{t-1}, the Seq2Seq generator outputs Δg_t; an oracle Update function applies Δg_t to G_belief_{t-1} to yield G_belief_t (adding nodes if missing, ignoring deletes of non-existent edges). Ground-truth operation orderings are fixed by rules (e.g., add before delete) during training.",
            "planning_approach": "Not implemented in this work — the paper focuses on a learned KG-update module (supervised Seq2Seq generation) rather than integrating a planner; intended to support downstream RL planners (graph-based RL) but no planning algorithm is evaluated here.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "The learned KG-update module can accurately generate add/delete operations when conditioned on the previous belief graph and new observation; models that encode relational structure (R-GCN) outperform simple GCNs, and R-GCNs with learned relation-label embeddings achieve significantly better free-run (cascading-error) performance. Conditioning on a belief graph improves handling of navigation ('go') actions where the immediate observation lacks prior-location information. Errors accumulate in free-run evaluation, and actions that consume multiple ingredients (prepare) are challenging because observations omit consumed constituents and require retrieving multiple facts from the belief graph.",
            "uuid": "e769.0",
            "source_info": {
                "paper_title": "Building Dynamic Knowledge Graphs from Text-based Games",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Ammanabrolu&Riedl KG-based agent",
            "name_full": "KG-based RL agent using OpenIE and game-specific rules (Ammanabrolu and Riedl, 2018)",
            "brief_description": "A graph-based deep reinforcement learning agent for text-adventure games that leverages an explicitly constructed knowledge graph; the KG is built using OpenIE extractions combined with hand-crafted game-specific rules to extract and maintain entities and relations.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "KG-based deep RL agent (Ammanabrolu & Riedl 2018)",
            "agent_description": "Described in the related work: the agent constructs a knowledge graph from text observations by applying OpenIE to extract entities/relations and using several game-specific heuristics/rules to build and update the KG; the resulting graph is used as structured memory to inform a graph-based deep RL policy for action selection.",
            "environment_name": "Text-adventure games (text-based games)",
            "environment_description": "Partially observable text-based environments where the agent receives textual scene descriptions and must act to achieve goals; partial observability arises from limited sensorimotor access to rooms/objects and the need to remember prior observations.",
            "is_partially_observable": true,
            "external_tools_used": "OpenIE (Open Information Extraction) for entity and relation extraction; additionally, several domain/game-specific rule modules (heuristics) are used to post-process and maintain the KG.",
            "tool_output_types": "Structured relation tuples/triples (extracted subject-relation-object triples and entity mentions), i.e., structured triplets derived from text.",
            "belief_state_mechanism": "An explicit knowledge graph constructed from OpenIE extractions and game-specific rules; the KG serves as the agent's memory/belief about observed entities, locations, and states.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "The agent ingests OpenIE extractions and applies game-specific rules to add/update nodes and edges in the KG; specifics of add/delete mechanics are not detailed here (only referenced), but the pipeline uses extraction outputs to populate and maintain the graph.",
            "planning_approach": "Graph-based deep reinforcement learning (learned policy) that conditions on the constructed KG; the KG is used to inform action selection by the RL agent rather than performing explicit search-based planning within the paper's description here.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Related work highlights that leveraging an explicit KG (constructed with OpenIE and rules) can improve performance in text games by providing structured memory; however, relying on an external extractor plus hand-crafted rules reduces generality and requires domain knowledge (not learned end-to-end).",
            "uuid": "e769.1",
            "source_info": {
                "paper_title": "Building Dynamic Knowledge Graphs from Text-based Games",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Das et al. MRC-KG",
            "name_full": "Dynamic KG construction using Machine Reading Comprehension (Das et al., 2018)",
            "brief_description": "A method that leverages a machine reading comprehension (MRC) mechanism to query text for entities and states, using attention to resolve aliasing and dynamically track entity states across short text passages.",
            "citation_title": "Building dynamic knowledge graphs from text using machine reading comprehension.",
            "mention_or_use": "mention",
            "agent_name": "MRC-conditioned KG builder (Das et al. 2018)",
            "agent_description": "Uses a machine reading comprehension module to query input text for entities and their states; attention mechanisms are used to handle aliased mentions and to dynamically update entity state representations, effectively building and maintaining a dynamic KG from local textual context.",
            "environment_name": "Short textual passages / text-based environments (as applied to textual state-tracking tasks)",
            "environment_description": "Partially observable in the sense that each passage supplies limited information about entities and states; challenges include aliasing (multiple mentions/references of same entity) and the need to aggregate information over time to build a coherent KG.",
            "is_partially_observable": true,
            "external_tools_used": "A machine reading comprehension (MRC) querying mechanism (treated as a module/tool to extract entity/state answers from passages).",
            "tool_output_types": "Text spans / extracted entity-state answers and associated attention scores; outputs are effectively structured assertions about entity states that can be integrated into a KG.",
            "belief_state_mechanism": "A dynamic knowledge graph assembled from MRC extraction outputs and attention-guided entity linking; entity states are tracked over time via the KG representation.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "MRC queries produce extracted entity/state mentions which are integrated into the dynamic KG; attention and alias-resolution mechanisms are used to match mentions to existing nodes and update their states.",
            "planning_approach": "Not primarily a planner; focused on information extraction and dynamic KG construction to support downstream tasks (could be used by planners), so planning approach is not specified in detail here.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "MRC mechanisms can be effective tools to extract entity-state information from text and to handle aliasing through attention, enabling the construction of dynamic KGs that track entities over sequences of observations; such modules can serve as external extractors that feed structured information to agents operating under partial observability.",
            "uuid": "e769.2",
            "source_info": {
                "paper_title": "Building Dynamic Knowledge Graphs from Text-based Games",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Building dynamic knowledge graphs from text using machine reading comprehension.",
            "rating": 2,
            "sanitized_title": "building_dynamic_knowledge_graphs_from_text_using_machine_reading_comprehension"
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games.",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Clutrr: A diagnostic benchmark for inductive reasoning from text.",
            "rating": 1,
            "sanitized_title": "clutrr_a_diagnostic_benchmark_for_inductive_reasoning_from_text"
        }
    ],
    "cost": 0.01116375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Building Dynamic Knowledge Graphs from Text-based Games
22 Oct 2019</p>
<p>Mikuláš Zelinka 
Faculty of Mathematics and Physics
Charles University
Czech Republic ‡ Microsoft Research
Montréal</p>
<p>Xingdi Yuan 
Marc-Alexandre Côté 
Romain Laroche 
Adam Trischler </p>
<p>Graph Representation Learning (GRL) Workshop
Microsoft Research, Montréal. 33rd Conference on Neural Information Processing Systems
2019)VancouverNeurIPSCanada</p>
<p>Building Dynamic Knowledge Graphs from Text-based Games
22 Oct 2019AC2E5444D80E2C7E430F39BAC6A29486arXiv:1910.09532v2[cs.CL]
We are interested in learning how to update Knowledge Graphs (KG) from text.In this preliminary work, we propose a novel Sequence-to-Sequence (Seq2Seq) architecture to generate elementary KG operations.Furthermore, we introduce a new dataset for KG extraction built upon text-based game transitions (over 300k data points).We conduct experiments and discuss the results.</p>
<p>Introduction</p>
<p>Text-based games are complex, interactive simulations in which text describes the game state and players make progress by entering text actions.They can be seen as sequential decision making tasks where accomplishing certain goals earns rewards (points).Solving these games requires both Reinforcement Learning (RL) and Natural Language Processing (NLP) techniques.</p>
<p>Given the complex, partially observable nature of text-based games, an explicit structured memorye.g., in the form of a graph -is a useful component for game-playing agents.In this work, we side step the game-playing aspect of the problem to focus solely on learning how to build and dynamically maintain a KG from text observations.Specifically, our proposed model learns to generate graph update operations to update an existing KG given new text information.We see this KG module as an independent block that can be leveraged by game-playing agents to improve their performance.</p>
<p>Related Work: Numerous recent works focus on learning or using KGs in textual environments.Das et al. (2018) leverage a machine reading comprehension (MRC) mechanism to query for entities and states in short text passages and use attention to address aliased entity occurrences and to track the entity states dynamically.Xiong et al. (2018) focus on one-shot learning of new relations from only one training instance.</p>
<p>For text games specifically, Ammanabrolu and Riedl (2018) leverage KGs to improve performance, relying on OpenIE for entity extraction and several game-specific rules for building and maintaining the KG.Sinha et al. (2019) introduce a dataset aimed at natural language understanding and generalization in reasoning about entity relations that was built using a KG-like structure.</p>
<p>To the best of our knowledge, all of the approaches for learning KGs are either concerned with building static KGs (rather than focusing on small, dynamic updates), or employ some domainspecific knowledge or rules to facilitate learning.In contrast, our proposed model learns to generate general update operations for modifying a KG. 2 The TextWorld KG Dataset</p>
<p>In this section, we introduce a new dynamic KG extraction dataset, TextWorld KG2 .TextWorld KG is based on a set of text-based games generated using TextWorld (Côté et al., 2018). 3That framework enables us to extract the underlying partial KG for every state, i.e., the subgraph that represents the agent's partial knowledge of the world -what it has observed so far.All games share the same overarching theme: the agent finds itself hungry in a simple modern house with the goal of gathering ingredients and cooking a meal.</p>
<p>To build the TextWorld KG dataset, we collect game transitions obtained by following each game's walkthrough (provided by TextWorld).Additionally, after each step in a walkthrough, we perform 5 additional actions sampled at random from the list of admissible commands4 (also provided by TextWorld).This presumably promotes robustness and generalizability of a training agent since it will encounter off-the-path transitions during game playing in the RL setting due to the absent of walkthroughs.Therefore an agent pre-trained on such data is more likely to work well in the RL setting.Formally, each data point in TextWorld KG is a tuple,
{G seen t−1 , A t−1 , O t , G seen t }, where G seen t−1
is a partial KG (in the format of Resource Description Framework (RDF) triples) representing the information an agent has seen during all previous game steps up to t − 1.After the agent issues an action A t−1 (a string of words), the game engine returns a new text observation O t describing its effects on the world.The task is to predict the updated partial KG G seen t given the above information.Note that the new observation might not contain new information, since some actions do not change the game state (e.g.look in the same room twice).Table 1 shows some statistics about TextWorld KG and Figure 1 illustrates an example data point.An important challenge posed by TextWorld is generalization.In each individual game instance, the interactable objects and their locations change along with the layout of the environment.Similarly, object names can be composed of multiple adjectives and a noun (e.g., red hot chili pepper), and at test time, players may encounter object names never seen during training.TextWorld KG inherits both of these challenging features.3 Learning to Update a KG</p>
<p>KG Definition</p>
<p>In a text-based game, at any given game step t, the game state s t can be represented as a graph
G full t = (V t , E t ).
In our setting, vertices V t represent entities (including objects, the player, and locations) and their states (e.g., closed, fried, sliced).Vertices are connected by edges E, which represent a set of relations between entities (e.g.north_of, in, is).</p>
<p>Since games are partially observable, at every step an agent only observes part of the full game state (e.g., the agent cannot know facts in a room it has not visited).Thus, an agent must build its belief about the world, G belief t , from its observations.Ideally, the belief graph should match the ground truth graph, G seen t , which is a subgraph of G full t representing what has been seen so far in the game.TextWorld games are deterministic, so by progressively exploring and observing, an agent should discover more knowledge to push into its belief graph.Eventually, this ought to converge to a graph that accurately represents the entire game state.</p>
<p>Updating a KG</p>
<p>Instead of generating the entire belief graph at every game step, we generate a set of update operations ∆g t such that G belief t = Update(G belief t−1 , ∆g t ), where Update is an oracle function that applies ∆g t .In our case, each update operation in ∆g t is represented as a text command.We define the following two types of update operation:</p>
<p>• add(node1, node2, relation): add a directed edge, named relation, between node1 and node2; if any of these nodes does not exist, add that node first.• delete(node1, node2, relation): delete a directed edge, named relation, between node1 and node2; if any of the nodes or the edge does not exist, ignore this operation.</p>
<p>Given a new observation string O t and an agent's current belief G belief t−1 , the agent is required to generate k ≥ 0 operations as defined above to merge newly observed information into its belief graph.For the example shown in Figure 1, the generated operations are listed in Table 2.</p>
<p>add (player, shed, at) add (shed, backyard, west_of) add (wooden door, shed, east_of) add (toolbox, shed, in) add (toolbox, closed, is) add (workbench, shed, in) delete (player, backyard, at) Table 2: Update operations corresponding to the transition shown in Figure 1.</p>
<p>We formulate the update generation task as a Seq2Seq problem.Specifically, we adopt the decoding strategy from Yuan et al. (2018), where given an observation sequence O t and a belief graph G belief t−1 , the agent generates a sequence of tokens consisting of multiple graph update operations separated by a delimiter token.</p>
<p>As pointed out by Meng et al. (2019), the order of ground-truth tokens and sequences (in our case, graph update operations) matters in Seq2Seq language generation.We therefore define a set of rules (e.g., always add before delete) to order ground-truth operations for teacher forcing during training.We use a transformer-based Seq2Seq model (Vaswani et al., 2017) to generate update operations.As shown in Figure 2 in Appendix A, the model consists of the following components:</p>
<p>1.A text encoder, which reads text inputs (the concatenation of observation O t and the action at the previous game step, A t−1 ), and generates hidden representations.</p>
<ol>
<li>
<p>A graph encoder, which encodes the previous belief G belief t−1 into hidden representations.3.An attention-based representation aggregator, which combines the two above representations.</p>
</li>
<li>
<p>A command generator, which takes aggregated representations and generates update operations token by token.</p>
</li>
</ol>
<p>For space considerations, we elaborate our model components in Appendix A. Following common practice in natural language generation (NLG), we train our operation generation model via teacher forcing.Specifically, during training, a right-shifted ground truth target sequence is provided as input to the decoder and the model is trained with the negative log-likelihood (NLL) loss.During test, the model starts generating from a start-of-sentence token and uses the previously generated token as input to the next step.The model terminates after generating an end-of-sequence token.</p>
<p>Experiments and Discussion</p>
<p>Model TF-F1 FR-F1  In this preliminary study we test 4 graph encoder variants of the proposed model.First, as a baseline, we disable the graph encoder, which renders the model a standard Seq2Seq transformer.Second, we utilize a Graph Convolutional Network (GCN) (Kipf and Welling, 2016) as the graph encoder.The GCN does not consider multiple relations.5Third, we enable conditioning on multiple relations by using a Relational Graph Convolutional Network (R-GCN) (Schlichtkrull et al., 2018).Although R-GCN takes into account multiple relations, it does not consider information in relation labels.In our task, this information is important (e.g., east_of and west_of are symmetric relations).Therefore, we finally learn a vector representation for each relation that is conditioned on the label's text embeddings.</p>
<p>The resulting relation representation is used as an extra input to the R-GCN layer.Table 3 shows the test results for all models.</p>
<p>During training, a model takes {G seen t−1 , A t−1 , O t } as input, where G seen t−1 is the input graph, and A t−1 and O t are the text action issued at the previous game step and the resulting text observation, respectively.The model outputs a sequence describing an update operation to the graph and the resulting G belief t .During evaluation:</p>
<p>• Teacher-force (TF) F 1 : we use the ground-truth G seen t−1 as input graph and compute the F 1 score between the model's generated graph update commands and the ground-truth commands.Note the F 1 score is computed on command level (i.e., if any token in a command is incorrect, this command is treated as incorrect).</p>
<p>• Free-run (FR) F 1 : we initialize the belief graph at the beginning of each game with an empty graph.For each game step, we use G belief t−1 (the graph generated by the model) as input.At the end of each game, we compute F 1 score between the final belief graph G belief T and ground truth G seen T , graphs are represented as RDF triples.In general, although all model variants show good performance on TF-F 1 , they perform worse on FR-F 1 .This is not surprising since errors accumulate in the latter setting.Models using R-GCN outperform those using GCN by a noticeable margin, which suggests relational information is essential in the proposed tasks.Interestingly, while the two R-GCN models perform similarly on TF-F 1 , the variant with relational embedding (considering information in relation labels) significantly outperforms the other on FR-F 1 .</p>
<p>To better understand the behavior of our proposed models on TextWorld KG, we conduct an error analysis, which we show in Appendix B.</p>
<p>The next step for this project is to leverage the KG update module while playing text-based games.We believe that maintaining such a graph could help an RL agent (1) to avoid re-discovering known facts about the world and (2) to discover new world knowledge efficiently.We are also interested in finding ways of transferring learned graphs from one game to another to improve agents' ability to generalize.</p>
<p>A Model Architecture</p>
<p>Text Encoder: An observation string O t is provided in response to the text action A t−1 an agent issued at previous game step.We concatenate the two text information together [A t−1 ; O t ] as the input to the text encoder, in which [•; •] indicates vector/string concatenation.The text encoder consists an embedding layer and a stack of transformer blocks.The text encoder results a sequence of hidden vectors h O ∈ R L O ×H , where L o is length of the concatenated string, H is hidden size.</p>
<p>Graph Encoder: At the same time, the graph encoder takes the agent's belief KG G belief t−1 (in which stores the agent's observations a all previous game steps) as input.We adopt different off-the-shelf graph neural networks (GNN) as the graph encoder (we will describe more details in experiment section).After several layers of propagation, graph representations h G ∈ R N ×H are generated, where N is number of nodes in the KG.</p>
<p>Representation Aggregator: We use an attention based layer to aggregate text and graph representations (Bahdanau et al., 2014;Seo et al., 2016).Specifically, we use weighted sum of text representations to represent graph information, which results h GO ∈ R N ×H ; similarly, we use weighted sum of graph representations to represent text information, resulting h OG ∈ R L O ×H .</p>
<p>Command Generator Finally, both h OG and h GO are used to condition text generation.Input tokens are first converted into embeddings, then they are fed into a stack of decoder transformer blocks which result a probability distribution over the vocabulary.To prevent model from utilizing future information, we follow Vaswani et al. (2017) to use a masked multi-head self attention layer in the beginning of each block.For being able to both generate a word from vocabulary, and point a word from the source text O t , we adopt the pointer-softmax mechanism (Gülçehre et al., 2016).In Figure 3, we report average TF-F 1 scores grouped by verbs found in input actions A t−1 .We can observe that vanilla transformer model performs poorly on go actions, which aligns with the fact that after issuing a go action, the resulting observation text O t does not contain any information of the agent's previous location.On the other hand, the other models benefit from their belief graph to retrieve that single information.</p>
<p>B Error Analysis</p>
<p>Also, we notice that all models perform relatively poorly on prepare actions.This also makes sense since in TextWorld games, the action of preparing a meal consumes multiple food ingredients at once in order to produce a meal object.The resulting observation text O t following a prepare action only contains information about the newly produced meal and does not mention what food ingredients have been consumed.Even though the information about the recipe (i.e.ingredients needed) is part of the KG graph, a model has to learn to retrieve multiple information at once from its belief graph G belief t−1 to figure out what ingredients will be consumed.</p>
<p>Figure 2 :
2
Figure 2: Graph update operation generation model.</p>
<p>Figure 3 :
3
Figure 3: Average TF-F 1 scores grouped by verbs.</p>
<p>You find yourself in a backyard.You make out a patio table.But it is empty.You see a patio chair.The patio chair is stylish.But there isn't a thing on it.You see a gleam over in a corner, where you can see a BBQ.There is a closed screen door leading south.There is an open wooden door leading west.to the shed.You can barely contain your excitement.You can make out a closed toolbox here.You can see a workbench.The workbench is wooden.Looks like someone's already been here and taken everything off it, though.You swear loudly.There is an open wooden door leading east.Illustration of an example in TextWorld KG.By issuing an action A t−1 at game step t − 1, the environment returns a new observation, O t .Given the KG at step t − 1, a model is required to predict the new KG given the text observation.
Welcome go west…𝑂 𝑡−1𝐴 𝑡−1𝑂 𝑡…kitchen patio chair BBQ in west of patio table in inbackyard living room player north of atnorth of south of west ofscreen door closed is open is wooden doorwest of in in kitchen BBQ patio in chair patio tablebackyard living room north of west of shed player atscreen door wooden north of south of door west of east of toolbox in workbench inupdate closed open closed is is isFigure 1:</p>
<p>Table 1 :
1
Statistics of TextWorld KG.Avg.Obs. is the average number of tokens an observation has.Avg.#Operations is the average number of update operations to generate per time step.#Vertices and #Edges correspond to the number of unique entities and relation types.Avg.#Connections is the average number of connections a graph has.</p>
<h1>Train#Valid#TestAvg. Obs.Avg. #Operations#Vertices#EdgesAvg. #Connections267,03113,44241,86529.3 tokens3.1991043.1</h1>
<p>Table 3 :
3
Test performance.</p>
<p>TextWorld KG is publicly available at https://github.com/MikulasZelinka/textworld_kg_ dataset
We use the games provided for the First TextWorld Problems competition, available at http://aka.ms/ftwp-dataset.
This is the set of game actions understood by the game in a given state.
For models that do not consider relational information, we use single relation KGs as ground-truth during evaluation.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. P Ammanabrolu, M O Riedl, CoRR, abs/1812.016282018</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.04732014arXiv preprint</p>
<p>M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, Textworld: A learning environment for text-based games. 2018</p>
<p>Building dynamic knowledge graphs from text using machine reading comprehension. R Das, T Munkhdalai, X Yuan, A Trischler, A Mccallum, CoRR, abs/1810.056822018</p>
<p>Pointing the unknown words. Ç Gülçehre, S Ahn, R Nallapati, B Zhou, Y Bengio, CoRR, abs/1603.081482016</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, CoRR, abs/1609.029072016</p>
<p>Does order matter? an empirical study on generating multiple keyphrases as a sequence. R Meng, X Yuan, T Wang, P Brusilovsky, A Trischler, D He, 2019CoRR</p>
<p>Modeling relational data with graph convolutional networks. M Schlichtkrull, T N Kipf, P Bloem, Van Den, R Berg, I Titov, M Welling, European Semantic Web Conference. Springer2018</p>
<p>Bidirectional attention flow for machine comprehension. M Seo, A Kembhavi, A Farhadi, H Hajishirzi, arXiv:1611.016032016arXiv preprint</p>
<p>K Sinha, S Sodhani, J Dong, J Pineau, W L Hamilton, arXiv:1908.06177Clutrr: A diagnostic benchmark for inductive reasoning from text. 2019arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, CoRR, abs/1706.037622017</p>
<p>One-shot relational learning for knowledge graphs. W Xiong, M Yu, S Chang, X Guo, W Y Wang, arXiv:1808.090402018arXiv preprint</p>
<p>Generating diverse numbers of diverse keyphrases. X Yuan, T Wang, R Meng, K Thaker, P Brusilovsky, D He, A Trischler, CoRR, abs/1810.052412018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4796 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4796</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4796</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-220250370</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2006.15892v1.pdf" target="_blank">Switchblade -- a Neural Network for Hard 2D Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Convolutional neural networks have become the main tools for processing two-dimensional data. They work well for images, yet convolutions have a limited receptive field that prevents its applications to more complex 2D tasks. We propose a new neural network model, named Switchblade, that can efficiently exploit long-range dependencies in 2D data and solve much more challenging tasks. It has close-to-optimal $\mathcal{O}(n^2 \log{n})$ complexity for processing $n \times n$ data matrix. Besides the common image classification and segmentation, we consider a diverse set of algorithmic tasks on matrices and graphs. Switchblade can infer highly complex matrix squaring and graph triangle finding algorithms purely from input-output examples. We show that our model is likewise suitable for logical reasoning tasks -- it attains perfect accuracy on Sudoku puzzle solving. Additionally, we introduce a new dataset for predicting the checkmating move in chess on which our model achieves 72.5% accuracy.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4796.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4796.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Switchblade (Sudoku)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Switchblade neural network (applied to Sudoku)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2D neural architecture (Switchblade) that adapts the 1D Neural Shuffle-Exchange idea to 2D via Quaternary Flatten/QSwitch/QShuffle layers and Beneš blocks; trained end-to-end to solve Sudoku by learning global constraints on the board.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Switchblade</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Custom 2D neural network introduced in this paper. Key components: Quaternary Flatten/Reshape (Z-order), Quaternary Switch Units (QSU) based on Residual Switch Unit (RSU), Quaternary Shuffle layers, organized into Beneš blocks. Complexity O(n^2 log n) for n×n inputs. Experimental Sudoku configuration: 192 feature maps, 2 Beneš blocks; trained end-to-end with RAdam optimizer and softmax cross-entropy loss for 100k iterations; randomized padding used (board placed anywhere on 16×16 matrix).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (9×9)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Standard 9×9 Sudoku: fill digits 1–9 so each row, column, and each 3×3 subgrid contains all digits exactly once — a spatial/relational constraint satisfaction task requiring reasoning across rows, columns and local 3×3 blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>End-to-end supervised learning mapping a padded 2D board (placed in a 16×16 matrix) to the completed board in one forward pass; Switchblade preserves locality with Z-order flattening and uses QSwitch/QShuffle layers to efficiently model long-range 2D dependencies (rows/columns/blocks) instead of iterative search/backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Empirical: the model attains perfect solutions on the provided test instances (see performance). Ablations in the paper show Quaternary Flatten/Reshape (which preserve 2D locality) drastically improve convergence and accuracy on 2D algorithmic tasks, supporting that Switchblade leverages spatial locality. The authors also used randomized padding to avoid memorization of fixed-board positions, indicating learned spatial-general constraints rather than positional overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>100% accuracy on the provided test set of 30 authentic Sudoku games (Park 2018 test set). Trained 100k iterations. Reported as a 14% absolute improvement over the referenced convolutional baseline (86% reported by Park, 2018).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit failure modes reported for the Sudoku test set in this paper, but authors note the test set is very small (30), which can make exact numbers misleading; risk of overfitting is mitigated by randomized padding. No analysis of reasoning traces or intermediate constraint representations is provided — the model is a black box.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Park (2018) convolutional neural network baseline (reported 86% on same small test set) — Switchblade reported 100% on that test set. Authors highlight they trained end-to-end (no explicit backtracking) and emphasize improved performance over the CNN baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Switchblade -- a Neural Network for Hard 2D Tasks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4796.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4796.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Switchblade (Chess mate-in-one)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Switchblade neural network (applied to mate-in-one chess)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Switchblade applied as a board-level classifier to predict a single mating move (mate-in-one) from a chess position, using a dense output over all possible move pairs after the Switchblade core.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Switchblade + dense output</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Switchblade core with 192 feature maps and 2 Beneš blocks, followed by a dense layer producing logits for all 4096 possible source-destination board-square pairs (moves). Trained on a large mate-in-one dataset (constructed from mate-in-two positions via Stockfish) for 500k steps.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess mate-in-one</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Given a legal chess position where White can mate in one move, predict a move that immediately checkmates Black. Requires understanding of piece movement rules, attacks, checks, and global board interactions (spatial reasoning about piece placement and lines of attack/defense).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>End-to-end supervised classification: input board encoded into Switchblade-compatible 2D representation; Switchblade encodes spatial relations across the board and the final dense layer scores all possible moves; model learns legal moves and mating patterns implicitly from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative analysis by authors: model solutions indicate it learned concepts such as legal moves per piece, attacks on pieces, giving check, and checkmate patterns. Failure analyses indicate errors typically arise when the model fails to consider opponent responses that could interpose or cover the king, showing model is using spatial signals but sometimes misses opponent-move reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>72.5% accuracy on the validation set (validation set constructed to contain only single-solution mate-in-one positions; dataset split: ~2.43M train, ~0.27M validation). Training: 500k steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Main failure mode: incorrect solutions that do not account for opponent pieces that can interpose or cover the king (i.e., limited opponent-move reasoning). Dataset excludes pawn promotion move types that require extra annotation, and around 0.7M problems with multiple solutions were simplified by including only one solution pair, which may affect learning on multi-solution cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No direct head-to-head baseline for mate-in-one is reported in the paper. Authors mention broader chess learning literature (reinforcement learning approaches that master full games) but do not provide a direct baseline comparison for this supervised mate-in-one task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Switchblade -- a Neural Network for Hard 2D Tasks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4796.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4796.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN (Sudoku baseline, Park 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional Neural Network (as used in Park, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional neural network approach previously applied to the Sudoku solving task (Park, 2018) used as a baseline comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can convolutional neural networks crack sudoku puzzles?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Convolutional Neural Network (Park, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Park (2018) used a CNN architecture trained end-to-end to map incomplete Sudoku boards to solutions. Exact architecture details are in the referenced work; the paper reports Park's CNN achieved 86% on the small authentic test set used here.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (9×9)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same 9×9 Sudoku puzzle description as above; the task requires learning global row/column/block constraints from board images/representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>End-to-end convolutional mapping from board to solution; no explicit search/backtracking, trained on computer-generated Sudoku instances.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performance (86%) demonstrates partial ability to capture spatial constraints but inferior to Switchblade on the small test set; no internal probing or ablation in this paper (Park 2018 contains the original analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported 86% accuracy on the 30-authentic-game test set (Park 2018 result quoted by the Switchblade paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower accuracy versus Switchblade on the small test set; suggests convolutional architectures may struggle to capture long-range constraints (rows/columns/3×3 blocks) without additional mechanisms. The Switchblade authors cite this as the baseline their model improves upon.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Park (2018) CNN is the direct baseline cited for Sudoku; Switchblade reports a 14 percentage-point improvement (100% vs 86%) on the small test set used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Switchblade -- a Neural Network for Hard 2D Tasks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4796.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4796.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory (Differentiable Neural Computer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture with an external differentiable memory that has been applied to puzzle-like and graph traversal tasks (referenced in the paper as prior work for solving small spatial/algorithmic tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural controller (RNN) coupled with a differentiable external memory and attention-based read/write heads; designed to learn algorithmic behaviors by using the memory for storage and retrieval. Mentioned as prior work applied to SGRDLU puzzle, shortest path finding, and traversal tasks on small synthetic graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SGRDLU and shortest-path / traversal tasks (small synthetic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>SGRDLU is a puzzle-like environment requiring language/shape/space reasoning; shortest-path and traversal tasks on small graphs require spatial/structural reasoning about connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Use of external differentiable memory and learned read/write attention to store and manipulate intermediate computation/traces; more akin to neural program induction than single-pass classification.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mentioned as prior art; this paper does not provide metrics. The reference indicates DNC has been used on small spatial/graph tasks, showing ability to learn procedural solutions when given memory and controller, but performance details must be taken from the original DNC paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes DNC was applied to small synthetic tasks; scaling and generalization to large inputs remain challenging. No detailed metrics or ablation provided in this Switchblade paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work to situate Switchblade among architectures for algorithmic/spatial tasks; no direct experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Switchblade -- a Neural Network for Hard 2D Tasks', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can convolutional neural networks crack sudoku puzzles? <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>Neural Shuffle-Exchange Networks-Sequence Processing in O (n log n) Time <em>(Rating: 2)</em></li>
                <li>Neural GPUs learn algorithms <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4796",
    "paper_id": "paper-220250370",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "Switchblade (Sudoku)",
            "name_full": "Switchblade neural network (applied to Sudoku)",
            "brief_description": "A 2D neural architecture (Switchblade) that adapts the 1D Neural Shuffle-Exchange idea to 2D via Quaternary Flatten/QSwitch/QShuffle layers and Beneš blocks; trained end-to-end to solve Sudoku by learning global constraints on the board.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Switchblade",
            "model_description": "Custom 2D neural network introduced in this paper. Key components: Quaternary Flatten/Reshape (Z-order), Quaternary Switch Units (QSU) based on Residual Switch Unit (RSU), Quaternary Shuffle layers, organized into Beneš blocks. Complexity O(n^2 log n) for n×n inputs. Experimental Sudoku configuration: 192 feature maps, 2 Beneš blocks; trained end-to-end with RAdam optimizer and softmax cross-entropy loss for 100k iterations; randomized padding used (board placed anywhere on 16×16 matrix).",
            "puzzle_name": "Sudoku (9×9)",
            "puzzle_description": "Standard 9×9 Sudoku: fill digits 1–9 so each row, column, and each 3×3 subgrid contains all digits exactly once — a spatial/relational constraint satisfaction task requiring reasoning across rows, columns and local 3×3 blocks.",
            "mechanism_or_strategy": "End-to-end supervised learning mapping a padded 2D board (placed in a 16×16 matrix) to the completed board in one forward pass; Switchblade preserves locality with Z-order flattening and uses QSwitch/QShuffle layers to efficiently model long-range 2D dependencies (rows/columns/blocks) instead of iterative search/backtracking.",
            "evidence_of_spatial_reasoning": "Empirical: the model attains perfect solutions on the provided test instances (see performance). Ablations in the paper show Quaternary Flatten/Reshape (which preserve 2D locality) drastically improve convergence and accuracy on 2D algorithmic tasks, supporting that Switchblade leverages spatial locality. The authors also used randomized padding to avoid memorization of fixed-board positions, indicating learned spatial-general constraints rather than positional overfitting.",
            "performance_metrics": "100% accuracy on the provided test set of 30 authentic Sudoku games (Park 2018 test set). Trained 100k iterations. Reported as a 14% absolute improvement over the referenced convolutional baseline (86% reported by Park, 2018).",
            "limitations_or_failure_cases": "No explicit failure modes reported for the Sudoku test set in this paper, but authors note the test set is very small (30), which can make exact numbers misleading; risk of overfitting is mitigated by randomized padding. No analysis of reasoning traces or intermediate constraint representations is provided — the model is a black box.",
            "comparison_baseline": "Compared to Park (2018) convolutional neural network baseline (reported 86% on same small test set) — Switchblade reported 100% on that test set. Authors highlight they trained end-to-end (no explicit backtracking) and emphasize improved performance over the CNN baseline.",
            "uuid": "e4796.0",
            "source_info": {
                "paper_title": "Switchblade -- a Neural Network for Hard 2D Tasks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Switchblade (Chess mate-in-one)",
            "name_full": "Switchblade neural network (applied to mate-in-one chess)",
            "brief_description": "Switchblade applied as a board-level classifier to predict a single mating move (mate-in-one) from a chess position, using a dense output over all possible move pairs after the Switchblade core.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Switchblade + dense output",
            "model_description": "Switchblade core with 192 feature maps and 2 Beneš blocks, followed by a dense layer producing logits for all 4096 possible source-destination board-square pairs (moves). Trained on a large mate-in-one dataset (constructed from mate-in-two positions via Stockfish) for 500k steps.",
            "puzzle_name": "Chess mate-in-one",
            "puzzle_description": "Given a legal chess position where White can mate in one move, predict a move that immediately checkmates Black. Requires understanding of piece movement rules, attacks, checks, and global board interactions (spatial reasoning about piece placement and lines of attack/defense).",
            "mechanism_or_strategy": "End-to-end supervised classification: input board encoded into Switchblade-compatible 2D representation; Switchblade encodes spatial relations across the board and the final dense layer scores all possible moves; model learns legal moves and mating patterns implicitly from examples.",
            "evidence_of_spatial_reasoning": "Qualitative analysis by authors: model solutions indicate it learned concepts such as legal moves per piece, attacks on pieces, giving check, and checkmate patterns. Failure analyses indicate errors typically arise when the model fails to consider opponent responses that could interpose or cover the king, showing model is using spatial signals but sometimes misses opponent-move reasoning.",
            "performance_metrics": "72.5% accuracy on the validation set (validation set constructed to contain only single-solution mate-in-one positions; dataset split: ~2.43M train, ~0.27M validation). Training: 500k steps.",
            "limitations_or_failure_cases": "Main failure mode: incorrect solutions that do not account for opponent pieces that can interpose or cover the king (i.e., limited opponent-move reasoning). Dataset excludes pawn promotion move types that require extra annotation, and around 0.7M problems with multiple solutions were simplified by including only one solution pair, which may affect learning on multi-solution cases.",
            "comparison_baseline": "No direct head-to-head baseline for mate-in-one is reported in the paper. Authors mention broader chess learning literature (reinforcement learning approaches that master full games) but do not provide a direct baseline comparison for this supervised mate-in-one task.",
            "uuid": "e4796.1",
            "source_info": {
                "paper_title": "Switchblade -- a Neural Network for Hard 2D Tasks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "CNN (Sudoku baseline, Park 2018)",
            "name_full": "Convolutional Neural Network (as used in Park, 2018)",
            "brief_description": "A convolutional neural network approach previously applied to the Sudoku solving task (Park, 2018) used as a baseline comparison in this paper.",
            "citation_title": "Can convolutional neural networks crack sudoku puzzles?",
            "mention_or_use": "mention",
            "model_name": "Convolutional Neural Network (Park, 2018)",
            "model_description": "Park (2018) used a CNN architecture trained end-to-end to map incomplete Sudoku boards to solutions. Exact architecture details are in the referenced work; the paper reports Park's CNN achieved 86% on the small authentic test set used here.",
            "puzzle_name": "Sudoku (9×9)",
            "puzzle_description": "Same 9×9 Sudoku puzzle description as above; the task requires learning global row/column/block constraints from board images/representations.",
            "mechanism_or_strategy": "End-to-end convolutional mapping from board to solution; no explicit search/backtracking, trained on computer-generated Sudoku instances.",
            "evidence_of_spatial_reasoning": "Performance (86%) demonstrates partial ability to capture spatial constraints but inferior to Switchblade on the small test set; no internal probing or ablation in this paper (Park 2018 contains the original analysis).",
            "performance_metrics": "Reported 86% accuracy on the 30-authentic-game test set (Park 2018 result quoted by the Switchblade paper).",
            "limitations_or_failure_cases": "Lower accuracy versus Switchblade on the small test set; suggests convolutional architectures may struggle to capture long-range constraints (rows/columns/3×3 blocks) without additional mechanisms. The Switchblade authors cite this as the baseline their model improves upon.",
            "comparison_baseline": "Park (2018) CNN is the direct baseline cited for Sudoku; Switchblade reports a 14 percentage-point improvement (100% vs 86%) on the small test set used.",
            "uuid": "e4796.2",
            "source_info": {
                "paper_title": "Switchblade -- a Neural Network for Hard 2D Tasks",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Differentiable Neural Computer (DNC)",
            "name_full": "Hybrid computing using a neural network with dynamic external memory (Differentiable Neural Computer)",
            "brief_description": "A neural architecture with an external differentiable memory that has been applied to puzzle-like and graph traversal tasks (referenced in the paper as prior work for solving small spatial/algorithmic tasks).",
            "citation_title": "Hybrid computing using a neural network with dynamic external memory",
            "mention_or_use": "mention",
            "model_name": "Differentiable Neural Computer (DNC)",
            "model_description": "Neural controller (RNN) coupled with a differentiable external memory and attention-based read/write heads; designed to learn algorithmic behaviors by using the memory for storage and retrieval. Mentioned as prior work applied to SGRDLU puzzle, shortest path finding, and traversal tasks on small synthetic graphs.",
            "puzzle_name": "SGRDLU and shortest-path / traversal tasks (small synthetic graphs)",
            "puzzle_description": "SGRDLU is a puzzle-like environment requiring language/shape/space reasoning; shortest-path and traversal tasks on small graphs require spatial/structural reasoning about connectivity.",
            "mechanism_or_strategy": "Use of external differentiable memory and learned read/write attention to store and manipulate intermediate computation/traces; more akin to neural program induction than single-pass classification.",
            "evidence_of_spatial_reasoning": "Mentioned as prior art; this paper does not provide metrics. The reference indicates DNC has been used on small spatial/graph tasks, showing ability to learn procedural solutions when given memory and controller, but performance details must be taken from the original DNC paper.",
            "performance_metrics": null,
            "limitations_or_failure_cases": "Paper notes DNC was applied to small synthetic tasks; scaling and generalization to large inputs remain challenging. No detailed metrics or ablation provided in this Switchblade paper.",
            "comparison_baseline": "Mentioned in related work to situate Switchblade among architectures for algorithmic/spatial tasks; no direct experimental comparison in this paper.",
            "uuid": "e4796.3",
            "source_info": {
                "paper_title": "Switchblade -- a Neural Network for Hard 2D Tasks",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can convolutional neural networks crack sudoku puzzles?",
            "rating": 2,
            "sanitized_title": "can_convolutional_neural_networks_crack_sudoku_puzzles"
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2,
            "sanitized_title": "hybrid_computing_using_a_neural_network_with_dynamic_external_memory"
        },
        {
            "paper_title": "Neural Shuffle-Exchange Networks-Sequence Processing in O (n log n) Time",
            "rating": 2,
            "sanitized_title": "neural_shuffleexchange_networkssequence_processing_in_o_n_log_n_time"
        },
        {
            "paper_title": "Neural GPUs learn algorithms",
            "rating": 2,
            "sanitized_title": "neural_gpus_learn_algorithms"
        }
    ],
    "cost": 0.01292575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Switchblade -a Neural Network for Hard 2D Tasks</p>
<p>Emīls Ozolin emils.ozolins@lumii.lv 
Institute of Mathematics and Computer Science
University of Latvia Raina bulvaris 29
LV-1459RigaLatvia</p>
<p>Kārlis Freivalds karlis.freivalds@lumii.lv 
Institute of Mathematics and Computer Science
University of Latvia Raina bulvaris 29
LV-1459RigaLatvia</p>
<p>Agris Šostaks agris.sostaks@lumii.lv 
Institute of Mathematics and Computer Science
University of Latvia Raina bulvaris 29
LV-1459RigaLatvia</p>
<p>Switchblade -a Neural Network for Hard 2D Tasks</p>
<p>Convolutional neural networks have become the main tools for processing twodimensional data. They work well for images, yet convolutions have a limited receptive field that prevents its applications to more complex 2D tasks. We propose a new neural network model, named Switchblade, that can efficiently exploit longrange dependencies in 2D data and solve much more challenging tasks. It has close-to-optimal O(n 2 log n) complexity for processing n × n data matrix. Besides the common image classification and segmentation, we consider a diverse set of algorithmic tasks on matrices and graphs. Switchblade can infer highly complex matrix squaring and graph triangle finding algorithms purely from input-output examples. We show that our model is likewise suitable for logical reasoning tasksit attains perfect accuracy on Sudoku puzzle solving. Additionally, we introduce a new dataset for predicting the checkmating move in chess on which our model achieves 72.5% accuracy.Preprint. Under review. Many neural architectures have been developed to solve 2D tasks. Convolutional Neural Networks (CNN) are the primary tools for processing data with a 2D grid-like topology. For instance, VGG[Simonyan and Zisserman, 2014] and ResNet[He et al., 2016] enable high-accuracy image classification. But CNNs may perform poorly on the image segmentation task due to low-resolution representation introduced by max-pooling layers. U-Net[Ronneberger et al., 2015] architecture attempts to remedy this problem by adding an upsampling part to the network and introducing skip connections. HRNet[Sun et al., 2019] retains high-resolution input representation thorough the network and couples it with several lower-resolution subnets for high-level feature extraction, making it suitable for pixel-level segmentation tasks.Convolution is an inherently local operation that limits CNN use for long-range dependency modeling. The problem can be mitigated by using dilated (atrous) convolutions that have an expanded receptive field. Such an approach works well on image segmentation[Yu and Koltun, 2015]but is not suitable for algorithmic tasks where generalization on larger inputs is crucial.The attention mechanism[Bahdanau et al., 2014, Vaswani et al., 2017is a widespread way of solving the long-range dependency problem in sequence tasks. Unfortunately, its application to 2D data is limited due to it's high O(n 4 ) time complexity for n × n input matrix. It is possible to reduce the set of locations to attend to[Child et al., 2019]and obtain O(n 2 √ n 2 ) time complexity, making attention applicable to image tasks.</p>
<p>Introduction</p>
<p>Neural networks are being established as the primary tools to reach state-of-the-art results in a broad range of applications. Numerous different neural architectures have been developed, often specific to the task at hand. Such tailored architectures work great for the task they were intended to but are often inapplicable to other tasks. When a new task arrives, the user is at a tough decision to choose a suitable architecture, and sometimes none of the existing performs well.</p>
<p>In this paper, we study a wide variety of 2D tasks and propose a new neural architecture, called Switchblade, that performs well on them. By 2D tasks we mean the tasks where input/output data can be represented as 2D matrices. All image processing tasks naturally reside in this category, but not only. We also consider graph tasks, especially for dense graphs, when it is natural to represent the graph with its adjacency matrix and also algorithmic tasks on matrices as well as logical reasoning games such as Sudoku or chess.</p>
<p>The Switchblade architecture can exploit both local dependencies among the data, often captured by convolution, and long-range dependencies where the attention mechanism [Bahdanau et al., 2014, Vaswani et al., 2017 would be necessary. The complexity of the proposed architecture is O(n 2 log n) for processing n×n data matrix, which is significantly lower than O(n 4 ) if one would use the attention in its pure form. The architecture is derived from the Neural Shuffle Exchange Networks [Freivalds et al., 2019, Draguns et al., 2020 by lifting their architecture from 1D to 2D. We validate our model on a broad range of 2D tasks with differently structured input/output data. It obtains satisfactory accuracy on CIFAR-10 [Krizhevsky et al., 2009] image classification and CityScapes [Cordts et al., 2016] pixel-level segmentation tasks. Furthermore, the model can handle complex data inter-dependencies present in algorithmic tasks on matrices and graphs. On algorithmic tasks, our model reaches the perfect accuracy on test instances of the same size it was trained on and generalizes to much larger instances. Generalization capability is an important measure for algorithmic tasks to say that the model has learned an algorithm, not only fitted the training data. We also evaluate the model's capabilities of complex logical reasoning in Sudoku puzzles and predicting a single move leading to mate in chess.</p>
<p>In summary, our main contributions are -a new neural network model validated for a broad range of 2D tasks; new 2D algorithmic tasks on matrices and graphs; new chess-related logical reasoning task mate-in-one.</p>
<p>Graph Convolutional Neural Networks [Micheli, 2009, Atwood andTowsley, 2016] generalizes the convolution operation from the grid to graph data. They have emerged as powerful tools for processing graphs with complex relations (see Wu et al. [2019] for great reference). Such networks have successfully been applied to image segmentation [Gong et al., 2019], program reasoning [Allamanis et al., 2018], and combinatorial optimization [Li et al., 2018] tasks.</p>
<p>Neural algorithm synthesis and induction is a widely explored topic for 1D sequence problems [Abolafia et al., 2020, Freivalds et al., 2019, Freivalds and Liepins, 2018, Kaiser and Sutskever, 2015, Draguns et al., 2020 but for 2D data, only a few works exist. Shin et al. [2018] has proposed Kerel program synthesis from the input-output image pairs and execution trace. Differentiable Neural Computer [Graves et al., 2016], which employs external memory, has been applied to the SGRDLU puzzle game, shortest path finding and traversal tasks on small synthetic graphs. Several neural network architectures have been developed for learning to play board games [Silver et al., 2018], including chess [David et al., 2016] and go [Silver et al., 2016[Silver et al., , 2017, often using complex architectures or reinforcement learning.</p>
<p>1D Shuffle-Exchange Networks</p>
<p>Here we review the Neural Shuffle-Exchange (NSE) network for sequence-to-sequence processing, recently introduced by Freivalds et al. [2019] and revised by Draguns et al. [2020]. This architecture offers an efficient alternative to the attention mechanism and allows modeling of long-range dependencies of sequences in O(n log n) time.</p>
<p>The network works for sequences of length 2 k for k ∈ Z + and is built of alternating Switch and Shuffle layers. Although all the levels are of the same structure, a network formed of 2k − 2 Switch and Shuffle layers can learn a wide class of functions, including arbitrary permutation of the elements. Such a network of depth 2k − 2 is called a Beneš block. A deeper and more powerful network may be produced by stacking several Beneš blocks, for most tasks two of them are enough.</p>
<p>In the Switch layer, elements of the input sequence are divided into adjacent non-overlapping pairs, and the Switch Unit is applied to each pair. The Residual Switch Unit (RSU) proposed by Draguns et al. [2020] works the best. RSU has two inputs and two outputs and two linear transformations on the feature dimension. After the first transformation, Layer Normalization (LayerNorm) without output gain and bias parameters [Xu et al., 2019] and Gaussian Error Linear Unit (GELU) [Hendrycks and Gimpel, 2016] follow. By default, the middle layer has 2x more feature maps than the first layer. The second linear transformation is applied after GELU and its output is scaled by a scalar h. Additionally, the output of the unit is connected to its input using a residual connection that is scaled by a learnable parameter S. RSU is defined as fallows:
i = [i 1 , i 2 ] g = GELU(LayerNorm(Zi)) c = W g + B [o 1 , o 2 ] = σ(S) i + h c
In the above equations, Z, W are weight matrices of size 2m × 4m and 4m × 2m, respectively, where m is the number of feature maps; S is a vector of size 2m and B is a bias vector -all of those are learnable parameters; denotes element-wise vector multiplication and σ is the sigmoid function. S is initialized as σ −1 (r) and h is initialized as √ 1 − r 2 * 0.25, where r = 0.9.</p>
<p>The Shuffle layer performs the perfect shuffle permutation of the sequence elements. In this permutation, the destination address of an element is obtained by cyclic bit rotation of its source address. The rotation to the right is used for the first half of the Beneš block, to the left for the second half. Shuffle layers do not have learnable parameters.</p>
<p>The first k − 1 Switch and Shuffle layers of the Beneš block forms Shuffle-Exchange block, the rest of the layers form its mirror counterpart. Weight sharing is employed between Switch layers of the same Shuffle-Exchange block.</p>
<p>The Switchblade Model</p>
<p>We propose Switchblade architecture -an adaption of NSE network to two dimensions. The model is suitable for processing n × n input array, where n = 2 k for k ∈ Z + and each element is vector of m feature maps. Inputs that don't fulfill this requirement has to be padded to the closest 2 k × 2 k array. The Switchblade model works by rearranging the input matrix into a 1D sequence (Quaternary Flatten layer), applying several interleaved Quaternary Switch (QSwitch) and Quaternary Shuffle (QShuffle) layers and then converting data back to 2D by Quaternary Reshape layer, as depicted in Fig. 1. Figure 1: Switchblade model consists of one or more Beneš block that is enclosed by Quaternary Flatten and Quaternary Reshape layers. For a 2 k × 2 k input array, Beneš block has total of 2k − 1 Quaternary Switch and 2k − 2 Quaternary Shuffle layers, giving rise to O(n 2 log n) time complexity.</p>
<p>The middle part involving QSwitch and QShuffle layers is structured as one or more Beneš blocks the same way as in the NSE network, but the layers themselves are different, performing the computation in groups of four instead of two. Namely, each Beneš block consists of interleaved k − 1 QSwitch and QShuffle layers, followed by k − 1 QSwitch and Inverse QShuffle layers, eventually finalizing block with one more QSwitch layer. Weight sharing is employed between the first k − 1 QSwitch layers. Similarly, the next k − 1 QSwitch layers also have shared weights. The last QSwitch layer of the Beneš block does not participate in weight sharing. If the network has more than one Beneš block, each receives a distinct set of weights.</p>
<p>The Quaternary Flatten layer transforms a 2D matrix into a sequence by reading out values according to the Z-order-curve. 1 The resulting ordering is the same as one would get from a depth-first traversal of a quadtree. Such transformation is required to preserve element locality in the sequence representation, and we validate its necessity in ablation study (see Appendix A). The left matrix in the Fig. 2 shows correctly indexed matrix of size 4 × 4. The Quaternary Reshape layer implements the inverse operation and transforms the sequence back to a matrix, according to the same indexing method inverted.</p>
<p>In the QSwitch layer, we divide adjacent elements of the sequence into non-overlapping 4 element tuples. That is implemented by reshaping the input sequence into a 4 times shorter sequence where 4 elements are concatenated along the feature dimension as
[i 1 , i 2 , i 3 , i 4 ]. Then Quaternary Switch Unit (QSU) is applied to each tuple. QSU is based on RSU but has 4 inputs [i 1 , i 2 , i 3 , i 4 ] and 4 outputs [o 1 , o 2 , o 3 , o 4 ].
We apply a small dropout (rate 0.1 works well) on the input elements -this helps with generalization on larger inputs. The rest of the Unit structure is left unchanged from RSU. The QSU is defined as follows:
i = [i 1 , i 2 , i 3 , i 4 ] d = Dropout(i) g = GELU(LayerNorm(Zd)) c = W g + B [o 1 , o 2 , o 3 , o 4 ] = σ(S) i + h c
In the above equations, Z, W are weight matrices of size 4m × 8m and 8m × 4m, respectively; S is a vector of size 4m and B is a bias vector − all of those are learnable parameters; h is a scalar value; denotes element-wise vector multiplication and σ is the sigmoid function. We initialize S as σ −1 (0.9) and h as √ 1 − 0.9 2 * 0.25. Shuffle permutation can be interpreted as splitting matrix rows into two halves (white and green) and interleaving the halves, then applying the same transformation to columns (white and red).</p>
<p>The QShuffle layer rearranges the elements according to the cyclic digit rotation permutation. This permutation for a sequence S is defined as
S[x] = S[qrotate(x, k)],
where qrotate(x, k) applies cyclic shift by one digit to x which is treated as a quaternary (base 4) number and k is its length in quaternary digits. 2 For the first k − 1 QShuffle layers of the Beneš block, we apply rotation to the right -to the left for the remaining k − 1 layers.</p>
<p>Combining with convolutions</p>
<p>For image processing tasks we combine the Switchblade model with convolutional layers. That allows training and evaluation on larger images and improves accuracy, we validate this in ablation study (see Appendix A). Fig. 3 depicts the Switchblade architecture used for the CityScapes segmentation task, where Switchblade is enclosed by 3 convolutional and 3 transposed convolutional layers. For the first 1 https://en.wikipedia.org/wiki/Z-order_curve 2 note that it matches the definition of k above derived from n and the last layers 1 × 2 stride is used to change the aspect ratio of the image to a square, suitable for Switchblade. The rest of the layers employ 2 × 2 stride. Skip connections between the corresponding convolution and transposed convolution layers are used. Additionally, Layer Normalization is used for the first 3 convolution layers. </p>
<p>Evaluation</p>
<p>We have implemented our proposed architecture in TensorFlow. The code is available on GitHub. 3 All models are trained and evaluated on single Nvidia RTX 2080 Ti (11 GB) GPU using Rectified Adam (RAdam) optimizer  and softmax cross-entropy loss, if not stated differently.</p>
<p>We evaluate the Switchblade model on several 2D tasks. For image tasks, we chose two widely used benchmarks: CIFAR-10 image classification and CityScapes semantic segmentation. But our main emphasis is on hard, previously unsolved tasks: algorithmic tasks on matrices and graphs and logical reasoning tasks -solving Sudoku puzzle and predicting the mating move in chess.</p>
<p>To achieve generalization on algorithmic tasks, we use curriculum learning introduced by Freivalds and Liepins [2018]. For each training size, from smallest to largest, we initiate the Switchblade model. All initiated models share the same weights and are trained simultaneously. Each feature of input data is placed on the first model that completely fits it.</p>
<p>We additionally conduct an ablation study to confirm that the model in the present form is the best for the chosen tasks. Ablation results are given in Appendix A.</p>
<p>Inferring matrix algorithms</p>
<p>We selected four matrix tasks of varying difficulty -transpose, rotation by 90 degrees, element-wise XOR of two matrices, and matrix squaring. For each task, we generate random inputs and train Switchblade to give the right output. For the transpose and rotation tasks, we generate a random input matrix from alphabet 1-11 and expect the network to learn the required element permutation. For the XOR task, we generate two binary matrices, place them side by side, add separator symbols between them, then apply padding to obtain a square matrix.</p>
<p>Matrix squaring is the hardest of the selected tasks. We generate a random binary square matrix and ask the model to output its square (matrix multiplication with itself) modulo 2. This is a challenging task for the Switchblade model having the computation power of O(n 2 log n), while the currently lowest asymptotic complexity for the matrix multiplication algorithm is O(n 2.373 ) [Le Gall, 2014b]. The lower bound for matrix multiplication for real and complex numbers in the model of arithmetic circuits is Ω(n 2 log n) proven by Raz [2002]. Matrix squaring has the same complexity as matrix multiplication since these problems can be easily reduced one to the other. We selected matrix squaring for evaluation because it has only one input.</p>
<p>Switchblade with 2 Beneš blocks and 96 feature maps is trained on inputs up to size 32 × 32 and evaluated on size up to 1024 × 1024. All tasks reach perfect accuracy on the test examples of size up to 32 × 32 (matrix squaring task requires more than 500k steps to converge) and all tasks, besides matrix square, generalize on larger inputs, see Fig. 4 and 5.</p>
<p>To see the benefits of Switchblade, we compared it with a Convolutional Neural Network on the matrix transpose task which is the easiest one. The convolutional network is not able to learn matrix transpose even on matrices of size 16 × 16. Results and the model details are given in Appendix B.  </p>
<p>Inferring graph algorithms</p>
<p>A graph can be naturally represented by its adjacency matrix making Switchblade a natural choice for tasks on graphs, especially on dense graphs. We consider 3 graph algorithmic tasks -connected component labeling, transitivity, and triangle finding.</p>
<p>For the connected component labeling task, we initialize a labeled graph with random edge labels in range 2-100. The task is to label all edges of each connected component with the lowest label among all the component's edges. For the transitivity task, the goal is to add edges to a directed graph for every two vertices that have a transitive path of length 2 between them. The hardest graph algorithmic task is the triangle finding. The fastest currently known algorithm relies on matrix multiplication and has asymptotic complexity of O(n 2.373 ), where n is the vertex count [Alon et al., 1997]. Triangle finding is a fruitful field for quantum computing where an algorithm of quantum query complexitỹ O(n 5/4 ) [Le Gall, 2014a] has been achieved. Therefore it is interesting if Switchblade can infer an O(n 2 log n) time algorithm. For this task, we generate random complete bipartite graphs and add a few random edges. Although dense, such graphs have only a few triangles. The goal is to return all the edges belonging to any triangle.</p>
<p>We train Switchblade with 2 Beneš blocks and 192 feature maps on graphs up to 32 vertices and test on graphs with up to 1024 vertices. For these tasks, the model reaches perfect accuracy on test examples with up to 32 vertices and generalizes to moderately larger graphs, see Fig. 6 and 7. Interestingly, Switchblade performs better on triangle finding than matrix squaring task, although the best complexity algorithms for both of them rely on matrix multiplication.  </p>
<p>CIFAR-10 image classification</p>
<p>CIFAR-10[ Krizhevsky et al., 2009] is a conventional image classification task that consists of 50000 train and 10000 test 32 × 32 images in 10 mutually exclusive classes. We use Switchblade with 192 feature maps and 1 Beneš block that is prepended with 3 convolutional layers. After each convolutional layer, Layer Normalization and GELU is applied. The model has a total of 7.5M learnable parameters. Visualization of its architecture is given in Appendix C.</p>
<p>We augment the training data by randomly flipping the input image and train Switchblade for 200k iterations with batch size 32. The model without additional pretraining achieves 83.22% test accuracy. Switchblade outperforms a feed-forward network, that is trained on a highly augmented dataset having 78.62% [Lin et al., 2015] accuracy. But our model has somewhat worse accuracy than a typical fully-convolutional neural network, consisting of 9 convolutional layers that can achieve approximately 90% accuracy without additional pretraining [Hendrycks and Gimpel, 2016].</p>
<p>CityScapes image segmentation</p>
<p>CityScapes [Cordts et al., 2016] dataset focuses on the understanding of urban street scenes -an essential skill for autonomous vehicles. It consists of 5000 fine and 30000 coarse annotated 1024 × 2048 images of 50 cities in a wide range of weather and daytime conditions. We study the challenging pixel-level segmentation task. Switchblade with 1 Beneš block and 192 feature maps is trained on 512 × 1024 (half-resolution) images. Switchblade, additionally, is combined with convolutional layers, as described in section 4.1. Weighted softmax cross-entropy loss in combination with Jaccard loss is used. We train on the coarse annotated and then fine-tune on fine annotated images. Additionally, we use data augmentation employing random contrast, lighting, and horizontal flipping.  [Fourure et al., 2017] 69.80 88.09 Switchblade (half-resolution) (this work) 62.57 85.04 Fast-SCNN (half-resolution) [Poudel et al., 2019] 62.83 80.51</p>
<p>For each class and group, the accuracy is measured as Intersection over Union (IoU), also known as the Jaccard index. In table 1 the average IoU of all classes and groups is reported. Switchblade scores an average of 62.57 % IoU for classes and 85.04 % IoU for groups. Detailed results are available on the CityScapes Benchmark. 4 It's worth mentioning that other compared models are highly sophisticated fully-convolutional neural networks developed for image segmentation purposes and are trained on full-resolution images. Our model performs only 7.66% worse on the groups than the current state-of-the-art model, that has been pretrained on other datasets. Accuracy of our model could probably be improved if more feature maps and full-resolution images are used.</p>
<p>Solving Sudoku</p>
<p>Sudoku is a popular logic-based puzzle that requires a player to fill each row, column, and nine 3 × 3 subregions of the 9 × 9 board with a digit from 1 to 9. The same digit may not appear twice in the same row, column, or subregion. Solving a Sudoku puzzle requires intricate reasoning and can be a challenging task for a human player. Contemporary solvers typically use a backtracking search to solve Sudoku [Norvig, 2009, Crook, 2009, therefore it's interesting to see if our model can solve it in one step.</p>
<p>We use the Sudoku dataset created by Park [2018], whose train set has 1M computer-generated Sudoku puzzles, and the test set has 30 authentic games of various difficulties. The Switchblade model with 192 feature maps and 2 Beneš blocks is trained for 100K iterations in an end-to-end fashion. To eliminate the risk of overfitting, we employ randomized padding -the game board can be placed anywhere on a 16 × 16 matrix. Labels are always positioned in the left-top corner of the matrix.</p>
<p>The trained model can solve all the given test examples and achieves 100% accuracy. That is 14% improvement 5 compared to the original 86% accuracy obtained by a convolutional neural network [Park, 2018]. Figure 12: Sudoku puzzle from the test set and its solution obtained by Switchblade.</p>
<p>Playing chess</p>
<p>Chess is one of the oldest and most popular board games that has been intensively researched for years. Although finite in size, it is not yet completely solved even by using modern computers. However, sophisticated machine-learned algorithms are playing chess much better than any human in the world. To achieve this, deep learning, more precisely, reinforcement learning together with huge computation power has been used [Silver et al., 2017].</p>
<p>We investigate how Switchblade may be used for playing chess. We do not aim for solving the whole chess game but consider only one move from it -the last one where the white has to move a piece to checkmate. We call this task mate-in-one. We feed a chess position where the mate is possible and train Switchblade to output the mating move. The model is required to learn the moves of chess pieces and the conditions of the mate. Such non-trivial requirements force solver to utilize information from all over the board to obtain a solution.</p>
<p>We have developed a new dataset for this task, see its description in Appendix D. For this task, we use Switchblade with 192 feature maps and 2 Beneš blocks. Additionally, a dense layer that outputs logits for all 4096 possible position pairs (moves) on the board is used after the Switchblade model. The model is trained for 500k steps and achieves 72.5% accuracy on a validation set. By examining the solutions obtained, we found that our model learns such chess concepts as legal moves for each piece, attacking a chess piece, giving a check to the king, and, of course, the checkmate itself. The incorrect solutions mainly do not take into account opponent pieces that can move and cover up the king from the attack.</p>
<p>Conclusions</p>
<p>We have introduced the Switchblade model with a total complexity of O(n 2 log n) for n × n input data. For model evaluation, we have proposed 2D algorithmic tasks on matrices and graphs and shown that Switchblade can infer O(n 2 log n) time complexity algorithms purely from input-output examples. For most of the algorithmic tasks, our model generalizes on larger inputs. Additionally, the Switchblade model is evaluated on the Sudoku puzzle-solving task and our introduced chess dataset mate-in-one. On both tasks, the model achieves great results. Unfortunately, on image 5 The exact numbers may be misleading due to the very small test set classification and segmentation, our model doesn't achieve the same accuracy as sophisticated taskspecific convolutional neural networks. We are excited to see our model incorporated into other learning frameworks and applied to new problems. We likewise hope that our proposed datasets will serve as future benchmarks for 2D algorithmic and reasoning tasks.</p>
<p>Broader Impact</p>
<p>The proposed Switchblade model is principally new and different from the existing alternatives. We have validated it as a powerful tool for 2D data processing where both local and long-range dependency modeling is required, for example, on algorithmic tasks. Its application to unforeseen areas could yield breakthroughs in deep learning and science as a whole.</p>
<p>In our paper, we present a way for solving hard previously unsolved tasks, where contemporary convolutional architectures do not give satisfactory results. Improvements in hard task solving and algorithm learning coupled with ever-increasing computational resources may eventually lead to the development of new yet unknown algorithms that are beyond human abilities to come up with. Similar tendencies already exist in deep learning uses for image and natural language processing. Our success in hard algorithmic tasks -matrix squaring and triangle finding tasks -for which the known algorithms are far from optimal, encourages us to believe that such algorithm automatic creation is possible. Opportunity to create algorithms just from a large amount of input/output examples could lead to the next industrial revolution, allowing rapid automation process even of jobs that require intricate reasoning ability.</p>
<p>Unfortunately, a deep learning model representing an algorithm acts like a black-box forbidding us to look into the working of the algorithm and validate its correctness. Unexplainable models could be potentially dangerous if they are incorporated in larger production systems, opening opportunities for a new range of security threats. That underlays the obligation for explainable deep learning models and encourages future research in this area.</p>
<p>Our proposed mate-in-one chess dataset and datasets for graph and matrix algorithms could become conventional future benchmarks for logical reasoning and algorithmic learning tasks.</p>
<p>This work as a whole may encourage researchers in making bigger steps towards solving different kinds and increasingly hard problems, not just advancing incremental progress on the existing ones.</p>
<p>A Ablation study</p>
<p>We study several ablations on matrix squaring, triangle finding, and CIFAR-10 tasks, as they represent a distinct set of problems.</p>
<p>Quaternary Flatten and Reshape enclose Beneš blocks of the Switchblade model and transforms a 2D input array into a sequence and vice-versa. Both layers transform input array in such a way, that preserves the locality of elements. For all tasks, we substitute Quaternary Flatten and Reshape layers with regular reshape operation (standard reshape) that concatenates rows of a 2D array to obtain a 1D sequence.</p>
<p>We also assess the effect of the feature map (feature maps) count and the model depth (Beneš blocks) on the convergence speed and accuracy. For the matrix squaring task, we use Switchblade with 2 Beneš blocks and 96 feature maps as a baseline, but, for triangle finding, Switchblade with 2 Beneš blocks and 192 feature maps is chosen as a baseline.</p>
<p>Switchblade model with 1 Beneš block, 192 feature maps, and 3 prepended convolutional layers is used as a baseline for CIFAR-10 task. We additionally verify the need for convolutional layers in this model by comparing them with a version without them (no convolutions). The ablation results are depicted on Fig. 13, Fig. 15 and Fig. 14.   We can see that the proposed baseline models work best for chosen tasks. On matrix squaring and triangle finding tasks Quaternary Flatten and Reshape layers drastically increase convergence speed and accuracy compared to regular reshape operation (standard reshape). A larger (feature maps) and deeper model (Beneš blocks) in most cases yield better results, but when reasonable accuracy is obtained, diminishing improvements are observed. We likewise observe that convolutional layers improve accuracy on the CIFAR-10 task. Additionally, we have done a comparable ablation study for the rest of the tasks yielding similar results.</p>
<p>B Convolutional Neural Network for algorithmic tasks</p>
<p>We compare the Switchblade model with Convolutional Neural Network (CNN) on the matrix transpose task, which is the easiest of proposed algorithmic tasks. For this task, we use Switcblade model with 2 Beneš blocks and 48 feature maps. Table. 2 depicts CNN used for this purpose, it consists of 9 convolutional layers (conv) with 3 × 3 kernel, and each layer is followed by GELU nonlinearity. We train both models for 50k iterations with the same hyperparameter set. After 50k iterations both models give diminishing accuracy improvements.</p>
<p>We train both models using curriculum learning [Freivalds and Liepins, 2018] on matrices up to size 32 × 32. Fig. 16 depicts error rate on test instances while training. Switchblade model quickly achieves perfect accuracy and generalizes perfectly on matrices up to size 1024 × 1024, yet CNN architecture fails to learn matrix transpose even on a 16 × 16 matrices. </p>
<p>D Mate-in-One Dataset</p>
<p>The Chess-Mate-In-One dataset contains problem-solution pairs for chess puzzles where the white has to move a piece to checkmate. We call this task mate-in-one (see Fig. 18). Chess position where the mate is possible in one move is a problem, and a mating move is a solution. Figure 18: Chess problem mate-in-one with a single solution -Qg3. Figure 19: Chess problem mate-in-one with multiple solutions -Qxe4, Nf7, Ng4.</p>
<p>Several chess-related datasets are available, but they do not provide mate-in-one positions directly. Therefore we created our own dataset from the chess database by Eduardo Sadier 6 , which contains 155K problems with mate in two moves. For each such instance we run the Stockfish chess engine 7 to find the solution and recorded the positions after one black move as the mate-in-one problems. Since there are more than one option for the black, we obtain several mate-in-one problems from every mate-in-two problem. After removing duplicates, we obtained~2.7M unique chess mate-in-one problems and their solutions. Note that promotion is a move type that requires additional information -the piece type the pawn is promoted to. Our data set does not include it. It should be noted, that there are chess problems with multiple solutions, see Fig. 19. In this case, just one of the problem-solution pairs is included in the dataset. It turned out that there are~0.7M problems with multiple solutions and~2M problems with a single solution. We split the data into~2.43M example train and~0.27M example validation sets so that the validation set contains only problems with a single solution. The dataset is available on GitHub. 8</p>
<p>Figure 2 :
2Depiction how the Quaternary Shuffle layer permutes the matrix. The left image shows the matrix elements ordered by the Quaternary Flatten, where the ordering indices are represented by base-4 numbers. The right image shows element order after the Quaternary Shuffle. Quaternary</p>
<p>Figure 3 :
3Switchblade enclosed by 3 strided convolutional and 3 transposed strided convolutions, forming U-Net[Ronneberger et al., 2015] style architecture. Such design improves accuracy and evaluation time on image tasks.</p>
<p>Figure 4 :
4Error on test instances up to size 32 × 32 while training.</p>
<p>Figure 5 :
5Generalization on larger test examples. The model is trained on size 32 × 32.</p>
<p>Figure 6 :
6Error on test instances with up to 32 vertices while training.</p>
<p>Figure 7 :
7Generalization on larger test instances. The model is trained on graphs with up to 32 vertices.</p>
<p>Figure 8 Figure 11 :
811Example from CityScapes validation set on pixel-level semantic segmentation task and prediction obtained by the Switchblade model.</p>
<p>Figure 13 :
13Matrix squaring ablation on inputs of size 32 × 32. Baseline has 2 Beneš blocks and 96 feature maps.</p>
<p>Figure 14 :
14Ablation study on CIFAR-10 task. Baseline has 1 Beneš block, 192 feature maps and prepended 3 convolutional layers.</p>
<p>Figure 15 :
15Triangle finding ablation on graphs with 32 vertices. Baseline has 2 Beneš blocks and 192 feature maps.</p>
<p>Table 1 :
1Results on CityScapes pixel-level semantic labeling task. Higher IoU is better.Model 
IoU Classes IoU Groups </p>
<p>HRNetV2 + OCR + SegFix [Yuan et al., 2019] 
84.5 
92.7 
GridNet </p>
<p>Table 2 :
2CNN model for matrix transpose task. × 3 conv with GELU 96 3 × 3 conv with GELU 96 3 × 3 conv with GELU 96 3 × 3 conv with GELU 192 3 × 3 conv with GELU 192 3 × 3 conv with GELU 192 3 × 3 conv with GELU 384 3 × 3 conv with GELU 384 3 × 3 conv with GELU 384Linear transformation 12Figure 16: Switchblade and Convolutional Neural Network (Conv) error rate on test instances on matrix transpose task.Layer Type </p>
<h1>feature maps</h1>
<p>Embedding layer 
96 
3 
https://github.com/LUMII-Syslab/Switchblade
https://www.cityscapes-dataset.com/anonymous-results/?id= 1399a25129d659c67e78e7b2fdd3d28424fe6d9d1a26437c829299e1f77b2b92
https://sites.google.com/site/edusadier/theartofdirectmateintwomoves 7 https://stockfishchess.org 8 https://github.com/LUMII-Syslab/Switchblade/wiki/Chess-Dataset---Mate-in-oneproblems
Acknowledgments and Disclosure of FundingWe would like to thank the IMCS UL Scientific Cloud for the computing power and Leo Trukšans for the technical support. This research is funded by the Latvian Council of Science, project No. lzp-2018/1-0327.C Switchblade model for CIFAR-10 task CIFAR-10 tasks consist of tiny 32 × 32 images and require classifying each image into one of ten classes. We use Switchblade with 1 Beneš block and 192 feature maps for this task. Additionally, the Switchblade model is prepended with 3 convolutional layers with a 2 × 2 kernel. Each convolutional layer is followed by Layer Normalization and GELU nonlinearity. We use Layer Normalization without output bias and gain parameters, same as in Quaternary Switch Unit.Fig. 17depicts the Switchblade model for the CIFAR-10 task. The top-left element of the output is used as a result. We find that this works better than the global pooling layer at the end of the model.
D A Abolafia, R Singh, M Zaheer, C Sutton, arXiv:2003.04227Towards modular algorithm induction. arXiv preprintD. A. Abolafia, R. Singh, M. Zaheer, and C. Sutton. Towards modular algorithm induction. arXiv preprint arXiv:2003.04227, 2020.</p>
<p>Learning to represent programs with graphs. M Allamanis, M Brockschmidt, M Khademi, International Conference on Learning Representations. M. Allamanis, M. Brockschmidt, and M. Khademi. Learning to represent programs with graphs. In International Conference on Learning Representations, 2018.</p>
<p>Finding and counting given length cycles. N Alon, R Yuster, U Zwick, Algorithmica. 173N. Alon, R. Yuster, and U. Zwick. Finding and counting given length cycles. Algorithmica, 17(3): 209-223, 1997.</p>
<p>Diffusion-convolutional neural networks. J Atwood, D Towsley, Advances in neural information processing systems. J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in neural information processing systems, pages 1993-2001, 2016.</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473arXiv preprintD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Generating long sequences with sparse transformers. R Child, S Gray, A Radford, I Sutskever, arXiv:1904.10509arXiv preprintR. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.</p>
<p>The Cityscapes Dataset for Semantic Urban Scene Understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</p>
<p>A pencil-and-paper algorithm for solving sudoku puzzles. J Crook, Notices of the AMS. 564J. Crook. A pencil-and-paper algorithm for solving sudoku puzzles. Notices of the AMS, 56(4): 460-468, 2009.</p>
<p>Deepchess: End-to-end deep neural network for automatic learning in chess. O E David, N S Netanyahu, L Wolf, International Conference on Artificial Neural Networks. SpringerO. E. David, N. S. Netanyahu, and L. Wolf. Deepchess: End-to-end deep neural network for automatic learning in chess. In International Conference on Artificial Neural Networks, pages 88-96. Springer, 2016.</p>
<p>Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences. A Draguns, E Ozolin, Š , A Šostaks, M Apinis, K Freivalds, A. Draguns, E. Ozolin , š, A. Šostaks, M. Apinis, and K. Freivalds. Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences, 2020.</p>
<p>Residual conv-deconv grid network for semantic segmentation. D Fourure, R Emonet, É Fromont, D Muselet, A Trémeau, C Wolf, arXiv:1707.07958arXiv preprintD. Fourure, R. Emonet, É. Fromont, D. Muselet, A. Trémeau, and C. Wolf. Residual conv-deconv grid network for semantic segmentation. arXiv preprint arXiv:1707.07958, 2017.</p>
<p>Improving the Neural GPU architecture for algorithm learning. K Freivalds, R Liepins, The ICML workshop Neural Abstract Machines &amp; Program Induction v2 (NAMPI 2018). K. Freivalds and R. Liepins. Improving the Neural GPU architecture for algorithm learning. The ICML workshop Neural Abstract Machines &amp; Program Induction v2 (NAMPI 2018), 2018.</p>
<p>Neural Shuffle-Exchange Networks-Sequence Processing in O (n log n) Time. K Freivalds, E Ozolin, A Šostaks, Advances in Neural Information Processing Systems. K. Freivalds, E. Ozolin , š, and A. Šostaks. Neural Shuffle-Exchange Networks-Sequence Processing in O (n log n) Time. In Advances in Neural Information Processing Systems, pages 6626-6637, 2019.</p>
<p>Graphonomy: Universal human parsing via graph transfer learning. K Gong, Y Gao, X Liang, X Shen, M Wang, L Lin, CVPR. K. Gong, Y. Gao, X. Liang, X. Shen, M. Wang, and L. Lin. Graphonomy: Universal human parsing via graph transfer learning. In CVPR, 2019.</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwińska, S G Colmenarejo, E Grefenstette, T Ramalho, J Agapiou, Nature. 5387626A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwińska, S. G. Col- menarejo, E. Grefenstette, T. Ramalho, J. Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476, 2016.</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.</p>
<p>D Hendrycks, K Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). arXiv preprintD. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.</p>
<p>Ł Kaiser, I Sutskever, arXiv:1511.08228Neural GPUs learn algorithms. arXiv preprintŁ. Kaiser and I. Sutskever. Neural GPUs learn algorithms. arXiv preprint arXiv:1511.08228, 2015.</p>
<p>Learning multiple layers of features from tiny images. A Krizhevsky, G Hinton, A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.</p>
<p>Improved quantum algorithm for triangle finding via combinatorial arguments. F , Le Gall, IEEE 55th Annual Symposium on Foundations of Computer Science. IEEEF. Le Gall. Improved quantum algorithm for triangle finding via combinatorial arguments. In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pages 216-225. IEEE, 2014a.</p>
<p>Powers of tensors and fast matrix multiplication. F , Le Gall, Proceedings of the 39th international symposium on symbolic and algebraic computation. the 39th international symposium on symbolic and algebraic computationF. Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings of the 39th international symposium on symbolic and algebraic computation, pages 296-303, 2014b.</p>
<p>Combinatorial optimization with graph convolutional networks and guided tree search. Z Li, Q Chen, V Koltun, Advances in Neural Information Processing Systems. Z. Li, Q. Chen, and V. Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. In Advances in Neural Information Processing Systems, pages 539-548, 2018.</p>
<p>How far can we go without convolution: Improving fullyconnected networks. Z Lin, R Memisevic, K Konda, arXiv:1511.02580arXiv preprintZ. Lin, R. Memisevic, and K. Konda. How far can we go without convolution: Improving fully- connected networks. arXiv preprint arXiv:1511.02580, 2015.</p>
<p>L Liu, H Jiang, P He, W Chen, X Liu, J Gao, J Han, arXiv:1908.03265On the variance of the adaptive learning rate and beyond. arXiv preprintL. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019.</p>
<p>Neural network for graphs: A contextual constructive approach. A Micheli, IEEE Transactions on Neural Networks. 203A. Micheli. Neural network for graphs: A contextual constructive approach. IEEE Transactions on Neural Networks, 20(3):498-511, 2009.</p>
<p>Solving every sudoku puzzle. P Norvig, PreprintP. Norvig. Solving every sudoku puzzle. Preprint, 2009.</p>
<p>Can convolutional neural networks crack sudoku puzzles?. K Park, K. Park. Can convolutional neural networks crack sudoku puzzles? https://github.com/ Kyubyong/sudoku, 2018.</p>
<p>Fast-scnn: fast semantic segmentation network. R P Poudel, S Liwicki, R Cipolla, arXiv:1902.04502arXiv preprintR. P. Poudel, S. Liwicki, and R. Cipolla. Fast-scnn: fast semantic segmentation network. arXiv preprint arXiv:1902.04502, 2019.</p>
<p>On the complexity of matrix product. R Raz, Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. the thiry-fourth annual ACM symposium on Theory of computingR. Raz. On the complexity of matrix product. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 144-151, 2002.</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234-241. Springer, 2015.</p>
<p>Improving neural program synthesis with inferred execution traces. E C Shin, I Polosukhin, D Song, Advances in Neural Information Processing Systems. E. C. Shin, I. Polosukhin, and D. Song. Improving neural program synthesis with inferred execution traces. In Advances in Neural Information Processing Systems, pages 8917-8926, 2018.</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 5297587484D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, arXiv:1712.01815arXiv preprintD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Ku- maran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, Science. 3626419D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Ku- maran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144, 2018.</p>
<p>Very deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.</p>
<p>Deep high-resolution representation learning for human pose estimation. K Sun, B Xiao, D Liu, J Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionK. Sun, B. Xiao, D. Liu, and J. Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5693-5703, 2019.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.</p>
<p>Z Wu, S Pan, F Chen, G Long, C Zhang, P S Yu, arXiv:1901.00596A comprehensive survey on graph neural networks. arXiv preprintZ. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.</p>
<p>Understanding and improving layer normalization. J Xu, X Sun, Z Zhang, G Zhao, J Lin, Advances in Neural Information Processing Systems. J. Xu, X. Sun, Z. Zhang, G. Zhao, and J. Lin. Understanding and improving layer normalization. In Advances in Neural Information Processing Systems, pages 4383-4393, 2019.</p>
<p>F Yu, V Koltun, arXiv:1511.07122Multi-scale context aggregation by dilated convolutions. arXiv preprintF. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015.</p>
<p>. Y Yuan, X Chen, J Wang, arXiv:1909.11065Object-Contextual Representations for Semantic Segmentation. arXiv preprintY. Yuan, X. Chen, and J. Wang. Object-Contextual Representations for Semantic Segmentation. arXiv preprint arXiv:1909.11065, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
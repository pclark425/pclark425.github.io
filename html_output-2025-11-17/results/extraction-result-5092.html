<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5092 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5092</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5092</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e" target="_blank">On the Paradox of Learning to Reason from Data</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has, in fact, learned statistical features that inherently exist in logical reasoning problems.</p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be trained end-to-end to solve logical reasoning problems presented in natural language? We attempt to answer this question in a confined problem space where there exists a set of parameters that perfectly simulates logical reasoning. We make observations that seem to contradict each other: BERT attains near-perfect accuracy on in-distribution test examples while failing to generalize to other data distributions over the exact same problem space. Our study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has, in fact, learned statistical features that inherently exist in logical reasoning problems. We also show that it is infeasible to jointly remove statistical features from data, illustrating the difficulty of learning to reason in general. Our result naturally extends to other neural models (e.g. T5) and unveils the fundamental difference between learning to reason and learning to achieve high performance on NLP benchmarks using statistical features.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5092.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5092.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (base pretrained Transformer encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained Transformer-based language model (Devlin et al., 2019) fine-tuned in this paper to evaluate deductive propositional reasoning on the SimpleLogic benchmark; used as the primary empirical model for all training/evaluation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained Transformer encoder (BERT) used as the backbone and fine-tuned end-to-end on SimpleLogic datasets (templated natural-language encodings of propositional definite-clause reasoning problems). The paper treats BERT as a representative Transformer-based NLP model and fine-tunes it on large sampled datasets from the SimpleLogic problem space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>SimpleLogic (propositional definite-clause deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A confined benchmark introduced in this paper consisting of templated natural-language encodings of propositional definite-clause reasoning problems (facts, rules, single query). Problem constraints: predicates 5–30 drawn from fixed vocab, rules with bodies of length 1–3, facts 1..#pred, reasoning depth 0..6; labels indicate whether query is provable by forward-chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning BERT end-to-end on sampled SimpleLogic datasets using two principal sampling regimes: Rule-Priority (RP) and Label-Priority (LP), and mixtures/variants (RP_balance, LP*, RP&LP). Training hyperparameters reported: 20 epochs, learning rate 4e-5, warmup ratio 0.05, batch size 64. Evaluation includes in-distribution testing and cross-distribution (train on RP test on LP, vice versa), and experiments with balanced/downsampled datasets to remove statistical features.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In-distribution performance was near-perfect: e.g., BERT trained on RP and tested on RP achieved accuracies by reasoning depth (0..6): [99.9%, 99.8%, 99.7%, 99.3%, 98.3%, 97.5%, 95.5%]. BERT trained on LP and tested on LP: [100.0%, 100.0%, 99.9%, 99.9%, 99.7%, 99.7%, 99.0%]. Cross-distribution performance dropped substantially: RP-trained tested on LP: [99.8%, 99.8%, 99.3%, 96.0%, 90.4%, 75.0%, 57.3%]; LP-trained tested on RP: [97.3%, 66.9%, 53.0%, 54.2%, 59.5%, 65.6%, 69.2%]. Training on RP&LP mixture still yielded near-perfect in-distribution but failed on slightly different LP* (mixture -> LP*: [98.1%, 97.2%, 92.5%, 80.3%, 65.8%, 55.6%, 55.2%]). Training on RP_balance (where #rule statistic is balanced) improved OOD generalization (on LP depth-6 accuracy improved from 57.3% to 68.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>BERT attains high in-distribution accuracy by exploiting statistical features inherent to reasoning examples (e.g., #rule, #fact, branching_factor and complex compositional statistics) rather than learning the true deductive function; this leads to dramatic failures when the sampling distribution changes (cross-distribution generalization failure). Even high in-distribution accuracies did not reach 100% and accuracy degrades with greater reasoning depth under distribution shifts. Removing a single statistical feature helps but is computationally costly; jointly removing many features is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Contrasted with a constructed parameterization of BERT (see next entry) that provably implements forward-chaining and achieves 100% on tasks up to the constructed depth, the standard BERT fine-tuned on sampled data fails to learn the correct reasoning function and instead learns dataset-specific statistical shortcuts. Balancing the #rule statistic (RP_balance) improved generalization compared to RP-trained model. The observed behavior contradicts claims that finetuned Transformers reliably learn logical reasoning (paper cites Clark et al. 2020 and Talmor et al. 2020 as prior claims).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analyses demonstrate that simple statistics are predictive of labels (Pr(label=1 | #rule) increases with #rule; branching_factor correlates negatively). Creating RP_balance by down-sampling to remove #rule as a statistical feature required pre-sampling k× more data; authors balanced #rule for up to 80 rules using k≈10 (5.6M pre-samples) and note full balancing up to 110 would require k>100. Jointly balancing multiple features shows rapidly increasing sampling factors (examples: balancing #fact=f=15 required k≈5.5; balancing #fact=15 together with branching_factor in [2.65,2.75] required k≈20.0; adding #rule=58 required k≈55.6), illustrating exponential sample requirements to remove compositional statistical features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Paradox of Learning to Reason from Data', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5092.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5092.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constructed BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constructed parameterization of BERT simulating forward-chaining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit parameter construction (presented in this paper) of a BERT-base model that simulates the forward-chaining deductive algorithm and thus provably solves SimpleLogic instances up to the constructed reasoning depth with perfect accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Constructed BERT (explicit parameterization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A BERT-base architecture with a hand-crafted set of parameters (first layer parsing, intermediate layers implementing one forward-chaining step each, final layer output) that encodes facts/rules as 'meaningful vectors' and uses attention+MLP dynamics to broadcast RHS truth values to LHS and update booleans deterministically. Implemented in PyTorch following BERT-base architecture; parameters are engineered (not learned) to implement logical forward-chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>SimpleLogic (propositional definite-clause deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same SimpleLogic task as above: templated natural-language encodings of propositional definite-clause problems; the constructed BERT is shown to exactly simulate forward-chaining on these encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Explicit constructive proof: design of predicate signatures (low inner-product), encoding rules/facts/queries into 'meaningful vectors' stored in token embeddings, use of attention heads with scaled keys (constant β) to make attention peaky for signature matches, and two-layer MLPs to update RHS boolean values using thresholded sums (effectively implementing logical conjunction). The construction maps one reasoning step to one layer; theorem states an n-layer BERT can solve examples requiring ≤ n-2 reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The constructed BERT (implemented following BERT-base architecture) solved all SimpleLogic problems requiring reasoning depth ≤ 10 with 100% accuracy in their implementation; theoretical statement: for BERT with n layers there exists parameters to solve any instance requiring ≤ n-2 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This construction demonstrates representational capacity but is not learned from data; it shows existence of parameters that implement exact reasoning but does not address whether such parameters are reachable via standard training on sampled data (empirically, fine-tuned BERT did not find them). The construction relies on careful engineered parameter assignments (predicate signatures, large β constants, bespoke MLP thresholds).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Direct contrast to empirically trained BERT: constructed BERT attains provable perfect performance up to its constructed depth while the data-trained BERTs (same architecture) fail to generalize across sampling distributions. Thus capacity is not the limiting factor—learnability from sampled data is the bottleneck due to statistical features in data.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Theorem and constructive proof (sketch in main text, full proof in appendix) showing mapping between layers and forward-chaining steps; empirical implementation verifies 100% accuracy up to depth 10 using only a portion of parameters. No ablation of the construction itself is reported (construction is presented as an existence proof).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Paradox of Learning to Reason from Data', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge <em>(Rating: 2)</em></li>
                <li>What can neural networks reason about? <em>(Rating: 2)</em></li>
                <li>Learning a SAT solver from single-bit supervision <em>(Rating: 1)</em></li>
                <li>Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver <em>(Rating: 1)</em></li>
                <li>Teaching temporal logics to neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5092",
    "paper_id": "paper-5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "BERT-base",
            "name_full": "BERT (base pretrained Transformer encoder)",
            "brief_description": "A pre-trained Transformer-based language model (Devlin et al., 2019) fine-tuned in this paper to evaluate deductive propositional reasoning on the SimpleLogic benchmark; used as the primary empirical model for all training/evaluation experiments.",
            "citation_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_description": "A pretrained Transformer encoder (BERT) used as the backbone and fine-tuned end-to-end on SimpleLogic datasets (templated natural-language encodings of propositional definite-clause reasoning problems). The paper treats BERT as a representative Transformer-based NLP model and fine-tunes it on large sampled datasets from the SimpleLogic problem space.",
            "model_size": null,
            "logical_reasoning_task": "SimpleLogic (propositional definite-clause deductive reasoning)",
            "task_description": "A confined benchmark introduced in this paper consisting of templated natural-language encodings of propositional definite-clause reasoning problems (facts, rules, single query). Problem constraints: predicates 5–30 drawn from fixed vocab, rules with bodies of length 1–3, facts 1..#pred, reasoning depth 0..6; labels indicate whether query is provable by forward-chaining.",
            "method_or_approach": "Fine-tuning BERT end-to-end on sampled SimpleLogic datasets using two principal sampling regimes: Rule-Priority (RP) and Label-Priority (LP), and mixtures/variants (RP_balance, LP*, RP&LP). Training hyperparameters reported: 20 epochs, learning rate 4e-5, warmup ratio 0.05, batch size 64. Evaluation includes in-distribution testing and cross-distribution (train on RP test on LP, vice versa), and experiments with balanced/downsampled datasets to remove statistical features.",
            "performance": "In-distribution performance was near-perfect: e.g., BERT trained on RP and tested on RP achieved accuracies by reasoning depth (0..6): [99.9%, 99.8%, 99.7%, 99.3%, 98.3%, 97.5%, 95.5%]. BERT trained on LP and tested on LP: [100.0%, 100.0%, 99.9%, 99.9%, 99.7%, 99.7%, 99.0%]. Cross-distribution performance dropped substantially: RP-trained tested on LP: [99.8%, 99.8%, 99.3%, 96.0%, 90.4%, 75.0%, 57.3%]; LP-trained tested on RP: [97.3%, 66.9%, 53.0%, 54.2%, 59.5%, 65.6%, 69.2%]. Training on RP&LP mixture still yielded near-perfect in-distribution but failed on slightly different LP* (mixture -&gt; LP*: [98.1%, 97.2%, 92.5%, 80.3%, 65.8%, 55.6%, 55.2%]). Training on RP_balance (where #rule statistic is balanced) improved OOD generalization (on LP depth-6 accuracy improved from 57.3% to 68.1%).",
            "limitations_or_failure_cases": "BERT attains high in-distribution accuracy by exploiting statistical features inherent to reasoning examples (e.g., #rule, #fact, branching_factor and complex compositional statistics) rather than learning the true deductive function; this leads to dramatic failures when the sampling distribution changes (cross-distribution generalization failure). Even high in-distribution accuracies did not reach 100% and accuracy degrades with greater reasoning depth under distribution shifts. Removing a single statistical feature helps but is computationally costly; jointly removing many features is infeasible.",
            "comparison": "Contrasted with a constructed parameterization of BERT (see next entry) that provably implements forward-chaining and achieves 100% on tasks up to the constructed depth, the standard BERT fine-tuned on sampled data fails to learn the correct reasoning function and instead learns dataset-specific statistical shortcuts. Balancing the #rule statistic (RP_balance) improved generalization compared to RP-trained model. The observed behavior contradicts claims that finetuned Transformers reliably learn logical reasoning (paper cites Clark et al. 2020 and Talmor et al. 2020 as prior claims).",
            "ablation_or_analysis_results": "Analyses demonstrate that simple statistics are predictive of labels (Pr(label=1 | #rule) increases with #rule; branching_factor correlates negatively). Creating RP_balance by down-sampling to remove #rule as a statistical feature required pre-sampling k× more data; authors balanced #rule for up to 80 rules using k≈10 (5.6M pre-samples) and note full balancing up to 110 would require k&gt;100. Jointly balancing multiple features shows rapidly increasing sampling factors (examples: balancing #fact=f=15 required k≈5.5; balancing #fact=15 together with branching_factor in [2.65,2.75] required k≈20.0; adding #rule=58 required k≈55.6), illustrating exponential sample requirements to remove compositional statistical features.",
            "uuid": "e5092.0",
            "source_info": {
                "paper_title": "On the Paradox of Learning to Reason from Data",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Constructed BERT",
            "name_full": "Constructed parameterization of BERT simulating forward-chaining",
            "brief_description": "An explicit parameter construction (presented in this paper) of a BERT-base model that simulates the forward-chaining deductive algorithm and thus provably solves SimpleLogic instances up to the constructed reasoning depth with perfect accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Constructed BERT (explicit parameterization)",
            "model_description": "A BERT-base architecture with a hand-crafted set of parameters (first layer parsing, intermediate layers implementing one forward-chaining step each, final layer output) that encodes facts/rules as 'meaningful vectors' and uses attention+MLP dynamics to broadcast RHS truth values to LHS and update booleans deterministically. Implemented in PyTorch following BERT-base architecture; parameters are engineered (not learned) to implement logical forward-chaining.",
            "model_size": null,
            "logical_reasoning_task": "SimpleLogic (propositional definite-clause deductive reasoning)",
            "task_description": "Same SimpleLogic task as above: templated natural-language encodings of propositional definite-clause problems; the constructed BERT is shown to exactly simulate forward-chaining on these encodings.",
            "method_or_approach": "Explicit constructive proof: design of predicate signatures (low inner-product), encoding rules/facts/queries into 'meaningful vectors' stored in token embeddings, use of attention heads with scaled keys (constant β) to make attention peaky for signature matches, and two-layer MLPs to update RHS boolean values using thresholded sums (effectively implementing logical conjunction). The construction maps one reasoning step to one layer; theorem states an n-layer BERT can solve examples requiring ≤ n-2 reasoning steps.",
            "performance": "The constructed BERT (implemented following BERT-base architecture) solved all SimpleLogic problems requiring reasoning depth ≤ 10 with 100% accuracy in their implementation; theoretical statement: for BERT with n layers there exists parameters to solve any instance requiring ≤ n-2 steps.",
            "limitations_or_failure_cases": "This construction demonstrates representational capacity but is not learned from data; it shows existence of parameters that implement exact reasoning but does not address whether such parameters are reachable via standard training on sampled data (empirically, fine-tuned BERT did not find them). The construction relies on careful engineered parameter assignments (predicate signatures, large β constants, bespoke MLP thresholds).",
            "comparison": "Direct contrast to empirically trained BERT: constructed BERT attains provable perfect performance up to its constructed depth while the data-trained BERTs (same architecture) fail to generalize across sampling distributions. Thus capacity is not the limiting factor—learnability from sampled data is the bottleneck due to statistical features in data.",
            "ablation_or_analysis_results": "Theorem and constructive proof (sketch in main text, full proof in appendix) showing mapping between layers and forward-chaining steps; empirical implementation verifies 100% accuracy up to depth 10 using only a portion of parameters. No ablation of the construction itself is reported (construction is presented as an existence proof).",
            "uuid": "e5092.1",
            "source_info": {
                "paper_title": "On the Paradox of Learning to Reason from Data",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2
        },
        {
            "paper_title": "Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge",
            "rating": 2
        },
        {
            "paper_title": "What can neural networks reason about?",
            "rating": 2
        },
        {
            "paper_title": "Learning a SAT solver from single-bit supervision",
            "rating": 1
        },
        {
            "paper_title": "Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
            "rating": 1
        },
        {
            "paper_title": "Teaching temporal logics to neural networks",
            "rating": 1
        }
    ],
    "cost": 0.01392,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the Paradox of Learning to Reason from Data</h1>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van den Broeck<br>University of California, Los Angeles</p>
<h4>Abstract</h4>
<p>Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be trained end-to-end to solve logical reasoning problems presented in natural language? We attempt to answer this question in a confined problem space where there exists a set of parameters that perfectly simulates logical reasoning. We make observations that seem to contradict each other: BERT attains near-perfect accuracy on in-distribution test examples while failing to generalize to other data distributions over the exact same problem space. Our study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems. We also show that it is infeasible to jointly remove statistical features from data, illustrating the difficulty of learning to reason in general. Our result naturally extends to other neural models and unveils the fundamental difference between learning to reason and learning to achieve high performance on NLP benchmarks using statistical features.</p>
<h2>1 Introduction</h2>
<p>Logical reasoning is needed in a wide range of NLP tasks including natural language inference (NLI) (Williams et al., 2018; Bowman et al., 2015), question answering (QA) (Rajpurkar et al., 2016; Yang et al., 2018) and common-sense reasoning (Zellers et al., 2018; Talmor et al., 2019). The ability to draw conclusions based on given facts and rules, is essential to solving these tasks. ${ }^{1}$ Though NLP models, empowered by the Transformer neural architecture (Vaswani et al., 2017), can achieve high performance on task-specific datasets (Devlin et al., 2019), it is unclear whether they are "reasoning" over the input following the rules of logic. A research question naturally arises: can neural</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Problem setting: a confined problem space consisting of logical reasoning problems; dots and triangles denote examples sampled from different distributions over the same problem space.
networks be trained to conduct logical reasoning presented in natural language?</p>
<p>Following prior work, we attempt to answer this question by training and testing a neural model (e.g. BERT (Devlin et al., 2019)) on a confined problem space (SimpleLogic, see Sec. 2 and Fig. 1) consisting of logical reasoning problems written in English (Johnson et al., 2017; Sinha et al., 2019; Clark et al., 2020). Yet, we observe evidences that seemingly lead to a contradiction.</p>
<p>On the one hand, the following evidences seem to imply that neural models can learn to reason:</p>
<p>E1: Logical reasoning problems in the problem space are self-contained: they have no language variance and require no prior knowledge.</p>
<p>E2: We show that theoretically, the BERT model has enough capacity to represent the correct reasoning function (Sec 2.2).</p>
<p>E3: The BERT model can be trained to achieve near-perfect test accuracy on a data distribution covering the whole problem space.</p>
<p>With these evidences (and with even fewer evidences), it has been claimed that neural models can learn to reliably emulate the correct reasoning function (Clark et al., 2020; Talmor et al., 2020).</p>
<p>On the other hand, however, we observe a seem-</p>
<p>ingly contradictory phenomenon: the models attaining near-perfect accuracy on one data distribution do not generalize to other distributions within the same problem space. Since the correct reasoning function does not change across data distributions, it follows that the model has not learned to reason.</p>
<p>The paradox lies in that if a neural model has learned reasoning, it should not exhibit such a generalization failure; if the model has not learned reasoning, it is baffling how it manages to achieve near-perfect test accuracy on a training distribution that covers the entire problem space. Note that what we observed is not a common case of out-of-distribution (OOD) generalization failure: (1) the problem space is confined and simple (see E1,E2); (2) the correct reasoning function is invariant across data distributions; (3) the training distribution covers the whole problem space. On the contrary, discussions on OOD generalization often involve open problem space (Lin et al., 2019; Gontier et al., 2020; Wald et al., 2021) and domain/concept mismatch between training and testing distribution (Yin et al., 2021; Koh et al., 2021).</p>
<p>Upon further investigation, we provide an explanation for this paradox: the model attaining high accuracy only on in-distribution test examples has not learned to reason. In fact, the model has learned to use statistical features in logical reasoning problems to make predictions rather than to emulate the correct reasoning function.</p>
<p>Our first observation is that even the simplest statistic of a reasoning problem can give away significant information about the true label (Sec.4.1): for example, by only looking at the number of rules in a reasoning problem, we can predict the correct label better than a random guess. Unlike dataset biases/artifacts identified in typical NLP datasets, which are often due to biases in the dataset collection/annotation process (Gururangan et al., 2018; Clark et al., 2019; He et al., 2019), statistical features inherently exist in reasoning problems and are not specific to certain data distributions. We show that statistical features can hinder model generalization performance; moreover, we argue that there are potentially countless statistical features and it is computationally expensive to jointly remove them from training distributions.</p>
<p>Our study establishes the dilemma of learning to reason from data: on the one hand, when a model is trained to learn a task from data, it always tends to learn statistical patterns, which inherently exist in reasoning examples; on the other hand, however, the rules of logic never rely on statistical patterns to conduct reasoning. Since it is difficult to construct a logical reasoning dataset that contains no statistical features, it follows that learning to reason from data is difficult.</p>
<p>Our findings unveil the fundamental difference between "learning to reason" and "learning to solve a typical NLP task." For most NLP tasks, one of the major goal for a neural model is to learn statistical patterns: for example, in sentiment analysis (Maas et al., 2011), a model is expected to learn the strong correlation between the occurrence of the word "happy" and the positive sentiment. However, for logical reasoning, even though numerous statistical features inherently exist, models should not be utilizing them to make predictions. Caution should be taken when we seek to train neural models end-to-end to solve NLP tasks that involve both logical reasoning and prior knowledge and are presented with language variance (Welleck et al., 2021; Yu et al., 2020), which could potentially lead to even stronger statistical features, echoing the findings of Elazar et al. (2021); McCoy et al. (2019).</p>
<h2>2 SimpleLogic: A Simple Problem Space for Logical Reasoning</h2>
<p>We define SimpleLogic, a class of logical reasoning problems based on propositional logic. We use SimpleLogic as a controlled testbed for testing neural models' ability to conduct logical reasoning.</p>
<p>SimpleLogic only contains deductive reasoning examples. To simplify the problem, we remove language variance by representing the reasoning problems in a templated language and limit their complexity (e.g., examples have limited input lengths, number of predicates, and reasoning depths).</p>
<p>Solving SimpleLogic does not require significant model capacity. We show that a popular pre-trained language model BERT (Devlin et al., 2019) ${ }^{2}$ has more than enough model capacity to solve SimpleLogic by constructing a parameterization of BERT that solves SimpleLogic with 100\% accuracy (Sec. 2.2).</p>
<h3>2.1 Problem Space Definition</h3>
<p>Before defining SimpleLogic, we introduce some basics for propositional logic. In general, reasoning</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>in propositional logic is NP-complete; hence, we only consider propositional reasoning with definite clauses. A definite clause in propositional logic is a rule of the form $A_{1} \wedge A_{2} \wedge \cdots \wedge A_{n} \rightarrow B$, where $A_{i}$ s and $B$ are predicates that take values in "True" or "False"; we refer to the left hand side of a rule as its body and the right hand side as its head. In particular, a definite clause is called a fact if its body is empty (i.e. $n=0$ ). A propositional theory (with only definite clauses) $T$ is a set of rules and facts, and we say a predicate $Q$ can be proved from $T$ if either (1) $Q$ is given in $T$ as a fact or (2) $A_{1} \wedge \cdots \wedge A_{n} \rightarrow Q$ is given in $T$ as a rule where $A_{i}$ s can be proved.</p>
<p>Each example in SimpleLogic is a propositional reasoning problem that only involves definite clauses. In particular, each example is a tuple (facts, rules, query, label) where (1) facts is a list of predicates that are known to be True, (2) rules is a list of rules represented as definite clauses, (3) query is a single predicate, and (4) label is either True or False, denoting whether the query predicate can be proved from facts and rules. Figure 1 shows such an example. Furthermore, we enforce some simple constraints to control the difficulty of the problems. For each example in SimpleLogic, we require that:</p>
<ul>
<li>the number of predicates (#pred) that appear in facts, rules and query ranges from 5 to 30 , and all predicates are sampled from a fixed vocabulary containing 150 adjectives such as "happy" and "complicated"; note that the predicates in SimpleLogic have no semantics;</li>
<li>the number of rules (#rule) ranges from 0 to $4 \times$ #pred, and the body of each rule contains 1 to 3 predicates; i.e. $A_{1} \wedge \ldots \wedge A_{n} \rightarrow B$ with $n&gt;3$ is not allowed;</li>
<li>the number of facts (#fact) ranges from 1 to #pred;</li>
<li>the reasoning depth ${ }^{3}$ required to solve an example ranges from 0 to 6 .
We use a simple template to encode examples in SimpleLogic as natural language input. For example, we use "Alice is X." to represent the fact that $X$ is True; we use " $A$ and $B, C$." to represent the rule $A \wedge B \rightarrow C$; we use "Query: Alice is $Q$." to represent the query predicate $Q$. Then we concatenate facts, rules and query as [CLS] facts. rules</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A BERT-base model that simulates the forward-chaining algorithm. The first layer parses text input into the desired format. Each reasoning layer performs one step of forward-chaining, adding some predicates to the Proved Facts, and the rules being used are underlined in green; e.g. Reasoning Layer 2 use the rule "smart $\rightarrow$ evil" to prove the predicate evil.
[SEP] query [SEP] and supplement it to BERT to predict the correct label.</p>
<h3>2.2 BERT Has Enough Capacity to Solve SimpleLogic</h3>
<p>In the following, we show that BERT has enough capacity to solve all examples in SimpleLogic. In particular, we explicitly construct a parameterization for BERT such that the fixed-parameter model solves all problem instances in SimlpleLogic. Note that we only prove the existence of such a parameterization, but do not discuss whether such a parameterization can be learned from sampled data until Sec. 3.
Theorem 1. For BERT with $n$ layers, there exists a set of parameters such that the model can correctly solve any reasoning problem in SimpleLogic that requires $\leq n-2$ steps of reasoning.</p>
<p>We prove this theorem by construction; in particular, we construct a fixed set of parameters for BERT to simulate the forward-chaining algorithm.</p>
<p>Here we show a sketch of the proof, and refer readers to Appendix C for the full proof. As illustrated in Figure 2, our construction solves a logical reasoning example in a layer-by-layer fashion. The</p>
<p>(1) Randomly sample facts &amp; rules.
Facts: B, C
Rules: A, B $\rightarrow$ D, B $\rightarrow$ E, B, C $\rightarrow$ F.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustration of a logical reasoning problem (right) in SimpleLogic being sampled by RulePriority (RP) and Label-Priority (LP), respectively. Predicates with label True are denoted by filled circles.</p>
<p>1st layer of BERT parses the input sequence into the desired format. Layer 2 to layer 10 are responsible for simulating the forward chaining algorithm: each layer performs one step of reasoning, updating the True/False label for predicates. The last layer also performs one step of reasoning, while implicitly checking if the query predicate has been proven and propagating the result to the first token. The parameters are the same across all layers except for the Parsing Layer (1st layer).</p>
<p>We implemented the construction in PyTorch, following the exact architecture of the BERT-base model. The "constructed BERT" solves all the problems in SimpleLogic of reasoning depth $\leq 10$ with 100% accuracy, using only a small proportion of the parameters.</p>
<h2>3 BERT Fails to Learn to Solve SimpleLogic</h2>
<p>In this section, we study whether it is possible to train a neural model (e.g., BERT) to reason on SimpleLogic. We follow prior work [clark2020] to randomly sample examples from the problem space and train BERT on a large amount of sampled data. We consider two natural ways to sample data from SimpleLogic and expect that if a model has learned to reason, the model should be able to solve examples generated by any sampling methods.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.1 Sampling Examples from SimpleLogic</h3>
<p>When sampling examples from a finite domain, one naive approach is to uniformly sample from the domain. However, uniform sampling is not desirable: by computation, it is easy to show that over 99.99% of the examples generated by uniform sampling have 30 predicates and 120 rules. This is a serious problem in terms of coverage: we expect a reasonable dataset to cover reasoning examples of different #pred, #fact and #rule. Hence, we instead consider the following two intuitive ways of sampling examples:</p>
<p>Rule-Priority (RP). To solve the major issue with uniform sampling, in Rule-Priority, we first randomly sample #pred, #fact and #rule uniformly at random from $[5,30],[1, # \mathrm{pred}]$ and $[1,4 \times # \mathrm{pred}]$ respectively, ensuring that all three aspects are covered by a non-trivial number of examples. Then, we randomly sample some predicates, facts and rules based on the given #pred, #rule and #fact. The query is also randomly sampled, and its label is computed by forward-chaining based on the given facts and rules.</p>
<p>Lable-Priority (LP). In Rule-Priority, we first randomly generate rules and facts, which then determines the label for each predicate. In LabelPriority (LP), we consider generating examples in the "reversed" order: we first randomly assign a True/False label to each predicate and then randomly sample some rules and facts that are consistent with the pre-assigned labels.</p>
<p>Figure 3 shows an example illustrating the two sampling methods. Both LP and RP are very general, covering the whole problem space. We refer readers to the Appendix for further details on the sampling algorithms.</p>
<h3>3.2 BERT Trained on Randomly Sampled Data Cannot Generalize</h3>
<p>Following the two sampling regimes described above, we randomly sample two sets of examples from SimpleLogic: for each reasoning depth from 0 to 6 , we sample $80 k$ examples from SimpleLogic via algorithm RP (LP) and aggregate them as dataset RP (LP), which contains $560 k$ examples in total. We then split it as training/validation/test set. We train a BERT-base model [devlin2019] on RP and LP, respectively. We train for 20 epochs with a learning rate of $4 \times 10^{-5}$, a warm-up ratio of 0.05 , and a batch size of 64 . Training takes less</p>
<p>than 2 days on 4 NVIDIA 1080Ti / 2080Ti GPUs with 12Gb GPU memory.</p>
<p>BERT performs well on the training distribution. The first and last rows of Table 1 show the test accuracy when the test and train examples are sampled by the same algorithm (e.g., for row 1, the model is trained on the training set of RP and tested on the test set of RP). In such scenarios, the models can achieve near-perfect performance similar to the observations in prior work <em>Clark et al. (2020)</em>. Both sampling algorithms are general in the sense that every instance in SimpleLogic has a positive probability to be sampled; hence, the intuition is that the model has learned to emulate the correct reasoning function.</p>
<p>BERT fails to generalize. However, at the same time, we observe a rather counter-intuitive finding: the test accuracy drops significantly when the train and test examples are sampled via different algorithms. Specifically, as shown in the second and third rows of Table 1, the BERT model trained on RP fails drastically on LP, and vice versa. Since the correct reasoning function does not change across different data distributions, this generalization failure indicates BERT is has not learned to conduct logical reasoning. A subsequent question naturally arise: is this simply because LP and RP are complementary? Can the model learn to reason if we train the model on data sampled by both algorithms?</p>
<p>Training on both RP and LP is not enough. We train BERT on the mixture of RP and LP, and BERT again achieves nearly perfect test accuracy. Can we now conclude that BERT has learned to approximate the correct reasoning function? We slightly tweak the sampling algorithm of LP by increasing the expected number of alternative proof trees to generate LP<em>. Unfortunately, we observe that the model performance again drops significantly on LP</em> (Table 2). Such a result resembles what we observed in Table 1, even when we are enriching our training distribution with different sampling methods. In fact, we find no evidence that consistently enriching the training distribution will bring a transformative change such that the model can learn to reason.</p>
<p>Discussion. The experiments above reveal a pattern of generalization failure: if we train the model on one data distribution, it fails almost inevitably on a different distribution. In other words, the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RP</td>
<td style="text-align: center;">RP</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">95.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LP</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: center;">LP</td>
<td style="text-align: center;">RP</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LP</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.0</td>
</tr>
</tbody>
</table>
<p>Table 1: Test accuracy on LP/RP for the BERT model trained on LP/RP; the accuracy is shown for test examples with reasoning depth from 0 to 6 . BERT trained on RP achieves almost perfect accuracy on its test set; however the accuracy drops significantly when it's tested on LP (vice versa).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RP\&amp;LP</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">95.6</td>
</tr>
<tr>
<td style="text-align: center;">LP*</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">55.2</td>
</tr>
</tbody>
</table>
<p>Table 2: BERT trained on a mixture over RP and LP fails on LP*, a test set that slightly differs from LP.
model seems to be emulating an incorrect "reasoning function" specific to its training distribution.</p>
<h2>4 BERT Learns Statistical Features</h2>
<p>To this point, we have shown that a BERT model achieving high in-distribution accuracy did not learn the correct reasoning function. In this section, we seek to provide an explanation for this peculiar generalization failure. Our analysis suggests that for the task of logical reasoning, even the simplest statistics of the example can give away significant information about the label, which we denote as statistical features. Such statistical features are inherent to the task of logical reasoning rather than a problem with specific datasets. When BERT is trained on data with statistical features, it tends to make predictions based on such features rather than learning to emulate the correct reasoning function; thus, BERT fails to generalize to the whole problem space. However, unlike the shallow shortcuts found in other typical NLP tasks, such statistical features can be countless and extremely complicated, and thus very difficult to be removed from training data.</p>
<h3>4.1 Statistical Features Inherently Exists</h3>
<p>What is a statistical feature? If a certain statistic of an example has a strong correlation with its label, we call it a statistical feature.</p>
<p>As an illustrating example, we consider the number of rules in a reasoning problem (#rule). As shown in Figure 4a, the #rule for reasoning prob-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Pr(label = 1 | #rule) (the blue columns) and Pr(#rule) (the green curves) for RP and RP_balance, respectively. After removing #rule as a statistical feature (RP_balance), Pr(label = 1 | #rule) approaches 0.5 for #rule ≤ 80 while Pr(#rule) does not change.</p>
<p>lems in RP exhibit a strong correlation with their labels: when #rule &gt; 40, the number of positive examples exceeds 50% by large margins; formally, Pr<sub>e~RP</sub>(label(e) = 1 | #rule(e) = x) &gt; 0.5 for x &gt; 40, which makes it possible for the model to guess the label of an example with relatively high accuracy by only using its #rule. Hence, we call #rule a statistical feature for the dataset RP.</p>
<h3>Statistical features are inherent to logical reasoning problems.</h3>
<p>Continuing with our example, we show that #rule <em>inherently</em> exists as a statistical feature for logical reasoning problems in general; that is, it is not specific to the RP dataset. Consider the following property about logical entailment:</p>
<p><strong>Property</strong> (Monotonicity of entailment). <em>Any additional facts and rules can be freely added to the hypothesis of any proven fact.</em></p>
<p>It follows that, intuitively, given a fixed set of predicates and facts, any predicate is more likely to be proved when more rules are given, that is, Pr(label(e) = 1 | #rule(e) = x) should increase (roughly) monotonically as x increases. Since this intuition assumes nothing about data distributions, it follows that such statistical patterns should naturally exist in any dataset that is not adversarially</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: For RP, Pr(label = 1 | branching_factor) decreases as branching_factor increases.</p>
<p>constructed. In addition to RP, we also verify that both LP and the uniform distribution exhibit similar statistical patterns, which we refer readers to Appendix for further details.</p>
<h3>Statistical features are countless.</h3>
<p>In addition to #rule, numerous statistical features potentially exist. For example, as facts can be seen as special form of rules, it follows from previous argument that #fact is also positively correlated with labels. Statistical features can be more complicated than just #rule or #fact. For example, the average number of predicates in rules of a reasoning problem can also leak information about its label. Note that the right-hand side of a rule is only proved if all predicates on its left-hand side are proved. Then, it is immediate that rules of the form A, B, C → D are less likely to be "activated" than rules of the form A → D. Following this intuition, we can define the following statistic: for an example e, let</p>
<p>$$
\begin{array}{c}
\text{branching_factor}(e) \
\qquad = \frac{\text{#fact}(e) + \sum_{\text{rule \in e}} \text{length of rule}}{\text{#fact}(e) + \text{#rule}(e)}.
\end{array}
$$</p>
<p>In this definition, we are computing the average number of predicates in the rules, where facts are treated as rules with one predicate.<sup>5</sup> Our intuition suggests that the larger the branching_factor, the less likely an example will be positive; we verify that this intuition holds for RP, as shown in Figure 5. Just like #rule, we observe that branching_factor is also a statistical feature for LP and the uniform distribution; see details in Appendix.</p>
<p>Now we have shown that, though there are simple statistical features like #rule, some (e.g. branching_factor) can be less intuitive to call to mind; in light of this, it is not hard to imagine that some statistical features can be so complex that they cannot</p>
<p><sup>5</sup>Branching_factor: with more predicates on the left-hand side of the rules, the proof tree has more branches.</p>
<table>
<thead>
<tr>
<th>Train</th>
<th>Test</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>RP</td>
<td>99.8</td>
<td>99.7</td>
<td>99.7</td>
<td>99.4</td>
<td>98.5</td>
<td>98.1</td>
<td>97.0</td>
</tr>
<tr>
<td>RP_b</td>
<td>RP_b</td>
<td>99.4</td>
<td>99.6</td>
<td>99.2</td>
<td>98.7</td>
<td>97.8</td>
<td>96.1</td>
<td>94.4</td>
</tr>
<tr>
<td></td>
<td>LP</td>
<td>99.6</td>
<td>99.6</td>
<td>99.6</td>
<td>97.6</td>
<td>93.1</td>
<td>81.3</td>
<td>68.1</td>
</tr>
<tr>
<td></td>
<td>RP</td>
<td>99.9</td>
<td>99.8</td>
<td>99.7</td>
<td>99.3</td>
<td>98.3</td>
<td>97.5</td>
<td>95.5</td>
</tr>
<tr>
<td>RP</td>
<td>RP_b</td>
<td>99.0</td>
<td>99.3</td>
<td>98.5</td>
<td>97.5</td>
<td>96.7</td>
<td>93.5</td>
<td>88.3</td>
</tr>
<tr>
<td></td>
<td>LP</td>
<td>99.8</td>
<td>99.8</td>
<td>99.3</td>
<td>96.0</td>
<td>90.4</td>
<td>75.0</td>
<td>57.3</td>
</tr>
</tbody>
</table>
<p>Table 3: The model trained on RP performs worse on RP_balance (RP_b). This indicates that the model is using the statistical feature #rule to make predictions.
even be manually constructed by humans. In particular, statistical features can also be compositional: one can define a joint statistical feature by combining multiple ones (e.g., branching_factor and #rule), which further adds to the complexity. Thus, it is infeasible to identify all statistical features.</p>
<h3>4.2 Statistical Features Inhibit Model Generalization</h3>
<p>Having verified that statistical features inherently exist for logical reasoning problems, in this section we study how they affect the model behavior. We show that (1) when statistical features are presented in training distributions, BERT tends to utilize them to make predictions; (2) after removing one statistical feature from training data, the model generalizes better. It follows that statistical features can hinder the model from learning the correct reasoning function, explaining the generalization failure we observed in Section 3.</p>
<p>Example: removing one statistical feature. We use #rule as an example to illustrate how to remove statistical features from a training dataset $\mathcal{D}$; in particular, there are three criteria that we need to satisfy: (1) label is balanced for the feature; (2) the marginal distribution of the feature remains unchanged; (3) the dataset size remains unchanged.</p>
<p>Formally, our first goal is to sample $\mathcal{D}^{\prime} \subset \mathcal{D}$ such that, for all $x$ :</p>
<p>$$
\operatorname{Pr}_{e \sim \mathcal{D}^{\prime}}(\operatorname{label}(e)=1 \mid # \operatorname{rule}(e)=x)=0.5
$$</p>
<p>Intuitively, this equation says that on $\mathcal{D}^{\prime}$, one cannot do better than $50 \%$ by only looking at the #rule of an example. Specifically, for all possible values of $x$, if $\operatorname{Pr}_{e \sim \mathcal{D}}(\operatorname{label}(e)=1 \mid # \operatorname{rule}(e)=x)&gt;0.5$, we drop some positive examples with #rule $=x$ from $\mathcal{D}$; otherwise, we drop some negative examples.</p>
<p>However, we would not meet the second criterion by naively dropping the minimum number of examples; consider the following statistics for RP:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#rule</th>
<th style="text-align: center;">before drop <br> #examples / positive \%</th>
<th style="text-align: center;">after drop <br> #examples / positive \%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">$6860 / 49.9 \%$</td>
<td style="text-align: center;">$6822 / 50.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">$2322 / 92.7 \%$</td>
<td style="text-align: center;">$339 / 50.0 \%$</td>
</tr>
</tbody>
</table>
<p>As shown in the table, if we were to naively drop the minimum number examples from RP such that Equation 1 is satisfied, we will be left with only 339 examples with #rule $=80$, where the number (6822) of examples with #rule $=38$ remains unchanged. This could be a serious issue in terms of dataset coverage: examples with some particular #rule will dominate $\mathcal{D}^{\prime}$ and there will not be enough examples for other #rule. Recall that this is also the reason we choose RP/LP over uniform sampling to generate our datasets (Sec. 3.1). Hence, we also need to make sure that as we remove statistical features from $\mathcal{D}$, their marginal distributions in $\mathcal{D}^{\prime}$ stay close to $\mathcal{D}$ :</p>
<p>$$
\operatorname{Pr}<em _mathcal_D="\mathcal{D" _sim="\sim" e="e">{e \sim \mathcal{D}^{\prime}}(# \operatorname{rule}(e))=\operatorname{Pr}</em>(e))
$$}}(# \operatorname{rule</p>
<p>In this way, $\mathcal{D}^{\prime}$ 's coverage of examples with different #rule remains the same as $\mathcal{D}$.</p>
<p>When both criteria (1) and (2) are satisfied, the size of $\mathcal{D}^{\prime}$ will be much smaller than $\mathcal{D}$ and the ratio $k=|\mathcal{D}| /\left|\mathcal{D}^{\prime}\right|$ can be estimated from $\min <em _mathcal_D="\mathcal{D" _sim="\sim" e="e">{x} \operatorname{Pr}</em>$ by down-sampling.}}(\operatorname{label}(e)=1 \mid # \operatorname{rule}(e)=x)$. Hence, to make sure that criterion (3) is met, that is the size of $\mathcal{D}^{\prime}$ is the same as $\mathcal{D}$, we need to pre-sample $k \times \mathcal{D}$ and obtain $\mathcal{D}^{\prime</p>
<p>Following this approach, by down-sampling from $k \times$ RP, we construct RP_balance, where #rule is no longer a statistical feature. A rough estimation shows that if we were to balance $\operatorname{Pr}_{e \sim \mathrm{RP}}(\operatorname{label}(e)=1 \mid # \operatorname{rule}(e)=x)$ for $x$ up to 110 , the ratio $k&gt;100$, that is, we need to spend over 100x running time ( 200 hours on a 40 -core CPU) to pre-sample roughly 56 million examples; the computational cost would be even more expensive if we want to completely remove #rule as a statistical feature. Hence, we only balance this conditional probability for $0 \leq x \leq 80$, which takes 10x running time ( 20 hours on a 40 -core CPU) to pre-sample 5.6 million examples. This would not be a major problem as $90 \%$ of the examples in RP have #rule $\leq 80$. We train the BERT model on RP_balance, and the results are reported in Table 3.</p>
<p>BERT uses statistical features to make predictions. As shown in Table 3, BERT trained on RP shows large performance drop when tested on RP_balance, while BERT trained on RP_balance</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$X$</th>
<th style="text-align: center;">$\operatorname{Pr}(\operatorname{label}=1 \mid X)$</th>
<th style="text-align: center;">k $\times$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{f}=15$</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">5.5</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{f}=15, \mathrm{~b} \in[2.65,2.75]$</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{f}=15, \mathrm{~b} \in[2.65,2.75], \mathrm{r}=58$</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">55.6</td>
</tr>
</tbody>
</table>
<p>shows even better performance on RP than RPtrained BERT. Since RP_balance is down-sampled from RP, the accuracy drop from RP to RP_balance can only be explained by that BERT trained on RP is using #rule to make predictions.</p>
<p>Removing statistical features helps generalization. As shown in Table 3, compared to RPtrained BERT, BERT trained on RP_balance achieves higher accuracy when tested on LP; in particular, for examples with reasoning depth 6 , the model trained on RP_balance attains an accuracy of $68.1 \%$, approximately $10 \%$ higher than the model trained on RP. This is a clear signal that when #rule is removed as a statistical feature, the model generalizes better, suggesting that statistical features can hinder model generalization.</p>
<p>Statistical features explain the paradox. Now we have a good explanation for the paradox: on the first hand, as we have discussed in Section 4.1, statistical features can be arbitrarily complex and powerful neural models can identify and use them to achieve high in-distribution accuracy; on the other hand, since the correlations between statistical features and the labels can change as the data distribution changes (see Appendix for details), the model that uses them to make predictions does not generalize to out-of-distribution examples. Besides, we notice that though the BERT model seem to be generalizing well for reasoning examples of depth $&lt;3$, it never achieve $100 \%$ accuracy even when tested in-distribution: no matter how strong the statistical features are, they almost never determine the label with $100 \%$ accuracy.</p>
<h3>4.3 On the Dilemma of Removing Statistical Features</h3>
<p>We show that though removing one statistical feature (e.g., #rule) from training data can benefit model generalization, it is computationally infeasible to jointly remove multiple statistical features.</p>
<p>Recall that, in the previous section, when we were trying to remove the statistical feature #rule from RP, we could only afford to remove it for $90 \%$ of the examples. The general idea is that if a statistical feature $X$ has a very strong correlation with the label on some dataset $\mathcal{D}$, i.e. $\operatorname{Pr}_{e \sim \mathcal{D}}(\operatorname{label}(e)=1 \mid X(e)=x)$ is very close to 1 or 0 , then we would need to sample a lot of examples to have a balanced set.</p>
<p>The combination of multiple statistical features can give much stronger signal about the label than</p>
<p>Table 4: Jointly removing statistical features is difficult; e.g. second row shows: we need to sample at least 20 $\times \operatorname{RP}$ to balance $\operatorname{Pr}($ label $=1 \mid \mathrm{f}=15, \mathrm{~b} \in[2.65,2.75])$.
the individual ones; thus it is much harder to jointly remove them. As an example, we consider removing three statistical features from RP: #fact (f), branching_factor (b) and #rule (r).</p>
<p>As shown in Table 4, as we try to jointly remove more statistical features $X, \operatorname{Pr}($ label $=1 \mid X)$ becomes more unbalanced; in particular, as we try to progressively remove #fact, branching_factor and #rule, the minimum times of examples we need to sample grows roughly exponentially: $5.5 \rightarrow$ $20.0 \rightarrow 55.6$. Besides, the third column in Table 4 only shows some lower-bounds for $k$ : we are only considering balancing the conditional probability for one particular assignment (#fact $=15$, braching_factor $\in[2.65,2.75]$, #rule $=58)$; for some other assignments, the conditional probability can be more unbalanced, making it even more difficult to jointly remove them.</p>
<h2>5 Related Work</h2>
<p>A great proportion of NLP tasks require logical reasoning. Prior work contextualizes the problem of logical reasoning by proposing reasoningdependent datasets and studies solving the tasks with neural models (Johnson et al., 2017; Sinha et al., 2019; Yu et al., 2020; Liu et al., 2020; Tian et al., 2021). However, most studies focus on solving a single task, and the datasets either are designed for a specific domain (Johnson et al., 2017; Sinha et al., 2019), or have confounding factors such as language variance (Yu et al., 2020); they can not be used to strictly or comprehensively study the logical reasoning abilities of models.</p>
<p>Another line studies leveraging deep neural models to solve pure logical problems. For examples, SAT (Selsam et al., 2019), maxSAT (Wang et al., 2019), temporal logical problems (Hahn et al., 2021), DNF counting (Crouse et al., 2019), logical reasoning by learning the embedding of logical formula (Crouse et al., 2019; Abdelaziz et al., 2020) and mathematical problems (Saxton et al., 2019; Lample and Charton, 2020). In this work, we focus</p>
<p>on deductive reasoning, which is a general and fundamental reasoning problem. Clark et al. (2020) conducts a similar study to show that models can be trained to reason over language, while we observe the difficulty of learning to reason from data. Xu et al. (2019) studies how well neural models can generalize on different types of reasoning problems from a theoretical perspective.</p>
<h2>6 Conclusion</h2>
<p>In this work, we study whether BERT can be trained to conduct logical reasoning in a confined problem space. Our work shows that though BERT can achieve near-perfect performance on a data distribution that covers the whole space, it always fails to generalize to other distributions that are even just slightly different. We demonstrate that the BERT model has not learned to emulate the correct reasoning function: it is in fact learning statistical features, which inherently exist in logical reasoning problems. We further show that it is computationally infeasible to identify and remove all such statistical features from training data, establishing the difficulty of learning to reason.</p>
<h2>Acknowledgements</h2>
<p>This work is partially supported by a DARPA PTG grant, NSF grants #IIS-1943641, #IIS-1956441, #CCF-1837129, Samsung, CISCO, and a Sloan Fellowship. This work is supported in part by Amazon scholarship.</p>
<h2>References</h2>
<p>Ibrahim Abdelaziz, Veronika Thost, Maxwell Crouse, and Achille Fokoue. 2020. An experimental study of formula embeddings for automated theorem proving in first-order logic. CoRR, abs/2002.00423.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP. The Association for Computational Linguistics.</p>
<p>Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. 2019. Don't take the easy way out: Ensemble based methods for avoiding known dataset biases. In EMNLP/IJCNLP (1), pages 4067-4080. Association for Computational Linguistics.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In IJCAI. ijcai.org.</p>
<p>Maxwell Crouse, Ibrahim Abdelaziz, Cristina Cornelio, Veronika Thost, Lingfei Wu, Kenneth D. Forbus, and Achille Fokoue. 2019. Improving graph neural network representations of logical formulae with subgraph pooling. CoRR, abs/1911.06904.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1). Association for Computational Linguistics.</p>
<p>Yanai Elazar, Hongming Zhang, Yoav Goldberg, and Dan Roth. 2021. Back to square one: Artifact detection, training and commonsense disentanglement in the winograd schema. arXiv preprint arXiv:2104.08161.</p>
<p>Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020. Measuring systematic generalization in neural proof generation with transformers. Advances in Neural Information Processing Systems, 33:22231-22242.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In NAACL-HLT (2). Association for Computational Linguistics.</p>
<p>Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. 2021. Teaching temporal logics to neural networks. In ICLR. OpenReview.net.</p>
<p>He He, Sheng Zha, and Haohan Wang. 2019. Unlearn dataset bias in natural language inference by fitting the residual. In DeepLo@EMNLP-IJCNLP, pages 132-142. Association for Computational Linguistics.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR. IEEE Computer Society.</p>
<p>Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. 2021. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637-5664. PMLR.</p>
<p>Guillaume Lample and François Charton. 2020. Deep learning for symbolic mathematics. In ICLR. OpenReview.net.</p>
<p>Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gardner. 2019. Reasoning over paragraph effects in situations. arXiv preprint arXiv:1908.05852.</p>
<p>Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In IJCAI. ijcai.org.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, pages 2383-2392. The Association for Computational Linguistics.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In ICLR (Poster). OpenReview.net.</p>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT solver from single-bit supervision. In ICLR (Poster). OpenReview.net.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In EMNLP/IJCNLP (1), pages 45054514. Association for Computational Linguistics.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In NAACL-HLT (1), pages 4149-4158. Association for Computational Linguistics.</p>
<p>Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Advances in Neural Information Processing Systems, 33:20227-20237.</p>
<p>Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. 2021. Diagnosing the firstorder logical reasoning ability through logicnli. In EMNLP (1). Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998-6008.</p>
<p>Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. 2021. On calibration and out-of-domain generalization. In NeurIPS, pages 2215-2227.</p>
<p>Po-Wei Wang, Priya L. Donti, Bryan Wilder, and J. Zico Kolter. 2019. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In ICML, Proceedings of Machine Learning Research. PMLR.</p>
<p>Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. 2021. Naturalproofs: Mathematical theorem proving in natural language. arXiv preprint arXiv:2104.01112.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL-HLT. Association for Computational Linguistics.</p>
<p>Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2019. What can neural networks reason about? arXiv preprint arXiv:1905.13211.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In EMNLP, pages 2369-2380. Association for Computational Linguistics.</p>
<p>Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. 2021. Broaden the vision: Geodiverse visual commonsense reasoning. In EMNLP (1), pages 2115-2129. Association for Computational Linguistics.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning. In ICLR. OpenReview.net.</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In EMNLP. Association for Computational Linguistics.</p>
<h1>A Statistical Features in Different Data Distributions</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: #rule is a statistical feature for RP, LP and the uniform distribution. Even though $\operatorname{Pr}($ label $=1 \mid # r u l e)$ increases as #rule increases for all three distributions, it follows a slightly different pattern for each distribution; that is to say, the correlation between #rule and the label changes as the underlying data distribution changes, which explains the generalization failure we observed.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Statistics for examples generated by Rule-Priority (RP).
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) Statistics for examples generated by Label-Priority (LP).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: #fact is a statistical feature for RP, LP and the uniform distribution.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: branching_factor is a statistical feature for RP, LP and the uniform distribution.</p>
<h1>B Sampling Examples from SimpleLogic</h1>
<h2>B. 1 Algorithms: Rule-Priority \&amp; Label-Priority</h2>
<h2>a Rule-Priority (RP)</h2>
<p>1: pred_num $\sim U[5,30]$
2: preds $\leftarrow$ Sample(vocab, pred_num)
3: fact_num $\sim U[1$, pred_num $]$
4: rule_num $\sim U[0,4 *$ pred_num $]$
5: rules $\leftarrow$ empty array size of rules $&lt;$ rule_num
6: body_num $\sim U[1,3]$
7: body $\leftarrow$ Sample(preds, body_num)
8: head $\leftarrow$ Sample(preds, 1) tail $\notin$ body
9: add body $\rightarrow$ head to rules
10: fact_num $\sim U[0$, pred_num $]$
11: facts $\leftarrow$ Sample(preds, fact_num)
12: query $\leftarrow$ Sample(preds, 1)
13: Compute label via forward-chaining.
14: (facts, rules, query, label)</p>
<h2>b Label-Priority (LP)</h2>
<p>1: pred_num $\sim U[5,30]$
2: preds $\leftarrow$ Sample(vocab, pred_num)
3: rule_num $\sim U[0,4 *$ pred_num $]$
4: set $l \sim U[1$, pred_num $/ 2]$ and group preds
5: into $l$ layers predicate $p$ in layer $1 \leq i \leq l$
6: $q \sim U[0,1]$
7: assign label $q$ to predicate $p i&gt;1$
8: $k \sim U[1,3]$
9: cand $\leftarrow$ nodes in layer $i-1$
10: with label $=q$
11: body $\leftarrow$ Sample(cand, $k$ )
12: add body $\rightarrow p$ to rules size of rules $&lt;$ rule_num
13: body_num $\sim U[1,3]$
14: body $\leftarrow$ Sample(preds, body_num)
15: head $\leftarrow$ Sample(preds, 1)
16: add body $\rightarrow$ tail to rules unless tail has label 0 and
17: all predicates in body has label 1.
18: facts $\leftarrow$ predicates in layer 1 with label $=1$
19: query $\leftarrow$ Sample(preds, 1)
20: label $\leftarrow$ pre-assigned label for query
21: (facts, rules, query, label)</p>
<p>Figure 9: Two sampling algorithms Rule-Priority and Label-Priority. Sample $(X, k)$ returns a random subset from $X$ of size $k . U[X, Y]$ denotes the uniform distribution over the integers between $X$ and $Y$.</p>
<h1>C Construction Proof of Theorem 1</h1>
<p>We prove theorem 1 by construction: in N-layer BERT model, we take the first layer as parsing layer, the last layer as output layer and the rest layers as forward chaining reasoning layer. Basically, in the parsing layer we preprocess the natural language input. In forward chaining reasoning layers, the model iteratively broadcast the RHSs to all LHSs, and check the left hand side (LHS) of each rule and update the status of the right hand side (RHS). Here we introduce the general idea of the construction, and we will release the source code for the detailed parameters assignments.</p>
<h2>C. 1 Pre-processing Parameters Construction</h2>
<p>Predicate Signature For each predicate $P$, we generate its signature $\operatorname{Sign}<em 1="1">{P}$, which is a 60 -dimensional unit vector, satisfying that for two different predicates $P</em>}, P_{2}, \operatorname{Sign<em 1="1">{P</em>}} \cdot \operatorname{Sign<em 2="2">{P</em>&lt;0.5$. We can randomly generate those vectors and check until the constraints are satisfied. Empirically it takes no more than 200 trials.}</p>
<p>Meaningful Vector In parsing layer, we process the natural language inputs as multiple "meaningful vectors". The meaningful vectors are stored in form of $L_{A}\left|L_{B}\right| L_{C} | R |^{512}$, representing a rule $L_{A} \wedge$ $L_{B} \wedge L_{C} \rightarrow R$. Each segment $L_{A}, L_{B}, L_{C}, R$ has 64 dimensions, representing a predicate or a always True/False dummy predicate. For each predicate $P$, the first 63 dimensions, denoted as $P^{\text {sign }}$, form the signature of the predicate, and the last dimension is a Boolean variable, denoted as $P^{v}$. The following information is converted into meaningful vectors:</p>
<ol>
<li>Rule $L H S \rightarrow R H S$ : if the LHS has less than 3 predicates, we make it up by adding always True dummy predicate(s), and then encode it into meaningful vector, stored in the separating token follows the rule. In addition, for each predicate $P$ in LHS, we encode a dummy meaningful vector as False $\rightarrow P$ and store it in the encoding of $P$. This operation makes sure that every predicate in the input sentence occurs at least once in RHS among all meaningful vectors. We will see the purpose later.</li>
<li>Fact $P$ : we represent it by a rule True $\rightarrow P$, and then encode it into meaningful vector and store it in the embedding of the separating token follows the fact.</li>
<li>Query $Q$ : we represent it by a rule $Q \rightarrow Q$, encode and store it in the [CLS] token at beginning.</li>
</ol>
<p>Hence, in the embedding, some positions are encoded by meaningful vectors. For the rest positions, we use zero vectors as their embeddings.</p>
<h2>C. 2 Forward Chaining Parameters Construction</h2>
<p>Generally, to simulate the forward chaining algorithm, we use the attention process to globally broadcast the true value in RHSs to LHSs, and use the MLP layers to do local inference for each rule from the LHS to the RHS.</p>
<p>In attention process, for each meaningful vector, the predicates in LHS look to the RHS of others (including itself). If a RHS has the same signature as the current predicate, the boolean value of the RHS is added to the boolean value of the current predicate. Specifically, we construct three heads. We denote $Q_{i}^{(k)}$ to stand for the query vector of the i-th token of the k-th attention head. For a meaningful vector written as $L_{A}\left|L_{B}\right| L_{C} | R |^{512}$,</p>
<p>$$
\begin{aligned}
&amp; Q_{i}^{(1)}=L_{A}^{\text {sign }}\left|\frac{1}{4}, Q_{i}^{(2)}=L_{B}^{\text {sign }}\right| \frac{1}{4}, Q_{i}^{(1)}=L_{C}^{\text {sign }}\left|\frac{1}{4}\right. \
&amp; K_{i}^{(1)}=\beta R, K_{i}^{(2)}=\beta R, K_{i}^{(3)}=\beta R \
&amp; V_{i}^{(1)}=0^{63} | R^{v}, V_{i}^{(2)}=0^{63} | R^{v}, V_{i}^{(3)}=0^{63} | R^{v}
\end{aligned}
$$</p>
<p>Here $\beta$ is a pre-defined constant. The attention weight to a different predicate is at most $\frac{3 \beta}{4}$, while the attention weight to the same predicate is at least $\beta$, and the predicate with positive boolean value has even</p>
<p>larger $\left(\frac{5\beta}{4}\right)$ attention weight. Thus, with a large enough constant $\beta$, we are able to make the attention distribution peaky. Theoretically, when $\beta&gt;300 \ln 10$, we can guarantee that the attention result</p>
<p>$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>satisfies that the value is in the range of $[0.8,1.0]$ if the predicate on LHS is boardcasted by some RHS with true value, otherwise it is in the range of $[0,0.2]$.</p>
<p>This attention results are added to the original vectors by the skipped connection. After that, we use the two-layer MLP to do the local inference in each meaningful vector. Specifically, we set</p>
<p>$$
\begin{gathered}
10\left[\operatorname{ReLU}\left(L_{A}^{v}+L_{B}^{v}+L_{C}^{v}-2.3\right)\right. \
\left.-\operatorname{ReLU}\left(L_{A}^{v}+L_{B}^{v}+L_{C}^{v}-2.4\right)\right]
\end{gathered}
$$</p>
<p>as the updated $R^{v}$. Thus, $R^{v}=1$ if and only if all the boolean values in LHS are true, otherwise $R^{v}=0$. We also set $L_{A}^{v}, L_{B}^{v}, L_{C}^{v}$ as 0 for the next round of inference.</p>
<h1>C. 3 Output Layer Parameters Construction</h1>
<p>In output layer, we take out the Boolean value of the RHS of the meaningful vector in [CLS] token.</p>
<h2>D Examples from SimpleLogic</h2>
<p>Rules: If messy and hypocritical and lonely, then shiny. If tame, then friendly. If plain and shiny and homely, then nervous. If tender, then hypocritical. If dull and impatient and plain, then tame. If spotless, then perfect. If elegant and tender, then homely. If lonely and inquisitive and plain, then homely. If proud, then quaint. If outrageous and homely and impatient, then messy. If quaint, then outrageous. If elegant and glamorous and ugly, then homely. If perfect and sincere and mean, then ambitious. If spotless and quaint and tame, then messy. If tame and sincere and homely, then elegant. If ambitious, then elegant. If shiny and proud, then combative. If quaint and elegant and nervous, then impatient. If glamorous, then outrageous. If proud, then friendly. If combative and nervous, then outrageous. If outrageous and quaint, then careless. If lonely and plain, then inquisitive. If lonely and ugly and combative, then tame. If friendly, then dull. If lonely, then tame. If tender and plain and lonely, then elegant. If glamorous, then hypocritical. If tame and helpless and impatient, then friendly. If careless and messy, then nervous. If combative and shiny, then inquisitive. If plain and outrageous and ugly, then glamorous. If careless and quaint and spotless, then combative. If homely, then helpless. If ambitious, then proud. If messy and ugly, then inquisitive. If perfect, then proud. If helpless and perfect, then elegant. If perfect, then lonely. If lonely and hypocritical, then perfect. If perfect, then friendly. If tender and messy and ambitious, then quaint. If proud, then mean. If outrageous, then perfect. If nervous, then inquisitive. If hypocritical and homely and nervous, then tender. If friendly and dull and outrageous, then ambitious. If glamorous, then proud. If impatient and nervous, then spotless. If mean and quaint and lonely, then spotless. If glamorous, then careless. If dull and mean, then elegant. If homely, then proud. If inquisitive and plain, then ugly. If tender, then homely. If proud and quaint and lonely, then outrageous. If glamorous and perfect and dull, then messy. If helpless and tame and tender, then proud. If friendly and mean, then helpless. If inquisitive, then spotless. If shiny, then tame. If perfect and quaint, then careless. If careless and nervous and combative, then homely. If outrageous and inquisitive and elegant, then hypocritical. If tender and quaint and perfect, then careless. If mean and friendly and ambitious, then combative.
Facts: Alice shiny. Alice tender. Alice lonely.
Query: Alice is dull?
Label: True
Proof Depth: 3
From: RP</p>
<p>Rules: If comfortable, then tense. If nervous, then blushing. If nervous and difficult, then beautiful. If disgusted, then clean. If talkative and aggressive, then light. If versatile and supportive, then beautiful. If aggressive, then different. If glamorous and supportive and pleasant, then inexpensive. If light and outrageous and modern, then pleasant. If blushing, then tense. If beautiful, then clean. If perfect and inexpensive, then comfortable. If modern and different, then supportive. If tense, then glamorous. If talkative and aggressive and perfect, then blushing. If versatile, then outrageous. If tense, then perfect. If modern and perfect and inexpensive, then difficult. If versatile and aggressive, then reserved. If comfortable and versatile, then modern. If pleasant and versatile, then reserved. If clean and tense and difficult, then outrageous. If glamorous and modern, then courageous. If elegant and clean, then perfect. If pleasant, then tense. If versatile and blushing and elegant, then light. If reserved, then clean. If clean and talkative and difficult, then reserved. If light, then courageous. If blushing, then light. If different and beautiful, then modern. If disgusted and talkative, then perfect. If elegant and reserved and talkative, then aggressive. If elegant and courageous, then outrageous. If modern and difficult, then disgusted. If supportive and beautiful, then light. If blushing, then glamorous. If comfortable and modern and glamorous, then blushing. If disgusted and inexpensive and talkative, then difficult. If different and clean and disgusted, then modern. If clean and talkative and light, then supportive. If modern and nervous, then difficult. If talkative and aggressive, then modern. If tense and beautiful, then supportive. If modern and inexpensive and glamorous, then comfortable. If difficult and beautiful and modern, then supportive. If nervous and elegant and aggressive, then modern. If tense, then light. If comfortable and inexpensive and disgusted, then tense. If inexpensive and elegant, then nervous. If nervous, then elegant. If glamorous and pleasant, then elegant. If elegant and outrageous, then pleasant. If aggressive and disgusted and comfortable, then light. If talkative and reserved, then clean. If aggressive and modern and inexpensive, then supportive. If reserved and versatile and glamorous, then modern. If comfortable and pleasant and beautiful, then outrageous. If nervous and different and elegant, then modern. If difficult and perfect and outrageous, then tense. If comfortable and blushing and glamorous, then clean. If disgusted, then inexpensive. If inexpensive and tense, then blushing. If elegant, then aggressive. If inexpensive and versatile, then pleasant. If supportive and tense and beautiful, then disgusted. If glamorous and beautiful, then talkative. If tense and reserved, then beautiful. If different, then pleasant. If glamorous and supportive, then clean.
Facts: Alice versatile. Alice beautiful. Alice light. Alice glamorous. Alice outrageous. Alice difficult. Query: Alice is comfortable ?
Label: True
Proof Depth: 6
From: RP</p>
<p>Rules: If blushing and disgusted, then fancy. If impatient, then long. If frantic, then long. If blushing and frail, then gifted. If frail and long and fancy, then disgusted. If frantic and helpless, then gifted. If broad-minded and frantic, then blushing. If helpless, then broad-minded. If frantic and disgusted and frail, then blushing. If helpless, then impatient. If blushing, then disgusted. If long and gifted and blushing, then frantic. If frantic, then blushing. If fancy, then impatient. If gifted, then fancy. If frail, then helpless. If blushing and frail, then helpless. If blushing, then gifted. If broad-minded and impatient, then long. If broad-minded and disgusted, then fancy. If impatient and disgusted and long, then broad-minded. If broad-minded, then helpless. If disgusted and gifted, then blushing. If gifted and frantic, then fancy. If frail, then broad-minded. If fancy, then broad-minded. If broad-minded, then helpless. If blushing and disgusted, then fancy. If frantic and blushing and gifted, then frail. If frantic, then disgusted. If disgusted, then fancy. If fancy and helpless, then frantic. If frail and disgusted and helpless, then broad-minded. If frantic, then gifted. If long and fancy, then frantic. If blushing, then gifted. If impatient and helpless and gifted, then frantic. If frail and gifted and impatient, then broad-minded. If helpless, then broad-minded. Facts: Alice frail.
Query: Alice is disgusted?
Label: False
Proof Depth: 3
From: LP
Rules: If frantic and helpful, then victorious. If inquisitive and zealous, then bad-tempered. If busy and vivacious, then condemned. If embarrassed, then rude. If thoughtful and rude and helpful, then zealous. If agreeable, then curious. If witty and perfect and thoughtful, then shiny. If impartial and tense, then fine. If frantic and thoughtful and busy, then embarrassed. If agreeable, then pessimistic. If busy and long and embarrassed, then thoughtful. If long and intellectual and fancy, then enchanting. If perfect and victorious and hurt, then zealous. If inquisitive and hurt, then vivacious. If disgusted and tense, then intellectual. If fine, then busy. If fancy and bad-tempered, then fine. If thoughtful, then long. If victorious and condemned, then hurt. If tense, then fine. If frantic, then enchanting. If victorious, then impartial. If agreeable, then enchanting. If hurt and zealous and inquisitive, then fancy. If curious, then frantic. If helpful and zealous, then intellectual. If busy and curious, then agreeable. If curious, then helpful. If curious and victorious, then pessimistic. If witty and shiny and busy, then perfect. If rude and condemned and victorious, then zealous. If witty and embarrassed, then frantic. If perfect and victorious and enchanting, then fancy. If zealous and witty, then rude. If hurt and curious and condemned, then embarrassed. If victorious and busy and disgusted, then intellectual. If fancy and shiny, then enchanting. If hurt and victorious and agreeable, then curious. If thoughtful and helpful, then disgusted. If fancy and intellectual, then shiny. If frantic and impartial, then embarrassed. If impartial, then thoughtful. If pessimistic, then curious. If condemned, then thoughtful. If enchanting, then witty. If zealous and inquisitive and agreeable, then condemned. If fancy and inquisitive, then bad-tempered. If enchanting and fancy and rude, then curious. If vivacious and condemned, then zealous. If perfect, then impartial. If helpful and embarrassed and frantic, then condemned. If helpful, then perfect. If curious, then embarrassed. If condemned, then enchanting. If fine and intellectual, then shiny. If hurt and agreeable, then victorious. If victorious and condemned and rude, then inquisitive. If fancy, then victorious. If impartial and frantic and curious, then hurt. If fancy and long, then vivacious. If hurt and vivacious, then tense. If witty and vivacious and helpful, then embarrassed. If curious, then hurt. If fancy and rude, then zealous. If impartial and shiny and rude, then tense. If pessimistic, then embarrassed. If disgusted and busy and rude, then long. If witty and embarrassed and victorious, then pessimistic. If curious and agreeable, then vivacious. If embarrassed and hurt, then victorious. If intellectual, then witty.
Facts: Alice tense. Alice disgusted.
Query: Alice is hurt?
Label: False
Proof Depth: 6
From: LP</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Code available at https://github.com/ joshuacnf/paradox-learning2reason.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
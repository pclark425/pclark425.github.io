<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5640 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5640</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5640</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-270562235</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12830v3.pdf" target="_blank">What Are the Odds? Language Models Are Capable of Probabilistic Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle. An important but rarely evaluated form of reasoning is understanding probability distributions. In this paper, we focus on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions. We perform a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities. We evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation. Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified. To conduct this work, we developed a comprehensive benchmark distribution dataset with associated question-answer pairs that we have released publicly.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5640",
    "paper_id": "paper-270562235",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0044434999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What Are the Odds? Language Models Are Capable of Probabilistic Reasoning
30 Sep 2024</p>
<p>Akshay Paruchuri akshay@cs.unc.edu 
Jake Garrison 
Shun Liao 
John Hernandez 
Jacob Sunshine 
Tim Althoff althoff@google.com 
Xin Liu 
Daniel Mcduff dmcduff@google.com 
What Are the Odds? Language Models Are Capable of Probabilistic Reasoning
30 Sep 2024B19D31B0A0C236A2768CEAA305424120arXiv:2406.12830v3[cs.CL]
Language models (LM) are capable of remarkably complex linguistic tasks; however, numerical reasoning is an area in which they frequently struggle.An important but rarely evaluated form of reasoning is understanding probability distributions.In this paper, we focus on evaluating the probabilistic reasoning capabilities of LMs using idealized and real-world statistical distributions.We perform a systematic evaluation of state-of-the-art LMs on three tasks: estimating percentiles, drawing samples, and calculating probabilities.We evaluate three ways to provide context to LMs 1) anchoring examples from within a distribution or family of distributions, 2) real-world context, 3) summary statistics on which to base a Normal approximation.Models can make inferences about distributions, and can be further aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified.To conduct this work, we developed a comprehensive benchmark distribution dataset with associated question-answer pairs that we have released publicly.* Work completed during an internship at Google.## Consider the following distribution:Type: Log-Normal Distribution Characteristics: This distribution models values that are the result of the multiplicative product of many independent random variables, such as income levels, stock prices, or city sizes.Log Mean (mu): 3.543 Log Sigma (sigma): 0.677 These parameters mean that the natural logarithm of the values follows a normal distribution with the speci�ed mean and standard deviation.Your task is to estimate the percentile of a given average exercise minutes count value for a population that regularly uses Fitbit devices and is active on a daily basis.The data is �ltered for individuals aged 18-65.The data is age-balanced and gender-balanced, and pe�ains to the U.S. population only.Consider the following parameters that describe a normal distribution:</p>
<p>Introduction</p>
<p>Language models (LMs) (Workshop et al., 2022;Touvron et al., 2023;Achiam et al., 2023) are versatile interfaces to knowledge, capable of remarkably complex linguistic tasks.Summarization of complex documents (Tang et al., 2023;Zhang et al., 2024b), reasoning over long passages of text (Shaham et al., 2022;Chen et al., 2023;Team et al., 2023) and zero-shot inference in specialist domains such as medicine (McDuff et al., 2023) are a few examples that demonstrate their abilities.While performance on primarily linguistic problems, can often be strong, effectiveness on operations that involve numerical reasoning is a domain that language models have struggled with (Kojima et al., Models can make inferences about distributions, but can be aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified.</p>
<p>2022</p>
<p>). Difficulties handling numbers may be due to model pretraining formulations (e.g., using autoregressive next token prediction pretext tasks) or numerical token representations not necessarily being suited to mathematical reasoning (Bachmann and Nagarajan, 2024), or simply a limited representation of these types of tasks in the training corpora.Nevertheless, some work suggests prompting techniques can substantially improve LM performance on numerical reasoning tasks, indicating that relevant knowledge may already be encoded within these models (Imani et al., 2023).</p>
<p>A form of numerical reasoning that is important for interpreting many different forms of data is contextualizing an individual measurement or measurements (a sample or samples) within a population (a distribution).Drawing insights from data frequently requires comparing and contrasting a sample from other samples.This is because absolute values in isolation can be hard to interpret, without the context of how probable they are or how close they are to the maximum or minimum values observed across the population.Probabilistic reasoning is something that the human brain appears to do (Knill and Pouget, 2004) and that is an important component in cognition (Chater et al., 2006).Thinking probabilistically is efficient as one does not have to represent every detail of every sample that one observes, and instead can have the data summarized with a small number of parameters that describe the distribution (Lindskog et al., 2021).Research has shown that some probabilistic reasoning processes lead to superior performance; for example, people are more accurate at answering questions about statistical properties when they estimate the full distribution first (Goldstein and Rothschild, 2014).Yet, there are limited examples evaluating or improving on the probabilistic reasoning by designing LMs that reason over sets (Ozturkler et al., 2023).</p>
<p>Understanding the distributions is important in many contexts.In population level data it is important when gauging whether an individual behavior is normative (e.g., Is sleeping 8 hours normal for a college aged student?).In climatology, inferences about distributions of temperature or precipitation data on a given day of the year at a particular location are important when determining if observed events are typical or abnormal.Is a maximum temperature of 35°C likely to be observed in Seattle every year?</p>
<p>In this work we propose and define a set of proba-bilistic reasoning tasks and use them to evaluate the capabilities of LMs -estimating percentiles, drawing samples, and calculating probabilities.Next we evaluate the impact of additional real-world context and parametric assumptions (Normal distribution) using the task of estimating percentiles (task choice is motivated in Section 3).</p>
<p>To summarize our research questions:</p>
<ol>
<li>
<p>Section 5.1: Provided with an idealized distribution, are language models able to accurately answer questions about them?Does this vary by the distribution family?Does providing prompt examples from different distributions in the same family or samples from the same distribution help?Do LMs simply repeat the nearest in-context example or is there evidence of more complex LM behavior?</p>
</li>
<li>
<p>Section 6.2: Can an LM answer questions about distributions in the world (e.g., income in the US population)?Are LMs able to retrieve statistics and answer questions about these distributions in a zero-shot manner?</p>
</li>
<li>
<p>Section 6.2: Using simple approximations such as assuming a Normal distribution, can we design prompts that lead to more accurate answers to probabilistic reasoning questions?</p>
</li>
</ol>
<p>To answer these questions we develop a distribution dataset with associated question-answer pairs that we will release publicly.The dataset includes questions about 12 families of standard, idealized distributions (e.g., Normal or Power-law distributions) and distributions of real-world data from the domains of population health, climate, and finance.Code and additional results for our work can be found here: https://github.com/yahskapar/LLMs-and-Probabilistic-Reasoning.</p>
<p>Related Work</p>
<p>Language Models and Numerical Reasoning.Working with numbers is necessary for many everyday tasks.Yet, while large language models pretrained on vast numbers of documents exhibit impressive linguistic capabilities, they often struggle at tasks involving numerical reasoning (Saxton et al., 2019;Kojima et al., 2022) (for a survey see Lu et al. (2022)).Different approaches to numerical reasoning have been proposed, many focusing on logical reasoning of mathematical tasks (Geva et al., 2020;Imani et al., 2023;Yang et al., 2022;Webb et al., 2023).In quantitative reasoning problems such as those in the domains of mathematics, science, and engineering, the process of fine-tuning models has been used to successfully remedy weaknesses (Lewkowycz et al., 2022).Automatic generation of data can be used as a way of obtaining training examples (Geva et al., 2020;Liu et al., 2022).The fact that specific prompting, such as providing examples, can improve the performance of LMs on numerical tasks, suggests that their training data may already include relevant information to perform these tasks (Imani et al., 2023;Yang et al., 2022).Benchmark datasets have helped the research community to develop these methods (e.g., (He-Yueya et al., 2023;Zhang et al., 2024a;Liu et al., 2024)) further and automatic evaluation of numerical reasoning problems has been proposed to help in cases where accuracy cannot be computed mathematically (Cobbe et al., 2021).Numerical Reasoning Prompt Design.Prompts designed to handle automatically generated content (from an LM) were used to improve on numerical and scientific commonsense reasoning tasks (Liu et al., 2022).Algorithms and code are useful tools when working with numbers, chain-ofthought prompts have been designed to leverage these specifically to improve the performance of LMs on arithmetic problems (Imani et al., 2023;Merrill et al., 2024).Retrieval of correlatedexamples (Yang et al., 2022), generating intermediate reasoning steps (Gao et al., 2023) and expressing reasoning as a program (Chen et al., 2022) are examples that can also help improve LM performance on mathematical and logic problems.Simple approaches such as zero-shot chainof-thought (Kojima et al., 2022) exist and are capable of leveraging multi-step reasoning, but for probabilistic reasoning tasks lead to severely degraded performance due to generally poor numerical reasoning performance.Probabilistic Reasoning and Cognition.Inspiration for AI systems is often drawn from our understanding of human cognition, cognitive science has revealed insights about how humans can think probabilistically (Cosmides and Tooby, 1996;Oaksford and Chater, 2001) and can build representations of relatively complex probability distributions (Lindskog et al., 2021), yet our perceptions of means and variances are subject to biases (Tversky and Kahneman, 1974).The thought processes people use when answering questions about distributions have an impact on their accuracy.Specifically, elic-iting a full distribution before computing summary or sample statistics can make answers more accurate (Goldstein and Rothschild, 2014).</p>
<p>Defining Probabilistic Reasoning Tasks</p>
<p>Our probabilistic reasoning benchmark contains three distinct tasks that explore a language model's (LM's) context-free (idealized) understanding of basic, idealized distributions, we describe these tasks as follows: Task 1: Estimating Percentiles.Given a distribution, the model is asked for the percentile a sample would appear in.</p>
<p>A question is composed of a value (a sample) that is calculated given the target percentile.The answer is expected to be a numerical response from 0 to 100.The target percentiles utilized were , 10, 20, 30, 40, 50, 60, 70, 80, 90, 99}.Language models responses to these questions are sampled 10 times with a random seed.
n st/th = {1
Return the percentile of the value {X} within a normal distribution with mean {Y} and standard deviation {Z}.</p>
<p>Task 2: Drawing Samples.Given a distribution the model is asked to draw samples at random from it.A random seed is used for each sample and we repeat this 1000 times per distribution.The language model is explicitly instructed to avoid generating any code or using additional tools to perform the sampling.The answer is expected to be a numerical response.</p>
<p>Sample a number from the normal distribution with mean {X} and standard deviation {Y}.</p>
<p>Task 3: Calculating Probabilities.Given a distribution the model is asked for the probability a sample from the distribution will fall between two given values.The target probabilities and the corresponding target ranges are computed based on a lower and upper quantile to form examples with different probabilities in the set P = {0.1,0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.The answer from the LM is expected to be a numerical response from 0 to 1. Calculate the probability that a value falls between {W} and {X} in a normal distribution with mean {Y} and standard dev.{Z}.</p>
<p>=</p>
<p>Idealized Distributions</p>
<p>Real-World Distributions</p>
<p>In order to evaluate the impact of real-world context on LM probabilistic reasoning, we explore distributions in the real-world that have additional real-world context (e.g., prior knowledge and expectations about US household incomes).We then leverage Task 1 of estimating percentiles to evaluate to what degree real-world context impacts performances.Task 1 is particularly well suited for this exploration, as we later demonstrate that this Task 1 elicits the highest variation in performance across distribution families and the number of incontext examples/shots (further detailed in Section 6).In Appendix A we provide full prompt templates for our three proposed benchmark tasks, as well as further details regarding template components and real-world distribution prompt templates.</p>
<p>Curating Data Distributions</p>
<p>We use two distinct datasets for the purpose of understanding the probabilistic reasoning capabilities of LMs -a dataset of idealized distributions and a dataset of real-world distributions.A visualization of both datasets is shown in Figure 2.</p>
<p>Idealized Distributions.</p>
<p>We identify 12 families of distributions: Normal, Log-Normal, Skew-Normal, Exponential, Power Law, Uniform, Gamma, Gumbel, Poisson, Geometric, Binomial and Multinomial.These encompass sets of distributions identified (Frank, 2009) and tested in prior studies (Goldstein and Rothschild, 2014).When an LM is asked about an idealized distribution, a distribution description is provided with parameters that can range from simple parameters such as the mean and standard deviation (e.g., for a Normal distribution) to less easily interpretable parameters such as location and scale (e.g., for a Gumbel distribution).Note that the distribution description does not include any distrbution probability density function (PDF) or cumulative distribution function (CDF).All parameters are captured in a popular, public Python library for scientific computing -NumPy.Real-World Distributions.We choose real-world distributions from the domains of health, finance, and climate for which there is presumed to be relevant information in the model's training set.Health: We sample 100K Fitbit users from the U.S. population, aged 18-65, and with at least 10 days of data from the calendar year 2023.The dataset was gender and age balanced (see Appendix).We analyze four common wearable metrics (A) step count, (B) resting heart rate, (C) sleep duration, and (D) exercise minutes.We aggregate this data to obtain averages for each user and ultimately distributions of daily metrics across 100K users.</p>
<p>In what percentile of the US population would someone with an average of 6000 steps per day be?</p>
<p>Finance: We use public census data from the Census Bureau's American Community Survey (ACS) Public Use Microdata Sample (PUMS) (Ruggles et al., 2020) that contains measures of (E) annual income, (F) monthly gross rent, (G) annual electricity costs, and (H) annual water costs ($) for individuals and households in the US.We select data from the calendar year 2018.</p>
<p>In what percentile of US households would someone with a household income of $70,000 be?</p>
<p>Climate: We use the public Global Historical Climatology Network daily (GHCNd) dataset (Menne et al., 2012) maintained by the National Oceanic and Atmospheric Administration (NOAA) that contains average daily measures for variables of (I) temperature, (J) precipitation, (K) wind speed, and (L) humidity level for weather stations across the continental United States.We consider data from the calendar year 2018 and filter erroneous measures indicated by measurement quality flags built into the GHCNd dataset.</p>
<p>In what percentile would an average temperature of 20 degrees Celsius be in the USA?</p>
<p>5 Experimental Setup</p>
<p>Idealized Distributions</p>
<p>Zero-shot Performance.We evaluate the zeroshot performance of three LMs (Gemini 1.0 Ultra, GPT4-Turbo, and GPT3.5-Turbo) across our three tasks and 12 idealized distributions.LM prompts are generated as formulated in Section 3. N-Shot Performance.We propose two types of shots that might be reasonably employed for the tasks -within distribution family distribution shots and within distribution shots.Within distribution family distribution shots are examples from a different distribution from the same family as the current distribution in question.The distribution parameters of the variant are randomly sampled from a specified range of reasonable parameter values.For example, if we are asking for a percentile of a value in a normal distribution with a mean of 100 and a standard deviation of 10, and we are providing three shots, the randomized shots may be generated from three variant normal distributions with means of 108, 118, and 112 and corresponding standard deviations of 13, 16, and 10.Within distribution shots entail shots from the same distribution that is being asked about in a question.</p>
<p>To help contextualize the performance in the N-shot experiments, for the task of estimating percentiles we compare both shot types to a baseline where the answer is picked based on the nearest corresponding target percentile value in the shot examples (i.e., the nearest neighbor).This baseline does not perform any interpolation between percentiles, which would be required for optimal performance.If the LM performance exceeds this baseline performance, it would suggest that the LMs does perform some form of interpolation, instead of simply reciting in-context examples.</p>
<p>We explicitly avoid using shots that involve an answer that could be an answer to one of our proposed questions.Specifically, we use n th shots = {5, 15, 25, 35, 45, 55, 65, 75, 85, 95} and P shots = {0.05,0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95}.For example, if we are asking for a percentile of a value in a Normal distribution with a mean of 100 and a standard deviation of 10, and we are providing three shots, the mapped shots will be generated from the same normal distribution and correspond to 35.0, 55.0, and 75.0.We sample LM responses 10 times per question with a random seed.We provide further details, including examples per shot type and shot count, in our Section 8.</p>
<p>Real-World Distributions</p>
<p>Zero-shot Performance.We evaluate the zeroshot performance of three LMs (Gemini 1.0 Ultra, GPT4-Turbo, and GPT3.5-Turbo) across the proposed task of estimating percentiles in order to evaluate an LM's understanding of probabilistic reasoning of the real-world distributions in the domains of health, finance, and climate mentioned in Section 4. We design prompts that contain information about the corresponding real-world data, such as where it was sourced from, the year in time for which the data is relevant, and any relevant filtering that was done, this acts as context that we with a specified mean and standard deviation.This approach can be further justified by the fact that, despite characteristics such as skewedness being present in real-world distributions, many distributions are similar to a Normal distribution.We quantitatively reinforce this observation using the Kolmogorov-Smirnov test (Chakravarti et al., 1967) to show that even if the Normal equivalent is not the best fit for a given real-world distribution, it can be remarkably close as evidenced by the K-S statistic.Additionally, we compare our proposed Normal approximation approach to simply providing a question involving a real-world distribution with three within distribution shots.</p>
<p>Experimental Results</p>
<p>We organize our results based on the research questions posed in Section 1. Alongside a concise answer in bold, we provide discussion based on our analysis of the experimental results.2, LMs are capable of varying degrees of zero-shot performance given different kinds of context.We consider the real-world context as the primary baseline in our investigation of real-world distributions, in contrast to idealized, context-free versions of the same realworld distributions and added context that simplifies assumptions (e.g., Normal approximation).On average with real-world context, Gemini 1.0 Ultra has superior zero-shot performance on distributions in the climate domain.In contrast, GPT4-Turbo has superior zero-shot performance in the health domain.Both Gemini 1.0 Ultra and GPT4-Turbo had comparable performance on the finance domain while being superior to GPT3.5-Turbo.We attribute these differences in zero-shot performance to underlying differences in the large amounts of training data used to train LMs, especially in the case Gemini 1.0 Ultra and GPT4-Turbo.Q5: Does the provided real-world context help with probabilistic reasoning performance?Answer: Yes.Discussion: Figure 5   restricted to distributions that already have a reasonable baseline performance, we suspect that the saturated performance conflicts with the model's ability to leverage real-world context and simpler assumptions such as the Normal approximation.Additionally, certain distributions such as household income show a decrease in performance when Normal approximation is applied, likely because the household income distribution follows a Power Law distribution.It is unhelpful to apply a Normal approximation on distributions that differ greatly from a normal distribution.We empirically show this with the same set of 12 idealized distributions in Appendix D.3.Lastly, we note that real-world context with 3 shots has the best performance.This is unsurprising, and furthermore does not invalidate the impact of simplified assumptions such as Normal approximation, which can be more efficient due to not relying on 3 shots.Q6: Do parametric assumptions such as a Normal approximation as a prompt design strategy improve performance?Answer: Yes.Discussion:</p>
<p>Idealized</p>
<p>On average across all domains, yes.The simple assumption of a Normal distribution performs well and when paired with real-world context, consistently improves performance on real-world distributions (see Figure 5).This seems reasonable given the aforementioned internal, potentially incorrect representations of real-world distributions that LMs can have, and subsequently how stats such as mean and standard deviation can help correct the LM's baseline knowledge.It is perhaps surprising that performance improves relatively consistently, despite the real-world distributions often differing from an idealized Normal distribution and therefore the LM is being conditioned on a misspecified, yet still helpful, model.Q7: How do simpler assumptions such as a Normal approximation compare to providing three few-shot examples?Answer: Providing three few-shot examples is generally better.Discussion: Generally speaking, providing three fewshot examples is better and provides superior performance across our proposed domain-specific datasets of health, finance, and climate.</p>
<p>Conclusion</p>
<p>LMs are able to answer questions about idealized and real-world distributions, with real-world results suggesting there is some internal representation that enables modeling or interpolation from distribution parameters.The probabilistic reasoning performance of LMs varies, with certain distributions (e.g., uniform, normal) having much better performance in contrast to other distributions (e.g., log-normal, skew-normal).Within distribution shots and context can improve probabilistic reasoning performance, as can a simplified Normal approximation.</p>
<p>Limitations</p>
<p>Numerical calculation and reasoning remains an area in which language models, even very large models, tend to perform poorly.Making approximations based on distributions is effective; however, it may also be a source of potential biases.</p>
<p>Our experiments have not focused on a deep exploration of the ability of language models to represent and answer questions about extreme values, such as outliers in distributions.Our results do suggest that language model particularly struggle with accounting for extreme values in very skewed (long-tail) distributions.In computing percentiles the model would often overestimate the percentiles (and thus underestimate the presence of extreme values) -see the Power Law family results in Fig. 6 of our appendices.Our work shows that, despite some promising zero-shot performance and ways to improve that performance, language models require more improvements with Non-Uniform and Non-Normal distributions before they are capable of being relied on for probabilistic reasoning of real-world distributions that follow other distributions (e.g., Power Law).We hope our insights as a part of this work, as well as our proposed tasks and datasets critical to probabilistic reasoning and to be publicly released, prove valuable to the community at large and their efforts to make language models more useful, safer, and ultimately more reliable.</p>
<p>To systematically investigate performance on idealized distributions using the three proposed tasks of estimating percentiles, drawing samples, and calculating probabilities, our method involves sets of questions, or in the case of drawing samples, a command, that systematically tests the model's knowledge of a given distribution.In addition to investigating our proposed tasks in a zero-shot setting, we consider two different ways of providing in-context examples (shots) in the prompt -within family shots and within distribution shots.</p>
<p>Zero-shot Performance.We evaluate the zeroshot performance of three LMs (Gemini 1.0 Ultra, GPT4-Turbo, and GPT3.5-Turbo) across our proposed tasks of estimating percentiles, drawing samples, and calculating probabilities in order to evaluate an LM's understanding of probabilistic reasoning of the 12 idealized distributions described in Section 4. LM prompts are generated as formulated in Section 3.</p>
<p>Performance by Shot Type.We propose two shot types used across the aforementioned tasks -within distribution family distribution shots and within distribution shots.Within distribution family distribution shots entail randomized shots from a different variant of the distribution being asked about in a question.The distribution parameters of the variant are randomly sampled from a specified range of reasonable parameter values.For example, if we are asking for a percentile of a value in a normal distribution with a mean of 100 and a standard deviation of 10, and we are providing three shots, the randomized shots may be generated from three variant normal distributions with means of 108, 118, and 112 and corresponding standard deviations of 13, 16, and 10.Within distribution shots entail shots from the exact same distribution that is being asked about in a question.The shots are mapped per shot count to allow for a reasonable spread of shots throughout the distribution.</p>
<p>Additionally, for the task of estimating percentiles, we compare both shot types to a baseline where the LM is asked to pick from one or more shots' answer based on the nearest corresponding target percentile value.This baseline represents a nearest neighbor approach using the set of incontext examples.This baseline makes appropriate use of the information given to the model, but importantly does not perform any interpolation between percentiles, which would be required for strong performance.If LM performance exceeded baseline performance, this would suggest that LMs perform some kind of interpolation, instead of simply reciting in-context examples.</p>
<p>To avoid biasing our results, in the case of the estimating percentiles and calculating probabilities tasks, we explicitly avoid using shots that involve an answer (percentile or probability respectively) that could potentially be an answer to one of our proposed questions.Specifically, we use n th shots = {5, 15, 25, 35, 45, 55, 65, 75, 85, 95} and P shots = {0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95}.For example, if we are asking for a percentile of a value in a normal distribution with a mean of 100 and a standard deviation of 10, and we are providing three shots, the mapped shots will be generated from the same normal distribution and correspond to 35.0, 55.0, and 75.0.We sample Gemini 1.0 Ultra responses 10 times per question with a random seed.Then, we elect to average and effectively use all answers as a part of our final evaluation in order to help capture the variability in language model responses and provide a broader understanding of their reasoning capabilities.This effectively is a form of self-consistency.We provide further details, including examples per shot type and shot count, in our Section 8. Key LM Parameters for Reproducibility.Language models typically have additional parameters, such as temperature and sampling strategies, which have default settings that can vary from model to model.For Gemini 1.0 Ultra, GPT4-Turbo, and GPT3.5-Turbo, we utilize a default temperature of 0.7 for all of our experiments because we empirically discovered that a temperature of 0.7 to 0.9 with a random seed yielded similar, optimal performance on a hold-out dataset.A temperature of 0.7 also happens to be a default for many language models, such as various versions of the GPT and Gemini family models.Unless noted otherwise, results are obtained using a random seed.</p>
<p>Additional hyperparameters, such as frequency and brevity penalties, are not utilized in our experiments.We define a frequency penalty as a penalty that is applied proportional to how many times a token has appeared in the response and prompt.We define a brevity penalty as a penalty that targets responses that are very short, for example translations that contain only a few words.The nature of our proposed probabilistic reasoning questions, where the final answer from the instruction-tuned language model is always a single, numerical answer that is constrained by the prompt (for example, percentile answers of 25.3, 55.7, and 82.1) means that in both cases these penalties are not particularly relevant or effective at producing optimal answers.We utilized the default decoding approach for each model, with a default top-p (0.95), and where applicable, top-k (40</p>
<p>B Idealized Distributions Results Summaries</p>
<p>You are an expe� on statistics.Your task is to estimate the percentile of a number within a speci�c distribution.Answer with just a numerical response from 0 to 100.Make sure your �nal answer is enclosed by xml tags <answer> and </answer>.</p>
<p>Consider the following distribution: \textbf{{distribution_description}}</p>
<p>Here is your question: Question: What is the percentile of the value \textbf{{target_number}} within the provided distribution?Answer:</p>
<p>Zero Shot Answer Three Shot Answer Ground-Truth</p>
<p>FinanceFigure 1 :
1
Figure 1: LMs &amp; Probabilistic Reasoning.Models can make inferences about distributions, but can be aided by the incorporation of real-world context, example shots and simplified assumptions, even if these assumptions are incorrect or misspecified.</p>
<p>Figure 2 :
2
Figure 2: Distributions.A visualization of the 12 idealized and 12 real-world distributions across the domains of health, finance, and climate involved in our evaluation.</p>
<p>Figure 6 :Figure 7 :Figure 8 :
678
Figure 6: Percentile Results.Zero and three-shot (within distribution) results for returning percentile estimations in each of the 12 families of distributions.</p>
<p>Table 1 :
1
Aggregated zero-shot task performance across different LMs.We evaluate zero-shot perfor-
Model Percentiles (%) Sampling (K-S) Probabilities (%)GPT3.5-Turbo 25.7 ± 3.110.73 ± 0.0732.7 ± 2.38GPT4-Turbo 14.9 ± 2.390.59 ± 0.0821.0 ± 2.11Gemini 1.0 Ultra 16.5 ± 2.670.76 ± 0.0919.4 ± 2.26mance for tasks such as percentiles, sampling, andprobabilities using Gemini 1.0 Ultra, GPT4-Turbo, andGPT3.5-Turbo. For the tasks of estimating percentilesand calculating probabilities, results are reported asMean Absolute Error (MAE) ± Standard Error (σ M ).For the task of drawing samples, the Mean K-S statistic± Standard Error (σ M ) is reported with all reported val-ues having p &lt; 0.01.5 10 15 20 25 Mean Absolute Error (MAE)# Shots 0 1 3 5 7 90Nearest ShotWithin Family Shots Within Distribution ShotsFigure 4: Language models appear to interpolatebetween in-context examples. Comparison of withinfamily and within distribution shot types to a baselinewhere the answer is based on the nearest correspondingshot to the target percentile value (nearest neighbor),importantly the baseline does not perform any interpo-lation between percentiles.others. Discussion: Language model performancevaried considerably across families of distribution.Zero-shot performance on the percentile task wasbest for the uniform (MAE = 0.54%) and normal(MAE = 2.29%) distributions (see Figure 3 forexamples and Figure 6 for more detailed results).Furthermore, the performance of LMs on answer-ing questions about distributions varied by task.Estimating percentiles showed rather impressivezero-shot performance (MAE μ = 16.52, min =0.54, max = 28.36). Calculating probabilities wasworse on average (MAE µ = 21.48, min = 9.03,max = 32.53) and sampling performance was gen-erally poor in the zero-shot case. In all cases,providing shots (for different percentiles, samplesor probabilities) within a distribution improved theperformance substantially (Percentile ∆ 0→1shot =+59.14%, Sampling ∆ 0→1shot = +55.26% K-S stat.,Probability ∆ 0→1shot = +70.13%).
DistributionsQ1: Are LMs able to accurately answer questions about idealized distributions in a zeroshot setting?Answer: It varies, performance on some idealized distributions is better than Q2: Does providing prompt examples from different distributions in the same family or samples from the same distribution help?Answer: Providing within distribution shots helps more than within family shots.Discussion: As Q4: What is the zero-shot accuracy of an LM on distributions in the world (e.g., income in the US population)?Answer: It varies, performance on some real-world distributions is better than others.Discussion: As shown in Table</p>
<p>Inferences can be aided by context and simplified assumptions.Mean absolute error in calculating percentiles for real-world distributions with different prompts, including idealized distributions without real-world context, added real-world context, and a Normal approximation approach that simplifies parameter content.(*) designates p &lt; 0.05 for all possible pairs using the Wilcoxon signed-rank test.-world Con.Norm.Approx.Idealized Real-world Con.Norm.Approx.Idealized Real-world Con.Norm.Approx.
0 10 20 30 Mean Absolute Error (MAE)+ Real-World Context &amp; 3 Shots &amp; Normal Approximation + Real-World Context + Real-World Context IdealizedʬʭyØĮëĒĮċ´ČÎØÍŘ|Ø´ĆʲOEĒĮĆÔ$ôĲĹĮôÍľĹôĒČ0 10 20 30 ʬʭyØĮëĒĮċ´ČÎØÍŘ$Ēċ´ôČ ʗ ʗ ʗS t e p s H e a r t R a t e S l e e p M i n s E x e r c i s e M i n s I n c o m e G r o s s R e n t E l e c C o s t W a t e r C o s t T e m p e r a t u r e P r e c i p i t a t i o n W i n d S p e e d H u m i d i t yH e a l t h F i n a n c e C l i m a t eFigure 5: HealthFinanceClimateModel Idealized RealGPT3.5-Turbo 20.5 ± 9.62 20.3 ± 8.516.81 ± 0.68 17.7 ± 4.5420.4 ± 2.887.55 ± 0.77 22.7 ± 6.8825.7 ± 6.327.90 ± 0.22GPT4-Turbo 11.0 ± 4.944.92 ± 3.183.15 ± 00.76 8.99 ± 1.1810.7 ± 3.245.50 ± 0.4818.5 ± 6.5315.2 ± 5.134.94 ± 0.58Gemini 1.0 Pro 25.30 ± 8.41 11.51 ± 1.0610.42 ± 1.32 29.35 ± 3.72 11.77 ± 0.9210.10 ± 1.01 26.20 ± 5.44 18.67 ± 2.0116.53 ± 1.94Gemini 1.0 Ultra 12.8 ± 4.4310.3 ± 2.495.89 ± 1.09 14.0 ± 4.4710.5 ± 2.757.62 ± 1.0616.9 ± 3.8610.5 ± 0.797.43 ± 1.11details bothdistribution-wise and domain-wise results usingGemini 1.0 Ultra with our four context categories-idealized, real-world context, real-world contextwith Normal approximation, and real-world contextwith 3 shots. On average, adding real world-contextimproves performance and adding real-world con-text with a Normal approximation improves perfor-mance still further. This trend is true on aggregatebut not for all individual distributions (e.g., rest-ing heart rate). This observation appears to be</p>
<p>Table 2 :
2
Zero-shot performance by domain and context category across different LMs.All results are reported as Mean Absolute Error (MAE) ± Standard Error (σ</p>
<p>M ) with (%) units.</p>
<p>). Describes the number of successes in a fixed number of trials with a given probability of success.Trials: {n} (Total number of trials.)Probability of Success: {p} (Probability of success in each trial.)
Binomial Within Distribution ShotsDistribution Type: Binomial Distribution Example 1: Distribution: Distribution Type: Normal Distribution Mean: 100 Characteristics: Multinomial Standard Deviation: 10Distribution Type: Multinomial DistributionCharacteristics: Generalizes the binomial distribution for scenarios where each trial can result in more than two outcomes.Trials: {n} number of trials.) {probs} Question:What is the percentile of 96.183 within the provided distribution?Answer:<answer>35.0</answer>A.2.2 Examples of Few-shotsExample 2:Within Family Shots DistributionExample 1: Distribution Type: Normal DistributionDistribution: Mean: 100Standard Deviation: 10Distribution Type: Normal DistributionMean: 80 Question:Standard Deviation: 10 What is the percentile of 101.298 within the provided distribution?Answer:Question: <answer>55.0</answer>What is the percentile of 74.722 within the provided distribution?Answer: Example 3:<answer>30.0</answer> Distribution:Example 2: Distribution Type: Normal DistributionDistribution: Mean: 100Standard Deviation: 10Distribution Type: Normal DistributionMean: 108 Question:Standard Deviation: 13 What is the percentile of 106.802 within the provided distribution?Answer:Question: <answer>75.0</answer>What is the percentile of 107.903 within the provided distribution?Answer:<answer>50.0</answer>Example 3:Distribution:Distribution Type: Normal DistributionMean: 82Standard Deviation: 8Question:What is the percentile of 88.708 within the provided distribution?Answer:<answer>80.0</answer>
Overview of AppendicesThe appendix is organized as follows: Appendix A contains additional experimental details related to the usage of idealized distribution prompts, examples of idealized distribution prompts used in Section 5.1, per task, as well as distribution description examples, and examples of few-shots.Appendix B contains 3x4 summary figures corresponding to idealized distribution results described in Section 6.1.Appendix C contains examples of real-world distribution prompts used in Section 5.2.Appendix D contains additional model-wise experimental results that extend Table1 and Table 2, normal approximation results that show whether or not invoking the true distribution name makes a difference, and results for Chain-of-Thought (CoT) and code tool-use.Appendix E contains our broader impacts statement.A Idealized DistributionLog-NormalDistribution Type: Log-Normal Distribution Characteristics: This distribution models values that are the result of the multiplicative product of many independent random variables, such as income levels, stock prices, or city sizes Log Mean (mu): {mean} Log Sigma (sigma): {sigma} These parameters mean that the natural logarithm of the values follows a normal distribution with the specified mean and standard deviation.ExponentialDistribution Type: Exponential Distribution Characteristics: Models the time between events in a process where events occur continuously and independently at a constant average rate.Rate: {rate} (The average number of events per unit time is {1/rate:.2f}.)Power LawDistribution Type: Power Law Distribution Characteristics: Known for its heavy tails suitable for describing phenomena with a high incidence of extreme values.Alpha: {alpha} (Controls the tail heaviness-the smaller the alpha, the fatter the tail.)Xmin: {xmin} (Minimum value for which the power law behavior holds.)Climate Example ## You are an expert on climate science and statistics.Your task is to estimate the percentile of a given average temperature value using data from U.S. weather stations in the year 2018, sourced from the National Oceanic and Atmospheric Administration (NOAA) Global Historical Climatology Network Daily (GHCNd).Do not use any additional tools such as code generation or search engines.Answer with just a numerical response from 0 to 100.Make sure your answer is enclosed by xml tags <answer> and </answer>.## Consider the following parameters that describe a normal distribution of this data: Mean: 10.643 Standard Deviation: 12.628 ## Here is your question: Question: What is the percentile of an average temperature of {targe_number} degrees Celsius?Do not use any additional tools such as code generation or search engines.Answer with just a numerical response from 0 to 100.Make sure your answer is enclosed by xml tags <answer> and </answer>.Answer:UniformD Additional Experimental ResultsD.1 Additional Model ResultsAdditional model results, extending Table1and Table 2, can be found as a part of our GitHub repo here: https://github.com/yahskapar/LLMs-and-Probabilistic-Reasoning.D.2 CoT and Code Tool-use ResultsWe provide additional zero-shot Chain-of-Thought (CoT)(Kojima et al., 2022)and code tool-use results in Table3.Though both CoT and code tooluse show benefits over assuming an idealized distribution or adding real-world context (Table2), neither is convincingly better than the normal approximation approach.The fact that CoT results do not exceed normal approximation suggest that it is non-trivial to instruct a model to improve its modeling of non-normal distributions.Additional improvements to CoT-based approaches could be achieved with further investigation and development of techniques useful for numerical reasoning tasks.Furthermore, though we chose to use a cutoff data for corresponding datasets and did not employ a retrieval approach to retrieve and rank recent information that may be relevant to a proposed probabilistic reasoning question, we acknowledge that a retrieval tool can be useful to get more upto-date information versus relying on parametric knowledge.D.3 Additional Normal Approximation ResultsIn Figure9, we additionally present results that involve a variant of normal approximation where the true distribution is assumed with a mean and standard deviation as a part of the prompt.In the case of idealized distributions, we utilize the same distribution name with provided mean and standard deviation.In the case of real-world distributions, we approximate the distribution based on the K-S statistic after a matching process across all 12 idealized distributions described in Section 4.E Broader ImpactsThough we pose more constrained, systematic questions as a part of our investigation of language models and their ability to perform probabilistic reasoning, real-world questions such as "is taking 8000 steps a day normal for an average adult in the U.S. population using wearable devices?" and others that we pose as a part of section 4 are completely reasonable questions for an average user of LMs to ask.There is a significant practical impact in improving the probabilistic reasoning capabilities of language models on real-world distributions, especially when answers to the aforementioned reasonable questions can affect a user's perception of real-world distributions and ultimately their perspective on potentially critical matters such as those in the domains of health, finance, and climate.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>The pitfalls of next-token prediction. Gregor Bachmann, Vaishnavh Nagarajan, arXiv:2403.069632024arXiv preprint</p>
<p>Handbook of methods of applied statistics. Mohan Indra, Radha Chakravarti, Jogabrata Govira Laha, Roy, Wiley Series in Probability and Mathematical Statistics (USA) eng. 1967</p>
<p>Probabilistic models of cognition: Conceptual foundations. Nick Chater, Joshua B Tenenbaum, Alan Yuille, Trends in cognitive sciences. 1072006</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, arXiv:2306.155952023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Are humans good intuitive statisticians after all? rethinking some conclusions from the literature on judgment under uncertainty. cognition. 5811996</p>
<p>The common patterns of nature. Frank Steven, Journal of evolutionary biology. 2282009</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Lay understanding of probability distributions. G Daniel, David Goldstein, Rothschild, Judgment and Decision making. 912014</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, arXiv:2304.09102arXiv:2303.05398Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. 2023arXiv preprint</p>
<p>The bayesian brain: the role of uncertainty in neural coding and computation. C David, Alexandre Knill, Pouget, TRENDS in Neurosciences. 27122004</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Can the brain build probability distributions?. Marcus Lindskog, Pär Nyström, Gustaf Gredebäck, Frontiers in Psychology. 125962312021</p>
<p>Generated knowledge prompting for commonsense reasoning. Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Le Ronan, Yejin Bras, Hannaneh Choi, Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng, arXiv:2402.176442024arXiv preprint</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, arXiv:2212.10535A survey of deep learning for mathematical reasoning. 2022arXiv preprint</p>
<p>Daniel Mcduff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, arXiv:2312.00164Towards accurate differential diagnosis with large language models. 2023arXiv preprint</p>
<p>An overview of the global historical climatology network-daily database. Imke Matthew J Menne, Russell S Durre, Byron E Vose, Tamara G Gleason, Houston, Journal of atmospheric and oceanic technology. 2972012</p>
<p>Mike A Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y Mclean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, and Xin Liu. </p>
<p>Transforming wearable data into health insights using large language model agents. arXiv:2406.06464Preprint</p>
<p>The probabilistic approach to human reasoning. Mike Oaksford, Nick Chater, Trends in cognitive sciences. 582001</p>
<p>Thinksum: Probabilistic reasoning over sets using large language models. Batu Ozturkler, Nikolay Malkin, Zhen Wang, Nebojsa Jojic, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>. Steven Ruggles, Sarah Flood, Ronald Goeken, Josiah Grover, Erin Meyer, Jose Pacas, Matthew Sobek, 2020Ipums usa: version 10.0 [dataset</p>
<p>. Minneapolis, Ipums10D010Mn</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, arXiv:1904.01557Analysing mathematical reasoning abilities of neural models. 2019arXiv preprint</p>
<p>Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, arXiv:2201.03533Scrolls: Standardized comparison over long language sequences. 2022arXiv preprint</p>
<p>Evaluating large language models on medical evidence summarization. Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin F Rousseau, Digital Medicine. 611582023</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Amos Tversky, Daniel Kahneman, science. 18541571974</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 792023</p>
<p>Bigscience Workshop, Le Teven, Angela Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ilić, Roman Hesslow, Alexandra Sasha Castagné, François Luccioni, Yvon, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Logicsolver: Towards interpretable math word problem solving with logical prompt-enhanced learning. Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, Xiaodan Liang, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Qin Lyu, et al. 2024a. A careful examination of large language model performance on grade school arithmetic. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, arXiv:2405.00332arXiv preprint</p>
<p>Benchmarking large language models for news summarization. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen Mckeown, Tatsunori B Hashimoto, Transactions of the Association for Computational Linguistics. 122024b</p>            </div>
        </div>

    </div>
</body>
</html>
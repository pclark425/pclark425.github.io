<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4191 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4191</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4191</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-266551695</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.15784v1.pdf" target="_blank">AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining</a></p>
                <p><strong>Paper Abstract:</strong> . In an era marked by a rapid increase in scientific publications, researchers grapple with the challenge of keeping pace with field-specific advances. We present the ‘AHAM’ methodology and a metric that guides the domain-specific adapt ation of the BERTopic topic modeling framework to improve scientific text analysis. By utilizing the LLaMa2 generative language model, we generate topic definitions via one-shot learning by crafting prompts with the help of domain experts to guide the LLM for literature mining by asking it to model the topic names. For inter-topic similarity evaluation, we leverage metrics from language generation and translation processes to assess lexical and semantic similarity of the generated topics. Our system aims to reduce both the ratio of outlier topics to the total number of topics and the similarity between topic definitions. The methodology has been assessed on a newly gathered corpus of scientific papers on literature-based discovery. Through rigorous evaluation by domain experts, AHAM has been validated as effective in uncovering intriguing and novel insights within broad research areas. We explore the impact of domain adaptation of sentence-transformers for the task of topic model ing using two datasets, each specialized to specific scientific domains within arXiv and medarxiv. We evaluate the impact of data size, the niche of adaptation, and the importance of domain adaptation. Our results suggest a strong interaction between domain adaptation and topic modeling precision in terms of outliers and topic definitions.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4191.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4191.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMa2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMa 2 (Llama-2-13b-chat-hf variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative causal large language model used in this work for one-shot in-context labeling and semantic categorization of documents and topic clusters; prompted to produce concise topic names from cluster keywords and representative documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLaMa2-based topic labeling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The authors used the Llama-2-13b-chat-hf model in a one-shot in-context prompting setup to produce short, domain-specific topic labels. Prompt engineering included a system prompt (persona as research assistant), a one-shot example prompt showing documents + keywords -> desired short label, and a query prompt that supplied cluster documents and keywords and requested a single concise computer-science label. Model settings: temperature=0, context window=500 tokens, repetition_penalty=1.1. Outputs were used as the canonical topic names subsequently compared with other topic names using lexical and semantic similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Literature-based discovery / meta-literature analysis; experiments applied to general arXiv (general science) and medarxiv (biomedical) corpora</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used to categorize a curated LBD corpus of 389 papers; applied for topic naming on topic clusters derived from arXiv (25,000 entries) and medarxiv (8,500 entries) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Semantic labeling via in-context generation from cluster-level keywords and representative documents (text-only prompting); did not parse equations or numeric tables — used to distill thematic patterns/topics rather than explicit quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Human expert evaluation of topic label quality; numerical assessment via inter-topic similarity metrics (Levenshtein fuzzy match, BERTScore, cosine on sentence-transformer embeddings) and inclusion in AHAM objective to select adaptation step.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics relate to topic-label similarity and clustering: Levenshtein, BERTscore, cosine similarity values reported per adaptation step (e.g., AHAM-selected arXiv model AHAM score 0.26 with 88.4% reduction in outliers). No metrics reported for extracting quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM used only for semantic labeling (topic names) rather than extracting formal quantitative relationships; potential sensitivity to prompt design and the single-shot example; labeling quality judged by experts but no automated ground-truth for labels; does not extract numeric laws or equations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used in the pipeline alongside baseline (no domain adaptation) and domain-adapted sentence-transformer pipelines; LLaMa2 labeling was part of the evaluated topic-modeling outputs compared via AHAM but not directly compared to alternative label-generation models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4191.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4191.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPL (T5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pseudo-Labeling using T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative pseudo-labeling pipeline that uses a T5 generative model to synthesize queries from passages to create triplets for unsupervised domain adaptation of sentence-transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative Pseudo-Labeling (GPL) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The GPL pipeline employed a T5 generative model to produce synthetic query sentences Q_synth from target-domain passages (up to q queries per passage). For each synthetic query, positive and negative passages were mined using a pre-trained sentence-embedding model (negative sampling via nucleus sampling for diversity). Triplets (query, positive, negative) were scored with a cross-encoder (CE) to produce a delta score δ(ti)=CE(q,p+)−CE(q,p−). High-scoring triplets formed supervised-like data used to fine-tune the sentence-transformer with a MarginMSE loss. The adapted sentence-transformer embeddings were then used in downstream UMAP → HDBSCAN → BERTopic clustering/keyword extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (generative model used for query synthesis); cross-encoder (CE) used for scoring</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied on arXiv (general science) and medarxiv (biomedical) corpora for topic modeling and literature-based discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>GPL adaptation performed over target corpora sizes: arXiv 25,000 entries and medarxiv 8,500 entries (GPL evaluated at checkpoints up to 50k training steps; AHAM evaluated every 10k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Generates synthetic natural-language queries from passages to create training triplets for sentence-embedding adaptation; used for improving semantic clustering and hence discovery of thematic/associative patterns across literature (text mining for patterns, not numerical law extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Quality of the adapted embeddings assessed by downstream topic modeling quality via AHAM objective (counts of outliers, inter-topic similarity metrics) and expert inspection of resulting clusters and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported downstream improvements: reductions in outlier counts (e.g., arXiv outliers reduced from 43 to 29 at certain steps; medarxiv reductions reported up to 72% in outliers). AHAM objective values reported (e.g., 0.26 best for arXiv broad-domain adaptation). No direct metrics for quantitative-law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Non-convex AHAM trajectory makes adaptation step selection delicate; GPL requires careful negative sampling and good cross-encoder scoring; synthetic queries may introduce noise or domain mismatch; adaptation may specialize too early on niche domains reducing broader generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared adapted sentence-transformer + BERTopic pipeline against baseline (no adaptation) and across two domain adaptation regimes (broad arXiv vs niche medarxiv); adaptation yielded fewer outliers and more focused topics relative to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4191.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4191.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTopic pipeline (sentence-transformer → UMAP → HDBSCAN → KeyBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTopic-based topic modeling pipeline with domain-adapted sentence-transformers and KeyBERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A topic-modeling pipeline that embeds documents with sentence-transformers (optionally domain-adapted via GPL), reduces dimensionality with UMAP, clusters with HDBSCAN, extracts cluster-level keywords with KeyBERT, and uses LLMs for label generation; used to discover thematic patterns in scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERTopic + domain-adapted sentence-transformer pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline steps: (1) Embed concatenated title+abstract with sentence-transformers (two specialized versions fine-tuned by the authors for arXiv and medarxiv or baseline pre-trained models). (2) Dimensionality reduction with UMAP (n_neighbors=5, n_components=5, min_dist=0.0, cosine metric). (3) Density-based clustering with HDBSCAN (min_cluster_size=5, Euclidean metric, 'eom' selection). (4) Create cluster-level bag-of-words and extract keywords per cluster using KeyBERT (max 10 keywords). (5) Use LLaMa2 for concise topic naming from cluster keywords/documents. AHAM objective (combining number of outliers, number of clusters, and topic-name similarity metrics) is used to select the best domain-adaptation checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom fine-tuned sentence-transformers (two specialized models introduced in this work); BERTopic framework; KeyBERT; UMAP; HDBSCAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Literature-based discovery; experiments on arXiv (general science) and medarxiv (biomedical) corpora</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to a curated LBD corpus of 389 papers for meta-analysis; large-scale adaptation experiments used arXiv (25,000) and medarxiv (8,500) corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text-mining-based pattern discovery: semantic embedding + clustering to uncover themes, cluster-level keyword extraction to represent topics, and LLM-assisted naming to distill thematic patterns; does not parse numeric equations or explicitly extract quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Quantitative evaluation via the AHAM heuristic (combining outlier ratio and inter-topic similarity measures: Levenshtein fuzzy matching, BERTScore, semantic cosine on embeddings) and qualitative human expert inspection of clusters and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: counts of topics (#T) and outliers (#O), outlier-to-topic ratios, Levenshtein/BERTScore/Cosine similarity statistics across adaptation steps; examples: AHAM-selected arXiv broad adaptation: AHAM score 0.26 with 88.4% reduction in outliers; medarxiv local optimum AHAM 1.09 with 72% reduction in outliers. No measures reported for extracting numeric laws.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Method uncovers thematic patterns rather than formal quantitative relationships; AHAM objective non-convex and may have multiple local optima; domain adaptation may over-specialize or require careful selection of source domain; dependence on clustering hyperparameters (UMAP/HDBSCAN) and the quality of synthetic pseudo-labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared baseline (no adaptation) vs arXiv-adapted vs medarxiv-adapted sentence-transformers; domain adaptation reduced outliers and produced more coherent/focused topics per AHAM and expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4191.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4191.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AHAM heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AHAM (Adapt, Help, Ask, Model) objective / heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic objective introduced in this paper that combines outlier ratio, number of clusters, and pairwise topic-name similarity to evaluate and select domain-adaptation checkpoints for improved topic modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AHAM objective for selecting domain-adaptation steps</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AHAM objective = 2 * |k_outliers| / |k| * (1 / (k(k-1))) * sum_{i<j} TopicSimilarity(k_i, k_j). It jointly combines the number of outliers, number of clusters, and average pairwise topic-name similarity (TopicSimilarity computed via Levenshtein fuzzy match, BERTScore, and semantic cosine using a sentence-transformer). The authors evaluate AHAM every 10k GPL steps and select the adaptation checkpoint with the lowest AHAM score as yielding the best topic-modeling outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Topic modeling / literature mining for literature-based discovery across general and biomedical scientific corpora</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>AHAM used to evaluate topic-modeling outputs from experiments on arXiv (25k) and medarxiv (8.5k) datasets and the curated LBD corpus (389 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Not an extractor of quantitative laws; a model-selection metric that quantifies topic-model output quality to guide domain-adaptation steps, thereby helping the pipeline better reveal thematic relationships and patterns in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Empirical evaluation over adaptation steps with tracked outlier counts, topic counts, and inter-topic similarity metrics; qualitative expert evaluation of selected best models and clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AHAM scores reported across checkpoints; example reported AHAM minima and associated reductions in outliers (e.g., arXiv AHAM 0.26 leading to 88.4% outlier reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Objective is non-convex and can show divergence-then-convergence behavior across adaptation steps; AHAM focuses on lexical/semantic similarity of labels and outlier counts, which are proxy measures and may not fully capture topical quality or discovery of substantive new relationships; not designed to extract formal quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used to compare baseline (no adaptation) and multiple adaptation checkpoints; selected checkpoints reported to improve topic coherence and reduce outliers compared to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4191.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4191.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned sentence-transformers (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two specialized sentence-transformers fine-tuned for arXiv (general science) and medarxiv (biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sentence-transformer models fine-tuned via GPL/MarginMSE on domain corpora to produce embeddings better suited for topic clustering and literature mining in their respective domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Domain-adapted sentence-transformers</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors fine-tuned two sentence-transformer models using generative pseudo-labeling (T5-generated queries, CE scoring, MarginMSE loss) to adapt to broad arXiv and niche medarxiv domains. The adapted embeddings were projected with UMAP and clustered with HDBSCAN in the BERTopic pipeline to improve topic differentiation and reduce outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom fine-tuned sentence-transformers (trained in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General science (arXiv) and biomedical (medarxiv) literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Fine-tuned on arXiv (25,000 entries) and medarxiv (8,500 entries); applied to curated LBD corpus (389 papers) for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Improved semantic embedding to facilitate clustering and keyword extraction, thereby surfacing thematic patterns and inter-document relationships; not used to extract explicit quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Downstream evaluation using AHAM objective (outlier reduction and topic-name similarity) and expert qualitative assessment of clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported reductions in outlier counts and topic counts at selected adaptation steps (examples: arXiv outliers reduced from 43 to 29 at step 20k; medarxiv saw reductions down to 10 outliers and other improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Domain adaptation dynamics depend on size/granularity of adaptation corpus; risk of over-specialization for niche datasets; requires selection of appropriate source/target domains and careful monitoring (AHAM) to avoid suboptimal checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against baseline (non-adapted) sentence-transformer embeddings; adapted models produced fewer outliers and more focused topics according to AHAM and expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval <em>(Rating: 2)</em></li>
                <li>Llama 2: Open foundation and fine-tuned chat models <em>(Rating: 2)</em></li>
                <li>Learning to generate novel scientific directions with contextualized literature-based discovery <em>(Rating: 2)</em></li>
                <li>Making monolingual sentence embeddings multilingual using knowledge distillation <em>(Rating: 1)</em></li>
                <li>Can language models learn from explanations in context? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4191",
    "paper_id": "paper-266551695",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "LLaMa2",
            "name_full": "LLaMa 2 (Llama-2-13b-chat-hf variant)",
            "brief_description": "A generative causal large language model used in this work for one-shot in-context labeling and semantic categorization of documents and topic clusters; prompted to produce concise topic names from cluster keywords and representative documents.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLaMa2-based topic labeling",
            "system_description": "The authors used the Llama-2-13b-chat-hf model in a one-shot in-context prompting setup to produce short, domain-specific topic labels. Prompt engineering included a system prompt (persona as research assistant), a one-shot example prompt showing documents + keywords -&gt; desired short label, and a query prompt that supplied cluster documents and keywords and requested a single concise computer-science label. Model settings: temperature=0, context window=500 tokens, repetition_penalty=1.1. Outputs were used as the canonical topic names subsequently compared with other topic names using lexical and semantic similarity metrics.",
            "model_name": "Llama-2-13b-chat-hf",
            "model_size": "13B",
            "scientific_domain": "Literature-based discovery / meta-literature analysis; experiments applied to general arXiv (general science) and medarxiv (biomedical) corpora",
            "number_of_papers": "Used to categorize a curated LBD corpus of 389 papers; applied for topic naming on topic clusters derived from arXiv (25,000 entries) and medarxiv (8,500 entries) in experiments",
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Semantic labeling via in-context generation from cluster-level keywords and representative documents (text-only prompting); did not parse equations or numeric tables — used to distill thematic patterns/topics rather than explicit quantitative laws.",
            "validation_approach": "Human expert evaluation of topic label quality; numerical assessment via inter-topic similarity metrics (Levenshtein fuzzy match, BERTScore, cosine on sentence-transformer embeddings) and inclusion in AHAM objective to select adaptation step.",
            "performance_metrics": "Reported metrics relate to topic-label similarity and clustering: Levenshtein, BERTscore, cosine similarity values reported per adaptation step (e.g., AHAM-selected arXiv model AHAM score 0.26 with 88.4% reduction in outliers). No metrics reported for extracting quantitative laws.",
            "success_rate": null,
            "challenges_limitations": "LLM used only for semantic labeling (topic names) rather than extracting formal quantitative relationships; potential sensitivity to prompt design and the single-shot example; labeling quality judged by experts but no automated ground-truth for labels; does not extract numeric laws or equations.",
            "comparison_baseline": "Used in the pipeline alongside baseline (no domain adaptation) and domain-adapted sentence-transformer pipelines; LLaMa2 labeling was part of the evaluated topic-modeling outputs compared via AHAM but not directly compared to alternative label-generation models.",
            "uuid": "e4191.0",
            "source_info": {
                "paper_title": "AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPL (T5)",
            "name_full": "Generative Pseudo-Labeling using T5",
            "brief_description": "A generative pseudo-labeling pipeline that uses a T5 generative model to synthesize queries from passages to create triplets for unsupervised domain adaptation of sentence-transformers.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Generative Pseudo-Labeling (GPL) pipeline",
            "system_description": "The GPL pipeline employed a T5 generative model to produce synthetic query sentences Q_synth from target-domain passages (up to q queries per passage). For each synthetic query, positive and negative passages were mined using a pre-trained sentence-embedding model (negative sampling via nucleus sampling for diversity). Triplets (query, positive, negative) were scored with a cross-encoder (CE) to produce a delta score δ(ti)=CE(q,p+)−CE(q,p−). High-scoring triplets formed supervised-like data used to fine-tune the sentence-transformer with a MarginMSE loss. The adapted sentence-transformer embeddings were then used in downstream UMAP → HDBSCAN → BERTopic clustering/keyword extraction.",
            "model_name": "T5 (generative model used for query synthesis); cross-encoder (CE) used for scoring",
            "model_size": null,
            "scientific_domain": "Applied on arXiv (general science) and medarxiv (biomedical) corpora for topic modeling and literature-based discovery tasks",
            "number_of_papers": "GPL adaptation performed over target corpora sizes: arXiv 25,000 entries and medarxiv 8,500 entries (GPL evaluated at checkpoints up to 50k training steps; AHAM evaluated every 10k steps).",
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Generates synthetic natural-language queries from passages to create training triplets for sentence-embedding adaptation; used for improving semantic clustering and hence discovery of thematic/associative patterns across literature (text mining for patterns, not numerical law extraction).",
            "validation_approach": "Quality of the adapted embeddings assessed by downstream topic modeling quality via AHAM objective (counts of outliers, inter-topic similarity metrics) and expert inspection of resulting clusters and labels.",
            "performance_metrics": "Reported downstream improvements: reductions in outlier counts (e.g., arXiv outliers reduced from 43 to 29 at certain steps; medarxiv reductions reported up to 72% in outliers). AHAM objective values reported (e.g., 0.26 best for arXiv broad-domain adaptation). No direct metrics for quantitative-law extraction.",
            "success_rate": null,
            "challenges_limitations": "Non-convex AHAM trajectory makes adaptation step selection delicate; GPL requires careful negative sampling and good cross-encoder scoring; synthetic queries may introduce noise or domain mismatch; adaptation may specialize too early on niche domains reducing broader generalization.",
            "comparison_baseline": "Compared adapted sentence-transformer + BERTopic pipeline against baseline (no adaptation) and across two domain adaptation regimes (broad arXiv vs niche medarxiv); adaptation yielded fewer outliers and more focused topics relative to baseline.",
            "uuid": "e4191.1",
            "source_info": {
                "paper_title": "AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "BERTopic pipeline (sentence-transformer → UMAP → HDBSCAN → KeyBERT)",
            "name_full": "BERTopic-based topic modeling pipeline with domain-adapted sentence-transformers and KeyBERT",
            "brief_description": "A topic-modeling pipeline that embeds documents with sentence-transformers (optionally domain-adapted via GPL), reduces dimensionality with UMAP, clusters with HDBSCAN, extracts cluster-level keywords with KeyBERT, and uses LLMs for label generation; used to discover thematic patterns in scientific literature.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "BERTopic + domain-adapted sentence-transformer pipeline",
            "system_description": "Pipeline steps: (1) Embed concatenated title+abstract with sentence-transformers (two specialized versions fine-tuned by the authors for arXiv and medarxiv or baseline pre-trained models). (2) Dimensionality reduction with UMAP (n_neighbors=5, n_components=5, min_dist=0.0, cosine metric). (3) Density-based clustering with HDBSCAN (min_cluster_size=5, Euclidean metric, 'eom' selection). (4) Create cluster-level bag-of-words and extract keywords per cluster using KeyBERT (max 10 keywords). (5) Use LLaMa2 for concise topic naming from cluster keywords/documents. AHAM objective (combining number of outliers, number of clusters, and topic-name similarity metrics) is used to select the best domain-adaptation checkpoint.",
            "model_name": "Custom fine-tuned sentence-transformers (two specialized models introduced in this work); BERTopic framework; KeyBERT; UMAP; HDBSCAN",
            "model_size": null,
            "scientific_domain": "Literature-based discovery; experiments on arXiv (general science) and medarxiv (biomedical) corpora",
            "number_of_papers": "Applied to a curated LBD corpus of 389 papers for meta-analysis; large-scale adaptation experiments used arXiv (25,000) and medarxiv (8,500) corpora.",
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Text-mining-based pattern discovery: semantic embedding + clustering to uncover themes, cluster-level keyword extraction to represent topics, and LLM-assisted naming to distill thematic patterns; does not parse numeric equations or explicitly extract quantitative laws.",
            "validation_approach": "Quantitative evaluation via the AHAM heuristic (combining outlier ratio and inter-topic similarity measures: Levenshtein fuzzy matching, BERTScore, semantic cosine on embeddings) and qualitative human expert inspection of clusters and labels.",
            "performance_metrics": "Reported metrics: counts of topics (#T) and outliers (#O), outlier-to-topic ratios, Levenshtein/BERTScore/Cosine similarity statistics across adaptation steps; examples: AHAM-selected arXiv broad adaptation: AHAM score 0.26 with 88.4% reduction in outliers; medarxiv local optimum AHAM 1.09 with 72% reduction in outliers. No measures reported for extracting numeric laws.",
            "success_rate": null,
            "challenges_limitations": "Method uncovers thematic patterns rather than formal quantitative relationships; AHAM objective non-convex and may have multiple local optima; domain adaptation may over-specialize or require careful selection of source domain; dependence on clustering hyperparameters (UMAP/HDBSCAN) and the quality of synthetic pseudo-labeling.",
            "comparison_baseline": "Compared baseline (no adaptation) vs arXiv-adapted vs medarxiv-adapted sentence-transformers; domain adaptation reduced outliers and produced more coherent/focused topics per AHAM and expert review.",
            "uuid": "e4191.2",
            "source_info": {
                "paper_title": "AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AHAM heuristic",
            "name_full": "AHAM (Adapt, Help, Ask, Model) objective / heuristic",
            "brief_description": "A heuristic objective introduced in this paper that combines outlier ratio, number of clusters, and pairwise topic-name similarity to evaluate and select domain-adaptation checkpoints for improved topic modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AHAM objective for selecting domain-adaptation steps",
            "system_description": "AHAM objective = 2 * |k_outliers| / |k| * (1 / (k(k-1))) * sum_{i&lt;j} TopicSimilarity(k_i, k_j). It jointly combines the number of outliers, number of clusters, and average pairwise topic-name similarity (TopicSimilarity computed via Levenshtein fuzzy match, BERTScore, and semantic cosine using a sentence-transformer). The authors evaluate AHAM every 10k GPL steps and select the adaptation checkpoint with the lowest AHAM score as yielding the best topic-modeling outcome.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Topic modeling / literature mining for literature-based discovery across general and biomedical scientific corpora",
            "number_of_papers": "AHAM used to evaluate topic-modeling outputs from experiments on arXiv (25k) and medarxiv (8.5k) datasets and the curated LBD corpus (389 papers).",
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Not an extractor of quantitative laws; a model-selection metric that quantifies topic-model output quality to guide domain-adaptation steps, thereby helping the pipeline better reveal thematic relationships and patterns in literature.",
            "validation_approach": "Empirical evaluation over adaptation steps with tracked outlier counts, topic counts, and inter-topic similarity metrics; qualitative expert evaluation of selected best models and clusters.",
            "performance_metrics": "AHAM scores reported across checkpoints; example reported AHAM minima and associated reductions in outliers (e.g., arXiv AHAM 0.26 leading to 88.4% outlier reduction).",
            "success_rate": null,
            "challenges_limitations": "Objective is non-convex and can show divergence-then-convergence behavior across adaptation steps; AHAM focuses on lexical/semantic similarity of labels and outlier counts, which are proxy measures and may not fully capture topical quality or discovery of substantive new relationships; not designed to extract formal quantitative laws.",
            "comparison_baseline": "Used to compare baseline (no adaptation) and multiple adaptation checkpoints; selected checkpoints reported to improve topic coherence and reduce outliers compared to baseline.",
            "uuid": "e4191.3",
            "source_info": {
                "paper_title": "AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Fine-tuned sentence-transformers (this work)",
            "name_full": "Two specialized sentence-transformers fine-tuned for arXiv (general science) and medarxiv (biomedical)",
            "brief_description": "Sentence-transformer models fine-tuned via GPL/MarginMSE on domain corpora to produce embeddings better suited for topic clustering and literature mining in their respective domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Domain-adapted sentence-transformers",
            "system_description": "Authors fine-tuned two sentence-transformer models using generative pseudo-labeling (T5-generated queries, CE scoring, MarginMSE loss) to adapt to broad arXiv and niche medarxiv domains. The adapted embeddings were projected with UMAP and clustered with HDBSCAN in the BERTopic pipeline to improve topic differentiation and reduce outliers.",
            "model_name": "Custom fine-tuned sentence-transformers (trained in this work)",
            "model_size": null,
            "scientific_domain": "General science (arXiv) and biomedical (medarxiv) literature",
            "number_of_papers": "Fine-tuned on arXiv (25,000 entries) and medarxiv (8,500 entries); applied to curated LBD corpus (389 papers) for meta-analysis.",
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Improved semantic embedding to facilitate clustering and keyword extraction, thereby surfacing thematic patterns and inter-document relationships; not used to extract explicit quantitative laws.",
            "validation_approach": "Downstream evaluation using AHAM objective (outlier reduction and topic-name similarity) and expert qualitative assessment of clusters.",
            "performance_metrics": "Reported reductions in outlier counts and topic counts at selected adaptation steps (examples: arXiv outliers reduced from 43 to 29 at step 20k; medarxiv saw reductions down to 10 outliers and other improvements).",
            "success_rate": null,
            "challenges_limitations": "Domain adaptation dynamics depend on size/granularity of adaptation corpus; risk of over-specialization for niche datasets; requires selection of appropriate source/target domains and careful monitoring (AHAM) to avoid suboptimal checkpoints.",
            "comparison_baseline": "Compared against baseline (non-adapted) sentence-transformer embeddings; adapted models produced fewer outliers and more focused topics according to AHAM and expert review.",
            "uuid": "e4191.4",
            "source_info": {
                "paper_title": "AHAM: Adapt, Help, Ask, Model - Harvesting LLMs for literature mining",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval",
            "rating": 2,
            "sanitized_title": "gpl_generative_pseudo_labeling_for_unsupervised_domain_adaptation_of_dense_retrieval"
        },
        {
            "paper_title": "Llama 2: Open foundation and fine-tuned chat models",
            "rating": 2,
            "sanitized_title": "llama_2_open_foundation_and_finetuned_chat_models"
        },
        {
            "paper_title": "Learning to generate novel scientific directions with contextualized literature-based discovery",
            "rating": 2,
            "sanitized_title": "learning_to_generate_novel_scientific_directions_with_contextualized_literaturebased_discovery"
        },
        {
            "paper_title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "rating": 1,
            "sanitized_title": "making_monolingual_sentence_embeddings_multilingual_using_knowledge_distillation"
        },
        {
            "paper_title": "Can language models learn from explanations in context?",
            "rating": 1,
            "sanitized_title": "can_language_models_learn_from_explanations_in_context"
        }
    ],
    "cost": 0.0130955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AHAM: Adapt, Help, Ask, Model -Harvesting LLMs for literature mining
25 Dec 2023</p>
<p>Boshko Koloski boshko.koloski@ijs.si 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>International Postgraduate School Jožef Stefan
LjubljanaSlovenia</p>
<p>Nada Lavrač 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>University of Nova Gorica
VipavaSlovenia</p>
<p>Bojan Cestnik 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>Temida d.o.o
LjubljanaSlovenia</p>
<p>Senja Pollak 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>Blaž Škrlj 
Jožef Stefan Institute
LjubljanaSlovenia</p>
<p>Andrej Kastrin andrej.kastrin@mf.uni-lj.si 
Institute for Biostatistics and Medical Informatics
University of Ljubljana
LjubljanaSlovenia</p>
<p>AHAM: Adapt, Help, Ask, Model -Harvesting LLMs for literature mining
25 Dec 20236F009DFA322C70804436DDA70D691BFEarXiv:2312.15784v1[cs.CL]topic modelingdomain adaptationsentence-transformersliterature-based discovery
In an era marked by a rapid increase in scientific publications, researchers grapple with the challenge of keeping pace with field-specific advances.We present the 'AHAM' methodology and a metric that guides the domain-specific adaptation of the BERTopic topic modeling framework to improve scientific text analysis.By utilizing the LLaMa2 generative language model, we generate topic definitions via one-shot learning by crafting prompts with the help of domain experts to guide the LLM for literature mining by asking it to model the topic names.For inter-topic similarity evaluation, we leverage metrics from language generation and translation processes to assess lexical and semantic similarity of the generated topics.Our system aims to reduce both the ratio of outlier topics to the total number of topics and the similarity between topic definitions.The methodology has been assessed on a newly gathered corpus of scientific papers on literature-based discovery.Through rigorous evaluation by domain experts, AHAM has been validated as effective in uncovering intriguing and novel insights within broad research areas.We explore the impact of domain adaptation of sentence-transformers for the task of topic modeling using two datasets, each specialized to specific scientific domains within arXiv and medarxiv.We evaluate the impact of data size, the niche of adaptation, and the importance of domain adaptation.Our results suggest a strong interaction between domain adaptation and topic modeling precision in terms of outliers and topic definitions.</p>
<p>Introduction</p>
<p>The large number of publications that appear every day makes it almost impossible for researchers to keep up with the latest findings, even within their narrow scientific field.In a flood of information, researchers can overlook valuable segments of knowledge.Text mining is an effective technology that not only supports the analysis of existing knowledge but also allows researchers to infer new knowledge facts based on information hidden in the data.Common text-mining tasks include information extraction from literature, document summarization, question-answering, and topic modeling, to name just a few.Topic modeling is a text-mining technique that enables researchers to explore the thematic landscape of a collection of documents from a high-level perspective [24].Although topic modeling is an established research field, the recent advent of Large Language Models (LLMs) has led to a qualitative leap in their development.</p>
<p>This paper presents methodological advancements in domain adaptation for topic modeling by utilizing state-of-the-art language models.The approach is demonstrated in a corpus of scientific papers from literature-based discovery (LBD), a research field that our team has been working on for the last two decades [9,6].The specific contributions of this paper are as follows:</p>
<p>-AHAM -a topic modeling objective function that evaluates the quality of topic modeling by measuring, on the one side, the semantic and lexical similarity in generated topic names, and outliers on the other side, while adapting to a new domain.-Two specialized sentence transformers fine-tuned for distinct arXiv domains: one for general science and another for biomedical science.-A qualitative and quantitative assessment of domain-adaptation effects on topic modeling for literature-based discovery (LBD).</p>
<p>The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the data used, Section 4 outlines the proposed methodology, Section 5 explains the research questions and presents the results, and Section 6 concludes with a qualitative and quantitative summary and discusses the further work.</p>
<p>Related work</p>
<p>The field of natural language processing has witnessed a remarkable transformation with the advent of Large Language Models (LLMs), which can be divided into two groups: Masked Language Models (MLMs) like BERT [1] and generative Causal Language Models (CLMs) such as LLaMa2 [23].These foundational models have set new standards in understanding and generating text with human-level precision [12].Building upon this groundwork, sentence transformers [15] have emerged as a specialized evolution.These models, tailored for the task of learning sentence representations, ensure that semantically similar sentences are closely aligned in the vector space.One notable application of sentence transformers is BERTopic [4], which has revolutionized topic modeling with its unique approach.BERTopic clusters sentences based on semantic similarity, providing a refined and context-sensitive thematic analysis that outperforms conventional methods.In a similar vein, KeyBERT [3] advances the field of keyword extraction.Utilizing sentence-transformer technologies, it effectively extracts key terms and phrases from extensive texts [19,7].A pivotal area of research in the use of these models is domain adaptation.Wang et al. [25] proposed an approach for unsupervised domain adaptation, employing sequential denoising auto-encoders to learn from corrupted data.Another approach to domain adaptation involves generative pseudo-labeling (GPL) [26], where researchers use a surrogate generative model, such as T5 trained to generate queries for specific passages [21].These queries are then ranked by a cross-encoder [16] and used as downstream fine-tuning data for the sentence transformer.The development of prompting techniques in LLMs [28], particularly in-context one-shot learning [8], represents a significant stride in model interaction.This approach involves crafting specific prompts that enable models to learn from a single example within the prompt context, thereby generating more relevant and contextually nuanced responses.This technique is crucial in eliciting accurate and specific outputs from models like LLaMa2 [23], demonstrating an advanced level of understanding and flexibility in language generation [14].</p>
<p>Literature-based discovery (LBD) is a vibrant area of research, with the first approach reported in the mid-1980s.Swanson [20] observed, by reading separate literature on fish oil and Raynaud's disease, that some knowledge concepts are common between both document sets.This serendipitous discovery led Swanson to propose that fish oil may be used in the treatment of Raynaud's disease.Swanson's hypothesis was later clinically confirmed.</p>
<p>LBD postulates that knowledge in one domain may be related to knowledge in another domain, but without the relationship being explicit.The general idea of LBD can be operationalized using three knowledge concepts, namely A, B, and C. If concept B is associated with concept A in the first document set, and concept B is associated with concept C in a disjoint document set, we may hypothesize a transitive relationship between concepts A and C through concept B, which is common in both literature sets.</p>
<p>Over the past four decades, drawing inspiration from the pioneering work of Swanson, numerous approaches to LBD have been developed.Researchers have proposed several different approaches, ranging from basic latent semantic indexing [2] to techniques based on knowledge graphs [17] and state-of-the-art large language models [27].An in-depth overview of LBD approaches can be found in recent studies [18,22].</p>
<p>LBD corpus and initial data analysis</p>
<p>We have compiled a comprehensive corpus of LBD publications by merging lists of representative publications on LBD that were manually prepared by the authors of two recent surveys of the LBD field [22,6].To ensure that the latest advances are included, we also added 11 papers published in the last two years that were not included in the previous surveys.Our corpus comprises 389 publications spanning from 1986 to 2023.We concatenated the title and abstract fields to eliminate empty features because six papers did not contain an abstract.The publication frequency of these articles peaked in 2015, with variations observed over the years.We applied the LLama2 model to semantically categorize these articles into two groups: those proposing 'methodology' and those applying already developed methodologies 'application'.We utilize the one-shot learning capability of the LLama2 model.Our goal in this pseudo-categorization was to trace the evolution of the field, assuming that the 'methodology' articles are those that introduce new techniques, while the 'application' articles are likely to focus on the use or refinement of existing methods.We report the results in Figure 1.The field of LBD started in 1986 with a paper reporting Swanson's [20] discovery of a link between fish oil and Raynaud disease, here categorized as a 'methodology' paper.In the early years, the literature mostly explored methodological issues.Over time, the focus of work expanded to include both 'methodology' and 'application', with the latter gaining traction from 1999 onwards.A notable surge in 'methodology' articles occurred around 2008, with a peak in 2015, followed by some fluctuation.On the other hand, 'application' articles saw a steady increase, underscoring the sustained interest and necessity for LBD research.By 2023, the production of both 'methodology' and 'application' articles appears to have stabilized, suggesting a balanced advancement in these research domains.</p>
<p>Methodology</p>
<p>In this section, we outline the proposed methodology, which builds on the versatility of the BERTopic framework and introduce the AHAM heuristic.</p>
<p>Corpus vectorization with Sentence Transformers</p>
<p>Sentence transformers [15] optimize a loss function that measures the discrepancy between predicted and actual outcomes.A commonly used loss function for these transformers is either contrastive loss or triplet loss.Given a query sentence q, a positive sentence p + (semantically similar to q), and a negative sentence p − (semantically different from q), we define the loss function as: L(q, p, n) = max(0, f (q, p + ) − f (q, p − ) + margin).Here, f (q, b) measures the similarity between sentences q and b, often using cosine similarity, and the margin is a hyperparameter that establishes the minimum distance between the positive and negative pairs.</p>
<p>Domain Adaptation via Generative Pseudo Labeling</p>
<p>Given a sentence transformer embedding S trained on a source domain data D s and target domain data D t , the goal is to adapt the sentence transformer embedding S to the target domain data.One approach to do this is via generative pseudo-labeling (GPL) [26].The GPL adaptation involves multiple steps: Query Generation with T5: To enable supervised training, first a generative model (e.g., T5), is employed to generate synthetic query sentences Q synth from the passages of the D t , Q synth = T5(D s ).We generate up to q queries per passage.Mining Negative Passages: For each generated query q i in Q synth , negative passages P neg are mined using a pre-trained sentence embedding model.The sentences are mined via nucleus sampling to ensure diversity.In this manner, for each query q i , the positive sample passage p + i and the negative sample p − i form a train tiplets t = (q i , p − i , p + i ).Scoring with Cross Encoder: Next for each query triplet, a cross encoder CE is utilized to score pairs of q i , p + i and q i , p − i as:
δ (ti) = CE(q i , p + i )) − CE(q i , p − i ).
In this manner, we obtain a synthetic dataset adapted to the D t for fine-tuning the sentence-transformer S. Finally, the model S is fitted to the new domain via MarginMSE loss-function [5].The final step of the vectorization phase projects the adapted sentence-transformer space into a lower dimension using UMAP [11] for dimensionality reduction.This step aims to make the clustering methods more effective, which otherwise struggle with high dimensional spaces.We configured the UMAP model with 5 nearest neighbors and 5 components, set the minimum distance to 0.0, used cosine similarity as the metric, and set a random state of 42 for reproducibility.In the subsequent phase of the BERTopic framework, we focus on identifying document clusters for later topic mining.We used an HDBSCAN [10] model, set the minimum cluster size to 5, employed the Euclidean metric for distance calculation, chose the 'eom' method for cluster selection, and enabled prediction data.Next, BERTopic calculates frequencies at the corpus level and creates a bag-of-words representation at the cluster level instead of for individual documents.This method highlights the importance of words at the topic level (i.e. the cluster level) and uses L1-normalization to accommodate clusters of different sizes, thus avoiding assumptions about the structural composition of the clusters.Finally, for each cluster, we employed the KeyBERT keyword extractor, which is based on the S sentence-transformer representation, and subsequently ranks and retrieves the keywords.We retrieved a maximum of 10 relevant keywords per article and then ranked them.</p>
<p>In the final stage of the processing pipeline for a set of N documents, we obtain k distinct clusters.Among these, one unique cluster is labeled as k outliers , which represents the outliers.These outliers are the documents that did not fit into any of the other clusters.</p>
<p>Prompt Engineering of LLMs to Design Topic Names</p>
<p>Prompt engineering.We utilize the Llama2 language model, specifically the Llama-2-13b-chat-hf variant from the HuggingFace6 repository, setting the temperature to 0, the context window to 500 tokens, and the repetition penalty to 1.1.The objective of this phase is to semantically utilize the extracted keywords and the most central documents in each topical cluster.We aim to leverage the LLM's capabilities to derive meaningful semantic labels.This is achieved through training via one-example in-context learning, guided by prompt designs crafted by domain experts of the meta-literature analysis application of our interest.Following related work, we have engineered a three-level prompt structure:</p>
<p>-System prompt, to give personality to the LLM for labeling topics as a domain expert:</p>
<p>You are a helpful, respectful, and honest research assistant for labeling topics.</p>
<p>-One shot, example prompt, from which we help the model with guidance for in-context learning.Notice that this prompt has been crafted in a specific manner, reflecting our contributions to the field, but it could have been crafted differently by the end user.</p>
<p>I have a topic that contains the following documents:</p>
<p>-Bisociative Knowledge Discovery by Literature Outlier Detection.</p>
<p>-Evaluating Outliers for Cross-Context Link Discovery.</p>
<p>-Exploring the Power of Outliers for Cross-Domain Literature Mining.The topic is described by the following keywords: bisociative, knowledge discovery, outlier detection, ,data mining, cross-context, link discovery, cross-domain, machine learning'.</p>
<p>Based on the information about the topic above, please create a simple, short, and concise computer science label for this topic.Make sure you only return the label and nothing more.</p>
<p>[INST]: Outlier-based knowledge discovery -The query prompt, which the model uses to label topics and keywords of the most central documents for a given query, is personalized.This personalization ensures that the model returns computer-science topics from the field of Literature-Based Discovery without explicitly mentioning them:</p>
<p>I have a topic that contains the following documents [DOCUMENTS] The topic is described by the following keywords: [KEYWORDS] Based on the information about the topic above, please create a simple, short and concise computer science label for this topic.Make sure you only return the label and nothing more.</p>
<p>Assessing Adaptation Through Evaluation of Topic Naming.To assess the effect of domain adaptation on topic modeling, we used the idea that, on average, the names of topic labels tend to be dissimilar.To measure the numerical similarities between topics, we have chosen to use three distinct similarity metrics TopicSimilarity to evaluate the similarity between topic names of Topic A and Topic B:</p>
<p>-Levenstein Fuzzy Matching: We employ the fuzzywizzy python implementation which utilizes normalization based on the length to provide a normalized lexical similarity score between the topic names.-BERTscore: Uses BERT model embeddings to evaluate the similarity between A and B, by comparing the semantic embedding similarity of present n-grams w BERTscore = 1</p>
<p>|A|</p>
<p>w∈A max w ′ ∈B cos(w, w ′ ) -Semantic Similarity (using all-mini-LM-v12): Evaluates the semantic closeness of A and B using the cosine of the angle between their vector representations, S(A) and S(B), as: cos(A, B) = S(A)•S(B)
∥S(A)∥•∥S(B)∥</p>
<p>AHAM heuristic</p>
<p>We propose a heuristic that jointly combines the number of outliers |k outliers |, the number of clusters |k|, and the similarity across the generated topic names with the TopicSimilarity metric to the number of steps n steps in the domainadaptation.We define the AHAM objective as:
AHAM objective = 2 • |k outliers | |k| • k i=1 j=i+1 TopicSimilarity(k i , k j ) k(k − 1)
We evaluate at every 10, 000 step of the GPL and select the topic modeling that yielded the lowest AHAM objective score within some evaluation budget of n steps .</p>
<p>Quantitative exploration of the AHAM objective</p>
<p>To our knowledge, no prior research has yet investigated the impact of further domain adaptation (using methods like the aforementioned GPL) of sentence transformers within a specific domain for topic discovery in the BERTopic framework.We selected two domain datasets from the ArXiv repository [13]: arXiv, which includes 25,000 entries from a wide range of scientific disciplines, and medarxiv, containing 8,500 entries from the medical science field.The general domain dataset, arXiv, with its larger data volume, allowed us to develop an experimental suite to assess the impact of size and domain granularity.In contrast, the smaller but domain-specific medarxiv enabled us to examine the effects of a smaller, more specialized dataset.To address this gap, this experimental setting concentrates on four research questions: RQ1.Does the domain-specific adaptation of the sentence transformer lead to a more precise topic differentiation and a decrease in outlier topics?Table 1 and Figure 2 provide insight into the impact of domainspecific adaptation of a sentence transformer on topic modeling within arXiv and medarxiv datasets.The results indicate a significant decrease in both the number of outliers and the number of topics at step 20k (improving from 43 outliers to 29 for the arXiv and 10 for the medarXiv), which suggests that the adaptation process likely improves the transformer's ability to discern more relevant topics and reduces the identification of outlier topics.This enhancement in precision is further evidenced by the sharp reduction in topics, pointing to a more focused topic differentiation.RQ2.Which is more effective for meta-literature analysis: broad domainspecific adaptation or niche-specific knowledge?The trajectory of the AHAM objective revealed that both lexical and semantic distance metrics peaked at step 20 for the arXiv and medarxiv datasets, indicating a two-phase process when adapting: obfuscation of knowledge and iterative refinement and improvement.We noticed that by utilizing broad domain knowledge, the adaptation process took more steps however it yielded a lower AHAM score (0.26) with 88.4% reduction in outliers.On the other side the medarXiv adaptation, found the local optimum after 30k steps, yielding a score of 1.09 and 72% reduction of outliers.The results suggest that if possible to identify what domain is best suited to transform from, then adapting such domain would enable the model to generalize better.In the case of medarXiv, we noticed that the model would specialize earlier for the topics correlated with a certain domain.</p>
<p>RQ3.What is the relationship between the domain adaptation granularity and performance?Integrating the insights from the AHAM objective with the similarity metrics reveals that the specific stages in the modeling process significantly influence the development of topic similarities.Both datasets exhibit a tendency to diverge and then re-converge, which may reflect a common stabilization point in topic similarity as the models are refined.Evaluating the AHAM objective trajectory reveals that the function is non-convex, which makes it difficult to optimize.The results indicate a non-linear and complex progression of topic modeling characterized by both convergence and divergence, suggesting that topic evolution is influenced by the interplay of the dataset characteristics (Figure 3 presents the topic evolution between the initial and the topic modeling after the domain adaptation selected by AHAM).RQ4.How do the topics evolve when the sentence-transformer language model is increasingly adapted to a specific domain?The results of the AHAM optimization (Figure 2) suggest that extensive training with both models enhances the subsequent topic modeling's understanding of the domain it aims to represent.It is crucial to select an adaptation dataset that is closely aligned with the specific niche observed.Given the observed lexical and contextual similarities, the AHAM aspects seem to serve as reliable metrics for assessing the efficacy of the topic modeling performance of the domain-specific adaptation of the model.</p>
<p>Conclusion and Further Work</p>
<p>In this study, we have proposed AHAMa metric to optimize the topic modeling via domain adaptation for meta-literature analysis and literature-based discovery by enhancing the BERTopic framework through domain adaptation.Our findings are twofold.Firstly, document partitioning has shown promising results in terms of cluster validity.Experts in the LBD manually examined three clustering solutions (i.e., baseline, ArXiv-, and medarxiv-specific adaptation models).The evaluated approaches yielded a single large general cluster, labeled "Discovery through Statistical and Knowledge-Based Methods" for the baseline approach, "BioMed Text Mining" in the ArXiv-specific model, and "Knowledge Linkage Discovery" for the medarxiv-specific model.This was followed by numerous smaller and highly specialized clusters, such as the "Kostoff's cluster," which is heavily focused on the author's notion of literature-related discovery; the "Biomedical Connections Discovery" cluster, which comprises works that employ NLP-specific techniques for LBD; or the "Knowledge Graph Embeddings" topic, which contains recent studies utilizing link prediction on vector embeddings derived from biomedical knowledge graphs.</p>
<p>Secondly, we have devised the AHAM heuristic based on the reduction of outliers and similarity between topics derived by LLMs by following the domainexperts prompt.This heuristic suggests a gradual, grid-like domain adaptation of the model, conditional upon a decreasing number of outliers when detecting topics.The best-defined topic modeling, yielded by lowest AHAM objective, conditioned on domain adaptation, yielded the aforementioned topics that multiple domain experts carefully inspected.The objective's optimization trajectory is complex and non-convex, suggesting that prolonged adaptation to a domain might not lead to an optimal topic modeling setting.</p>
<p>For future work, we plan to rigorously test domain adaptation in topic modeling, driven by the AHAM objective across various domains.We are also interested in exploring the effects of training with in-domain data.Additionally, an important direction is to analyze the model's capability to adapt and identify topics in different low-and less-resourced languages.An intriguing area for further investigation is evaluating the impact of the generative LLM size on topic name generation, as well as the selection of different sentence-transformers for</p>
<p>Fig. 1 .
1
Fig. 1.Documents distribution per year, semantically labeled by LLaMa2 prompting.</p>
<p>Fig. 2 .
2
Fig.2.Assessment of the optimization objective's trajectory and similarity, conducted on intervals of 10k steps within a total budget of 50k steps.</p>
<p>Fig. 3 .
3
Fig. 3. Evolution of topics between the first and best step of domain adaptation following AHAM objective.</p>
<p>Table 1 .
1
Comparison of Topic Modeling Performance Post-Adaptation for arXiv and medarxiv Datasets Relative to Baseline, Detailing Topic Counts (#T), Outlier Frequencies (#O), and Outlier-to-Topic ( #O #T ) Ratios.Bolded values represent the setting selected by the AHAM metric.
steps domain #T #O #O #TLev BERT Cos domain #T #O #O #T Lev BERT Cosbase/15 43 2.87 0.32 0.86 0.25/15 43 2.87 0.32 0.86 0.251020 32 1.60 0.32 0.86 0.3111 45 4.09 0.32 0.86 0.25204 29 7.25 0.45 0.88 0.455 10 2.00 0.39 0.85 0.4530 arXiv 8 35 4.38 0.33 0.87 0.36 medarXiv 11 12 1.09 0.32 0.88 0.414019 5 0.26 0.31 0.85 0.304 23 5.75 0.29 0.87 0.425019 27 1.42 0.35 0.87 0.385 9 1.80 0.32 0.86 0.47
https://huggingface.co/
AcknowledgementsThe authors acknowledge the financial support from the Slovenian Research And Innovation Agency through research core funding (No.P2-0103) and research projects: Research collaboration prediction using literature-based discovery approach (No.J5-2552), Embeddings-based techniques for Media Monitoring Applications (No. L2-50070) and Hate speech in contemporary conceptualizations of nationalism, racism, gender and migration (No.J5-3102).this task.ImplementationCode available at https://github.com/bkolosk1/aham.
BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational LinguisticsJun 20191</p>
<p>Using latent semantic indexing for literature based discovery. M D Gordon, S Dumais, Journal of the American Society for Information Science. 4981998</p>
<p>Keybert: Minimal keyword extraction with bert. M Grootendorst, 2020</p>
<p>Bertopic: Neural topic modeling with a class-based tf-idf procedure. M Grootendorst, 2022</p>
<p>Improving efficient neural ranking models with cross-architecture knowledge distillation. S Hofstätter, S Althammer, M Schröder, M Sertkan, A Hanbury, 2020</p>
<p>Scientometric analysis and knowledge mapping of literature-based discovery. A Kastrin, D Hristovski, Scientometrics. 12621986-2020. 2021</p>
<p>Out of thin air: Is zero-shot crosslingual keyword detection better than unsupervised?. B Koloski, S Pollak, B Škrlj, M Martinc, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationJun 2022</p>
<p>Can language models learn from explanations in context?. A Lampinen, I Dasgupta, S Chan, K Mathewson, M Tessler, A Creswell, J Mcclelland, J Wang, F Hill, Findings of the Association for Computational Linguistics: EMNLP 2022. Y Goldberg, Z Kozareva, Y Zhang, Association for Computational LinguisticsDec 2022</p>
<p>N Lavrač, M Martinc, S Pollak, M Pompe Novak, B Cestnik, Bisociative Literature-Based Discovery: Lessons Learned and New Word Embedding Approach. 202038</p>
<p>hdbscan: Hierarchical density based clustering. L Mcinnes, J Healy, S Astels, J. Open Source Softw. 2112052017</p>
<p>Umap: Uniform manifold approximation and projection. L Mcinnes, J Healy, N Saul, L Großberger, Journal of Open Source Software. 3298612018</p>
<p>Recent advances in natural language processing via large pretrained language models: A survey. B Min, H Ross, E Sulem, A P B Veyseh, T H Nguyen, O Sainz, E Agirre, I Heintz, D Roth, ACM Computing Surveys. 5622023</p>
<p>N Muennighoff, N Tazi, L Magne, N Reimers, 10.48550/ARXIV.2210.07316arXiv:2210.07316Mteb: Massive text embedding benchmark. 2022arXiv preprint</p>
<p>What in-context learning "learns" in-context: Disentangling task recognition and task learning. J Pan, T Gao, H Chen, D Chen, 10.18653/v1/2023.findings-acl.527Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJul 2023</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>Making monolingual sentence embeddings multilingual using knowledge distillation. N Reimers, I Gurevych, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing11 2020</p>
<p>SemaTyP: A knowledge graph based literature mining method for drug discovery. S Sang, Z Yang, L Wang, X Liu, H Lin, J Wang, 10.1186/s12859-018-2167-5BMC Bioinformatics. 1911932018</p>
<p>Emerging approaches in literature-based discovery: Techniques and performance review. The Knowledge Engineering Review. Y Sebastian, E G Siew, S O Orimaye, 10.1017/S0269888917000042201732e12</p>
<p>Retrieval-efficiency trade-off of unsupervised keyword extraction. B Škrlj, B Koloski, S Pollak, Discovery Science. P Pascal, D Ienco, ChamSpringer Nature Switzerland2022</p>
<p>Fish oil, Raynaud's syndrome, and undiscovered public knowledge. D R Swanson, 10.1353/pbm.1986.0087Perspectives in Biology and Medicine. 3011986</p>
<p>BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. N Thakur, N Reimers, A Rücklé, A Srivastava, I Gurevych, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>A systematic review on literaturebased discovery workflow. M Thilakaratne, K Falkner, T Atapattu, PeerJ Computer Science. 5e2352019</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, 2023</p>
<p>I Vayansky, S A P Kumar, A review of topic modeling methods. Dec 202094101582</p>
<p>Tsdae: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning. K Wang, N Reimers, I Gurevych, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicNov 2021</p>
<p>GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. K Wang, N Thakur, N Reimers, I Gurevych, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsJul 2022</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Q Wang, D Downey, H Ji, T Hope, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 352022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3135 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3135</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3135</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-0e97b13624bc15e7f30c82402252eca4d1a8eeba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0e97b13624bc15e7f30c82402252eca4d1a8eeba" target="_blank">LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models, and is the first work that explicitly injects numeracy capability into language models using Number Plugins.</p>
                <p><strong>Paper Abstract:</strong> Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Number Plugins. Besides evaluating toy models on toy tasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT, TabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans), and observe the performances of language models are constantly improved by LUNA. The augmented models also improve the official baseline of TAT-QA (EM: 50.15 ->59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3135.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3135.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LUNA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments encoder-only transformer language models with number-aware tokenization (NumTok), number embeddings (NumBed), and intermediate pre-training (MLM with a regression loss for numbers and optional momentum distillation) to improve numerical reasoning and arithmetic-related understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LUNA (applied to RoBERTa, BERT, TabBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LUNA is not a single LM but a patch: it wraps an existing encoder-only transformer (RoBERTa, BERT, TabBERT in experiments) with NumTok (recognizes numbers and inserts special <num?> tokens) and NumBed (learned neural encoder that maps raw number strings to vectors of the model embedding dimensionality). It uses intermediate pre-training on downstream-like (table+text) data with a modified MLM objective that adds a regression head predicting log(abs(value)) and sign for masked <num?> tokens and optional momentum-based distillation to regularize when starting from checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Decoding numbers (reconstructing numeric string), addition/subtraction of two numbers (toy probing), list maximum selection, arithmetic questions in table QA (addition/subtraction/multiplication/division derived answers)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Introduce dedicated whole-number token representations (NumTok) and learned numeric embeddings (NumBed) so that numbers are treated as atomic inputs; align numeric embeddings with vocabulary embeddings via regression loss (predict log magnitude + sign) and distillation so the transformer can attend to and fuse number semantics into deeper layers, enabling numerical comparisons and simple arithmetic reasoning as part of the transformer's attention and classification/sequence-tagging computations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) Toy probing (Table 2): CharLSTM NumBed obtains very low RMSE on significand and near-100% exponent accuracy for decoding and binary arithmetic probes (e.g., addition significand RMSE 0.5572, exponent acc 99.46%). 2) Ablations: AddBack NumTok + CharLSTM + pretraining with distillation yields large downstream gains (TAT-QA EM 50.15 → 59.58; EM_num 38.37 → 63.33). 3) Visualization: TSNE of embeddings shows deeper-layer fusion of NumBed and word embeddings into semantic clusters (dates, money, etc.); attention maps (Figure 3) show baseline attention is diffuse while LUNA focuses attention on the relevant numeric token (e.g., '3%'). 4) Controlled experiments: AddBack > Replace; CharLSTM > CharFormer > DICE on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Regression loss and other components have task-dependent effects: regression loss helped tagging tasks (CrediTrans) but sometimes hurt TAT-QA; DICE (a value-based embedding) performed reasonably on some toy tasks but worse on real tasks. Authors note some failed designs (number prompts, NumBed trained on toy tasks used as initialization) and that LUNA is limited to decimal formats and encoder-only models.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Tokenization & embedding intervention (NumTok + NumBed), intermediate pre-training with a regression loss on masked number tokens, and optional momentum distillation (teacher-student) when starting from checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Consistent improvements in numeric and numeric-heavy downstream tasks: example improvements include RoBERTa on TAT-QA EM 50.15 → 59.58 and EM_num 38.37 → 63.33; TabBERT on CrediTrans F1 84.79 → 86.17 (with LUNA), and strong gains on arithmetic subset of TAT-QA (EM on arithmetic questions 36.39 → 69.57). Toy-probe accuracy/RMSE for arithmetic operations improved substantially with CharLSTM NumBed compared to baseline encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Toy probing (numbers extracted from real tables): CharLSTM decoding significand RMSE=0.0946, exponent acc=99.97%; addition significand RMSE=0.5572, exponent acc=99.46%; subtraction significand RMSE=1.367, exponent acc=97.17%; list max acc=98.55%. Baselines: RoBERTa (no NumBed) decoding sig RMSE=2.400 exp acc=34.75%; DICE decoding sig RMSE=2.309 exp acc=67.90%. Downstream: TAT-QA overall EM: baseline RoBERTa 50.15 → LUNA 59.58 (Δ +9.43); TAT-QA EM_num (arithmetic questions) baseline 38.37 → LUNA 63.33 (Δ +24.96). TAT-QA F1: 57.84 → 67.15. CrediTrans (TabBERT): baseline F1 84.79 → LUNA 86.17. TabFact accuracy modestly improved in controlled variants (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Original transformers split numbers into subword tokens causing loss/scattering of numeracy; some pretraining choices can hurt (regression loss can reduce performance on classification tasks that rely on pooled CLS features), NumTok implementation ignores scientific notation, non-decimal formats, units and multimodal number-word mixtures; LUNA in current form only supports encoder-only models and cannot address number decoding (generation) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison to humans or symbolic calculators; however, improvements are contrasted against symbolic-like numeric baselines (DICE) and classic tokenization strategies — LUNA aims to bring transformers closer to symbolic numeric awareness by explicitly encoding magnitude/sign and preserving whole-number identity in embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3135.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3135.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CharLSTM (NumBed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level BiLSTM Number Embedder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NumBed variant that encodes raw number character strings with one-hot character embeddings fed through a bidirectional LSTM, pooled to produce an embedding vector matched to the LM embedding space; designed to capture digit-order and positional significance (left/right) to represent numeric value and semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CharLSTM NumBed</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A trainable numeric encoder: character tokens (digits, sign, percent, separators) are embedded and processed by a BiLSTM; final hidden states are averaged to form a fixed-size numeric embedding of the same dimensionality as the transformer's token embeddings. Variants with different parameter counts (0.1M, 1M, 9M) were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Number decoding (reconstruction), binary arithmetic (addition/subtraction) probes, list maximum selection, and use inside downstream table QA/fact verification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By encoding character-level structure and digit-order with BiLSTM, CharLSTM captures positional digit significance and sign, producing embeddings that preserve scale and enable the transformer to perform comparisons and simple arithmetic-like operations via attention and learned heads; alignment via regression loss grounds embeddings to log-magnitude and sign.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Toy probing (Table 2) shows CharLSTM dramatically outperforms baseline LM encodings: decoding sig RMSE=0.0946 and exonent=99.97%; addition/subtraction probes have low RMSE and near-100% exponent prediction. Ablations show larger CharLSTM models improve downstream task scores and CharLSTM outperforms CharFormer and DICE in most settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CharFormer sometimes competitive; effectiveness relies on pre-training alignment — without proper pretraining or with poor NumTok (Replace vs AddBack) gains are reduced. CharLSTM alone cannot help generation/decoding tasks (encoder-only limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural intervention: replace default numeric token encoding with learned CharLSTM numeric embedding; used with AddBack NumTok and intermediate pretraining (regression + distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Strongly reduced probe RMSEs and substantially improved downstream metrics when combined with NumTok + pretraining; larger CharLSTM sizes yield better performance (9M > 1M > 0.1M).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probe: decoding significand RMSE=0.0946, exponent acc=99.97%; addition sig RMSE=0.5572 exp acc=99.46%; subtraction sig RMSE=1.367 exp acc=97.17%; list max acc=98.55%. Downstream improvements when used in LUNA: TAT-QA EM and EM_num increases as reported in the LUNA entry.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Depends on correct recognition by NumTok; only handles decimal forms and limited punctuation; cannot be directly used in autoregressive decoding; potential overfit if pretraining not regularized by distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human/symbolic comparison; CharLSTM provides a learned embedding that captures magnitude-like features approximating aspects of symbolic numeric representation (e.g., digit significance) but is not itself an algorithmic calculator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3135.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3135.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CharFormer (NumBed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level Transformer + Positional Encoding Number Embedder (CharFormer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NumBed variant that embeds characters, adds rule-based positional encodings indicating digit significance relative to the decimal point, then processes via transformer encoder (and an LSTM), pooling to a fixed embedding — an alternative to CharLSTM intended to capture digit relations with self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CharFormer NumBed</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Character-level encoder: characters embedded, added to positional encodings reflecting digit significance, passed through a transformer encoder layer followed by an LSTM; final states averaged to form a numeric embedding compatible with the LM embedding dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Number decoding, addition/subtraction probes, list max selection, downstream table QA/fact verification.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Use self-attention across digit characters plus digit-significance positional signals to encode magnitude and local patterns; embedding supports attention-based fusion with words in the transformer layers to facilitate arithmetic-like comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Toy probing shows CharFormer outperforms baseline LM but is slightly worse than CharLSTM on RMSE and accuracies (e.g., decoding sig RMSE=0.1289, decoding exp acc=99.85%; addition sig RMSE=0.6718 exp acc=99.44%). Downstream experiments show CharFormer improves performance compared to baselines but CharLSTM is overall better in this paper's tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Slightly higher RMSE and slightly lower downstream scores than CharLSTM in these experiments, suggesting transformer-based NumBed did not outperform LSTM-based NumBed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural intervention (NumBed variant) used within LUNA with NumTok and intermediate pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved numeric probe performance and downstream metrics vs baseline tokenized numbers, but generally underperformed CharLSTM in the authors' controlled experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probe: decoding sig RMSE=0.1289 exp acc=99.85%; addition sig RMSE=0.6718 exp acc=99.44%; subtraction sig RMSE=1.546 exp acc=96.86%; list max acc=97.71%. Downstream improvements present but smaller than CharLSTM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Still limited to decimal formats and encoder-only use; larger NumBed size yields better performance suggesting CharFormer may require greater capacity to match CharLSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; functions as a learned encoder capturing digit relations rather than performing symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3135.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3135.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DICE (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DICE: Methods for Numeracy-Preserving Word Embeddings (baseline from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed value-based number embedding method that constructs numeric feature vectors from values (e.g., capturing magnitude) and was used as a competitive baseline for numeric probes in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Methods for numeracy-preserving word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DICE (as baseline NumBed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A value-based embedding approach (Sundararaman et al., 2020) that maps numeric values to fixed numeric features/vectors and was applied as a NumBed baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Toy numeric probes (decoding, addition/subtraction, list max) and used as an embedding baseline for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Represent numbers via engineered/value-based features that capture magnitude and relative differences, enabling shallow arithmetic-like predictions via downstream probes/classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>On toy probes, DICE achieved moderate performance: decoding sig RMSE=2.309 exp acc=67.90% (better exponent accuracy than raw RoBERTa baseline but worse than CharLSTM/CharFormer); addition/subtraction probe exponent accuracies decent but lower than CharLSTM. On real downstream tasks, DICE performed worse than CharLSTM and CharFormer (e.g., much lower TAT-QA and TabFact metrics in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Although DICE did fairly on some toy probes, it performed poorly on realistic downstream tasks: authors concluded DICE is not a strong baseline for NumBed in real tasks and can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Alternative NumBed (value-feature encoding) used as a baseline instead of learned CharLSTM/CharFormer encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>On toy tasks sometimes competitive on specific metrics, but in downstream table/text tasks DICE led to degraded end-task performance compared to learned character-based NumBeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probe: decoding sig RMSE=2.309 exp acc=67.90%; addition sig RMSE=2.453 exp acc=69.29%; subtraction sig RMSE=3.819 exp acc=39.53%; list max acc=92.45%. Downstream: with DICE in LUNA variant, TAT-QA EM and F1 worse than CharLSTM variants (e.g., Table 3 row (3) EM=47.15, F1=54.93 vs CharLSTM EM=59.58, F1=67.15 for best LUNA).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Loses fine-grained similarity relationships across scales and buckets; not robust on real-world table+text tasks where learned character-level encoders generalize better.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human/symbolic comparison; DICE is an engineered numeric feature approach rather than algorithmic/symbolic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3135.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3135.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa (baseline model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (encoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained encoder-only transformer language model (used as baseline in experiments) that uses subword tokenization (BPE) and was evaluated with and without LUNA augmentations on numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer pretrained on large text corpora with BPE subword tokenization (the paper uses a standard RoBERTa checkpoint as baseline; specific model size not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Toy numeric probes (decoding/add/subtract/list max when numbers represented via default tokenization), and downstream table QA arithmetic questions (TAT-QA arithmetic subset).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>With default tokenization, RoBERTa treats numbers as subword pieces leading to scattered numeracy information; arithmetic performance is thus poor because the model lacks integrated numeric representations — it relies on memorized textual patterns and brittle subword compositions rather than grounded numeric embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Probe baselines show RoBERTa poor on numeric probes: decoding significand RMSE=2.400 and exponent acc=34.75%; addition RMSE=2.332 exp acc=37.80%; subtraction RMSE=4.166 exp acc=52.16%; list max acc=52.84%. Downstream, baseline RoBERTa on TAT-QA EM=50.15 and EM_num=38.37 indicating difficulty on arithmetic questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None presented that RoBERTa intrinsically performs arithmetic well; LUNA intervention demonstrates RoBERTa's deficits are remediable by whole-number embeddings and pretraining, implying tokenization/representation (not transformer capacity) is the core issue.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Receives LUNA interventions (NumTok AddBack + NumBed CharLSTM/CharFormer + intermediate pretraining with regression & distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large gains on arithmetic and numeric-heavy tasks: TAT-QA EM 50.15 → 59.58, EM_num 38.37 → 63.33, F1 57.84 → 67.15 when applying the best LUNA configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline probe: decoding sig RMSE=2.400 exp acc=34.75%; downstream baseline TAT-QA EM=50.15, F1=57.84, EM_num=38.37; with LUNA (best config): EM=59.58, F1=67.15, EM_num=63.33.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Tokenization splits numbers across subword tokens (e.g., '3.1415' -> '3', '.', '14', '15'), causing scattered numeric information and poor numeric generalization; attention maps on numeric questions are noisy without LUNA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human comparison; the baseline performs far worse than symbolic arithmetic (calculator) and shows that a pretrained LM without numeric-specific encoding does not behave like a symbolic calculator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Methods for numeracy-preserving word embeddings <em>(Rating: 2)</em></li>
                <li>Numeracy for language models: Evaluating and improving their ability to predict numbers <em>(Rating: 2)</em></li>
                <li>Do language embeddings capture scales? <em>(Rating: 1)</em></li>
                <li>Time2Vec: Learning a vector representation of time <em>(Rating: 1)</em></li>
                <li>Tabular transformers for modeling multivariate time series <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3135",
    "paper_id": "paper-0e97b13624bc15e7f30c82402252eca4d1a8eeba",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "LUNA",
            "name_full": "Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
            "brief_description": "A framework that augments encoder-only transformer language models with number-aware tokenization (NumTok), number embeddings (NumBed), and intermediate pre-training (MLM with a regression loss for numbers and optional momentum distillation) to improve numerical reasoning and arithmetic-related understanding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LUNA (applied to RoBERTa, BERT, TabBERT)",
            "model_description": "LUNA is not a single LM but a patch: it wraps an existing encoder-only transformer (RoBERTa, BERT, TabBERT in experiments) with NumTok (recognizes numbers and inserts special &lt;num?&gt; tokens) and NumBed (learned neural encoder that maps raw number strings to vectors of the model embedding dimensionality). It uses intermediate pre-training on downstream-like (table+text) data with a modified MLM objective that adds a regression head predicting log(abs(value)) and sign for masked &lt;num?&gt; tokens and optional momentum-based distillation to regularize when starting from checkpoints.",
            "arithmetic_task_type": "Decoding numbers (reconstructing numeric string), addition/subtraction of two numbers (toy probing), list maximum selection, arithmetic questions in table QA (addition/subtraction/multiplication/division derived answers)",
            "reported_mechanism": "Introduce dedicated whole-number token representations (NumTok) and learned numeric embeddings (NumBed) so that numbers are treated as atomic inputs; align numeric embeddings with vocabulary embeddings via regression loss (predict log magnitude + sign) and distillation so the transformer can attend to and fuse number semantics into deeper layers, enabling numerical comparisons and simple arithmetic reasoning as part of the transformer's attention and classification/sequence-tagging computations.",
            "evidence_for_mechanism": "1) Toy probing (Table 2): CharLSTM NumBed obtains very low RMSE on significand and near-100% exponent accuracy for decoding and binary arithmetic probes (e.g., addition significand RMSE 0.5572, exponent acc 99.46%). 2) Ablations: AddBack NumTok + CharLSTM + pretraining with distillation yields large downstream gains (TAT-QA EM 50.15 → 59.58; EM_num 38.37 → 63.33). 3) Visualization: TSNE of embeddings shows deeper-layer fusion of NumBed and word embeddings into semantic clusters (dates, money, etc.); attention maps (Figure 3) show baseline attention is diffuse while LUNA focuses attention on the relevant numeric token (e.g., '3%'). 4) Controlled experiments: AddBack &gt; Replace; CharLSTM &gt; CharFormer &gt; DICE on many tasks.",
            "evidence_against_mechanism": "Regression loss and other components have task-dependent effects: regression loss helped tagging tasks (CrediTrans) but sometimes hurt TAT-QA; DICE (a value-based embedding) performed reasonably on some toy tasks but worse on real tasks. Authors note some failed designs (number prompts, NumBed trained on toy tasks used as initialization) and that LUNA is limited to decimal formats and encoder-only models.",
            "intervention_type": "Tokenization & embedding intervention (NumTok + NumBed), intermediate pre-training with a regression loss on masked number tokens, and optional momentum distillation (teacher-student) when starting from checkpoints.",
            "effect_of_intervention": "Consistent improvements in numeric and numeric-heavy downstream tasks: example improvements include RoBERTa on TAT-QA EM 50.15 → 59.58 and EM_num 38.37 → 63.33; TabBERT on CrediTrans F1 84.79 → 86.17 (with LUNA), and strong gains on arithmetic subset of TAT-QA (EM on arithmetic questions 36.39 → 69.57). Toy-probe accuracy/RMSE for arithmetic operations improved substantially with CharLSTM NumBed compared to baseline encodings.",
            "performance_metrics": "Toy probing (numbers extracted from real tables): CharLSTM decoding significand RMSE=0.0946, exponent acc=99.97%; addition significand RMSE=0.5572, exponent acc=99.46%; subtraction significand RMSE=1.367, exponent acc=97.17%; list max acc=98.55%. Baselines: RoBERTa (no NumBed) decoding sig RMSE=2.400 exp acc=34.75%; DICE decoding sig RMSE=2.309 exp acc=67.90%. Downstream: TAT-QA overall EM: baseline RoBERTa 50.15 → LUNA 59.58 (Δ +9.43); TAT-QA EM_num (arithmetic questions) baseline 38.37 → LUNA 63.33 (Δ +24.96). TAT-QA F1: 57.84 → 67.15. CrediTrans (TabBERT): baseline F1 84.79 → LUNA 86.17. TabFact accuracy modestly improved in controlled variants (see Table 3).",
            "notable_failure_modes": "Original transformers split numbers into subword tokens causing loss/scattering of numeracy; some pretraining choices can hurt (regression loss can reduce performance on classification tasks that rely on pooled CLS features), NumTok implementation ignores scientific notation, non-decimal formats, units and multimodal number-word mixtures; LUNA in current form only supports encoder-only models and cannot address number decoding (generation) tasks.",
            "comparison_to_humans_or_symbolic": "No direct comparison to humans or symbolic calculators; however, improvements are contrasted against symbolic-like numeric baselines (DICE) and classic tokenization strategies — LUNA aims to bring transformers closer to symbolic numeric awareness by explicitly encoding magnitude/sign and preserving whole-number identity in embeddings.",
            "uuid": "e3135.0",
            "source_info": {
                "paper_title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CharLSTM (NumBed)",
            "name_full": "Character-level BiLSTM Number Embedder",
            "brief_description": "NumBed variant that encodes raw number character strings with one-hot character embeddings fed through a bidirectional LSTM, pooled to produce an embedding vector matched to the LM embedding space; designed to capture digit-order and positional significance (left/right) to represent numeric value and semantics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CharLSTM NumBed",
            "model_description": "A trainable numeric encoder: character tokens (digits, sign, percent, separators) are embedded and processed by a BiLSTM; final hidden states are averaged to form a fixed-size numeric embedding of the same dimensionality as the transformer's token embeddings. Variants with different parameter counts (0.1M, 1M, 9M) were evaluated.",
            "arithmetic_task_type": "Number decoding (reconstruction), binary arithmetic (addition/subtraction) probes, list maximum selection, and use inside downstream table QA/fact verification tasks.",
            "reported_mechanism": "By encoding character-level structure and digit-order with BiLSTM, CharLSTM captures positional digit significance and sign, producing embeddings that preserve scale and enable the transformer to perform comparisons and simple arithmetic-like operations via attention and learned heads; alignment via regression loss grounds embeddings to log-magnitude and sign.",
            "evidence_for_mechanism": "Toy probing (Table 2) shows CharLSTM dramatically outperforms baseline LM encodings: decoding sig RMSE=0.0946 and exonent=99.97%; addition/subtraction probes have low RMSE and near-100% exponent prediction. Ablations show larger CharLSTM models improve downstream task scores and CharLSTM outperforms CharFormer and DICE in most settings.",
            "evidence_against_mechanism": "CharFormer sometimes competitive; effectiveness relies on pre-training alignment — without proper pretraining or with poor NumTok (Replace vs AddBack) gains are reduced. CharLSTM alone cannot help generation/decoding tasks (encoder-only limitation).",
            "intervention_type": "Architectural intervention: replace default numeric token encoding with learned CharLSTM numeric embedding; used with AddBack NumTok and intermediate pretraining (regression + distillation).",
            "effect_of_intervention": "Strongly reduced probe RMSEs and substantially improved downstream metrics when combined with NumTok + pretraining; larger CharLSTM sizes yield better performance (9M &gt; 1M &gt; 0.1M).",
            "performance_metrics": "Probe: decoding significand RMSE=0.0946, exponent acc=99.97%; addition sig RMSE=0.5572 exp acc=99.46%; subtraction sig RMSE=1.367 exp acc=97.17%; list max acc=98.55%. Downstream improvements when used in LUNA: TAT-QA EM and EM_num increases as reported in the LUNA entry.",
            "notable_failure_modes": "Depends on correct recognition by NumTok; only handles decimal forms and limited punctuation; cannot be directly used in autoregressive decoding; potential overfit if pretraining not regularized by distillation.",
            "comparison_to_humans_or_symbolic": "No direct human/symbolic comparison; CharLSTM provides a learned embedding that captures magnitude-like features approximating aspects of symbolic numeric representation (e.g., digit significance) but is not itself an algorithmic calculator.",
            "uuid": "e3135.1",
            "source_info": {
                "paper_title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CharFormer (NumBed)",
            "name_full": "Character-level Transformer + Positional Encoding Number Embedder (CharFormer)",
            "brief_description": "NumBed variant that embeds characters, adds rule-based positional encodings indicating digit significance relative to the decimal point, then processes via transformer encoder (and an LSTM), pooling to a fixed embedding — an alternative to CharLSTM intended to capture digit relations with self-attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CharFormer NumBed",
            "model_description": "Character-level encoder: characters embedded, added to positional encodings reflecting digit significance, passed through a transformer encoder layer followed by an LSTM; final states averaged to form a numeric embedding compatible with the LM embedding dimension.",
            "arithmetic_task_type": "Number decoding, addition/subtraction probes, list max selection, downstream table QA/fact verification.",
            "reported_mechanism": "Use self-attention across digit characters plus digit-significance positional signals to encode magnitude and local patterns; embedding supports attention-based fusion with words in the transformer layers to facilitate arithmetic-like comparisons.",
            "evidence_for_mechanism": "Toy probing shows CharFormer outperforms baseline LM but is slightly worse than CharLSTM on RMSE and accuracies (e.g., decoding sig RMSE=0.1289, decoding exp acc=99.85%; addition sig RMSE=0.6718 exp acc=99.44%). Downstream experiments show CharFormer improves performance compared to baselines but CharLSTM is overall better in this paper's tasks.",
            "evidence_against_mechanism": "Slightly higher RMSE and slightly lower downstream scores than CharLSTM in these experiments, suggesting transformer-based NumBed did not outperform LSTM-based NumBed here.",
            "intervention_type": "Architectural intervention (NumBed variant) used within LUNA with NumTok and intermediate pretraining.",
            "effect_of_intervention": "Improved numeric probe performance and downstream metrics vs baseline tokenized numbers, but generally underperformed CharLSTM in the authors' controlled experiments.",
            "performance_metrics": "Probe: decoding sig RMSE=0.1289 exp acc=99.85%; addition sig RMSE=0.6718 exp acc=99.44%; subtraction sig RMSE=1.546 exp acc=96.86%; list max acc=97.71%. Downstream improvements present but smaller than CharLSTM variants.",
            "notable_failure_modes": "Still limited to decimal formats and encoder-only use; larger NumBed size yields better performance suggesting CharFormer may require greater capacity to match CharLSTM.",
            "comparison_to_humans_or_symbolic": "No direct comparison; functions as a learned encoder capturing digit relations rather than performing symbolic computation.",
            "uuid": "e3135.2",
            "source_info": {
                "paper_title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DICE (baseline)",
            "name_full": "DICE: Methods for Numeracy-Preserving Word Embeddings (baseline from prior work)",
            "brief_description": "A previously proposed value-based number embedding method that constructs numeric feature vectors from values (e.g., capturing magnitude) and was used as a competitive baseline for numeric probes in this paper.",
            "citation_title": "Methods for numeracy-preserving word embeddings",
            "mention_or_use": "mention",
            "model_name": "DICE (as baseline NumBed)",
            "model_description": "A value-based embedding approach (Sundararaman et al., 2020) that maps numeric values to fixed numeric features/vectors and was applied as a NumBed baseline in the paper's experiments.",
            "arithmetic_task_type": "Toy numeric probes (decoding, addition/subtraction, list max) and used as an embedding baseline for downstream tasks.",
            "reported_mechanism": "Represent numbers via engineered/value-based features that capture magnitude and relative differences, enabling shallow arithmetic-like predictions via downstream probes/classifiers.",
            "evidence_for_mechanism": "On toy probes, DICE achieved moderate performance: decoding sig RMSE=2.309 exp acc=67.90% (better exponent accuracy than raw RoBERTa baseline but worse than CharLSTM/CharFormer); addition/subtraction probe exponent accuracies decent but lower than CharLSTM. On real downstream tasks, DICE performed worse than CharLSTM and CharFormer (e.g., much lower TAT-QA and TabFact metrics in Table 3).",
            "evidence_against_mechanism": "Although DICE did fairly on some toy probes, it performed poorly on realistic downstream tasks: authors concluded DICE is not a strong baseline for NumBed in real tasks and can hurt performance.",
            "intervention_type": "Alternative NumBed (value-feature encoding) used as a baseline instead of learned CharLSTM/CharFormer encoders.",
            "effect_of_intervention": "On toy tasks sometimes competitive on specific metrics, but in downstream table/text tasks DICE led to degraded end-task performance compared to learned character-based NumBeds.",
            "performance_metrics": "Probe: decoding sig RMSE=2.309 exp acc=67.90%; addition sig RMSE=2.453 exp acc=69.29%; subtraction sig RMSE=3.819 exp acc=39.53%; list max acc=92.45%. Downstream: with DICE in LUNA variant, TAT-QA EM and F1 worse than CharLSTM variants (e.g., Table 3 row (3) EM=47.15, F1=54.93 vs CharLSTM EM=59.58, F1=67.15 for best LUNA).",
            "notable_failure_modes": "Loses fine-grained similarity relationships across scales and buckets; not robust on real-world table+text tasks where learned character-level encoders generalize better.",
            "comparison_to_humans_or_symbolic": "No direct human/symbolic comparison; DICE is an engineered numeric feature approach rather than algorithmic/symbolic arithmetic.",
            "uuid": "e3135.3",
            "source_info": {
                "paper_title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "RoBERTa (baseline model)",
            "name_full": "RoBERTa (encoder-only transformer)",
            "brief_description": "A pre-trained encoder-only transformer language model (used as baseline in experiments) that uses subword tokenization (BPE) and was evaluated with and without LUNA augmentations on numeric tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa",
            "model_description": "Encoder-only transformer pretrained on large text corpora with BPE subword tokenization (the paper uses a standard RoBERTa checkpoint as baseline; specific model size not detailed in this paper).",
            "arithmetic_task_type": "Toy numeric probes (decoding/add/subtract/list max when numbers represented via default tokenization), and downstream table QA arithmetic questions (TAT-QA arithmetic subset).",
            "reported_mechanism": "With default tokenization, RoBERTa treats numbers as subword pieces leading to scattered numeracy information; arithmetic performance is thus poor because the model lacks integrated numeric representations — it relies on memorized textual patterns and brittle subword compositions rather than grounded numeric embeddings.",
            "evidence_for_mechanism": "Probe baselines show RoBERTa poor on numeric probes: decoding significand RMSE=2.400 and exponent acc=34.75%; addition RMSE=2.332 exp acc=37.80%; subtraction RMSE=4.166 exp acc=52.16%; list max acc=52.84%. Downstream, baseline RoBERTa on TAT-QA EM=50.15 and EM_num=38.37 indicating difficulty on arithmetic questions.",
            "evidence_against_mechanism": "None presented that RoBERTa intrinsically performs arithmetic well; LUNA intervention demonstrates RoBERTa's deficits are remediable by whole-number embeddings and pretraining, implying tokenization/representation (not transformer capacity) is the core issue.",
            "intervention_type": "Receives LUNA interventions (NumTok AddBack + NumBed CharLSTM/CharFormer + intermediate pretraining with regression & distillation).",
            "effect_of_intervention": "Large gains on arithmetic and numeric-heavy tasks: TAT-QA EM 50.15 → 59.58, EM_num 38.37 → 63.33, F1 57.84 → 67.15 when applying the best LUNA configuration.",
            "performance_metrics": "Baseline probe: decoding sig RMSE=2.400 exp acc=34.75%; downstream baseline TAT-QA EM=50.15, F1=57.84, EM_num=38.37; with LUNA (best config): EM=59.58, F1=67.15, EM_num=63.33.",
            "notable_failure_modes": "Tokenization splits numbers across subword tokens (e.g., '3.1415' -&gt; '3', '.', '14', '15'), causing scattered numeric information and poor numeric generalization; attention maps on numeric questions are noisy without LUNA.",
            "comparison_to_humans_or_symbolic": "No direct human comparison; the baseline performs far worse than symbolic arithmetic (calculator) and shows that a pretrained LM without numeric-specific encoding does not behave like a symbolic calculator.",
            "uuid": "e3135.4",
            "source_info": {
                "paper_title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Methods for numeracy-preserving word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
            "rating": 2
        },
        {
            "paper_title": "Do language embeddings capture scales?",
            "rating": 1
        },
        {
            "paper_title": "Time2Vec: Learning a vector representation of time",
            "rating": 1
        },
        {
            "paper_title": "Tabular transformers for modeling multivariate time series",
            "rating": 1
        }
    ],
    "cost": 0.014700249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training</h1>
<p>Hongwei Han ${ }^{1 \text { a }}$, Jialiang Xu ${ }^{2 * \dagger}$, Mengyu Zhou ${ }^{3}$; Yijia Shao ${ }^{4 *}$, Shi Han ${ }^{3}$, Dongmei Zhang ${ }^{3}$<br>${ }^{1}$ Tsinghua University, ${ }^{2}$ University of Illinois at Urbana-Champaign<br>${ }^{3}$ Microsoft Research, ${ }^{4}$ Peking University<br>hhw20@mails.tsinghua.edu.cn, jx17@illinois.edu, shaoyj@pku.edu.cn, {mezho, shihan, dongmeiz}@microsoft.com</p>
<h4>Abstract</h4>
<p>Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformerbased language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Number Plugins. Besides evaluating toy models on toy tasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT, TabBERT) over three different downstream tasks (TAT-QA, TabFact, CrediTrans), and observe the performances of language models are constantly improved by LUNA. The augmented models also improve the official baseline of TAT-QA ( $E M$ : $50.15 \rightarrow 59.58$ ) and achieve SOTA performance on CrediTrans $(F 1=86.17)$.</p>
<h2>1 Introduction</h2>
<p>Numbers are common in everyday NLP scenarios. Transformer-based language models (Qiu et al., 2020) are popular for number-rich tasks such as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Table 1: TAT-QA Example: A Financial Report Table.
TAT-QA (Zhu et al., 2021), TabFact (Chen et al., 2020) and CrediTrans (Padhi et al., 2021) (see §2.1 for details). For example, Table 1 comes from a financial report in TAT-QA. To correctly understand the question and give the answer, a model must understand the semantics of each number in this example. Current approaches to TAT-QA (see §2.1) often serialize the (table, text, query) triplet into a sequence, and then feed it into a transformer-based language model such as RoBERTa.</p>
<p>The weak spot of number understanding of transformers is magnified when facing rich-number tasks (e.g., the above QA, fact check, time series modeling tasks) that heavily depends on numerical reasoning and calculation. Errors could easily occur by adopting existing number processing and representation approaches in NLP (e.g., a bad case for Table 1 in §E.1). There are two great challenges: tokenization and representation of numbers.</p>
<p>Recent studies have already shown that current tokenization methods for numbers in language are suboptimal and sporadic (Thawani et al., 2021): Numbers are either filtered out, collapsed into <unk> token, quantized into finite bins, or split into arbitrary subword tokens. In other words, numeracy information is abandoned or scattered around. E.g., "3.1415" is split into "3", ".", "14", "15" by</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: Overview of LUNA Framework. Part I: number plugins (introduced in §3.1) include NumTok (number tokenizer) and NumBed (number embedder). Part II: number pre-training (introduced in §3.2). L<sub>D</sub>, L<sub>R</sub>, and L<sub>C</sub> respectively denote distillation loss (on soft labels), regression loss (on real values), and the original classification loss (classifying the masked tokens, on hard labels). HEAD<sub>REG</sub> and HEAD<sub>CLA</sub> are learnable modules that respectively project the last hidden states into 2 and vocab-size dimensions. The transformer and momentum transformer are initialized with the same checkpoint.</p>
<p>BPE (Sennrich et al., 2016) (used in RoBERTa) and into "3", ".", "141", "##5" by WordPiece (Wu et al., 2016).</p>
<p>On the other hand, to augment the numerical reasoning and calculation capability of language models, previous work such as Time2Vec (Kazemi et al., 2019) and DICE (Sundararaman et al., 2020) design value-based feature vectors for number representation. However, those methods are not evaluated on transformers or real-world tasks (as discussed in §4, those methods indeed hurt).</p>
<p>To better solve the number issues, we propose <strong>LUNA (Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training)</strong> framework. As shown in Figure 1, LUNA works as a patch for existing language models when handling rich-number tasks. Through tough and massive attempts, we also exclude many failed designs like number prompts (as discussed in <strong>Limitations</strong> section), obtaining the simple and effective LUNA.</p>
<p>Our first key idea behind LUNA is that: <em>Each number should be represented as a whole for model input</em> (rather than broken into subword tokens or quantized into binned tokens). Learning embedding representation for raw number strings properly could help exploit numeracy information for downstream tasks. In LUNA, this idea corresponds to Part I: number plugin in Figure 1. Each number in the input (table and text) string will be first recognized by our <strong>NumTok</strong> (number tokenizer, see §3.1) and inserted as a special <em><num ?></em> (? denotes the recognized number string) token into the original input token sequence. Then, the embedding of <em><num ?></em> is computed by our <strong>NumBed</strong> (number embedder, see §3.1) which encodes the recognized number string as an embedding vector of the same dimension as the token embedding of the transformer. In detail, for Numtok, we discuss two designs: AddBack and Replace, which keep or delete the original representation of numbers, as will be introduced in §3.1. And for Numbed, we compare CharLSTM and CharFormer which encode the char sequence of the number string with an LSTM or a transformer, and in §3.1, we further compare different NumBed model sizes.</p>
<p>The newly introduced number embedding may be incompatible with the embedding space of the model's vocabulary tokens. To bridge the gap between number and vocabulary embeddings, Part II of LUNA – number pre-training on data from downstream tasks (see §3.2) – is designed for either starting the language model from scratch or from a pre-trained checkpoint. First, to incorporate the <em><num ?></em> token, we modified the classical MLM (masked language model) objective with specially designed regression loss for it. Then, when starting from a pre-trained checkpoint, it is easy for the</p>
<p>model to forget existing knowledge and overfit. To prevent this, a teacher-student model distillation process is designed as a regularization constraint, and this paper is the first work to leverage model distillation in number pre-training.</p>
<p>By applying LUNA to existing transformer-based language models, we observe constant performance improvements on QA, fact-check, and timeseries tasks (see §4.3) over three transformer-based language models. We evaluate RoBERTa <em>Liu et al. (2019)</em> on TAT-QA <em>Zhu et al. (2021)</em>, BERT <em>Devlin et al. (2018)</em> on TabFact <em>Chen et al. (2020)</em>, and TabBERT <em>Padhi et al. (2021)</em> on CrediTrans. With the help of LUNA framework, RoBERTa beats the baseline ($EM: 50.15 \rightarrow 59.58$, and $EM_{num}: 38.37 \rightarrow 63.33$) on TAT-QA. TabBERT reaches new SOTA numbers on CrediTrans ($F 1=86.17$, the previous SOTA is 84.79). By further analyzing the dataset and conducting empirical studies, we further demonstrate how LUNA helps preserve numeracy information for downstream tasks. According to the controlled experiments, we find that: 1) For NumTok and NumBed choices, AddBack is much better than Replace, CharLSTM is an overall good choice, and a larger numbed model size is better. 2) For number pre-training, distillation loss usually helps and regression loss only helps on tagging tasks. These conclusions benefit future model design and selection.</p>
<p>In summary, our major contributions are:</p>
<ul>
<li>To the best of our knowledge, our work first attempts to explicitly deal with the number issues on existing transformer-based language models. Besides evaluating on toy tasks, we evaluate the numeracy ability of our designs on real tasks as well.</li>
<li>We propose LUNA framework to enhance the number understanding capabilities of language models. The code and data of LUNA are open-sourced at https://github.com/zmy/LUNA</li>
<li>We do controlled experiments to discuss NumTok&amp;NumBed choices and number pre-training objectives.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Rich-number Tasks</h3>
<p>In this paper, we take three typical rich-number datasets and corresponding tasks as examples. TAT-QA <em>Zhu et al. (2021)</em> is a recent QA dataset requiring numerical reasoning over realistic tabular and textual data. In total, TAT-QA contains 16,552 questions associated with 2,757 hybrid (table + paragraphs) contexts from real-world financial reports. 20% of the hybrid contexts are used as development and test sets. A table in TAT-QA has 3~30 rows and 3~6 columns with at least 2 human-verified relevant nearby paragraphs.</p>
<p>TabFact (Chen et al., 2020) is a fact verification dataset with 16k Wikipedia tables as evidence for 118k human-annotated statements. In TabFact, for each Wikipedia table (with caption), the goal is to distinguish which given statements are entailed by the table and which are refuted by it. A table in TabFact has less than 50 rows and 10 columns.</p>
<p>CrediTrans (Padhi et al., 2021) is a time series corpus for credit card transactions with 24 million transactions from 20,000 users. Each transaction (row) has 12 fields (columns) consisting of both continuous and discrete attributes, such as merchant name, merchant address, transaction amount, etc. We can see an example of TAT-QA over Table 1, the one of TabFact and CrediTrans over Table 7 and Table 8 in appendix. The numeracy statistics are collected in §B in appendix as well.</p>
<p>Math Word Problems <em>Sundaram et al. (2022)</em> are another set of rich-number tasks besides tabular tasks. We leave it as future work because neural-based math-word-problem solvers are often generation models, which need the capability of decoding numbers. We focus this paper on understanding instead of generation.</p>
<h3>2.2 Numeracy in NLP</h3>
<p>In recent years, some effort has been made in boosting numeracy capability in NLP. Position Embedding <em>Vaswani et al. (2017)</em> embeds integers into vectors, and as an extension, Time2Vec <em>Kazemi et al. (2019)</em> and DICE <em>Sundararaman et al. (2020)</em> embed float numbers into vectors. TabBERT <em>Padhi et al. (2021)</em> quantifies numbers into buckets by their value. This might be a good attempt, but loses the information of similarity between nearby numbers in different buckets. These approaches embed numbers based on their values. DICE is famous for beating many methods on toy tasks like predicting</p>
<p>[1] Tabular And Textual dataset for Question Answering, https://nextplusplus.github.io/TAT-QA/</p>
<p>[2] TabFact, https://tabfact.github.io/</p>
<p>[3] Credit Card Transaction Dataset, https://ibm.ent.box.com/v/tabformer-data</p>
<p>+/- of two numbers, and we treat it as a baseline of NumBed choices. We find that DICE is not a good choice for real tasks, and even not good on toy tasks when measured with different metrics, as discussed in §4.</p>
<p>In most cases, numbers are tokenized in language ways in transformer-based language models. The common practice of language models usually adopts a fixed vocabulary and the sub-word tokenization approaches like BPE tokenizer (Sennrich et al., 2016) in RoBERTa and WordPiece tokenizer (Wu et al., 2016) in BERT. Instead of considering the number as an ensemble, the tokenizer splits the number into arbitrary tokens. NumBERT (Zhang et al., 2020) is pre-trained from scratch over a modified dataset where all numbers have been replaced with scientific notation (e.g., replacing 314.1 with 3141[EXP]2), in order to help the model understand the significance and exponent of numbers. However, NumBERT still breaks a number into sub-word pieces. As we have discussed in §1, breaking a number into sub-word pieces or quantifying it is not a grounded practice. A bad case analysis is provided in §E.1.</p>
<p>Some previous work also focuses on number decoding (Spithourakis and Riedel, 2018).</p>
<h2>3 Methods</h2>
<p>As shown in Figure 1, there are two major parts in LUNA: Part I, number plugin modules - NumTok (see §3.1) and NumBed (see §3.1) - which change the input to a language model such as RoBERTa or BERT. Part II, number pre-training (part II, see §3.2) from scratch or from an existing checkpoint of the language model.</p>
<p>In LUNA, we take the following steps to enhance a language model: First, NumTok changes the result token sequence of the model's tokenizer by inserting a special <num ?> (? denotes the recognized number string) token for each number. Second, the embedding of <num ?> is computed by NumBed which encodes the recognized number string as an embedding vector of the same dimension as the word-embedding. Third, number pre-training is taken place to align the space of number embedding and that of vocabulary embedding. This will also allow the model to fit unlabeled downstream data. Finally, the common fine-tuning process on the language model (with its additional task-specific</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: The Two Options of NumTok, Compared with BPE/WordPiece.
modules) is taken for a downstream task.</p>
<h3>3.1 Number Plugins</h3>
<p>The first step (part I) in LUNA is Number Plugins.
NumTok (as a wrapper) slightly changes the behavior of a model's default tokenizer by inserting "<num>" into an input string at locations where a number appears. Then, "<num>" will be recognized as a new special token <num ?> by NumTok in its output. As shown in Figure 2, there are two ways <num ?> to be inserted by NumTok: AddBack and Replace. The former adds <num ?> to the end of the original vocabulary token(s) of the number. The latter fully replaces these token(s).</p>
<p>The key algorithm in NumTok is recognizing numbers in the input string. We adopt a simple but effective filter-split-check approach to ensure characters in the same number are recognized as a whole, while irrelevant characters are excluded. E.g., " $1.76 \%-2.50 \%$ " should be recognized as two positive percentages " $1.76 \%$ " and " $2.50 \%$ ". Regular expressions used by NumTok are listed in §C.2.</p>
<p>Then, we should assign each <num ?> token an embedding vector that could be used for operations such as attention with other vocabulary tokens. For this, we propose NumBed, which is a trainable neural network aiming at generating embedding reflecting numeracy properties of the original number. As shown in Figure 1, the number (<num ?> token) embedding is the generated NumBed embedding, which will be part of the input sequence to the language model.</p>
<p>For the model design of NumBed, there is a wide range of choices, some provided by previous work like DICE (Sundararaman et al., 2020) and some designed by us: CharLSTM embeds the number on character level, i.e. the characters in the number string are first embedded with one-hot lookup embedding and then passed into a Bidirectional LSTM model. The underlying intuition is that 1) semantics in numbers could be queried either left-to-right or right-to-left, and 2) LSTM naturally encodes positional order of the digits, which mimics the</p>
<p>definition of number value (the sum of numbers on all digits, multiplied by its digit significance). The characteristic embedding is obtained by averaging the final hidden state of all LSTM layers. CharFormer utilizes a transformer encoder layer and rule- based positional encoding. Characters in the number string are first embedded with onehot lookup embedding and then added by a positional embedding indicating its digit significance (i.e. the relative position w.r.t. the decimal point). The result embedding sequence is then sequentially passed through the transformer encoder layer and an LSTM model. The characteristic embedding is obtained by averaging the final hidden state of all LSTM layers.</p>
<h3>3.2 Number Pre-training</h3>
<p>The next step (part II) in LUNA is number pretraining on downstream data. There are two reasons to do so: First, randomly initialized <num ?> embedding could damage the distribution of original input embeddings. Pre-training could help bridge the gap between number and vocabulary embeddings. Second, by pre-training on data of downstream tasks, the model can learn better the domain distributions of the tasks compared to directly fine-tuning (Gururangan et al., 2020).</p>
<h3>3.2.1 Pre-training from Checkpoints</h3>
<p>Reproducing the original pre-training process of a language model can be very costly. Thus, the first option in LUNA part II is to conduct intermediate pre-training from a pre-trained checkpoint (like RoBERTa and BERT) of the model.</p>
<p>The classical mask language model (MLM) objective (Devlin et al., 2018) is adapted for LUNA. We keep the common practice that overall 15\% of the input tokens are masked for recovery at the corresponding positions of the model output. For 80\% of them, the input tokens are replaced by a [MASK] token. For 50\% of the rest, they are replaced by a randomly selected token from the vocabulary; For the other 50\%, they keep what they are.</p>
<p>For a masked <num ?> token, a newly designed regression supervision is applied to fit the log absolute and the sign of its corresponding numeric value. And for a masked text token, classification supervision is used. To be exact, let $k$ denote the number of masked tokens in a mini-batch (about $15 \%$ of total), $\mathcal{N}={(o, v)}$ denote the set of output vector and real value of masked <num ?> tokens, $\mathcal{T}={(o, t)}$ denote the set of output vector and original token of masked vocabulary tokens. Obviously, $|\mathcal{N}|+|\mathcal{T}|=k$. Let a learnable MLP $H E A D_{R E G}$ denote the regression head and $H E A D_{C L A}$ denote the classification head. Then we can define MLM loss $\left(\mathcal{L}_{M L M}\right)$ as:</p>
<p>$$
\begin{gathered}
\mathcal{L}<em E="E" G="G" R="R">{M L M}=\frac{1}{k}\left(\mathcal{L}</em>}+\mathcal{L<em E="E" G="G" R="R">{C L A}\right) \
\mathcal{L}</em>(o)\right. \
\left.[ \ln (e p s+|v|), \operatorname{sgn}(v)]\right) \
\mathcal{L}}=\sum_{\mathcal{N}} M S E\left(H E A D_{R E G<em _mathcal_T="\mathcal{T">{C L A}=\sum</em>(t)\right)
\end{gathered}
$$}} C E\left(H E A D_{C L A}(o), \operatorname{onehot</p>
<p>Here the mean-squared-error regression loss $\left(\mathcal{L}<em R="R">{R E G}\right.$ or $\left.\mathcal{L}</em>}\right)$ takes in $\ln (e p s+|v|)$ to reflect actual range rather than exact value. The classification loss $\left(\mathcal{L<em C="C">{C L A}\right.$ or $\left.\mathcal{L}</em>\right)$ is the usual cross entropy.</p>
<p>Since downstream datasets are usually far smaller than the original pre-train datasets of the models, the overfitting problem may happen during the LUNA number pre-training phase. To solve the problem, we adopt a regularization method called model distillation (Li et al., 2021).</p>
<p>During model distillation, there is a student network $N e t$ and a teacher network (also called momentum network) $N e t_{m}$. They both start with the same parameter initialization (from a checkpoint), take the same masked embedding sequence as input, and generate probability distribution vectors ( $p$ by $N e t$ and $p_{m}$ by $N e t_{m}$ ) over the vocabulary. Please note that there is not a so-called distillation head, and $p$ is projected by $H E A D_{C L A}$. When treating $p_{m}$ as soft label (different from the hard label $t$ in $\mathcal{L}<em _distill="{distill" _text="\text">{C}$ ), the distillation loss ( $\mathcal{L}</em>$ ) is defined as:}}$ or $\mathcal{L}_{D</p>
<p>$$
\mathcal{L}<em _mathcal_N="\mathcal{N">{\text {distill }}=\frac{1}{k} \sum</em>\right)
$$}+\mathcal{T}} C E\left(p, p_{m</p>
<p>When applying $\mathcal{L}<em m="m">{D}, N e t$ is updated via gradient, but $N e t</em>+(1-\tau) N e t$.}$ is updated only via a fraction of Net. In other words, $N e t_{m}$ is the momentum accumulation of $N e t$. Let $\tau$ denote the momentum coefficient (default set to 0.995 ). At each step, $N e t_{m} \leftarrow \tau N e t_{m</p>
<p>The total pre-train loss ( $\mathcal{L}_{\text {pre-train }}$ ) is defined as (where $\alpha$ is a warm-up coefficient which grows with the training steps):</p>
<p>$$
\mathcal{L}<em L="L" M="M">{\text {pre-train }}=(1-\alpha) \mathcal{L}</em>
$$}+\alpha \mathcal{L}_{\text {distill }</p>
<p>Overall, $H E A D_{C L A}$ is under the supervision of $\mathcal{L}<em D="D">{C}$ and $\mathcal{L}</em>$ is under the supervi-}, H E A D_{R E G</p>
<p>sion of $\mathcal{L}<em m="m">{R}$, and $H E A D</em>$ CLA.}$ CLA is the momentum accumulation of $H E A D_{m</p>
<h3>3.2.2 Pre-training from Scratch</h3>
<p>When there are enough resources, one can still rerun the original pre-training process of a language model (like TabBERT) with LUNA number plugins and MLM loss ( $\mathcal{L}_{M L M}$, which includes the regression loss) without model distillation. However, pre-training from scratch is not the focus of this paper and we just want to verify the effectiveness of LUNA in different situations.</p>
<h2>4 Experiments</h2>
<h3>4.1 Datasets and Evaluation Metrics</h3>
<p>In $\S 2$, we have already discussed several language models (RoBERTa, BERT, TabBERT) and their corresponding tasks (TAT-QA, TabFact, CrediTrans). Both part I and part II of the LUNA framework will be evaluated on these models and tasks. For simplicity, in this paper, we only implemented limited model\&amp;task combinations: RoBERTa on TAT-QA, BERT on TabFact, and TabBERT on CrediTrans.</p>
<h3>4.1.1 Number Pre-training Datasets</h3>
<p>As discussed in $\S 3.2$, before fine-tuning on the aforementioned tasks, in LUNA a language model will be pre-trained with rich-number data.</p>
<p>When pre-training from the checkpoint with tuned parameters, MLM loss will be added together with distillation loss (see §3.2.1). For RoBERTa and BERT running on text and table inputs, we collect unlabeled table-text pairs (which are serialized into sequences) from TAT-QA and WikiTables (where TabFact comes from). When pre-training from scratch with randomly initialized parameters, MLM with regression loss is adopted in LUNA (see §3.2.2). In our experiments, we pre-train TabBERT (with number plugin) from scratch on CrediTrans samples. Please see $\S$ D. 1 for more details about pre-training datasets.</p>
<h3>4.1.2 Evaluation Tasks and Metrics</h3>
<p>First, we evaluate NumBed on toy tasks purely about numbers. We construct a pure number dataset from real-world numbers in the number pretraining dataset mentioned above. Utilizing probing models and training processes of DICE (Sundararaman et al., 2020), we evaluate models' numeracy abilities on a series of number-related tasks: 1) decoding the number, 2) predicting the result of adding/subtracting two numbers, 3) locating the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Decoding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Addition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Subtraction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">List Max</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sig $\downarrow$</td>
<td style="text-align: center;">Exp</td>
<td style="text-align: center;">Sig $\downarrow$</td>
<td style="text-align: center;">Exp</td>
<td style="text-align: center;">Sig $\downarrow$</td>
<td style="text-align: center;">Exp</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">CharLSTM</td>
<td style="text-align: center;">0.0946</td>
<td style="text-align: center;">99.97\%</td>
<td style="text-align: center;">0.5572</td>
<td style="text-align: center;">99.46\%</td>
<td style="text-align: center;">1.367</td>
<td style="text-align: center;">97.17\%</td>
<td style="text-align: center;">98.55\%</td>
</tr>
<tr>
<td style="text-align: center;">CharFormer</td>
<td style="text-align: center;">0.1289</td>
<td style="text-align: center;">99.85\%</td>
<td style="text-align: center;">0.6718</td>
<td style="text-align: center;">99.44\%</td>
<td style="text-align: center;">1.546</td>
<td style="text-align: center;">96.86\%</td>
<td style="text-align: center;">97.71\%</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">2.400</td>
<td style="text-align: center;">34.75\%</td>
<td style="text-align: center;">2.332</td>
<td style="text-align: center;">37.80\%</td>
<td style="text-align: center;">4.166</td>
<td style="text-align: center;">52.16\%</td>
<td style="text-align: center;">52.84\%</td>
</tr>
<tr>
<td style="text-align: center;">DICE</td>
<td style="text-align: center;">2.309</td>
<td style="text-align: center;">67.90\%</td>
<td style="text-align: center;">2.453</td>
<td style="text-align: center;">69.29\%</td>
<td style="text-align: center;">3.819</td>
<td style="text-align: center;">39.53\%</td>
<td style="text-align: center;">92.45\%</td>
</tr>
</tbody>
</table>
<p>Table 2: Probing numeracy on a number dataset constructed from numbers in real-world tables. We report the RMSE value for all significand (sig) predicting tasks, and classification accuracy for all exponent (exp) predicting tasks and the list maximum task. $\downarrow$ denotes lower is better. We use the same probing models as in Sundararaman et al. (2020).
maximum value in a list of numbers. For 1) and 2), unlike DICE, we separately report RMSE for the significand and ACC for the exponent ${ }^{5}$ instead of RMSE for the original value, because we think numbers with different scales are equally important (giving 10 when $\mathrm{gt}=1$ should be worse than giving 1200 when $\mathrm{gt}=1000$ ).</p>
<p>Then, we evaluate LUNA on real-world richnumber tasks: TAT-QA, TabFact, and CrediTrans. We split the dataset into train, valid, and test sets according to their previously reported ratios: 8:1:1, 8:1:1, and 6:2:2, respectively. In this paper, by default, the test sets will be used for reporting evaluation metrics.</p>
<p>On TAT-QA, the original evaluation metric $E M$ (Exact Match) and $F 1$ scores will be adopted. $E M_{\text {num }}$ is the exact match score over arithmetic questions that are officially split from TAT-QA by their authors. The arithmetic questions are those whose answer is given by operating " $+/-/ \times / \div$ " on numbers from the table or the text. On TabFact, beyond the original accuracy $A C C$, we also calculate the $A C C_{c x}$ on its complex subset. On CrediTrans, its original $F 1$ score is calculated.</p>
<h3>4.2 Experimental Setups</h3>
<h3>4.2.1 Baseline and Ablation Studies</h3>
<p>To understand how each component of LUNA contributes to the overall improvements, we design a series of ablation studies. First, with part I number plugin and part II number pre-training vs. without them ( 0.8 and $\times$ in Table 3). Here, row $\times$ or (3) correspond to the baseline models where LUNA is not applied or DICE is used as NumBed. Then, during pre-training, with the new losses $\mathcal{L}<em _distill="{distill" _text="\text">{R E G}$ and $\mathcal{L}</em>$ vs. without them ( 0.6 and (7).}</p>
<p>For fair comparisons, all evaluations are done on 1 node with the same environment configuration.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>LUNA Choices</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>RoBERTa</th>
<th></th>
<th></th>
<th>BERT</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Part I</td>
<td></td>
<td></td>
<td></td>
<td>Part II</td>
<td></td>
<td></td>
<td>TAT-QA</td>
<td></td>
<td></td>
<td>TabFact</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>NumTok</td>
<td>NumBed</td>
<td>+Size</td>
<td>$\mathcal{L}_{D}$</td>
<td>$\mathcal{L}_{R}$</td>
<td>$\mathcal{L}_{C}$</td>
<td></td>
<td>$E M$</td>
<td>$F 1$</td>
<td>$E M_{\text {rearn }}$</td>
<td>$A C C$</td>
<td>$A C C_{\text {sw }}$</td>
<td>AVG</td>
</tr>
<tr>
<td>(I)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>9M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$\mathbf{5 9 . 5 8}_{2.7}$</td>
<td>$\mathbf{6 7 . 1 5}_{1.7}$</td>
<td>$\mathbf{6 3 . 3 3}_{0.4}$</td>
<td>$66.07_{0.5}$</td>
<td>$62.24_{0.4}$</td>
<td>$\underline{\mathbf{6 3 . 6 7}}$</td>
</tr>
<tr>
<td>(1)</td>
<td>Replace</td>
<td>CharLSTM</td>
<td>9M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$51.87_{1.7}$</td>
<td>$59.94_{1.6}$</td>
<td>$50.21_{4.0}$</td>
<td>$65.41_{0.5}$</td>
<td>$61.43_{0.6}$</td>
<td>57.77</td>
</tr>
<tr>
<td>(2)</td>
<td>AddBack</td>
<td>CharFormer</td>
<td>9M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$57.03_{4.0}$</td>
<td>$64.83_{3.8}$</td>
<td>$57.48_{9.8}$</td>
<td>$\underline{66.81_{0.1}}$</td>
<td>$\underline{62.58_{0.2}}$</td>
<td>61.75</td>
</tr>
<tr>
<td>(3)</td>
<td>AddBack</td>
<td>DICE</td>
<td>$\sim$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$47.15_{1.2}$</td>
<td>$54.93_{1.9}$</td>
<td>$35.39_{3.1}$</td>
<td>$62.61_{1.2}$</td>
<td>$59.51_{0.6}$</td>
<td>51.92</td>
</tr>
<tr>
<td>(4)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>1M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$57.86_{2.7}$</td>
<td>$\underline{65.56}_{1.6}$</td>
<td>$58.41_{6.4}$</td>
<td>$65.73_{0.1}$</td>
<td>$61.79_{0.3}$</td>
<td>$\underline{61.87}$</td>
</tr>
<tr>
<td>(5)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>0.1M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$56.99_{2.0}$</td>
<td>$64.96_{1.7}$</td>
<td>$57.81_{4.5}$</td>
<td>$65.40_{0.7}$</td>
<td>$61.67_{0.6}$</td>
<td>61.37</td>
</tr>
<tr>
<td>(6)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>9M</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>$\checkmark$</td>
<td></td>
<td>$55.51_{2.9}$</td>
<td>$63.62_{2.8}$</td>
<td>$53.75_{5.3}$</td>
<td>$\mathbf{6 7 . 2 3}_{0.7}$</td>
<td>$\mathbf{6 2 . 9 4}_{0.5}$</td>
<td>60.61</td>
</tr>
<tr>
<td>(7)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>9M</td>
<td>$\times$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$54.33_{2.3}$</td>
<td>$62.13_{2.0}$</td>
<td>$51.73_{0.5}$</td>
<td>$65.40_{0.3}$</td>
<td>$62.19_{0.1}$</td>
<td>59.16</td>
</tr>
<tr>
<td>(8)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>9M</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td></td>
<td>$52.05_{2.8}$</td>
<td>$59.92_{2.8}$</td>
<td>$44.75_{6.7}$</td>
<td>$62.41_{1.8}$</td>
<td>$59.87_{1.1}$</td>
<td>55.80</td>
</tr>
<tr>
<td>(9)</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$50.15_{0.8}$</td>
<td>$57.84_{0.7}$</td>
<td>$38.37_{1.9}$</td>
<td>$62.13_{0.6}$</td>
<td>$59.71_{0.4}$</td>
<td>53.64</td>
</tr>
</tbody>
</table>
<p>Table 3: LUNA Evaluations on Language Models (RoBERTa, BERT) and Downstream Tasks (TAT-QA, TabFact). The last column AVG is the average of the five metrics, we add this column to demonstrate the overall capability of each controlled experiment.</p>
<table>
<thead>
<tr>
<th></th>
<th>LUNA Choices</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>CrediTrans</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>NumTok</td>
<td>NumBed</td>
<td>+Size</td>
<td>$\mathcal{L}_{R}$</td>
<td>$\mathcal{L}_{C}$</td>
<td></td>
<td>$F 1$</td>
</tr>
<tr>
<td>(A)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>9M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$\mathbf{8 6 . 1 7}_{0.1}$</td>
</tr>
<tr>
<td>(B)</td>
<td>Replace</td>
<td>CharLSTM</td>
<td>9M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$83.04_{0.0}$</td>
</tr>
<tr>
<td>(C)</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$\underline{85.65}_{1.8}$</td>
</tr>
<tr>
<td>(D)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>1M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$84.89_{0.6}$</td>
</tr>
<tr>
<td>(E)</td>
<td>AddBack</td>
<td>CharLSTM</td>
<td>0.1M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$84.01_{0.8}$</td>
</tr>
<tr>
<td>(9)</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\checkmark$</td>
<td></td>
<td>$84.79_{0.4}$</td>
</tr>
</tbody>
</table>
<p>Table 4: LUNA Evaluations on TabBERT and CrediTrans. Since TabBERT is pre-trained from scratch, nothing can be learnt from the teacher model and $\mathcal{L}_{D}$ is not used.</p>
<p>By default, all evaluation metrics reported in the following are averaged over $\mathbf{3}$ runs for experiments with randomness, and the standard error is reported along with the mean as a subscript (in the form of $m e a n_{s t d}$ ). Please find more training details in Appendix §D.</p>
<h3>4.2.2 Comparing Number Plugin Choices</h3>
<p>There could be many possible ways to design NumTok (see §3.1) and NumBed (see §3.1). Among many, we choose several promising ones to compare through controlled experiments. For NumTok, we compare the AddBack and Replace algorithms. They correspond to row (I) and (1) respectively. For NumBed, CharLSTM (I) and CharFormer (2) are compared with each other. For part I of LUNA, the major hyper-parameter is the model size of NumBed. In column " + Size" of Table 3 and Table 4, we will examine how model size impact the improvement margin on downstream tasks. Specially, NumBed choices are also discussed on toy tasks in Table 2, where row "RoBERTa"6 and "DICE" are baselines.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 5: LUNA Evaluations on TAT-QA Subsets. For TAT-QA, the metric is $E M$, and "baseline" and "ours" respectively denote row (9) and (II) in Table 3.</p>
<h3>4.2.3 Exploring Pre-training from Scratch</h3>
<p>As we have discussed in $\S 3.2$, pre-training from scratch is an option when enough computing resources and time are available. Based on our experiments of pre-training from checkpoints in Table 3, in Table 4 we design several controlled experiments on NumTok strategies, NumBed model size and loss designs. Here, row (9) corresponds to the baseline TabBERT where LUNA is not applied. Row (C) denotes adding $\mathcal{L}_{R}$ to the baseline TabBERT but without using Number Plugins.</p>
<p>Note that TabBERT is mainly designed for multivariate time series data, it models both inter- and intra-row dependencies by tokenizing rows at the cell level (i.e., each cell corresponds to a single token) and using a hierarchical structure to convert cell embeddings in the same row into row embedding. Since each cell corresponds to a single token, the original-defined AddBack option of NumTok is hard to apply on TabBERT while the Replace option is still applicable. As an alternative, we introduce a new way to "insert" <num ?> in the case of TabBERT by adding the NumBed embedding back to the original vocabulary token embeddings as a variant of AddBack. This variant of AddBack still tries to retain original token information while infusing additional number information.</p>
<p>4.3 Results and Insights</p>
<p>By examining Table 3 and Table 4, one can find following insights.</p>
<p>On all rich-number tasks, models with LUNA outperform their original ones. For each column in Table 3 and Table 4, the top results are always achieved by applying one variation of LUNA (row ⑪ - ⑯) and higher than the original models (row Ⓐ) or related work (row ③). This means by properly selecting part I and part II options for a model and a task, LUNA could help boost performance.</p>
<p>On TAT-QA task, the previous baseline RoBERTa is improved by more than 9% after applying LUNA (row ⑪, column "RoBERTa" in Table 3). On CrediTrans task, the SOTA model TabBERT is also improved by applying LUNA (row ⑧ in Table 4).</p>
<p>AddBack option (row ⑪) for NumTok is generally a good choice comparing to the Replace option (row ①). The gap is significant in RoBERTa and BERT results. AddBack also yields the best result (row ⑧) on CrediTrans task.</p>
<p>The best choice of NumBed varies for models and tasks, but CharLSTM is an overall good choice. By comparing row ⑪, ②, ③ with each other, we can find that NumBed choices matter. CharLSTM and CharFormer are significantly better than DICE, and this phenomenon occurs on the toy task Table 2 as well. According to those experiment results, we can say that DICE is not a strong baseline for NumBed. Specially, we also run another baseline for RoBERTa on TAT-QA that replaces number strings with their language representations (e.g., replacing "1,100" with "a thousand and one hundred"), and the result (EM = 33.35, F1 = 41.29, and EMnum = 55.13) is too bad to be comparable.</p>
<p>For part II number pre-training of LUNA, we find that model distillation is also an overall good choice. Comparing row ⑪ and ⑦, we find the gain of adding the distillation loss as regularization is large while no significant drawback is shown. However, the effect of regression loss on number tokens is dependent on tasks. E.g., for TAT-QA, row ⑪ is worse than row ⑪; for CrediTrans, row Ⓐ is worse than row ⑧; but for TabFact, row ⑪ is better than row ⑪. This might because, TAT-QA and CrediTrans are tagging tasks that tag on each token of the last hidden state, and TabFact is a classification task that only uses the <cls> token of the last hidden state. When we add L<sub>D</sub> to each masked token, the information of <cls> gets diluted and perturbed.</p>
<p>The size of the NumBed model also matters. From ⑤ to ④ to ⑪, and from ⑪ to ⑫ to ⑧, with the NumBed size grows, the performances always become better.</p>
<p>In detail, we also analyze the improvement we bring to different subsets of TAT-QA. We select the best random seed among all three repeats to do subset result analysis. As shown in Table 5, our method mostly improves the results on arithmetic questions in TAT-QA (from 36.39 to 69.57). This might because arithmetic questions need the most numeracy capability which LUNA provides.</p>
<p>Let's summarize,</p>
<ul>
<li>LUNA indeed helps for transformer-based language models.</li>
<li>For NumTok choices, AddBack is much better than Replace.</li>
<li>For NumBed choices, CharLSTM is an overall good choice (much better than the previously popular DICE), and enlarging numbed model size also helps.</li>
<li>For number pre-training, distillation loss helps when pre-training from a checkpoint, and regression loss only helps on tagging tasks.</li>
</ul>
<p>To figure out how LUNA works, we also do empirical studies in §E, including visualization of attention maps (in §E.1) and embeddings from different layers (in §E.2).</p>
<h2>5 Conclusion</h2>
<p>In this paper, we propose a patch framework for language models – LUNA (Language Understanding Number Augmentations on Transformers via Number Plugins and Pre-training) – to enhance their numerical reasoning and calculation capabilities. Through thorough experiments and empirical studies, we show that by adding number embeddings from the whole raw number string, and continuously pre-training language models with downstream data, the performance of the language models could improve considerably. We believe that the techniques proposed in LUNA could be applied to more scenarios including Math Word Problems and other rich-number tasks.</p>
<h2>Limitations</h2>
<p>LUNA is still preliminary work that requires in-depth explorations in the future. Many possible</p>
<p>choices and evaluations are not included in the paper due to time and space limitations.</p>
<p>First, our current NumTok design only handles decimal numbers, ignoring edge cases such as numbers in scientific notation (e.g., "1.5e-9"), nondecimal bases (e.g., "0x12BF"), and formats out of arabic-hindu notations (e.g., Roman number "XII", natural language numbers "twenty-one" and "veintiuno"). Also, units and magnitudes are not taken into the tokenizer design. Some words could even only be understood by a mixture of numbers and words (e.g., "H2O"). All these aspects could be improved in future work.</p>
<p>Second, our current NumBed approach only supports encoder-only language models. In this paper, we only explore number encoders and leave out number decoders. This prevents us from applying the LUNA ideas to generative language models such as TaPeX, TableGPT, etc. Also, the best choice of NumBed and how to initialize its parameters is still an open question. How to bring the best number representing methods together still requires lots of research efforts.</p>
<p>Third, MLM with regression loss and model distillation is a relatively simple approach for intermediate pre-training. There are many other possible pre-training objectives to be tried for better number understanding. For example, can we design pretraining objectives for a series/column of numbers?</p>
<p>Finally, before proposing LUNA, we have made many failed attempts. E.g., using the Numbed trained on toy tasks as initialization for number pre-training, or using number prompts that gives additional key and value for each transformer layer instead of directly inputting NumBed. We cannot derive rigorous proof of why they don't work.</p>
<h2>Ethics Statement</h2>
<p>Datasets This work collects the public dataset for research perposes. We believe there is no privacy issue, because TAT-QA, TabFact, and CrediTrans are accessible to the public.
Models This work uses three large-scale language models - RoBERTa, BERT, and TabBERT - among which RoBERTa and BERT are PLMs pre-trained on clean and non-evil text, and all the three models are number-pretrained/finetuned on clean downstream data. Therefore, we can make sure that the models we trained will not produce discriminatory answers.
Computational Resources Our methods require
low computational resources because the designed number pre-training is an intermediate pre-training that takes a few hours to be completed. Besides, the carbon footprints of all GPUs are monitored in real-time.</p>
<h2>References</h2>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020. TabFact: A large-scale dataset for table-based fact verification. In International Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.</p>
<p>Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.</p>
<p>Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet Sahota, Sanjay Thakur, Stella Wu, Cathal Smyth, Pascal Poupart, and Marcus A. Brubaker. 2019. Time2vec: Learning a vector representation of time. CoRR, abs/1907.05321.</p>
<p>Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. CoRR, abs/1907.11692.</p>
<p>Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre Dognin, Jerret Ross, Ravi Nair, and Erik Altman. 2021. Tabular transformers for modeling multivariate time series. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3565-3569. IEEE.</p>
<p>Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language processing: A survey. CoRR, abs/2003.08271.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual</p>
<p>Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Georgios Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2104-2115, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Sowmya S. Sundaram, Sairam Gurajada, Marco Fisichella, Deepak P, and Savitha Sam Abraham. 2022. Why are NLP models fumbling at elementary math? A survey of deep learning based word problem solvers. CoRR, abs/2205.15683.</p>
<p>Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Hazarika, and Lawrence Carin. 2020. Methods for numeracypreserving word embeddings. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4742-4753, Online. Association for Computational Linguistics.</p>
<p>Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. 2021. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644-656, Online. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 48894896. Association for Computational Linguistics.</p>
<p>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3277-3287, Online. Association for Computational Linguistics.</p>
<h2>A Code and Data</h2>
<p>Please find our current code and data at https: //github.com/zmy/LUNA</p>
<h2>B Number Statistics</h2>
<p>In TAT-QA and TabFact datasets, $100 \%$ and $94.79 \%$ of the tables have numbers in them. In our number pre-train dataset, $97 \%$ of the tables have numbers in them, $54.94 \%$ of all the 2,471,520 table cells have numbers in them. (While only $17.72 \%$ table cells contain at least 2 numbers.) When analyzing tables at row-level or column levels, numbers are also hard to ignore for $71.36 \%$ of the table columns and $85.25 \%$ of the table rows.</p>
<p>Numbers are shown as strings in tables and texts. After ignoring edge cases, such as scientific notation, the remaining decimal number strings have clear format patterns in them: Most of them are positive numbers ( $0.12 \%$ of all numbers are negative); $91.00 \%$ and $9.00 \%$ of all numbers are integers and float numbers, respectively; $1.51 \%$ and $5.02 \%$ of all numbers have percent " $\%$ " or comma "," character in them, respectively.</p>
<p>Compared to tables, in plain texts numbers are relatively sparse. Out of 59,645 pieces of texts (including queries, paragraphs, and captions) in the three datasets, there are 934,121 English words but there are only 76,532 numbers in them.</p>
<h2>C NumTok Details</h2>
<h2>C. 1 NumTok Character Set</h2>
<p>In this work, we chose a set of characters that suits our downstream tasks the best, which consists of numeric digits $0-9$, the percentage symbol ("\%"), the plus and minus signs (" + ", "-"), the decimal point ("."), and the thousands separator (","). This set can also be updated to suit different downstream tasks.</p>
<table>
<thead>
<tr>
<th>Number shape</th>
<th>Regular Expression</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conventional</td>
<td>$[+-] ?\backslash d+(?\backslash, \backslash d *) ?\%?$</td>
</tr>
<tr>
<td>Thousands-separated</td>
<td>$[+-] ?\backslash d{1,3}\left(?\backslash, \backslash d{3}\right) *\left(?\backslash, \backslash d+)\right.$ ?\%?</td>
</tr>
<tr>
<td>Dot-started Decimal</td>
<td>$[+-] ?\backslash, \backslash d+\%?$</td>
</tr>
</tbody>
</table>
<p>Table 6: NumTok Regular Expressions</p>
<h2>D Training Details</h2>
<h3>D.1 Number Pre-training Datasets</h3>
<p>We first construct a number pre-training dataset (TAT-QA+WikiTables) for RoBERTa and BERT (pre-training from checkpoints). For TAT-QA, questions and paragraphs will be equally treated as text, and for TabFact, only positive statements of a table from the training set (to avoid data leakage) will be considered as text. Totally, 103K table-text pairs are collected. The collected dataset contains 18K tables. Over these tables, the average count of rows and columns is 14.5 and 6.8 respectively, and the average number of non-empty cells is 30.2. The average number of words in the text is 19.3. (see $\S B$ for more statistics. )</p>
<p>For TabBERT (pre-training from scratch), following (Padhi et al., 2021), we create samples by combining 10 contiguous rows in a time-dependent manner and quantize continuous fields to build a local finite vocabulary. All 2.4 M samples are used in pre-training. To avoid data leaking for the downstream fraud detection task, we exclude the label column "isFraud?" in the pre-training corpus.</p>
<h3>D.2 RoBERTa</h3>
<p>For number pre-training, we set $\mathrm{lr}=4 \mathrm{e}-5$, epoch=4, batch-size $=96$, and totally train 4.2 K steps on 32 GPUs. For TAT-QA finetune, we set $\mathrm{lr}=5 \mathrm{e}-6$ for RoBERTa and $\mathrm{lr}=1.5 \mathrm{e}-4$ for the rest of parameters, epoch=25, batch-size=32, and totally train 10 K steps on 8 GPUs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">time</th>
<th style="text-align: center;">time</th>
<th style="text-align: center;">no</th>
<th style="text-align: center;">no data</th>
<th style="text-align: center;">no device</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1984</td>
<td style="text-align: center;">pretty mess</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">1985</td>
<td style="text-align: center;">mechanical emotion</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2018</td>
</tr>
<tr>
<td style="text-align: center;">1986</td>
<td style="text-align: center;">under the influence</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">1986</td>
<td style="text-align: center;">animals</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1988</td>
<td style="text-align: center;">undress</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: TabFact Example: A Wikipedia Table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">time</th>
<th style="text-align: center;">time</th>
<th style="text-align: center;">no</th>
<th style="text-align: center;">no data</th>
<th style="text-align: center;">no device</th>
<th style="text-align: center;">time</th>
<th style="text-align: center;">no</th>
<th style="text-align: center;">no</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1984</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">20110</td>
<td style="text-align: center;">20100</td>
<td style="text-align: center;">2010</td>
<td style="text-align: center;">10100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: A part of attention map between question and table+paragraph of Table 1. "baseline" and "ours" respectively denote row 5 and 1 in Table 3.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: The TSNE visualization of the output of different layers when 0 handles Table 1. Note that the same number strings may have different embeddings due to their different positions in the serialized sequence.</p>
<p>shows that the model cannot understand numbers and correlate numbers to text. When seeing the word "percentage change", the model arbitrarily filters the same-number pairs.</p>
<p>Figure 3 shows the attention map of the last but one transformer layer of RoBERTa with Table 1 as input. As you can see, the original attention map is a mess, which leads to the wrong prediction. Meanwhile, our method focuses on the core information, "3%", which results in the correct answer.</p>
<h3>E.2 Number Embedding</h3>
<p>We apply TSNE visualization to different transformer layers when taking Table 1 as input. As you can see in Figure 4, the deeper the layer is, the better fusion between numbed and word em-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: The TSNE visualization of other rows in Table 3</p>
<p>bedding occurs. Especially in the last layer, these embeddings cluster according to their concept. For example, "2019", "2018", "Year", "May" and "31" belong to the concept of date. "2,082", "2,025", "08", "025", "S" and "Expense" mean the money. On the other hand, in shallow layers like layer 8, low-level and fine-granted knowledge is learned. For instance, the NumBed "2.0", "6.0", and "10.0" are close to each other but the word embedding "2", "6", and "10" are kind of far away.</p>
<p>Figure 5 shows the TSNE visualization of another two rows in Table 3. As shown in the left sub-figure, the cluster is not obvious without LUNA. In the right sub-figure, without intermediate pre-training, the distance between "2,082" and "2,025" is longer than row 0, which means the model cannot understand the numbers as well as row 0.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ encoding the number string with RoBERTa and using the pooled output as the number embedding&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3159 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3159</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3159</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-0db0af0cd3ceb0531a050a03e6ceb849580ff53b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0db0af0cd3ceb0531a050a03e6ceb849580ff53b" target="_blank">Teaching Arithmetic to Small Transformers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective.</p>
                <p><strong>Paper Abstract:</strong> Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3159.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3159.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NanoGPT (scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NanoGPT trained from random initialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small decoder-only transformer (NanoGPT) trained from random init on next-token prediction to learn arithmetic tasks (addition, subtraction, multiplication, sine, sqrt) under different data formats and sampling regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NanoGPT (6-layer decoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (NanoGPT) with 6 self-attention layers, 6 heads, embedding dim 384, ~10.6M parameters, character-level tokenization and absolute positional encodings; trained from random initialization with autoregressive next-token prediction on task-specific datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit integer addition (primary: 3-digit, up to 10-digit), subtraction, 2-digit multiplication, unary functions (sin, sqrt) with fixed precision</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learns arithmetic via data-driven mapping; when data format encourages local LSB-first operations (reverse or scratchpad) the model learns a local, digit-wise algorithmic mapping (carry-propagation style); behavior is partially explainable by low-rank matrix-completion for addition but NanoGPT develops generalization beyond that.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical phase transitions in accuracy as training sample count increases; reverse and scratchpad formats yield rapid learning consistent with a local digit-wise algorithm (Lemma 2); comparison to rank-2 matrix completion shows similar sample-complexity phase transitions (Figure 5) supporting a low-complexity mapping; experiments excluding numbers/digits show NanoGPT generalizes where LRMC cannot (Tables 1 & 2); noise and perturbation tests (Table 3) show differing dependence on prior tokens consistent with local vs global learned functions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>When trained in plain (MSB-first) format, model performance plateaus (~85%) even with many samples, suggesting failure to learn an efficient global algorithm; models struggle with length generalization (fail to generalize to unseen digit lengths), indicating the learned mapping is not a fully general algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Data-format interventions (reverse output, simplified/detailed scratchpad), structured/balanced sampling, fine-tuning from pretrained models, few-shot prompting, adding noise to intermediate steps and auto-regressive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Reverse/scratchpad greatly improved sample efficiency and final accuracy; simplified scratchpad reached 100% on 3-digit addition with ~2000 samples, detailed scratchpad with ~1000 samples, while reverse showed a sharp phase transition ~2500 samples; balanced sampling improved accuracy versus naive uniform sampling; fine-tuning from pretrained models improved few-shot/finetune performance but could suffer format-interference; noisy intermediate labels slowed learning (reduced sample efficiency) but did not prevent eventual convergence to full accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Plain 3-digit addition plateaus ~85% accuracy even with 10k samples; Reverse 3-digit addition achieves ~99.97% overall accuracy (Table 1); Simplified Scratchpad reaches 100% with ~2000 samples; Detailed Scratchpad reaches 100% with ~1000 samples (Figure 6); Exclusion experiments: reverse remained ~100% when excluding 100/200/500 operands, plain remained ~87% overall (Table 1); Noise robustness (Table 3): Exact Acc under random preceding-output perturbation: Plain 49.88%, Reverse 81.26%; under precise +/-1 perturbation: Plain 99.85%, Reverse 90.47%; Relaxed Acc (±1) under random perturbation: Plain 61.55%, Reverse 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Poor length generalization beyond trained digit lengths; plain-format models depend on global input and make brittle predictions (error propagation); subtraction formatting that requires operand-comparison can cause many errors if comparison-step design is poor; reverse formatting less effective or not clearly beneficial for multiplication and for some unary tasks; fine-tuning can cause catastrophic forgetting in plain format when exposed to higher-digit data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors compare learned behavior to the standard digitwise addition algorithm (human LSB-first algorithm) via Lemma 1/2, and to low-rank (rank-2) matrix completion as an analytic proxy; NanoGPT can learn mappings that resemble algorithmic digit-wise addition and also generalizes beyond what LRMC can recover (e.g., missing rows/columns).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3159.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3159.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reverse format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reversed-output training format (LSB-first)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-format intervention that trains the model to emit sum digits from least-significant to most-significant (wraps samples with delimiters) to encourage local carry-based computation aligned with autoregressive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NanoGPT (evaluated with reverse outputs); also used in fine-tuning experiments on larger pretrained models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same transformer architecture as NanoGPT but trained/evaluated using reversed-output sequences where target output digits appear from LSB to MSB and sample wrapped with a delimiter symbol.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition (primary), subtraction (reverse-formatted), evaluated also for multiplication (less effective).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Encourages the model to learn a local per-digit function f(digitA[i], digitB[i], carry_in) for each position, matching standard addition algorithm; reduces need to compute global function for MSB-first outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Lemma 2 formalizes that LSB-first allows access only to local digits and previous carry; empirical results show reverse format requires far fewer samples and exhibits a sharp phase transition to perfect accuracy (e.g., ~2500 samples for 3-digit addition, 99.97% accuracy), and noise tests show robustness to preceding-output noise consistent with local per-digit computation (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Reverse format is not always beneficial (paper notes reverse is not particularly effective for multiplication) and can conflict with pretrained weights when pretraining assumed plain formatting (format mismatch can reduce fine-tuning performance).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Data formatting intervention (training data target order reversal and delimiter usage).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Greatly improved sample efficiency and final accuracy for addition and some other tasks; produced sharp learning phase transitions; made outputs more robust to autoregressive perturbations (often producing outputs within ±1 of true digit).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>3-digit addition: Reverse achieved ~99.97% overall accuracy in experiments; phase transition around 1k–4k samples (notable sharp jump around 2500 samples); robust exact accuracy under random preceding-output perturbation: 81.26% (Table 3), relaxed accuracy 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Less effective for multiplication; can interfere with fine-tuning on pretrained models that expect MSB-first/plain formatting (format mismatch); does not solve length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Aligns with the canonical human algorithm for addition (LSB-first carry propagation); theoretical justification via Lemma 2 and connection to digit-wise algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3159.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3159.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad / Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simplified and Detailed Scratchpad (Chain-of-Thought) training formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training data that includes intermediate digit-wise sums, carries, and natural-language-like step-by-step traces (simplified or detailed), providing intermediate supervision to guide the model's internal computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NanoGPT (primary), applied in fine-tuning contexts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two scratchpad variants: Simplified Scratchpad (records digit-sums and carries per step) and Detailed Scratchpad (more verbose natural-language steps and bookkeeping); used as training targets in autoregressive next-token objective.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition (primary), subtraction, multiplication, and attempts on unary functions (sine, sqrt) with task-specific scratchpad designs.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By decomposing computation into explicit intermediate steps, the model learns a higher-dimensional but simpler-to-learn compositional mapping (digitwise operators + carry), effectively shaping internal representations to implement piecewise/local computations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical sample-efficiency gains: Simplified Scratchpad reached 100% on 3-digit addition with ~2000 samples; Detailed Scratchpad reached 100% with ~1000 samples (Figure 6); Detailed Scratchpad also mitigates catastrophic forgetting when fine-tuning to longer-digit additions (Figure 11); variants for subtraction show that the design of intermediate steps crucially affects accuracy (Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>For some non-algorithmic unary tasks (sine, sqrt), scratchpad provides limited benefit because the intermediate steps (Taylor series, Newton iterations) are not simpler than the original operation; scratchpad is also less token-efficient although sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Training-data augmentation / intermediate supervision (Chain-of-Thought style scratchpads); also tested with noisy intermediate labels.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved sample complexity, final accuracy, and convergence speed on arithmetic tasks where steps are simple (addition/subtraction/multiplication); careful step-design (e.g., subtraction Version 1 vs Version 2) matters; noisy intermediate steps degrade sample efficiency but models eventually recover with sufficient data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simplified Scratchpad: 100% accuracy on 3-digit addition with ~2000 samples; Detailed Scratchpad: 100% with ~1000 samples (Figure 6); for subtraction and multiplication, detailed scratchpad outperformed plain/reverse in sample efficiency (Figure 15); for sine and sqrt, gains were small.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Requires careful manual design of intermediate steps (bad designs reduce accuracy); noisy intermediate steps slow learning; not token-efficient; limited benefit for functions whose intermediate steps are comparably complex (e.g., sine, sqrt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Analogous to teaching humans step-by-step procedures; similar in spirit to presenting explicit algorithms (e.g., digit-wise addition) and to algorithmic reasoning through intermediate symbolic state.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3159.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3159.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LRMC equivalence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Matrix Completion (LRMC) explanation for addition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Theoretical/empirical framing that learning an n-digit addition map can be cast as completing an n×n rank-2 matrix (i+j mapping), offering an explanation for observed sample-complexity phase transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conceptual/theoretical framing (compared experimentally to NanoGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An addition-table matrix M (i,j) -> i+j decomposes as N 1^T + 1 N^T (rank-2); low-rank matrix completion algorithms yield sharp sample-complexity phase transitions when recovering such matrices from random revealed entries.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>2-digit addition conceptualized as matrix entries; generalizes to n-digit addition mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Addition mapping has low algebraic complexity (rank-2) so recovering missing outputs from random samples exhibits an O(n) sample threshold; observed sharp phase transitions in NanoGPT mirror LRMC behavior, suggesting similar effective complexity in learned mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 5 shows iterative LRMC algorithm and NanoGPT both undergo sharp phase transitions at comparable numbers of revealed entries/samples; theoretical results (Recht 2011 style) align with empirical sample thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>LRMC cannot recover when entire rows/columns are missing (empty rows/columns), yet NanoGPT can generalize to excluded numbers/digits in training—demonstrating that NanoGPT's learned mapping goes beyond LRMC's capabilities (Table 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Analytic framing and comparison experiment (no direct mechanical intervention on model, but used to explain behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Provides an explanation for sharp empirical phase transitions in sample complexity for addition, but is incomplete as it doesn't capture NanoGPT generalization beyond LRMC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LRMC algorithms show a phase transition at ~O(n) revealed entries for exact recovery (empirically similar to NanoGPT's transition near ~1500 samples for 2-digit addition with n=100 in Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>LRMC fails on structured missingness (entire excluded numbers/digits correspond to empty rows/columns and are unrecoverable using pure LRMC), whereas NanoGPT can still generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>LRMC is a symbolic/matrix-analytic explanation (not a cognitive/computational human algorithm), used as a theoretical baseline for sample complexity rather than a cognitive model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3159.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3159.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Balanced sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured balanced sampling of digits and carry counts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-sampling intervention that upweights low-digit operands and balances examples by number of carry operations to correct natural skew in uniform sampling and improve learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NanoGPT (training datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training dataset construction that ensures coverage across operand digit-lengths (1..n) and equalizes distribution over carry counts (0..n), e.g., for 3-digit addition include all 1-digit pairs, more 2-digit pairs and many 3-digit pairs in a balanced split.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition (especially multi-digit), evaluated primarily on 3-digit experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Mitigates dataset skew so the model sees sufficient examples of rare cases (short operands, extreme carry counts), enabling the model to learn the mapping across digit-position regimes rather than overfitting to common patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 3 shows balanced-digit and balanced-carry sampling outperform random uniform sampling on 3-digit addition; random sampling performed poorly even on 2-digit addition due to skew.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct counter-evidence in paper; balanced sampling increases sample diversity but does not address length generalization beyond trained lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Data-sampling intervention (dataset construction)</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved accuracy across the board for addition; improved sample efficiency and coverage of rare digit/carry cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative improvements shown in Figure 3 (no single summary number provided in text); authors set default datasets to be both balanced digits and balanced carry-ons for remaining experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Does not solve algorithmic length generalization; balancing is a heuristic requiring manual design per digit-range.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Analogous to curriculum/data-augmentation strategies used to expose learners to edge cases; not directly compared to symbolic baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3159.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3159.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining & fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of language model pretraining and supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments that examine fine-tuning pretrained GPT-2 / GPT-3 models (and NanoGPT pretrained checkpoints) on arithmetic datasets and study zero-shot, few-shot prompting and format interference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained GPT-2, GPT-3 (davinci) and fine-tuned NanoGPT variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained language models (GPT-2 family; GPT-3 davinci used for supervised fine-tuning experiments) that have prior language skills from large-scale pretraining and are then fine-tuned on arithmetic data in various formats.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same set: addition, subtraction, multiplication, sine, sqrt; few-shot/zero-shot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pretraining provides prior linguistic/representational structure that can help arithmetic after limited fine-tuning; however, mismatch between pretraining-format expectations and finetune data format (e.g., reverse vs plain) can reduce performance (interference).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Authors report pretrained models show poor zero-shot arithmetic but finetuning yields reasonable performance with few samples; format mismatch can hurt (reverse formatting can interfere if pretraining saw plain format operations); section 10 discusses effect of scale and pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Pretraining is not necessary for arithmetic emergence in small models trained from scratch with proper data formatting (reverse/scratchpad), showing pretraining is helpful but not required.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning of pretrained LMs; few-shot prompting; format-aware fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Finetuning boosts arithmetic performance for pretrained models given small supervised datasets; format mismatch between pretraining and finetuning can cause degraded accuracy; scale helps but is not necessary for arithmetic capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No comprehensive numeric table provided for GPT-2/3 finetuning in the excerpt; authors qualitatively report reasonable performance after finetuning and that 1-shot prompting gives large improvements but more shots do not always help.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Format interference (fine-tuning with a different output ordering than pretraining can reduce accuracy); limited length generalization persists despite pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Pretraining is likened to prior skills that humans bring; not directly compared to symbolic calculators but contrasted with models trained from scratch where formatting and sampling suffice to elicit arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3159.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3159.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noisy-scratchpad / Output perturbation tests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noisy intermediate-step and autoregressive output perturbation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Interventions introducing random/noisy intermediate labels in scratchpads and perturbations to earlier generated output tokens to probe reliance on intermediate steps and autoregressive error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NanoGPT (trained on various formats and tested under noisy conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models trained with correct scratchpad intermediate steps vs scratchpads where A (digit sums) and/or C (carries) are randomized; also models trained without noise are tested with noisy preceding output tokens during autoregressive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction (noise experiments primarily on addition).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Tests whether model uses scratchpad as true intermediate reasoning or merely as extra expressivity; tests whether predicted digit i depends on operands vs previously generated tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Noisy scratchpad experiments: noisy A/C labels reduce sample efficiency but with enough training data the model reaches full accuracy, indicating model can both leverage and ignore intermediate steps; perturbation tests show reverse-trained models rely less on preceding (higher-significance) output tokens and are robust to random perturbation (Reverse exact acc 81.26% vs Plain 49.88%), consistent with local-digit mechanism (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Noisy labels do not completely prevent learning (model eventually recovers), which suggests the scratchpad is not strictly required for eventual acquisition of mapping when data volume is large.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Label-noise injection in scratchpad; adversarial/noisy preceding-output injection during autoregressive prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Noisy scratchpad slows learning (worse sample efficiency); autoregressive noise exposes format-specific robustness differences (reverse more robust than plain to random perturbation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Noisy-scratchpad: qualitative degradation in sample efficiency (Figure 9) but eventual 100% with large data; Output perturbation (Table 3) exact acc under random perturbation: Plain 49.88% vs Reverse 81.26%; relaxed acc under random perturbation: Plain 61.55% vs Reverse 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Autoregressive generation error propagation is severe for plain/MSB-first format; noisy intermediate steps degrade early learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Findings align with the intuition that giving step-by-step supervision helps novices (humans) learn algorithms faster, and that noise in steps impairs learning rate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3159.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3159.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Length generalization failure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limited length generalization across digit lengths</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed limitation: models trained on some digit-lengths fail to generalize to unseen digit-lengths, indicating learned mapping is length-constrained rather than a fully algorithmic procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NanoGPT and other transformer variants (GPT-2/3 in fine-tuning analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformers trained on datasets covering a set of digit lengths but with specific lengths omitted as held-out show poor performance on the omitted lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition across varying digit lengths (1..k digits), evaluated for generalization to omitted lengths and for k -> k+1 transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>The learned mapping is largely a mapping function fitted to the distribution of trained lengths; when a length is not seen it is not inferred via a length-agnostic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Authors report models struggle if a specific n-digit length was excluded during training (they cannot accurately calculate that missing length) and length generalization beyond trained lengths is difficult; fine-tuning to k+1 digit examples can cause forgetting in plain format, whereas scratchpad helps incremental learning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some fine-tuning experiments (reverse/scratchpad) show more stable incremental learning to larger k with modest numbers of new samples, suggesting partial transfer within formats, but full general algorithmic generalization was not observed.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Training-set design and fine-tuning experiments testing generalization</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Scratchpad/reverse formats help make k->k+1 transfer easier (require fewer new samples) but do not produce strong zero-shot length generalization to totally unseen digit lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: plain format requires increasing number of samples to learn k+1 digits; reverse/scratchpad require relatively consistent numbers of additional samples (1000–5000) across digit increases (Figure 12). Exact failure rates on unseen lengths are not given as single numbers in excerpt but described as 'struggles significantly.'</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Cannot reliably extrapolate arithmetic algorithm to longer or omitted digit lengths; models learn a mapping tied to training-length distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Contrasts with algorithmic symbolic methods and some prior neural algorithmic models (e.g., Neural GPUs) that have shown stronger length generalization; suggests transformer-learned mappings differ from explicit algorithmic implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Arithmetic to Small Transformers', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Learning to Execute <em>(Rating: 2)</em></li>
                <li>Neural GPUs <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3159",
    "paper_id": "paper-0db0af0cd3ceb0531a050a03e6ceb849580ff53b",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "NanoGPT (scratch)",
            "name_full": "NanoGPT trained from random initialization",
            "brief_description": "A small decoder-only transformer (NanoGPT) trained from random init on next-token prediction to learn arithmetic tasks (addition, subtraction, multiplication, sine, sqrt) under different data formats and sampling regimes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NanoGPT (6-layer decoder-only transformer)",
            "model_description": "Decoder-only transformer (NanoGPT) with 6 self-attention layers, 6 heads, embedding dim 384, ~10.6M parameters, character-level tokenization and absolute positional encodings; trained from random initialization with autoregressive next-token prediction on task-specific datasets.",
            "arithmetic_task_type": "Multi-digit integer addition (primary: 3-digit, up to 10-digit), subtraction, 2-digit multiplication, unary functions (sin, sqrt) with fixed precision",
            "reported_mechanism": "Learns arithmetic via data-driven mapping; when data format encourages local LSB-first operations (reverse or scratchpad) the model learns a local, digit-wise algorithmic mapping (carry-propagation style); behavior is partially explainable by low-rank matrix-completion for addition but NanoGPT develops generalization beyond that.",
            "evidence_for_mechanism": "Empirical phase transitions in accuracy as training sample count increases; reverse and scratchpad formats yield rapid learning consistent with a local digit-wise algorithm (Lemma 2); comparison to rank-2 matrix completion shows similar sample-complexity phase transitions (Figure 5) supporting a low-complexity mapping; experiments excluding numbers/digits show NanoGPT generalizes where LRMC cannot (Tables 1 & 2); noise and perturbation tests (Table 3) show differing dependence on prior tokens consistent with local vs global learned functions.",
            "evidence_against_mechanism": "When trained in plain (MSB-first) format, model performance plateaus (~85%) even with many samples, suggesting failure to learn an efficient global algorithm; models struggle with length generalization (fail to generalize to unseen digit lengths), indicating the learned mapping is not a fully general algorithm.",
            "intervention_type": "Data-format interventions (reverse output, simplified/detailed scratchpad), structured/balanced sampling, fine-tuning from pretrained models, few-shot prompting, adding noise to intermediate steps and auto-regressive outputs.",
            "effect_of_intervention": "Reverse/scratchpad greatly improved sample efficiency and final accuracy; simplified scratchpad reached 100% on 3-digit addition with ~2000 samples, detailed scratchpad with ~1000 samples, while reverse showed a sharp phase transition ~2500 samples; balanced sampling improved accuracy versus naive uniform sampling; fine-tuning from pretrained models improved few-shot/finetune performance but could suffer format-interference; noisy intermediate labels slowed learning (reduced sample efficiency) but did not prevent eventual convergence to full accuracy.",
            "performance_metrics": "Plain 3-digit addition plateaus ~85% accuracy even with 10k samples; Reverse 3-digit addition achieves ~99.97% overall accuracy (Table 1); Simplified Scratchpad reaches 100% with ~2000 samples; Detailed Scratchpad reaches 100% with ~1000 samples (Figure 6); Exclusion experiments: reverse remained ~100% when excluding 100/200/500 operands, plain remained ~87% overall (Table 1); Noise robustness (Table 3): Exact Acc under random preceding-output perturbation: Plain 49.88%, Reverse 81.26%; under precise +/-1 perturbation: Plain 99.85%, Reverse 90.47%; Relaxed Acc (±1) under random perturbation: Plain 61.55%, Reverse 100%.",
            "notable_failure_modes": "Poor length generalization beyond trained digit lengths; plain-format models depend on global input and make brittle predictions (error propagation); subtraction formatting that requires operand-comparison can cause many errors if comparison-step design is poor; reverse formatting less effective or not clearly beneficial for multiplication and for some unary tasks; fine-tuning can cause catastrophic forgetting in plain format when exposed to higher-digit data.",
            "comparison_to_humans_or_symbolic": "Authors compare learned behavior to the standard digitwise addition algorithm (human LSB-first algorithm) via Lemma 1/2, and to low-rank (rank-2) matrix completion as an analytic proxy; NanoGPT can learn mappings that resemble algorithmic digit-wise addition and also generalizes beyond what LRMC can recover (e.g., missing rows/columns).",
            "uuid": "e3159.0",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Reverse format",
            "name_full": "Reversed-output training format (LSB-first)",
            "brief_description": "A data-format intervention that trains the model to emit sum digits from least-significant to most-significant (wraps samples with delimiters) to encourage local carry-based computation aligned with autoregressive generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NanoGPT (evaluated with reverse outputs); also used in fine-tuning experiments on larger pretrained models",
            "model_description": "Same transformer architecture as NanoGPT but trained/evaluated using reversed-output sequences where target output digits appear from LSB to MSB and sample wrapped with a delimiter symbol.",
            "arithmetic_task_type": "Multi-digit addition (primary), subtraction (reverse-formatted), evaluated also for multiplication (less effective).",
            "reported_mechanism": "Encourages the model to learn a local per-digit function f(digitA[i], digitB[i], carry_in) for each position, matching standard addition algorithm; reduces need to compute global function for MSB-first outputs.",
            "evidence_for_mechanism": "Lemma 2 formalizes that LSB-first allows access only to local digits and previous carry; empirical results show reverse format requires far fewer samples and exhibits a sharp phase transition to perfect accuracy (e.g., ~2500 samples for 3-digit addition, 99.97% accuracy), and noise tests show robustness to preceding-output noise consistent with local per-digit computation (Table 3).",
            "evidence_against_mechanism": "Reverse format is not always beneficial (paper notes reverse is not particularly effective for multiplication) and can conflict with pretrained weights when pretraining assumed plain formatting (format mismatch can reduce fine-tuning performance).",
            "intervention_type": "Data formatting intervention (training data target order reversal and delimiter usage).",
            "effect_of_intervention": "Greatly improved sample efficiency and final accuracy for addition and some other tasks; produced sharp learning phase transitions; made outputs more robust to autoregressive perturbations (often producing outputs within ±1 of true digit).",
            "performance_metrics": "3-digit addition: Reverse achieved ~99.97% overall accuracy in experiments; phase transition around 1k–4k samples (notable sharp jump around 2500 samples); robust exact accuracy under random preceding-output perturbation: 81.26% (Table 3), relaxed accuracy 100%.",
            "notable_failure_modes": "Less effective for multiplication; can interfere with fine-tuning on pretrained models that expect MSB-first/plain formatting (format mismatch); does not solve length generalization.",
            "comparison_to_humans_or_symbolic": "Aligns with the canonical human algorithm for addition (LSB-first carry propagation); theoretical justification via Lemma 2 and connection to digit-wise algorithmic reasoning.",
            "uuid": "e3159.1",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Scratchpad / Chain-of-Thought",
            "name_full": "Simplified and Detailed Scratchpad (Chain-of-Thought) training formats",
            "brief_description": "Training data that includes intermediate digit-wise sums, carries, and natural-language-like step-by-step traces (simplified or detailed), providing intermediate supervision to guide the model's internal computation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NanoGPT (primary), applied in fine-tuning contexts",
            "model_description": "Two scratchpad variants: Simplified Scratchpad (records digit-sums and carries per step) and Detailed Scratchpad (more verbose natural-language steps and bookkeeping); used as training targets in autoregressive next-token objective.",
            "arithmetic_task_type": "Addition (primary), subtraction, multiplication, and attempts on unary functions (sine, sqrt) with task-specific scratchpad designs.",
            "reported_mechanism": "By decomposing computation into explicit intermediate steps, the model learns a higher-dimensional but simpler-to-learn compositional mapping (digitwise operators + carry), effectively shaping internal representations to implement piecewise/local computations.",
            "evidence_for_mechanism": "Empirical sample-efficiency gains: Simplified Scratchpad reached 100% on 3-digit addition with ~2000 samples; Detailed Scratchpad reached 100% with ~1000 samples (Figure 6); Detailed Scratchpad also mitigates catastrophic forgetting when fine-tuning to longer-digit additions (Figure 11); variants for subtraction show that the design of intermediate steps crucially affects accuracy (Figure 8).",
            "evidence_against_mechanism": "For some non-algorithmic unary tasks (sine, sqrt), scratchpad provides limited benefit because the intermediate steps (Taylor series, Newton iterations) are not simpler than the original operation; scratchpad is also less token-efficient although sample-efficient.",
            "intervention_type": "Training-data augmentation / intermediate supervision (Chain-of-Thought style scratchpads); also tested with noisy intermediate labels.",
            "effect_of_intervention": "Improved sample complexity, final accuracy, and convergence speed on arithmetic tasks where steps are simple (addition/subtraction/multiplication); careful step-design (e.g., subtraction Version 1 vs Version 2) matters; noisy intermediate steps degrade sample efficiency but models eventually recover with sufficient data.",
            "performance_metrics": "Simplified Scratchpad: 100% accuracy on 3-digit addition with ~2000 samples; Detailed Scratchpad: 100% with ~1000 samples (Figure 6); for subtraction and multiplication, detailed scratchpad outperformed plain/reverse in sample efficiency (Figure 15); for sine and sqrt, gains were small.",
            "notable_failure_modes": "Requires careful manual design of intermediate steps (bad designs reduce accuracy); noisy intermediate steps slow learning; not token-efficient; limited benefit for functions whose intermediate steps are comparably complex (e.g., sine, sqrt).",
            "comparison_to_humans_or_symbolic": "Analogous to teaching humans step-by-step procedures; similar in spirit to presenting explicit algorithms (e.g., digit-wise addition) and to algorithmic reasoning through intermediate symbolic state.",
            "uuid": "e3159.2",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LRMC equivalence",
            "name_full": "Low-Rank Matrix Completion (LRMC) explanation for addition",
            "brief_description": "Theoretical/empirical framing that learning an n-digit addition map can be cast as completing an n×n rank-2 matrix (i+j mapping), offering an explanation for observed sample-complexity phase transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Conceptual/theoretical framing (compared experimentally to NanoGPT)",
            "model_description": "An addition-table matrix M (i,j) -&gt; i+j decomposes as N 1^T + 1 N^T (rank-2); low-rank matrix completion algorithms yield sharp sample-complexity phase transitions when recovering such matrices from random revealed entries.",
            "arithmetic_task_type": "2-digit addition conceptualized as matrix entries; generalizes to n-digit addition mapping.",
            "reported_mechanism": "Addition mapping has low algebraic complexity (rank-2) so recovering missing outputs from random samples exhibits an O(n) sample threshold; observed sharp phase transitions in NanoGPT mirror LRMC behavior, suggesting similar effective complexity in learned mapping.",
            "evidence_for_mechanism": "Figure 5 shows iterative LRMC algorithm and NanoGPT both undergo sharp phase transitions at comparable numbers of revealed entries/samples; theoretical results (Recht 2011 style) align with empirical sample thresholds.",
            "evidence_against_mechanism": "LRMC cannot recover when entire rows/columns are missing (empty rows/columns), yet NanoGPT can generalize to excluded numbers/digits in training—demonstrating that NanoGPT's learned mapping goes beyond LRMC's capabilities (Table 1 and 2).",
            "intervention_type": "Analytic framing and comparison experiment (no direct mechanical intervention on model, but used to explain behavior).",
            "effect_of_intervention": "Provides an explanation for sharp empirical phase transitions in sample complexity for addition, but is incomplete as it doesn't capture NanoGPT generalization beyond LRMC.",
            "performance_metrics": "LRMC algorithms show a phase transition at ~O(n) revealed entries for exact recovery (empirically similar to NanoGPT's transition near ~1500 samples for 2-digit addition with n=100 in Figure 5).",
            "notable_failure_modes": "LRMC fails on structured missingness (entire excluded numbers/digits correspond to empty rows/columns and are unrecoverable using pure LRMC), whereas NanoGPT can still generalize.",
            "comparison_to_humans_or_symbolic": "LRMC is a symbolic/matrix-analytic explanation (not a cognitive/computational human algorithm), used as a theoretical baseline for sample complexity rather than a cognitive model.",
            "uuid": "e3159.3",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Balanced sampling",
            "name_full": "Structured balanced sampling of digits and carry counts",
            "brief_description": "A data-sampling intervention that upweights low-digit operands and balances examples by number of carry operations to correct natural skew in uniform sampling and improve learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NanoGPT (training datasets)",
            "model_description": "Training dataset construction that ensures coverage across operand digit-lengths (1..n) and equalizes distribution over carry counts (0..n), e.g., for 3-digit addition include all 1-digit pairs, more 2-digit pairs and many 3-digit pairs in a balanced split.",
            "arithmetic_task_type": "Addition (especially multi-digit), evaluated primarily on 3-digit experiments.",
            "reported_mechanism": "Mitigates dataset skew so the model sees sufficient examples of rare cases (short operands, extreme carry counts), enabling the model to learn the mapping across digit-position regimes rather than overfitting to common patterns.",
            "evidence_for_mechanism": "Figure 3 shows balanced-digit and balanced-carry sampling outperform random uniform sampling on 3-digit addition; random sampling performed poorly even on 2-digit addition due to skew.",
            "evidence_against_mechanism": "No direct counter-evidence in paper; balanced sampling increases sample diversity but does not address length generalization beyond trained lengths.",
            "intervention_type": "Data-sampling intervention (dataset construction)",
            "effect_of_intervention": "Improved accuracy across the board for addition; improved sample efficiency and coverage of rare digit/carry cases.",
            "performance_metrics": "Qualitative improvements shown in Figure 3 (no single summary number provided in text); authors set default datasets to be both balanced digits and balanced carry-ons for remaining experiments.",
            "notable_failure_modes": "Does not solve algorithmic length generalization; balancing is a heuristic requiring manual design per digit-range.",
            "comparison_to_humans_or_symbolic": "Analogous to curriculum/data-augmentation strategies used to expose learners to edge cases; not directly compared to symbolic baselines.",
            "uuid": "e3159.4",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Pretraining & fine-tuning",
            "name_full": "Effect of language model pretraining and supervised fine-tuning",
            "brief_description": "Experiments that examine fine-tuning pretrained GPT-2 / GPT-3 models (and NanoGPT pretrained checkpoints) on arithmetic datasets and study zero-shot, few-shot prompting and format interference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pretrained GPT-2, GPT-3 (davinci) and fine-tuned NanoGPT variants",
            "model_description": "Pretrained language models (GPT-2 family; GPT-3 davinci used for supervised fine-tuning experiments) that have prior language skills from large-scale pretraining and are then fine-tuned on arithmetic data in various formats.",
            "arithmetic_task_type": "Same set: addition, subtraction, multiplication, sine, sqrt; few-shot/zero-shot evaluations.",
            "reported_mechanism": "Pretraining provides prior linguistic/representational structure that can help arithmetic after limited fine-tuning; however, mismatch between pretraining-format expectations and finetune data format (e.g., reverse vs plain) can reduce performance (interference).",
            "evidence_for_mechanism": "Authors report pretrained models show poor zero-shot arithmetic but finetuning yields reasonable performance with few samples; format mismatch can hurt (reverse formatting can interfere if pretraining saw plain format operations); section 10 discusses effect of scale and pretraining.",
            "evidence_against_mechanism": "Pretraining is not necessary for arithmetic emergence in small models trained from scratch with proper data formatting (reverse/scratchpad), showing pretraining is helpful but not required.",
            "intervention_type": "Fine-tuning of pretrained LMs; few-shot prompting; format-aware fine-tuning.",
            "effect_of_intervention": "Finetuning boosts arithmetic performance for pretrained models given small supervised datasets; format mismatch between pretraining and finetuning can cause degraded accuracy; scale helps but is not necessary for arithmetic capabilities.",
            "performance_metrics": "No comprehensive numeric table provided for GPT-2/3 finetuning in the excerpt; authors qualitatively report reasonable performance after finetuning and that 1-shot prompting gives large improvements but more shots do not always help.",
            "notable_failure_modes": "Format interference (fine-tuning with a different output ordering than pretraining can reduce accuracy); limited length generalization persists despite pretraining.",
            "comparison_to_humans_or_symbolic": "Pretraining is likened to prior skills that humans bring; not directly compared to symbolic calculators but contrasted with models trained from scratch where formatting and sampling suffice to elicit arithmetic.",
            "uuid": "e3159.5",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Noisy-scratchpad / Output perturbation tests",
            "name_full": "Noisy intermediate-step and autoregressive output perturbation experiments",
            "brief_description": "Interventions introducing random/noisy intermediate labels in scratchpads and perturbations to earlier generated output tokens to probe reliance on intermediate steps and autoregressive error propagation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NanoGPT (trained on various formats and tested under noisy conditions)",
            "model_description": "Models trained with correct scratchpad intermediate steps vs scratchpads where A (digit sums) and/or C (carries) are randomized; also models trained without noise are tested with noisy preceding output tokens during autoregressive generation.",
            "arithmetic_task_type": "Addition and subtraction (noise experiments primarily on addition).",
            "reported_mechanism": "Tests whether model uses scratchpad as true intermediate reasoning or merely as extra expressivity; tests whether predicted digit i depends on operands vs previously generated tokens.",
            "evidence_for_mechanism": "Noisy scratchpad experiments: noisy A/C labels reduce sample efficiency but with enough training data the model reaches full accuracy, indicating model can both leverage and ignore intermediate steps; perturbation tests show reverse-trained models rely less on preceding (higher-significance) output tokens and are robust to random perturbation (Reverse exact acc 81.26% vs Plain 49.88%), consistent with local-digit mechanism (Table 3).",
            "evidence_against_mechanism": "Noisy labels do not completely prevent learning (model eventually recovers), which suggests the scratchpad is not strictly required for eventual acquisition of mapping when data volume is large.",
            "intervention_type": "Label-noise injection in scratchpad; adversarial/noisy preceding-output injection during autoregressive prediction.",
            "effect_of_intervention": "Noisy scratchpad slows learning (worse sample efficiency); autoregressive noise exposes format-specific robustness differences (reverse more robust than plain to random perturbation).",
            "performance_metrics": "Noisy-scratchpad: qualitative degradation in sample efficiency (Figure 9) but eventual 100% with large data; Output perturbation (Table 3) exact acc under random perturbation: Plain 49.88% vs Reverse 81.26%; relaxed acc under random perturbation: Plain 61.55% vs Reverse 100%.",
            "notable_failure_modes": "Autoregressive generation error propagation is severe for plain/MSB-first format; noisy intermediate steps degrade early learning.",
            "comparison_to_humans_or_symbolic": "Findings align with the intuition that giving step-by-step supervision helps novices (humans) learn algorithms faster, and that noise in steps impairs learning rate.",
            "uuid": "e3159.6",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Length generalization failure",
            "name_full": "Limited length generalization across digit lengths",
            "brief_description": "Observed limitation: models trained on some digit-lengths fail to generalize to unseen digit-lengths, indicating learned mapping is length-constrained rather than a fully algorithmic procedure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NanoGPT and other transformer variants (GPT-2/3 in fine-tuning analyses)",
            "model_description": "Transformers trained on datasets covering a set of digit lengths but with specific lengths omitted as held-out show poor performance on the omitted lengths.",
            "arithmetic_task_type": "Addition across varying digit lengths (1..k digits), evaluated for generalization to omitted lengths and for k -&gt; k+1 transfer.",
            "reported_mechanism": "The learned mapping is largely a mapping function fitted to the distribution of trained lengths; when a length is not seen it is not inferred via a length-agnostic algorithm.",
            "evidence_for_mechanism": "Authors report models struggle if a specific n-digit length was excluded during training (they cannot accurately calculate that missing length) and length generalization beyond trained lengths is difficult; fine-tuning to k+1 digit examples can cause forgetting in plain format, whereas scratchpad helps incremental learning.",
            "evidence_against_mechanism": "Some fine-tuning experiments (reverse/scratchpad) show more stable incremental learning to larger k with modest numbers of new samples, suggesting partial transfer within formats, but full general algorithmic generalization was not observed.",
            "intervention_type": "Training-set design and fine-tuning experiments testing generalization",
            "effect_of_intervention": "Scratchpad/reverse formats help make k-&gt;k+1 transfer easier (require fewer new samples) but do not produce strong zero-shot length generalization to totally unseen digit lengths.",
            "performance_metrics": "Qualitative: plain format requires increasing number of samples to learn k+1 digits; reverse/scratchpad require relatively consistent numbers of additional samples (1000–5000) across digit increases (Figure 12). Exact failure rates on unseen lengths are not given as single numbers in excerpt but described as 'struggles significantly.'",
            "notable_failure_modes": "Cannot reliably extrapolate arithmetic algorithm to longer or omitted digit lengths; models learn a mapping tied to training-length distribution.",
            "comparison_to_humans_or_symbolic": "Contrasts with algorithmic symbolic methods and some prior neural algorithmic models (e.g., Neural GPUs) that have shown stronger length generalization; suggests transformer-learned mappings differ from explicit algorithmic implementations.",
            "uuid": "e3159.7",
            "source_info": {
                "paper_title": "Teaching Arithmetic to Small Transformers",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Learning to Execute",
            "rating": 2
        },
        {
            "paper_title": "Neural GPUs",
            "rating": 2
        }
    ],
    "cost": 0.021409749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Teaching Arithmetic to Small Transformers</h1>
<p>Nayoung Lee*<br>University of Wisconsin-Madison<br>nayoung.lee@wisc.edu<br>Jason D. Lee<br>Princeton University<br>jasonlee@princeton.edu</p>
<p>Kartik Sreenivasan*<br>University of Wisconsin-Madison<br>ksreenivasa2@wisc.edu<br>Kangwook Lee<br>University of Wisconsin-Madison<br>kangwook.lee@wisc.edu<br>Dimitris Papailiopoulos<br>University of Wisconsin-Madison<br>dimitris@papail.io</p>
<h4>Abstract</h4>
<p>Large language models like GPT-4 exhibit emergent capabilities across generalpurpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the nexttoken prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities. ${ }^{2}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 3
2 Related Works ..... 4
3 Preliminaries and Experimental Setup ..... 5
4 Learning Addition in Small Models ..... 7
4.1 Training on Conventional Data ..... 7
4.2 Reversing the Output ..... 8
5 Connection to Low-Rank Matrix Completion ..... 8
5.1 Addition Tables are Rank-2 Matrices ..... 9
5.2 NanoGPT Generalizes better than Matrix Completion solutions ..... 9
6 The power of Chain-of-Thought: Incorporating Intermediate Steps in Training Data ..... 11
6.1 Training on Chain-of-Thought Data ..... 11
6.2 The Importance of Intermediate Step Design: Subtraction ..... 11
6.3 The Effect of Noisy Inputs on Accuracy ..... 13
7 Extending to Longer Digit Addition ..... 15
7.1 Training from Random Initialization ..... 15
7.2 Fine-Tuning from Pretrained Models ..... 16
7.3 Impact of Formats on Fine-Tuning ..... 17
8 Teaching Arithmetic Operations Beyond Addition ..... 18
8.1 Extended Arithmetic Operations ..... 19
8.2 Jointly Training on All Five Arithmetic Tasks ..... 20
9 Mixing Shakespeare with Arithmetic Data ..... 21
10 Fine-tuning, Scaling, and Pretraining in Larger Models ..... 23
11 Token Efficiency Across Data Formats ..... 26
12 Length Generalization ..... 27
13 Limitations ..... 30
14 Conclusion ..... 30
Appendix ..... 34</p>
<p>1 Introduction</p>
<p>Large language models like GPT-3/4, PaLM, LaMDA Brown et al., 2020, Chowdhery et al., 2022, Thoppilan et al., 2022 have demonstrated general-purpose properties, often referred to as <em>emergent abilities</em> Wei et al., 2022b, for a wide range of downstream tasks like language and code translation, compositional reasoning, and basic arithmetic operations Webb et al., 2022, Nye et al., 2021, Wei et al., 2022c, Shi et al., 2022, Wang et al., 2022, Srivastava et al., 2022, Chen et al., 2023. What is perhaps surprising, is that these tasks are not explicitly encoded in the model’s training objective, which typically is an auto-regressive, next-token-prediction loss.</p>
<p>Prior research has delved into exploring these capabilities and how they emerge as the scale and of training compute, type of data, and model size vary Wei et al., 2022b, Chung et al., 2022, Tay et al., 2022. Untangling the factors, however, remains challenging due to the data complexity and the variety of tasks examined. Driven by the curiosity to understand the factors that elicit these capabilities in next-token predictors, we set out to pinpoint the key contributors that accelerate the emergence of such abilities. These contributors may include the format and scale of data, model scale, the presence of pre-training, and the manner of prompting.</p>
<p>To provide a more precise examination of these factors, our study is conducted in a controlled setting: we focus on teaching arithmetic to small transformer models, such as NanoGPT and GPT-2, when trained from random init. Starting with a model of 10.6 million parameters and scaling up to 124 million parameters, we use the standard autoregressive next-token prediction loss. Our objective is to understand how these models can efficiently learn basic arithmetic operations like addition, subtraction, multiplication, square root, and sine, thereby providing us with a clearer lens through which to view the elicitation of emergent abilities. Below, we summarize our findings.</p>
<p>Data format and sampling matters. We first observe that teaching a model addition (or any other operation) using standard addition samples, i.e., $\mathrm{A}<em 2="2">{3}\mathrm{A}</em>}\mathrm{A<em 3="3">{1}+\mathrm{B}</em>}\mathrm{B<em 1="1">{1}\mathrm{B}</em>}=\mathrm{C<em 2="2">{3}\mathrm{C}</em>}\mathrm{C<em 3="3">{1}$, is suboptimal, as it requires the model to evaluate the most significant digit $\mathrm{C}</em>}$ of the result first, which depends globally on all the digits of the two summands. By training on samples with reversed results, i.e., $\mathrm{A<em 2="2">{3}\mathrm{A}</em>}\mathrm{A<em 3="3">{1}+\mathrm{B}</em>}\mathrm{B<em 1="1">{1}\mathrm{B}</em>}=\mathrm{C<em 2="2">{1}\mathrm{C}</em>$, we enable the model to learn a simpler function, significantly improving sample complexity. Additionally, balanced sampling of different “variations” of addition, based on the number of carries and digits involved, further enhances learning. Even in this simple setting, we observe relatively sharp phase transitions from 0 to 100% accuracy as a function of the size of the training data. Although this may seem surprising, we observe that learning an addition map on $n$ digits from random samples is equivalent to completing a low-rank matrix. This connection allows us to offer a reasonable explanation for such phase transitions.}\mathrm{C}_{3</p>
<p>Chain-of-thought data during training. Building on these findings, we then explore the potential benefits of chain-of-thought (CoT) data during training. This format includes step-by-step operations and intermediate results, allowing the model to learn the individual components of complex tasks. This format is directly borrowed from related literature, e.g., Ling et al., 2017, Nye et al., 2021, Wei et al., 2022c, Zhou et al., 2022a, Anil et al., 2022, Zhou et al., 2022b. We found that CoT-type training data significantly improved learning in terms of both sample complexity and accuracy in agreement with CoT fine-tuning literature Nye et al., 2021, Chung et al., 2022, though our observation holds <em>even in the absence of language pretraining.</em> We conjecture that this is because breaking down the required compositional function to be learned into individual components allows the model to learn a higher-dimensional but easier-to-learn function map. In Figure 1, we provide examples of the four data formatting methods explored in our work.</p>
<p>Training on text and arithmetic mixtures and the role of few-shot prompting. We also explore the interplay between arithmetic and text data during training, as LLMs are trained on massive amounts of data scraped from the internet Bubeck et al., 2023, Peterson et al., 2019, where it is impractical to carefully separate different types of data. We observe how the model’s perplexity and accuracy vary with the ratio of text to arithmetic data. We find that learning all arithmetic operations discussed earlier (from addition to square root) can improve the individual performance of each task, and that going from zero-shot to 1-shot prompting (showing one arithmetic example) yields a large accuracy improvement, but there is no significant improvement in accuracy by showing more examples.</p>
<p>The role of pre-training and model scale. We also investigate the role of pretraining by finetuning models like GPT-2 and GPT-3 davinci and observe that while the zero-shot performance</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The four data formatting methods investigated in this work: (i) Plain: standard addition formatting (Section 4), (ii) Reverse: reversing the output (Section 4), (iii) Simplified Scratchpad: recording the digit-wise sum and carry-ons (Section 6), and (iv) Detailed Scratchpad: providing detailed intermediate steps of addition (Section 6). We train small transformer models from scratch using data transformed with these various formatting methods for addition. The results (shown on the right) highlight the crucial role of data formatting in performance and sample efficiency. Plain never reaches 100% accuracy and the sample complexity for the remaining methods to learn addition perfectly steadily reduces as we increase the level of detail in the data format.</p>
<p>on arithmetic operations is poor, the prior "skills" acquired during pretraining facilitate reasonable performance on some basic arithmetic tasks even with a small number of finetuning samples. However, finetuning with non-standard formatting, such as reverse formatting, can interfere with the model's performance when pretrained on standard-formatted operations, leading to decreased accuracy. Finally, we conduct studies on how performance in arithmetic changes with scale, and although we find that scale does indeed aid in learning arithmetic operations, it is not a necessary trait.</p>
<p>Compositional and length generalization. One might question if our trained models truly grasp arithmetic. Our findings present a nuanced answer. We find length generalization beyond trained digit lengths difficult. For instance, if a model is trained on all n-digit lengths, excluding a specific length, it struggles to compensate and accurately calculate this missing digit length. Consequently, the models achieve high accuracy within trained digit lengths but struggle significantly beyond this range. This suggests that the models learn arithmetic not as a flexible algorithm, but more as a mapping function constrained to trained digit lengths. While this surpasses mere memorization, it falls short of comprehensive arithmetic "understanding".</p>
<p>Novelty over prior work. Our approach heavily builds upon prior work that uses instructive data to enhance model performance, and we do not claim novelty in the style of training data employed. What sets our work apart is the primary focus on randomly initialized models and extensive ablation studies on various sampling/data formatting and model scale settings to isolate the factors that contribute to the fast emergence of arithmetic capabilities. Furthermore, our work offers a few simple but perhaps insightful theoretical justifications of some of the phenomena we observe.</p>
<h2>2 Related Works</h2>
<p>Instructional data/chain-of-thought. The idea of using detailed reasoning training data predates Transformers (Vaswani et al., 2017). Ling et al. (2017); Cobbe et al. (2021); Nye et al. (2021) use natural language to generate reasoning steps while Roy &amp; Roth (2016); Reed &amp; De Freitas (2015); Chen et al. (2017); Cai et al. (2017) show that symbolic reasoning may suffice. Nogueira et al. (2021) note that large number of samples with small digits is important for arithmetic tasks (Yuan et al., 2023). Razeghi et al. (2022) observe a correlation between the frequency of numbers in the dataset and the performance involving them whereas we find that transformers can learn to add numbers that were not seen during training. Chain-of-thought (Wei et al., 2022c) refers to the model's improved performance when prompted to produce rationale. Zhou et al. (2022b) show that this can be achieved by providing sufficiently informative exemplars as a few-shot prompt (Brown et al., 2020). Zhou et al. (2022a) showed that least-to-most prompting can help GPT-3 solve problems that can be decomposed into simpler sub-problems. Least-to-most prompting consists of first decomposing a complex problem into easier subproblems, and then sequentially solving these subproblems. We extend this notion to simple addition and show that asking the model to output the least significant bit first has a similar</p>
<p>effect. <em>Kojima et al. (2022)</em> shows that very often even just prompting the model with “let’s think step by step” is sufficient to achieve competitive zero-shot accuracy on several benchmark datasets.</p>
<p>Arithmetic using Transformer models. Our work focuses on decoder-only models since they are well-suited for text generation and are widely used in LLMs (Brown et al., 2020; Touvron et al., 2023; MosaicML, 2023). However, encoder-decoder models have also been extensively studied in the literature in the context of learning arithmetic (Kim et al., 2021; Wang et al., 2021). Qian et al. (2022); Lightman et al. (2023); Uesato et al. (2022) explore techniques to improve the arithmetic abilities of pretrained LLMs. <em>Wallace et al. (2019)</em> on the other hand, focus on the impact of the learned embeddings. Most results that show Turing-completeness or the universal approximation typically rely on encoder models (Yun et al., 2019; Pérez et al., 2021; Wei et al., 2022a; Giannou et al., 2023). <em>Ontanón et al. (2021)</em> study the problem of compositional generalization extensively on benchmark datasets such as SCAN (Lake &amp; Baroni, 2018; Drozdov et al., 2022) and conclude that design changes like relative position encoding (Shaw et al., 2018) can improve performance significantly. <em>Charton (2022, 2021)</em> show that Transformers can learn linear algebra operations with carefully chosen encodings. <em>Hanna et al. (2023)</em> use mechanistic interpretability techniques to explain the limited numerical reasoning capabilities of GPT-2.</p>
<p>Beyond Transformers. While we focus our attention on GPT-like models, there is a rich literature studying other sequence-to-sequence models such as recurrent neural networks (RNNs) (Bowman, 2013; Bowman et al., 2014; Zaremba et al., 2014). Zaremba &amp; Sutskever (2014) show that RNNs can learn how to execute simple programs with for-loops provided they are trained with curriculum learning. <em>Sutskever et al. (2014)</em> show that LSTMs show improved performance on text-based tasks such as translation when the source sentences are reversed, which is closely related to what we observe in addition. <em>Kaiser &amp; Sutskever (2015)</em> propose Neural GPUs which outperform prior RNNs on binary arithmetic tasks and even show length generalization i.e., they can perform arithmetic on inputs of lengths that were unseen during training. This is yet to be seen even in modern pre-trained models (Bubeck et al., 2023) and therefore it is interesting to see if we can leverage some of these techniques and apply them to existing modern architectures. <em>Dehghani et al. (2018)</em> propose Universal Transformers (UTs) which introduce a recurrent transition function to apply recurrence over revisions of the vector representation at each position as opposed to the different positions in the input. They show that on the tasks from Zaremba &amp; Sutskever (2014), UTs outperform traditional Transformers and RNNs.</p>
<p>Data-centric AI. More recently, there has been increasing interest in Data-Centric AI which emphasizes techniques to improve datasets in order to ensure better performance (Motamedi et al., 2021; Hajij et al., 2021). Gadre et al. (2023) propose a new benchmark where the training code is fixed and the only way to improve performance is to construct new training sets. Several works have also tried to see if the model’s reasoning ability can be leveraged to generate explanations and leverage it to solve complicated reasoning tasks (Rajani et al., 2019; Talmor et al., 2020; Zelikman et al., 2022; Huang et al., 2022).</p>
<h2>3 Preliminaries and Experimental Setup</h2>
<p>In this section, we provide a detailed description of our experimental setup, including the model architecture and an overview of the different data formatting and sampling techniques that we employ and evaluate.</p>
<p>Model and Data. To examine the individual factors at play, we use NanoGPT (Karpathy, 2022), a lightweight implementation of the GPT family of models, chosen primarily for its feasibility to train from random initialization under numerous settings. NanoGPT features a decoder-only transformer architecture with six self-attention layers, six heads, and an embedding dimension of 384, resulting in approximately 10.6 million parameters. Unless stated otherwise, we use character-level tokenization and absolute position encoding. We train the NanoGPT model from random initialization, which we refer to as training from ‘scratch’, using the conventional next-token prediction objective.</p>
<p>To understand the effect of scale, we extend our experiments to GPT-2 and GPT-3 in Section 10. We investigate teaching arithmetic from scratch as well as fine-tuning using a pretrained GPT-2. However, for GPT-3, we exclusively use supervised fine-tuning on a pretrained model. Refer to Appendix C.2 for a more detailed description.</p>
<p>For arithmetic tasks like addition, subtraction, and multiplication, we define the training dataset for a binary operator $f(\cdot)$ as $\mathcal{D}<em i="i">{\text {train }}=\left{\left(a</em>\right}}, b_{i}\right), y_{i<em i="i">{i=1}^{N}$ where $y</em>}=f\left(a_{i}, b_{i}\right)$. For unary operations such as the sine and square root functions, the training dataset is formulated as $\mathcal{D<em i="i">{\text {train }}=\left{a</em>\right}}, y_{i<em i="i">{i=1}^{N}$, where $y</em>}=f\left(a_{i}\right)$. The test dataset $\mathcal{D<em _text="\text" _train="{train">{\text {test }}$ is constructed by randomly sampling pairs of operands not included in $\mathcal{D}</em>$. Throughout training and inference, we apply different data formatting techniques on each data sample from the training dataset, creating the final sequence that serves as the model's input.}</p>
<p>Data Formatting. In the following sections, we will delve into the detailed intuition, and results of the four data formatting approaches that we have deployed in our arithmetic experiments. For this section, we provide a high-level summary of these approaches, each progressively incorporating additional information to form a more comprehensive format. The scratchpad formats are largely adopted from the literature of chain-of-thought (CoT) training (Nye et al., 2021; Zhou et al., 2022b). See Figure 2 and Appendix D for detailed examples.</p>
<h1>Different data formatting methods for addition</h1>
<p>Four input formatting methods used for the addition task:
(i) Plain: standard formatting of addition
(ii) Reverse: flips the order of the output and encapsulates each data sample with the '\$' symbol at the start and end.
(iii) Simplified Scratchpad: provides carry and digit-sum information for each step of addition, from the LSB to the MSB $^{3}$.
(iv) Detailed Scratchpad: provides explicit details of intermediate steps of addition.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Plain</th>
<th style="text-align: center;">Detailed Scratchpad</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">128+367=495</td>
<td style="text-align: center;">Input:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">128+367</td>
</tr>
<tr>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">Target:</td>
</tr>
<tr>
<td style="text-align: center;">\$128+367=594\$</td>
<td style="text-align: center;"><scratch></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$[1,2,8]$ has 3 digits.</td>
</tr>
<tr>
<td style="text-align: center;">Simplified Scratchpad</td>
<td style="text-align: center;">$[3,6,7]$ has 3 digits.</td>
</tr>
<tr>
<td style="text-align: center;">Input: 128+367</td>
<td style="text-align: center;">$[1,2,8]+[3,6,7], C=0,8+7+0=15, A-&gt;5, C-&gt;1$</td>
</tr>
<tr>
<td style="text-align: center;">Target:</td>
<td style="text-align: center;">$[1,2]+[3,6], A=[5], 2+6+1=9, A-&gt;9, C-&gt;0$</td>
</tr>
<tr>
<td style="text-align: center;">A-&gt;5, C-&gt;1</td>
<td style="text-align: center;">$[1]+[3], A=[9,5], C=0,1+3+0=4, A-&gt;4, C-&gt;0$</td>
</tr>
<tr>
<td style="text-align: center;">A-&gt;9, C-&gt;0</td>
<td style="text-align: center;">[] + [] , A= $[4,9,5], C=0$, END</td>
</tr>
<tr>
<td style="text-align: center;">A-&gt;4, C-&gt;0.</td>
<td style="text-align: center;"><scratch></td>
</tr>
<tr>
<td style="text-align: center;">495</td>
<td style="text-align: center;">495</td>
</tr>
</tbody>
</table>
<p>Figure 2: The four input formatting methods used for the addition task. We progressively increase the amount of detail with each format.</p>
<p>Note that we wrap each data sample in the reverse format with the ' $\$$ ' symbol at the beginning and end as a delimiter. We originally observed improved performance in both the plain and reverse formats when the operands and outputs were zero-padded to a fixed length (e.g., 3 and 4 digits, respectively, for 3-digit addition). But later realized that a single symbol can effectively replace zero-padding. While we maintain the original plain format without padding as a baseline - emphasizing the necessity for improved data formatting for efficient emergence - we incorporate the ' $\$$ '-encapsulation in our modified reverse format. For further details, refer to Appendix B.1.</p>
<p>In Section 4, we explore the limitations of the conventional plain-format data and demonstrate how a simple reversal of the output order can lead to substantial performance improvements and enhanced sample efficiency. We introduce two Lemmas to support and explain these findings. Additionally, in Section 6, we present results on the simplified and detailed scratchpad formats, highlighting significant enhancements in sample efficiency for learning addition. We also emphasize the importance of carefully designing the intermediate steps in the detailed scratchpad method.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Structured Data Sampling. While data formatting plays a crucial role, we also discover that selecting the appropriate samples for inclusion in the training data is also essential. When sampling operands for $n$-digit addition uniformly at random between 1 to $10^{n}-1$, the dataset becomes highly skewed in terms of the number of samples with (i) operands containing a specific number of digits and (ii) operands resulting in a certain number of carry-on operations. For instance, in the case of 3-digit addition, random sampling results in a meager $0.01\%$ probability of selecting a 1-digit number. Additionally, 1 or 2 carry-on operations are more likely to occur than 0 or 3 . To address this imbalance, we employ a structured sampling approach. Specifically, we aim to (i) balance digits by assigning higher weights to lower-digit numbers during the sampling process as in Nogueira et al. (2021) and (ii) balance carry-ons by ensuring an equal distribution of examples with $0,1, \ldots, n$ carry-on operations.</p>
<p>When sampling 10, 000 examples of 3-digit addition, we include all possible 100 1-digit additions, 900 2-digit samples and 9000 3-digit samples. Note that while the number of samples increase, the fraction of all possible $k$-digit additions that we sample for $k=2,3$ decreases due to the inherent skew. The split was chosen heuristically to ensure we saw a "reasonable" fraction of all possible $k$-digit samples for all $k$. Similarly, we ensure that the number of samples with $0,1,2$, or 3 carry-on operations are all approximately 2500 .</p>
<p>Figure 3 reveals the importance of "balancing". We observe improvements in accuracy across the board while using Balanced data when compared to random sampling. Further, random sampling performs relatively poorly even for the simple task of $2-$ digit addition. We conjecture that this is due to the fact that the model has not seen enough of these examples. For the remaining experiments, we set the default dataset for addition to be one that has both balanced digits and carry-ons.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Performance of 3-digit addition on various data sampling methods used: (i) Random: uniform sampling of operands; (ii) Balanced digits: assigning higher sampling weights to operations involving 1 and 2-digit numbers; (iii) Balanced carry: balancing the dataset to contain an equal number of carry-on operations. Experiments on addition with zeropadding both operands and output to have 3 and 4 digits respectively.</p>
<h1>4 Learning Addition in Small Models</h1>
<p>We start by examining one of the most basic arithmetic tasks: addition. Initially, we concentrate on the 3-digit addition, where the two operands have at most 3 digits (999). Later, in Section 7, we demonstrate that our findings can be applied to larger digits. We assess whether NanoGPT can learn addition from training data of various sizes. As we will soon discover, learning addition may not be as straightforward as one might anticipate.</p>
<h3>4.1 Training on Conventional Data</h3>
<p>We begin by training NanoGPT on conventional addition data in the form of ' $\mathrm{A}<em 2="2">{3} \mathrm{~A}</em>} \mathrm{~A<em 3="3">{1}+\mathrm{B}</em>} \mathrm{~B<em 1="1">{1} \mathrm{~B}</em>}=\mathrm{C<em 2="2">{3} \mathrm{C}</em>$ ', which we denote as the plain data format. However, as shown in Figure 4, this leads to fairly poor performance. We believe that this is because the next-token prediction objective is not optimized for generating the most significant digit (MSB) first.
} \mathrm{C}_{1<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Comparison of NanoGPT model performance on the addition task, trained on plain and reverse formatted data. The conventional plain format exhibits suboptimal performance, even with a larger number of addition examples, whereas a distinct phase transition is observed for the reverse format around 2500 train samples where it learns addition perfectly.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The following lemma clarifies the necessity to access all operand digits in order to output the MSB first:
Lemma 1. Let $A$ and $B$ be two n-digit numbers, and let $C=A+B$. Suppose an algorithm $\mathcal{A}$ outputs the digits of $C$ in decreasing order of significance, then $\mathcal{A}$ must have access to all digits of $A$ and $B$ starting from the first digit that it outputs.</p>
<p>The lemma suggests that to train the model for addition and to output the most significant digit first, it is necessary for the model to learn a global algorithm. Unlike the standard algorithm for addition which consists of computing digit-wise sums and carry-ons, approximating a global algorithm would necessitate learning a more complicated function than necessary. The increased complexity results in decreased accuracy, as observed throughout our experiments. Liu et al. (2023) refer to this phenomenon as attention glitches.</p>
<h1>4.2 Reversing the Output</h1>
<p>This leads us to ask, "is it possible to guide the model to learn a simpler algorithm for addition?" We propose an intuitive approach to improve performance by training the model to generate the least significant digit (LSB) first, following the way humans typically perform addition. By starting with the LSB and progressing towards the most significant digit (MSB) from right to left, the model can learn a simpler algorithm that relies on just three inputs: the corresponding digits from the operands and the carry-on information ( 0 or 1 ) carried from the LSB to the MSB. This approach offers an advantage over the plain format, where generating the MSB first would necessitate the model to learn a more complex function involving all digits in the two operands.
We propose that using this reverse format ( $\$ \mathrm{~A}<em 2="2">{3} \mathrm{~A}</em>} \mathrm{~A<em 3="3">{1}+\mathrm{B}</em>} \mathrm{~B<em 1="1">{1} \mathrm{~B}</em>}=\mathrm{C<em 2="2">{1} \mathrm{C}</em>$ ) is more suitable for next-word prediction models. The rationale behind this is that when generating the sum by starting with the least significant digit (LSB), the model only needs to learn a local function of three inputs per digit - the two relevant digits of the operands and the carry-on from the previous digit. This local operation simplifies the learning function. The following lemma substantiates this idea:
Lemma 2. There exists an algorithm that computes $C=A+B$ for two n-digit numbers $A$ and $B$ and outputs its digits in increasing order of significance such that, at each position $i$, the algorithm only requires access to the $i^{\text {th }}$ digits of $A$ and $B$, as well as the carry-on from the previous position.} \mathrm{C}_{3} \$^{\prime</p>
<p>Lemma 2 directly follows from the standard algorithm for addition, which performs the sum and carryon operations digit by digit. The implications of these two lemmas are evident in our experiments when comparing training NanoGPT on plain and reverse samples. As shown in Figure 4, the accuracy of plain addition plateaus at slightly over $85 \%$ even with 10,000 samples. In contrast, simply training the model on reversed output significantly enhances the performance. Additionally, we observe that the reverse format requires considerably fewer training data to achieve good performance, further reinforcing that the reverse format's associated function has less complexity than the plain format. What is particularly remarkable is the occurrence of a notable phase transition between 1000 and 4000 samples for reverse. At this point, the model rapidly transitions from being unable to perform addition to being capable of perfectly adding two 3-digit numbers. This leads to an important question:</p>
<p>Why does addition rapidly emerge as the number of training examples increases?</p>
<h2>5 Connection to Low-Rank Matrix Completion</h2>
<p>Although the rapid phase transition observed in the previous section may initially seem surprising, closer examination reveals a fascinating equivalence: learning an addition map on $n$ digits from random samples can be considered as completing a rank-2 matrix. This equivalence offers a compelling explanation for the phenomenon we observed. In this section, we delve into the intricate details of this connection and elucidate how learning the addition map can be formulated as low-rank matrix completion (LRMC). Establishing this connection provides meaningful insights into the observed phenomenon. Further, our investigation goes beyond that and highlights the enhanced capabilities of Transformer models. We demonstrate that Transformers possess expanded capabilities that surpass what traditional LRMC algorithms can do.</p>
<p>5.1 Addition Tables are Rank-2 Matrices
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: (a) We run Algorithm 1 ( [Király et al., 2015]), a simple iterative algorithm for 2-rank matrix completion for the addition matrix $(n=20,50,100,500)$ and report the success probability over multiple random trials while varying the number of revealed entries. As anticipated, a sharp phase transition occurs when approximately $\mathcal{O}(n)$ entries are revealed. (b) We compare the performance of a NanoGPT model trained on a dataset containing $n=100$ samples (i.e., 2-digit addition) to that of the corresponding LRMC problem using the same sample set. Notably, the phase transition at around 1500 samples, where both NanoGPT and Algorithm 1 begin learning addition almost flawlessly, is remarkably similar.</p>
<p>Learning addition from samples can be formulated as a rank-2 Matrix Completion (MC) problem involving an $n \times n$ matrix $\boldsymbol{M}$, where the $(i, j)$-th entry $M_{i, j}$ represents the output of the addition ' $i+j$ '. Such $\boldsymbol{M}$ can be decomposed into the sum of two rank-one matrices, $\boldsymbol{N} \mathbf{1}^{T}+\mathbf{1} \boldsymbol{N}^{T}$, where $\boldsymbol{N}$ is a column vector with entries ${1, \ldots n}$, and $\mathbf{1}$ is a vector of $n$ ones. Thus, learning addition from samples can be viewed as solving the MC problem in which only the entries corresponding to those samples are revealed. When the underlying matrix is noiseless and of rank-2, <em>Király et al. (2015)</em> demonstrates that a simple iterative algorithm (Algorithm 1 in Appendix B.2) is optimal. As depicted in Figure 5a, a sharp phase transition occurs at $\mathcal{O}(n)$. This aligns with Theorem-2 from <em>Recht (2011)</em> which states that the exact convex relaxation to the MC problem has a unique solution as long as $\widetilde{\mathcal{O}}(n)$ samples are observed.</p>
<p>The sharp phase transition observed in LRMC bears a resemblance to what we notice in NanoGPT. To further investigate this phenomenon, we focus on 2-digit addition $(n=100)$ as shown in Figure 5a. We evaluate the performance of learning addition through NanoGPT in comparison to LRMC by constructing a training dataset consisting of the matrix's revealed entries in either plain or reverse format. It is important to note that the training dataset is no longer "balanced", as the revealed entries are randomly and uniformly sampled for the LRMC experiments. The comparison between NanoGPT and LRMC results is presented in Figure 5b. Remarkably, both NanoGPT and LRMC exhibit a similar phase transition at approximately 1500 samples, where they both start to learn addition almost perfectly. This observation regarding LRMC offers an explanation for the rapid emergence of addition in NanoGPT.</p>
<h1>5.2 NanoGPT Generalizes better than Matrix Completion solutions</h1>
<p>We noted above that there are some striking similarities between the addition map learned by NanoGPT and LRMC. However, we now delve deeper and find that this map exhibits capabilities beyond LRMC. A well-known limitation of LRMC is its inability to generalize when entire rows or columns are empty. Therefore, we intentionally hide certain numbers in the training dataset or specific digit positions, and examine whether our model can still learn addition.</p>
<p>Generalizing to unseen numbers. In order to further investigate the connection with LRMC, we exclude an increasing fraction of the numbers from the training data and evaluate the model's ability to learn addition. As shown in Table 1, the answer to this question is a resounding Yes! The model achieves almost perfect accuracy even when excluding half of all possible 3-digit numbers. More</p>
<p>precisely, we randomly choose 100/200/500 numbers and exclude them from the training data. We then evaluate the trained models two metrics: (i) Overall accuracy: which measures the accuracy over a random set of 10, 000 examples and (ii) Exclusion accuracy: which measures the accuracy only over the excluded set. Remarkably, excluding numbers from the training data sometimes leads to improved performance. We conjecture that this may be due to the effect of regularization, similar to random masking or cropping images in vision tasks. Note that these results indicate that the model is not simply performing LRMC. In the LRMC setting, even a single missing number corresponds to an empty row or column, which cannot be recovered. Hence, the ability of the NanoGPT model to generalize to missing numbers signifies its distinct capabilities beyond LRMC.</p>
<p>Table 1: Impact of excluding numbers on addition task: NanoGPT models trained with 100/200/500 excluded operands show no significant drop in accuracy and in some cases, the performance even improves. Note that models trained with reverse data remain consistently at $100 \%$ accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">No Exclusion</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Excluding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Excluding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Excluding</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100 numbers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">200 numbers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">500 numbers</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">Rev</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">Rev</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">Rev</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">Rev</td>
</tr>
<tr>
<td style="text-align: left;">Overall Accuracy</td>
<td style="text-align: center;">$87.18 \%$</td>
<td style="text-align: center;">$99.97 \%$</td>
<td style="text-align: center;">$87.94 \%$</td>
<td style="text-align: center;">$100.00 \%$</td>
<td style="text-align: center;">$87.24 \%$</td>
<td style="text-align: center;">$99.99 \%$</td>
<td style="text-align: center;">$88.15 \%$</td>
<td style="text-align: center;">$99.99 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Exclusion Accuracy</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$92.55 \%$</td>
<td style="text-align: center;">$100.00 \%$</td>
<td style="text-align: center;">$92.15 \%$</td>
<td style="text-align: center;">$99.95 \%$</td>
<td style="text-align: center;">$90.85 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
</tbody>
</table>
<p>Generalizing to unseen digits. Building upon the model's robustness to excluded numbers, we further investigate its ability to handle excluded digits. Intuitively, this should be even more challenging since excluding a digit means the model cannot learn directly how to operate in that position. Instead, it would have to generalize and infer that digits act similarly across all positions. We construct datasets with the number 5 excluded in 1st (LSB), 2nd, and 3rd (MSB) positions, and train separate models on each of these datasets. We compare the resulting models by evaluating overall accuracy on a test set of 10,000 randomly sampled numbers, as well as their accuracy specifically on samples with 5 in each position which we call exclusion accuracy.</p>
<p>The results presented in Table 2 indicate that the model is not as robust to excluding digits compared to excluding numbers. However, it still achieves more than $66 \%$ accuracy on every test and maintains an overall accuracy above $85 \%$. Moreover, it appears that excluding a number in the least significant position yields the worst performance. This can be attributed to the fact that learning addition in this position is transferable to other positions since it is unaffected by carry-on operations. Failing to learn addition in this position, however, will have a detrimental impact on other positions as well.</p>
<p>Table 2: Impact of excluding digits on addition task: We investigate whether GPT-based models can infer addition on an excluded digit in a specific position from training data on other positions. We compare NanoGPT models trained with and without an excluded digit and find that excluding digits is harder to learn but not entirely impossible, with the worst performance observed when excluding the least significant digit.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Excluded position</th>
<th style="text-align: center;">Input <br> format</th>
<th style="text-align: center;">Overall Acc</th>
<th style="text-align: center;">"5" in the <br> 1st (LSB) digit</th>
<th style="text-align: center;">"5" in the <br> 2nd digit</th>
<th style="text-align: center;">"5" in the <br> 3rd (MSB) digit</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No exclusion</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">87.18\%</td>
<td style="text-align: center;">87.50\%</td>
<td style="text-align: center;">88.65\%</td>
<td style="text-align: center;">91.80\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">99.97\%</td>
<td style="text-align: center;">99.90\%</td>
<td style="text-align: center;">99.95\%</td>
<td style="text-align: center;">100\%</td>
</tr>
<tr>
<td style="text-align: center;">1st (LSB) digit</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">85.05\%</td>
<td style="text-align: center;">76.70\%</td>
<td style="text-align: center;">85.80\%</td>
<td style="text-align: center;">88.35\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">93.31\%</td>
<td style="text-align: center;">66\%</td>
<td style="text-align: center;">94.80\%</td>
<td style="text-align: center;">94.45\%</td>
</tr>
<tr>
<td style="text-align: center;">2nd digit</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">85.44\%</td>
<td style="text-align: center;">84.55\%</td>
<td style="text-align: center;">78.50\%</td>
<td style="text-align: center;">90.15\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">98.85\%</td>
<td style="text-align: center;">98.85\%</td>
<td style="text-align: center;">94.20\%</td>
<td style="text-align: center;">99.50\%</td>
</tr>
<tr>
<td style="text-align: center;">3rd (MSB) digit</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">85.70\%</td>
<td style="text-align: center;">85.35\%</td>
<td style="text-align: center;">87.35\%</td>
<td style="text-align: center;">83.45\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">97.18\%</td>
<td style="text-align: center;">97.25\%</td>
<td style="text-align: center;">97.35\%</td>
<td style="text-align: center;">85.45\%</td>
</tr>
</tbody>
</table>
<p>The distinct learning mechanism of NanoGPT. The phase transition of LRMC offers significant insights into NanoGPT's learning process. Nevertheless, further experiments clearly demonstrate that NanoGPT's mechanism for learning addition is fundamentally different from LRMC. It can successfully learn addition even when numbers or digits are intentionally excluded from the training data, thereby exhibiting generalization capabilities that far exceed that of typical LRMC algorithms.</p>
<h1>6 The power of Chain-of-Thought: Incorporating Intermediate Steps in Training Data</h1>
<p>So far, we observed that utilizing the straightforward method of reversing the output can result in remarkable performance, exceeding that of LRMC in learning addition. Nonetheless, it may be possible to expedite the emergence of addition by further enhancing the data format. As addition is a multi-step process, we further explore the idea of incorporating additional information about each step. We adopt a Chain-of-Thought (CoT) style approach, where we guide the model to learn addition step-by-step. In the subsequent sections, we assess the effect of incorporating these intermediate steps on the performance of small models. We demonstrate that this results in a substantial improvement in sample complexity of learning addition and carefully analyze how the level of detail offered for each step impacts the model's performance.</p>
<h3>6.1 Training on Chain-of-Thought Data</h3>
<p>In the following experiments, we evaluate if training on scratchpad data further improves the learning of addition. As described briefly in Section 3, scratchpad data incorporates step-by-step instructions in varying amounts of detail into the samples. This approach aims to help the model learn addition as a compositional function. We explore two levels of detail in the provided instruction steps: Simplified Scratchpad format offers minimal information - the sum and carry information for each digit/step. Detailed Scratchpad provides comprehensive information on how to execute each step in the addition process in natural language. By comparing the performance of the model trained with these different levels of detail, we can analyze its impact on the model's ability to learn addition effectively.</p>
<p>The results presented in Figure 6 demonstrate the effectiveness of different data formats for training addition. The model trained on Simplified Scratchpad data achieves $100 \%$ accuracy with only 2000 samples, whereas the Reverse format requires more than double the number of samples. Furthermore, the Detailed Scratchpad format, which provides even more detailed information, achieves perfect addition with just 1000 samples. This indicates that incorporating more information enables the model to learn addition more efficiently, requiring fewer examples. We conjecture that this is because breaking down the required compositional function to be learned into individual components allows the model to learn a higher-dimensional but easier-to-learn function map. We note that while CoT-style training enhances sample efficiency, it may not necessarily be the most "token-efficient" approach. We delve into this aspect in more detail in Section 11. In summary, incorporating scratchpad data and decomposing the addition task into steps offer a promising strategy to improve the performance and efficiency of small models in learning addition from scratch.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Comparison of sample efficiency: evaluating performance on training datasets with different numbers of addition samples. While all modified methods (reverse, simplified scratchpad, and detailed scratchpad) achieve $100 \%$ test accuracy, they exhibit varying requirements in terms of the number of addition examples in the training dataset to reach optimal performance.</p>
<h3>6.2 The Importance of Intermediate Step Design: Subtraction</h3>
<p>In this section, we underscore the significance of meticulously designing the intermediate steps in a Chain-of-Thought manner. Specifically, we focus on the subtraction task and conduct experiments to compare two different versions of the detailed scratchpad for this operation (see examples in Figure 7). These trials shed light on the importance of decomposing the subtraction task into simpler intermediate steps. Unlike addition, subtraction behaves differently depending on whether the first operand $(a)$ is greater than the second operand $(b)$ or vice versa.</p>
<h1>Detailed scratchpad formatting for different arithmetic tasks</h1>
<p>Examples of two variations of detailed scratchpad formatting for subtraction, considering the scenario where the first operand $a$ is greater than the second operand $b$, and vice versa. In Version 1, a result processing step is included in the final stage to handle negative outputs. In Version 2, the operands are compared at the beginning, and if $b$ is larger, their order is reversed.</p>
<h2>Prompt (Case 1. $a-b \geq 0$ ) :</h2>
<p>Input:
$367-128$
Target:
Version 1.
Version 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">・.</th>
<th style="text-align: left;">・.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><scratch></td>
<td style="text-align: left;"><scratch></td>
</tr>
<tr>
<td style="text-align: left;">$[3,6,7]$ has 3 digits.</td>
<td style="text-align: left;">$[3,6,7]$ has 3 digits.</td>
</tr>
<tr>
<td style="text-align: left;">$[1,2,8]$ has 3 digits.</td>
<td style="text-align: left;">$[1,2,8]$ has 3 digits.</td>
</tr>
<tr>
<td style="text-align: left;">$[3,6,7]-[1,2,8], A=[], C=0$,</td>
<td style="text-align: left;">$367&gt;=128 #$ comparison of two operands</td>
</tr>
<tr>
<td style="text-align: left;">$7-8-0=10=9, A-&gt;9, C-&gt;-1$</td>
<td style="text-align: left;">$[3,6,7]-[1,2,8], A=[], C=0$,</td>
</tr>
<tr>
<td style="text-align: left;">$[3,6]-[1,2], A=[9], C=-1,6-2-1=3$,</td>
<td style="text-align: left;">$7-8-0=10=9, A-&gt;9, C-&gt;-1$</td>
</tr>
<tr>
<td style="text-align: left;">$-&gt;3, C-&gt;0$</td>
<td style="text-align: left;">$[3,6]-[1,2], A=[9], C=-1,6-2-1=3$, A</td>
</tr>
<tr>
<td style="text-align: left;">$[3]-[1], A=[3,9], C=0,3-1-0=2, A-&gt;2$</td>
<td style="text-align: left;">$-&gt;3, C-&gt;0$</td>
</tr>
<tr>
<td style="text-align: left;">$, C-&gt;0$</td>
<td style="text-align: left;">$[3]-[1], A=[3,9], C=0,3-1-0=2, A-&gt;2$</td>
</tr>
<tr>
<td style="text-align: left;">$[]-[], A=[2,3,9]$</td>
<td style="text-align: left;">$C-&gt;0$</td>
</tr>
<tr>
<td style="text-align: left;">$200=39=239$, END # result processing</td>
<td style="text-align: left;">$[]-[], A=[2,3,9]$, END</td>
</tr>
<tr>
<td style="text-align: left;"></scratch></td>
<td style="text-align: left;"></scratch></td>
</tr>
<tr>
<td style="text-align: left;">239</td>
<td style="text-align: left;">239</td>
</tr>
</tbody>
</table>
<h2>Prompt (Case 2. $a-b&lt;0$ ) :</h2>
<p>Input:
$128-367$
Target:
Version 1.
Version 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">・.</th>
<th style="text-align: left;">・.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><scratch></td>
<td style="text-align: left;"><scratch></td>
</tr>
<tr>
<td style="text-align: left;">$[1,2,8]$ has 3 digits.</td>
<td style="text-align: left;">$[1,2,8]$ has 3 digits.</td>
</tr>
<tr>
<td style="text-align: left;">$[3,6,7]$ has 3 digits.</td>
<td style="text-align: left;">$[3,6,7]$ has 3 digits.</td>
</tr>
<tr>
<td style="text-align: left;">$[1,2,8]-[3,6,7], A=[], C=0,8-7-0=1$,</td>
<td style="text-align: left;">$128&lt;367: 128-367=-(367-128) #$ comparison</td>
</tr>
<tr>
<td style="text-align: left;">$A-&gt;1, C-&gt;0$</td>
<td style="text-align: left;">$[3,6,7]-[1,2,8], A=[], C=0$,</td>
</tr>
<tr>
<td style="text-align: left;">$[1,2]-[3,6], A=[1], C=0,2-6-0=10=6$,</td>
<td style="text-align: left;">$7-8-0&lt;10=9, A-&gt;9, C-&gt;-1$</td>
</tr>
<tr>
<td style="text-align: left;">$A-&gt;6, C-&gt;-1$</td>
<td style="text-align: left;">$[3,6]-[1,2], A=[9], C=-1,6-2-1=3$, A</td>
</tr>
<tr>
<td style="text-align: left;">$[1]-[3], A=[6,1], C=-1,1-3-1=-3$,</td>
<td style="text-align: left;">$-&gt;3, C-&gt;0$</td>
</tr>
<tr>
<td style="text-align: left;">$-&gt;-3, C-&gt;-1$</td>
<td style="text-align: left;">$[3]-[1], A=[3,9], C=0,3-1-0=2, A-&gt;2$</td>
</tr>
<tr>
<td style="text-align: left;">$[]-[], A=[-3,6,1]$</td>
<td style="text-align: left;">$C-&gt;0$</td>
</tr>
<tr>
<td style="text-align: left;">$-300=61=-239$, END # result processing</td>
<td style="text-align: left;">$[]-[], A=[2,3,9]$, END</td>
</tr>
<tr>
<td style="text-align: left;"></scratch></td>
<td style="text-align: left;"></scratch></td>
</tr>
<tr>
<td style="text-align: left;">-239</td>
<td style="text-align: left;">-239</td>
</tr>
</tbody>
</table>
<p>Figure 7: Two versions of detailed scratchpad formatting for subtraction.
The first strategy (Version 1 in Figure 7) involves performing digit-wise subtraction starting from the least significant bit (LSB) and considering borrows when necessary. However, this strategy produces incorrect results when the first operand is smaller than the second operand. In such cases, we subtract the number in the most significant bit (MSB) position multiplied by 10 to the power of (number of digits in the output - 1) from the remaining digits in the output. An example illustrating this approach is shown in Version 1, Case 2. Alternatively, we can adopt a more familiar strategy. If the first operand is smaller than the second, we swap the operands and compute the negation of the subtraction of the swapped operands: $a-b=-(b-a)$ (referred to as Version 2).
The results in Figure 8 indicate that Version 2, which involves comparing two operands, performs considerably worse than Version 1. In Version 1, each intermediate step only requires the simpler 1-digit subtraction, along with addition in the final result processing step. Upon analyzing the failure cases of Version 2, we observe that the majority of errors stem from incorrectly identifying which of the two operands is larger, while the intermediate steps are handled correctly. This finding underscores the significance of breaking down arithmetic operations into simpler intermediate steps. Unless otherwise specified, we use Version 1 in all detailed scratchpad experiments.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Comparison of performance among various data formatting approaches (plain, reverse, and two versions of detailed scratchpad (DS)) for the subtraction task. The experiments were conducted on a NanoGPT model trained on a dataset of 10,000 examples. Version 2, which incorporates operand comparison, exhibits significantly lower performance compared to Version 1. This observation highlights the substantial impact of the construction of intermediate steps on the model's performance.</p>
<h1>6.3 The Effect of Noisy Inputs on Accuracy</h1>
<p>(a) Test accuracy on Addition
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) Test accuracy on Subtraction
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Comparison of training with simplified scratchpad formatting using correct A and C information with formatting using random $\mathrm{A} / \mathrm{C}$ and their effect on sample efficiency and accuracy. Results show that noisy labels degrade sample efficiency, but with sufficient training data, the model eventually reaches full accuracy.</p>
<p>Noisy intermediate steps in the scratchpad data. We further investigate the significance of providing accurate intermediate steps in the scratchpad during the training process. While this was inspired by the findings of Min et al. (2022), it is inherently different. Min et al. (2022) show that using random labels in ICL demonstrations caused minimal degradation when compared to the gold labels. However, those models were trained on gold labels and then evaluated on multiple downstream tasks. In our setting, the model is trained and evaluated on a single arithmetic task. Further, the final result(or label) is left untouched as the correct answer to the arithmetic operation. We only replace the intermediate steps. The goal of this study is to verify whether the model actually learns to reason using the given intermediate steps or merely uses the scratchpad to improve its expressivity. We compare the performance of training with our simplified scratchpad formatting, which includes accurate $A$ (digit sum) and $C$ (carry) information, with formatting that includes random $A$, random $C$, or random $A$ and $C$ for each intermediate step, as depicted in Figure 1.</p>
<p>The results in Figure 9, demonstrate that the inclusion of noisy labels can impede sample efficiency. However, with enough samples, the model ultimately achieves full accuracy. This suggests that while the model is capable of leveraging the information contained in the intermediate steps, it can also gradually learn how to perform addition while disregarding the presence of noisy intermediate steps.</p>
<p>Model robustness to noise in the auto-regressive output. In this analysis, we explore the robustness of models trained on plain or reverse formatted data (without noise) when exposed to noise during an auto-regressive generation process. In particular, we aim to unravel how much the</p>
<p>learned mapping of the $i$-th output relies on the operands and preceding tokens in the addition result, given that transformer models generate tokens sequentially in an autoregressive manner, making them prone to error propagation.</p>
<p>For this experiment, we focus on 3-digit addition. We train models on either plain or reverse format data and evaluate the accuracy of next-token predictions when the output sequence contains noise. Specifically, in the plain format setting, we expect a well-performing model to generate the correct output tokens $\mathrm{O}<em 2="2">{3}, \mathrm{O}</em>}, \mathrm{O<em 3="3">{1}$ sequentially, where $\mathrm{O}</em>}=\mathrm{C<em 2="2">{3}, \mathrm{O}</em>}=\mathrm{C<em 1="1">{2}, \mathrm{O}</em>}=\mathrm{C<em 3="3">{1}$, and $\mathrm{C}</em>} \mathrm{C<em 1="1">{2} \mathrm{C}</em>}$ represents the correct answer. We consider two types of perturbation: (i) random perturbation, where we modify the first two output tokens $\mathrm{O<em 2="2">{3} \mathrm{O}</em>}$ to random numbers different from $\mathrm{C<em 2="2">{3} \mathrm{C}</em>}$, and (ii) precise perturbation, where we perturb only the second output token $\mathrm{O<em 3="3">{2}$ by 1 . The second case is particularly relevant since a common error case is where the model misses a digit by 1 . We provide the model with an expression of the form " $\mathrm{A}</em>} \mathrm{~A<em 1="1">{2} \mathrm{~A}</em>}+\mathrm{B<em 1="1">{3} \mathrm{~B}</em>} \mathrm{~B<em 3="3">{1}=\mathrm{O}</em>} \mathrm{O<em 3="3">{2}$ ", where $\mathrm{O}</em>} \mathrm{O<em 3="3">{2}$ can be either (i) a random incorrect number, i.e., $\mathrm{O}</em>} \mathrm{O<em 3="3">{2} \neq \mathrm{C}</em>} \mathrm{C<em 2="2">{2}$, or (ii) $\mathrm{O}</em> \pm 1 \bmod 10$, and observe the next token generated by the model. A corresponding process is deployed for the reverse format, introducing a noisy sequence to models trained on reverse format data.}=\mathrm{C}_{2</p>
<p>To evaluate the performance, we define two accuracy criteria for $\mathrm{O}<em 1="1">{1}$ : exact accuracy, reckoning $\mathrm{O}</em>}$ as accurate only when $\mathrm{O<em 1="1">{1}=\mathrm{C}</em>}$, and relaxed accuracy, considering $\mathrm{O<em 1="1">{1}$ correct if it deviates from the original output $\mathrm{C}</em>}$ by at most 1 . In other words, $\mathrm{C<em 1="1">{1}=\mathrm{O}</em>}, \mathrm{C<em 1="1">{1}=\mathrm{O}</em>}+1 \bmod 10$ or $\mathrm{C<em 1="1">{1}=\mathrm{O}</em>-1 \bmod 10$.</p>
<p>Table 3: Prediction accuracy for the third digit output under different types of noise in the preceding output tokens. Random perturbation, applies random flips whereas precise perturbation shifts the preceding output tokens by 1 . Relaxed accuracy, allows for a $\pm 1$ deviation from the true output whereas Exact accuracy is strict. Reverse consistently outputs a number that is at most 1 different from the true output, even in the presence of noise. The plain format has high exact accuracy in the presence of precise perturbation, as the noise in the output token has a lower impact on predicting the next token, which is of lower significance. However, with completely random noise, the plain format shows poor performance, suggesting a strong dependence on all digits. (See Lemma 1 and 2).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Perturbation Type</th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Precise</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">Plain</td>
<td style="text-align: center;">Reverse</td>
</tr>
<tr>
<td style="text-align: left;">Exact Acc</td>
<td style="text-align: center;">$49.88 \%$</td>
<td style="text-align: center;">$\mathbf{8 1 . 2 6 \%}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 8 5 \%}$</td>
<td style="text-align: center;">$90.47 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Relaxed Acc</td>
<td style="text-align: center;">$61.55 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
</tr>
</tbody>
</table>
<p>The results presented in Table 3 reveal intriguing findings. We observe that the reverse format consistently outputs a result that deviates by no more than 1 from the true answer, regardless of whether the preceding outputs $\mathrm{O}<em 2="2">{3} \mathrm{O}</em>$ are subjected to random or precise perturbation. This consistency can be explained by Lemma 2, indicating that the reverse format only requires learning a straightforward function of digit-wise addition for each corresponding position, along with the carry-on ( 0 or 1 ). Therefore, even with noise in the preceding tokens, the model accurately performs digit-wise addition, albeit with occasional carry-on prediction errors. With an exact accuracy of $81.26 \%$ even in the presence of random perturbation, the reverse format demonstrates the model's ability to rely less on the preceding output tokens, indicating a robust learned output mapping.
On the contrary, models using the plain format have to decipher a more intricate function drawing from all digits within the sequence, as described by Lemma 1. Given that in addition, carry operations transition from right to left (i.e., least to most significant digit), the introduction of precise perturbation on preceding output tokens, which possess higher significance, has a minor impact on the output (which has less significance). As a result, models trained using the plain format attain an exact accuracy rate of $99.85 \%$ and a relaxed accuracy of $100 \%$ for cases involving precise perturbation. Interestingly, under purely random perturbation, the plain format struggles, leading to a reduced relaxed accuracy of $61.55 \%$ and exact accuracy of $49.88 \%$. This suggests that the output mapping learned by the plain format is not merely a function of the two operands but rather enmeshed in complex dependencies on preceding output tokens.</p>
<h1>7 Extending to Longer Digit Addition</h1>
<p>In this section, we extend our experiments beyond 3-digit addition and explore longer-digit settings, ranging up to 10 digits. Our aim is to investigate whether our previous findings regarding the sample efficiency of reverse and scratchpad formats hold true for larger numbers of digits.
We begin by observing that the phase transition behavior observed in previous sections also applies to longer-digit addition. Furthermore, we discover that the advantages of using reverse and scratchpad formats become even more pronounced as the number of digits increases. Next, we examine the number of training samples required to learn $k+1$ digit addition when fine-tuning a pretrained model trained on $k$ digit addition. We find that while the number of samples needed to further learn $k+1$ digit addition remains relatively consistent for reverse and scratchpad formats, the plain format requires an increasing number of samples.</p>
<p>Experimental setup and data generation. To explore the performance of the model in higher-digit addition scenarios, we extend the experimental setup described in Section 3. We adopt a balanced sampling approach for training data with $D$ digits, ensuring an equal number $d$ of all combinations of digits for both operands as follows:
We begin by sampling all 100-digit additions. For the remaining number of digits, ranging from 2 to $D$, we generate addition examples of the form "A + B = C". The two operands, A and B, are randomly sampled $d=\lfloor(N-100) /(D(D+1) / 2-1)\rfloor$ times for every $D$, where $N$ is the total number of training examples. Operand A is sampled between $\left[10^{k_{1}-1}, 10^{k_{1}}-1\right]$ and operand B is sampled between $\left[10^{k_{2}-1}, 10^{k_{2}}-1\right]$, for all $1 \leq k_{1} \leq k_{2} \leq D$, excluding the case where $k_{1}=k_{2}=1$. After sampling the two operands, we randomly interchange them to cover cases where A has fewer digits than B and vice versa.</p>
<h3>7.1 Training from Random Initialization</h3>
<p>(a) 5-digit Addition
(b) 7-digit Addition
(c) 10-digit Addition
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Comparison of sample efficiency for 5, 7 and 10-digit additions: performance of models trained with varying numbers of addition samples on each data format. The plain format data requires an increasing number of training examples for higher digits, while the number of samples required for other methods remains relatively consistent.</p>
<p>We repeat the experiment from Section 3 on nanoGPT with longer digits. The results shown in Figure 10 demonstrate a similar behavior to the findings observed in Figure 6 for 3-digit addition. This indicates that our previous observations generalize to longer sequence lengths. Notably, the performance gap between the modified formats (reverse, simplified scratchpad, and detailed scratchpad) and the plain format becomes even more significant in the context of higher digits. While the plain format requires an increasing number of training examples to learn higher-digit additions, the reverse or scratchpad formats exhibit a more consistent requirement in terms of the number of training examples.
This prompts us to explore the differences between each format in a fine-tuning setting. Specifically, we ask whether a model trained on reverse or scratchpad-formatted $k$ digit addition data would find it easier to learn $k+1$ digit addition compared to a model trained with plain format addition.</p>
<h1>7.2 Fine-Tuning from Pretrained Models</h1>
<p>In this section, we investigate the generalization ability of transformer models, specifically focusing on their capacity to learn higher-digit additions based on their knowledge of lower-digit additions. Additionally, we explore how the choice of data format affects the number of samples required to learn higher-digit additions.</p>
<h2>Forgetting of $k$-digit addition when trained on $k+1$-digit addition.</h2>
<p>We begin by fine-tuning a model that was initially trained on 3-digit addition. We fine-tune this model using 4-digit addition training data, with each data format being used separately. To mitigate the "catastrophic forgetting" phenomenon, we experiment with different learning rates, gradually reducing the magnitude. We continue this process until the learning rate becomes too small for the model to effectively learn 4-digit addition.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Accuracy of 1 to 4-digit additions during fine-tuning of a pretrained model on 3-digit additions using different data formats. The model is fine-tuned using only 4-digit addition data with corresponding formats. We observe that the plain format 'forgets' 1 to 3-digit additions entirely when learning 4-digit addition. In contrast, the detailed scratchpad method successfully learns 4-digit addition while maintaining high performance on 1 to 3-digit additions.</p>
<p>The results depicted in Figure 11 reveal interesting insights about the fine-tuning process. When training the model using the plain format with only 4-digit addition data, there is an immediate drop in accuracy for 1 to 3 digit additions. This indicates that the model experiences significant forgetting of previously learned additions. In contrast, the reverse and scratchpad methods exhibit a more favorable behavior. The model trained with these methods does not completely forget 1 or 2 digit additions while learning 4-digit addition. Remarkably, the detailed scratchpad method stands out by enabling the model to learn 4-digit addition without compromising its performance on 1 to 3 digit additions. Although there is a slight decrease in performance for 3-digit additions initially, the model quickly recovers and picks up the knowledge again as it trains on 4-digit additions.
This result can be explained by the hypothesis that learning a $k+1$ digit addition from a $k$-digit model is an incremental process for the detailed scratchpad method. The model already has a solid foundation in understanding the intermediate steps involved in addition, so it only needs to adapt to longer sequences. In contrast, for the plain format, learning higher-digit additions requires the model to establish new mappings to generate correct outputs, which is a more challenging task.</p>
<p>Sample efficiency of fine-tuning $k$-digit models with $k+1$-digit examples. Building upon our previous findings that fine-tuning a model solely on $k+1$-digit addition leads to a loss in performance for $k$-digit addition, we modify our approach to prevent the loss of performance in the $k$-digit addition task. Instead of training solely on $k+1$-digit examples, we construct a dataset that includes all addition tasks from 1-digit to $k+1$-digit, with the method described in the previous section. By doing so, we aim to maintain the performance of 1 to $k$-digit addition while enabling the model to learn $k+1$-digit addition during fine-tuning.</p>
<p>In this experiment, we investigate the number of $k+1$-digit training examples required for the model to effectively learn $k+1$-digit addition when fine-tuning a pretrained model on $k$-digit addition. It is important to note that this setting differs from the previous section (Section 7.1), where we focused on training models from random initialization. Here, we specifically focus on the fine-tuning process. We fine-tune individual models pretrained on each data format (using $k$-digit addition) and further train them using the same data format on a new dataset that includes all addition examples from 1-digit to $k+1$-digit.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Fine-tuning performance of pretrained $k$-digit models using varying numbers of $k+1$ digit examples, with corresponding data formats. The plain format requires an increasing number of $k+1$-digit examples as the number of digits $(k+1)$ increases. In contrast, the modified formats (reverse, scratchpad) exhibit consistent performance across different numbers of digits, requiring a relatively consistent number of examples to learn the additional digit.
The results in Figure 12 demonstrate the number of $k+1$-digit addition samples required for a pretrained model capable of performing $k$-digit addition to learn the addition of $k+1$ digits. The findings reveal that modified formats (reverse, scratchpad) require a relatively small number of samples (between 1000 and 5000) to learn the addition of an extra digit. In contrast, the plain format necessitates a significantly larger number of training examples, with the requirement increasing as the number of digits grows.
This observation aligns with our previously established Lemma 2 and Lemma 1, which suggest that learning higher-digit addition in the reverse format involves processing the $i$-th digit of the operands and carrying from the previous position. This operation remains consistent regardless of the number of digits being added. As a result, the model primarily needs to learn how to handle longer digits to perform addition effectively.
In contrast, the plain addition format requires the model to learn a more complex function that incorporates all digits from both operands. As the number of digits increases, the complexity of this function grows as well. This highlights the greater difficulty faced by the plain format in accommodating additions with a larger number of digits.</p>
<h1>7.3 Impact of Formats on Fine-Tuning</h1>
<p>We delve deeper into the impact of different formats on the fine-tuning process. Specifically, we investigate whether training a model in one format helps in learning addition in another format,</p>
<p>and vice versa. To conduct this analysis, we begin with a model trained on each data format using 3-digit addition examples. We then individually fine-tune these pretrained models using different data formats, on 4-digit addition examples.
The results depicted in Figure 13 highlight some interesting findings. Firstly, we observe that a model trained with the same format as the finetuning format exhibits faster learning in terms of the number of iterations. For instance, training a model with the plain format outperforms training a model pretrained with scratchpad formats. This suggests that the model benefits from the consistency and familiarity provided by the same format throughout the training process.
Additionally, we notice that fine-tuning a detailed scratchpad pretrained model on other formats proves to be more challenging. This observation can be attributed to the need for the model to "unlearn" the intricacies of the verbose detailed scratchpad format and adapt to the new format. For example, the plain format does not involve the use of alphabet characters in the data, so a model pretrained with the plain format would have a low probability of generating alphabetic outputs. In contrast, a detailed scratchpad pretrained model would have encountered various alphabets and may have a tendency to output them. Therefore, adjusting to a new format requires additional effort for the model to "unlearn" the patterns specific to the previous format and effectively learn the new format it is being trained on.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Performance of fine-tuning a 3-digit model trained on different data formats (plain, reverse, simple scratchpad, detailed scratchpad, and random initialization) individually with different data formats of 4-digit addition. The results demonstrate that fine-tuning yields the best performance when the pretrained model and the fine-tuning format are consistent. Notably, fine-tuning a detailed scratchpad format model shows suboptimal performance. We hypothesize that this is due to the need for the model to "unlearn" the rigid and verbose format and adapt to the new format.</p>
<p>These findings highlight the importance of considering format consistency during the fine-tuning process, as it can impact the efficiency and effectiveness of the learning process. We will delve further into this topic in the upcoming section 10, where we fine-tune pretrained GPT-3 models. Notably, we observe that fine-tuning with reverse or simplified scratchpad formats actually yields worse results compared to fine-tuning with plain formats. For a detailed exploration of these observations, please refer to the forthcoming section.</p>
<h1>8 Teaching Arithmetic Operations Beyond Addition</h1>
<p>While this study has a primary focus on the addition operation and aims to comprehend the significance of data sampling and formatting, its findings are applicable beyond the realm of addition alone. In this section, we expand our examination to include other arithmetic operations, thus demonstrating the broader applicability of our insights. We consider a mix of arithmetic tasks, including binary operations like subtraction and multiplication, and unary operations such as sine and square root. Each operation entails its unique challenges and intricacies. For instance, subtraction introduces the concept of negative numbers, multiplication can generate significantly longer outputs, and sine and square root functions entail computations involving floating-point numbers, which are considered up to four digits of precision in our work.
We acknowledge that while our examination is detailed, it does not encompass all the fundamental arithmetic operations or the entire scope of floating-point arithmetic. Specifically, our focus is primarily on integer arithmetic for binary operations, considering a limited length of digits. Additionally, for unary operations, we confine ourselves to a restricted number of digits below the decimal point.
In Section 8.1, we delve into each arithmetic operation individually, exploring the impact of data formatting and determining the relevancy of our insights across disparate tasks. Further, in Section 8.2,</p>
<p>we perform an analysis of joint training across all five tasks, investigating the potential performance implications for each individual task.</p>
<h1>8.1 Extended Arithmetic Operations</h1>
<p>In order to extend our analysis to arithmetic operations beyond addition, we consider the following tasks:</p>
<p>Subtraction (-). We consider subtraction of positive numbers up to 3 digits, written as $\mathrm{A}<em 2="2">{3} \mathrm{~A}</em>} \mathrm{~A<em 3="3">{1}-\mathrm{B}</em>} \mathrm{~B<em 1="1">{2} \mathrm{~B}</em>}=\mathrm{C<em 2="2">{3} \mathrm{C}</em>} \mathrm{C<em 3="3">{1}$ in (i) plain formatting, and $\$ \mathrm{~A}</em>} \mathrm{~A<em 1="1">{2} \mathrm{~A}</em>}-\mathrm{B<em 1="1">{3} \mathrm{~B}</em>} \mathrm{~B<em 1="1">{1}=\mathrm{C}</em>} \mathrm{C<em 3="3">{2} \mathrm{C}</em> \$$ in (ii) reverse formatting. As with addition, scratchpad-based methods (iii, iv), present the intermediate steps of digit-wise subtraction and handling of carry-ons. These steps proceed from the least significant bit (LSB) to the most significant bit (MSB). If the final result after computing all the digit-wise subtractions is negative, we subtract the number in the most significant bit (MSB) position multiplied by 10 to the power of (number of digits in the output - 1) from the remaining digits in the output. In Section 6.2, we present an alternative version of the detailed scratchpad formatting for subtraction.</p>
<p>Multiplication ( $\times$ ). We consider multiplication of positive numbers up to 2-digits. (i) Plain formatting examples are formatted as $\mathrm{A}<em 1="1">{2} \mathrm{~A}</em>} * \mathrm{~B<em 1="1">{2} \mathrm{~B}</em>}=\mathrm{C<em 3="3">{4} \mathrm{C}</em>} \mathrm{C<em 1="1">{2} \mathrm{C}</em>}$, while (ii) reverse formatting is formatted as $\$ \mathrm{~A<em 1="1">{2} \mathrm{~A}</em>} * \mathrm{~B<em 1="1">{2} \mathrm{~B}</em>}=\mathrm{C<em 2="2">{1} \mathrm{C}</em>} \mathrm{C<em 4="4">{3} \mathrm{C}</em> \$$. The (iv) detailed scratchpad method simplifies each intermediate step by conducting a series of multiplications between the first operand and each digit of the second operand, starting from the least significant bit (LSB) and moving toward the most significant bit (MSB). For each step, we multiply the result by an exponentiation of 10 corresponding to the relative digit position.</p>
<p>Sine (sin). We consider decimal numbers within the range $[-\pi / 2, \pi / 2]$, truncated to 4-digit precision. (i) Plain formatting examples are formatted as $\sin \left(\mathrm{A}<em 1="1">{0} . \mathrm{A}</em>} \mathrm{~A<em 3="3">{2} \mathrm{~A}</em>} \mathrm{~A<em 0="0">{4}\right)=\mathrm{B}</em>} . \mathrm{B<em 2="2">{1} \mathrm{~B}</em>} \mathrm{~B<em 4="4">{3} \mathrm{~B}</em>+\cdots$. These intermediate steps involve exponentiation, which may not be any easier to compute than the sine operation itself.}$. For (iv) detailed scratchpad method, we include the Taylor series expansion steps for sine, which is represented as $\sin (x)=x-\frac{1}{3!} x^{3}+\frac{1}{5!} x^{5}-\frac{1}{7!} x^{7</p>
<p>Square Root $(\sqrt{ })$. We consider decimal numbers within $[1,10)$, truncated to 4 -digits of precision with the format, written as $\operatorname{sqrt}\left(\mathrm{A}<em 1="1">{0} . \mathrm{A}</em>} \mathrm{~A<em 3="3">{2} \mathrm{~A}</em>} \mathrm{~A<em 0="0">{4}\right)=\mathrm{B}</em>} . \mathrm{B<em 2="2">{1} \mathrm{~B}</em>} \mathrm{~B<em 4="4">{3} \mathrm{~B}</em>$ is initialized as the floor of the square root value of the operand $x$. These intermediate steps involve a division operation, which can be as complex as the square root operation itself.
For evaluation of sine and square root, we classify the result $\hat{y_{i}}$ as correct if the absolute difference between $\hat{y_{i}}$ and the ground truth value $y_{i}$ is less than or equal to a predefined threshold $\epsilon \geq 0$.
For each arithmetic task, we explore both the plain format and the detailed scratchpad format. The detailed scratchpad formatting for each task is illustrated in Figure 14 and Appendix D. For subtraction, the process involves breaking down the operation into intermediate steps of digit-wise subtraction, including carry-ons when necessary. Unlike addition, subtraction requires an additional step to handle cases where the first operand is smaller than the second. Further details on the detailed scratchpad for subtraction can be found in Section 6.2. For multiplication, each intermediate step carries out a 2-digit $\times 1$-digit multiplication between the first operand and each separate digit of the second operand. For sine and square root, we utilize a sequence of iterative approximations instead of algorithmic explanations. Specifically, Taylor's series expansion steps for sine and Newton's method steps for square root are used. It is important to note that while addition, subtraction, and multiplication are broken down into simpler operations at each step, CoT for sine and square root functions requires intermediate steps involving operations like exponentiation or division, which might not be inherently simpler.}$ for (i) plain formatting. For (iv) detailed scratchpad method, we enumerate each step of Newton's method to compute the square root function. The iterative formula is given by $x_{n}=\frac{1}{2}\left(x_{n-1}+\frac{x}{x_{n-1}}\right)$, where $x_{0</p>
<h1>Detailed scratchpad formatting for different arithmetic tasks</h1>
<p>Examples of detailed scratchpad formatting for different arithmetic tasks:
(1) Subtraction - includes borrows for intermediate steps, (2) Multiplication - decomposes the second operand for 2-digit $\times 1$-digit multiplication at each step, (3) Sine - utilizes Taylor series expansion, and (4) Square root - employs Newton's method.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subtraction</th>
<th style="text-align: center;">Sine</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input:</td>
<td style="text-align: center;">Input:</td>
</tr>
<tr>
<td style="text-align: center;">$128-367$</td>
<td style="text-align: center;">$\sin (1.5707)$</td>
</tr>
<tr>
<td style="text-align: center;">Target:</td>
<td style="text-align: center;">Target:</td>
</tr>
<tr>
<td style="text-align: center;"><scratch></td>
<td style="text-align: center;"><scratch></td>
</tr>
<tr>
<td style="text-align: center;">$[1,2,8]$ has 3 digits.</td>
<td style="text-align: center;">$\mathrm{x} _0=1.5707$</td>
</tr>
<tr>
<td style="text-align: center;">$[3,6,7]$ has 3 digits.</td>
<td style="text-align: center;">$\mathrm{x} _1: \mathrm{x} _0-1 / 3!\star\left(\mathrm{x}^{\wedge} 3\right), \mathrm{x} _1=0.9247$</td>
</tr>
<tr>
<td style="text-align: center;">$[1,2,8]-[3,6,7], A=[], C=0,8-7-0=1$</td>
<td style="text-align: center;">$\mathrm{x} _2: \mathrm{x} _1+1 / 5!\star\left(\mathrm{x}^{\wedge} 5\right), \mathrm{x} _2=1.0043$</td>
</tr>
<tr>
<td style="text-align: center;">, A-&gt;1, C-&gt;0</td>
<td style="text-align: center;">$\mathrm{x} _3: \mathrm{x} _2-1 / 7!\star\left(\mathrm{x}^{\wedge} 7\right), \mathrm{x} _3=0.9996$</td>
</tr>
<tr>
<td style="text-align: center;">$[1,2]-[3,6], A=[1], C=0,2-6-0+10=6$</td>
<td style="text-align: center;">$\mathrm{x} _4: \mathrm{x} _3+1 / 9!\star\left(\mathrm{x}^{\wedge} 9\right), \mathrm{x} _4=0.9997$, END</td>
</tr>
<tr>
<td style="text-align: center;">, A-&gt;6, C-&gt;-1</td>
<td style="text-align: center;"></scratch></td>
</tr>
<tr>
<td style="text-align: center;">$[1]-[3], A=[6,1], C=-1,1-3-1=-3$,</td>
<td style="text-align: center;">0.9997</td>
</tr>
<tr>
<td style="text-align: center;">A-&gt;-3, C-&gt;-1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">[] - [], A=[-3,6,1]</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">-300+61=-239 , END</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></scratch></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">-239</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Multiplication</td>
<td style="text-align: center;">Sqrt</td>
</tr>
<tr>
<td style="text-align: center;">Input:</td>
<td style="text-align: center;">Input:</td>
</tr>
<tr>
<td style="text-align: center;">$12 * 36$</td>
<td style="text-align: center;">sqrt (2.7174)</td>
</tr>
<tr>
<td style="text-align: center;">Target:</td>
<td style="text-align: center;">Target:</td>
</tr>
<tr>
<td style="text-align: center;"><scratch></td>
<td style="text-align: center;"><scratch></td>
</tr>
<tr>
<td style="text-align: center;">$[1,2]$ has 2 digits.</td>
<td style="text-align: center;">$\mathrm{x} _0=1$</td>
</tr>
<tr>
<td style="text-align: center;">$[3,6]$ has 2 digits.</td>
<td style="text-align: center;">$\mathrm{x} _1: 1 / 2 *(1+2.7175 / 1)=1.8587, \mathrm{x} _1=1.8587$</td>
</tr>
<tr>
<td style="text-align: center;">$[1,2]* 6, A=[7,2], k=1, B=[7,2], C$</td>
<td style="text-align: center;">$\mathrm{x} _2: 1 / 2 *(1.8587+2.7175 / 1.8587)=1.6603, \mathrm{x} _2$</td>
</tr>
<tr>
<td style="text-align: center;">$=0+72=72$</td>
<td style="text-align: center;">$=1.6603$</td>
</tr>
<tr>
<td style="text-align: center;">$[1,2]* 3, A=[3,6], k=10, B=[3,6,0]$,</td>
<td style="text-align: center;">$\mathrm{x} _3: 1 / 2 *(1.6603+2.7175 / 1.6603)=1.6485, \mathrm{x} _3$</td>
</tr>
<tr>
<td style="text-align: center;">C=72+360=432 , END</td>
<td style="text-align: center;">$=1.6485$</td>
</tr>
<tr>
<td style="text-align: center;"></scratch></td>
<td style="text-align: center;">$\mathrm{x} _4: 1 / 2 *(1.6485+2.7175 / 1.6485)=1.6484, \mathrm{x} _4$</td>
</tr>
<tr>
<td style="text-align: center;">432</td>
<td style="text-align: center;">$=1.6484$, END</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></scratch></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.6484</td>
</tr>
</tbody>
</table>
<p>Figure 14: Examples of the detailed scratchpad format for different arithmetic tasks such as subtraction, sine, multiplication, and square root.</p>
<p>The results depicted in Figure 15 indicate that similar to the findings of addition, the detailed scratchpad format significantly improves performance over plain or reverse formats and yields efficient results even with few samples for subtraction and multiplication tasks. Interestingly, we find reverse is not particularly effective in multiplication. On the other hand, the detailed scratchpad format exhibits reduced efficiency for $\sin$ and $\sqrt{ }$ compared to other operations $(+,-, \times)$. This discrepancy can be traced back to the complexity of the intermediate steps involved in the detailed scratchpad. While addition, subtraction, and multiplication are decomposed into simpler functions, sine and square root operations involve more intricate operations. For a broader analysis of the error profile, see Appendix B.4.</p>
<h3>8.2 Jointly Training on All Five Arithmetic Tasks</h3>
<p>So far, we only considered the problem of learning different arithmetic operations individually. In this section, we study the effect of jointly training on all five arithmetic tasks - addition, subtraction, multiplication, sine, and square root. We construct a single train dataset incorporating all task $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}=\left{\mathcal{D}</em>}}^{+}, \mathcal{D<em _text="\text" _train="{train">{\text {train }}^{-}, \mathcal{D}</em>}}^{+}, \mathcal{D<em _text="\text" _train="{train">{\text {train }}^{\text {tio }}, \mathcal{D}</em>, \ldots)$. We consider 10,000 training examples for each task of addition, subtraction, sine, and square root and 3,000 for multiplication.}}^{\sqrt{ }}\right}$, and randomize the sequence of tasks in our train samples. For example, a randomly chosen segment of the training data may exhibit a task order such as $(+,-, \sin .-, \times, \times, \sqrt{ </p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 15: Performance of 3-digit subtraction, 2-digit multiplication, 4-digit precision sine and square root with varying data formats. As with addition, reverse always produces improved sample complexity and performance for all operations. For sine and square root, scratchpad formatting provides limited improvement. This discrepancy can be attributed to the complexity of the intermediate steps involved in the detailed scratchpad.</p>
<p>The model's performance, after training on our joint dataset $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}$, is evaluated in both zero-shot and few-shot settings. These results are also compared with the performance of models that were trained separately on each dataset $\left(\mathcal{D}</em>}}^{+}, \mathcal{D<em _text="\text" _train="{train">{\text {train }}^{-}, \mathcal{D}</em>}}^{\times}, \mathcal{D<em _text="\text" _train="{train">{\text {train }}^{\text {sine }}, \mathcal{D}</em>$. In the few-shot setting, each task is given examples from any of the five arithmetic tasks (not necessarily related to the test task under consideration) or prompt texts, followed by test queries specific to the task of interest. For further details on the few-shot prompting methods used, please refer to Section 9.}}^{\vee}\right)$, identical to those used to construct $\mathcal{D}_{\text {train }</p>
<p>Table 4 shows that joint training significantly enhances the zero-shot performance for multiplication and square root tasks, yet it slightly reduces the performance for subtraction. Generally, few-shot prompting exhibits improved performance. Notably, the performance of few-shot prompting remains consistent regardless of whether the exemplars provided are from unrelated tasks or are task-specific. We propose that this consistency is due to our randomized task sequence during training, which presents the model with numerous instances where one task directly follows another, thus simulating few-shot prompting with different tasks. Furthermore, we observe that text prompting performs similar to zero-shot. We conjecture that this is because the training data does not include text data and the model has never encountered text and therefore, text prompting serves as a random prefix attached to our test query.</p>
<h1>9 Mixing Shakespeare with Arithmetic Data</h1>
<p>Until now, our focus was primarily on models trained exclusively on arithmetic tasks. However, in practice, large language models (LLMs) utilize a combination of arithmetic and text data for training. In this section, we broaden our scope by incorporating both addition samples and text into our pretraining data. We then evaluate the trained models with various few-shot prompts to analyze if the model is able to effectively identify the correct context.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ In this paper, we adopt the definition that a carry-on operation involves transferring information from one digit position to another position of higher significance. Therefore, we refer to the "borrow" operation in subtraction as a carry operation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
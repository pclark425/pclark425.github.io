<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6088 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6088</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6088</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-101fc5b569ee9b9e11850f8b5d86a6dd74ee7258</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/101fc5b569ee9b9e11850f8b5d86a6dd74ee7258" target="_blank">Universal Sentence Encoder for English</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer.</p>
                <p><strong>Paper Abstract:</strong> We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6088",
    "paper_id": "paper-101fc5b569ee9b9e11850f8b5d86a6dd74ee7258",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0025119999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Universal Sentence Encoder for English</h1>
<p>Daniel Cer ${ }^{a 1}$, Yinfei Yang ${ }^{a 1}$, Sheng-yi Kong ${ }^{a}$, Nan Hua ${ }^{a}$, Nicole Limtiaco ${ }^{b}$, Rhomni St. John ${ }^{a}$, Noah Constant ${ }^{a}$, Mario Guajardo-Céspedes ${ }^{a}$, Steve Yuan ${ }^{c}$, Chris Tar ${ }^{a}$, Yun-Hsuan Sung ${ }^{a}$, Brian Strope ${ }^{a}$, Ray Kurzweil ${ }^{a}$<br>${ }^{a}$ Google AI<br>${ }^{b}$ Google AI<br>${ }^{c}$ Google<br>${ }^{a}$ Google AI<br>${ }^{a}$ New York, NY Cambridge, MA</p>
<h2>Abstract</h2>
<p>We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.</p>
<h2>1 Introduction</h2>
<p>We present easy-to-use sentence-level embedding models with good transfer task performance even when using remarkably little training data. ${ }^{1}$ Model engineering characteristics allow for tradeoffs between accuracy versus memory and compute resource consumption.</p>
<h2>2 Model Toolkit</h2>
<p>Models are implemented in TensorFlow (Abadi et al., 2016) and are made publicly available on TensorFlow Hub. ${ }^{2}$ Listing 1 provides an example</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><code>import tensorflow_hub as hub
embed = hub.Module("https://tfhub.dev/google/"
*universal-sentence-encoder/2")
embedding = embed(["Hello World!"])</code></p>
<p>Listing 1: Python sentence embedding code.
code snippet to compute a sentence-level embedding from a raw untokenized input string. ${ }^{3}$ The resulting embedding can be used directly or incorporated into a downstream model for a specific task. ${ }^{4}$</p>
<h2>3 Encoders</h2>
<p>Two sentence encoding models are provided: (i) transformer (Vaswani et al., 2017), which achieves high accuracy at the cost of greater resource consumption; (ii) deep averaging network (DAN) (Iyyer et al., 2015), which performs efficient inference but with reduced accuracy.</p>
<h3>3.1 Transformer</h3>
<p>The transformer sentence encoding model constructs sentence embeddings using the encoding sub-graph of the transformer architecture (Vaswani et al., 2017). The encoder uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of other words. The context aware word representations are averaged together to obtain a sentence-level embedding.</p>
<p>We train for broad coverage using multi-task learning, with the same encoding model supporting multiple downstream tasks. The task types include: a Skip-Thought like task (Kiros et al.,
${ }^{3}$ Basic text preprocessing and white-space tokenization is performed internally. Preprocessing lowercases the text and removes punctuation. OOV items are handled using string hashing to index into 400,000 OOV embeddings.
${ }^{4}$ Visit https://colab.research.google.com/ to try the code snippet in Listing 1. Example code and documentation is available on the TF Hub website.</p>
<p>2015); ${ }^{5}$ conversational response prediction (Henderson et al., 2017); and a select supervised classification task that improves sentence embeddings. ${ }^{6}$ The transformer encoder achieves the best transfer performance. However, this comes at the cost of compute time and memory usage scaling dramatically with sentence length.</p>
<h3>3.2 Deep Averaging Network (DAN)</h3>
<p>The DAN sentence encoding model begins by averaging together word and bi-gram level embeddings. Sentence embeddings are then obtain by passing the averaged representation through a feedforward deep neural network (DNN). The DAN encoder is trained similar to the transformer encoder. Multitask learning trains a single DAN encoder to support multiple downstream tasks. An advantage of the DAN encoder is that compute time is linear in the length of the input sequence. Similar to Iyyer et al. (2015), our results demonstrate that DANs achieve strong baseline performance on text classification tasks.</p>
<h3>3.3 Encoder Training Data</h3>
<p>Unsupervised training data are drawn from a variety of web sources. The sources are Wikipedia, web news, web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) in order to further improve our representations (Conneau et al., 2017). Since the only supervised training data is SNLI, the models can be used for a wide range of downstream supervised tasks that do not overlap with this dataset. ${ }^{7}$</p>
<h2>4 Transfer Tasks</h2>
<p>This section presents the data used for the transfer learning experiments and word embedding association tests (WEAT): (MR) Movie review sentiment on a five star scale (Pang and Lee, 2005); (CR) Sentiment of customer reviews (Hu and Liu, 2004); (SUBJ) Subjectivity of movie reviews and plot summaries (Pang and Lee, 2004);</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST</td>
<td style="text-align: center;">67,349</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">1,821</td>
</tr>
<tr>
<td style="text-align: center;">STS Bench</td>
<td style="text-align: center;">5,749</td>
<td style="text-align: center;">1,500</td>
<td style="text-align: center;">1,379</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">5,452</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">MR</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10,662</td>
</tr>
<tr>
<td style="text-align: center;">CR</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3,775</td>
</tr>
<tr>
<td style="text-align: center;">SUBJ</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10,000</td>
</tr>
<tr>
<td style="text-align: center;">MPQA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10,606</td>
</tr>
</tbody>
</table>
<p>Table 1: Transfer task evaluation sets.
(MPQA) Phrase opinion polarity from news data (Wiebe et al., 2005); (TREC) Fine grained question classification sourced from TREC (Li and Roth, 2002); (SST) Binary phrase sentiment classification (Socher et al., 2013); (STS Benchmark) Semantic textual similarity (STS) between sentence pairs scored by Pearson $r$ with human judgments (Cer et al., 2017); (WEAT) Word pairs from the psychology literature on implicit association tests (IAT) that are used to characterize model bias (Caliskan et al., 2017). ${ }^{8}$ Table 1 gives the number of samples for each transfer task.</p>
<h2>5 Transfer Learning Models</h2>
<p>For sentence classification transfer tasks, the output of the sentence encoders are provided to a task specific DNN. For the pairwise semantic similarity task, the similarity of sentence embeddings $u$ and $v$ is assessed using $-\arccos \left(\frac{u v}{|u||v|}\right) .{ }^{9}$</p>
<h3>5.1 Baselines</h3>
<p>For each transfer task, we include baselines that only make use of word-level transfer and baselines that make use of no transfer learning at all. For word-level transfer, we incorporate word embeddings from a word2vec skip-gram model trained on a corpus of news data (Mikolov et al., 2013). The pretrained word embeddings are included as input to two model types: a convolutional neural network model (CNN) (Kim, 2014); a DAN. The baselines that use pretrained word embeddings allow us to contrast word- vs. sentence-level transfer. Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings. For reference, we compare with InferSent (Conneau et al., 2017) and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Skip-Thought with layer normalization (Ba et al., 2016) on sentence-classification tasks. On the STS Benchmark, we compare with InferSent and the state-of-the-art neural STS systems CNN (HCTI) (Shao, 2017) and gConv (Yang et al., 2018).</p>
<h3>5.2 Combined Transfer Models</h3>
<p>We explore combining the sentence and wordlevel transfer models by concatenating their representations prior to the classification layers. For completeness, we report results providing the classification layers with the concatenating of the sentence-level embeddings and the representations produced by baseline models that do not make use of word-level transfer learning.</p>
<h2>6 Experiments</h2>
<p>Experiments use our most recent transformer and DAN encoding models. ${ }^{10}$ Transfer task model hyperparamaters are tuned using a combination of Vizier (Golovin et al., 2017) and light manual tuning. When available, model hyperparameters are tuned using task dev sets. Otherwise, hyperparameters are tuned by cross-validation on task training data or the evaluation test data when neither training nor dev data are provided. Training repeats ten times for each task with randomly initialized weights and we report results by averaging across runs. Transfer learning is important when training data is limited. We explore using varying amounts of training data for SST. Contrasting the transformer and DAN encoders demonstrates trade-offs in model complexity and the training data required to reach a desired level of task accuracy. Finally, to assess bias in our encoders, we evaluate the strength of biased model associations on WEAT. We compare to Caliskan et al. (2017) who discovered that word embeddings reproduce human-like biases on implicit association tasks.</p>
<h2>7 Results</h2>
<p>Table 2 presents results on classification tasks. Using transformer sentence-level embeddings alone outperforms InferSent on MR, SUBJ, and TREC. The transformer sentence encoder also strictly outperforms the DAN encoder. Models that make use of just the transformer sentence-level embeddings tend to outperform all models that only use wordlevel transfer, with the exception of TREC and</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Model | MR | CR | SUBJ | MPQA | TREC | SST |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Sentence Embedding Transfer Learning |  |  |  |  |  |  |
| $U_{T}$ | 82.2 | 84.2 | 95.5 | 88.1 | 93.2 | 83.7 |
| $U_{D}$ | 72.2 | 78.5 | 92.1 | 86.9 | 88.1 | 77.5 |
| Word Embedding Transfer Learning |  |  |  |  |  |  |
| $\mathrm{CNN}<em 2="2" v="v" w="w">{w 2 v}$ | 75.1 | 80.5 | 91.1 | 80.3 | 96.6 | 84.1 |
| $\mathrm{DAN}</em>$ | 74.7 | 75.3 | 90.2 | 82.1 | 83.5 | 80.6 |
| Sentence Embedding Transfer Learning |  |  |  |  |  |  |
| + DNN/CNN with word-level transfer |  |  |  |  |  |  |
| $U_{T}+\mathrm{CNN}<em T="T">{w 2 v}$ | 80.1 | 85.2 | 95.8 | 88.4 | 98.7 | 85.3 |
| $U</em>}+\mathrm{DAN<em D="D">{w 2 v}$ | 81.4 | 86.4 | 93.7 | 87.5 | 97.0 | 86.0 |
| $U</em>}+\mathrm{CNN<em D="D">{w 2 v}$ | 76.7 | 82.0 | 91.2 | 85.2 | 97.1 | 85.1 |
| $U</em>}+\mathrm{DAN<em T="T">{w 2 v}$ | 76.4 | 81.0 | 94.0 | 88.0 | 92.6 | 82.2 |
| Sentence Embedding Transfer Learning |  |  |  |  |  |  |
| + DNN/CNN without word-level transfer |  |  |  |  |  |  |
| $U</em>}+\mathrm{CNN<em T="T">{r n d}$ | 82.7 | 88.6 | 93.6 | 87.8 | 98.5 | 88.9 |
| $U</em>}+\mathrm{DAN<em D="D">{r n d}$ | 80.6 | 84.8 | 94.3 | 86.0 | 98.6 | 86.2 |
| $U</em>}+\mathrm{CNN<em D="D">{r n d}$ | 78.0 | 82.9 | 90.2 | 87.8 | 96.2 | 83.2 |
| $U</em>}+\mathrm{DAN<em d="d" n="n" r="r">{r n d}$ | 76.4 | 84.9 | 94.0 | 85.3 | 98.1 | 86.2 |
| Baselines with No Transfer Learning |  |  |  |  |  |  |
| $\mathrm{CNN}</em>$ | 76.5 | 81.0 | 89.6 | 82.2 | 97.9 | 85.0 |
| $\mathrm{DAN}_{r n d}$ | 74.6 | 81.2 | 91.8 | 79.9 | 93.9 | 82.0 |
| Prior Work |  |  |  |  |  |  |
| InferSent | 81.1 | 86.3 | 92.4 | 90.2 | 88.2 | 84.6 |
| Skip Thght | 79.4 | 83.1 | 93.7 | 89.3 | - | - |</p>
<p>Table 2: Classification tasks. $U_{T}$ uses the transformer encoder for transfer learning, while $U_{D}$ uses the DAN encoder. $\mathrm{DAN} / \mathrm{CNN}<em d="d" n="n" r="r">{w 2 v}$ use pretrained w2v emb. $\mathrm{DAN} / \mathrm{CNN}</em>$ train rand. init. word emb. on the final classification task.</p>
<p>SST, on which $\mathrm{CNN}_{w 2 v}$ performs better. Transfer learning with DAN sentence embeddings tends to outperform a DAN with word-level transfer, except on MR and SST. Models with sentence- and word-level transfer often outperform similar models with sentence-level transfer alone.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">DEV</th>
<th style="text-align: center;">TEST</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Transformer Encoder</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.766</td>
</tr>
<tr>
<td style="text-align: center;">DAN Encoder</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.717</td>
</tr>
<tr>
<td style="text-align: center;">Prior Work</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">gConv (Yang et al., 2018)</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.808</td>
</tr>
<tr>
<td style="text-align: center;">CNN (HCTI) (Shao, 2017)</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.784</td>
</tr>
<tr>
<td style="text-align: center;">InferSent (Conneau et al., 2017)</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.758</td>
</tr>
</tbody>
</table>
<p>Table 3: STS Benchmark Pearson's $r$. Our prior gConv model (Yang et al., 2018) is a variant of our TF Hub transformer model tuned to STS.</p>
<p>Table 3 compares our models to strong baselines on the STS Benchmark. Our transformer embeddings outperform the sentence representations produced by InferSent. Moreover, computing similarity scores by directly comparing the representations produced by our encoders approaches</p>
<p>the performance of state-of-the-art neural models whose representations are fit to the STS task.</p>
<p>Table 4 illustrates transfer task performance for varying amounts of training data. With small quantities of training data, sentence-level transfer achieves surprisingly good performance. Using only 1 k labeled examples and the transformer embeddings for sentence-level transfer surpasses the performance of transfer learning using InferSent on the full training set of 67.3 k examples. Training with 1 k labeled examples and the transformer sentence embeddings surpasses wordlevel transfer using the full training set, $\mathrm{CNN}<em _rnd="{rnd" _text="\text">{w 2 v}$, and approaches the performance of the best model without transfer learning trained on the complete dataset, $\mathrm{CNN}</em>$.}} @ 67.3 \mathrm{k}$. Transfer learning is not always helpful when there is enough task training data. However, we observe that our best performing model still makes use of transformer sentencelevel transfer but combined with a CNN with no word-level transfer, $U_{T}+\mathrm{CNN}_{\text {rnd }</p>
<p>Table 5 contrasts Caliskan et al. (2017)'s findings on bias within GloVe embeddings with results from the transformer and DAN encoders. Similar to GloVe, our models reproduce human associations between flowers vs. insects and pleasantness vs. unpleasantness. However, our models demonstrate weaker associations than GloVe for probes targeted at revealing ageism, racism and sexism. ${ }^{11}$ Differences in word association patterns can be attributed to training data composition and the mixture of tasks used to train the representations.</p>
<h2>8 Resource Usage</h2>
<p>This section describes memory and compute resource usage for the transformer and DAN sentence encoding models over different batch sizes and sentence lengths. Figure 1 plots model resource consumption against sentence length. ${ }^{12}$</p>
<p>Compute Usage The transformer model time complexity is $O\left(n^{2}\right)$ in sentence length, while the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Model SST 1K SST 4K SST 16K SST 67.3K
Sentence Embedding Transfer Learning</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$U_{T}$</th>
<th style="text-align: center;">$\mathbf{8 4 . 8}$</th>
<th style="text-align: center;">$\mathbf{8 4 . 8}$</th>
<th style="text-align: center;">$\mathbf{8 4 . 8}$</th>
<th style="text-align: center;">$\mathbf{8 3 . 7}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$U_{D}$</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">77.5</td>
</tr>
</tbody>
</table>
<p>Word Embedding Transfer Learning</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\mathrm{CNN}_{w 2 v}$</th>
<th style="text-align: center;">$\mathbf{7 0 . 7}$</th>
<th style="text-align: center;">73.8</th>
<th style="text-align: center;">$\mathbf{8 1 . 5}$</th>
<th style="text-align: center;">$\mathbf{8 4 . 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{DAN}_{w 2 v}$</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">$\mathbf{7 5 . 1}$</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">80.6</td>
</tr>
</tbody>
</table>
<p>Sentence Embedding Transfer Learning</p>
<table>
<thead>
<tr>
<th style="text-align: center;">+ DNN/CNN with word-level transfer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$U_{T}+\mathrm{CNN}_{w 2 v}$</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.3</td>
</tr>
<tr>
<td style="text-align: center;">$U_{T}+\mathrm{DAN}_{w 2 v}$</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">86.0</td>
</tr>
<tr>
<td style="text-align: center;">$U_{D}+\mathrm{CNN}_{w 2 v}$</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">85.1</td>
</tr>
<tr>
<td style="text-align: center;">$U_{D}+\mathrm{DAN}_{w 2 v}$</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">82.2</td>
</tr>
</tbody>
</table>
<p>Sentence Embedding Transfer Learning</p>
<table>
<thead>
<tr>
<th style="text-align: center;">+ DNN/CNN without word-level transfer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$U_{T}+\mathrm{CNN}_{r n d}$</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">88.9</td>
</tr>
<tr>
<td style="text-align: center;">$U_{T}+\mathrm{DAN}_{r n d}$</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">86.2</td>
</tr>
<tr>
<td style="text-align: center;">$U_{D}+\mathrm{CNN}_{r n d}$</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">83.2</td>
</tr>
<tr>
<td style="text-align: center;">$U_{D}+\mathrm{DAN}_{r n d}$</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">86.2</td>
</tr>
</tbody>
</table>
<p>Baselines with No Transfer Learning</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\mathrm{CNN}_{r n d}$</th>
<th style="text-align: center;">$\mathbf{6 8 . 9}$</th>
<th style="text-align: center;">$\mathbf{7 4 . 6}$</th>
<th style="text-align: center;">$\mathbf{8 1 . 5}$</th>
<th style="text-align: center;">$\mathbf{8 5 . 0}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{DAN}_{r n d}$</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">82.0</td>
</tr>
</tbody>
</table>
<p>Prior Work</p>
<table>
<thead>
<tr>
<th style="text-align: left;">InferSent</th>
<th style="text-align: left;">-</th>
<th style="text-align: left;">-</th>
<th style="text-align: left;">-</th>
<th style="text-align: left;">84.6</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: SST performance varying the amount of training data. Model types are the same as Table 2. Using 1 k examples, $U_{T}$ transfer learning rivals models trained on the full training set, 67.3 k .</p>
<p>DAN model is $O(n)$. As seen in Figure 1 (ab), for short sentences, the transformer encoding model is only moderately slower than the much simpler DAN model. However, compute time for transformer increases noticeably with sentence length. In contrast, the compute time for the DAN model stays nearly constant across different lengths. When running on GPU, even for large batches and longer sentence lengths, the transformer model still achieves performance that can be used within an interactive systems.</p>
<p>Memory Usage The transformer model space complexity also scales quadratically, $O\left(n^{2}\right)$, in sentence length, while the DAN is linear, $O(n)$. Similar to compute usage, memory for the transformer model increases quickly with sentence length, while the memory for the DAN model remains nearly constant. For the DAN model, memory is dominated by the parameters used to store the model unigram and bigram embeddings. Since the transformer model only stores unigrams, for</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Resource usage for the Universal Sentence Encoder DAN (USE-D) and Transformer (USE-T) models for different batch sizes and sentence lengths.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Target words</th>
<th style="text-align: center;">Attrib. words</th>
<th style="text-align: center;">Ref</th>
<th style="text-align: center;">GloVe</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">U. Enc. DAN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">U. Enc. Trans.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">d</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">d</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">d</td>
<td style="text-align: center;">p</td>
</tr>
<tr>
<td style="text-align: center;">Eur.- vs. Afr.-American names</td>
<td style="text-align: center;">Pleasant vs. Unpleasant</td>
<td style="text-align: center;">$a$</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">$10^{-8}$</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;">Eur.- vs. Afr.-American names</td>
<td style="text-align: center;">Pleasant vs.Unpleasant from (a)</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">$10^{-4}$</td>
<td style="text-align: center;">-0.37</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;">Eur.- vs. Afr.-American names</td>
<td style="text-align: center;">Pleasant vs. Unpleasant from (c)</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">$10^{-3}$</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">Male vs. female names</td>
<td style="text-align: center;">Career vs. family</td>
<td style="text-align: center;">c</td>
<td style="text-align: center;">1.81</td>
<td style="text-align: center;">$10^{-3}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: center;">Math vs. arts</td>
<td style="text-align: center;">Male vs. female terms</td>
<td style="text-align: center;">c</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;">Science vs. arts</td>
<td style="text-align: center;">Male vs. female terms</td>
<td style="text-align: center;">d</td>
<td style="text-align: center;">1.24</td>
<td style="text-align: center;">$10^{-2}$</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">-0.21</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">Mental vs. physical disease</td>
<td style="text-align: center;">Temporary vs. permanent</td>
<td style="text-align: center;">e</td>
<td style="text-align: center;">1.38</td>
<td style="text-align: center;">$10^{-2}$</td>
<td style="text-align: center;">1.60</td>
<td style="text-align: center;">$10^{-2}$</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;">Young vs old peoples names</td>
<td style="text-align: center;">Pleasant vs unpleasant</td>
<td style="text-align: center;">c</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">$10^{-2}$</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;">Flowers vs. Insects</td>
<td style="text-align: center;">Pleasant vs. Unpleasant</td>
<td style="text-align: center;">$a$</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">$10^{-7}$</td>
<td style="text-align: center;">1.38</td>
<td style="text-align: center;">$10^{-6}$</td>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;">$10^{-7}$</td>
</tr>
<tr>
<td style="text-align: center;">Instruments vs. Weapons</td>
<td style="text-align: center;">Pleasant vs Unpleasant</td>
<td style="text-align: center;">$a$</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">$10^{-7}$</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">$10^{-7}$</td>
<td style="text-align: center;">1.65</td>
<td style="text-align: center;">$10^{-7}$</td>
</tr>
</tbody>
</table>
<p>Table 5: WEAT for GloVe vs. our DAN and transformer encoding models. Effect size is reported as Cohen's d over the mean cosine similarity scores across grouped attribute words. Statistical significance uses one-tailed p-scores. The Ref column indicates the source of the IAT word lists: (a) Greenwald et al. (1998) (b) Bertrand and Mullainathan (2004) (c) Nosek et al. (2002a) (d) Nosek et al. (2002b) (e) Monteith and Pettit (2011).
very short sequences transformer requires almost half as much memory as the DAN model.</p>
<h2>9 Conclusion</h2>
<p>Our encoding models provide sentence-level embeddings that demonstrate strong transfer performance on a number of NLP tasks. The encoding models make different trade-offs regarding accuracy and model complexity that should be considered when choosing the best one for a particular application. Overall, our sentence-level embeddings tend to surpass the performance of transfer using word-level embeddings alone. Models that make use of sentence- and word-level transfer often achieve the best performance. Sentencelevel transfer using our models can be exceptionally helpful when limited training data is available. The pre-trained encoding models are publicly available for research and use in industry
applications that can benefit from a better understanding of natural language.</p>
<h2>Acknowledgments</h2>
<p>We thank our teammates from Descartes, Ai.h and other Google groups for their feedback and suggestions. Special thanks goes to Ben Packer and Yoni Halpern for implementing the WEAT assessments and discussions on model bias.</p>
<h2>References</h2>
<p>Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Tensorflow: A system for large-scale machine learning. In Proceedings of USENIX OSDI'16.</p>
<p>Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. CoRR, abs/1607.06450.</p>
<p>Marianne Bertrand and Sendhil Mullainathan. 2004. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market discrimination. The American Economic Review, 94(4).</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of EMNLP.</p>
<p>Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186.</p>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of SemEval-2017.</p>
<p>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364.</p>
<p>Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. 2017. Google vizier: A service for black-box optimization. In Proceedings of KDD '17.</p>
<p>Anthony G. Greenwald, Debbie E. McGhee, and Jordan L. K. Schwartz. 1998. Measuring individual differences in implicit cognition: the implicit association test. Journal of personality and social psychology, 74(6).</p>
<p>Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply. CoRR, abs/1705.00652.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):17351780 .</p>
<p>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD '04.</p>
<p>Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. 2015. Deep unordered composition rivals syntactic methods for text classification. In Proceedings of ACL/IJCNLP.</p>
<p>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of EMNLP.</p>
<p>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In In Proceedings of NIPS.</p>
<p>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of COLING '02.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS'13.</p>
<p>Lindsey L. Monteith and Jeremy W. Pettit. 2011. Implicit and explicit stigmatizing attitudes and stereotypes about depression. Journal of Social and Clinical Psychology, 30(5).</p>
<p>Brian A. Nosek, Mahzarin R. Banaji, and Anthony G. Greenwald. 2002a. Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics, 6(1).</p>
<p>Brian A. Nosek, Mahzarin R. Banaji, and Anthony G Greenwald. 2002b. Math = male, me = female, therefore math me. Journal of Personality and Social Psychology, 83(1).</p>
<p>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL'04), Main Volume.</p>
<p>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of $A C L^{\prime} 05$.</p>
<p>Yang Shao. 2017. Hcti at semeval-2017 task 1: Use convolutional neural network to evaluate semantic textual similarity. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 130-133.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NIPS.</p>
<p>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2):165-210.</p>
<p>Yinfei Yang, Steve Yuan, Daniel Cer, Sheng yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Learning semantic textual similarity from conversations. Proceedings of RepL4NLP workshop at ACL.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ The development of our models did not target reducing bias. Researchers and developers are strongly encouraged to independently verify whether biases in their overall model or model components impacts their use case. For resources on ML fairness visit https://developers.google.com/machinelearning/fairness-overview/.
${ }^{12}$ All benchmark values are averaged over 25 runs that follow 5 priming runs. CPU and mem. benchmarks are performed on a machine with an Intel(R) Xeon(R) Platinum P-8136 CPU @ 2.00 GHz CPU. GPU benchmarks use an Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20 GHz CPU and NVIDIA Tesla P100 GPU.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
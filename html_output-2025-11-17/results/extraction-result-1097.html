<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1097 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1097</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1097</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-3b608ce4c0d222a003d64935d3d492a62031d6dd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3b608ce4c0d222a003d64935d3d492a62031d6dd" target="_blank">Automatic Curriculum Learning For Deep RL: A Short Survey</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The ambition of this work is to present a compact and accessible introduction to the Automatic Curriculum Learning literature and to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.</p>
                <p><strong>Paper Abstract:</strong> Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1097.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1097.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Rubik's Hand (ADR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solving Rubik's Cube with a Robot Hand (OpenAI) / Automatic Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dexterous robotic manipulation policy trained in simulation with automatic domain randomization (an ACL mechanism sequencing environment variation) to enable transfer to a physical robot hand; uses RL with procedural variation of simulator parameters to improve robustness and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving rubik's cube with a robot hand</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Robotic dexterous-hand policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL policy controlling a multi-fingered robotic hand for dexterous manipulation (solving a Rubik's cube); trained in simulation with domain-randomized physics and visuals, leveraging ACL/ADR to adapt parameter distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (trained in simulation, transferred to physical robot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated dexterous Rubik's-cube manipulation (with domain randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-dimensional contact-rich manipulation environment simulated with many randomized parameters (e.g., friction, object mass, actuator delays, visual appearance); complexity arises from high DoF hand, contact dynamics and precise control; variation arises from parametric domain randomization across physics and perception parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Qualitative: dexterous multi-finger manipulation (high DoF), contact dynamics complexity, precision required to reorient cube faces; not given as a numeric metric in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization degree: number and range of randomized simulator parameters (physics, visuals); ADR sequences/expands parameter distributions over training (procedural/parametric variation).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (broad parametric variation via ADR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success rate / ability to solve Rubik's cube on the real robot (generalization to real world)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey reports ADR sequences domain randomizations to maintain intermediate difficulty: curriculum adjusts variation so the agent attains decent performance while still being challenged; high variation improves Sim2Real generalization but must be scheduled to avoid overwhelming learning (trade-off between variation and learnability).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic Domain Randomization (ACL sequencing of simulator parameter variation) + deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey states ADR was used to train a policy that transferred to the real robotic hand to solve the Rubik's cube; no numeric transfer metrics provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sequencing and automatic adaptation of simulator parameter variation (ADR) enables robust policies that generalize from simulation to a real robot; variation must be controlled (intermediate difficulty) to avoid hindering learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1097.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Domain Randomization (Mila)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Domain Randomization (Mehta et al., Mila)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACL approach that actively optimizes the distribution of simulated environment parameters (domain randomization) during training to improve generalization to target domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active domain randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Simulation-trained control policy (various robotic/control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL policies trained in parameterized simulators where the distribution of environment parameters is adaptively selected by an ACL mechanism (active domain randomization) to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / intended physical transfer</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Parametric simulated environments for Sim2Real</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulators parameterized over physics and observation variables; complexity from task dynamics and control precision; variation introduced by changing simulator parameters (domain randomization) actively during training.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task/control difficulty (qualitative), physics interaction complexity; no specific numeric metrics given in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Range and selection of randomized simulator parameters; active selection of which parameters/degree to randomize (PCG-like parametric variation).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (controlled via active selection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Generalization / robustness to target (real) domain; policy performance under distribution shifts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes active domain randomization sequences variation to obtain robustness while maintaining learnability — implicates a trade-off: too much variation early can hinder learning; controlled variation improves generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active domain randomization (curriculum over simulator parameter distributions) + deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Reported as used to improve Sim2Real generalization; survey does not report quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Actively adapting the distribution of simulation parameters during training (an ACL mechanism) improves transfer/generalization, but variation must be balanced to keep tasks of intermediate difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1097.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALP-GMM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALP-GMM: Absolute Learning Progress with Gaussian Mixture Models (Portelas et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACL algorithm that measures absolute learning progress in a continuous, procedurally parameterized task space using a Gaussian Mixture Model to identify regions of the task parameter space where learning progress is highest.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep RL agent (Bipedal-Walker environments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL control policy trained in a procedurally-parameterized Bipedal-Walker environment; ALP-GMM selects environment parameters (PCG) to maximize local absolute learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Procedurally-parameterized Bipedal-Walker environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous task parameter space generating varied terrain and dynamics for a bipedal walker; complexity stems from locomotion control and contact dynamics; variation comes from continuous parametric procedural generation of environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty as determined by procedural parameters (terrain difficulty, physical parameters); localized learning progress used as proxy for complexity rather than explicit numeric complexity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>variable (ranges from easy to hard depending on sampled parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Continuous procedural content generation (PCG) parameter space dimensionality and parameter ranges; number of distinct environment instances sampled over training.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (continuous PCG yields broad variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Learning progress (LP) and ultimate task performance on target distributions; not quantified numerically in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>ALP-GMM explicitly exploits the relationship by sampling environments where absolute learning progress is high (neither too easy nor too hard), indicating a trade-off: choose variation that yields intermediate/positive LP to drive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via ALP-GMM over procedurally parameterized environments (PCG) + deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Designed to discover niches of progress in continuous task spaces to improve robustness/generalization; survey does not detail quantitative generalization metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using absolute learning progress over a continuous environment parameter space enables focusing sampling on regions that drive learning; this balances complexity and variation by targeting instances with positive progress.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1097.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reverse Curriculum (RC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reverse Curriculum Generation for Reinforcement Learning (Florensa et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACL method that controls the initial-state distribution by starting training from states close to goal states and progressively expanding to harder initial states, to solve sparse-reward control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reverse curriculum generation for reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Robotic/control policy (sparse reward tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL policies for control tasks with sparse rewards; training begins from easy initial states near the goal and moves outward (reverse curriculum) to encounter harder initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / intended robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sparse-reward control environments (goal-reaching tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where success requires reaching specific goal states and reward is sparse; complexity measured by distance (in state space) between initial states and goal and complexity of required control.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Distance from initial state to goal (state-space distance), control difficulty; no numeric thresholds provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high for original task (far initial states), lower for proximate starts</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distribution of initial states (ρ0) over training; curriculum progressively increases variance/distance of initial-state distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varies during training from low (near-goal) to high (full task space)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate on target tasks / ability to solve originally hard sparse-reward tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Reverse curriculum exploits a relationship where reducing initial-state complexity (by starting near goals) and gradually increasing variation/difficulty yields better learning; intermediate difficulty sequencing supports progress.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning over initial-state distribution (reverse curriculum) + deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Controlling initial-state variation to present tasks of intermediate difficulty (start easy, expand) is effective for solving sparse-reward and otherwise unsolvable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1097.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoalGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Goal Generation for Reinforcement Learning Agents (GoalGAN, Florensa et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative-adversarial approach that produces goals of intermediate difficulty for a goal-conditioned agent; the generator is trained to propose goals where agent performance is neither too low nor too high.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic goal generation for reinforcement learning agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Goal-conditioned RL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A goal-conditioned deep RL policy trained with goals proposed by a GoalGAN generator that targets goals of intermediate difficulty to maximize learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Goal-conditioned tasks (navigation/manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where tasks vary by target goals; complexity tied to goal feasibility and distance, and variation comes from the generated distribution of goals.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Agent performance on candidate goals (used as proxy for difficulty/feasibility); goal distance/feasibility implicitly measures complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varied (GoalGAN targets intermediate difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distribution of generated goals (GAN output), coverage of goal space; no numeric counts provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (GAN produces diverse intermediate-difficulty goals)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Goal-reaching success rate / improvement in learning progress over goal space</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>GoalGAN explicitly targets the trade-off: it produces goals of intermediate difficulty (balancing complexity and variation) to maximize learning progress; too easy or too hard goals are avoided.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic goal generation (GAN) to produce curriculum + goal-conditioned deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generating goals of intermediate difficulty (balancing complexity and variation) improves learning progress across the goal space and helps agents acquire diverse competencies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1097.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSCL (Teacher-Student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teacher-Student Curriculum Learning (Matiisen et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum learning framework treating tasks as arms in a multi-armed bandit; tasks (e.g., discrete game levels) are selected based on estimated learning progress to build an automatic curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teacher-student curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep RL agents in discrete-level environments (e.g., Minecraft)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents trained with RL on a discrete set of environment instances (levels); a teacher algorithm chooses which level to present based on measured learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete-level game environments (e.g., Minecraft mazes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Sets of discrete levels with varying difficulty; complexity measured per-level (e.g., maze complexity) and variation is the number/diversity of levels available.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Per-level difficulty (qualitative; estimated performance derivative used as LP); no numeric metric provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varied across discrete levels (low to high)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number and diversity of discrete environment instances/levels (DS = discrete set)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high depending on level set</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Learning progress per level; overall performance on target levels after curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>TSCL uses learning progress over discrete levels to balance presenting tasks that are neither too easy nor too hard — a curriculum that manages complexity via selective variation accelerates learning.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning using multi-armed bandit over discrete task set (teacher-student) + RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Selecting discrete environment instances by estimated learning progress improves training efficiency relative to unguided sampling; managing complexity by targeted variation is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1097.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RgC (Reward-guided Curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-Guided Curriculum for Robust Reinforcement Learning (Mysore et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACL that chooses among discrete environment instances (game levels) guided by reward signals to build a curriculum improving robustness and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reward-guided curriculum for robust reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep RL agent (game domains, e.g., Sonic the Hedgehog levels)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL agent trained on discrete sets of game levels where a curriculum selects levels based on reward-related signals to shape training difficulty and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated/virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete game level suites (e.g., Sonic levels)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Discrete set of levels with varying structural complexity and challenge; variation is across levels in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Per-level difficulty (game-specific challenge); measured via reward/performance trajectories in survey discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varied (levels range low to high)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of discrete levels and their diversity (DS selection); procedural aspects not emphasized here.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (discrete variation across levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reward / robustness across levels</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>RgC biases level selection towards those that yield informative rewards for the agent, implicitly balancing complexity and variation to maintain learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Reward-guided curriculum over discrete environment instances + deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using reward signals to guide selection among discrete environment instances constructs curricula that improve robustness and learning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1097.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BaRC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Backwards Reachability Curriculum (Ivanovic et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum method for robotic reinforcement learning that uses backward reachability analysis to generate sequences of initial states increasing difficulty from reachable to target regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Barc: Backward reachability curriculum for robotic reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Robotic control policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL policy for robotics tasks where the curriculum constructs initial states via backward reachability computations to ensure intermediate difficulty progression.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / intended robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Robotic control environments with sparse rewards (reachability tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Tasks where reachability from states to goals can be computed; complexity relates to reachability/controllability and contact dynamics; variation controlled by initial-state sampling guided by reachability sets.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Reachability distance/sets; difficulty determined by whether initial states are backward-reachable to goal under dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high for full task; intermediate for reachability-seeded starts</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Distribution of initial states sampled across reachability levels; extent of state-space covered by reachability sets.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>controlled/gradually increasing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Ability to learn to solve sparse-reward tasks; success rate on target task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>BaRC constructs curricula that reduce initial complexity and then expand variation using reachability analysis; trade-off: presenting reachable (less complex) instances first improves learning for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum over initial-state distribution using backward reachability + deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using reachability to seed curricula produces intermediate-difficulty training instances that ease solving sparse-reward robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1097.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hindsight Experience Replay (Andrychowicz et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A replay-based ACL technique for multi-goal RL that modifies stored transitions by relabeling goals with achieved outcomes, increasing the density of rewarded transitions and improving sample efficiency on goal-conditioned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hindsight experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Multi-goal robotic arm policies (and other goal-conditioned agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Goal-conditioned deep RL policies trained with Hindsight Experience Replay which substitutes achieved outcomes as goals in replay to create additional positive training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-goal manipulation/navigation environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Goal-conditioned tasks with potentially sparse rewards; complexity is tied to goal difficulty and state-action requirements; variation arises from the set of goals and goal substitutions in replay.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Goal difficulty/feasibility; sparsity of reward signal; no numeric metric provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varied (many goals can be hard due to sparsity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number/diversity of goals and substituted achieved outcomes used in replay; goal-space coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (via goal relabeling increases effective variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Sample efficiency (speed of learning), success rate on specified goals</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>HER shifts training distribution toward goals that were actually achieved, effectively reducing apparent task complexity and increasing the density of solvable (less complex) training instances; this trade-off improves learning on sparse-reward tasks but can bias training toward easier/achieved outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Replay-based ACL via goal relabeling (HER) + goal-conditioned deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves sample efficiency relative to naive replay in sparse multi-goal tasks (no numeric values in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Relabeling goals in replay increases the proportion of rewarded transitions, reducing effective task complexity during training and improving sample efficiency on multi-goal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1097.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIAYN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity Is All You Need (Eysenbach et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised skill discovery method that maximizes behavioral diversity by learning a set of skills without external reward, encouraging exploration and open-ended acquisition of diverse behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diversity is all you need: Learning skills without a reward function</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Skill-discovering policies (unsupervised RL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policies learned via intrinsic objectives (mutual information between skills and states) to produce diverse behaviors in an environment; used for open-ended exploration and building repertoires.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Open-ended exploration environments (various simulated domains)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where the goal is to discover diverse achievable behaviors; complexity arises from environment dynamics and richness of state space; variation measured by diversity of states/behaviors achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State-space richness and dynamics; no single numeric metric provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varied; often medium-high depending on domain</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Behavioral diversity / novelty measures, density in state/skill space; number of distinct skills discovered.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (explicit objective to maximize diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Diversity/coverage of behaviors; robustness of discovered skills and usefulness for downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>DIAYN maximizes variation (diversity) independent of external task complexity, useful when goal space is unknown; emphasizes that promoting variation can drive discovery of behavior repertoires even in complex environments.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Unsupervised skill discovery maximizing diversity (intrinsic objective) + RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Optimizing for diversity yields robust repertoires of behaviors and can organize exploration in complex/unknown goal spaces; variation (diversity) is the primary driver rather than task-specific complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1097.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-play / ARM (AlphaGo/AlphaStar/Hide&Seek)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-play and adversarial reward maximization approaches (e.g., AlphaGo Zero, AlphaStar, Hide&Seek)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Self-play ACL methods train agents against current/past versions or leagues of opponents to drive adversarial reward maximization, maintaining and exploiting opponent diversity to improve performance and robustness in complex multi-agent games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering the game of go without human knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Game-playing agents (e.g., AlphaGo Zero, AlphaStar, Hide&Seek agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL / self-play trained policies where opponents are current or past copies or a maintained league; training optimizes performance against an evolving set of adversaries (adversarial reward maximization).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated/virtual agents (game domains)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Complex multi-agent game environments (Go, StarCraft II, emergent tool-use environments like Hide&Seek)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Highly structured strategic games with large state/action spaces and varied opponent behaviors; complexity from combinatorial game trees, long horizons; variation from opponent pool diversity and asymmetric roles.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Game complexity (state/action branching, horizon length), opponent behavior diversity; no numeric metrics provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>very high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Size and diversity of opponent pool/league (current and past policies), number of agent types/roles; procedural environment variation less emphasized here.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (diverse opponent pools/leagues)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Winning rate against opponents, league performance, robustness to varied opponents</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes maintaining a diverse opponent pool (variation) is crucial to avoid catastrophic forgetting and to improve robustness; adversarial curricula implicitly exploit a trade-off where opponent variation must be managed to provide appropriately challenging (intermediate-difficulty) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Self-play / league training (adversarial reward maximization) + deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Self-play approaches showed strong performance and robustness in complex games when using opponent leagues/diverse sampling; survey reports success but no numeric figures.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Opponent diversity (variation) maintained via leagues or sampling past policies improves robustness and performance in very complex multi-agent games; curricula over opponents help manage task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1097.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1097.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Experience Replay (Schaul et al., 2015b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A replay buffer sampling method that prioritizes transitions with high TD-error (surprise) during training to improve learning efficiency; an ACL applied at data-exploitation level via transition selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prioritized experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep RL agents using replay buffers (Atari / general domains)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Off-policy deep RL agents that bias sampling of stored transitions by TD-error magnitude to focus updates on surprising/learning-relevant experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated/virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Various RL environments using experience replay (e.g., Atari benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments vary in dynamics and complexity; PER treats replayed transitions to manage variation in training data importance rather than environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Not directly an environment complexity metric; uses TD-error as proxy for informative/complex transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>N/A (applies across complexities)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation in stored transitions and their importance distribution; no explicit environment-instance variation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Learning speed / sample efficiency (improvements in convergence and final performance reported in original work; survey references benefit but gives no numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>PER focuses on transition-level 'variation' (surprise) rather than environmental param variation; it implies a trade-off where focusing on high-error transitions accelerates learning but may introduce bias if not corrected.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Replay-based ACL via prioritized sampling (TD-error) + deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported to boost sample efficiency in practice (no numeric values provided in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prioritizing surprising transitions in replay improves learning efficiency; this is a data-exploitation ACL complementary to curricula over environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning For Deep RL: A Short Survey', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Active domain randomization <em>(Rating: 2)</em></li>
                <li>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments <em>(Rating: 2)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Teacher-student curriculum learning <em>(Rating: 2)</em></li>
                <li>Reward-guided curriculum for robust reinforcement learning <em>(Rating: 2)</em></li>
                <li>Barc: Backward reachability curriculum for robotic reinforcement learning <em>(Rating: 2)</em></li>
                <li>Hindsight experience replay <em>(Rating: 2)</em></li>
                <li>Diversity is all you need: Learning skills without a reward function <em>(Rating: 2)</em></li>
                <li>Mastering the game of go without human knowledge <em>(Rating: 2)</em></li>
                <li>Prioritized experience replay <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1097",
    "paper_id": "paper-3b608ce4c0d222a003d64935d3d492a62031d6dd",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "OpenAI Rubik's Hand (ADR)",
            "name_full": "Solving Rubik's Cube with a Robot Hand (OpenAI) / Automatic Domain Randomization",
            "brief_description": "A dexterous robotic manipulation policy trained in simulation with automatic domain randomization (an ACL mechanism sequencing environment variation) to enable transfer to a physical robot hand; uses RL with procedural variation of simulator parameters to improve robustness and generalization.",
            "citation_title": "Solving rubik's cube with a robot hand",
            "mention_or_use": "mention",
            "agent_name": "Robotic dexterous-hand policy",
            "agent_description": "A deep RL policy controlling a multi-fingered robotic hand for dexterous manipulation (solving a Rubik's cube); trained in simulation with domain-randomized physics and visuals, leveraging ACL/ADR to adapt parameter distributions.",
            "agent_type": "physical robot (trained in simulation, transferred to physical robot)",
            "environment_name": "Simulated dexterous Rubik's-cube manipulation (with domain randomization)",
            "environment_description": "High-dimensional contact-rich manipulation environment simulated with many randomized parameters (e.g., friction, object mass, actuator delays, visual appearance); complexity arises from high DoF hand, contact dynamics and precise control; variation arises from parametric domain randomization across physics and perception parameters.",
            "complexity_measure": "Qualitative: dexterous multi-finger manipulation (high DoF), contact dynamics complexity, precision required to reorient cube faces; not given as a numeric metric in survey.",
            "complexity_level": "high",
            "variation_measure": "Domain randomization degree: number and range of randomized simulator parameters (physics, visuals); ADR sequences/expands parameter distributions over training (procedural/parametric variation).",
            "variation_level": "high (broad parametric variation via ADR)",
            "performance_metric": "Task success rate / ability to solve Rubik's cube on the real robot (generalization to real world)",
            "performance_value": null,
            "complexity_variation_relationship": "Survey reports ADR sequences domain randomizations to maintain intermediate difficulty: curriculum adjusts variation so the agent attains decent performance while still being challenged; high variation improves Sim2Real generalization but must be scheduled to avoid overwhelming learning (trade-off between variation and learnability).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic Domain Randomization (ACL sequencing of simulator parameter variation) + deep reinforcement learning",
            "generalization_tested": true,
            "generalization_results": "Survey states ADR was used to train a policy that transferred to the real robotic hand to solve the Rubik's cube; no numeric transfer metrics provided in this survey.",
            "sample_efficiency": null,
            "key_findings": "Sequencing and automatic adaptation of simulator parameter variation (ADR) enables robust policies that generalize from simulation to a real robot; variation must be controlled (intermediate difficulty) to avoid hindering learning.",
            "uuid": "e1097.0",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Active Domain Randomization (Mila)",
            "name_full": "Active Domain Randomization (Mehta et al., Mila)",
            "brief_description": "An ACL approach that actively optimizes the distribution of simulated environment parameters (domain randomization) during training to improve generalization to target domains.",
            "citation_title": "Active domain randomization",
            "mention_or_use": "mention",
            "agent_name": "Simulation-trained control policy (various robotic/control tasks)",
            "agent_description": "Deep RL policies trained in parameterized simulators where the distribution of environment parameters is adaptively selected by an ACL mechanism (active domain randomization) to improve robustness.",
            "agent_type": "simulated agent / intended physical transfer",
            "environment_name": "Parametric simulated environments for Sim2Real",
            "environment_description": "Simulators parameterized over physics and observation variables; complexity from task dynamics and control precision; variation introduced by changing simulator parameters (domain randomization) actively during training.",
            "complexity_measure": "Task/control difficulty (qualitative), physics interaction complexity; no specific numeric metrics given in survey.",
            "complexity_level": "medium-high",
            "variation_measure": "Range and selection of randomized simulator parameters; active selection of which parameters/degree to randomize (PCG-like parametric variation).",
            "variation_level": "medium-high (controlled via active selection)",
            "performance_metric": "Generalization / robustness to target (real) domain; policy performance under distribution shifts",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes active domain randomization sequences variation to obtain robustness while maintaining learnability — implicates a trade-off: too much variation early can hinder learning; controlled variation improves generalization.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active domain randomization (curriculum over simulator parameter distributions) + deep RL",
            "generalization_tested": true,
            "generalization_results": "Reported as used to improve Sim2Real generalization; survey does not report quantitative results.",
            "sample_efficiency": null,
            "key_findings": "Actively adapting the distribution of simulation parameters during training (an ACL mechanism) improves transfer/generalization, but variation must be balanced to keep tasks of intermediate difficulty.",
            "uuid": "e1097.1",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "ALP-GMM",
            "name_full": "ALP-GMM: Absolute Learning Progress with Gaussian Mixture Models (Portelas et al., 2019)",
            "brief_description": "An ACL algorithm that measures absolute learning progress in a continuous, procedurally parameterized task space using a Gaussian Mixture Model to identify regions of the task parameter space where learning progress is highest.",
            "citation_title": "Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments",
            "mention_or_use": "mention",
            "agent_name": "Deep RL agent (Bipedal-Walker environments)",
            "agent_description": "A deep RL control policy trained in a procedurally-parameterized Bipedal-Walker environment; ALP-GMM selects environment parameters (PCG) to maximize local absolute learning progress.",
            "agent_type": "simulated agent",
            "environment_name": "Procedurally-parameterized Bipedal-Walker environments",
            "environment_description": "Continuous task parameter space generating varied terrain and dynamics for a bipedal walker; complexity stems from locomotion control and contact dynamics; variation comes from continuous parametric procedural generation of environment instances.",
            "complexity_measure": "Task difficulty as determined by procedural parameters (terrain difficulty, physical parameters); localized learning progress used as proxy for complexity rather than explicit numeric complexity metric.",
            "complexity_level": "variable (ranges from easy to hard depending on sampled parameters)",
            "variation_measure": "Continuous procedural content generation (PCG) parameter space dimensionality and parameter ranges; number of distinct environment instances sampled over training.",
            "variation_level": "high (continuous PCG yields broad variation)",
            "performance_metric": "Learning progress (LP) and ultimate task performance on target distributions; not quantified numerically in the survey.",
            "performance_value": null,
            "complexity_variation_relationship": "ALP-GMM explicitly exploits the relationship by sampling environments where absolute learning progress is high (neither too easy nor too hard), indicating a trade-off: choose variation that yields intermediate/positive LP to drive learning.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via ALP-GMM over procedurally parameterized environments (PCG) + deep RL",
            "generalization_tested": true,
            "generalization_results": "Designed to discover niches of progress in continuous task spaces to improve robustness/generalization; survey does not detail quantitative generalization metrics.",
            "sample_efficiency": null,
            "key_findings": "Using absolute learning progress over a continuous environment parameter space enables focusing sampling on regions that drive learning; this balances complexity and variation by targeting instances with positive progress.",
            "uuid": "e1097.2",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Reverse Curriculum (RC)",
            "name_full": "Reverse Curriculum Generation for Reinforcement Learning (Florensa et al., 2017)",
            "brief_description": "An ACL method that controls the initial-state distribution by starting training from states close to goal states and progressively expanding to harder initial states, to solve sparse-reward control tasks.",
            "citation_title": "Reverse curriculum generation for reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Robotic/control policy (sparse reward tasks)",
            "agent_description": "Deep RL policies for control tasks with sparse rewards; training begins from easy initial states near the goal and moves outward (reverse curriculum) to encounter harder initial conditions.",
            "agent_type": "simulated agent / intended robotic control",
            "environment_name": "Sparse-reward control environments (goal-reaching tasks)",
            "environment_description": "Environments where success requires reaching specific goal states and reward is sparse; complexity measured by distance (in state space) between initial states and goal and complexity of required control.",
            "complexity_measure": "Distance from initial state to goal (state-space distance), control difficulty; no numeric thresholds provided in survey.",
            "complexity_level": "high for original task (far initial states), lower for proximate starts",
            "variation_measure": "Distribution of initial states (ρ0) over training; curriculum progressively increases variance/distance of initial-state distribution.",
            "variation_level": "varies during training from low (near-goal) to high (full task space)",
            "performance_metric": "Success rate on target tasks / ability to solve originally hard sparse-reward tasks",
            "performance_value": null,
            "complexity_variation_relationship": "Reverse curriculum exploits a relationship where reducing initial-state complexity (by starting near goals) and gradually increasing variation/difficulty yields better learning; intermediate difficulty sequencing supports progress.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning over initial-state distribution (reverse curriculum) + deep RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Controlling initial-state variation to present tasks of intermediate difficulty (start easy, expand) is effective for solving sparse-reward and otherwise unsolvable tasks.",
            "uuid": "e1097.3",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "GoalGAN",
            "name_full": "Automatic Goal Generation for Reinforcement Learning Agents (GoalGAN, Florensa et al., 2018)",
            "brief_description": "A generative-adversarial approach that produces goals of intermediate difficulty for a goal-conditioned agent; the generator is trained to propose goals where agent performance is neither too low nor too high.",
            "citation_title": "Automatic goal generation for reinforcement learning agents",
            "mention_or_use": "mention",
            "agent_name": "Goal-conditioned RL agent",
            "agent_description": "A goal-conditioned deep RL policy trained with goals proposed by a GoalGAN generator that targets goals of intermediate difficulty to maximize learning progress.",
            "agent_type": "simulated agent",
            "environment_name": "Goal-conditioned tasks (navigation/manipulation)",
            "environment_description": "Environments where tasks vary by target goals; complexity tied to goal feasibility and distance, and variation comes from the generated distribution of goals.",
            "complexity_measure": "Agent performance on candidate goals (used as proxy for difficulty/feasibility); goal distance/feasibility implicitly measures complexity.",
            "complexity_level": "varied (GoalGAN targets intermediate difficulty)",
            "variation_measure": "Distribution of generated goals (GAN output), coverage of goal space; no numeric counts provided in survey.",
            "variation_level": "medium (GAN produces diverse intermediate-difficulty goals)",
            "performance_metric": "Goal-reaching success rate / improvement in learning progress over goal space",
            "performance_value": null,
            "complexity_variation_relationship": "GoalGAN explicitly targets the trade-off: it produces goals of intermediate difficulty (balancing complexity and variation) to maximize learning progress; too easy or too hard goals are avoided.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic goal generation (GAN) to produce curriculum + goal-conditioned deep RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Generating goals of intermediate difficulty (balancing complexity and variation) improves learning progress across the goal space and helps agents acquire diverse competencies.",
            "uuid": "e1097.4",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "TSCL (Teacher-Student)",
            "name_full": "Teacher-Student Curriculum Learning (Matiisen et al., 2017)",
            "brief_description": "A curriculum learning framework treating tasks as arms in a multi-armed bandit; tasks (e.g., discrete game levels) are selected based on estimated learning progress to build an automatic curriculum.",
            "citation_title": "Teacher-student curriculum learning",
            "mention_or_use": "mention",
            "agent_name": "Deep RL agents in discrete-level environments (e.g., Minecraft)",
            "agent_description": "Agents trained with RL on a discrete set of environment instances (levels); a teacher algorithm chooses which level to present based on measured learning progress.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Discrete-level game environments (e.g., Minecraft mazes)",
            "environment_description": "Sets of discrete levels with varying difficulty; complexity measured per-level (e.g., maze complexity) and variation is the number/diversity of levels available.",
            "complexity_measure": "Per-level difficulty (qualitative; estimated performance derivative used as LP); no numeric metric provided here.",
            "complexity_level": "varied across discrete levels (low to high)",
            "variation_measure": "Number and diversity of discrete environment instances/levels (DS = discrete set)",
            "variation_level": "medium-high depending on level set",
            "performance_metric": "Learning progress per level; overall performance on target levels after curriculum",
            "performance_value": null,
            "complexity_variation_relationship": "TSCL uses learning progress over discrete levels to balance presenting tasks that are neither too easy nor too hard — a curriculum that manages complexity via selective variation accelerates learning.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning using multi-armed bandit over discrete task set (teacher-student) + RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Selecting discrete environment instances by estimated learning progress improves training efficiency relative to unguided sampling; managing complexity by targeted variation is beneficial.",
            "uuid": "e1097.5",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "RgC (Reward-guided Curriculum)",
            "name_full": "Reward-Guided Curriculum for Robust Reinforcement Learning (Mysore et al., 2018)",
            "brief_description": "An ACL that chooses among discrete environment instances (game levels) guided by reward signals to build a curriculum improving robustness and performance.",
            "citation_title": "Reward-guided curriculum for robust reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Deep RL agent (game domains, e.g., Sonic the Hedgehog levels)",
            "agent_description": "An RL agent trained on discrete sets of game levels where a curriculum selects levels based on reward-related signals to shape training difficulty and diversity.",
            "agent_type": "simulated/virtual agent",
            "environment_name": "Discrete game level suites (e.g., Sonic levels)",
            "environment_description": "Discrete set of levels with varying structural complexity and challenge; variation is across levels in the dataset.",
            "complexity_measure": "Per-level difficulty (game-specific challenge); measured via reward/performance trajectories in survey discussion.",
            "complexity_level": "varied (levels range low to high)",
            "variation_measure": "Number of discrete levels and their diversity (DS selection); procedural aspects not emphasized here.",
            "variation_level": "medium (discrete variation across levels)",
            "performance_metric": "Reward / robustness across levels",
            "performance_value": null,
            "complexity_variation_relationship": "RgC biases level selection towards those that yield informative rewards for the agent, implicitly balancing complexity and variation to maintain learning progress.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Reward-guided curriculum over discrete environment instances + deep RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Using reward signals to guide selection among discrete environment instances constructs curricula that improve robustness and learning efficiency.",
            "uuid": "e1097.6",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "BaRC",
            "name_full": "Backwards Reachability Curriculum (Ivanovic et al., 2018)",
            "brief_description": "A curriculum method for robotic reinforcement learning that uses backward reachability analysis to generate sequences of initial states increasing difficulty from reachable to target regions.",
            "citation_title": "Barc: Backward reachability curriculum for robotic reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Robotic control policy",
            "agent_description": "A deep RL policy for robotics tasks where the curriculum constructs initial states via backward reachability computations to ensure intermediate difficulty progression.",
            "agent_type": "simulated agent / intended robotic control",
            "environment_name": "Robotic control environments with sparse rewards (reachability tasks)",
            "environment_description": "Tasks where reachability from states to goals can be computed; complexity relates to reachability/controllability and contact dynamics; variation controlled by initial-state sampling guided by reachability sets.",
            "complexity_measure": "Reachability distance/sets; difficulty determined by whether initial states are backward-reachable to goal under dynamics.",
            "complexity_level": "high for full task; intermediate for reachability-seeded starts",
            "variation_measure": "Distribution of initial states sampled across reachability levels; extent of state-space covered by reachability sets.",
            "variation_level": "controlled/gradually increasing",
            "performance_metric": "Ability to learn to solve sparse-reward tasks; success rate on target task",
            "performance_value": null,
            "complexity_variation_relationship": "BaRC constructs curricula that reduce initial complexity and then expand variation using reachability analysis; trade-off: presenting reachable (less complex) instances first improves learning for complex tasks.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum over initial-state distribution using backward reachability + deep RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Using reachability to seed curricula produces intermediate-difficulty training instances that ease solving sparse-reward robotic tasks.",
            "uuid": "e1097.7",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "HER",
            "name_full": "Hindsight Experience Replay (Andrychowicz et al., 2017)",
            "brief_description": "A replay-based ACL technique for multi-goal RL that modifies stored transitions by relabeling goals with achieved outcomes, increasing the density of rewarded transitions and improving sample efficiency on goal-conditioned tasks.",
            "citation_title": "Hindsight experience replay",
            "mention_or_use": "mention",
            "agent_name": "Multi-goal robotic arm policies (and other goal-conditioned agents)",
            "agent_description": "Goal-conditioned deep RL policies trained with Hindsight Experience Replay which substitutes achieved outcomes as goals in replay to create additional positive training examples.",
            "agent_type": "simulated agent / robotic manipulation",
            "environment_name": "Multi-goal manipulation/navigation environments",
            "environment_description": "Goal-conditioned tasks with potentially sparse rewards; complexity is tied to goal difficulty and state-action requirements; variation arises from the set of goals and goal substitutions in replay.",
            "complexity_measure": "Goal difficulty/feasibility; sparsity of reward signal; no numeric metric provided in survey.",
            "complexity_level": "varied (many goals can be hard due to sparsity)",
            "variation_measure": "Number/diversity of goals and substituted achieved outcomes used in replay; goal-space coverage.",
            "variation_level": "medium-high (via goal relabeling increases effective variation)",
            "performance_metric": "Sample efficiency (speed of learning), success rate on specified goals",
            "performance_value": null,
            "complexity_variation_relationship": "HER shifts training distribution toward goals that were actually achieved, effectively reducing apparent task complexity and increasing the density of solvable (less complex) training instances; this trade-off improves learning on sparse-reward tasks but can bias training toward easier/achieved outcomes.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Replay-based ACL via goal relabeling (HER) + goal-conditioned deep RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Improves sample efficiency relative to naive replay in sparse multi-goal tasks (no numeric values in survey).",
            "key_findings": "Relabeling goals in replay increases the proportion of rewarded transitions, reducing effective task complexity during training and improving sample efficiency on multi-goal tasks.",
            "uuid": "e1097.8",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "DIAYN",
            "name_full": "Diversity Is All You Need (Eysenbach et al., 2018)",
            "brief_description": "An unsupervised skill discovery method that maximizes behavioral diversity by learning a set of skills without external reward, encouraging exploration and open-ended acquisition of diverse behaviors.",
            "citation_title": "Diversity is all you need: Learning skills without a reward function",
            "mention_or_use": "mention",
            "agent_name": "Skill-discovering policies (unsupervised RL)",
            "agent_description": "Policies learned via intrinsic objectives (mutual information between skills and states) to produce diverse behaviors in an environment; used for open-ended exploration and building repertoires.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Open-ended exploration environments (various simulated domains)",
            "environment_description": "Environments where the goal is to discover diverse achievable behaviors; complexity arises from environment dynamics and richness of state space; variation measured by diversity of states/behaviors achieved.",
            "complexity_measure": "State-space richness and dynamics; no single numeric metric provided in survey.",
            "complexity_level": "varied; often medium-high depending on domain",
            "variation_measure": "Behavioral diversity / novelty measures, density in state/skill space; number of distinct skills discovered.",
            "variation_level": "high (explicit objective to maximize diversity)",
            "performance_metric": "Diversity/coverage of behaviors; robustness of discovered skills and usefulness for downstream tasks",
            "performance_value": null,
            "complexity_variation_relationship": "DIAYN maximizes variation (diversity) independent of external task complexity, useful when goal space is unknown; emphasizes that promoting variation can drive discovery of behavior repertoires even in complex environments.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Unsupervised skill discovery maximizing diversity (intrinsic objective) + RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Optimizing for diversity yields robust repertoires of behaviors and can organize exploration in complex/unknown goal spaces; variation (diversity) is the primary driver rather than task-specific complexity.",
            "uuid": "e1097.9",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Self-play / ARM (AlphaGo/AlphaStar/Hide&Seek)",
            "name_full": "Self-play and adversarial reward maximization approaches (e.g., AlphaGo Zero, AlphaStar, Hide&Seek)",
            "brief_description": "Self-play ACL methods train agents against current/past versions or leagues of opponents to drive adversarial reward maximization, maintaining and exploiting opponent diversity to improve performance and robustness in complex multi-agent games.",
            "citation_title": "Mastering the game of go without human knowledge",
            "mention_or_use": "mention",
            "agent_name": "Game-playing agents (e.g., AlphaGo Zero, AlphaStar, Hide&Seek agents)",
            "agent_description": "Deep RL / self-play trained policies where opponents are current or past copies or a maintained league; training optimizes performance against an evolving set of adversaries (adversarial reward maximization).",
            "agent_type": "simulated/virtual agents (game domains)",
            "environment_name": "Complex multi-agent game environments (Go, StarCraft II, emergent tool-use environments like Hide&Seek)",
            "environment_description": "Highly structured strategic games with large state/action spaces and varied opponent behaviors; complexity from combinatorial game trees, long horizons; variation from opponent pool diversity and asymmetric roles.",
            "complexity_measure": "Game complexity (state/action branching, horizon length), opponent behavior diversity; no numeric metrics provided in survey.",
            "complexity_level": "very high",
            "variation_measure": "Size and diversity of opponent pool/league (current and past policies), number of agent types/roles; procedural environment variation less emphasized here.",
            "variation_level": "high (diverse opponent pools/leagues)",
            "performance_metric": "Winning rate against opponents, league performance, robustness to varied opponents",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes maintaining a diverse opponent pool (variation) is crucial to avoid catastrophic forgetting and to improve robustness; adversarial curricula implicitly exploit a trade-off where opponent variation must be managed to provide appropriately challenging (intermediate-difficulty) tasks.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Self-play / league training (adversarial reward maximization) + deep RL",
            "generalization_tested": true,
            "generalization_results": "Self-play approaches showed strong performance and robustness in complex games when using opponent leagues/diverse sampling; survey reports success but no numeric figures.",
            "sample_efficiency": null,
            "key_findings": "Opponent diversity (variation) maintained via leagues or sampling past policies improves robustness and performance in very complex multi-agent games; curricula over opponents help manage task difficulty.",
            "uuid": "e1097.10",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "PER",
            "name_full": "Prioritized Experience Replay (Schaul et al., 2015b)",
            "brief_description": "A replay buffer sampling method that prioritizes transitions with high TD-error (surprise) during training to improve learning efficiency; an ACL applied at data-exploitation level via transition selection.",
            "citation_title": "Prioritized experience replay",
            "mention_or_use": "mention",
            "agent_name": "Deep RL agents using replay buffers (Atari / general domains)",
            "agent_description": "Off-policy deep RL agents that bias sampling of stored transitions by TD-error magnitude to focus updates on surprising/learning-relevant experiences.",
            "agent_type": "simulated/virtual agent",
            "environment_name": "Various RL environments using experience replay (e.g., Atari benchmarks)",
            "environment_description": "Environments vary in dynamics and complexity; PER treats replayed transitions to manage variation in training data importance rather than environment instances.",
            "complexity_measure": "Not directly an environment complexity metric; uses TD-error as proxy for informative/complex transitions.",
            "complexity_level": "N/A (applies across complexities)",
            "variation_measure": "Variation in stored transitions and their importance distribution; no explicit environment-instance variation metric.",
            "variation_level": "N/A",
            "performance_metric": "Learning speed / sample efficiency (improvements in convergence and final performance reported in original work; survey references benefit but gives no numbers).",
            "performance_value": null,
            "complexity_variation_relationship": "PER focuses on transition-level 'variation' (surprise) rather than environmental param variation; it implies a trade-off where focusing on high-error transitions accelerates learning but may introduce bias if not corrected.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Replay-based ACL via prioritized sampling (TD-error) + deep RL",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Reported to boost sample efficiency in practice (no numeric values provided in this survey).",
            "key_findings": "Prioritizing surprising transitions in replay improves learning efficiency; this is a data-exploitation ACL complementary to curricula over environment instances.",
            "uuid": "e1097.11",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning For Deep RL: A Short Survey",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2
        },
        {
            "paper_title": "Active domain randomization",
            "rating": 2
        },
        {
            "paper_title": "Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments",
            "rating": 2
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2
        },
        {
            "paper_title": "Teacher-student curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Reward-guided curriculum for robust reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Barc: Backward reachability curriculum for robotic reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Hindsight experience replay",
            "rating": 2
        },
        {
            "paper_title": "Diversity is all you need: Learning skills without a reward function",
            "rating": 2
        },
        {
            "paper_title": "Mastering the game of go without human knowledge",
            "rating": 2
        },
        {
            "paper_title": "Prioritized experience replay",
            "rating": 2
        }
    ],
    "cost": 0.02019625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatic Curriculum Learning For Deep RL: A Short Survey</h1>
<p>Rémy Portelas ${ }^{1}$, Cédric Colas ${ }^{1}$, Lilian Weng ${ }^{2}$, Katja Hofmann ${ }^{3}$ and Pierre-Yves Oudeyer ${ }^{1}$<br>${ }^{1}$ Inria, France<br>${ }^{2}$ OpenAI, USA<br>${ }^{3}$ Microsoft Research, UK<br>remy.portelas@inria.fr</p>
<h4>Abstract</h4>
<p>Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.</p>
<h2>1 Introduction</h2>
<p>Human learning is organized into a curriculum of interdependent learning situations of various complexities. For sure, Homer learned to formulate words before he could compose the Iliad. This idea was first transferred to machine learning in Selfridge et al. [1985], where authors designed a learning scheme to train a cart pole controller: first training on long and light poles, then gradually moving towards shorter and heavier poles. A related concept was also developed by Schmidhuber [1991], who proposed to improve world model learning by organizing exploration through artificial curiosity. In the following years, curriculum learning was applied to organize the presentation of training examples or the growth in model capacity in various supervised learning settings [Elman, 1993; Krueger and Dayan, 2009; Bengio et al., 2009]. In parallel, the developmental robotics community proposed learning progress as a way to selforganize open-ended developmental trajectories of learning agents [Oudeyer et al., 2007]. Inspired by these earlier works, the Deep Reinforcement Learning (DRL) community devel-
oped a family of mechanisms called Automatic Curriculum Learning, which we propose to define as follows:</p>
<p>Automatic Curriculum Learning (ACL) for DRL is a family of mechanisms that automatically adapt the distribution of training data by learning to adjust the selection of learning situations to the capabilities of DRL agents.
Related fields. ACL shares many connections with other fields. For example, ACL can be used in the context of Transfer Learning where agents are trained on one distribution of tasks and tested on another [Taylor and Stone, 2009]. Continual Learning trains agents to be robust to unforeseen changes in the environment while ACL assumes agents to stay in control of learning scenarios [Lesort et al., 2019]. Policy Distillation techniques [Czarnecki et al., 2019] form a complementary toolbox to target multi-task RL settings, where knowledge can be transferred from one policy to another (e.g. from task-expert policies to a generalist policy).
Scope. This short survey proposes a typology of ACL mechanisms when combined with DRL algorithms and, as such, does not review population-based algorithms implementing ACL (e.g. Forestier et al. [2017], Wang et al. [2019]). As per our adopted definition, ACL refers to mechanisms explicitly optimizing the automatic organization of training data. Hence, they should not be confounded with emergent curricula, by-products of distinct mechanisms. For instance, the on-policy training of a DRL algorithm is not considered ACL, because the shift in the distribution of training data emerges as a by-product of policy learning. Given this is a short survey, we do not present the details of every particular mechanism. As the current ACL literature lacks theoretical foundations to ground proposed approaches in a formal framework, this survey focuses on empirical results.</p>
<h2>2 Automatic Curriculum Learning for DRL</h2>
<p>This section formalizes the definition of ACL for Deep RL and proposes a classification.
Deep Reinforcement Learning is a family of algorithms which leverage deep neural networks for function approximation to tackle reinforcement learning problems. DRL agents learn to perform sequences of actions $a$ given states $s$ in an environment so as to maximize some notion of cumulative reward $r$ [Sutton and Barto, 2018]. Such problems are usually called tasks and formalized as Markov Decision Pro-</p>
<p>cesses (MDPs) of the form $T=\left\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \rho_{0}\right\rangle$ where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{P}: S \times A \times S \rightarrow[0,1]$ is a transition function characterizing the probability of switching from the current state $s$ to the next state $s^{\prime}$ given action $a, \mathcal{R}: S \times A \rightarrow \mathbb{R}$ is a reward function and $\rho_{0}$ is a distribution of initial states. To challenge the generalization capacities of agents [Cobbe et al., 2018], the community introduced multitask DRL problems where agents are trained on tasks sampled from a task space: $T \sim \mathcal{T}$. In multi-goal DRL, policies and reward functions are conditioned on goals, which augments the task-MDP with a goal space $\mathcal{G}$ [Schaul et al., 2015a].
Automatic Curriculum Learning mechanisms propose to learn a task selection function $\mathcal{D}: \mathcal{H} \rightarrow \mathcal{T}$ where $\mathcal{H}$ can contain any information about past interactions. This is done with the objective of maximizing a metric $P$ computed over a distribution of target tasks $\mathcal{T}_{\text {target }}$ after $N$ training steps:</p>
<p>$$
O b j: \max <em T="T" _mathcal_T="\mathcal{T" _sim="\sim">{\mathcal{D}} \int</em><em _mathcal_T="\mathcal{T">{\text {target }}} P</em> T
$$}}^{N} \mathrm{~d</p>
<p>where $P_{T}^{N}$ quantifies the agent's behavior on task $T$ after $N$ training steps (e.g. cumulative reward, exploration score). In that sense, ACL can be seen as a particular case of metalearning, where $\mathcal{D}$ is learned along training to improve further learning.
ACL Typology. We propose a classification of ACL mechanisms based on three dimensions:</p>
<ol>
<li>Why use $A C L$ ? We review the different objectives that ACL has been used for (Section 3).</li>
<li>What does ACL control? ACL can target different aspects of the learning problem (e.g. environments, goals, reward functions, Section 4)</li>
<li>What does ACL optimize? ACL mechanisms usually target surrogate objectives (e.g. learning progress, diversity) to alleviate the difficulty to optimize the main objective $O b j$ directly (Section 5).</li>
</ol>
<h2>3 Why use ACL?</h2>
<p>ACL mechanisms can be used for different purposes that can be seen as particular instantiations of the general objective defined in Eq 1.
Improving performance on a restricted task set. Classical RL problems are about solving a given task, or a restricted task set (e.g. which vary by their initial state). In these simple settings, ACL has been used to improve sample efficiency or asymptotical performance [Schaul et al., 2015b; Horgan et al., 2018].
Solving hard tasks. Sometimes the target tasks cannot be solved directly (e.g. too hard or sparse rewards). In that case, ACL can be used to pose auxiliary tasks to the agent, gradually guiding its learning trajectory from simple to difficult tasks until the target tasks are solved. In recent works, ACL was used to schedule DRL agents from simple mazes to hard ones [Matissen et al., 2017], or from close-to-success initial states to challenging ones in robotic control scenarios [Florensa et al., 2017; Ivanovic et al., 2018] and video games [Salimans and Chen, 2018]. Another line of work proposes
to use ACL to organize the exploration of the state space so as to solve sparse reward problems [Bellemare et al., 2016; Pathak et al., 2017; Shyam et al., 2018; Pathak et al., 2019; Burda et al., 2019]. In these works, the performance reward is augmented with an intrinsic reward guiding the agent towards uncertain areas of the state space.
Training generalist agents. Generalist agents must be able to solve tasks they have not encountered during training (e.g. continuous task spaces or distinct training and testing set). ACL can shape learning trajectories to improve generalization, e.g. by avoiding unfeasible task subspaces [Portelas et al., 2019]. ACL can also help agents to generalize from simulation settings to the real world (Sim2Real) [OpenAI et al., 2019; Mehta et al., 2019] or to maximize performance and robustness in multi-agent settings via Self-Play [Silver et al., 2017; Pinto et al., 2017; Bansal et al., 2017; Baker et al., 2019; Vinyals et al., 2019].
Training multi-goal agents. In multi-goal RL, agents are trained and tested on tasks that vary by their goals. Because agents can control the goals they target, they learn a behavioral repertoire through one or several goal-conditioned policies. The adoption of ACL in this setting can improve performance on a testing set of pre-defined goals. Recent works demonstrated the benefits of using ACL in scenarios such as multi-goal robotic arm manipulation [Andrychowicz et al., 2017; Zhao and Tresp, 2018; Fournier et al., 2018; Zhao and Tresp, 2019; Fang et al., 2019] or multi-goal navigation [Sukhbaatar et al., 2017; Florensa et al., 2018; Racanière et al., 2019; Cideron et al., 2019].
Organizing open-ended exploration. In some multi-goal settings, the space of achievable goals is not known in advance. Agents must discover achievable goals as they explore and learn how to represent and reach them. For this problem, ACL can be used to organize the discovery and acquisition of repertoires of robust and diverse behaviors, e.g. from visual observations [Eysenbach et al., 2018; Pong et al., 2019; Jabri et al., 2019] or from natural language interactions with social peers [Lair et al., 2019; Colas et al., 2020].</p>
<h2>4 What does ACL control?</h2>
<p>While on-policy DRL algorithms directly use training data generated by the current behavioral policy, off-policy algorithms can use trajectories collected from other sources. This practically decouples data collection from data exploitation. Hence, we organize this section into two categories: one reviewing ACL for data collection, the other ACL for data exploitation.</p>
<h3>4.1 ACL for Data Collection</h3>
<p>During data collection, ACL organizes the sequential presentation of tasks as a function of the agent's capabilities. To do so, it generates tasks by acting on elements of task MDPs (e.g. $\mathcal{R}, \mathcal{P}, \rho_{0}$, see Fig. 1). The curriculum can be designed on a discrete set of tasks or on a continuous task space. In single-task problems, ACL can define a set of auxiliary tasks to be used as stepping stones towards the resolution of the main task. The following paragraphs organize the literature according to the nature of the control exerted by ACL:</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ACL for data collection. ACL can control each elements of task MDPs to shape the learning trajectories of agents. Given metrics of the agent's behavior like performance or visited states, ACL methods generate new tasks adapted to the agent's abilities.</p>
<p>Initial state $\left(\rho_{0}\right)$. The distribution of initial states $\rho_{0}$ can be controlled to modulate the difficulty of a task. Agents start learning from states close to a given target (i.e. easier tasks), then move towards harder tasks by gradually increasing the distance between the initial states and the target. This approach is especially effective to design auxiliary tasks for complex control scenarios with sparse rewards [Florensa et al., 2017; Ivanovic et al., 2018; Salimans and Chen, 2018].
Reward functions $(\mathcal{R})$. ACL can be used for automatic reward shaping: adapting the reward function $\mathcal{R}$ as a function of the learning trajectory of the agent. In curiosity-based approaches especially, an internal reward function guides agents towards areas associated with high uncertainty to foster exploration [Bellemare et al., 2016; Pathak et al., 2017; Shyam et al., 2018; Pathak et al., 2019; Burda et al., 2019]. As the agent explores, uncertain areas -and thus the reward function-change, which automatically devises a learning curriculum guiding the exploration of the state space. In Fournier et al. [2018], an ACL mechanism controls the tolerance in a goal reaching task. Starting with a low accuracy requirement, it gradually and automatically shifts towards stronger accuracy requirements as the agent progresses. In Eysenbach et al. [2018] and Jabri et al. [2019], authors propose to learn a skill space in unsupervised settings (from state space and pixels respectively), from which are derived reward functions promoting both behavioral diversity and skill separation.
Goals $(\mathcal{G})$. In multi-goal DRL, ACL techniques can be applied to order the selection of goals from discrete sets [Lair et al., 2019], continuous goal spaces [Sukhbaatar et al., 2017; Florensa et al., 2018; Pong et al., 2019; Racanière et al., 2019] or even sets of different goal spaces [Colas et al., 2019]. Although goal spaces are usually pre-defined, recent work proposed to apply ACL on a goal space learned from pixels using a generative model [Pong et al., 2019].
Environments $(\mathcal{S}, \mathcal{P})$. ACL has been successfully applied to organize the selection of environments from a discrete set, e.g. to choose among Minecraft mazes [Matiisen et al., 2017] or Sonic the Hedgehog levels [Mysore et al., 2018]. A more general -and arguably more powerful- approach is to leverage parametric Procedural Content Generation (PCG) techniques [Risi and Togelius, 2019] to generate rich task spaces. In that case, ACL allows to detect relevant niches of progress [OpenAI et al., 2019; Portelas et al., 2019;</p>
<p>Mehta et al., 2019].
Opponents $(\mathcal{S}, \mathcal{P})$. Self-play algorithms train agents against present or past versions of themselves [Silver et al., 2017; Bansal et al., 2017; Vinyals et al., 2019; Baker et al., 2019]. The set of opponents directly maps to a set of tasks, as different opponents results in different transition functions $\mathcal{P}$ and possibly state spaces $\mathcal{S}$. Self-play can thus be seen as a form of ACL, where the sequence of opponents (i.e. tasks) is organized to maximize performance and robustness. In single-agent settings, an adversary policy can be trained to perturb the main agent [Pinto et al., 2017].</p>
<h3>4.2 ACL for Data Exploitation</h3>
<p>ACL can also be used in the data exploitation stage, by acting on training data previously collected and stored in a $r e$ play memory. It enables the agent to "mentally experience the effects of its actions without actually executing them", a technique known as experience replay [Lin, 1992]. At the data exploitation level, ACL can exert two types of control on the distribution of training data: transition selection and transition modification.
Transition selection $(\mathcal{S} \times \mathcal{A})$. Inspired from the prioritized sweeping technique that organized the order of updates in planning methods [Moore and Atkeson, 1993], Schaul et al. [2015b] introduced prioritized experience replay (PER) for model-free RL to bias the selection of transitions for policy updates, as some transitions might be more informative than others. Different ACL methods propose different metrics to evaluate the importance of each transition [Schaul et al., 2015b; Zhao and Tresp, 2018; Colas et al., 2019; Zhao and Tresp, 2019; Lair et al., 2019; Colas et al., 2020].
Transition modification $(\mathcal{G})$. In multi-goal settings, Hindsight Experience Replay (HER) proposes to reinterpret trajectories collected with a given target goal with respect to a different goal [Andrychowicz et al., 2017]. In practice, HER modifies transitions by substituting target goals $g$ with one of the outcomes $g^{\prime}$ achieved later in the trajectory, as well as the corresponding reward $r^{\prime}=R_{g^{\prime}}(s, a)$. By explicitly biasing goal substitution to increase the probability of sampling rewarded transitions, HER shifts the training data distribution from simpler goals (achieved now) towards more complex goals as the agent makes progress. Substitute goal selection can be guided by other ACL mechanisms (e.g. favoring diversity [Fang et al., 2019; Cideron et al., 2019]).</p>
<h2>5 What Does ACL Optimize?</h2>
<p>Objectives such as the average performance on a set of testing tasks after $N$ training steps can be difficult to optimize directly. To alleviate this difficulty, ACL methods use a variety of surrogate objectives.
Reward. As DRL algorithms learn from reward signals, rewarded transitions are usually considered as more informative than others, especially in sparse reward problems. In such problems, ACL methods that act on transition selection may artificially increase the ratio of high versus low rewards in the batches of transitions used for policy updates [Narasimhan et</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;">Why use ACL?</th>
<th style="text-align: center;">What does ACL control?</th>
<th style="text-align: center;">What does ACL optimize?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ACL for Data Collection (§ 4.1):</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADR (OpenAI) [OpenAI et al., 2019]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Environments $(\mathcal{S}, \mathcal{P})(\mathrm{PCG})$</td>
<td style="text-align: center;">Intermediate difficulty</td>
</tr>
<tr>
<td style="text-align: center;">ADR (Mila) [Mehta et al., 2019]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Environments $(\mathcal{P})(\mathrm{PCG})$</td>
<td style="text-align: center;">Intermediate diff. \&amp; Diversity</td>
</tr>
<tr>
<td style="text-align: center;">ALP-GMM [Portelas et al., 2019]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Environments $(\mathcal{S})(\mathrm{PCG})$</td>
<td style="text-align: center;">LP</td>
</tr>
<tr>
<td style="text-align: center;">RARL [Pinto et al., 2017]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Opponents $(\mathcal{P})$</td>
<td style="text-align: center;">ARM</td>
</tr>
<tr>
<td style="text-align: center;">AlphaGO Zero [Silver et al., 2017]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Opponents $(\mathcal{P})$</td>
<td style="text-align: center;">ARM</td>
</tr>
<tr>
<td style="text-align: center;">Hide\&amp;Seek [Baker et al., 2019]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Opponents $(\mathcal{P})$</td>
<td style="text-align: center;">ARM</td>
</tr>
<tr>
<td style="text-align: center;">AlphaStar [Vinyals et al., 2019]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Opponents $(\mathcal{P})$</td>
<td style="text-align: center;">ARM \&amp; Diversity</td>
</tr>
<tr>
<td style="text-align: center;">Competitive SP [Bansal et al., 2017]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Opponents $(\mathcal{P})$</td>
<td style="text-align: center;">ARM \&amp; Diversity</td>
</tr>
<tr>
<td style="text-align: center;">RgC [Mysore et al., 2018]</td>
<td style="text-align: center;">Generalization</td>
<td style="text-align: center;">Environments $(\mathcal{S})(\mathrm{DS})$</td>
<td style="text-align: center;">LP</td>
</tr>
<tr>
<td style="text-align: center;">RC [Florensa et al., 2017]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Initial states $\left(\rho_{0}\right)$</td>
<td style="text-align: center;">Intermediate difficulty</td>
</tr>
<tr>
<td style="text-align: center;">1-demo RC [Salimans and Chen, 2018]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Initial states $\left(\rho_{0}\right)$</td>
<td style="text-align: center;">Intermediate difficulty</td>
</tr>
<tr>
<td style="text-align: center;">Count-based [Bellemare et al., 2016]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Diversity</td>
</tr>
<tr>
<td style="text-align: center;">RND [Burda et al., 2019]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Surprise (model error)</td>
</tr>
<tr>
<td style="text-align: center;">ICM [Pathak et al., 2017]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Surprise (model error)</td>
</tr>
<tr>
<td style="text-align: center;">Disagreement [Pathak et al., 2019]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Surprise (model disagreement)</td>
</tr>
<tr>
<td style="text-align: center;">MAX [Shyam et al., 2018]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Surprise (model disagreement)</td>
</tr>
<tr>
<td style="text-align: center;">BaRC [Ivanovic et al., 2018]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Intermediate difficulty</td>
</tr>
<tr>
<td style="text-align: center;">TSCL [Matiisen et al., 2017]</td>
<td style="text-align: center;">Hard Task</td>
<td style="text-align: center;">Environments $(\mathcal{S})(\mathrm{DS})$</td>
<td style="text-align: center;">LP</td>
</tr>
<tr>
<td style="text-align: center;">Acc-based CL [Fournier et al., 2018]</td>
<td style="text-align: center;">Multi-Goal</td>
<td style="text-align: center;">Reward function $(\mathcal{R})$</td>
<td style="text-align: center;">LP</td>
</tr>
<tr>
<td style="text-align: center;">Asym. SP [Sukhbaatar et al., 2017]</td>
<td style="text-align: center;">Multi-Goal</td>
<td style="text-align: center;">Goals $(\mathcal{G})$, initial states $\left(\rho_{0}\right)$</td>
<td style="text-align: center;">Intermediate difficulty</td>
</tr>
<tr>
<td style="text-align: center;">GoalGAN [Florensa et al., 2018]</td>
<td style="text-align: center;">Multi-Goal</td>
<td style="text-align: center;">Goals $(\mathcal{G})$</td>
<td style="text-align: center;">Intermediate difficulty</td>
</tr>
<tr>
<td style="text-align: center;">Setter-Solver [Racanière et al., 2019]</td>
<td style="text-align: center;">Multi-Goal</td>
<td style="text-align: center;">Goals $(\mathcal{G})$</td>
<td style="text-align: center;">Intermediate difficulty</td>
</tr>
<tr>
<td style="text-align: center;">CURIOUS [Colas et al., 2019]</td>
<td style="text-align: center;">Multi-Goal</td>
<td style="text-align: center;">Goals $(\mathcal{G})$</td>
<td style="text-align: center;">LP</td>
</tr>
<tr>
<td style="text-align: center;">Skew-fit [Pong et al., 2019]</td>
<td style="text-align: center;">Open-Ended Explo.</td>
<td style="text-align: center;">Goals $(\mathcal{G})$ (from pixels)</td>
<td style="text-align: center;">Diversity</td>
</tr>
<tr>
<td style="text-align: center;">DIAYN [Eysenbach et al., 2018]</td>
<td style="text-align: center;">Open-Ended Explo.</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Diversity</td>
</tr>
<tr>
<td style="text-align: center;">CARML [Jabri et al., 2019]</td>
<td style="text-align: center;">Open-Ended Explo.</td>
<td style="text-align: center;">Reward functions $(\mathcal{R})$</td>
<td style="text-align: center;">Diversity</td>
</tr>
<tr>
<td style="text-align: center;">LE2 [Lair et al., 2019]</td>
<td style="text-align: center;">Open-Ended Explo.</td>
<td style="text-align: center;">Goals $(\mathcal{G})$</td>
<td style="text-align: center;">Reward \&amp; Diversity</td>
</tr>
<tr>
<td style="text-align: center;">ACL for Data Exploitation (§ 4.2):</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Prioritized ER [Schaul et al., 2015b]</td>
<td style="text-align: center;">Performance boost</td>
<td style="text-align: center;">Transition selection $(\mathcal{S} \times \mathcal{A})$</td>
<td style="text-align: center;">Surprise (TD-error)</td>
</tr>
<tr>
<td style="text-align: center;">CURIOUS [Colas et al., 2019]</td>
<td style="text-align: center;">Multi-goal</td>
<td style="text-align: center;">Trans. select. \&amp; mod. $(\mathcal{S} \times \mathcal{A}, \mathcal{G})$</td>
<td style="text-align: center;">LP \&amp; Energy</td>
</tr>
<tr>
<td style="text-align: center;">HER [Andrychowicz et al., 2017]</td>
<td style="text-align: center;">Multi-goal</td>
<td style="text-align: center;">Transition modification $(\mathcal{G})$</td>
<td style="text-align: center;">Reward</td>
</tr>
<tr>
<td style="text-align: center;">HER-curriculum [Fang et al., 2019]</td>
<td style="text-align: center;">Multi-goal</td>
<td style="text-align: center;">Transition modification $(\mathcal{G})$</td>
<td style="text-align: center;">Diversity</td>
</tr>
<tr>
<td style="text-align: center;">Language HER [Cideron et al., 2019]</td>
<td style="text-align: center;">Multi-goal</td>
<td style="text-align: center;">Transition modification $(\mathcal{G})$</td>
<td style="text-align: center;">Reward</td>
</tr>
<tr>
<td style="text-align: center;">Curiosity Prio. [Zhao and Tresp, 2019]</td>
<td style="text-align: center;">Multi-goal</td>
<td style="text-align: center;">Transition selection $(\mathcal{S} \times \mathcal{A})$</td>
<td style="text-align: center;">Diversity</td>
</tr>
<tr>
<td style="text-align: center;">En. Based ER [Zhao and Tresp, 2018]</td>
<td style="text-align: center;">Multi-goal</td>
<td style="text-align: center;">Transition selection $(\mathcal{S} \times \mathcal{A})$</td>
<td style="text-align: center;">Energy</td>
</tr>
<tr>
<td style="text-align: center;">LE2 [Lair et al., 2019]</td>
<td style="text-align: center;">Open-Ended Explo.</td>
<td style="text-align: center;">Trans. select. \&amp; mod. $(\mathcal{S} \times \mathcal{A}, \mathcal{G})$</td>
<td style="text-align: center;">Reward</td>
</tr>
<tr>
<td style="text-align: center;">IMAGINE [Colas et al., 2020]</td>
<td style="text-align: center;">Open-Ended Explo.</td>
<td style="text-align: center;">Trans. select. \&amp; mod. $(\mathcal{S} \times \mathcal{A}, \mathcal{G})$</td>
<td style="text-align: center;">Reward</td>
</tr>
</tbody>
</table>
<p>Table 1: Classification of the surveyed papers. The classification is organized along the three dimensions defined in the above text. In Why use $A C L$, we only report the main objective of each work. When ACL controls the selection of environments, we precise whether it is selecting them from a discrete set $(D S)$ or through parametric Procedural Content Generation $(P C G)$. We abbreviate adversarial reward maximization by $A R M$ and learning progress by $L P$.
al., 2015; Jaderberg et al., 2016; Colas et al., 2020]. In multigoal RL settings where some goals might be much harder than others, this strategy can be used to balance the proportion of positive rewards for each of the goals [Colas et al., 2019; Lair et al., 2019]. Transition modification methods favor rewards as well, substituting goals to increase the probability of observing rewarded transitions [Andrychowicz et al., 2017; Cideron et al., 2019; Lair et al., 2019; Colas et al., 2020]. In data collection however, adapting training distributions towards more rewarded experience leads the agent to focus on tasks that are already solved. Because collecting data from already solved tasks hinders learning, data collection ACL methods rather focus on other surrogate objectives.</p>
<p>Intermediate difficulty. A more natural surrogate objective for data collection is intermediate difficulty. Intuitively, agents should target tasks that are neither too easy (already solved) nor too difficult (unsolvable) to maximize their learning progress. Intermediate difficulty has been used to adapt the distribution of initial states from which to perform a hard task [Florensa et al., 2017; Salimans and Chen, 2018; Ivanovic et al., 2018]. This objective is also implemented in GoalGAN, where a curriculum generator based on a Generative Adversarial Network is trained to propose goals for which the agent reaches intermediate performance [Florensa et al., 2018]. Racanière et al. [2019] further introduced a judge network trained to predict the feasibility of a given goal for the current learner. Instead of labelling tasks with an inter-</p>
<p>mediate level of difficulty as in GoalGAN, this Setter-Solver model generates goals associated to a random feasibility uniformly sampled from $[0,1]$. The type of goals varies as the agent progresses, but the agent is always asked to perform goals sampled from a distribution balanced in terms of feasibility. In Sukhbaatar et al. [2017], tasks are generated by an RL policy trained to propose either goals or initial states so that the resulting navigation task is of intermediate difficulty w.r.t. the current agent. Intermediate difficulty ACL has also been driving successes in Sim2Real applications, where it sequences domain randomizations to train policies that are robust enough to generalize from simulators to real-world robots [Mehta et al., 2019; OpenAI et al., 2019]. OpenAI et al. [2019] trains a robotic hand control policy to solve a Rubik's cube by automatically adjusting the task distribution so that the agent achieves decent performance while still being challenged.
Learning progress. The Obj objective of ACL methods can be seen as the maximization of a global learning progress: the difference between the final score $\int_{T \sim T} P_{T}^{\mathrm{Sl}} \mathrm{d} T$ and the initial score $\int_{T \sim T} P_{T}^{\mathrm{0}} \mathrm{d} T$. To approximate this complex objective, measures of competence learning progress (LP) localized in space and time were proposed in earlier developmental robotics works [Baranes and Oudeyer, 2013; Forestier et al., 2017]. Like Intermediate difficulty, maximizing LP drives learners to practice tasks that are neither too easy nor too difficult, but LP does not require a threshold to define what is "intermediate" and is robust to tasks with intermediate scores but where the agent cannot improve. LP maximization is usually framed as a multi-armed bandit (MAB) problem where tasks are arms and their LP measures are associated values. Maximizing LP values was shown optimal under the assumption of concave learning profiles [Lopes and Oudeyer, 2012]. Both Matiisen et al. [2017] and Mysore et al. [2018] measure LP as the estimated derivative of the performance for each task in a discrete set (Minecraft mazes and Sonic the Hedgehog levels respectively) and apply a MAB algorithm to automatically build a curriculum for their learning agents. At a higher level, CURIOUS uses absolute LP to select goal spaces to sample from in a simulated robotic arm setup [Colas et al., 2019] (absolute LP enables to redirect learning towards tasks that were forgotten or that changed). There, absolute LP is also used to bias the sampling of transition used for policy updates towards high-LP goals. ALP-GMM uses absolute LP to organize the presentation of procedurally-generated Bipedal-Walker environments sampled from a continuous task space through a stochastic parameterization [Portelas et al., 2019]. They leverage a Gaussian Mixture Model to recover a MAB setup over the continuous task space. LP can also be used to guide the choice of accuracy requirements in a reaching task [Fournier et al., 2018], or to train a replay policy via RL to sample transitions for policy updates [Zha et al., 2019].
Diversity. Some ACL methods choose to maximize measures of diversity (also called novelty or low density). In multi-goal settings for example, ACL might favor goals from low-density areas either as targets [Pong et al., 2019] or as substitute goals for data exploitation [Fang et al., 2019]. Sim-
ilarly, Zhao and Tresp [2019] biases the sampling of trajectories falling into low density areas of the trajectory space. In single-task RL, count-based approaches introduce internal reward functions as decreasing functions of the state visitation count, guiding agent towards rarely visited areas of the state space [Bellemare et al., 2016]. Through a variational expectation-maximization framework, Jabri et al. [2019] propose to alternatively update a latent skill representation from experimental data (as in Eysenbach et al. [2018]) and to metalearn a policy to adapt quickly to tasks constructed by deriving a reward function from sampled skills. Other algorithms do not optimize directly for diversity but use heuristics to maintain it. For instance, Portelas et al. [2019] maintains exploration by using a residual uniform task sampling and Bansal et al. [2017] sample opponents from past versions of different policies to maintain diversity.
Surprise. Some ACL methods train transition models and compute intrinsic rewards based on their prediction errors [Pathak et al., 2017; Burda et al., 2019] or based on the disagreement (variance) between several models from an ensemble [Shyam et al., 2018; Pathak et al., 2019]. The general idea is that models tend to give bad prediction (or disagree) for states rarely visited, thus inducing a bias towards less visited states. However, a model might show high prediction errors on stochastic parts of the environment (TV problem [Pathak et al., 2017]), a phenomenon that does not appear with model disagreement, as all models of the ensemble eventually learn to predict the (same) mean prediction [Pathak et al., 2019]. Other works bias the sampling of transitions for policy update depending on their temporal-difference error (TD-error), i.e. the difference between the transition's value and its next-step bootstrap estimation [Schaul et al., 2015b; Horgan et al., 2018]. Whether the error computation involves value models or transition models, ACL mechanisms favor states related to maximal surprise, i.e. a maximal difference between the expected (model prediction) and the truth.
Energy. In the data exploitation phase of multi-goal settings, Zhao and Tresp [2018] prioritize transitions from highenergy trajectories (e.g. kinetic energy) while Colas et al. [2019] prioritize transitions where the object relevant to the goal moved (e.g. cube movement in a cube pushing task).
Adversarial reward maximization (ARM). Self-Play is a form of ACL which optimizes agents' performance when opposed to current or past versions of themselves, an objective that we call adversarial reward maximization (ARM) [Hernandez et al., 2019]. While agents from Silver et al. [2017] and Baker et al. [2019] always oppose copies of themselves, Bansal et al. [2017] train several policies in parallel and fill a pool of opponents made of current and past versions of all policies. This maintains a diversity of opponents, which helps to fight catastrophic forgetting and to improve robustness. In the multi-agent game Starcraft II, Vinyals et al. [2019] train three main policies in parallel (one for each of the available player types). They maintain a league of opponents composed of current and past versions of both the three main policies and additional adversary policies. Opponents are not selected at random but to be challenging (as measured by winning rates).</p>
<h2>6 Discussion</h2>
<p>The bigger picture. In this survey, we unify the wide range of ACL mechanisms used in symbiosis with DRL under a common framework. ACL mechanisms are used with a particular goal in mind (e.g. organizing exploration, solving hard tasks, etc. $\S 3$ ). It controls a particular element of task MDPs (e.g. $\mathcal{S}, \mathcal{R}, \rho_{0}, \S 4$ ) and maximizes a surrogate objective to achieve its goal (e.g. diversity, learning progress, $\S 5$ ). Table 1 organizes the main works surveyed here along these three dimensions. Both previous sections and Table 1 present what has been implemented in the past, and thus, by contrast, highlight potential new avenues for ACL.
Expanding the set of ACL targets. Inspired by the maturational mechanisms at play in human infants, Elman [1993] proposed to gradually expand the working memory of a recurrent model in a word-to-word natural language processing task. The idea of changing the properties of the agent (here its memory) was also studied in developmental robotics [Baranes and Oudeyer, 2011], policy distillation methods [Czarnecki et al., 2018; Czarnecki et al., 2019] and evolutionary approaches [Ha, 2019] but is absent from the ACL-DRL literature. ACL mechanisms could indeed be used to control the agent's body $(\mathcal{S}, \mathcal{P})$, its action space (how it acts in the world, $\mathcal{A})$, its observation space (how it perceives the world, $\mathcal{S}$ ), its learning capacities (e.g. capacities of the memory, or the controller) or the way it perceives time (controlling discount factors [François-Lavet et al., 2015]).
Combining approaches. Many combinations of previously defined ACL mechanisms remain to be investigated. Could we use LP to optimize the selection of opponents in selfplay approaches? To drive goal selection in learned goal spaces (e.g. Laversanne-Finot et al. [2018], populationbased)? Could we train an adversarial domain generator to robustify policies trained for Sim2Real applications?
On the need of systematic ACL studies. Given the positive impact that ACL mechanisms can have in complex learning scenarios, one can only deplore the lack of comparative studies and standard benchmark environments. Besides, although empirical results advocate for their use, a theoretical understanding of ACL mechanisms is still missing. Although there have been attempts to frame CL in supervised settings [Bengio et al., 2009; Hacohen and Weinshall, 2019], more work is needed to see whether such considerations hold in DRL scenarios.
ACL as a step towards open-ended learning agents. Alan Turing famously wrote "Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's?" [Turing, 1950]. The idea of starting with a simple machine and to enable it to learn autonomously is the cornerstone of developmental robotics but is rarely considered in DRL [Colas et al., 2020; Eysenbach et al., 2018; Jabri et al., 2019]. Because they actively organize learning trajectories as a function of the agent's properties, ACL mechanisms could prove extremely useful in this quest. We could imagine a learning architecture leveraging ACL mechanisms to control many aspects of the learning odyssey, guiding agents from their simple original state towards fully capable agents able to reach a multiplicity
of goals. As we saw in this survey, these ACL mechanisms could control the development of the agent's body and capabilities (motor actions, sensory apparatus), organize the exploratory behavior towards tasks where agents learn the most (maximization of information gain, competence progress) or guide acquisitions of behavioral repertoires.</p>
<h2>References</h2>
<p>[Andrychowicz et al., 2017] Marcin Andrychowicz et al. Hindsight experience replay. In NeurIPS.
[Baker et al., 2019] Bowen Baker et al. Emergent tool use from multi-agent autocurricula. arXiv.
[Bansal et al., 2017] Trapit Bansal et al. Emergent complexity via multi-agent competition. arXiv.
[Baranes and Oudeyer, 2011] A. Baranes and P. Oudeyer. The interaction of maturational constraints and intrinsic motivations in active motor development. In ICDL.
[Baranes and Oudeyer, 2013] Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robot. Auton. Syst.
[Bellemare et al., 2016] Marc Bellemare et al. Unifying countbased exploration and intrinsic motivation. In NeurIPS.
[Bengio et al., 2009] Yoshua Bengio et al. Curriculum learning. In ICML.
[Burda et al., 2019] Yuri Burda et al. Exploration by random network distillation. ICLR.
[Cideron et al., 2019] Geoffrey Cideron et al. Self-educated language agent with hindsight experience replay for instruction following. arXiv.
[Cobbe et al., 2018] Karl Cobbe et al. Quantifying generalization in reinforcement learning. arXiv.
[Colas et al., 2019] Cédric Colas et al. CURIOUS: Intrinsically motivated modular multi-goal reinforcement learning. In ICML.
[Colas et al., 2020] Cédric Colas et al. Language as a cognitive tool to imagine goals in curiosity-driven exploration. arXiv.
[Czarnecki et al., 2018] Wojciech Czarnecki et al. Mix \&amp; match agent curricula for reinforcement learning. In ICML.
[Czarnecki et al., 2019] Wojciech Marian Czarnecki et al. Distilling policy distillation. arXiv.
[Elman, 1993] Jeffrey L. Elman. Learning and development in neural networks: the importance of starting small. Cognition.
[Eysenbach et al., 2018] Benjamin Eysenbach et al. Diversity is all you need: Learning skills without a reward function. arXiv.
[Fang et al., 2019] Meng Fang et al. Curriculum-guided hindsight experience replay. In NeurIPS.
[Florensa et al., 2017] Carlos Florensa et al. Reverse curriculum generation for reinforcement learning. CoRL.
[Florensa et al., 2018] Carlos Florensa et al. Automatic goal generation for reinforcement learning agents. In ICML.
[Forestier et al., 2017] Sébastien Forestier et al. Intrinsically motivated goal exploration processes with automatic curriculum learning. arXiv.</p>
<p>[Fournier et al., 2018] Pierre Fournier et al. Accuracy-based curriculum learning in deep reinforcement learning. arXiv.
[François-Lavet et al., 2015] Vincent François-Lavet et al. How to discount deep reinforcement learning: Towards new dynamic strategies. arXiv.
[Ha, 2019] David Ha. Reinforcement learning for improving agent design. Arti. Life.
[Hacohen and Weinshall, 2019] Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, ICML.
[Hernandez et al., 2019] D. Hernandez et al. A generalized framework for self-play training. In IEEE CoG.
[Horgan et al., 2018] Dan Horgan et al. Distributed prioritized experience replay. arXiv.
[Ivanovic et al., 2018] Boris Ivanovic et al. Barc: Backward reachability curriculum for robotic reinforcement learning. ICRA.
[Jabri et al., 2019] Allan Jabri et al. Unsupervised curricula for visual meta-reinforcement learning. In NeurIPS.
[Jaderberg et al., 2016] Max Jaderberg et al. Reinforcement learning with unsupervised auxiliary tasks. arXiv.
[Krueger and Dayan, 2009] Kai A. Krueger and Peter Dayan. Flexible shaping: How learning in small steps helps. Cognition.
[Lair et al., 2019] Nicolas Lair et al. Language grounding through social interactions and curiosity-driven multi-goal learning. arXiv.
[Laversanne-Finot et al., 2018] Adrien Laversanne-Finot et al. Curiosity driven exploration of learned disentangled goal spaces.
[Lesort et al., 2019] Timothée Lesort et al. Continual learning for robotics. arXiv.
[Lin, 1992] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach. lear.
[Lopes and Oudeyer, 2012] Manuel Lopes and Pierre-Yves Oudeyer. The strategic student approach for life-long exploration and learning. In ICDL.
[Matiisen et al., 2017] Tambet Matiisen et al. Teacher-student curriculum learning. IEEE TNNLS.
[Mehta et al., 2019] Bhairav Mehta et al. Active domain randomization. CoRL.
[Moore and Atkeson, 1993] Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforcement learning with less data and less time. Mach. learn.
[Mysore et al., 2018] S. Mysore et al. Reward-guided curriculum for robust reinforcement learning. preprint.
[Narasimhan et al., 2015] Karthik Narasimhan et al. Language understanding for text-based games using deep reinforcement learning. arXiv.
[OpenAI et al., 2019] OpenAI et al. Solving rubik's cube with a robot hand. arXiv.
[Oudeyer et al., 2007] Pierre-Yves Oudeyer et al. Intrinsic motivation systems for autonomous mental development. IEEE trans. on evolutionary comp.
[Pathak et al., 2017] Deepak Pathak et al. Curiosity-driven exploration by self-supervised prediction. In CVPR.
[Pathak et al., 2019] Deepak Pathak et al. Self-supervised exploration via disagreement. arXiv.
[Pinto et al., 2017] Lerrel Pinto et al. Robust adversarial reinforcement learning. arXiv.
[Pong et al., 2019] Vitchyr H. Pong et al. Skew-fit: Statecovering self-supervised reinforcement learning. arXiv.
[Portelas et al., 2019] Rémy Portelas et al. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. CoRL.
[Racanière et al., 2019] Sébastien Racanière et al. Automated curricula through setter-solver interactions. arXiv.
[Risi and Togelius, 2019] Sebastian Risi and Julian Togelius. Procedural content generation: From automatically generating game levels to increasing generality in machine learning. arXiv.
[Salimans and Chen, 2018] Tim Salimans and Richard Chen. Learning montezuma's revenge from a single demonstration. NeurIPS.
[Schaul et al., 2015a] Tom Schaul et al. Universal value function approximators. In ICML.
[Schaul et al., 2015b] Tom Schaul et al. Prioritized experience replay. arXiv.
[Schmidhuber, 1991] Jürgen Schmidhuber. Curious modelbuilding control systems. In IJCNN. IEEE.
[Selfridge et al., 1985] Oliver G Selfridge et al. Training and tracking in robotics. In IJCAI.
[Shyam et al., 2018] Pranav Shyam et al. Model-based active exploration. arXiv.
[Silver et al., 2017] David Silver et al. Mastering the game of go without human knowledge. Nature.
[Sukhbaatar et al., 2017] Sainbayar Sukhbaatar et al. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv.
[Sutton and Barto, 2018] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press.
[Taylor and Stone, 2009] Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. JMLR.
[Turing, 1950] Alan M Turing. Computing machinery and intelligence.
[Vinyals et al., 2019] Oriol Vinyals et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature.
[Wang et al., 2019] Rui Wang et al. Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv.
[Zha et al., 2019] Daochen Zha et al. Experience replay optimization. arXiv.
[Zhao and Tresp, 2018] Rui Zhao and Volker Tresp. Energybased hindsight experience prioritization. arXiv.
[Zhao and Tresp, 2019] Rui Zhao and Volker Tresp. Curiositydriven experience prioritization via density estimation. arXiv.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2698 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2698</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2698</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-227305541</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.02757v1.pdf" target="_blank">Playing Text-Based Games with Common Sense</a></p>
                <p><strong>Paper Abstract:</strong> Text based games are simulations in which an agent interacts with the world purely through natural language. They typically consist of a number of puzzles interspersed with interactions with common everyday objects and locations. Deep reinforcement learning agents can learn to solve these puzzles. However, the everyday interactions with the environment, while trivial for human players, present as additional puzzles to agents. We explore two techniques for incorporating commonsense knowledge into agents. Inferring possibly hidden aspects of the world state with either a commonsense inference model COMET, or a language model BERT. Biasing an agents exploration according to common patterns recognized by a language model. We test our technique in the 9to05 game, which is an extreme version of a text based game that requires numerous interactions with common, everyday objects in common, everyday scenarios. We conclude that agents that augment their beliefs about the world state with commonsense inferences are more robust to observational errors and omissions of common elements from text descriptions.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2698.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2698.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph A2C</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline graph-constrained deep RL agent that extracts subject-relation-object triples from observations into an ever-growing knowledge graph, encodes that graph with a Graph Attention Network, and uses an Actor-Critic policy to select natural-language commands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-constrained actor-critic agent: information extraction produces subject-relation-object triples which form a knowledge graph; the graph is encoded by a Graph Attention Network and combined with state encoding to feed an Actor-Critic model that outputs candidate text commands.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>9:05 (Jericho)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A 'slice of life' text-based interactive fiction where the player must perform a sequence of everyday tasks (e.g., get ready, shower, drive to work); sparse terminal reward but experiments use shaped intermediate rewards for checkpoints (enter bedroom, bathroom, shower, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>An ever-growing knowledge graph of subject-relation-object triples (nodes/entities and relations) encoded by a Graph Attention Network</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Extracted entities and relations from textual observations (objects, locations, relations) representing agent beliefs about world state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Relevant graph encoding via Graph Attention Network; commands filtered if they reference entities not present in the graph (i.e., relevance-based filtering/attention over graph nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>At each time step KG-A2C's information-extraction pipeline adds newly observed triples to the knowledge graph (updated after each observation/action)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Track world state across partial observability, constrain action space by filtering commands referencing unknown entities, and provide state features to the policy network for action selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Baseline performance: in Experiment 1 (full observations) KG-A2C enters the bathroom but never reaches the shower (fails to progress past early checkpoints). In Experiment 2 (missing bathroom object mentions) KG-A2C never progresses past reward 2 (enters bathroom but cannot complete tasks). (Qualitative descriptions; no numeric rates reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The knowledge-graph memory is necessary but insufficient: KG-A2C's graph built only from extracted text fails when observations omit common objects. Augmenting the graph with commonsense inferences improves robustness to missing/failed observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Performance degrades sharply when text descriptions omit common objects (knowledge graph built solely from extraction lacks implicit/common-sense entries); sparse rewards and high branching factor remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Knowledge graph augmented with commonsense inferences (COMET-A2C or Q*BERT) outperforms the unaugmented KG-A2C baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2698.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2698.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT (ALBERT-based question-answering augmented KG agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that generates questions about the environment and uses a pre-trained ALBERT question-answering model to produce answers which are converted into triples and inserted into the agent's knowledge graph, augmenting its beliefs with inferred entities/relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends KG-A2C by generating natural-language questions about the current environment and using ALBERT (a BERT variant fine-tuned for QA) to answer; answers are converted into subject-relation-object triples and added to the knowledge graph, which is then encoded for policy.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>9:05 (Jericho)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same as above: a slice-of-life interactive fiction requiring many everyday object interactions; sparse terminal reward, experiments used shaped intermediate rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory augmented via external language model inferences</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Ever-growing knowledge graph of S-R-O triples; additional nodes/relations injected based on ALBERT QA outputs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Observed triples plus inferred entities/relations (diverse set of inferred facts beyond HasA, e.g., attributes and connections among entities) intended to make implicit objects explicit in the graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph encoded with Graph Attention Network; policy uses attention-encoded features and filters commands by presence of entities in graph (relevance-based retrieval via GAT and filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>After observations, the agent generates questions whose ALBERT answers are converted to triples and added to the knowledge graph (periodic/stepwise updates tied to observation/questioning loop)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Augment perceptual observations with inferred commonsense facts to recover missing objects, enable commands that reference implicit entities, and inform action selection to reach long-horizon goals</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Experiment 1 (full observations): Q*BERT reliably gets past the shower and progresses to driving phase, performing similarly to COMET-A2C. Experiment 2 (missing bathroom object mentions): Q*BERT successfully uses inferred sink/toilet/shower to complete bathroom tasks and converges faster and more consistently than COMET-A2C. (Qualitative descriptions; no numeric percentages provided.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Adding inferred nodes to the knowledge graph makes the agent robust to missing textual mentions; Q*BERT's diverse set of inferences (via QA prompting) yields richer connections than only HasA relations and helps solve intermediate steps, improving convergence speed and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Knowledge graph augmented by ALBERT-based QA inferences (Q*BERT) performed best in the missing-observation condition due to producing diverse, task-relevant inferences beyond simple HasA relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2698.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2698.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMET-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMET-augmented KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that augments the KG-A2C knowledge graph with commonsense HasA inferences produced by the COMET model (trained on ConceptNet), adding likely objects present in locations to the graph to improve robustness to missing mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>COMET-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends KG-A2C by running COMET (a commonsense inference transformer trained on ConceptNet) on extracted sentences to produce HasA (and other) inferences; inferred short phrases are converted into triples and added to the knowledge graph which is encoded for policy.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>9:05 (Jericho)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Slice-of-life text-based game requiring everyday interactions (shower, clothing, driving) with sparse long-horizon reward; experiments use shaped checkpoint rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory augmented via external commonsense inference model</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Ever-growing knowledge graph of S-R-O triples; additional nodes inserted from COMET HasA inferences</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Observed entities/relations plus COMET-inferred commonly-present objects in locations (primarily HasA relations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph encoded by Graph Attention Network; policy uses the encoded graph features and filters candidate commands by whether entities exist in the graph (attention/relevance-based retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>KG-A2C information extraction produces triples which are augmented by COMET's inferences and added to the knowledge graph at each step</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Make implicit/common objects explicit in the agent's belief state so the agent can issue actions referencing those objects and thus progress in tasks when observations omit mentions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Experiment 1 (full observations): COMET-A2C reliably gets past the shower and onto the driving phase, comparable to Q*BERT. Experiment 2 (missing bathroom object mentions): COMET-A2C is able to complete bathroom tasks and achieve higher reward than unaugmented baselines, but requires more iterations to converge than Q*BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>COMET's HasA inferences improve robustness to missing mentions by adding expected objects to the knowledge graph, but a narrower focus on HasA relations can make convergence slower compared to a QA-based inference method that yields more diverse facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>COMET-A2C converges more slowly than Q*BERT in the extreme missing-observation condition, attributed to COMET focusing primarily on HasA relations (less diverse inference set).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Graph plus commonsense inferences (COMET or Q*BERT) outperforms unaugmented graph; Q*BERT-style diverse inferences give faster convergence in the missing-data setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2698.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2698.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C with BERT-based policy-shaping (exploration bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent that keeps the KG-A2C knowledge graph but biases exploration by re-ranking candidate actions using BERT's next-sentence-prediction/sequence likelihood over command histories to prefer command sequences that BERT considers coherent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KG-A2C architecture with a policy-shaping exploration module: sample top-k commands from the policy, compute sequence likelihood of candidate command appended to prior commands using pre-trained BERT (NSP-style scoring), re-rank candidates by this score and re-sample to bias toward procedurally coherent command sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>9:05 (Jericho)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same slice-of-life text game; tasks require procedural sequences (e.g., remove watch, remove soiled clothes, enter shower) and many common everyday object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (same KG-A2C knowledge graph); additional component is an exploration prior from BERT but not a distinct memory store</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Knowledge graph of S-R-O triples (unchanged); BERT is used to score sequences but does not add persistent memory entries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Extracted graph triples stored in KG-A2C baseline; BERT's sequence scores are computed from command history but are not stored as a separate memory structure</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Commands are re-scored by computing the likelihood of the candidate appended to prior command history using BERT (sequence-likelihood based ranking); graph encoding still used by policy</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Knowledge graph updated as in KG-A2C (information extraction per step); BERT scoring uses the running command history to compute sequence likelihoods but does not explicitly modify the graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Bias exploration toward procedurally coherent action sequences recognized by BERT (procedural commonsense), aiming to improve search through large action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Experiment 1 (full observations): KG-A2C-BERT sometimes reaches reward 5 (dropping clothes) but on average performs similar to KG-A2C; in Experiment 2 (missing bathroom object mentions) KG-A2C-BERT performs identically to KG-A2C and fails to complete bathroom tasks, indicating the exploration bias did not compensate for missing entity mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Policy-shaping via BERT helps somewhat when observations are complete (occasional better progress), but provides no benefit when the agent cannot generate necessary commands because of missing entities in the knowledge graph; augmenting the knowledge graph with inferred entities is more effective than exploration bias alone.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>BERT-based exploration bias is ineffective if the agent lacks the underlying entities in its knowledge graph (i.e., it cannot generate the necessary candidate commands to be re-ranked).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Augmenting the knowledge graph with commonsense inferences (COMET-A2C or Q*BERT) is superior to using BERT-based exploration bias alone; exploration bias helps only when the underlying perceptual/graph information is present.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Based Games with Common Sense', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Comet: Commonsense transformers for automatic knowledge graph construction <em>(Rating: 2)</em></li>
                <li>BERT: pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 1)</em></li>
                <li>Enhancing text-based reinforcement learning agents with commonsense knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2698",
    "paper_id": "paper-227305541",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "KG-A2C",
            "name_full": "Knowledge Graph A2C",
            "brief_description": "Baseline graph-constrained deep RL agent that extracts subject-relation-object triples from observations into an ever-growing knowledge graph, encodes that graph with a Graph Attention Network, and uses an Actor-Critic policy to select natural-language commands.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "Graph-constrained actor-critic agent: information extraction produces subject-relation-object triples which form a knowledge graph; the graph is encoded by a Graph Attention Network and combined with state encoding to feed an Actor-Critic model that outputs candidate text commands.",
            "base_model_size": null,
            "game_benchmark_name": "9:05 (Jericho)",
            "game_description": "A 'slice of life' text-based interactive fiction where the player must perform a sequence of everyday tasks (e.g., get ready, shower, drive to work); sparse terminal reward but experiments use shaped intermediate rewards for checkpoints (enter bedroom, bathroom, shower, etc.).",
            "uses_memory": true,
            "memory_type": "graph-based memory",
            "memory_structure": "An ever-growing knowledge graph of subject-relation-object triples (nodes/entities and relations) encoded by a Graph Attention Network",
            "memory_content": "Extracted entities and relations from textual observations (objects, locations, relations) representing agent beliefs about world state",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Relevant graph encoding via Graph Attention Network; commands filtered if they reference entities not present in the graph (i.e., relevance-based filtering/attention over graph nodes)",
            "memory_update_strategy": "At each time step KG-A2C's information-extraction pipeline adds newly observed triples to the knowledge graph (updated after each observation/action)",
            "memory_usage_purpose": "Track world state across partial observability, constrain action space by filtering commands referencing unknown entities, and provide state features to the policy network for action selection",
            "performance_with_memory": "Baseline performance: in Experiment 1 (full observations) KG-A2C enters the bathroom but never reaches the shower (fails to progress past early checkpoints). In Experiment 2 (missing bathroom object mentions) KG-A2C never progresses past reward 2 (enters bathroom but cannot complete tasks). (Qualitative descriptions; no numeric rates reported.)",
            "performance_without_memory": null,
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "The knowledge-graph memory is necessary but insufficient: KG-A2C's graph built only from extracted text fails when observations omit common objects. Augmenting the graph with commonsense inferences improves robustness to missing/failed observations.",
            "memory_limitations": "Performance degrades sharply when text descriptions omit common objects (knowledge graph built solely from extraction lacks implicit/common-sense entries); sparse rewards and high branching factor remain challenges.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Knowledge graph augmented with commonsense inferences (COMET-A2C or Q*BERT) outperforms the unaugmented KG-A2C baseline.",
            "uuid": "e2698.0",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Q*BERT",
            "name_full": "Q*BERT (ALBERT-based question-answering augmented KG agent)",
            "brief_description": "Agent that generates questions about the environment and uses a pre-trained ALBERT question-answering model to produce answers which are converted into triples and inserted into the agent's knowledge graph, augmenting its beliefs with inferred entities/relations.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Q*BERT",
            "agent_description": "Extends KG-A2C by generating natural-language questions about the current environment and using ALBERT (a BERT variant fine-tuned for QA) to answer; answers are converted into subject-relation-object triples and added to the knowledge graph, which is then encoded for policy.",
            "base_model_size": null,
            "game_benchmark_name": "9:05 (Jericho)",
            "game_description": "Same as above: a slice-of-life interactive fiction requiring many everyday object interactions; sparse terminal reward, experiments used shaped intermediate rewards.",
            "uses_memory": true,
            "memory_type": "graph-based memory augmented via external language model inferences",
            "memory_structure": "Ever-growing knowledge graph of S-R-O triples; additional nodes/relations injected based on ALBERT QA outputs",
            "memory_content": "Observed triples plus inferred entities/relations (diverse set of inferred facts beyond HasA, e.g., attributes and connections among entities) intended to make implicit objects explicit in the graph",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Graph encoded with Graph Attention Network; policy uses attention-encoded features and filters commands by presence of entities in graph (relevance-based retrieval via GAT and filtering)",
            "memory_update_strategy": "After observations, the agent generates questions whose ALBERT answers are converted to triples and added to the knowledge graph (periodic/stepwise updates tied to observation/questioning loop)",
            "memory_usage_purpose": "Augment perceptual observations with inferred commonsense facts to recover missing objects, enable commands that reference implicit entities, and inform action selection to reach long-horizon goals",
            "performance_with_memory": "Experiment 1 (full observations): Q*BERT reliably gets past the shower and progresses to driving phase, performing similarly to COMET-A2C. Experiment 2 (missing bathroom object mentions): Q*BERT successfully uses inferred sink/toilet/shower to complete bathroom tasks and converges faster and more consistently than COMET-A2C. (Qualitative descriptions; no numeric percentages provided.)",
            "performance_without_memory": null,
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Adding inferred nodes to the knowledge graph makes the agent robust to missing textual mentions; Q*BERT's diverse set of inferences (via QA prompting) yields richer connections than only HasA relations and helps solve intermediate steps, improving convergence speed and consistency.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Knowledge graph augmented by ALBERT-based QA inferences (Q*BERT) performed best in the missing-observation condition due to producing diverse, task-relevant inferences beyond simple HasA relations.",
            "uuid": "e2698.1",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "COMET-A2C",
            "name_full": "COMET-augmented KG-A2C",
            "brief_description": "Agent that augments the KG-A2C knowledge graph with commonsense HasA inferences produced by the COMET model (trained on ConceptNet), adding likely objects present in locations to the graph to improve robustness to missing mentions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "COMET-A2C",
            "agent_description": "Extends KG-A2C by running COMET (a commonsense inference transformer trained on ConceptNet) on extracted sentences to produce HasA (and other) inferences; inferred short phrases are converted into triples and added to the knowledge graph which is encoded for policy.",
            "base_model_size": null,
            "game_benchmark_name": "9:05 (Jericho)",
            "game_description": "Slice-of-life text-based game requiring everyday interactions (shower, clothing, driving) with sparse long-horizon reward; experiments use shaped checkpoint rewards.",
            "uses_memory": true,
            "memory_type": "graph-based memory augmented via external commonsense inference model",
            "memory_structure": "Ever-growing knowledge graph of S-R-O triples; additional nodes inserted from COMET HasA inferences",
            "memory_content": "Observed entities/relations plus COMET-inferred commonly-present objects in locations (primarily HasA relations)",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Graph encoded by Graph Attention Network; policy uses the encoded graph features and filters candidate commands by whether entities exist in the graph (attention/relevance-based retrieval)",
            "memory_update_strategy": "KG-A2C information extraction produces triples which are augmented by COMET's inferences and added to the knowledge graph at each step",
            "memory_usage_purpose": "Make implicit/common objects explicit in the agent's belief state so the agent can issue actions referencing those objects and thus progress in tasks when observations omit mentions",
            "performance_with_memory": "Experiment 1 (full observations): COMET-A2C reliably gets past the shower and onto the driving phase, comparable to Q*BERT. Experiment 2 (missing bathroom object mentions): COMET-A2C is able to complete bathroom tasks and achieve higher reward than unaugmented baselines, but requires more iterations to converge than Q*BERT.",
            "performance_without_memory": null,
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "COMET's HasA inferences improve robustness to missing mentions by adding expected objects to the knowledge graph, but a narrower focus on HasA relations can make convergence slower compared to a QA-based inference method that yields more diverse facts.",
            "memory_limitations": "COMET-A2C converges more slowly than Q*BERT in the extreme missing-observation condition, attributed to COMET focusing primarily on HasA relations (less diverse inference set).",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Graph plus commonsense inferences (COMET or Q*BERT) outperforms unaugmented graph; Q*BERT-style diverse inferences give faster convergence in the missing-data setting.",
            "uuid": "e2698.2",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "KG-A2C-BERT",
            "name_full": "KG-A2C with BERT-based policy-shaping (exploration bias)",
            "brief_description": "Agent that keeps the KG-A2C knowledge graph but biases exploration by re-ranking candidate actions using BERT's next-sentence-prediction/sequence likelihood over command histories to prefer command sequences that BERT considers coherent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-A2C-BERT",
            "agent_description": "KG-A2C architecture with a policy-shaping exploration module: sample top-k commands from the policy, compute sequence likelihood of candidate command appended to prior commands using pre-trained BERT (NSP-style scoring), re-rank candidates by this score and re-sample to bias toward procedurally coherent command sequences.",
            "base_model_size": null,
            "game_benchmark_name": "9:05 (Jericho)",
            "game_description": "Same slice-of-life text game; tasks require procedural sequences (e.g., remove watch, remove soiled clothes, enter shower) and many common everyday object interactions.",
            "uses_memory": true,
            "memory_type": "graph-based memory (same KG-A2C knowledge graph); additional component is an exploration prior from BERT but not a distinct memory store",
            "memory_structure": "Knowledge graph of S-R-O triples (unchanged); BERT is used to score sequences but does not add persistent memory entries",
            "memory_content": "Extracted graph triples stored in KG-A2C baseline; BERT's sequence scores are computed from command history but are not stored as a separate memory structure",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Commands are re-scored by computing the likelihood of the candidate appended to prior command history using BERT (sequence-likelihood based ranking); graph encoding still used by policy",
            "memory_update_strategy": "Knowledge graph updated as in KG-A2C (information extraction per step); BERT scoring uses the running command history to compute sequence likelihoods but does not explicitly modify the graph",
            "memory_usage_purpose": "Bias exploration toward procedurally coherent action sequences recognized by BERT (procedural commonsense), aiming to improve search through large action spaces",
            "performance_with_memory": "Experiment 1 (full observations): KG-A2C-BERT sometimes reaches reward 5 (dropping clothes) but on average performs similar to KG-A2C; in Experiment 2 (missing bathroom object mentions) KG-A2C-BERT performs identically to KG-A2C and fails to complete bathroom tasks, indicating the exploration bias did not compensate for missing entity mentions.",
            "performance_without_memory": null,
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Policy-shaping via BERT helps somewhat when observations are complete (occasional better progress), but provides no benefit when the agent cannot generate necessary commands because of missing entities in the knowledge graph; augmenting the knowledge graph with inferred entities is more effective than exploration bias alone.",
            "memory_limitations": "BERT-based exploration bias is ineffective if the agent lacks the underlying entities in its knowledge graph (i.e., it cannot generate the necessary candidate commands to be re-ranked).",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Augmenting the knowledge graph with commonsense inferences (COMET-A2C or Q*BERT) is superior to using BERT-based exploration bias alone; exploration bias helps only when the underlying perceptual/graph information is present.",
            "uuid": "e2698.3",
            "source_info": {
                "paper_title": "Playing Text-Based Games with Common Sense",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Comet: Commonsense transformers for automatic knowledge graph construction",
            "rating": 2,
            "sanitized_title": "comet_commonsense_transformers_for_automatic_knowledge_graph_construction"
        },
        {
            "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "rating": 2,
            "sanitized_title": "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 1,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "rating": 1,
            "sanitized_title": "enhancing_textbased_reinforcement_learning_agents_with_commonsense_knowledge"
        }
    ],
    "cost": 0.011538249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Playing Text-Based Games with Common Sense</p>
<p>Sahith Dambekodi sdambekodi3@gatech.edu 
Georgia Institute of Technology</p>
<p>Spencer Frazier sfrazier7@gatech.edu 
Georgia Institute of Technology</p>
<p>Prithviraj Ammanabrolu raj.ammanabrolu@gatech.edu 
Georgia Institute of Technology</p>
<p>Mark O Riedl riedl@gatech.edu 
Georgia Institute of Technology</p>
<p>Playing Text-Based Games with Common Sense</p>
<p>Text-based games are simulations in which an agent interacts with the world purely through natural language. They typically consist of a number of puzzles interspersed with interactions with common everyday objects and locations. Deep reinforcement learning agents can learn to solve these puzzles. However, the everyday interactions with the environment, while trivial for human players, present as additional puzzles to agents. We explore two techniques for incorporating commonsense knowledge into agents. (1) Inferring possibly hidden aspects of the world state with either a commonsense inference model (COMET [1]), or a language model (BERT [2]). (2) Biasing an agent's exploration according to common patterns recognized by a language model. We test our technique in the 9:05 game, which is an extreme version of a text-based game that requires numerous interactions with common, everyday objects in common, everyday scenarios. We conclude that agents that augment their beliefs about the world state with commonsense inferences are more robust to observational errors and omissions of common elements from text descriptions.</p>
<p>Common Sense in Text-Based Worlds</p>
<p>Text-based games-also called interactive fictions or text adventures-are simulations in which an agent interacts with the world purely through natural language by reading textual descriptions of the current state of the environment and composing text commands to enact change in the environment. Text-based games are partially observable in the sense that the agent can only observe the details of one particular "room" or location at a time. See Figure 1 (left) for a sample text-based game interaction. Text-based games can have a very high branching factor; the popular commercial textadventure, Zork has 1.64  10 14 possible action commands that can be entered per state [3]. A number of agents have been developed to attempt to play text-based games [3][4][5][6][7][8][9][10][11][12]. State of the art text-based game playing agents use knowledge graphs to represent the agent's belief about the full world state of the game. A knowledge graph is a set of subject, relation, object triples. The knowledge graph is encoded into a deep reinforcement learning agent that learns to infer the best action to take based on this state information. KG-A2C [10], in particular, uses information extraction techniques and rules to identify nodes and relations from text descriptions.</p>
<p>Many real-world activities can be thought of as a sequence of sub-goals in a partially observable environment. These activities-getting ready to go to work, for example-are considered trivial for humans because of commonsense knowledge. Commonsense knowledge is a set of facts, beliefs, and procedures shared among many people in the same society or culture. However, to an agent that knows only what it has learned from interacting with the environment, even tasks that humans take for granted can involve considerable trial-and-error effort. In this paper, we look at how commonsense knowledge can be incorporated in to text-based reinforcement learning agents that operates in games Observation: This bedroom is extremely spare, with dirty laundry scattered haphazardly allover the floor. Cleaner clothing can be found in the dresser. A bathroom lies to the south,while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys.The phone rings.</p>
<p>Action: Answer phone</p>
<p>Observation: You pick up the phone. "Hadley!" a shrill voice cries. "Hadley, haven't you even left yet?? You knew that our presentation was at nine o'clock sharp! First the thing with the printers, now this -there won't even be enough left of you for Bowman to fire once he's done with you. Now get the hell down here!!" Action: Examine me Observation: You're covered with mud and dried sweat. It was an exhausting night -no wonder you overslept! Even making it to the bed before conking out was a heroic accomplishment.  that simulate common, everyday situations. We hypothesize that access to commonsense knowledge can enable an agent to more quickly converge on a policy that completes common, everyday tasks. We further hypothesize that commonsense knowledge can allow the agent to infer the presence of elements in the world when observations are noisy or fail. For example, if the agent fails to observe the presence of a sink in a bathroom, it can still infer that an action such as wash hands may still be applicable given common knowledge of what a bathroom contains.</p>
<p>We experiment with agents in the text-game, 9:05, 2 which is a "slice of life" simulation game. The player must get ready for work, navigate a simple drive to work, and perform some workplace interactions. It is one of the hardest games for an agent to solve [3]; to date no agent with unrestricted ability to generate commands has completed the game. Other text-based game playing techniques that incorporate commonsense knowledge include using semantic word vectors to infer actions that can be applied to objects [5], or looking up information about objects in ConceptNet [12].</p>
<p>We experiment with ways to incorporate commonsense knowledge into a deep reinforcement learning game playing agent. The first approach is to incorporate commonsense knowledge into the world state, specifically the knowledge graph, which is the agent's beliefs about the world state. We look at two sources of commonsense inference. COMET [1] is a neural model that takes a simple sentence and infers what will be commonly believed about the people and objects referenced in the sentence. An alternative source of commonsense knowledge is BERT [2], which is believed to have acquired significant commonsense knowledge from texts on which it was trained [13]. Commonsense knowledge also manifests itself as procedural knowledge-common situations are addressed by following familiar patterns of behavior. Our third technique is to bias the agent toward certain sequences of action commands using BERTs next sentence prediction mode.</p>
<p>Methods</p>
<p>We experiment with three agent designs, each using a different source or a different way of incorporating commonsense knowledge into the agent. All three agents build off the KG-A2C [10] agent framework, which is shown in Figure 1 (right). At every step, KG-A2C uses an information extraction process to identify subject, relation, object triples in text descriptions of the current location. These triples are added to an ever-growing knowledge graph, which is encoded into the neural architecture of the agent to inform the choice of command. The knowledge graph is the agent's belief about the state of the world. Commands that reference objects are filtered out if they reference entities not in the knowledge graph. KG-A2C is our baseline.</p>
<p>The Q<em>BERT Agent. The first agent is Q</em>BERT [11], which augments the knowledge graph by using the pre-trained language model, ALBERT [14], a variant of BERT that is fine-tuned for question answering. Q<em>BERT generates questions about the current environment and ALBERT proposes answers, which are converted into subject, relation, object and added to the knowledge graph (see Figure 1 (right)). While Q</em>BERT was not explicitly design with commonsense knowledge in mind, we hypothesize that ALBERT is linking text observations to a broader set of knowledge about the world that has been acquired through training on a very large corpus of texts. That is, Q*BERT may infer things about the environment that are not directly observed but are also commonly believed by humans in similar situations.</p>
<p>The COMET-A2C Agent. Our second agent is similar to Q*BERT but replaces ALBERT with COMET [1], a neural commonsense inference model. Unlike ALBERT, COMET was trained on the ConceptNet [15] dataset to take text sentences and generate a number of short phrases that people are likely to directly infer. COMET produces several types of inferences. We use COMET's HasA inference class. COMET-A2C uses KG-A2C's information extraction process to produce subject, relation, object triples, and then we add additional nodes inferred by COMET. We hypothesize that COMET will make the agent's understanding of the world state more robust by adding object commonly found in certain types of locations. See Figure 1 (right) which shows the addition of COMET or ALBERT to the KGA2C model. The updated knowledge graph is sent as input to the Graph Attention Network which converts nodes into features and applies self-attention on all these features. The output of the model is then embedded with the encoded vector outputs for the state feedback of the environment. This final embedded vector is sent to the Actor-Critic model to decode the action.</p>
<p>KG-A2C-BERT. Our third agent is identical to KG-A2C, except that it uses a policy-shaping approach to exploration [16]. KG-A2C-BERT samples the top k commands generated by the network and scores each based on a history of previous commands. This is done by concatenating the currently proposed command to prior commands and computing the likelihood of that entire text sequence using BERT to compute P r(c t |c 1 ...c t1 ; ) where c i is a command at time step t and  is BERT's pre-trained weights. The k candidate commands are re-ranked according to the score and the agent re-samples from the new distribution. In this way, KG-A2C-BERT is biased toward exploring commands which logically entail one another, according to what BERT has learned from NSP pretraining on its very large corpus of text. In Figure 1 (right), the green box is removed.</p>
<p>Environment.</p>
<p>We conduct experiments in the 9:05 slice of life text-based game. In this game, the player must get ready for work by taking a shower, wearing clean clothes and then travel to the workplace by car. There are a large variety of different tasks that can be performed by the agent, most of which have no impact on reaching the end goal. There are very few sequences that reach the end of the game; and each of these sequences are of 25 to 30 specific actions. The 9:05 game provides a single score at the end of the game: a score of 1 at the end or 0 if the player fails. Due to the extreme sparseness of feedback all agents struggle to make any significant progress except by accident; the branching factor is too high and the only reward feedback requires 25-30 steps executed in the perfect order. Consequently, we provide a shaped reward function. We define a sequence of observations that the agent should see if progress is being made and give +1 reward for each observation: entering the bedroom, entering the bathroom, taking off the watch, taking off soiled clothes, dropping clothes. and entering the shower. The critical checkpoints are at reward 2 for when the agent enters the bathroom and at reward 6 for when the agent successfully enters the shower. The reward states are chosen such that they can only be observed in one particular order and that loops cannot occur.</p>
<p>Experiments. We conducted two experiments. (1) In the first experiment, we used the version of 9:05 where reward is given for passing key states. (2) In the second experiment, we test agents' abilities to supplement missing/failed observations with commonsense inferences. We introduce a modified version of 9:05 that has the shaped reward but also deletes all textual references to sink, toilet, and shower from the description of the bathroom location. This simulates the situation in which the agent's observations have failed to notice any of the objects, or to correctly parse and extract relations pertaining to these objects. It also simulates the situation in which a human deems it unnecessary or obvious to state the presence of these objects in a bathroom. The player must interact with all three objects in order to progress in the game. The player can still interact with the objects even they are not present in the description text; objects were not removed, only the text mentions. </p>
<p>Results and Discussion</p>
<p>The results of Experiment 1 are shown in Figure 2 (left). KG-A2C never makes it to the shower and gets stuck after it enters the bathroom. KG-A2C-BERT sometimes makes it to reward 5 (dropping clothes just before getting in the shower), but not reliably so its average performance is similar to that of KG-A2C. COMET-A2C and Q<em>BERT are both able to reliably get past the shower and on to the next phase of the game where the player must drive to work. Their performances in this experiment are not significantly different. This experiment tells us that the commonsense knowledge helps agent performance in 9:05, which makes heavy use of locations and situations that also commonly occur in the real world. KG-A2C-BERT performs better than KG-A2C because BERT informs the agent's exploration by comparing action command sequences to patterns BERT recognizes. COMET-A2C adds HasA relations to the knowledge graph; its improvement over baseline and KG-A2C-BERT is due to inferring entities that might not have been properly extracted from text descriptions. Q</em>BERT performs similarly to COMET-A2C and is likely due to inferring entities.</p>
<p>The results for Experiment 2 are shown in Figure 2 (right). In this experiment agents must contend with missing object references in room descriptions. KG-A2C never makes it past a score of 2-it enters the bathroom but cannot complete any tasks due to the inability to directly observe the sink, toilet, or shower. KG-A2C-BERT performs identical to KG-A2C; BERT's commonsense procedural guidance doesn't help if it cannot generate the necessary commands to begin with. COMET-A2C and Q<em>BERT are able to use the sink, toilet, and shower to successfully complete all the tasks required in the bathroom which leads to greater reward. Q</em>BERT converges faster in this extreme experimental condition. Experiment 2 confirms our intuitions about the role that commonsense inferences are playing in the agent's decision-making. By making the presence of key objects in one location implicit instead of explicit, we can verify in a controlled fashion that commonsense inferences are able to augment agents' senses. Similar to Experiment 1, Q<em>BERT more consistently reaches the end of the bathroom task. While COMET-A2C does reach the successfully complete the task, it requires more iterations to actually successfully complete the task. This is due to the difference in the way COMET-A2C and Q</em>BERT infer commonsense information to the knowledge graph. Both infer the existence of the missing entities, allowing them to progress through the game. However, Q<em>BERT infers a diverse set of information using a small set of questions to be answered whereas COMET-A2C focuses on HasA relations. This variance allows Q</em>BERT to solve the intermediate steps that are required to reach the end of the bathroom task by capturing information beyond what the HasA relation produces, such as using the question "What attributes does X possess?" that explicitly connects entities in richer ways.</p>
<p>The 9:05 game is a "slice of life game", which requires the player to recreate behavior that might be also conducted routinely in the real-world. Slice of life games are extreme in their invocation of common locations and common situations; agents that can effectively use commonsense knowledge are going to naturally fare better than those without. Most text-based games have a mix of fantasy and science fiction elements along with common locations and situations and it remains to be seen if commonsense can help in these scenarios.</p>
<p>Conclusions</p>
<p>We conducted experiments in slice of life text-based games to understand how commonsense knowledge can help agents handle puzzles that involve locations and scenarios commonly found in the real world. Slice of life games, and 9:05 in particular, are extreme versions of text-based games that require the player to recreate behavior that might be also conducted routinely in the real-world. Although slice of life games are dominated by these scenarios, most text-based games interleave mundane world interactions with the world between fantasy and science fiction puzzles. Our experiments show that commonsense inferences can be used to augment an agent's beliefs about the state of the world, making the agent more robust against observation failures or against missing information in text descriptions. We find that-regardless of the source of commonsense knowledge-augmenting the agent's world state beliefs is more successful than biasing the agent's exploration.</p>
<p>We contend that text-based, slice of life games are stepping stones toward goal-based natural language interactions with humans; situations in which an agent primarily understands the dynamic world through listening and acts to change the world by speaking. It is natural for commonsense details to be omitted in such environments. Our work shows how a deep reinforcement learning framework for "acting through language" can be made more robust to real-world natural language phenomena.</p>
<p>Figure 1 :
1Excerpt from the 9:05 text-based game (left). KG-A2C with augmentations (right).</p>
<p>Figure 2 :
2Reward performance for all agents on 9:05 with full observations (left) or modified observations (right). The solid lines show smoothed average performance over 4 runs, with faded bars showing max across runs. A reward of 2 indicates that the agent has entered the bathroom and a reward of 6 indicates that the agent has entered the shower.
9:05 is part of the Jericho[3] suite of games at https://github.com/microsoft/jericho</p>
<p>Comet: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, A elikyilmaz, Yejin Choi, ACL. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, A. elikyilmaz, and Yejin Choi. Comet: Commonsense transformers for automatic knowledge graph construction. In ACL, 2019.</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, abs/1810.04805CoRRJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Ct, Xingdi Yuan, Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). 2020Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Ct, and Xingdi Yuan. In- teractive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas D Kulkarni, Regina Barzilay, EMNLP. Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In EMNLP, pages 1-11, 2015.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, IJCAI. Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock? affordance extraction via word embeddings. In IJCAI, pages 1039-1045, 2017.</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, J Daniel, Shie Mankowitz, Mannor, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3562-3573. Curran Associates, Inc., 2018.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark O Riedl, Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Prithviraj Ammanabrolu and Mark O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, 2019.</p>
<p>Ledeepchef: Deep reinforcement learning agent for families of text-based games. Leonard Adolphs, Thomas Hofmann, arXiv:1909.01646arXiv preprintLeonard Adolphs and Thomas Hofmann. Ledeepchef: Deep reinforcement learning agent for families of text-based games. arXiv preprint arXiv:1909.01646, 2019.</p>
<p>Comprehensible context-driven text game playing. CoRR, abs. Xusen Yin, Jonathan May, Xusen Yin and Jonathan May. Comprehensible context-driven text game playing. CoRR, abs/1905.02265, 2019.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020.</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew J Hausknecht, M Riedl, ArXiv, absPrithviraj Ammanabrolu, Ethan Tien, Matthew J. Hausknecht, and M. Riedl. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. ArXiv, abs/2006.07409, 2020.</p>
<p>Enhancing text-based reinforcement learning agents with commonsense knowledge. ArXiv, abs. K Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, Kartik Talamadupula, K. Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, and Kartik Talamadupula. Enhancing text-based reinforcement learning agents with commonsense knowledge. ArXiv, abs/2005.00811, 2020.</p>
<p>Evaluating commonsense in pre-trained language models. Xuhui Zhou, Y Zhang, Leyang Cui, Dandan Huang, AAAI. Xuhui Zhou, Y. Zhang, Leyang Cui, and Dandan Huang. Evaluating commonsense in pre-trained language models. In AAAI, 2020.</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, International Conference on Learning Representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.</p>
<p>ConceptNet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, AAAI. Robyn Speer, Joshua Chin, and Catherine Havasi. ConceptNet 5.5: An open multilingual graph of general knowledge. In AAAI, 2016.</p>
<p>Policy shaping: Integrating human feedback with reinforcement learning. Shane Griffith, Kaushik Subramanian, Jonathan Scholz, L Charles, Andrea L Isbell, Thomaz, Advances in neural information processing systems. Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. In Advances in neural information processing systems, pages 2625-2633, 2013.</p>            </div>
        </div>

    </div>
</body>
</html>
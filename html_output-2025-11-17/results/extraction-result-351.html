<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-351 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-351</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-351</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-247939706</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2204.01691v2.pdf" target="_blank">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></p>
                <p><strong>Paper Abstract:</strong> : Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a signiﬁcant weakness of language models is that they lack real-world experience, which makes it difﬁcult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project’s website, video, and open source can be found at say-can.github.io .</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e351.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e351.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan (PaLM-SayCan)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan: Grounding Language in Robotic Affordances (PaLM-SayCan variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines a large language model (LLM) for high-level, procedural and semantic planning with learned, language‑conditioned value functions (affordances) to ground those plans in what a particular robot embodiment can actually do in the current state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM is used as a scoring/generative LLM to provide p(skill_text | instruction) by scoring candidate skill textual descriptions; it is not given direct sensory input — instead the LLM's outputs are constrained and grounded by external affordance/value-function signals. Prompt engineering (dialog format) and few-shot examples are used, and chain-of-thought prompting is optionally incorporated for harder reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-world kitchen long-horizon instruction following (101-instruction benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A mobile manipulator (7-DoF arm + mobile base) must execute long-horizon, abstract natural-language instructions in a real/mock office kitchen using a library of low-level skills (pick, place, go-to, open/close drawer, terminate, etc.). The system repeatedly selects a skill via LLM scoring combined with affordance values and executes it until termination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; object manipulation; navigation; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational (primary) with spatial grounding via affordances (spatial+procedural+object-relational)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora for the LLM (PaLM) + in-context (prompt) examples; affordances derived from RL/BC training on embodied interaction data (visual observations → learned Q-functions)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting in dialog format, LLM scoring of fixed candidate skill descriptions (probability ranking); optionally chain-of-thought generation followed by scoring</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>LLM: implicit knowledge encoded in parameters and token probability distributions over natural-language skill descriptions; Skills: short natural-language labels/embeddings (USE or other encoder); Affordances: explicit language-conditioned Q-value functions (v_pi(s)) calibrated to probabilities; combined by multiplying p(skill|instruction) * p(success|state,skill).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>planning success rate (human-rated plan validity) and execution success rate (human-rated actual completion)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Mock kitchen: planning = 84% (PaLM-SayCan), execution = 74%; Real kitchen: planning = 81%, execution = 60% (reported totals across 101 instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully used LLM semantic/procedural knowledge to decompose abstract, long-horizon instructions into appropriate sequences of available primitive skills (ordering, multi-step composition, choosing semantically-appropriate objects like 'bring water and an apple after workout'), and combined that with affordances to avoid infeasible suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>LLM-inherited failures (≈65% of errors) such as premature termination, negation/ambiguous reference mistakes, and some affordance misclassifications (≈35%) where value functions mis-predicted feasibility (e.g., pick affordance missing for an object). Performance worse on very long-horizon tasks and negation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No VF (LLM-only max skill) planning ≈ 67% (worse than SayCan); Generative (LLM generative then USE projection) planning ≈ 74%; BC NL and BC USE baselines: BC NL (policy fed full instruction) achieved 0% overall, BC USE (project instruction to nearest known skill) achieved 60% for single primitives but 0% for multi-step tasks — demonstrating the importance of LLM planning combined with affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing affordances (No VF) dropped planning from 84% to ≈67%; replacing PaLM with smaller LMs/FLAN reduced planning and execution (PaLM 540B better than FLAN 137B: PaLM-SayCan planning 84% vs FLAN-SayCan planning 70%; execution 74% vs 61%). BC NL (no LLM planning) yielded 0% overall, showing that the LLM's procedural knowledge is critical; projecting instructions with USE (BC USE) limited to single primitives only.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs encode rich procedural and object-relational knowledge in their weights that can be extracted (by scoring textual skill descriptions) to propose semantically sensible action sequences, but they are not grounded in embodiment or current state; combining LLM-provided task-grounding (p(skill|instruction)) with learned, visual language‑conditioned affordances/value functions (p(success|state,skill)) yields a practical, interpretable system that executes long-horizon embodied tasks zero-shot and substantially outperforms ungrounded LLM-only or policy-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e351.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e351.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM skill-scoring interface</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM scoring of fixed skill textual descriptions (p(skill | instruction))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method of eliciting procedural/object-relational knowledge from LLMs by scoring a closed set of natural-language skill descriptions (candidate completions) given a high-level instruction, producing p(skill_text | instruction) used for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (primary), alternatives: FLAN, smaller PaLMs, GPT-3 (open-source Colab)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B (PaLM primary); also 137B FLAN, 62B and 8B PaLM variants, GPT-3 used in open-source Colab</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM is used in a scoring mode (token-probability based) rather than free sampling: the prompt+n examples constrain outputs; the model assigns likelihoods to each candidate skill description which reflect procedural relevance and object-relational preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Skill selection for embodied instruction execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a high-level user instruction and a closed set of textual skill labels (e.g., 'find a sponge', 'pick up the coke can', 'go to table'), the LLM produces probabilities over these labels to propose the next skill in a plan.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural planning; instruction decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (which skill advances the goal, object preferences), implicitly spatial ordering via language</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora; prompt engineering with few-shot plan examples (dialog format)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting; LLM scoring (computing p(candidate | prompt) for each candidate skill), also generative decoding for explanations or chain-of-thought then scoring</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in LLM weights and outputs as probability distributions over natural-language skill sequences; plans emitted as ordered natural-language steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>planning success rate when combined with affordances; standalone LLM generative planning (without affordances) measured via projection to skills using USE and resulting planning success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used alone (No VF baseline): planning ≈ 67%; Generative+USE projection baseline ≈ 74% planning (but less interpretable and poorer execution when affordances absent). Larger LMs produced better scoring (PaLM > FLAN; larger PaLMs > smaller).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good at producing valid procedural decompositions and ordering (approach before pick, pick before bring), choosing semantically appropriate objects given instruction context.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Proposes steps infeasible for the current embodiment or environment (e.g., suggesting vacuum cleaner when none present), early termination of long-horizon plans, trouble with negation and ambiguous references; without affordances, cannot account for object presence or robot capability.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Generative LLM outputs projected via USE (Generative baseline) vs scoring interface: Generative baseline performed similarly on planning but lacked explicit probability over fixed skills and was less interpretable and harder to combine with affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Smaller LMs and FLAN reduced the quality of scoring and downstream planning; adding more few-shot examples in the prompt improved LLM termination behavior and planning reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be queried as scorers over a fixed action vocabulary to extract procedural and object-relational suggestions, but these suggestions must be combined with perceptual affordances to be feasible for an embodied agent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e351.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e351.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language-conditioned value functions (Affordances)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-conditioned value functions as affordances (p(success | state, skill))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learned Q-functions (via TD/RL with sparse rewards or behavioral cloning with value estimates) conditioned on skill text that provide the probability that executing a given skill will succeed from the current visual state — used to ground LLM proposals in embodiment and scene context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Language-conditioned Q-functions (trained with MT-Opt RL and/or BC-Z pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-task RL (MT-Opt) and BC pipelines produce policies and Q-functions conditioned on natural-language skill embeddings (from a frozen sentence encoder); Q(s, a, pi) maximized to yield v_pi(s) and calibrated to a probability-like affordance p_affordance_pi.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Affordance estimation for skill feasibility in embodied execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For each candidate low-level skill (textually described), compute the likelihood it will succeed from the current visual observation; used to weight candidate skills proposed by the LLM and select feasible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perceptual grounding; skill feasibility estimation; action selection</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (object presence and distances), procedural (can this skill be executed now), object-relational (is object pickable, relative configurations)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>multi-task RL and behavioral cloning on real and simulated robot interaction data with human-annotated success labels; value functions trained with sparse rewards (success=1, else 0).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>value query: compute v_pi(s) = max_a Q_pi(s,a) via CEM optimization; normalize/calibrate v to [0,1] to obtain p_affordance</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit affordance probabilities derived from learned Q-value functions conditioned on language embeddings; used as an explicit world-grounding signal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>contribution to execution success (combined with LLM scoring) and standalone affordance accuracy per-skill in evaluations; ablation effect on planning/execution rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Affordance grounding nearly doubled execution performance relative to ungrounded baselines in some conditions; SayCan with affordances: planning 84% / execution 74% vs No-VF planning ≈67% (execution lower). Affordance failures accounted for ≈35% of errors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Correctly identified when picks/placements/navigation are feasible (object in view, reachable); enabled rejection of LLM-suggested but impossible steps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Misclassification where affordance failed to detect pickability (e.g., sponge not recognized as pickable), or value calibration errors; some navigation affordances simplified via distance heuristics which can be coarse.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No affordance (No VF) baseline: higher rate of infeasible LLM suggestions and lower execution success; scripted affordances (distance-based for go-to) used for some skills vs learned Q for picks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing calibrated value functions reduced planning success (No VF baseline) and increased infeasible or impossible actions chosen by the LLM; calibrating Q outputs to probabilities was necessary for reliable combination.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit, language‑conditioned value functions provide the necessary embodiment and scene grounding LLMs lack, enabling LLM-proposed procedural knowledge to be filtered for feasibility and thereby enabling reliable embodied execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e351.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e351.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USE projection baseline (BC_USE / Generative+USE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Sentence Encoder (USE) projection baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that map either the high-level instruction or a generative LLM output into the nearest known skill(s) via sentence embeddings (USE), used to test whether projecting language into known skills suffices without richer LLM planning or affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Universal Sentence Encoder (USE); BC-Z policy for imitation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>USE provides fixed sentence embeddings; BC_USE projects instruction embeddings to nearest skill label embedding to select skills; Generative baseline uses free-form LLM output then projects via USE to nearest skill.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Skill selection via embedding projection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Map a user instruction (or generative LLM plan) into the closed set of available skill labels by cosine similarity over USE embeddings and execute via the learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction-following via nearest-skill retrieval; single-step and multi-step planning (limited)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational (limited); primarily tests lexical/semantic similarity</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained sentence encoder (USE) and gathered BC policy data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>embedding-based nearest-neighbor projection (cosine similarity) from instruction or LLM text to skill labels</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Natural-language instructions and candidate skills represented as fixed vector embeddings; action chosen by nearest match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>planning and execution success rates on the 101-instruction benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>BC_USE: succeeded on single primitives (≈60%) but failed on multi-step tasks (≈0% for long-horizon); Generative+USE (no affordance) achieved planning ≈74% but poorer execution when not combined with affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Sufficient for simple, single-primitive retrieval when instruction closely matches known skill phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails to compose multi-step plans or reason about feasibility; sensitive to phrase/wording mismatches; cannot handle abstraction or ordering beyond nearest-neighbor mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to PaLM-SayCan: much lower execution/planning on complex tasks, illustrating limitations of pure embedding projection.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>BC_USE is a strong baseline for single primitives but does not scale to long-horizon or abstract instructions; combining USE projection with affordances without richer LLM planning does not recover full performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple embedding projection (USE) can capture surface-level object/verb semantics but cannot replace LLM procedural decomposition; grounding via affordances is necessary for feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e351.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e351.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting integrated with SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using CoT prompting with the LLM to generate intermediate explanations/reasoning traces before scoring candidate skills, improving handling of negation and more complex reasoning about objects and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B (when used in SayCan CoT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modify prompt to include an 'Explanation' (CoT) section: first generate a free-form explanation/chain-of-thought, then use that text as additional context in the scoring stage to select skills; used to improve reasoning (negation, selection among similar items).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex object-relational/negation queries within embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Handle instructions requiring compositional or negation reasoning (e.g., 'bring a snack that isn't an apple') by producing reasoning that clarifies constraints and then selecting an appropriate skill/object.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural + object-relational reasoning; instruction interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + constrained selection (negation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM pre-training and few-shot CoT exemplars included in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>generative CoT decoding followed by scoring (the explanation is included in subsequent scoring prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Temporary natural-language reasoning traces that condition subsequent probability scoring of skill candidates; still implicit in LLM weights but made explicit via generated text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>qualitative improvements shown on negation and constrained selection cases; measured by planning/execution success on selected cases</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CoT-integrated SayCan enabled handling of negation examples and other reasoning-heavy queries that vanilla SayCan struggled with; specific case rollouts in paper show successful disambiguation (table with examples).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improved selection for constrained queries (negation, attribute-based choices), able to articulate rationale and pick semantically appropriate alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>CoT can increase verbosity and occasionally produce incorrect reasoning; underlying LLM biases and factual errors can still lead to wrong selections; still requires affordance grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Vanilla SayCan without CoT struggled on negation; CoT improved those cases while keeping affordance grounding intact.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Adding CoT (generate explanation then score) fixes certain negation/complex-reasoning failures that comprise a subset of LLM errors; not a universal fix for all LLM-inherited mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly eliciting intermediate natural-language reasoning from the LLM (CoT) can surface latent procedural/object-relational knowledge in a usable form and improves decision-making for constraints like negation when combined with affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Value function spaces: Skill-centric state abstractions for long-horizon reasoning <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>BC-Z: Zero-shot task generalization with robotic imitation learning <em>(Rating: 1)</em></li>
                <li>MT-Opt: Continuous multi-task robotic reinforcement learning at scale <em>(Rating: 1)</em></li>
                <li>CLIPort: What and where pathways for robotic manipulation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-351",
    "paper_id": "paper-247939706",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "SayCan (PaLM-SayCan)",
            "name_full": "SayCan: Grounding Language in Robotic Affordances (PaLM-SayCan variant)",
            "brief_description": "A method that combines a large language model (LLM) for high-level, procedural and semantic planning with learned, language‑conditioned value functions (affordances) to ground those plans in what a particular robot embodiment can actually do in the current state.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "model_description": "PaLM is used as a scoring/generative LLM to provide p(skill_text | instruction) by scoring candidate skill textual descriptions; it is not given direct sensory input — instead the LLM's outputs are constrained and grounded by external affordance/value-function signals. Prompt engineering (dialog format) and few-shot examples are used, and chain-of-thought prompting is optionally incorporated for harder reasoning.",
            "task_name": "Real-world kitchen long-horizon instruction following (101-instruction benchmark)",
            "task_description": "A mobile manipulator (7-DoF arm + mobile base) must execute long-horizon, abstract natural-language instructions in a real/mock office kitchen using a library of low-level skills (pick, place, go-to, open/close drawer, terminate, etc.). The system repeatedly selects a skill via LLM scoring combined with affordance values and executes it until termination.",
            "task_type": "multi-step planning; object manipulation; navigation; instruction following",
            "knowledge_type": "procedural+object-relational (primary) with spatial grounding via affordances (spatial+procedural+object-relational)",
            "knowledge_source": "pre-training on text corpora for the LLM (PaLM) + in-context (prompt) examples; affordances derived from RL/BC training on embodied interaction data (visual observations → learned Q-functions)",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting in dialog format, LLM scoring of fixed candidate skill descriptions (probability ranking); optionally chain-of-thought generation followed by scoring",
            "knowledge_representation": "LLM: implicit knowledge encoded in parameters and token probability distributions over natural-language skill descriptions; Skills: short natural-language labels/embeddings (USE or other encoder); Affordances: explicit language-conditioned Q-value functions (v_pi(s)) calibrated to probabilities; combined by multiplying p(skill|instruction) * p(success|state,skill).",
            "performance_metric": "planning success rate (human-rated plan validity) and execution success rate (human-rated actual completion)",
            "performance_result": "Mock kitchen: planning = 84% (PaLM-SayCan), execution = 74%; Real kitchen: planning = 81%, execution = 60% (reported totals across 101 instructions).",
            "success_patterns": "Successfully used LLM semantic/procedural knowledge to decompose abstract, long-horizon instructions into appropriate sequences of available primitive skills (ordering, multi-step composition, choosing semantically-appropriate objects like 'bring water and an apple after workout'), and combined that with affordances to avoid infeasible suggestions.",
            "failure_patterns": "LLM-inherited failures (≈65% of errors) such as premature termination, negation/ambiguous reference mistakes, and some affordance misclassifications (≈35%) where value functions mis-predicted feasibility (e.g., pick affordance missing for an object). Performance worse on very long-horizon tasks and negation.",
            "baseline_comparison": "No VF (LLM-only max skill) planning ≈ 67% (worse than SayCan); Generative (LLM generative then USE projection) planning ≈ 74%; BC NL and BC USE baselines: BC NL (policy fed full instruction) achieved 0% overall, BC USE (project instruction to nearest known skill) achieved 60% for single primitives but 0% for multi-step tasks — demonstrating the importance of LLM planning combined with affordances.",
            "ablation_results": "Removing affordances (No VF) dropped planning from 84% to ≈67%; replacing PaLM with smaller LMs/FLAN reduced planning and execution (PaLM 540B better than FLAN 137B: PaLM-SayCan planning 84% vs FLAN-SayCan planning 70%; execution 74% vs 61%). BC NL (no LLM planning) yielded 0% overall, showing that the LLM's procedural knowledge is critical; projecting instructions with USE (BC USE) limited to single primitives only.",
            "key_findings": "LLMs encode rich procedural and object-relational knowledge in their weights that can be extracted (by scoring textual skill descriptions) to propose semantically sensible action sequences, but they are not grounded in embodiment or current state; combining LLM-provided task-grounding (p(skill|instruction)) with learned, visual language‑conditioned affordances/value functions (p(success|state,skill)) yields a practical, interpretable system that executes long-horizon embodied tasks zero-shot and substantially outperforms ungrounded LLM-only or policy-only baselines.",
            "uuid": "e351.0",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "LLM skill-scoring interface",
            "name_full": "LLM scoring of fixed skill textual descriptions (p(skill | instruction))",
            "brief_description": "A method of eliciting procedural/object-relational knowledge from LLMs by scoring a closed set of natural-language skill descriptions (candidate completions) given a high-level instruction, producing p(skill_text | instruction) used for planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM (primary), alternatives: FLAN, smaller PaLMs, GPT-3 (open-source Colab)",
            "model_size": "540B (PaLM primary); also 137B FLAN, 62B and 8B PaLM variants, GPT-3 used in open-source Colab",
            "model_description": "LLM is used in a scoring mode (token-probability based) rather than free sampling: the prompt+n examples constrain outputs; the model assigns likelihoods to each candidate skill description which reflect procedural relevance and object-relational preferences.",
            "task_name": "Skill selection for embodied instruction execution",
            "task_description": "Given a high-level user instruction and a closed set of textual skill labels (e.g., 'find a sponge', 'pick up the coke can', 'go to table'), the LLM produces probabilities over these labels to propose the next skill in a plan.",
            "task_type": "procedural planning; instruction decomposition",
            "knowledge_type": "procedural + object-relational (which skill advances the goal, object preferences), implicitly spatial ordering via language",
            "knowledge_source": "pre-training on large text corpora; prompt engineering with few-shot plan examples (dialog format)",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting; LLM scoring (computing p(candidate | prompt) for each candidate skill), also generative decoding for explanations or chain-of-thought then scoring",
            "knowledge_representation": "Implicit in LLM weights and outputs as probability distributions over natural-language skill sequences; plans emitted as ordered natural-language steps.",
            "performance_metric": "planning success rate when combined with affordances; standalone LLM generative planning (without affordances) measured via projection to skills using USE and resulting planning success",
            "performance_result": "When used alone (No VF baseline): planning ≈ 67%; Generative+USE projection baseline ≈ 74% planning (but less interpretable and poorer execution when affordances absent). Larger LMs produced better scoring (PaLM &gt; FLAN; larger PaLMs &gt; smaller).",
            "success_patterns": "Good at producing valid procedural decompositions and ordering (approach before pick, pick before bring), choosing semantically appropriate objects given instruction context.",
            "failure_patterns": "Proposes steps infeasible for the current embodiment or environment (e.g., suggesting vacuum cleaner when none present), early termination of long-horizon plans, trouble with negation and ambiguous references; without affordances, cannot account for object presence or robot capability.",
            "baseline_comparison": "Generative LLM outputs projected via USE (Generative baseline) vs scoring interface: Generative baseline performed similarly on planning but lacked explicit probability over fixed skills and was less interpretable and harder to combine with affordances.",
            "ablation_results": "Smaller LMs and FLAN reduced the quality of scoring and downstream planning; adding more few-shot examples in the prompt improved LLM termination behavior and planning reliability.",
            "key_findings": "LLMs can be queried as scorers over a fixed action vocabulary to extract procedural and object-relational suggestions, but these suggestions must be combined with perceptual affordances to be feasible for an embodied agent.",
            "uuid": "e351.1",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Language-conditioned value functions (Affordances)",
            "name_full": "Language-conditioned value functions as affordances (p(success | state, skill))",
            "brief_description": "Learned Q-functions (via TD/RL with sparse rewards or behavioral cloning with value estimates) conditioned on skill text that provide the probability that executing a given skill will succeed from the current visual state — used to ground LLM proposals in embodiment and scene context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Language-conditioned Q-functions (trained with MT-Opt RL and/or BC-Z pipelines)",
            "model_size": null,
            "model_description": "Multi-task RL (MT-Opt) and BC pipelines produce policies and Q-functions conditioned on natural-language skill embeddings (from a frozen sentence encoder); Q(s, a, pi) maximized to yield v_pi(s) and calibrated to a probability-like affordance p_affordance_pi.",
            "task_name": "Affordance estimation for skill feasibility in embodied execution",
            "task_description": "For each candidate low-level skill (textually described), compute the likelihood it will succeed from the current visual observation; used to weight candidate skills proposed by the LLM and select feasible actions.",
            "task_type": "perceptual grounding; skill feasibility estimation; action selection",
            "knowledge_type": "spatial (object presence and distances), procedural (can this skill be executed now), object-relational (is object pickable, relative configurations)",
            "knowledge_source": "multi-task RL and behavioral cloning on real and simulated robot interaction data with human-annotated success labels; value functions trained with sparse rewards (success=1, else 0).",
            "has_direct_sensory_input": true,
            "elicitation_method": "value query: compute v_pi(s) = max_a Q_pi(s,a) via CEM optimization; normalize/calibrate v to [0,1] to obtain p_affordance",
            "knowledge_representation": "Explicit affordance probabilities derived from learned Q-value functions conditioned on language embeddings; used as an explicit world-grounding signal.",
            "performance_metric": "contribution to execution success (combined with LLM scoring) and standalone affordance accuracy per-skill in evaluations; ablation effect on planning/execution rates.",
            "performance_result": "Affordance grounding nearly doubled execution performance relative to ungrounded baselines in some conditions; SayCan with affordances: planning 84% / execution 74% vs No-VF planning ≈67% (execution lower). Affordance failures accounted for ≈35% of errors.",
            "success_patterns": "Correctly identified when picks/placements/navigation are feasible (object in view, reachable); enabled rejection of LLM-suggested but impossible steps.",
            "failure_patterns": "Misclassification where affordance failed to detect pickability (e.g., sponge not recognized as pickable), or value calibration errors; some navigation affordances simplified via distance heuristics which can be coarse.",
            "baseline_comparison": "No affordance (No VF) baseline: higher rate of infeasible LLM suggestions and lower execution success; scripted affordances (distance-based for go-to) used for some skills vs learned Q for picks.",
            "ablation_results": "Removing calibrated value functions reduced planning success (No VF baseline) and increased infeasible or impossible actions chosen by the LLM; calibrating Q outputs to probabilities was necessary for reliable combination.",
            "key_findings": "Explicit, language‑conditioned value functions provide the necessary embodiment and scene grounding LLMs lack, enabling LLM-proposed procedural knowledge to be filtered for feasibility and thereby enabling reliable embodied execution.",
            "uuid": "e351.2",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "USE projection baseline (BC_USE / Generative+USE)",
            "name_full": "Universal Sentence Encoder (USE) projection baselines",
            "brief_description": "Baselines that map either the high-level instruction or a generative LLM output into the nearest known skill(s) via sentence embeddings (USE), used to test whether projecting language into known skills suffices without richer LLM planning or affordances.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Universal Sentence Encoder (USE); BC-Z policy for imitation baseline",
            "model_size": null,
            "model_description": "USE provides fixed sentence embeddings; BC_USE projects instruction embeddings to nearest skill label embedding to select skills; Generative baseline uses free-form LLM output then projects via USE to nearest skill.",
            "task_name": "Skill selection via embedding projection",
            "task_description": "Map a user instruction (or generative LLM plan) into the closed set of available skill labels by cosine similarity over USE embeddings and execute via the learned policy.",
            "task_type": "instruction-following via nearest-skill retrieval; single-step and multi-step planning (limited)",
            "knowledge_type": "procedural+object-relational (limited); primarily tests lexical/semantic similarity",
            "knowledge_source": "pre-trained sentence encoder (USE) and gathered BC policy data",
            "has_direct_sensory_input": true,
            "elicitation_method": "embedding-based nearest-neighbor projection (cosine similarity) from instruction or LLM text to skill labels",
            "knowledge_representation": "Natural-language instructions and candidate skills represented as fixed vector embeddings; action chosen by nearest match",
            "performance_metric": "planning and execution success rates on the 101-instruction benchmark",
            "performance_result": "BC_USE: succeeded on single primitives (≈60%) but failed on multi-step tasks (≈0% for long-horizon); Generative+USE (no affordance) achieved planning ≈74% but poorer execution when not combined with affordances.",
            "success_patterns": "Sufficient for simple, single-primitive retrieval when instruction closely matches known skill phrasing.",
            "failure_patterns": "Fails to compose multi-step plans or reason about feasibility; sensitive to phrase/wording mismatches; cannot handle abstraction or ordering beyond nearest-neighbor mapping.",
            "baseline_comparison": "Compared directly to PaLM-SayCan: much lower execution/planning on complex tasks, illustrating limitations of pure embedding projection.",
            "ablation_results": "BC_USE is a strong baseline for single primitives but does not scale to long-horizon or abstract instructions; combining USE projection with affordances without richer LLM planning does not recover full performance.",
            "key_findings": "Simple embedding projection (USE) can capture surface-level object/verb semantics but cannot replace LLM procedural decomposition; grounding via affordances is necessary for feasibility.",
            "uuid": "e351.3",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Chain-of-Thought prompting",
            "name_full": "Chain-of-Thought (CoT) prompting integrated with SayCan",
            "brief_description": "Using CoT prompting with the LLM to generate intermediate explanations/reasoning traces before scoring candidate skills, improving handling of negation and more complex reasoning about objects and relations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM (Chain-of-Thought prompting)",
            "model_size": "540B (when used in SayCan CoT experiments)",
            "model_description": "Modify prompt to include an 'Explanation' (CoT) section: first generate a free-form explanation/chain-of-thought, then use that text as additional context in the scoring stage to select skills; used to improve reasoning (negation, selection among similar items).",
            "task_name": "Complex object-relational/negation queries within embodied planning",
            "task_description": "Handle instructions requiring compositional or negation reasoning (e.g., 'bring a snack that isn't an apple') by producing reasoning that clarifies constraints and then selecting an appropriate skill/object.",
            "task_type": "procedural + object-relational reasoning; instruction interpretation",
            "knowledge_type": "procedural + object-relational + constrained selection (negation)",
            "knowledge_source": "LLM pre-training and few-shot CoT exemplars included in prompt",
            "has_direct_sensory_input": false,
            "elicitation_method": "generative CoT decoding followed by scoring (the explanation is included in subsequent scoring prompt)",
            "knowledge_representation": "Temporary natural-language reasoning traces that condition subsequent probability scoring of skill candidates; still implicit in LLM weights but made explicit via generated text",
            "performance_metric": "qualitative improvements shown on negation and constrained selection cases; measured by planning/execution success on selected cases",
            "performance_result": "CoT-integrated SayCan enabled handling of negation examples and other reasoning-heavy queries that vanilla SayCan struggled with; specific case rollouts in paper show successful disambiguation (table with examples).",
            "success_patterns": "Improved selection for constrained queries (negation, attribute-based choices), able to articulate rationale and pick semantically appropriate alternatives.",
            "failure_patterns": "CoT can increase verbosity and occasionally produce incorrect reasoning; underlying LLM biases and factual errors can still lead to wrong selections; still requires affordance grounding.",
            "baseline_comparison": "Vanilla SayCan without CoT struggled on negation; CoT improved those cases while keeping affordance grounding intact.",
            "ablation_results": "Adding CoT (generate explanation then score) fixes certain negation/complex-reasoning failures that comprise a subset of LLM errors; not a universal fix for all LLM-inherited mistakes.",
            "key_findings": "Explicitly eliciting intermediate natural-language reasoning from the LLM (CoT) can surface latent procedural/object-relational knowledge in a usable form and improves decision-making for constraints like negation when combined with affordances.",
            "uuid": "e351.4",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Value function spaces: Skill-centric state abstractions for long-horizon reasoning",
            "rating": 2,
            "sanitized_title": "value_function_spaces_skillcentric_state_abstractions_for_longhorizon_reasoning"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "BC-Z: Zero-shot task generalization with robotic imitation learning",
            "rating": 1,
            "sanitized_title": "bcz_zeroshot_task_generalization_with_robotic_imitation_learning"
        },
        {
            "paper_title": "MT-Opt: Continuous multi-task robotic reinforcement learning at scale",
            "rating": 1,
            "sanitized_title": "mtopt_continuous_multitask_robotic_reinforcement_learning_at_scale"
        },
        {
            "paper_title": "CLIPort: What and where pathways for robotic manipulation",
            "rating": 1,
            "sanitized_title": "cliport_what_and_where_pathways_for_robotic_manipulation"
        }
    ],
    "cost": 0.01967575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</p>
<p>Michael Ahn 
Robotics at Google
† Everyday Robots</p>
<p>Anthony Brohan 
Robotics at Google
† Everyday Robots</p>
<p>Noah Brown 
Robotics at Google
† Everyday Robots</p>
<p>Yevgen Chebotar 
Robotics at Google
† Everyday Robots</p>
<p>Omar Cortes 
Robotics at Google
† Everyday Robots</p>
<p>Byron David 
Robotics at Google
† Everyday Robots</p>
<p>Chelsea Finn 
Robotics at Google
† Everyday Robots</p>
<p>Chuyuan Fu 
Robotics at Google
† Everyday Robots</p>
<p>Keerthana Gopalakrishnan 
Robotics at Google
† Everyday Robots</p>
<p>Karol Hausman 
Robotics at Google
† Everyday Robots</p>
<p>Alex Herzog 
Robotics at Google
† Everyday Robots</p>
<p>Daniel Ho 
Robotics at Google
† Everyday Robots</p>
<p>Jasmine Hsu 
Robotics at Google
† Everyday Robots</p>
<p>Julian Ibarz 
Robotics at Google
† Everyday Robots</p>
<p>Brian Ichter 
Robotics at Google
† Everyday Robots</p>
<p>Alex Irpan 
Robotics at Google
† Everyday Robots</p>
<p>Eric Jang 
Robotics at Google
† Everyday Robots</p>
<p>Rosario Jauregui Ruano 
Robotics at Google
† Everyday Robots</p>
<p>Kyle Jeffrey 
Robotics at Google
† Everyday Robots</p>
<p>Sally Jesmonth 
Robotics at Google
† Everyday Robots</p>
<p>Nikhil J Joshi 
Robotics at Google
† Everyday Robots</p>
<p>Ryan Julian 
Robotics at Google
† Everyday Robots</p>
<p>Dmitry Kalashnikov 
Robotics at Google
† Everyday Robots</p>
<p>Yuheng Kuang 
Robotics at Google
† Everyday Robots</p>
<p>Kuang-Huei Lee 
Robotics at Google
† Everyday Robots</p>
<p>Sergey Levine 
Robotics at Google
† Everyday Robots</p>
<p>Yao Lu 
Robotics at Google
† Everyday Robots</p>
<p>Linda Luu 
Robotics at Google
† Everyday Robots</p>
<p>Carolina Parada 
Robotics at Google
† Everyday Robots</p>
<p>Peter Pastor 
Robotics at Google
† Everyday Robots</p>
<p>Jornell Quiambao 
Robotics at Google
† Everyday Robots</p>
<p>Kanishka Rao 
Robotics at Google
† Everyday Robots</p>
<p>Jarek Rettinghouse 
Robotics at Google
† Everyday Robots</p>
<p>Diego Reyes 
Robotics at Google
† Everyday Robots</p>
<p>Pierre Sermanet 
Robotics at Google
† Everyday Robots</p>
<p>Nicolas Sievers 
Robotics at Google
† Everyday Robots</p>
<p>Clayton Tan 
Robotics at Google
† Everyday Robots</p>
<p>Alexander Toshev 
Robotics at Google
† Everyday Robots</p>
<p>Vincent Vanhoucke 
Robotics at Google
† Everyday Robots</p>
<p>Fei Xia 
Robotics at Google
† Everyday Robots</p>
<p>Ted Xiao 
Robotics at Google
† Everyday Robots</p>
<p>Peng Xu 
Robotics at Google
† Everyday Robots</p>
<p>Sichun Xu 
Robotics at Google
† Everyday Robots</p>
<p>Mengyuan Yan 
Robotics at Google
† Everyday Robots</p>
<p>Andy Zeng 
Robotics at Google
† Everyday Robots</p>
<p>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</p>
<p>Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website, the video, and open sourced code in a tabletop domain can be found at say-can.github.io. Figure 1: LLMs have not interacted with their environment and observed the outcome of their responses, and thus are not grounded in the world. SayCan grounds LLMs via value functions of pretrained skills, allowing them to execute real-world, abstract, long-horizon commands on robots.Recent progress in training large language models (LLMs) has led to systems that can generate complex text based on prompts, answer questions, or even engage in dialogue on a wide range of topics. These models absorb vast quantities of knowledge from text corpora mined from the web,</p>
<p>Introduction</p>
<p>and we might wonder whether knowledge of everyday tasks that is encoded in such models can be used by robots to perform complex tasks in the real world. But how can embodied agents extract and harness the knowledge of LLMs for physically grounded tasks?</p>
<p>This question poses a major challenge. LLMs are not grounded in the physical world and they do not observe the consequences of their generations on any physical process [1]. This can lead LLMs to not only make mistakes that seem unreasonable or humorous to people, but also to interpret instructions in ways that are nonsensical or unsafe for a particular physical situation. Figure 1 shows an example -a kitchen robot capable of executing skills such as "pick up the sponge" or "go to the table" may be asked for help cleaning up a spill ("I spilled my drink, can you help?"). A language model may respond with a reasonable narrative that is not feasible or useful for the robot. "You could try using a vacuum cleaner" is impossible if there is no vacuum in the scene or if the robot is incapable of using one. With prompt engineering, a LLM may be capable of splitting the high-level instruction into sub-tasks, but it cannot do so without the context of what the robot is capable of given its abilities and the current state of the robot and the environment.</p>
<p>Motivated by this example, we study the problem of how to extract the knowledge in LLMs for enabling an embodied agent, such as a robot, to follow high-level textual instructions. The robot is equipped with a repertoire of learned skills for "atomic" behaviors that are capable of low-level visuomotor control. We make use of the fact that, in addition to asking the LLM to simply interpret an instruction, we can use it to score the likelihood that an individual skill makes progress towards completing the high-level instruction. Then, if each skill has an affordance function that quantifies how likely it is to succeed from the current state (such as a learned value function), its value can be used to weight the skill's likelihood. In this way, the LLM describes the probability that each skill contributes to completing the instruction, and the affordance function describes the probability that each skill will succeed -combining the two provides the probability that each skill will perform the instruction successfully. The affordance functions make the LLM aware of the current scene, and constraining the completions to the skill descriptions makes the LLM aware of the robot's capabilities. Furthermore, this combination results in a fully explainable sequence of steps that the robot will execute to accomplish an instruction -an interpretable plan that is expressed through language.</p>
<p>Our method, SayCan, extracts and leverages the knowledge within LLMs in physically-grounded tasks. The LLM (Say) provides a task-grounding to determine useful actions for a high-level goal and the learned affordance functions (Can) provide a world-grounding to determine what is possible to execute upon the plan. We use reinforcement learning (RL) as a way to learn languageconditioned value functions that provide affordances of what is possible in the world. We evaluate the proposed approach on 101 real-world robotic tasks that involve a mobile robot accomplishing a large set of language instructions in a real kitchen in a zero-shot fashion. Our experiments validate that SayCan can execute temporally-extended, abstract instructions. Grounding the LLM in the real-world via affordances nearly doubles the performance over the non-grounded baselines. Additionally, by evaluating the performance of the system with different LLMs, we show that a robot's performance can be improved simply by enhancing the underlying language model. The goal of TD methods is to learn state or state-action value functions (Q-function) Q π (s, a), which represents the discounted sum of rewards when starting from state s and action a, followed by the actions produced by the policy π, i.e. Q π (s, a) = E a∼π(a|s) t R(s t , a t ). The Q-function, Q π (s, a) can be learned via approximate dynamic programming approaches that optimize the following loss: L T D (θ) = E (s,a,s )∼D [R(s, a) + γE a * ∼π Q π θ (s , a * ) − Q π θ (s, a)], where D is the dataset of states and actions and θ are the parameters of the Q-function.</p>
<p>In this work, we utilize TD-based methods to learn said value function that is additionally conditioned on the language command and utilize those to determine whether a given command is feasible from the given state. It is worth noting that in the undiscounted, sparse reward case, where the agent receives the reward of 1.0 at the end of the episode if it was successful and 0.0 otherwise, the value function trained via RL corresponds to an affordance function [10] that specifies whether a skill is possible in a given state. We leverage that intuition in our setup and express affordances via value functions of sparse reward tasks.</p>
<p>3 SayCan: Do As I Can, Not As I Say Problem Statement. Our system receives a user-provided natural language instruction i that describes a task that the robot should execute. The instruction can be long, abstract, or ambiguous. We also assume that we are given a set of skills Π, where each skill π ∈ Π performs a short task, such as picking up a particular object, and comes with a short language description π (e.g., "find a sponge") and an affordance function p(c π |s, π ), which indicates the probability of c-ompleting the skill with description π successfully from state s. Intuitively, p(c π |s, π ) means "if I ask the robot to do π , will it do it?". In RL terminology, p(c π |s, π ) is the value function for the skill if we take the reward to be 1 for successful completion and 0 otherwise.</p>
<p>As mentioned above, π denotes the textual label of skill π and p(c π |s, π ) denotes the probability that skill π with textual label π successfully completes if executed from state s, where c π is a Bernoulli random variable. The LLM provides us with p( π |i), the probability that a skill's textual label is a valid next step for the user's instruction. However, what we are interested in is the probability that a given skill successfully makes progress toward actually completing the instruction, which we denote as p(c i |i, s, π ). Assuming that a skill that succeeds makes progress on i with probability p( π |i) (i.e., its probability of being the right skill), and a skill that fails makes progress with probability zero, we can factorize this as p(c i |i, s, π ) ∝ p(c π |s, π )p( π |i). This corresponds to multiplying the probability of the language description of the skill given the instruction p( π |i), which we refer to as task-grounding, and the probability of the skill being possible in the current state of the world p(c π |s, π ), which we refer to as world-grounding.</p>
<p>Connecting Large Language Models to Robots. While large language models can draw on a wealth of knowledge learned from copious amounts of text, they will not necessarily break down high-level commands into low-level instructions that are suitable for robotic execution. If a language model were asked "how would a robot bring me an apple", it may respond "a robot could go to a nearby store and purchase an apple for you". Though this response is a reasonable completion for the prompt, it is not necessarily actionable to an embodied agent, which may have a narrow and fixed set of abilities. Therefore, to adapt language models to our problem statement, we must somehow inform them that we specifically want the high-level instruction to be broken down into sequences of available low-level skills. One approach is careful prompt engineering [5,11], a technique to coax a language model to a specific response structure. Prompt engineering provides examples in the context text ("prompt") for the model that specify the task and the response structure which the model will emulate; the prompt used in this work is shown in Appendix D.3 along with experiments ablating it. However, this is not enough to fully constrain the output to admissible primitive skills for an embodied agent, and indeed at times it can produce inadmissible actions or language that is not formatted in a way that is easy to parse into individual steps.</p>
<p>Scoring language models open an avenue to constrained responses by outputting the probabilities assigned by a language model to fixed outputs. A language model represents a distribution over potential completions p(w k |w &lt;k ), where w k is a word that appears at a k th position in a text. While typical generation applications (e.g., conversational agents) sample from this distribution or decode the maximum likelihood completion, we can also use the model to score a candidate completion selected from a set of options. Formally in SayCan, given a set of low-level skills Π, their language descriptions Π and an instruction i, we compute the probability of a language description of a skill π ∈ Π making progress towards executing the instruction i: p( π |i), which corresponds to querying the model over potential completions. The optimal skill according to the language model is computed via π = arg max π ∈ Π p( π |i). Once selected, the process proceeds by iteratively selecting a skill and appending it to the instruction. Practically, in this work we structure the planning as a dialog between a user and a robot, in which a user provides the high level-instruction (e.g., "How would you bring me a coke can?") and the language model responds with an explicit sequence ("I would: 1. π ", e.g., "I would: 1. find a coke can, 2. pick up the coke can, 3. bring it to you").</p>
<p>This has the added benefit of interpretability, as the model not only outputs generative responses, but also gives a notion of likelihood across many possible responses. Figure 3 (and Appendix Figure 12 in more detail) shows this process of forcing the LLM into a language pattern, where the set of tasks are the skills the low-level policy is capable of and prompt engineering shows plan examples and dialog between the user and the robot. With this approach, we are able to effectively extract knowledge from the language model, but it leaves a major issue: while the decoding of the instruction obtained in this way always consists of skills that are available to the robot, these skills may not necessarily be appropriate for executing the desired high-level task in the specific situation that the robot is currently in. For example, if I ask a robot to "bring me an apple", the optimal set of skills changes if there is no apple in view or if it already has one in its hand.</p>
<p>SayCan.</p>
<p>The key idea of SayCan is to ground large language models through value functions -affordance functions that capture the log likelihood that a particular skill will be able to succeed in the current state. Given a skill π ∈ Π, its language description π and its corresponding value function, which provides p(c π |s, π ), the probability of c-ompletion for the skill described by π in state s, we form an affordance space {p(c π |s, π )} π∈Π . This value function space captures affordances across all skills [12] (see Figure 2). For each skill, the affordance function and the LLM probability are then multiplied together and ultimately the most probable skill is selected, i.e. π = arg max π∈Π p(c π |s, π )p( π |i). Once the skill is selected, the corresponding policy is executed by the agent and the LLM query is amended to include π and the process is run again until a termination token (e.g., "done") is chosen. This process is shown in Figure 3 and described in Algorithm 1. These two mirrored processes together lead to a probabilistic interpretation of Say-Can, where the LLM provides probabilities of a skill being useful for the high-level instruction and the affordances provide probabilities of successfully executing each skill. Combining these two probabilities together provides a probability that this skill furthers the execution of the high-level instruction commanded by the user.</p>
<p>Implementing SayCan in a Robotic System</p>
<p>Language-Conditioned Robotic Control Policies. To instantiate SayCan, we must provide it with a set of skills, each of which has a policy, a value function, and a short language description (e.g., "pick up the can"). These skills, value functions, and descriptions can be obtained in a variety of different ways. In our implementation, we train the individual skills either with image-based behavioral cloning, following the BC-Z method [13], or reinforcement learning, following MT-Opt [14]. Regardless of how the skill's policy is obtained, we utilize value functions trained via TD backups as the affordance model for that skill. While we find that the BC policies achieve higher success rates at the current stage of our data collection process, the value functions provided by the RL policies are crucial as an abstraction to translate control capabilities to a semantic understanding of the scene. In order to amortize the cost of training many skills, we utilize multi-task BC and multitask RL, respectively, where instead of training a separate policy and value function per skill, we train multi-task policies and models that are conditioned on the language description. Note, however, Algorithm 1 SayCan Given: A high level instruction i, state s 0 , and a set of skills Π and their language descriptions Π 1: n = 0, π = ∅ 2: while πn−1 = "done" do 3:
C = ∅ 4:
for π ∈ Π and π ∈ Π do 5:</p>
<p>p LLM π = p( π |i, πn−1 , ..., π0 ) Evaluate scoring of LLM 6: p affordance π = p(c π |s n , π ) Evaluate affordance function 7: p combined π = p affordance π p LLM π 8:
C = C ∪ p combined π 9:
end for 10: π n = arg max π∈Π C</p>
<p>11:</p>
<p>Execute π n (s n ) in the environment, updating state s n+1 12:</p>
<p>n = n + 1 13: end while Figure 3: Given a high-level instruction, SayCan combines probabilities from a LLM (the probability that a skill is useful for the instruction) with the probabilities from a value function (the probability of successfully executing said skill) to select the skill to perform. This emits a skill that is both possible and useful. The process is repeated by appending the skill to the response and querying the models again, until the output step is to terminate. Appendix that this description only corresponds to low level skills -it is still the role of the LLM in SayCan to interpret the high-level instruction and break it up into individual low level skill descriptions.</p>
<p>To condition the policies on language, we utilize a pre-trained large sentence encoder language model [15]. We freeze the language model parameters during training and use the embeddings generated by passing in text descriptions of each skill. These text embeddings are used as the input to the policy and value function that specify which skill should be performed (see the details of the architectures used in the Appendix C.1). Since the language model used to generate the text embeddings is not necessarily the same as the language model used for planning, SayCan is able to utilize different language models well suited for different abstraction levels -understanding planning with respect to many skills as opposed to expressing specific skills more granularly.</p>
<p>Training the Low-Level Skills. We utilize both BC and RL policy training procedures to obtain the language-conditioned policies and value functions, respectively. To complete the description of the underlying MDP that we consider, we provide the reward function as well as the skill specification that is used by the policies and value functions. As mentioned previously, for skill specification we use a set of short, natural language descriptions that are represented as language model embeddings. We utilize sparse reward functions with reward values of 1.0 at the end of an episode if the language command was executed successfully, and 0.0 otherwise. The success of language command execution is rated by humans where the raters are given a video of the robot performing the skill, together with the given instruction. If two out of the three raters agree that the skill was accomplished successfully, the episode is labelled with a positive reward.</p>
<p>To learn language-conditioned BC policies at scale in the real world, we build on top of BC-Z [13] and use a similar policy-network architecture (shown in Fig. 10). To learn a language-conditioned RL policy, we use MT-Opt [14] in the Everyday Robots simulator using RetinaGAN sim-to-real transfer [16]. We bootstrap the performance of simulation policies by utilizing simulation demonstrations to provide initial successes, and then continuously improve the RL performance with online data collection. We use a network architecture similar to MT-Opt (shown in Fig. 9). The action space of our policies includes the six degrees of freedom of the end-effector pose as well as gripper open and close commands, x-y position and yaw orientation delta of the mobile base of the robot, and the terminate action. Additional details on data collection and training are in Appendix Section C.2.</p>
<p>Robotic System and Skills. For the control policies, we study a diverse set of manipulation and navigation skills using a mobile manipulator robot. Inspired by common skills one might pose to a robot in a kitchen environment, we propose 551 skills that span seven skill families and 17 objects, which include picking, placing and rearranging objects, opening and closing drawers, navigating to various locations, and placing objects in a specific configurations. In this study we utilize the skills that are most amenable to more complex behaviors via composition and planning as well as those that have high performance at the current stage of data collection; for more details, see Appendix D. Experimental Setup. We evaluate SayCan with a mobile manipulator and a set of object manipulation and navigation skills in two office kitchen environments. Figure 4 shows the environment setup and the robot. We use 15 objects commonly found in an office kitchen and 5 known locations with semantic meaning (two counters, a table, a trash can, and the user location). We test our method in two environments: a real office kitchen and a mock environment mirroring the kitchen, which is also the environment in which the robot's skills were trained. The robot used is a mobile manipulator from Everyday Robots 2 with a 7 degree-of-freedom arm and a two-fingered gripper. The LLM used is 540B PaLM [9] unless stated otherwise for LLM ablations. We refer to SayCan with PaLM as PaLM-SayCan.</p>
<p>Experimental Evaluation</p>
<p>Instructions. To evaluate PaLM-SayCan, we test across 101 instructions from 7 instruction families, summarized in Table 1 and enumerated in Appendix E.1. These were developed to test various aspects of SayCan and were inspired by crowd sourcing via Amazon Mechanical Turk and in-person kitchen users, as well as benchmarks such as ALFRED [17] and BEHAVIOR [18]. The instructions span multiple axes of variation: time-horizon (from single primitives to 10+ in a row), language complexity (from structured language to fully crowd-sourced requests), and embodiment (variations over the robot and environment state). Table 1   Metrics. To understand the performance of the proposed method we measure two main metrics. The first is plan success rate, which measures whether the skills selected by the model are correct for the instruction, regardless of whether or not they actually successfully executed. We ask 3 human raters to indicate whether the plan generated by the model can achieve the instruction, and if 2 out of 3 raters agree that the plan is valid, it is marked a success. Note that for many instructions there may be multiple valid solutions. For example if the instruction is to "bring a sponge and throw away the soda can", the plan can choose to bring sponge first or throw away the soda can first.</p>
<p>The second metric is execution success rate, which measures whether the full PaLM-SayCan system actually performs the desired instruction successfully. We ask 3 human raters to watch the robot execution. The raters are asked to answer the question "whether the robot achieves the task specified by the task string?" We mark an execution successful if 2 out of 3 raters agree that it is successful. Table 2 shows the performance of PaLM-SayCan across 101 tasks. In the mock kitchen, PaLM-SayCan achieved a planning success rate of 84% and an execution rate of 74%. We also investigate PaLM-SayCan out of the lab setting and in the real kitchen to verify the performance of the policies and value functions in this setting. We find a reduction of planning performance by 3% and execution by 14%, indicating PaLM-SayCan and the underlying policies generalize reasonably well to the full kitchen. The full task list and results can be found in the Appendix Table 5, and videos of experiment rollouts and the decision making process can be found on the project website: say-can.github. io. Figure 5 shows two long-horizon queries and the resulting rollouts. These tasks require PaLM-SayCan to plan many steps without error and for the robot to navigate and interact with a significant portion of the kitchen. Each query requires PaLM-SayCan to understand context implicit within the instruction. In Figure 5a, the algorithm must understand the operator has asked for something to "recover from a workout", i.e. something healthy, and thus it brings water and an apple rather than, e.g., a soda and chips. Furthermore, the algorithm must understand ordering and history, that it has already brought a drink and now must bring a snack before terminating. In Figure 5b, PaLM-SayCan must track which objects are the "them" that need to be disposed of and where the sponge should be brought.</p>
<p>Results</p>
<p>(a) "I just worked out, can you bring me a drink and a snack to recover?" (b) "I left out a coke, apple, and water, can you throw them away and then bring me a sponge to wipe the table?" Figure 5: Timelapse of rollouts to two long-horizon queries. The robot interacts with a large portion of the kitchen environment and successfully performs sequences of manipulation and navigation skills. Figure 6 highlights PaLM-SayCan's decision making, along with its interpretability. The decision making process can be understood as it solves instructions by visualizing what the two sides of the algorithm output. This allows a user to understand what options PaLM-SayCan is considering as language completions and what it believes is possible. We find that sequence order is understood (approaching objects before picking them up and picking them up before bringing them). Figure 6 shows that though the query mentions a coke, PaLM-SayCan understands that the important object is something to clean and brings a sponge. Appendix E.6 shows additional rollouts with complex decisions, embodiment grounding, and long-horizon tasks in Figures 14-17 as well as failures in Figure 16. We believe such real-time and clear interpretability opens avenues to more interactive operation. When comparing the performance of different instruction families in Table 2 (see Table 1 for an explanation of families), we see that the natural language nouns performed worse than natural language verbs, due to the number of nouns possible (15 objects and 5 locations) versus number of verbs (6). The structured language tasks (created to ablate the performance loss of spelling out the solution versus understanding the query) were planned correctly 93% of the time, while their natural language verb counterparts were planned correctly 100%. This indicates the language model effectively parses the queries. The embodiment tasks were planned correctly 64% of the time, generally with failures as a result of affordance function misclassification. PaLM-SayCan planned and executed crowd-sourced natural queries with performance on par with other instruction families.</p>
<p>PaLM-SayCan performed worst on the most challenging long-horizon tasks, where most failures were a result of early termination by the LLM (e.g., bringing one object but not the second). We also find that PaLM-SayCan struggles with negation (e.g., "bring me a snack that isn't an apple") and ambiguous references (e.g. asking for drinks with caffeine), which is a known issue inherited from underlying language models [19]. Overall, 65% of the errors were LLM failures and 35% were affordance failures.</p>
<p>Returning to our initial example, "I spilled something, can you help?", an ungrounded language model would respond with statements like "I can call you a cleaner" or "I can vacuum that up for you", which given our robot are unreasonable. We have shown that PaLM-SayCan responds "I would: 1. find a sponge, 2. pick up the sponge, 3. bring it to you, 4. done" and is able execute this sequence on the robot in a real kitchen. This requires long-horizon reasoning over a required order, an abstract understanding of the instruction, and knowledge of both the environment and robot's capabilities.</p>
<p>Ablating Language. To study the importance of the LLM, we conduct two ablation experiments using the language-conditioned policy (see . In BC NL we feed the full instruction i into the policy -this approach is representative of standard RL or BC-based instruction following methods [13,20,21,22]. In BC USE we project the high-level instruction into the set of known language commands via the Universal Sentence Encoder (USE) embeddings [15] by embedding the instruction, all the tasks, and the combinatorial set of sequences tasks (i.e., we consider "pick coke can" as well as "1. find coke can, 2. pick coke can" and so on), and selecting the highest cosine similarity instruction. The results in Table 2 illustrate the necessity of the language grounding where BC NL achieves 0% in all tasks and BC USE achieves 60% for single primitives, but 0% otherwise.</p>
<p>Ablating Value Functions. Table 2 illustrates the necessity of the affordance grounding. We compare PaLM-SayCan to (1) No VF, which removes the value function grounding (i.e., choosing the maximum language score skill) and to (2) Generative, which uses the generative output of the LLM and then projects each planned skill to its maximal cosine similarity skill via USE embeddings. The latter in effect compares to [23], which loses the explicit option probabilities, and thus is less interpretable and cannot be combined with affordance probabilities. For Generative we also tried BERT embeddings [3], but found poor performance. The No VF and Generative approaches performed similarly, achieving 67% and 74% planning success rate respectively, and worse than PaLM-SayCan's 84%.  Table 2: Success rates of instructions by family. PaLM-SayCan achieves a planning success rate of 84% and execution success rate of 74% in the training environment and 81% planning and 60% execution in a real kitchen. No VF uses the maximum score skill from the LLM, Generative (Gen.) uses a generative LLM and then projects to the nearest skill via USE embeddings, BC NL uses the policy with the natural language instruction, and BC USE uses the policy with the natural language instruction projected to the nearest skill via USE embeddings.</p>
<p>Ablating the Language Model. SayCan is able to improve with improved language models. The LLM used herein was PaLM [9], a 540B parameter model. In this section we ablate over 8B, 62B, and 540B parameter models as well as the 137B parameter FLAN model [8] which is finetuned on a "instruction answering" dataset. Appendix Table 6 shows each model on a set of generative problems, where we find that generally larger models perform better, though the difference between the 62B and 540B model is small. Results in other works, such as Chain of Thought Prompting [24], indicate this difference may be more pronounced on more challenging problems -this is shown in Section 5.2. We also find that PaLM outperforms FLAN. Though FLAN was fine-tuned on instruction answering, the broader and improved dataset for PaLM may make up for this difference in training.</p>
<p>While it is expected that the generative performance of the language model will improve with better language models, it is unclear how the LLM size influences the final robotics success rate. Table 3 shows PaLM 540B and FLAN on robot running the full SayCan algorithm. The results show that the system using PaLM with affordance grounding (PaLM-SayCan) chooses the correct sequence of skills 84% of the time and executes them successfully 74% of the time, reducing errors by half compared to FLAN. This is particularly exciting because it represents the first time we can see how an improvement in language models translates to a similar improvement in robotics. This result indicates a potential future where the fields of language processing and robotics can collaboratively improve each other and scale together.  Table 3: Success rates of instructions by family. SayCan achieves a planning success rate of 84% and execution success rate of 74% with PaLM and FLAN achieves 70% planning and 61% success. SayCan scales and improves with improved LLMs.</p>
<p>Case Studies of New Capabilities of PaLM-SayCan</p>
<p>PaLM-SayCan enables new capabilities. First, we show that it is very easy to incorporate new skills into the system, and use drawer manipulation as an example. Second, we show by leveraging chain of thought reasoning, we are able to solve complex tasks that require reasoning. Finally we show the system can work with multilingual queries, without explicitly being designed to.</p>
<p>Adding Skills: Drawer Manipulation (Appendix E.3). SayCan is capable of integrating new skills by simply adding the new skills as options for the LLM and providing accompanying value functions and add an example in the prompt with that skill. For example, with the skills open, close, and go to the drawer, SayCan is capable of solving tasks such as "restock the coke and pepsi into the drawer". Over 21 queries we found a planning rate of 100% and an execution rate of 33% (due to failures of the chained manipulation policy), with no loss in performance for other instructions. Figure 7: A sequence of the robot taking rice chips from the drawers. Note the robot only has one arm, so it needs to plan a long sequence to first take rice chips out of drawer and place on the counter, and then pick it up again after closing the drawer.</p>
<p>CHAIN-OF-THOUGHT PLANNING ROLLOUT WITH PALM-SAYCAN.</p>
<p>Human: Can you bring a fruit-flavored drink without caffeine? Explanation: The user has asked for a drink that is fruit-flavored and does not have caffeine, I will bring the lime soda. Robot: 1. find a lime soda, 2. pick up the lime soda, 3. bring it to you, 4. put down the lime soda, 5. done Human: Bring me the spicy chips. Explanation: The user has asked for chips that are spicy, I will bring the Jalapeno chips. Robot: 1. find the jalapeno chips, 2. pick up the jalapeno chips, 3. bring it to you, 4. put down the jalapeno chips, 5.done Human: The snack is too small, bring me something more filling. Explanation: The user has asked for a snack that is more filling, I will bring the multigrain chips. Robot: 1. find the multigrain chips, 2. pick up the multigrain chips, 3. bring it to you, 4. put down the multigrain chips, 5. done Table 4: Chain-of-thought planning rollout with PaLM-SayCan. The highlighted part is the chain of thought generated by PaLM-SayCan.</p>
<p>Chain of Thought Reasoning. SayCan can be integrated with recent work improving LLM reasoning, such as Chain of Thought [24]. One limitation of vanilla SayCan is that it doesn't handle tasks that involves negation. This is inherited from underline language models, and studied in the NLP community [19]. However, we found by using chain-of-thought prompting [24] we can improve SayCan on this front.</p>
<p>For chain-of-thought prompting-based SayCan, we need to modify the prompt to include a part called "Explanation". We also slightly change how we use the language model. Instead of directly using the scoring interface to rank possible options, we first use the generative decoding of LLM to create an explanation, and then use the scoring mode, by including the explanation into the prompt. The full prompt is shown in Appendix E.4 Listing 3.</p>
<p>A few successful rollouts of the model at evaluation time is shown in Table 4. As we can see, with chain of thought prompting, the model can handle negations and tasks that require reasoning.</p>
<p>Multilingual Queries (Appendix E.5). While not explicitly designed to work with multilingual queries, PaLM-SayCan is able to handle them. The LLM was trained on multilingual corpora and thus SayCan can handle multilingual queries other than English. The results of SayCan on multilingual queries are summarized in Table. 8, and there is almost no performance drop in planning success rate when changing the queries from English to Chinese, French and Spanish.</p>
<p>Closed-Loop Planning. As presented herein, SayCan only receives environmental feedback through value functions at the current decision step, meaning if a skill fails or the environment changes, the necessary feedback may not be available. Owing to the extendibility and the natural language interface, Huang et al. [25] builds upon SayCan to enable closed-loop planning by leveraging environment feedback (from e.g., success detectors, scene descriptors, or even human feedback) through an inner monologue.</p>
<p>Open Source Environment</p>
<p>We have open-sourced an implementation of SayCan in a Google Colab notebook at say-can. github.io/#open-source. The environment is shown in Figure 8 and is a tabletop with a UR5 robot and randomly generated sets of colored blocks and bowls. The low-level policy is implemented with CLIPort [26], which is trained to output a pick and place location. Due to the lack of a value function for this policy, the affordances are implemented with a ViLD object detector [27]. GPT-3 is used as the open source language model [5]. Steps are output in the form "pick up the object and place it in location", leveraging the ability of LLMs to output code structures.</p>
<p>Task: move all the blocks into their matching colored bowls.</p>
<p>Step 1. pick up the blue block and place it in the blue bowl</p>
<p>Step 2. pick up the green block and place it in the green bowl</p>
<p>Step 3. pick up the yellow block and place it in the yellow bowl </p>
<p>Related Work</p>
<p>Grounding Language Models. A significant body of work has focused on grounding language [28,29]. Recent works have studied how to ground modern language models, by training them to accept additional environment inputs [30,31,32,33,34] or to directly output actions [35,36,37]. Others grounded language in an environment through prompt engineering [24]. Concurrently with SayCan, Huang et al. [23] use prompt engineering to extract temporally extended plans, but without any additional mechanism to ensure grounding, roughly corresponding to the "Generative" baseline in our experiments. The above methods are all trained without interaction with a physical environment, thus limiting their ability to reason over embodied interactions. One approach to grounding language models in interaction is by learning downstream networks with pre-trained LLM representations [38,22,21,39,40,41,42,43]. Another approach finetunes language models with interactive data, such as rewards or ranking feedback of the interaction [11,44,45]. In our work, SayCan is able to ground language models in the given environment through previously-trained value functions, enabling general, long-horizon behaviors in a zero-shot manner, i.e., without additional training.</p>
<p>Learning Language-Conditioned Behavior. There is a long history of research studying how to connect language and behavior [46,47,48,49,50]. A large number of prior works have learned language-conditioned behavior via imitation learning [51,22,20,13,26,37] or reinforcement learning [52,53,49,54,55,56,21,41]. Most of these prior works focus on following low-level instructions, such as for pick-and-place tasks and other robotic manipulation primitives [20,22,56,21,13,26], though some methods address long-horizon, compound tasks in simulated domains [57,58,54]. Like these latter works, we focus on completing temporally extended tasks. However, a central aspect of our work is to solve such tasks by extracting and leveraging the knowledge in large language models. While prior works have studied how pre-trained language embeddings can improve generalization to new instructions [38,22,21] and to new low-level tasks [13], we extract much more substantial knowledge from LLMs by grounding them within the robot's affordances. This allows robots to use language models for planning.</p>
<p>Task Planning and Motion Planning. Task and motion planning [59,60] is a problem of sequencing tasks to solve a high-level problem, while ensuring the feasibility given an embodiment (task planning [61,62,63]; motion planning [64]). Classically, this problem has been solved through symbolic planning [61,63] or optimization [65,66], but these require explicit primitives and constraints. Machine learning has recently been applied to enable abstract task specification, allow general primitives, or relax constraints [67,68,69,70,71,72,73,74,75,76,77,78]. Others learn to hierarchically solve such long-horizon problems [79,80,12,81,54]. SayCan leverages an LLM's semantic knowledge about the world for interpreting instructions and understanding how to execute them. The use of LLMs and generality of learned low-level policies enables long-horizon, abstract tasks that scale effectively to the real world, as demonstrated in our robot experiments.</p>
<p>Conclusions, Limitations and Future Work</p>
<p>We presented SayCan, a method that enables leveraging and grounding the rich knowledge in large language models to complete embodied tasks. For real-world grounding, we leverage pre-trained skills, which are then used to condition the model to choose natural language actions that are both feasible and contextually appropriate. More specifically, we use reinforcement learning as a way to learn value functions for the individual skills that provide affordances of what is possible in the world, and then use textual labels for these skills as potential responses that are scored by a language model. This combination results in a symbiotic relationship where the skills and their value functions can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about how to complete a task. We evaluated the proposed approach on a number of real-world robotic tasks that involve a mobile manipulator robot accomplishing a large set of long-horizon natural language instructions in a real kitchen. We also demonstrated an exciting property of our method where a robot's performance can be improved simply by enhancing the underlying language model. While SayCan presents a viable way to ground language models in agents' affordances, it has a number of limitations. First, we expect this method to inherit the limitations and biases of LLMs [82,83], including the dependence on the training data. Secondly, we observe that even though SayCan allows the users to interact with the agents using natural language commands, the primary bottleneck of the system is in the range and capabilities of the underlying skills. To illustrate this, we present representative failure cases in Appendix E. Future work that extends the repertoire of skills and improves their robustness would mitigate this limitation. In addition, at the current stage, the system is not easily able to react to situations where individual skills fail despite reporting a high value, though this could potentially be addressed by appropriate prompting of the language model for a correction.</p>
<p>There are many other potential avenues for future work. A natural question that this work raises is how the information gained through grounding the LLM via real-world robotic experience can be leveraged to improve the language model itself, both in terms of its factuality and its ability to perform common-sense reasoning about real-world environments and physics. Furthermore, since our method uses generic value functions to score affordances, it is intriguing to consider what other sources of grounding could be incorporated in the same manner, such as non-robotic contexts.</p>
<p>In the future, it is also interesting to examine whether natural language is the right ontology to use to program robots: natural language naturally incorporates contextual and semantic cues from the environment, and provides a level of abstraction which enables robots do decide on how to execute a strategy based on its own perception and affordances. At the same time, as opposed to, e.g., hindsight goal images [84], it requires supervision and it might not be the most descriptive medium for certain tasks.</p>
<p>Lastly, SayCan presents a particular way of connecting and factorizing the challenges of language understanding and robotics, and many further extensions can be proposed. Ideas such as combining robot planning and language [85], using language models as a pre-training mechanism for policies [44] and many other ways of combining language and interaction [21,22,38,86]  • Created open-source environment: Brian Ichter, Andy Zeng.</p>
<p>B.2 By Person</p>
<p>Michael Ahn developed the deployment system that enabled the ability to scale up data collection on real robots. Anthony Brohan implemented the logging system for the project and designed and implemented the data labeling pipelines. Noah Brown led and coordinated the real-robot operations including data collection with teleoperators, evaluations and the real-world setup. Yevgen Chebotar designed and implemented multiple offline RL methods allowing the manipulation policies to process data coming from different sources. Omar Cortes collected data on the robots and ran and supervised real-world evaluations. Byron David developed simulation assets and performed system identification. Chelsea Finn advised on the project, helped set the research direction and wrote parts of the paper. Chuyuan Fu developed deformable objects and experimented with sim-to-real techniques.</p>
<p>Keerthana Gopalakrishnan provided multiple infrastructure contributions that allowed for scalable learning of manipulation policies.</p>
<p>Karol Hausman co-led the project as well as developed SayCan, helped set the research direction, trained the underlying manipulation policies, and wrote the paper. Alex Herzog developed the teleoperation tools and implemented multiple infrastructure tools that allowed for continuous robot operation. Daniel Ho helped develop sim-to-real pipelines for manipulation policies.</p>
<p>Jasmine Hsu provided logging and monitoring infrastructure tools as well as data labeling pipelines. Julian Ibarz provided multiple contributions that enabled scaling learning algorithms for manipulation policies, and helped set the research direction. Brian Ichter initiated and led the SayCan algorithm, combined the manipulation and navigation skills, ran experiments for the paper, created the open-sourced SayCan Colab, and wrote the paper. Alex Irpan set up and led the autonomous data collection effort as well as verified the data collected by the robots, and wrote parts of the paper. Eric Jang helped set the research and team direction, managed the data for learning, developed the behavioral cloning manipulation policies, and wrote parts of the paper. Rosario Jauregui Ruano collected data on the robots and ran and supervised real-world evaluations.</p>
<p>Kyle Jeffrey collected data on the robots and ran and supervised real-world evaluations.</p>
<p>Sally Jesmonth was the program manager for the project.</p>
<p>Nikhil J Joshi developed a number of simulation and infrastructure tools that allowed to scale up simulation training. Ryan Julian developed multi-modal network architectures and trained manipulation policies. Dmitry Kalashnikov contributed infrastructure pieces that enabled training from logged data. Yuheng Kuang implemented the logging system for the project and designed and implemented the data labeling pipelines Kuang-Huei Lee made improvements to training algorithms for manipulation policies. Sergey Levine advised on the project, helped set the research direction, developed SayCan, and wrote parts of the paper. Yao Lu led and designed the robot learning infrastructure for the project providing most of the tools and improving manipulation policies. Linda Luu ran multiple evaluations, collected data and helped establish real-robot operations.</p>
<p>Carolina Parada advised on the project, managed the team, helped write the paper, and helped set the research direction.</p>
<p>Peter Pastor provided infrastructure tools that allowed for continuous robot operations.</p>
<p>Jornell Quiambao collected data on the robots and ran and supervised real-world evaluations. Kanishka Rao co-led the project, managed the team, helped set the research direction and contributed to training manipulation policies.</p>
<p>Jarek Rettinghouse collected data on the robots and ran and supervised real-world evaluations. Diego Reyes collected data on the robots and ran and supervised real-world evaluations. Pierre Sermanet set up the crowd compute rating pipeline. Nicolas Sievers provided simulation assets and environments used for simulation training. Clayton Tan collected data on the robots and ran and supervised real-world evaluations and helped establish real-robot operations. Alexander Toshev advised on the project, developed SayCan, helped write the paper, and helped set research direction. Vincent Vanhoucke advised on the project, managed the team, and helped write the paper. Fei Xia developed, implemented, and led on-robot SayCan, ran the experiments for the paper, created the demos, and wrote the paper. Ted Xiao led the scaling of manipulation skills, designed and developed learning from simulation for manipulation skills, and developed multi-modal network architectures. Peng Xu was the engineering lead for integrating manipulation and navigation and developed the underlying infrastructure for SayCan. Sichun Xu developed remote teleoperation tools that allowed scaling up data collection in simulation.</p>
<p>Mengyuan Yan implemented infrastructure and learning tools that allowed for learning manipulation policies from different data sources. Andy Zeng provided codebase and simulation assets for the open-source SayCan Colab.</p>
<p>B.3 Corresponding Emails:</p>
<p>{ichter,xiafei,karolhausman}@google.com</p>
<p>C RL and BC Policies</p>
<p>C.1 RL and BC Policy Architecture</p>
<p>The RL models use an architecture similar to MT-Opt [14], with slight changes to support natural language inputs (see Fig. 9 for the network diagram). The camera image is first processed by 7 con-volutional layers. The language instruction is embedded by the LLM, then concatenated with the robot action and non-image parts of the state, such as the gripper height. To support asynchronous control, inference occurs while the robot is still moving from the previous action. The model is given how much of the previous action is left to execute [87]. The conditioning input goes through FC layers, then tiled spatially and added to the conv. volume, before going through 11 more convolutional layers. The output is gated through a sigmoid, so the Q-value is always in [0, 1].</p>
<p>The BC models use an architecture similar to BC-Z [13] (see Fig. 10 for the network diagram). The language instruction is embedded by a universal sentence encoder [15], then used to FiLM condition a Resnet-18 based architecture. Unlike the RL model, we do not provide the previous action or gripper height, since this was not necessary to learn the policy. Multiple FC layers are applied to the final visual features, to output each action component (arm position, arm orientation, gripper, and the termination action). </p>
<p>C.2 RL and BC Policy Training</p>
<p>RL training. In addition to using demonstrations in the BC setup, we also learn languageconditioned value functions with RL. For this purpose, we complement our real robot fleet with a simulated version of the skills and environment. To reduce the simulation-to-real gap we transform robot images via RetinaGAN [16] to look more realistic while preserving genera object structure. In order to learn a language-conditioned RL policy, we utilize MT-Opt [14] in the Everyday Robots simulator using said simulation-to-real transfer. We bootstrap the performance of simulation policies by utilizing simulation demonstrations to provide initial successes, and then continuously improve the RL performance with online data collection in simulation. Standard image augmentations (random brightness and contrast) as well as random cropping were applied. The 640 x 512 input image was padded by 100 pixels left-right and 40 pixels top-down, then cropped back down to a 640 x 512 image, so as to allow for random spatial shifts without limiting the field of view. We use a network architecture similar to MT-Opt (shown in Fig. 9).</p>
<p>The RL model is trained using 16 TPUv3 chips and for about 100 hours, as well as a pool of 3000 CPU workers to collect episodes and another 3000 CPU workers to compute target Q-values. Computing target Q-values outside the TPU allows the TPU to be used solely for computing gradient updates. Episode rewards are sparse and always 0 or 1, so the Q-function is updated using a log loss. Models were trained using prioritized experience replay [88], where episode priority was tuned to encourage replay buffer training data for each skill to be close to 50% success. Episodes were BC training. We use 68000 teleoperated demonstrations that were collected over the course of 11 months using a fleet of 10 robots. The operators use VR headset controllers to track the motion of their hand, which is then mapped onto the robot's end-effector pose. The operators can also use a joystick to move the robot's base. We expand the demonstration dataset with 276000 autonomous episodes of learned policies which are later success-filtered and included in BC training, resulting in an additional 12000 successful episodes. To additionally process the data, we also ask the raters to mark the episodes as unsafe (i.e., if the robot collided with the environment), undesirable (i.e., if the robot perturbed objects that were not relevant to the skill) or infeasible (i.e., if the skill cannot be done or is already accomplished). If any of these conditions are met, the episode is excluded from training.</p>
<p>To learn language-conditioned BC policies at scale in the real world, we build on top of BC-Z [13] and use a similar policy-network architecture (shown in Fig. 10). It is trained with an MSE loss for the continuous action components, and a cross-entropy loss for the discrete action components. Each action component was weighted evenly. Standard image augmentations (random brightness and contrast) as well as random cropping were used. The 640 x 512 input image was padded by 100 pixels left-right and 40 pixels top-down, then cropped back down to a 640 x 512 image, so as to allow for random spatial shifts without limiting the field of view. For faster iteration speeds with negligible training performance reduction, image inputs were down sampled to half-size (256 x 320 images). Affordance value functions were trained with full-size images, since half-size images did not work as well when learning Q(s, a, π ). The BC model is trained using 16 TPUv3 chips and trained for about 27 hours.</p>
<p>C.3 RL and BC Policy Evaluations</p>
<p>In order to obtain the best possible manipulation capabilities for use in SayCan, we use a separate evaluation protocol for iterating on the RL and BC policies in the Mock Office Kitchen stations. Evaluations are divided by skill (pick up, knock over, place upright, open/close drawers, move object close to another one), and within each skill, 18-48 skills are sampled from a predetermined set of three objects. Object positions are randomized on each episode, with one or two objects serving as a distractor.</p>
<p>The episode ends when 50 actions have been taken or the policy samples a terminate action. A human operator supervises multiple robots performing evaluation and performs scene resets as needed, and records each episode as a success or failure. Models whose per-skill performance outperforms prior models are "graduated" to the same evaluation protocol in the real kitchen, and then integrated into SayCan. We found that despite the domain shift from Mock Office Kitchen stations to the actual kitchen counter and drawers, higher success rates on mock stations usually corresponded to higher success rates in the real kitchen setting. Figure 11 shows the development of the manipulation skills over time. It reports the per-skill success rate, the average success rate across all skills, and the number of instructions the policy was trained on. Over the course of the project, we increased the number of skills evaluated, from 1 instruction in April 2021 to hundreds of instructions at time of publication over the course of 366 real-world model evaluations.  To iteratively plan the next steps, the selected skill is added to the natural language query and the language model is queried again.</p>
<p>D.2 Policies and Affordance Functions</p>
<p>We also note a few practical considerations for setting up our affordance functions and policies. The flexibility of our approach allows us to mix and match policies and affordances from different methods. For the pick manipulation skills we use a single multi-task, language-conditioned policy, for the place manipulation skills we use a scripted policy with an affordance based on the gripper state, and for navigation policies we use a planning-based approach which is aware of the locations Figure 13: (A copy of Figure 3 here for clarity) Given a high-level instruction, SayCan combines probabilities from a language model (representing the probability that a skill is useful for the instruction) with the probabilities from a value function (representing the probability of successfully executing said skill) to select the skill to perform. This emits a skill that is both possible and useful. The process is repeated by appending the selected skill to the robot response and querying the models again, until the output step is to terminate.</p>
<p>where specific objects can be found and a distance measure. In order to avoid a situation where a skill is chosen but has already been performed or will have no effect, we set a cap for the affordances indicating that the skill has been completed and the reward received.</p>
<p>SayCan is capable of incorporating many different policies and affordance functions through its probability interface. Though in principle each type of skill has been trained with the pipeline described in Appendix C, to the success rates seen in Figure 11, we wish to show the generality of SayCan to different policies and affordance functions as well as the robustness of other functions (e.g. distance for navigation). Furthermore, some skills (such as the manipulation skill "move object near object" and "knock object over") are not naturally part of long-horizon tasks and thus we do not utilize them. Other skills, such as drawer opening, were not consistent enough for long-horizon planning and thus unused. However, we note that as skills become performant or as new skills are learned, it is straightforward to incorporate these skills by adding them as options for LLM scoring and as examples in the prompt. We use the following for each skill family:</p>
<p>• Pick. For pick we use the learned policies in Appendix C and Section 4 with actions from BC and value functions from RL trained on the same skill. In natural language these are specified as "pick up the object".</p>
<p>• Go to. Since the focus of this work is mainly on planning, we assume the location of objects are known. Thus any navigation skill maps to the coordinate of the object with a classical planning-based navigation stack. In natural language these are specified as "go to location" and "find object".</p>
<p>• Place. Though our manipulation policies have a "place upright" skill, this skill only applies to objects that have a canonical upright direction, e.g., a water bottle but not a bag of chips. One could also train a universal "place" command, but our current policies are trained in a setup-free environment and thus are not amenable to an initial pick. Thus to have a consistent place policy across all objects we use a classical motion planning policy. We use Cartesian space motion planning to plan a path from pre-grasp pose shown in Figure 4 to a gripper release pose. The robot executes that path until the gripper is in contact with a supporting surface, and then the gripper opens and releases the object. In natural language these are specified as "put down the object".</p>
<p>Recall that we wish to find the affordance function p(c π |s, π ), which indicates the probability of c-ompleting the skill with description π successfully from state s. Our learned policies produce a Qfunction, Q π (s, a). Given Q π (s, a) with action a and state s, value v(s) = max a Q π (s, a) is found through optimization via the cross entropy method, similar to MT-Opt. For brevity below we refer to the value functions by their skill-text description π as v π and the affordance function as p affordance π . Due to artifacts of training and each implementation, the value functions require calibration to be directly applied as a probability. The parameters used for calibration are determined empirically. Furthermore, SayCan enforces logic that if a skill that has already been completed and the reward received (e.g., navigating to the table the robot is already in front of) then it should not be performed.</p>
<p>• Pick. We find the trained value functions generally have a minimum value for when a skill is not possible and a maximum when the skill is successful and thus we normalize the value function to get a affordance function with
p affordance pick = clamp( v pick − v pick min v pick max − v pick min , 0, 1), where v pick max = 0.5, v pick min = 0.2.
• Go to. The affordance function of go to skills are based on the distance d (in meters) to the location. We use
p affordance goto = clamp( d goto max − d goto d goto max − d goto min
, 0, 1), where d goto max = 100, d goto min = 0.</p>
<p>• Place. We assume place is always possible, p affordance place = 1.0, since we find language is sufficient to understand place is only possible after a pick. In the future work having an affordance function module for place could further improve the performance of SayCan. • Terminate. We give terminate a small affordance value, to make sure the planning process terminates when there is no feasible skills to choose from. p affordance terminate = 0.1.</p>
<p>D.3 LLM Prompt</p>
<p>The LLM uses prompt engineering and a strict response structure to score skills. But, as SayCan as a whole requires affordances from a world embodiment, it is not straightforward to optimize this structure and tune parameters quickly. Thus we built a language-based simulator which, given a query and a solution sequence of skills, outputs affordances consistent with the query and solution.</p>
<p>It also generates consistent distractor affordances to ensure robustness. The simulator then verifies that SayCan recovers the correct solution and tests how confident SayCan is in the correct solutions.</p>
<p>In Table 5  We note here briefly a few lessons learned in prompt engineering and structuring the final prompt. Providing explicit numbers between steps (e.g., 1., 2., instead of combining skills with "and then" or other phrases) improved performance, as did breaking each step into a separate line (e.g. adding a "\n" between steps). Examples which overly include objects used in the actual planning tend to bias results to those objects (e.g., if every example is about apples then the apple scoring will be off in planning). Phrasing of the natural language names of skills and objects is important due to the auto-regressive nature of the LLM scoring -skills and objects should be naturally named and errors such as misspellings or mismatches in "a" vs "an" can be problematic. Notably, since user generated instructions are taken as given such fragility is not issues for the input, allowing a robustness to user queries. For our language model, PaLM [9], structuring the interaction as dialog (How would you -I would) was both more natural and performant. Although dialog is used as prompt, the model generalized to imperative sentences at deployment time. 0  10%  52%  1  64%  74%  2  68%  76%  4  82%  84%  8 80% 80% Full Prompt (17) 88% 88% • NL Verbs. Given a natural language query that replaces a verb (typically an action) with a synonym, can SayCan execute an appropriate sequence?</p>
<p>Num Examples Require Termination No Termination Required</p>
<p>• Structured Language. Given a structure language query that mirrors the NL Verbs and spells out the sequence of commands, how well can SayCan plan compared to NL Verbs? This acts as an ablation to see the performance loss of understanding a natural language query over an explicit solution.</p>
<p>• Embodiment. Given a query with different environment and robot states, can SayCan still execute at a high rate? This tests the performance of SayCan's affordance model and the LLM's ability to reason within it.</p>
<p>• Crowd-Sourced. These queries were crowd sourced from Mechanical Turk by giving humans a description of what occurred (e.g., an apple was moved in front of you) and asking them what they would ask the robot to do. They were also crowd sourced by asking humans in a real office kitchen to command the robot to perform tasks (given knowledge of the robot's abilities). This tests SayCan's performance with natural requests.</p>
<p>• Long-Horizon. These challenging queries require SayCan to reason over temporally extended instructions to investigate how well it scales to such regimes.</p>
<p>E.2 Ablating Over Language Model Size</p>
<p>To test how SayCan scales with LLM size, we ablate over varying PaLM [9] sizes (8B, 62B, and 540B parameter models) as well as the 137B parameter FLAN model [8]. The results are discussed in Section 5.1 and shown in Table 6.</p>
<p>E.3 Adding Skills: Drawer Manipulation</p>
<p>In order to support drawer manipulation we added another category of skills in SayCan.</p>
<p>• Drawer Manipulation. For drawer manipulation we use the learned policies in Appendix C and Section 4 with actions from BC and value functions from heuristics (If the robot is next to the drawer, all drawer tasks are possible). In natural language these are specified as "open the drawer", "close the drawer", "put the object in the drawer", "take the object out of the drawer".</p>
<p>A few drawer-specific prompts also need to be added to teach the robot how to chain the drawer skills together. The prompts are shown in Listing 2.  The results of the drawer tasks are shown in Table. 7. SayCan achieved an overall planning success rate of 100% and execution success rate of 33%. The main failure cases are manipulation failures, where the robot fails to open the drawer wide enough to put objects in it, or fails to completely close the drawer.  </p>
<p>Family</p>
<p>E.4 Chain of Thought Reasoning</p>
<p>Here we show the chain of thought prompt that generates the rollout in Table 4. (c) Language model terminates a long-horizon task prematurely. Figure 16: Failure cases. The planning success rate was 84%. Of the errors, 65% were a result of an LLM error and 35% were affordance errors.</p>
<p>Figure 2 :
2A value function module (a) is queried to form a value function space of action primitives based on the current observation. Visualizing "pick" value functions, in (b) "Pick up the red bull can" and "Pick up the apple" have high values because both objects are in the scene, while in (c) the robot is navigating an empty space, and thus none of the pick up actions receive high values.</p>
<p>Figures 12 and 2 focus on the LLM and VFS components.</p>
<p>Figure 4 :
4The experiments were performed in an office kitchen and a mock kitchen mirroring this setup, with 5 locations and 15 objects. The robot is a mobile manipulator with policies trained from an RGB observation.</p>
<p>Figure 6 :
6Visualization of PaLM-SayCan's decision making, where the top combined score chooses the correct skill.</p>
<p>Figure 8 :
8We have open sourced a Colab with a tabletop environment, a UR5 robot, and CLIPort-based policy.</p>
<p>Figure 9 :
9Network architecture in RL policy</p>
<p>Figure 10 :
10Network architecture in BC policy sampled proportionally to their priority, defined as 1 + 10 · |p − 0.5|, where p is the average success rate of episodes in the replay buffer.</p>
<p>Figure 11 :Figure 12
1112Per-skill evaluation performance of the best policies and number of skills over the duration of the project. The performance as well as the number of skills that the robots are able to handle grow over time due to the continuous data collection efforts as well as improving the policy training algorithms. shows scoring approach used and prompt engineering for the LLM side of SayCan.Figure 2shows how robotic affordances are computed with value functions and real value function computations at different states. These two components are combined to form SayCan, as detailed in Algorithm 1 and inFigure 13.</p>
<p>Figure 12 :
12A scoring language model is queried with a prompt-engineered context of examples and the highlevel instruction to execute and outputs the probability of each skill being selected.</p>
<p>Figure 14 :
14Visualization of the decision making process of SayCan shows its interpretability and successful temporally extended execution, where the top combined score chooses the correct skill.(a) The affordance model fails to identify either bag of chips as pickable, though the language model approaches the counter twice.(b) The affordance model fails to identify the sponge as pickable.</p>
<p>details examples for each family.Instruction Family 
Num Explanation 
Example Instruction 
NL Single Primitive 
15 </p>
<p>NL queries for a single primitive 
Let go of the coke can </p>
<p>NL Nouns 
15 </p>
<p>NL queries focused on abstract nouns 
Bring me a fruit </p>
<p>NL Verbs 
15 </p>
<p>NL queries focused on abstract verbs 
Restock the rice chips on the far counter </p>
<p>Structured Language 15 </p>
<p>Structured language queries, mirror NL Verbs 
Move the rice chips to the far counter. </p>
<p>Embodiment 
11 </p>
<p>Queries to test SayCan's understanding of the </p>
<p>current state of the environment and robot </p>
<p>Put the coke on the counter. (starting </p>
<p>from different completion stages) </p>
<p>Crowd-Sourced 
15 </p>
<p>Queries in unstructured formats 
My favorite drink is redbull, bring one </p>
<p>Long-Horizon 
15 </p>
<p>Long-horizon queries that require many steps </p>
<p>of reasoning </p>
<p>I spilled my coke on the table, throw it </p>
<p>away and bring me something to clean </p>
<p>Table 1 :
1List of instruction family definitions: We evaluate the algorithm on 101 instructions. We group the instructions into different families, with each family focusing on testing one aspect of the proposed method.</p>
<p>No VF Gen. BC NL BC USEMock Kitchen </p>
<p>Kitchen 
No Affordance 
No LLM 
PaLM-
SayCan </p>
<p>PaLM-
SayCan </p>
<p>PaLM-
SayCan </p>
<p>PaLM-
SayCan </p>
<p>Family 
Num Plan 
Execute Plan 
Execute Plan 
Plan 
Execute Execute 
NL Single 
15 
100% 
100% 
93% 
87% 
73% 
87% 
0% 
60% 
NL Nouns 
15 
67% 
47% 
60% 
40% 
53% 
53% 
0% 
0% 
NL Verbs 
15 
100% 
93% 
93% 
73% 
87% 
93% 
0% 
0% 
Structured 
15 
93% 
87% 
93% 
47% 
93% 
100% 0% 
0% 
Embodiment 
11 
64% 
55% 
64% 
55% 
18% 
36% 
0% 
0% 
Crowd Sourced 15 
87% 
87% 
73% 
60% 
67% 
80% 
0% 
0% 
Long-Horizon 
15 
73% 
47% 
73% 
47% 
67% 
60% 
0% 
0% 
Total 
101 
84% 
74% 
81% 
60% 
67% 
74% 
0% 
9% </p>
<p>are exciting avenues for future research. v1 → v2: Added PaLM results. Added study about new capabilities (drawer manipulation, chain of thought prompting, multilingual instructions). Added an ablation study of language model size. Added an open-source version of SayCan on a simulated tabletop environment. Improved readability. Designed or implemented SayCan: Karol Hausman, Brian Ichter, Sergey Levine, Alexander Toshev, and Fei Xia. • Managed or advised on the project: Chelsea Finn, Karol Hausman, Eric Jang, Sally Jesmonth, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Alexander Toshev, and Vincent Vanhoucke.A Version Control </p>
<p>B Contributions </p>
<p>B.1 By Type </p>
<p>• Designed and built distributed robot learning infrastructure: Michael Ahn, Anthony 
Brohan, Noah Brown, Yevgen Chebotar, Byron David, Chuyuan Fu, Keerthana Gopalakr-
ishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Alex Irpan, 
Eric Jang, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Yao Lu, Pe-
ter Pastor, Kanishka Rao, Nicolas Sievers, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and 
Mengyuan Yan. </p>
<p>• Designed, implemented, or trained the underlying manipulation policies: Yevgen 
Chebotar, Keerthana Gopalakrishnan, Karol Hausman, Julian Ibarz, Alex Irpan, Eric Jang, 
Nikhil Joshi, Ryan Julian, Kuang-Huei Lee, Yao Lu, Kanishka Rao, and Ted Xiao. </p>
<p>• Designed or implemented the data generation and curation or collected data: Noah 
Brown, Omar Cortes, Jasmine Hsu, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle 
Jeffrey, Linda Luu, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, 
Pierre Sermanet, Clayton Tan, and Sichun Xu. </p>
<p>• • Ran evaluations or experiments: Noah Brown, Omar Cortes, Brian Ichter, Rosario Jau-
regui Ruano, Kyle Jeffrey, Linda Luu, Jornell Quiambao, Jarek Rettinghouse, Diego Reyes, 
Clayton Tan, and Fei Xia. </p>
<p>• Scaled simulation infrastructure: Nikhil J Joshi, Yao Lu, Kanishka Rao, and Ted Xiao. </p>
<p>• Wrote the paper: Chelsea Finn, Karol Hausman, Brian Ichter, Alex Irpan, Sergey Levine, 
Fei Xia, and Ted Xiao. </p>
<p>we test the effect of the number of examples in the prompt on the planning success rate in the language-based simulator (over 50 demonstrative instructions). We show a success rate with and without requiring the plan to terminate; without examples we found the LLM was unlikely to issue a "done" phase. With no examples SayCan is able to successfully plan 54% without the done condition, but only 10% with the done condition. Though it makes mistakes, clearly some information is already imbued within the language model. It is able to correctly solve "Can I have a redbull please?" and "Move the chips bag from the table to the counter.". With only one example the LLM quickly improves in both planning rates, though still fails to terminate the plan occasionally. After only four examples the LLM is performant, planning 82% of the queries correctly, though the remaining errors are largely within a single instruction family: Long-Horizon. Finally, the prompt used in this work, Listing 1, involved 17 examples and recovered 88% of the solutions correctly.</p>
<p>Table 5 :
5Number of prompt engineering examples versus planning rate (when requiring the plan to know when to terminate, e.g., issue a "done" command, or not).Listing 1: Prompt in SayCan.</p>
<p>Num PaLM 540B[9] PaLM 62B PaLM 8B FLAN 137B[8] NL Single 
15 
87% 
73% 
20% 
40% 
NL Nouns 
15 
53% 
47% 
20% 
40% 
NL Verbs 
15 
93% 
100% 
60% 
87% 
Structured 
15 
100% 
100% 
67% 
73% 
Embodiment 
11 
36% 
27% 
27% 
0% 
Crowd Sourced 
15 
80% 
73% 
47% 
47% 
Long-Horizon 
15 
60% 
73% 
20% 
0% 
Total 
101 
74% 
72% 
38% 
43% </p>
<p>Table 6 :
6Ablations over the size of the LLM. Compared only with the generative outputs (no value function) with USE embeddings[15].Listing 2: Prompt for drawer tasks in SayCan. They can be used standalone, or appended to the main prompt in Listing 1. : open the drawer Robot: 1. go to the drawers, 2. open the drawer, 3. done. Human: restock orange juice into the drawer Robot: 1. go to the drawers, 2. open the drawer, 3. put orange juice in the drawer, 4. close the drawer, 5. done. Human: restock two bottles of orange juice into the drawer Robot: 1. go to the drawers, 2. open the drawer, 3. put orange juice in the drawer, 4. put orange juice in the drawer, 5. close the drawer, 6. done.Human</p>
<p>Table 7 :
7Plan and execution success rate of drawer tasks
https://everydayrobots.com/
AcknowledgmentsThe authors would like to thank Fred Alcober, Yunfei Bai, Matt Bennice, Maarten Bosma, Justin Boyd, Bill Byrne, Kendra Byrne, Noah Constant, Pete Florence, Laura Graesser, Rico Jonschkowski, Daniel Kappler, Hugo Larochelle, Benjamin Lee, Adrian Li, Maysam Moussalem, Suraj Nair, Jane Park, Evan Rapoport, Krista Reymann, Jeff Seto, Dhruv Shah, Ian Storz, Razvan Surdulescu, Tom Small, and Vincent Zhao for their help and support in various aspects of the project.Robot: Hi there, I'm a robot operating in an office kitchen. Robot: You can ask me to do various tasks and I'll tell you the sequence of actions I would do to accomplish your task. Human: How would you hold the snickers? Robot: 1. pick up the snickers, 2. done. Human: How would you put the trailmix here? Robot: 1. put down the trailmix, 2. done. Human: How would you put a water bottle and an oatmeal next to the microwave? Robot: 1. find a water bottle, 2. pick up the water bottle, 3. go to microwave, 4. put down the water bottle, 5. find an oatmeal, 6. pick up the oatmeal, 7. go to the microwave,8. put down the oatmeal,9.done. Human: How would you put a grapefruit from the table into the bowl? Robot: 1. find a grapefruit, 2. pick up the grapefruit, 3. go to the bowl, 4. put down the grapefruit,5.done. Human: How would you get a sponge from the counter and put it in the sink? Robot: 1. find a sponge, 2. pick up the sponge, 3. go to the sink, 4. put down the sponge,5E ExperimentsE.1 TasksBelow we include every instruction run, which environment it was run in, and its planning and execution success rate.Table 5shows all instructions as broken down by instruction family, listed below and initially defined in Section 5Table 1.• Natural Language (NL) Single Primitive. Given a natural language command corresponding to performing a single primitive, can SayCan recover that primitive skill and terminate? • NL Noun. Given a natural language query that replaces a noun (typically an object or location) with a synonym, can SayCan execute the appropriate sequence?Listing 3: Chain-of-thought prompt in SayCan.Robot: Hi there, I'm a robot operating in an office kitchen. You can ask me to do various tasks and I'll tell you the sequence of actions I would do to accomplish your task. The following objects are in the scene: 7up, apple, tea, multigrain chips, kettle chips, jalapeno chips, rice chips, coke, grapefruit soda, pepsi, redbull, energy bar, lime soda, sponge, and water bottle. The following locations are in the scene: close counter, far counter, table, you, trash, bowl. Human: Hold the sponge Explanation: The user has asked for a single skill, I will just pick up the sponge. Robot: 1. pick up the sponge, 2. done. Human: Put the kettle chips here Explanation: The user has asked for a type of chips, I will bring those to them. Robot: 1. put down the kettle chips, 2. done. Human: Move the grapefruit drink from the table to the close counter Explanation: The user has asked me to move the grapefruit drink to the counter. Robot: 1. find a grapefruit soda, 2. pick up the grapefruit soda, 3. go to the counter, 4. put down the grapefruit soda, 5. done. Human: Bring me some snacks Explanation: The user has asked for snacks, I will bring jalapeno chips and an apple. Robot: 1. find a jalapeno chips, 2. pick up the jalapeno chips, 3. bring it to you, 4. put down the jalapeno chips, 5. find an apple, 6. pick up the apple, 7. bring it to you, 8. put down the apple, 9. done. Human: Bring me something that isn't a fruit Explanation: The user has asked for something food that isn't an fruit, I will bring an energy bar. Robot: 1. find an energy bar, 2. pick up the energy bar, 3. bring it to you, 4. put down the energy bar, 5. done. Human: Put the rice chips in the bowl and then move the tea to theE.5 Multilingual QueriesSince the underlying LM we used[9]has been trained on multilingual corpora, SayCan can handle multilingual queries out of the box. The results of SayCan on multilingual queries are summarized inTable.8, and there is almost no performance drop on planning success rate when changing the queries from English to Chinese, French and Spanish.InstructionPlan rate bring me a can of cokeE.6 Additional ResultsAdditional results are shown inFigure 14andFigure 17and some failure cases inFigure 16. For videos of the rollouts, please visit the our website https://say-can.github.io (a) In this long-horizon task, the language model gives high score to the two sodas. After the coke is delivered, the language model scores pepsi higher. The affordance rating overcomes potential early termination after the first can has been delivered.    (a) In this long-horizon task, the language model gives high score to the two sodas. After the coke is delivered, the language model scores pepsi higher. The affordance rating overcomes potential early termination after the first can has been delivered. (b) In this task, the model completes a 9-step plan. It narrowly avoids an early termination at step 5.Figure 17: Long horizon sequences, see the video on our website say-can.github.io for more.
Climbing towards nlu: On meaning, form, and understanding in the age of data. E M Bender, A Koller, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsE. M. Bender and A. Koller. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics, 2020.</p>
<p>Polosukhin. Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I , 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo- sukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of Machine Learning Research. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, pages 1-67, 2019.</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 33T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, arXiv:2112.11446arXiv preprintJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>R Thoppilan, D Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239Language models for dialog applications. arXiv preprintR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652arXiv preprintJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, A. Chowdhery, S. Narang, J. Devlin, et al. Palm: Scaling language modeling with path- ways. 2022. URL https://storage.googleapis.com/pathways-language-model/ PaLM-paper.pdf.</p>
<p>The theory of affordances. The Ecological Approach to Visual Perception. J J Gibson, J. J. Gibson. The theory of affordances. The Ecological Approach to Visual Perception, 1977.</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, PreprintL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Preprint, 2022.</p>
<p>Value function spaces: Skill-centric state abstractions for long-horizon reasoning. ICLR, 2022. D Shah, P Xu, Y Lu, T Xiao, A Toshev, S Levine, B Ichter, D. Shah, P. Xu, Y. Lu, T. Xiao, A. Toshev, S. Levine, and B. Ichter. Value function spaces: Skill-centric state abstractions for long-horizon reasoning. ICLR, 2022. URL https://arxiv.org/pdf/2111.03189.pdf.</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, Conference on Robot Learning. PMLRE. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learn- ing, pages 991-1002. PMLR, 2021.</p>
<p>D Kalashnikov, J Varley, Y Chebotar, B Swanson, R Jonschkowski, C Finn, S Levine, K Hausman, Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv. D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv, 2021.</p>
<p>Universal sentence encoder. D Cer, Y Yang, S Kong, N Hua, N Limtiaco, R S John, N Constant, M Guajardo-Cespedes, S Yuan, C Tar, arXiv:1803.11175arXiv preprintD. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. Guajardo- Cespedes, S. Yuan, C. Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175, 2018.</p>
<p>Retinagan: An object-aware approach to sim-to-real transfer. D Ho, K Rao, Z Xu, E Jang, M Khansari, Y Bai, 2021 IEEE International Conference on Robotics and Automation (ICRA). D. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y. Bai. Retinagan: An object-aware ap- proach to sim-to-real transfer. 2021 IEEE International Conference on Robotics and Automa- tion (ICRA), pages 10920-10926, 2021.</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionM. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740-10749, 2020.</p>
<p>Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. S Srivastava, C Li, M Lingelbach, R Martín-Martín, F Xia, K E Vainio, Z Lian, C Gokmen, S Buch, K Liu, Conference on Robot Learning. PMLRS. Srivastava, C. Li, M. Lingelbach, R. Martín-Martín, F. Xia, K. E. Vainio, Z. Lian, C. Gok- men, S. Buch, K. Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pages 477-490. PMLR, 2022.</p>
<p>Understanding by understanding not: Modeling negation in language models. A Hosseini, S Reddy, D Bahdanau, R D Hjelm, A Sordoni, A Courville, arXiv:2105.03519arXiv preprintA. Hosseini, S. Reddy, D. Bahdanau, R. D. Hjelm, A. Sordoni, and A. Courville. Un- derstanding by understanding not: Modeling negation in language models. arXiv preprint arXiv:2105.03519, 2021.</p>
<p>Languageconditioned imitation learning for robot manipulation tasks. ArXiv, abs. S Stepputtis, J Campbell, M Phielipp, S Lee, C Baral, H B Amor, S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor. Language- conditioned imitation learning for robot manipulation tasks. ArXiv, abs/2010.12083, 2020.</p>
<p>Learning languageconditioned robot behavior from offline data and crowd-sourced annotation. S Nair, E Mitchell, K Chen, B Ichter, S Savarese, C Finn, Conference on Robot Learning. PMLRS. Nair, E. Mitchell, K. Chen, B. Ichter, S. Savarese, and C. Finn. Learning language- conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pages 1303-1315. PMLR, 2021.</p>
<p>Grounding language in play. C Lynch, P Sermanet, C. Lynch and P. Sermanet. Grounding language in play. 2020.</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, arXiv:2201.07207arXiv preprintW. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Ex- tracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903arXiv preprintJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.05608arXiv preprintW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu- lation. In Conference on Robot Learning, 2022.</p>
<p>Open-vocabulary object detection via vision and language knowledge distillation. X Gu, T.-Y Lin, W Kuo, Y Cui, arXiv:2104.13921arXiv preprintX. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and lan- guage knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.</p>
<p>Grounding language in perception. J M Siskind, Artificial Intelligence Review. J. M. Siskind. Grounding language in perception. Artificial Intelligence Review, 1994.</p>
<p>Understanding natural language. T Winograd, Cognitive psychology. T. Winograd. Understanding natural language. Cognitive psychology, 1972.</p>
<p>Videobert: A joint model for video and language representation learning. C Sun, A Myers, C Vondrick, K Murphy, C Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionC. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.</p>
<p>Visualbert: A simple and performant baseline for vision and language. L H Li, M Yatskar, D Yin, C.-J Hsieh, K.-W Chang, arXiv:1908.03557arXiv preprintL. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. Visualbert: A simple and perfor- mant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Advances in neural information processing systems. J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic repre- sentations for vision-and-language tasks. Advances in neural information processing systems, 2019.</p>
<p>Merlot: Multimodal neural script knowledge models. R Zellers, X Lu, J Hessel, Y Yu, J S Park, J Cao, A Farhadi, Y Choi, Advances in Neural Information Processing Systems. R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi. Merlot: Multi- modal neural script knowledge models. Advances in Neural Information Processing Systems, 2021.</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi- sion. In International Conference on Machine Learning, 2021.</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. A Suglia, Q Gao, J Thomason, G Thattai, G Sukhatme, arXiv:2108.04927arXiv preprintA. Suglia, Q. Gao, J. Thomason, G. Thattai, and G. Sukhatme. Embodied bert: A transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021.</p>
<p>Episodic transformer for vision-and-language navigation. A Pashevich, C Schmid, C Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionA. Pashevich, C. Schmid, and C. Sun. Episodic transformer for vision-and-language naviga- tion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.</p>
<p>Skill induction and planning with latent language. P Sharma, A Torralba, J Andreas, arXiv:2110.01517arXiv preprintP. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.</p>
<p>Human instruction-following with deep reinforcement learning via transfer-learning from text. F Hill, S Mokra, N Wong, T Harley, arXiv:2005.09382arXiv preprintF. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep reinforce- ment learning via transfer-learning from text. arXiv preprint arXiv:2005.09382, 2020.</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. V Blukis, C Paxton, D Fox, A Garg, Y Artzi, Conference on Robot Learning. V. Blukis, C. Paxton, D. Fox, A. Garg, and Y. Artzi. A persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, 2022.</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, arXiv:2203.12601arXiv preprintS. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual represen- tation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.</p>
<p>Grounding language to autonomously-acquired skills via goal generation. A Akakzia, C Colas, P.-Y Oudeyer, M Chetouani, O Sigaud, arXiv:2006.07185arXiv preprintA. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, and O. Sigaud. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020.</p>
<p>Piglet: Language grounding through neuro-symbolic interaction in a 3d world. R Zellers, A Holtzman, M Peters, R Mottaghi, A Kembhavi, A Farhadi, Y Choi, arXiv:2106.00188arXiv preprintR. Zellers, A. Holtzman, M. Peters, R. Mottaghi, A. Kembhavi, A. Farhadi, and Y. Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. arXiv preprint arXiv:2106.00188, 2021.</p>
<p>A data-driven approach for learning to control computers. P C Humphreys, D Raposo, T Pohlen, G Thornton, R Chhaparia, A Muldal, J Abramson, P Georgiev, A Goldin, A Santoro, arXiv:2202.08137arXiv preprintP. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson, P. Georgiev, A. Goldin, A. Santoro, et al. A data-driven approach for learning to control computers. arXiv preprint arXiv:2202.08137, 2022.</p>
<p>Can wikipedia help offline reinforcement learning. M Reid, Y Yamada, S S Gu, arXiv:2201.12122arXiv preprintM. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help offline reinforcement learning. arXiv preprint arXiv:2201.12122, 2022.</p>
<p>Pretrained language models for interactive decision-making. S Li, X Puig, Y Du, C Wang, E Akyurek, A Torralba, J Andreas, I Mordatch, arXiv:2202.01771arXiv preprintS. Li, X. Puig, Y. Du, C. Wang, E. Akyurek, A. Torralba, J. Andreas, and I. Mordatch. Pre- trained language models for interactive decision-making. arXiv preprint arXiv:2202.01771, 2022.</p>
<p>Walk the talk: Connecting language, knowledge, and action in route instructions. M Macmahon, B Stankiewicz, B Kuipers, 01M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowl- edge, and action in route instructions. 01 2006.</p>
<p>Toward understanding natural language directions. T Kollar, S Tellex, D Roy, N Roy, HRI 2010. T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In HRI 2010, 2010.</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, N Roy, 2S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. volume 2, 01 2011.</p>
<p>A survey of reinforcement learning informed by natural language. J Luketina, N Nardelli, G Farquhar, J N Foerster, J Andreas, E Grefenstette, S Whiteson, T Rocktäschel, IJCAI. J. Luketina, N. Nardelli, G. Farquhar, J. N. Foerster, J. Andreas, E. Grefenstette, S. Whiteson, and T. Rocktäschel. A survey of reinforcement learning informed by natural language. In IJCAI, 2019.</p>
<p>Robots that use language. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Robotics, and Autonomous Systems. Annual Review of ControlS. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems, 2020.</p>
<p>Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. H Mei, M Bansal, M R Walter, AAAI. H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI, 2016.</p>
<p>Mapping instructions and visual observations to actions with reinforcement learning. D K Misra, J Langford, Y Artzi, D. K. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017.</p>
<p>Grounded language learning in a. K Hermann, F Hill, S Green, F Wang, R Faulkner, H Soyer, D Szepesvari, W Czarnecki, M Jaderberg, D Teplyashin, M Wainwright, C Apps, D Hassabis, P Blunsom, simulated 3d world. ArXiv, abs/1706.06551K. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, M. Wainwright, C. Apps, D. Hassabis, and P. Blunsom. Grounded language learning in a simulated 3d world. ArXiv, abs/1706.06551, 2017.</p>
<p>Language as an abstraction for hierarchical deep reinforcement learning. Y Jiang, S Gu, K Murphy, C Finn, NeurIPS. Y. Jiang, S. Gu, K. Murphy, and C. Finn. Language as an abstraction for hierarchical deep reinforcement learning. In NeurIPS, 2019.</p>
<p>Self-educated language agent with hindsight experience replay for instruction following. G Cideron, M Seurin, F Strub, O Pietquin, abs/1910.09451ArXiv. G. Cideron, M. Seurin, F. Strub, and O. Pietquin. Self-educated language agent with hindsight experience replay for instruction following. ArXiv, abs/1910.09451, 2019.</p>
<p>Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. ArXiv, abs. P Goyal, S Niekum, R Mooney, P. Goyal, S. Niekum, and R. Mooney. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. ArXiv, abs/2007.15543, 2020.</p>
<p>Zero-shot task generalization with multi-task deep reinforcement learning. J Oh, S Singh, H Lee, P Kohli, abs/1706.05064ArXiv. J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. ArXiv, abs/1706.05064, 2017.</p>
<p>Modular multitask reinforcement learning with policy sketches. J Andreas, D Klein, S Levine, abs/1611.01796ArXiv. J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy sketches. ArXiv, abs/1611.01796, 2017.</p>
<p>Hierarchical planning in the now. L P Kaelbling, T Lozano-Pérez, Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence. L. P. Kaelbling and T. Lozano-Pérez. Hierarchical planning in the now. In Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.</p>
<p>Combined task and motion planning through an extensible planner-independent interface layer. S Srivastava, E Fang, L Riano, R Chitnis, S Russell, P Abbeel, 2014 IEEE international conference on robotics and automation (ICRA). S. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel. Combined task and motion planning through an extensible planner-independent interface layer. In 2014 IEEE international conference on robotics and automation (ICRA), 2014.</p>
<p>Strips: A new approach to the application of theorem proving to problem solving. R E Fikes, N J Nilsson, Artificial intelligence. R. E. Fikes and N. J. Nilsson. Strips: A new approach to the application of theorem proving to problem solving. Artificial intelligence, 1971.</p>
<p>A structure for plans and behavior. E D Sacerdoti, SRI International. Menlo Park California Artificial Intelligence CenterTechnical reportE. D. Sacerdoti. A structure for plans and behavior. Technical report, SRI International, Menlo Park California Artificial Intelligence Center, 1975.</p>
<p>Shop: Simple hierarchical ordered planner. D Nau, Y Cao, A Lotem, H Munoz-Avila, D. Nau, Y. Cao, A. Lotem, and H. Munoz-Avila. Shop: Simple hierarchical ordered planner. 1999.</p>
<p>Planning algorithms. S M Lavalle, S. M. LaValle. Planning algorithms. 2006.</p>
<p>Logic-geometric programming: An optimization-based approach to combined task and motion planning. M Toussaint, Twenty-Fourth International Joint Conference on Artificial Intelligence. M. Toussaint. Logic-geometric programming: An optimization-based approach to combined task and motion planning. In Twenty-Fourth International Joint Conference on Artificial Intel- ligence, 2015.</p>
<p>Differentiable physics and stable modes for tool-use and manipulation planning. M A Toussaint, K R Allen, K A Smith, J B Tenenbaum, M. A. Toussaint, K. R. Allen, K. A. Smith, and J. B. Tenenbaum. Differentiable physics and stable modes for tool-use and manipulation planning. 2018.</p>
<p>Neural task programming: Learning to generalize across hierarchical tasks. D Xu, S Nair, Y Zhu, J Gao, A Garg, L Fei-Fei, S Savarese, 2018 IEEE International Conference on Robotics and Automation (ICRA). D. Xu, S. Nair, Y. Zhu, J. Gao, A. Garg, L. Fei-Fei, and S. Savarese. Neural task programming: Learning to generalize across hierarchical tasks. In 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018.</p>
<p>Regression planning networks. D Xu, R Martín-Martín, D.-A Huang, Y Zhu, S Savarese, L F Fei-Fei, Advances in Neural Information Processing Systems. 32D. Xu, R. Martín-Martín, D.-A. Huang, Y. Zhu, S. Savarese, and L. F. Fei-Fei. Regression planning networks. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Neural task graphs: Generalizing to unseen tasks from a single video demonstration. D.-A Huang, S Nair, D Xu, Y Zhu, A Garg, L Fei-Fei, S Savarese, J C Niebles, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionD.-A. Huang, S. Nair, D. Xu, Y. Zhu, A. Garg, L. Fei-Fei, S. Savarese, and J. C. Niebles. Neural task graphs: Generalizing to unseen tasks from a single video demonstration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.</p>
<p>Search on the replay buffer: Bridging planning and reinforcement learning. B Eysenbach, R R Salakhutdinov, S Levine, Advances in Neural Information Processing Systems. B. Eysenbach, R. R. Salakhutdinov, and S. Levine. Search on the replay buffer: Bridging planning and reinforcement learning. Advances in Neural Information Processing Systems, 2019.</p>
<p>N Savinov, A Dosovitskiy, V Koltun, arXiv:1803.00653Semi-parametric topological memory for navigation. arXiv preprintN. Savinov, A. Dosovitskiy, and V. Koltun. Semi-parametric topological memory for naviga- tion. arXiv preprint arXiv:1803.00653, 2018.</p>
<p>Broadly-exploring, local-policy trees for long-horizon task planning. B Ichter, P Sermanet, C Lynch, Conference on Robot Learning (CoRL). 2021B. Ichter, P. Sermanet, and C. Lynch. Broadly-exploring, local-policy trees for long-horizon task planning. Conference on Robot Learning (CoRL), 2021.</p>
<p>A joint model of language and perception for grounded attribute learning. C Matuszek, N Fitzgerald, L Zettlemoyer, L Bo, D Fox, arXiv:1206.6423arXiv preprintC. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. A joint model of language and perception for grounded attribute learning. arXiv preprint arXiv:1206.6423, 2012.</p>
<p>T Silver, R Chitnis, N Kumar, W Mcclinton, T Lozano-Perez, L P Kaelbling, J , arXiv:2203.09634Tenenbaum. Inventing relational state and action abstractions for effective and efficient bilevel planning. arXiv preprintT. Silver, R. Chitnis, N. Kumar, W. McClinton, T. Lozano-Perez, L. P. Kaelbling, and J. Tenen- baum. Inventing relational state and action abstractions for effective and efficient bilevel plan- ning. arXiv preprint arXiv:2203.09634, 2022.</p>
<p>Online replanning in belief space for partially observable task and motion problems. C R Garrett, C Paxton, T Lozano-Pérez, L P Kaelbling, D Fox, 2020 IEEE International Conference on Robotics and Automation (ICRA). 2020C. R. Garrett, C. Paxton, T. Lozano-Pérez, L. P. Kaelbling, and D. Fox. Online replanning in belief space for partially observable task and motion problems. In 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020.</p>
<p>Visual semantic planning using deep successor representations. Y Zhu, D Gordon, E Kolve, D Fox, L Fei-Fei, A Gupta, R Mottaghi, A Farhadi, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionY. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi, and A. Farhadi. Visual semantic planning using deep successor representations. In Proceedings of the IEEE international conference on computer vision, 2017.</p>
<p>Tell me dave: Context-sensitive grounding of natural language to manipulation instructions. D K Misra, J Sung, K Lee, A Saxena, The International Journal of Robotics Research. D. K. Misra, J. Sung, K. Lee, and A. Saxena. Tell me dave: Context-sensitive grounding of natural language to manipulation instructions. The International Journal of Robotics Research, 2016.</p>
<p>Example-driven model-based reinforcement learning for solving long-horizon visuomotor tasks. B Wu, S Nair, L Fei-Fei, C Finn, 5th Annual Conference on Robot Learning. B. Wu, S. Nair, L. Fei-Fei, and C. Finn. Example-driven model-based reinforcement learning for solving long-horizon visuomotor tasks. In 5th Annual Conference on Robot Learning, 2021.</p>
<p>Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. S Nair, C Finn, abs/1909.05829ArXiv. S. Nair and C. Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. ArXiv, abs/1909.05829, 2020.</p>
<p>Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation. F Xia, C Li, R Martín-Martín, O Litany, A Toshev, S Savarese, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021F. Xia, C. Li, R. Martín-Martín, O. Litany, A. Toshev, and S. Savarese. Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation. In 2021 IEEE Interna- tional Conference on Robotics and Automation (ICRA), 2021.</p>
<p>Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators. C Li, F Xia, R Martin-Martin, S Savarese, Conference on Robot Learning. C. Li, F. Xia, R. Martin-Martin, and S. Savarese. Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators. In Conference on Robot Learning, 2020.</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and TransparencyE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021.</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Actionable models: Unsupervised offline reinforcement learning of robotic skills. Y Chebotar, K Hausman, Y Lu, T Xiao, D Kalashnikov, J Varley, A Irpan, B Eysenbach, R C Julian, C Finn, S Levine, abs/2104.07749ArXiv. Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan, B. Eysenbach, R. C. Julian, C. Finn, and S. Levine. Actionable models: Unsupervised offline reinforcement learning of robotic skills. ArXiv, abs/2104.07749, 2021.</p>
<p>Asking for help using inverse semantics. S Tellex, R Knepper, A Li, D Rus, N Roy, S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy. Asking for help using inverse semantics. 2014.</p>
<p>S I Wang, P Liang, C D Manning, arXiv:1606.02447Learning language games through interaction. arXiv preprintS. I. Wang, P. Liang, and C. D. Manning. Learning language games through interaction. arXiv preprint arXiv:1606.02447, 2016.</p>
<p>Thinking while moving: Deep reinforcement learning with concurrent control. T Xiao, E Jang, D Kalashnikov, S Levine, J Ibarz, K Hausman, A Herzog, International Conference on Learning Representations. T. Xiao, E. Jang, D. Kalashnikov, S. Levine, J. Ibarz, K. Hausman, and A. Herzog. Thinking while moving: Deep reinforcement learning with concurrent control. In International Confer- ence on Learning Representations, 2019.</p>
<p>Prioritized experience replay. T Schaul, J Quan, I Antonoglou, D Silver, ICLR. T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In ICLR, 2016.</p>            </div>
        </div>

    </div>
</body>
</html>
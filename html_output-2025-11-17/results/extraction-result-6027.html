<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6027 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6027</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6027</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-258449651</p>
                <p><strong>Paper Title:</strong> Language Models for Multimessenger Astronomy</p>
                <p><strong>Paper Abstract:</strong> : With the increasing reliance of astronomy on multi-instrument and multi-messenger observations for detecting transient phenomena, communication among astronomers has become more critical. Apart from automatic prompt follow-up observations, short reports, e.g., GCN circulars and ATels, provide essential human-written interpretations and discussions of observations. These reports lack a deﬁned format, unlike machine-readable messages, making it challenging to associate phenomena with speciﬁc objects or coordinates in the sky. This paper examines the use of large language models (LLMs)—machine learning models with billions of trainable parameters or more that are trained on text—such as InstructGPT-3 and open-source Flan-T5-XXL for extracting information from astronomical reports. The study investigates the zero-shot and few-shot learning capabilities of LLMs and demonstrates various techniques to improve the accuracy of predictions. The study shows the importance of careful prompt engineering while working with LLMs, as demonstrated through edge case examples. The study’s ﬁndings have signiﬁcant implications for the development of data-driven</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6027.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6027.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT (text-davinci-003/davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (OpenAI instruction‑tuned GPT-3 family; e.g., text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction‑fine-tuned GPT-3 model family optimized to follow natural‑language instructions via human feedback and RL fine-tuning; used here for zero‑ and few‑shot extraction of named entities from astronomical reports.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training language models to follow instructions with human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Zero-/few-shot extraction with InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use of InstructGPT as a general-purpose LLM to extract structured named entities (event type, event ID/object name, physical phenomenon/object type) from freeform astronomical telegrams via carefully engineered prompts or via fine‑tuning; sampling multiple completions and ranking them.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>OpenAI GPT-3 family: text-davinci-003 / davinci (InstructGPT variants; ~175B pretrained base before instruction tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Natural-language ATel and GCN circular messages; evaluation dataset of 202 manually labeled messages (ground truth for 3 entity types); sampling 5 completions per prompt; additional fine-tune experiments used 202 or 606 example pairs and a test split of 31 messages.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Information-extraction / NER-style distillation from individual reports via prompt-based zero-shot/few-shot extraction or supervised fine-tuning for specific entity outputs (single-entity or multi-entity completions).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured single-entity strings (event type, event ID/object name, physical phenomenon/object type) or multi-entity completions (they experimented with both but found single-entity outputs more accurate).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automated comparison to human-labeled ground truth; top-5 and top-1 accuracy metrics computed over 202 messages (and separate 31-message test sets for fine-tuning experiments); ranking by perplexity baseline and by embedding-ranking FFN alternative; Δ accuracy (gap between top‑5 and top‑1) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Zero-shot InstructGPT (text-davinci-003) achieved top-1 accuracies reported in Table 2: event type ≈ 0.495, event ID/object name ≈ 0.801, physical phenomena/object ≈ 0.648 (perplexity-ranked top-1). Few-shot / fine-tuned variants showed mixed results: fine-tuning on single-entity completions produced improvements for some entities (see Table 4), but overall the embedding-ranking pipeline outperformed plain perplexity-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom dataset assembled from The Astronomer's Telegram (ATel) and NASA GCN circulars; labeled subset of 202 messages (with a 31-message test subset for fine-tuning experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Hallucination and contamination from prompt enumerations (models tend to output items listed in the prompt even if absent), difficulty distinguishing absent vs. present entities, domain-specific terminology not always covered by pretraining, and imperfect ranking of multiple stochastic samples when using perplexity; computational cost for sampling and API usage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared against Flan-T5-XXL zero-shot, Flan-T5 models via HuggingFace, fine-tuned GPT-3 variants, and the embedding-ranking pipeline; InstructGPT performed well but was outperformed (top-1) by the embedding-ranking pipeline when multiple samples were re-ranked using an FFN predictor.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6027.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6027.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-XXL (instruction-finetuned T5, 11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-fine-tuned version of the T5 text‑to‑text transformer (the largest public Flan-T5 variant, ~11B parameters) that was used zero‑shot to extract entities from astrophysical telegrams.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Zero-shot Flan-T5-XXL prompt extraction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Zero-shot prompt engineering with Flan-T5-XXL to directly extract single entity types from messages by prepending concise task instructions; no weight updates during use.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5-XXL (instruction-finetuned T5, ~11B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Same ATel/GCN messages dataset (202 labeled messages); prompts constructed with task descriptions or example-based few-shot contexts; five stochastic samples per message tested in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Zero-shot prompt-based information extraction (NER-style single-entity outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured single-entity outputs for each prompt type (event type, object name/ID, physical phenomenon/object).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Top-5 and perplexity-ranked top-1 accuracy on the 202-message labeled dataset; comparison against InstructGPT and fine-tuned GPT-3 and embedding-ranking pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Flan-T5-XXL zero-shot achieved top-1 accuracies in Table 2: event type ≈ 0.544, event ID/object name ≈ 0.861, physical phenomena/object ≈ 0.514 (perplexity-ranked top-1). Performance was competitive with InstructGPT but lower than the embedding-ranking pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom ATel + GCN dataset of 202 labeled messages.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Similar prompt-sensitivity issues as other LLMs; some entity classes requiring deeper comprehension (event type, physical phenomena) are harder for zero-shot instruction prompts; output format contamination if not strictly specified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Performed better for some simpler entities (object name) than InstructGPT but was still surpassed by embedding-ranking re-ranking of samples; authors note larger models generally perform better on these prompts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6027.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6027.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt engineering (zero-shot / few-shot / APE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering methods including zero-shot task prompts, few-shot examples with explanations, chain-of-thought style prompts, and Automatic Prompt Engineer (APE)-generated prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of prompting strategies used to coax LLMs to perform extraction tasks without weight updates: direct task descriptions, in-context examples, explanations, chain-of-thought decomposition, and automatic prompt generation (APE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prompt engineering (manual and automated)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructed multiple prompt types—(1) simple task description, (2) example-enriched prompts listing possible entities, (3) few-shot examples with and without explanations, (4) chain-of-thought prompts ('Let's solve ... by splitting into steps'), and (5) prompts sampled/generated using Automatic Prompt Engineer (APE) meta-prompts copied from prior work—to improve zero-shot/few-shot extraction behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied to InstructGPT (text-davinci-003 / davinci), Flan-T5-XXL and other LLMs in experiments; APE itself was implemented by prompting an LLM to propose prompts (citing prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Each ATel/GCN message prefixed with a chosen prompt; evaluation on the 202-message labeled dataset across multiple prompt variants; few-shot prompts included several short examples; APE sampled 10 prompts per target to select candidate prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Prompt-mediated zero-shot/few-shot extraction and constrained generation to produce distilled structured outputs from each document.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Short structured answers (single tokens/phrases) representing entity types or names; the outputs were constrained by explicit output instructions to reduce verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Empirical ablation across prompt types measured by top-1/top-5 accuracy on labeled dataset; comparison of zero-shot vs few-shot vs chain-of-thought vs APE-derived prompts (see Appendix A comparative study).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Zero-shot prompts (with careful engineering and chain-of-thought style decomposition) often outperformed naive few-shot prompts; APE-generated prompts produced viable task descriptions but zero-shot still outperformed many few-shot variants; the paper reports that zero-shot with tuned prompts achieved high accuracy for simpler entities.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom ATel + GCN labeled dataset (202 messages) used for prompt evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Prompt-derived biases: listing candidate entities in prompts can bias outputs (false positives when entity absent); careful output-format specification required to avoid extraneous content; few-shot examples without explanations decreased generalization in some cases; prompt design is labor intensive though APE can reduce manual effort.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared numerous prompt types empirically; chain-of-thought prompts improved truth-seeking behavior; APE reduced human prompt authoring but zero-shot manual prompt tuning still yielded best results in many cases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6027.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6027.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-3 (OpenAI davinci family) for single-entity or multi-entity extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised fine-tuning of the GPT-3 davinci model on small labeled sets to map an astronomical message (optionally prefixed by a prompt) to the target entity output; experimented with single-entity completions, multi-entity completions, and prompt-in-input variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fine-tuned GPT-3 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Fine-tune davinci on (message, target-entity) pairs; experimented with three approaches: (a) have model return all entities in one multi-field completion, (b) fine-tune a model for each entity (single-entity completion), and (c) fine-tune model with prompts in input to output one entity—then run the respective model at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3 davinci / text-davinci variants (OpenAI, 175B family) fine-tuned via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Training sets: small datasets (first two datasets size 202 examples; third dataset 606 message-entity pairs when using three prompts per message); held-out test of 31 messages for fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Supervised mapping from full-text messages to distilled structured entity strings (NER-like) via gradient-based fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Single-entity strings or structured multi-entity completions (they found single-entity outputs more accurate).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Top-1 accuracy on test splits (31 messages) and comparison to zero-shot and embedding-ranking methods; Table 4 lists top-1 accuracies for fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Fine-tuned GPT-3 variants achieved mixed improvements: best per-entity numbers in Table 4 show, e.g., fine-tuning one entity in completion yields event type ≈ 0.705, event ID/object name ≈ 0.862, physical phenomena/object ≈ 0.588 (on 31-message test). However, overall these supervised models underperformed compared to the embedding-ranking pipeline and sometimes even zero-shot leads depending on the configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom ATel + GCN labeled set (202 messages; 606 message-prompt pairs for multi-prompt fine-tune), with 31-message test subsets reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Fine-tuning small LLMs on limited labeled data can overfit or fail to discriminate entity types (especially when requiring simultaneous multi-entity outputs); supervised fine-tuning is more computationally and financially expensive (OpenAI API) and requires separate models or prompt regimes per entity to reach best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to zero-shot prompting (InstructGPT/Flan-T5) and embedding-ranking re-ranking; fine-tuned single-entity models improved over naive multi-entity fine-tuning but did not surpass embedding-ranking pipeline top-1 accuracies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6027.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6027.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding‑ranking pipeline (FFN re-ranker)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-ranking pipeline using sampled LLM completions, a feature-extractor embedding model, and a trained FFN to predict the correct-completion embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A re-ranking system that samples multiple LLM outputs, embeds them, and uses a small feed-forward network (FFN) trained to predict the embedding of the correct answer so that the sampled completion with smallest L2 distance to the predicted embedding is selected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Embedding-ranking pipeline (FFN-based re-ranker)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sample n=5 stochastic completions from an LLM (text-davinci-002/003 used for sampling), embed each completion using a lightweight feature extractor (e.g., text-embedding-ada-001, GPT-2 variants), train a small FFN (two linear layers + dropout) that, given the message and sampled answer embeddings, predicts the embedding of the correct answer; select the sampled answer with minimum L2 distance to the predicted embedding. TripletMarginLoss used for best performance during FFN training.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Sampling LLM: text-davinci-002 / text-davinci-003 for answer generation; feature-extractor embedding models evaluated: text-embedding-ada-001, GPT-2, GPT-2 Exo-Machina (pearsonkyle/gpt2-exomachina), GPT-NEO 125M, and InstructGPT embeddings. The FFN is a small supervised network (not an LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs: same ATel + GCN messages (202 labeled messages; sampling 5 outputs per message). For FFN training they used ~151 messages in an experiment; they report MSE/Contrastive/TripletMarginLoss variants and best hyperparameters (12 epochs, lr=5e-4 for FFN on 151 messages). Feature-extractor fine-tuning on 606 message-prompt pairs was also tried.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Re-ranking of multiple sampled LLM completions via embedding-space supervision—effectively turning multiple stochastic outputs into a single high-confidence distilled structured entity per document.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Selected single-entity string (the sampled completion chosen by embedding L2 proximity to FFN-predicted embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Top-1 accuracy (selected answer) and top-5 accuracy (presence of any correct answer among samples) on the 202-message dataset; ablation of different embedding feature extractors and loss functions (MSELoss, ContrastiveLoss, TripletMarginLoss); measure Δ accuracy (gap between top-5 and top-1).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Embedding-ranking pipeline substantially improved top-1 accuracy versus perplexity-based ranking: Table 2 reports embedding-ranking top-1 accuracies of event type = 0.941, event ID/object name = 0.980, physical phenomena/object = 0.823 (dramatic improvements over other methods). Best FFN training used TripletMarginLoss and hidden layer dimension 512. Feature extractor pearsonkyle/gpt2-exomachina (fine-tuned) offered best compute/accuracy trade-offs (Table 3). The method required only ~100–200 labeled messages to learn a new entity category.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom ATel + GCN labeled dataset (202 messages) used for evaluation; FFN trained on subsets (e.g., 151 messages) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Computationally intensive (sampling multiple LLM completions and embedding them), requires a small labeled training set to train FFN re-ranker, and depends on quality of chosen feature-extractor embeddings; complexity increases at inference time compared to single-pass prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Direct comparison in paper shows embedding-ranking outperforms perplexity-based ranking and fine-tuning baselines on top-1 accuracy; also compared multiple feature extractors and loss objectives and found TripletMarginLoss + GPT-2 Exo‑Machina embeddings effective.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6027.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6027.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity-based ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity-based ranking of multiple sampled LLM completions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline strategy that samples multiple completions from an LLM and ranks them by their perplexity (the model's internal next-token probability-based score), selecting the lowest-perplexity completion as the top-1 answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Perplexity-ranked sampling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sample n outputs from the LLM with temperature (T=0.7 used) and rank sampled completions by their perplexity; pick lowest perplexity as the selected prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used with text-davinci-003 sampling in experiments; general baseline applicable to any autoregressive LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Applied to the 5 sampled completions per message across the 202-message dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Re-ranking of stochastic samples via internal model perplexity instead of external supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Single selected completion string per message.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Top-1 accuracy measured by counting whether the perplexity-selected completion matches human-labeled ground truth; compared to top-5 presence.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Perplexity ranking often failed to pick the correct completion even when a correct completion was present among samples; resulted in significantly lower top-1 accuracy than embedding-ranking. The paper reports a nontrivial Δ accuracy (difference between 'best possible if always choosing correct among samples' and perplexity-ranked top-1).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>202-message ATel + GCN labeled dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Perplexity is not a reliable indicator of semantic correctness; can favor grammatically likely but incorrect completions; thus suboptimal for selecting correct knowledge-extraction outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Directly compared against embedding-ranking pipeline; embedding-ranking substantially outperformed perplexity ranking on top-1 accuracy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6027.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6027.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic Prompt Engineer (APE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Prompt Engineer (APE) — LLM-based prompt generation / search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated method that uses an LLM to generate candidate prompts (meta-prompts) and resample them, reducing human effort in prompt creation; authors sampled 10 candidate prompts via APE-style generation and evaluated them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models Are Human-Level Prompt Engineers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>APE-generated prompt selection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use of meta-prompts from prior work to prompt an LLM to generate candidate task prompts; select promising prompts from a small sample (they used sample size 10) and apply them zero-shot to the extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Generated by the same class of LLMs (noted prior work used LLMs to propose prompts); applied to InstructGPT/Flan-T5 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Applied to the 202-message dataset for prompt discovery; generated 10 candidate prompts per task type and evaluated zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Automated prompt discovery to enable zero-shot knowledge extraction from documents without manual prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Candidate prompt text strings used to drive LLM extraction; final answers are the same structured single-entity outputs as other prompt approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Empirical evaluation of candidate prompts by measuring top-1/top-5 accuracy on labeled messages; compared APE prompts to manually authored prompts and few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>APE-generated prompts produced reasonable task descriptions and required less human effort, but zero-shot manually tuned prompts still yielded the best performance in many cases; authors observed that pure zero-shot with tuned prompts outperformed many few-shot prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom ATel + GCN 202-message dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Automated prompts can be semantically similar and require selection; APE may still produce prompts that bias outputs if not carefully validated; selection requires labeled data for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared APE-derived prompts against manually designed zero-shot and few-shot prompts; APE reduced human time but did not universally beat tuned manual prompts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6027.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6027.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting / decomposition prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting style that asks the model to generate intermediate reasoning steps (e.g., 'Let's solve this problem by splitting into steps.') to improve truth-seeking and reasoning for extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought style prompts</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Inject a short reasoning instruction into the zero-shot prompt to encourage stepwise decomposition before giving the final structured answer; the output is then post-processed to extract the final concise entity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied principally to InstructGPT (text-davinci) and Flan-T5 prompts in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Applied to each ATel/GCN message in the 202-message set; prompts contained additional instructions to produce intermediate reasoning then a final closed-form answer.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Prompt-induced internal chain-of-thought to improve decision-making for extraction; not supervised chain-of-thought fine-tuning, purely prompt-time decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Concise final answer (structured entity) following model-produced intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Top-1/top-5 accuracy compared across prompt types; qualitative inspection of edge cases where CoT reduces errors.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>CoT-style prompts improved the model's truth-seeking behavior and helped in some edge cases; authors explicitly used the tactic 'Thus, the correct answer is' to steer closed-form outputs. Overall, carefully designed CoT prompts contributed to improvements over naive zero-shot prompts for some entity types.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>202-message ATel + GCN dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Chain-of-thought increases verbosity and inference cost; some LLMs may refuse or truncate stepwise outputs; post-processing needed to extract concise structured answers from multi-sentence reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared empirically to few-shot examples with explanations and to simple zero-shot prompts; CoT often helped compared to naïve prompts and aligned with literature describing improved reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training language models to follow instructions with human feedback. <em>(Rating: 2)</em></li>
                <li>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Scaling Instruction-Finetuned Language Models. <em>(Rating: 2)</em></li>
                <li>The First Corpus in Time-Domain Astrophysics: Analysis and First Experiments on Named Entity Recognition. <em>(Rating: 2)</em></li>
                <li>Large Language Models Are Human-Level Prompt Engineers. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6027",
    "paper_id": "paper-258449651",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "InstructGPT (text-davinci-003/davinci)",
            "name_full": "InstructGPT (OpenAI instruction‑tuned GPT-3 family; e.g., text-davinci-003)",
            "brief_description": "An instruction‑fine-tuned GPT-3 model family optimized to follow natural‑language instructions via human feedback and RL fine-tuning; used here for zero‑ and few‑shot extraction of named entities from astronomical reports.",
            "citation_title": "Training language models to follow instructions with human feedback.",
            "mention_or_use": "use",
            "system_name": "Zero-/few-shot extraction with InstructGPT",
            "system_description": "Use of InstructGPT as a general-purpose LLM to extract structured named entities (event type, event ID/object name, physical phenomenon/object type) from freeform astronomical telegrams via carefully engineered prompts or via fine‑tuning; sampling multiple completions and ranking them.",
            "llm_model_used": "OpenAI GPT-3 family: text-davinci-003 / davinci (InstructGPT variants; ~175B pretrained base before instruction tuning)",
            "input_type_and_size": "Natural-language ATel and GCN circular messages; evaluation dataset of 202 manually labeled messages (ground truth for 3 entity types); sampling 5 completions per prompt; additional fine-tune experiments used 202 or 606 example pairs and a test split of 31 messages.",
            "distillation_approach": "Information-extraction / NER-style distillation from individual reports via prompt-based zero-shot/few-shot extraction or supervised fine-tuning for specific entity outputs (single-entity or multi-entity completions).",
            "output_type": "Structured single-entity strings (event type, event ID/object name, physical phenomenon/object type) or multi-entity completions (they experimented with both but found single-entity outputs more accurate).",
            "evaluation_methods": "Automated comparison to human-labeled ground truth; top-5 and top-1 accuracy metrics computed over 202 messages (and separate 31-message test sets for fine-tuning experiments); ranking by perplexity baseline and by embedding-ranking FFN alternative; Δ accuracy (gap between top‑5 and top‑1) reported.",
            "results": "Zero-shot InstructGPT (text-davinci-003) achieved top-1 accuracies reported in Table 2: event type ≈ 0.495, event ID/object name ≈ 0.801, physical phenomena/object ≈ 0.648 (perplexity-ranked top-1). Few-shot / fine-tuned variants showed mixed results: fine-tuning on single-entity completions produced improvements for some entities (see Table 4), but overall the embedding-ranking pipeline outperformed plain perplexity-based selection.",
            "datasets_or_benchmarks": "Custom dataset assembled from The Astronomer's Telegram (ATel) and NASA GCN circulars; labeled subset of 202 messages (with a 31-message test subset for fine-tuning experiments).",
            "challenges_or_limitations": "Hallucination and contamination from prompt enumerations (models tend to output items listed in the prompt even if absent), difficulty distinguishing absent vs. present entities, domain-specific terminology not always covered by pretraining, and imperfect ranking of multiple stochastic samples when using perplexity; computational cost for sampling and API usage.",
            "comparisons_to_other_methods": "Compared against Flan-T5-XXL zero-shot, Flan-T5 models via HuggingFace, fine-tuned GPT-3 variants, and the embedding-ranking pipeline; InstructGPT performed well but was outperformed (top-1) by the embedding-ranking pipeline when multiple samples were re-ranked using an FFN predictor.",
            "uuid": "e6027.0"
        },
        {
            "name_short": "Flan-T5-XXL",
            "name_full": "Flan-T5-XXL (instruction-finetuned T5, 11B)",
            "brief_description": "An instruction-fine-tuned version of the T5 text‑to‑text transformer (the largest public Flan-T5 variant, ~11B parameters) that was used zero‑shot to extract entities from astrophysical telegrams.",
            "citation_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.",
            "mention_or_use": "use",
            "system_name": "Zero-shot Flan-T5-XXL prompt extraction",
            "system_description": "Zero-shot prompt engineering with Flan-T5-XXL to directly extract single entity types from messages by prepending concise task instructions; no weight updates during use.",
            "llm_model_used": "Flan-T5-XXL (instruction-finetuned T5, ~11B parameters)",
            "input_type_and_size": "Same ATel/GCN messages dataset (202 labeled messages); prompts constructed with task descriptions or example-based few-shot contexts; five stochastic samples per message tested in evaluation.",
            "distillation_approach": "Zero-shot prompt-based information extraction (NER-style single-entity outputs).",
            "output_type": "Structured single-entity outputs for each prompt type (event type, object name/ID, physical phenomenon/object).",
            "evaluation_methods": "Top-5 and perplexity-ranked top-1 accuracy on the 202-message labeled dataset; comparison against InstructGPT and fine-tuned GPT-3 and embedding-ranking pipeline.",
            "results": "Flan-T5-XXL zero-shot achieved top-1 accuracies in Table 2: event type ≈ 0.544, event ID/object name ≈ 0.861, physical phenomena/object ≈ 0.514 (perplexity-ranked top-1). Performance was competitive with InstructGPT but lower than the embedding-ranking pipeline.",
            "datasets_or_benchmarks": "Custom ATel + GCN dataset of 202 labeled messages.",
            "challenges_or_limitations": "Similar prompt-sensitivity issues as other LLMs; some entity classes requiring deeper comprehension (event type, physical phenomena) are harder for zero-shot instruction prompts; output format contamination if not strictly specified.",
            "comparisons_to_other_methods": "Performed better for some simpler entities (object name) than InstructGPT but was still surpassed by embedding-ranking re-ranking of samples; authors note larger models generally perform better on these prompts.",
            "uuid": "e6027.1"
        },
        {
            "name_short": "Prompt engineering (zero-shot / few-shot / APE)",
            "name_full": "Prompt engineering methods including zero-shot task prompts, few-shot examples with explanations, chain-of-thought style prompts, and Automatic Prompt Engineer (APE)-generated prompts",
            "brief_description": "A set of prompting strategies used to coax LLMs to perform extraction tasks without weight updates: direct task descriptions, in-context examples, explanations, chain-of-thought decomposition, and automatic prompt generation (APE).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Prompt engineering (manual and automated)",
            "system_description": "Constructed multiple prompt types—(1) simple task description, (2) example-enriched prompts listing possible entities, (3) few-shot examples with and without explanations, (4) chain-of-thought prompts ('Let's solve ... by splitting into steps'), and (5) prompts sampled/generated using Automatic Prompt Engineer (APE) meta-prompts copied from prior work—to improve zero-shot/few-shot extraction behavior.",
            "llm_model_used": "Applied to InstructGPT (text-davinci-003 / davinci), Flan-T5-XXL and other LLMs in experiments; APE itself was implemented by prompting an LLM to propose prompts (citing prior work).",
            "input_type_and_size": "Each ATel/GCN message prefixed with a chosen prompt; evaluation on the 202-message labeled dataset across multiple prompt variants; few-shot prompts included several short examples; APE sampled 10 prompts per target to select candidate prompts.",
            "distillation_approach": "Prompt-mediated zero-shot/few-shot extraction and constrained generation to produce distilled structured outputs from each document.",
            "output_type": "Short structured answers (single tokens/phrases) representing entity types or names; the outputs were constrained by explicit output instructions to reduce verbosity.",
            "evaluation_methods": "Empirical ablation across prompt types measured by top-1/top-5 accuracy on labeled dataset; comparison of zero-shot vs few-shot vs chain-of-thought vs APE-derived prompts (see Appendix A comparative study).",
            "results": "Zero-shot prompts (with careful engineering and chain-of-thought style decomposition) often outperformed naive few-shot prompts; APE-generated prompts produced viable task descriptions but zero-shot still outperformed many few-shot variants; the paper reports that zero-shot with tuned prompts achieved high accuracy for simpler entities.",
            "datasets_or_benchmarks": "Custom ATel + GCN labeled dataset (202 messages) used for prompt evaluation.",
            "challenges_or_limitations": "Prompt-derived biases: listing candidate entities in prompts can bias outputs (false positives when entity absent); careful output-format specification required to avoid extraneous content; few-shot examples without explanations decreased generalization in some cases; prompt design is labor intensive though APE can reduce manual effort.",
            "comparisons_to_other_methods": "Compared numerous prompt types empirically; chain-of-thought prompts improved truth-seeking behavior; APE reduced human prompt authoring but zero-shot manual prompt tuning still yielded best results in many cases.",
            "uuid": "e6027.2"
        },
        {
            "name_short": "Fine-tuned GPT-3 (davinci)",
            "name_full": "Fine-tuned GPT-3 (OpenAI davinci family) for single-entity or multi-entity extraction",
            "brief_description": "Supervised fine-tuning of the GPT-3 davinci model on small labeled sets to map an astronomical message (optionally prefixed by a prompt) to the target entity output; experimented with single-entity completions, multi-entity completions, and prompt-in-input variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Fine-tuned GPT-3 extraction",
            "system_description": "Fine-tune davinci on (message, target-entity) pairs; experimented with three approaches: (a) have model return all entities in one multi-field completion, (b) fine-tune a model for each entity (single-entity completion), and (c) fine-tune model with prompts in input to output one entity—then run the respective model at inference.",
            "llm_model_used": "GPT-3 davinci / text-davinci variants (OpenAI, 175B family) fine-tuned via OpenAI API.",
            "input_type_and_size": "Training sets: small datasets (first two datasets size 202 examples; third dataset 606 message-entity pairs when using three prompts per message); held-out test of 31 messages for fine-tuning experiments.",
            "distillation_approach": "Supervised mapping from full-text messages to distilled structured entity strings (NER-like) via gradient-based fine-tuning.",
            "output_type": "Single-entity strings or structured multi-entity completions (they found single-entity outputs more accurate).",
            "evaluation_methods": "Top-1 accuracy on test splits (31 messages) and comparison to zero-shot and embedding-ranking methods; Table 4 lists top-1 accuracies for fine-tuned models.",
            "results": "Fine-tuned GPT-3 variants achieved mixed improvements: best per-entity numbers in Table 4 show, e.g., fine-tuning one entity in completion yields event type ≈ 0.705, event ID/object name ≈ 0.862, physical phenomena/object ≈ 0.588 (on 31-message test). However, overall these supervised models underperformed compared to the embedding-ranking pipeline and sometimes even zero-shot leads depending on the configuration.",
            "datasets_or_benchmarks": "Custom ATel + GCN labeled set (202 messages; 606 message-prompt pairs for multi-prompt fine-tune), with 31-message test subsets reported.",
            "challenges_or_limitations": "Fine-tuning small LLMs on limited labeled data can overfit or fail to discriminate entity types (especially when requiring simultaneous multi-entity outputs); supervised fine-tuning is more computationally and financially expensive (OpenAI API) and requires separate models or prompt regimes per entity to reach best performance.",
            "comparisons_to_other_methods": "Compared to zero-shot prompting (InstructGPT/Flan-T5) and embedding-ranking re-ranking; fine-tuned single-entity models improved over naive multi-entity fine-tuning but did not surpass embedding-ranking pipeline top-1 accuracies.",
            "uuid": "e6027.3"
        },
        {
            "name_short": "Embedding‑ranking pipeline (FFN re-ranker)",
            "name_full": "Embedding-ranking pipeline using sampled LLM completions, a feature-extractor embedding model, and a trained FFN to predict the correct-completion embedding",
            "brief_description": "A re-ranking system that samples multiple LLM outputs, embeds them, and uses a small feed-forward network (FFN) trained to predict the embedding of the correct answer so that the sampled completion with smallest L2 distance to the predicted embedding is selected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Embedding-ranking pipeline (FFN-based re-ranker)",
            "system_description": "Sample n=5 stochastic completions from an LLM (text-davinci-002/003 used for sampling), embed each completion using a lightweight feature extractor (e.g., text-embedding-ada-001, GPT-2 variants), train a small FFN (two linear layers + dropout) that, given the message and sampled answer embeddings, predicts the embedding of the correct answer; select the sampled answer with minimum L2 distance to the predicted embedding. TripletMarginLoss used for best performance during FFN training.",
            "llm_model_used": "Sampling LLM: text-davinci-002 / text-davinci-003 for answer generation; feature-extractor embedding models evaluated: text-embedding-ada-001, GPT-2, GPT-2 Exo-Machina (pearsonkyle/gpt2-exomachina), GPT-NEO 125M, and InstructGPT embeddings. The FFN is a small supervised network (not an LLM).",
            "input_type_and_size": "Inputs: same ATel + GCN messages (202 labeled messages; sampling 5 outputs per message). For FFN training they used ~151 messages in an experiment; they report MSE/Contrastive/TripletMarginLoss variants and best hyperparameters (12 epochs, lr=5e-4 for FFN on 151 messages). Feature-extractor fine-tuning on 606 message-prompt pairs was also tried.",
            "distillation_approach": "Re-ranking of multiple sampled LLM completions via embedding-space supervision—effectively turning multiple stochastic outputs into a single high-confidence distilled structured entity per document.",
            "output_type": "Selected single-entity string (the sampled completion chosen by embedding L2 proximity to FFN-predicted embedding).",
            "evaluation_methods": "Top-1 accuracy (selected answer) and top-5 accuracy (presence of any correct answer among samples) on the 202-message dataset; ablation of different embedding feature extractors and loss functions (MSELoss, ContrastiveLoss, TripletMarginLoss); measure Δ accuracy (gap between top-5 and top-1).",
            "results": "Embedding-ranking pipeline substantially improved top-1 accuracy versus perplexity-based ranking: Table 2 reports embedding-ranking top-1 accuracies of event type = 0.941, event ID/object name = 0.980, physical phenomena/object = 0.823 (dramatic improvements over other methods). Best FFN training used TripletMarginLoss and hidden layer dimension 512. Feature extractor pearsonkyle/gpt2-exomachina (fine-tuned) offered best compute/accuracy trade-offs (Table 3). The method required only ~100–200 labeled messages to learn a new entity category.",
            "datasets_or_benchmarks": "Custom ATel + GCN labeled dataset (202 messages) used for evaluation; FFN trained on subsets (e.g., 151 messages) in experiments.",
            "challenges_or_limitations": "Computationally intensive (sampling multiple LLM completions and embedding them), requires a small labeled training set to train FFN re-ranker, and depends on quality of chosen feature-extractor embeddings; complexity increases at inference time compared to single-pass prompting.",
            "comparisons_to_other_methods": "Direct comparison in paper shows embedding-ranking outperforms perplexity-based ranking and fine-tuning baselines on top-1 accuracy; also compared multiple feature extractors and loss objectives and found TripletMarginLoss + GPT-2 Exo‑Machina embeddings effective.",
            "uuid": "e6027.4"
        },
        {
            "name_short": "Perplexity-based ranking",
            "name_full": "Perplexity-based ranking of multiple sampled LLM completions",
            "brief_description": "A baseline strategy that samples multiple completions from an LLM and ranks them by their perplexity (the model's internal next-token probability-based score), selecting the lowest-perplexity completion as the top-1 answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Perplexity-ranked sampling",
            "system_description": "Sample n outputs from the LLM with temperature (T=0.7 used) and rank sampled completions by their perplexity; pick lowest perplexity as the selected prediction.",
            "llm_model_used": "Used with text-davinci-003 sampling in experiments; general baseline applicable to any autoregressive LLM.",
            "input_type_and_size": "Applied to the 5 sampled completions per message across the 202-message dataset.",
            "distillation_approach": "Re-ranking of stochastic samples via internal model perplexity instead of external supervision.",
            "output_type": "Single selected completion string per message.",
            "evaluation_methods": "Top-1 accuracy measured by counting whether the perplexity-selected completion matches human-labeled ground truth; compared to top-5 presence.",
            "results": "Perplexity ranking often failed to pick the correct completion even when a correct completion was present among samples; resulted in significantly lower top-1 accuracy than embedding-ranking. The paper reports a nontrivial Δ accuracy (difference between 'best possible if always choosing correct among samples' and perplexity-ranked top-1).",
            "datasets_or_benchmarks": "202-message ATel + GCN labeled dataset.",
            "challenges_or_limitations": "Perplexity is not a reliable indicator of semantic correctness; can favor grammatically likely but incorrect completions; thus suboptimal for selecting correct knowledge-extraction outputs.",
            "comparisons_to_other_methods": "Directly compared against embedding-ranking pipeline; embedding-ranking substantially outperformed perplexity ranking on top-1 accuracy.",
            "uuid": "e6027.5"
        },
        {
            "name_short": "Automatic Prompt Engineer (APE)",
            "name_full": "Automatic Prompt Engineer (APE) — LLM-based prompt generation / search",
            "brief_description": "An automated method that uses an LLM to generate candidate prompts (meta-prompts) and resample them, reducing human effort in prompt creation; authors sampled 10 candidate prompts via APE-style generation and evaluated them.",
            "citation_title": "Large Language Models Are Human-Level Prompt Engineers.",
            "mention_or_use": "use",
            "system_name": "APE-generated prompt selection",
            "system_description": "Use of meta-prompts from prior work to prompt an LLM to generate candidate task prompts; select promising prompts from a small sample (they used sample size 10) and apply them zero-shot to the extraction tasks.",
            "llm_model_used": "Generated by the same class of LLMs (noted prior work used LLMs to propose prompts); applied to InstructGPT/Flan-T5 in experiments.",
            "input_type_and_size": "Applied to the 202-message dataset for prompt discovery; generated 10 candidate prompts per task type and evaluated zero-shot performance.",
            "distillation_approach": "Automated prompt discovery to enable zero-shot knowledge extraction from documents without manual prompt engineering.",
            "output_type": "Candidate prompt text strings used to drive LLM extraction; final answers are the same structured single-entity outputs as other prompt approaches.",
            "evaluation_methods": "Empirical evaluation of candidate prompts by measuring top-1/top-5 accuracy on labeled messages; compared APE prompts to manually authored prompts and few-shot examples.",
            "results": "APE-generated prompts produced reasonable task descriptions and required less human effort, but zero-shot manually tuned prompts still yielded the best performance in many cases; authors observed that pure zero-shot with tuned prompts outperformed many few-shot prompt variants.",
            "datasets_or_benchmarks": "Custom ATel + GCN 202-message dataset.",
            "challenges_or_limitations": "Automated prompts can be semantically similar and require selection; APE may still produce prompts that bias outputs if not carefully validated; selection requires labeled data for evaluation.",
            "comparisons_to_other_methods": "Compared APE-derived prompts against manually designed zero-shot and few-shot prompts; APE reduced human time but did not universally beat tuned manual prompts.",
            "uuid": "e6027.6"
        },
        {
            "name_short": "Chain-of-Thought prompting",
            "name_full": "Chain-of-Thought (CoT) prompting / decomposition prompts",
            "brief_description": "Prompting style that asks the model to generate intermediate reasoning steps (e.g., 'Let's solve this problem by splitting into steps.') to improve truth-seeking and reasoning for extraction tasks.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought style prompts",
            "system_description": "Inject a short reasoning instruction into the zero-shot prompt to encourage stepwise decomposition before giving the final structured answer; the output is then post-processed to extract the final concise entity.",
            "llm_model_used": "Applied principally to InstructGPT (text-davinci) and Flan-T5 prompts in the experiments.",
            "input_type_and_size": "Applied to each ATel/GCN message in the 202-message set; prompts contained additional instructions to produce intermediate reasoning then a final closed-form answer.",
            "distillation_approach": "Prompt-induced internal chain-of-thought to improve decision-making for extraction; not supervised chain-of-thought fine-tuning, purely prompt-time decomposition.",
            "output_type": "Concise final answer (structured entity) following model-produced intermediate steps.",
            "evaluation_methods": "Top-1/top-5 accuracy compared across prompt types; qualitative inspection of edge cases where CoT reduces errors.",
            "results": "CoT-style prompts improved the model's truth-seeking behavior and helped in some edge cases; authors explicitly used the tactic 'Thus, the correct answer is' to steer closed-form outputs. Overall, carefully designed CoT prompts contributed to improvements over naive zero-shot prompts for some entity types.",
            "datasets_or_benchmarks": "202-message ATel + GCN dataset.",
            "challenges_or_limitations": "Chain-of-thought increases verbosity and inference cost; some LLMs may refuse or truncate stepwise outputs; post-processing needed to extract concise structured answers from multi-sentence reasoning.",
            "comparisons_to_other_methods": "Compared empirically to few-shot examples with explanations and to simple zero-shot prompts; CoT often helped compared to naïve prompts and aligned with literature describing improved reasoning.",
            "uuid": "e6027.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training language models to follow instructions with human feedback.",
            "rating": 2,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.",
            "rating": 2,
            "sanitized_title": "exploring_the_limits_of_transfer_learning_with_a_unified_texttotext_transformer"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Scaling Instruction-Finetuned Language Models.",
            "rating": 2,
            "sanitized_title": "scaling_instructionfinetuned_language_models"
        },
        {
            "paper_title": "The First Corpus in Time-Domain Astrophysics: Analysis and First Experiments on Named Entity Recognition.",
            "rating": 2,
            "sanitized_title": "the_first_corpus_in_timedomain_astrophysics_analysis_and_first_experiments_on_named_entity_recognition"
        },
        {
            "paper_title": "Large Language Models Are Human-Level Prompt Engineers.",
            "rating": 1,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        }
    ],
    "cost": 0.014660099999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models for Multimessenger Astronomy
Published: 1 May 2023</p>
<p>V Sotnikov 
A Chaikova 
Language Models for Multimessenger Astronomy
Published: 1 May 202310.3390/galaxies11030063Received: 1 March 2023 Revised: 24 April 2023 Accepted: 26 April 2023Language Models for Multimessenger Astronomy. Galaxies 2023, 11, 63. https://doi.org/ 10.3390/galaxies11030063 Academic Editors: Fabian Schüssler, Copyright: This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). galaxies Articleneural networknatural language processinglarge language modelastronomical reportmulti-messenger astronomy
With the increasing reliance of astronomy on multi-instrument and multi-messenger observations for detecting transient phenomena, communication among astronomers has become more critical. Apart from automatic prompt follow-up observations, short reports, e.g., GCN circulars and ATels, provide essential human-written interpretations and discussions of observations. These reports lack a defined format, unlike machine-readable messages, making it challenging to associate phenomena with specific objects or coordinates in the sky. This paper examines the use of large language models (LLMs)-machine learning models with billions of trainable parameters or more that are trained on text-such as InstructGPT-3 and open-source Flan-T5-XXL for extracting information from astronomical reports. The study investigates the zero-shot and few-shot learning capabilities of LLMs and demonstrates various techniques to improve the accuracy of predictions. The study shows the importance of careful prompt engineering while working with LLMs, as demonstrated through edge case examples. The study's findings have significant implications for the development of data-driven applications for astrophysical text analysis.</p>
<p>Introduction</p>
<p>As multi-instrument and multi-messenger observations become increasingly important in astronomy, effective communication among astronomers is crucial. In addition to messages for prompt follow-up observations, human-written reports such as GCN circulars and ATels provide in-depth analysis and interpretation of observations. However, the highly technical and unstructured language of these reports makes them difficult to process automatically.</p>
<p>The Astronomer's Telegram (ATel) [1] is a freely available online database of astronomical research and discoveries of transient sources written in natural language. The data contained in an ATel entry usually includes basic information about the object, such as its name, coordinates, and event type, as well as more detailed information, such as its frequency band, energy, spectral and temporal evolution, and the results of follow-up observations. The data can also include images and tabular data, as well as references to related papers and other ATel telegrams.</p>
<p>GCN Circulars is a dataset of astronomical circulars released by the Gamma-ray Coordinates Network (GCN) [2]. The GCN system distributes two types of messages: notices, which contain information about the location of GRBs (gamma-ray bursts) and other transients obtained by various spacecraft, and circulars, which contain information about follow-up observations made by ground-based and space-based optical, radio, X-ray, and TeV observers.</p>
<p>ATels are typically released within hours or days of an observation and provide a quick overview of the discovery and initial analysis. On the other hand, GCN circulars are more comprehensive reports released after a more thorough analysis of the observations. Both types of reports are crucial sources of information for astronomers and astrophysicists studying transient phenomena such as supernovae, gamma-ray bursts, and others. An example report is shown in Figure 1.</p>
<p>However, these reports' unstructured and often highly technical language can make them difficult to process automatically. This is a natural language processing task that requires advanced techniques, such as the use of large language models (LLMs), to extract relevant information. By using machine learning to analyze and interpret these reports, researchers can more efficiently process and understand the vast amount of data being generated in the field of astrophysics. The successful application of LLMs to analyzing ATels and GCN circulars could facilitate the rapid and accurate interpretation of transient astrophysical phenomena. Because Fermi operates in an all−sky scanning mode, regular gamma−ray monitoring of this source will continue. We encourage further multifrequency observations of this source. For this source the Fermi LAT contact person is Giovanna Pivato (giovanna.pivato@pi.infn.it). The Fermi LAT is a pair conversion telescope designed to cover the energy band from 20 MeV to greater than 300 GeV. It is the product of an international collaboration between NASA and DOE in the U.S. and many scientific institutions across France, Italy, Japan and Sweden. Our research aims to develop a technique for extracting structured information from human-written astrophysical reports and make it easily accessible to the astrophysical community. From these astronomical observations, we aim to extract the following named entities: event ID or object name, observed event (e.g., gamma-ray burst), observed object, or physical phenomena (e.g., BL Lac object, supernova explosion). However, an observation does not necessarily contain all these entities, and/or it may contain mentions of other objects or events, information that is not needed in the output.</p>
<p>To tackle this problem and multiple related challenges (little to no labeled data suitable for training and evaluation, highly technical domain language that makes it impossible to use third-party data labeling services, such as Mechanical Turk [3], named entities that are too complex for the existing NER (Named Entity Recognition) tools), we are exploring the capabilities of large language models (LLMs) such as InstructGPT-175B and Flan-T5-XXL in few-shot and zero-shot learning via fine-tuning and prompt engineering, respectively. These models have shown promising results [4,5] in various natural language processing tasks and have the potential to extract relevant information from ATels and GCN circulars accurately. We have developed several end-to-end methods that rely on large language models and require less annotated data since they only need the annotation of entities to be extracted instead of every word in a text.</p>
<p>To make the results of our research publicly accessible, we are also developing a web application and publicly available server API that would provide all the data extracted with LLMs in a convenient way, along with the source code of the application. The beta version of the API is available at [6], and the web application will become available at the same address in the summer of 2023.</p>
<p>1.1. Named Entity Recognition and Summarization 1.1.1. Overview Named Entity Recognition (NER) is an active research area that aims to identify and classify named entities in a text, such as people, organizations, locations, dates, etc. There are several approaches to NER, including rule-based [7], dictionary-based, and machine learning-based methods [8,9].</p>
<p>Flat NER refers to the task of identifying and classifying named entities in a given text without considering the relationships or hierarchies among the entities. This is in contrast to nested NER, also known as structured NER, which involves identifying and extracting hierarchically structured information from the text, such as relations between entities or their attributes. Nested NER is typically more challenging than flat NER, as it requires not only identifying the named entities in the text but also extracting the relationships and attributes between them.</p>
<p>Text summarization is the task of generating a shorter version of a document that preserves the important information and key points while reducing the length. There are several approaches to text summarization, including extractive and abstractive methods. Extractive summarization involves selecting a subset of the most important sentences or phrases from the original document to create the summary. These approaches are relatively simple to implement and can achieve good results, but they cannot paraphrase or rephrase the text. Abstractive summarization involves generating a summary that is a shorter version of the original document but may use different words and phrases to convey the same information. These approaches require the ability to understand the meaning and context of the text. They are also more prone to errors but can produce more fluent and coherent summaries that better capture the structure and key points of the document.</p>
<p>Existing Approaches</p>
<p>Several approaches have been proposed for flat NER. Some of the early techniques, such as rule-based [7] and dictionary-based systems, rely on a set of hand-crafted rules to identify and classify named entities. These systems typically use a combination of dictionary lookup, string matching, and heuristics to identify named entities in the text. While these systems can be effective, they are limited in their ability to handle novel or ambiguous named entities and require significant manual effort to develop and maintain.</p>
<p>Another approach is the use of machine learning-based methods. These methods typically involve the use of statistical models, such as hidden Markov models (e.g., Ref. [10]), maximum entropy models [11], and conditional random fields (e.g., Ref. [12]), to learn the characteristics of named entities and classify them based on those features.</p>
<p>Despite their effectiveness, these methods can be sensitive to the quality and quantity of the training data and can be difficult to adapt to new tasks or transfer to different domains. In recent years, progress has been made in improving NER using deep learning techniques, such as transformer-based models, convolutional neural networks, and recurrent neural networks, which can automatically learn hierarchical representations of the input text and make more accurate predictions. These models have achieved state-of-the-art results on many NER benchmarks and have the advantage of being able to handle long-range dependencies and contextual information in the input.</p>
<p>One of the recent works in the field of astronomical name entity recognition introduced TDAC, a Time-Domain Astrophysics Corpus for NLP [13]. Aside from collecting the first annotated and publicly available dataset of such kind (75 fully annotated time-domain astrophysical reports), the work also presents a series of experiments with fine-tuning of LLMs based on Bidirectional Encoder Representations from Transformers (BERT) architecture [14] that have shown promising results. Another important takeaway from this work is the time-consuming nature of full-text annotation: authors highlight that it takes about 4.5 min for the PhD student to annotate a single document. Thus, we find it very important to explore end-to-end deep learning approaches that only require a single text label that is much easier to prepare compared to an annotation.</p>
<p>Challenges</p>
<p>There are several challenges when extracting named entities from astronomical texts. One is the use of technical terms, which can be unfamiliar to a language model and may not be included in its training data [15]. The peculiarity of a task in a specific domain language means the absence of existing labeled datasets suitable for training and evaluation, as well as the difficulty of outsourcing data for labeling. Additionally, there may be ambiguity in the text, such as when words refer to an event that was not observed in the process described in the astronomical telegram merely mentioned, making it challenging to output only relevant entities. One more difficulty connected to extracting specific entities is that a text may contain a named entity while not clearly stating it, e.g., we can infer that the event type in the text is an X-ray burst due to the telescope that was used in observation. Named entities can also have complex multi-word structures, where omitting one word can lead to the loss of information, e.g., low mass X-ray binary (LMXB) compared to simply binary.</p>
<p>Large Language Models</p>
<p>Overview</p>
<p>Large language models (LLMs) are advanced natural language processing tools that use machine learning to gain knowledge from massive datasets of human language texts. They are trained on vast amounts of text data and are able to perform a variety of tasks, such as language translation, text generation, and information extraction. LLMs have tens to hundreds of billions of trainable parameters.</p>
<p>In addition to their ability to learn from large amounts of data, LLMs also have the ability to perform few-shot and zero-shot learning. Few-shot learning involves training a model on a small amount of labeled data and then fine-tuning it for a specific task. Zeroshot learning goes a step further and involves a model's ability to recognize and classify new objects or categories without any training examples for those specific classes.</p>
<p>Both few-shot and zero-shot learning are helpful when there is a limited amount of labeled data available for a specific task, as is often the case in natural language processing. By leveraging the powerful capabilities of LLMs and techniques such as few-shot and zero-shot learning, it is possible to achieve accurate results even with limited data. LLMs are often built using the Transformer architecture, which is a type of neural network that uses self-attention to process sequential data. Transformer networks are highly efficient and have shown excellent results in a variety of natural language processing tasks. Selfattention allows the model to weigh the importance of different words in a given input and focus on the most relevant ones. This is achieved through the use of attention mechanisms, which assign a weight to each word based on its relevance to the task at hand.</p>
<p>Few-Shot Learning via Fine-Tuning</p>
<p>One of the most important properties of large language models is their ability to perform well in few-shot learning tasks. Few-shot learning refers to the ability of a model to perform well on a new task with only a small amount of labeled data. This is particularly useful in situations where labeled data is scarce or difficult to obtain, such as in the field of natural language astrophysical reports, where data collection can be challenging.</p>
<p>Large language models, such as InstructGPT-3, have been shown to have exceptional few-shot capabilities due to their pre-training on large amounts of diverse data. InstructGPT-3, for example, is pre-trained on a massive amount of text data from the internet, which allows it to understand the nuances and complexities of natural language. This pre-training allows InstructGPT-3 to generalize to new tasks with minimal fine-tuning, which is the process of adapting a pre-trained model to a new task using a small amount of labeled data.</p>
<p>The few-shot learning capabilities of GPT-3 have been demonstrated in a number of studies. For example, in a study by Brown et al. (2020) [16], GPT-3 was fine-tuned on a small amount of labeled data to perform named entity recognition on scientific papers and was able to achieve comparable performance to models trained on much larger amounts of labeled data.</p>
<p>Another study by Raffel et al. (2019) [4] found that large text-to-text neural networks, such as T-5, are similarly able to perform well on a wide range of natural language understanding tasks with minimal fine-tuning. Unlike GPT-3, the T-5 was pre-trained on a diverse set of text-to-text tasks such as summarization, translation, and question-answering.</p>
<p>Zero-Shot Learning via Prompt Engineering</p>
<p>Another important property of large language models is their ability to perform zero-shot learning through prompt engineering [17]. Zero-shot learning, also known as "zero-shot transfer" or "meta-learning", is the ability of a model to perform a task without any fine-tuning (i.e., no gradient updates are performed) specific to that task. This is achieved through the use of carefully designed prompts that guide the model to perform the desired task.</p>
<p>For example, in the case of NER, a prompt could be designed to guide the model to identify specific entities such as "astronomical objects' names" or "astronomical event types" in astrophysical reports. Similarly, in the case of classification, a prompt could be designed to guide the model to classify reports based on their content.</p>
<p>We should note that there is some ambiguity about the term "zero-shot"-in some literature, it means "learning from zero examples", i.e., the ability of a model to perform a task without any labeled data specific to that task. However, that is not the case with prompt engineering, as it involves providing inference-time task-specific information to the model. Our paper explores zero-shot learning only in the sense that no updates of model weights are performed.</p>
<p>Decoding Strategies</p>
<p>Significant progress in natural language processing has not only led to advancements in Transformer architecture but also in decoding techniques. Decoding algorithms, such as the greedy search, select the highest probability word at each time step as the subsequent output token until reaching a maximum length or generating an end-of-sequence token. However, this approach fails to consider the entire sequence context, potentially resulting in suboptimal outputs. A more sophisticated decoding algorithm, beam search, addresses this limitation by considering a set of probable output sequences instead of choosing the highest probability word at every step. This process involves maintaining a fixed-size candidate set called the beam. The model generates a set of potential candidates at each time step, scores them based on their probability, and selects the top-K candidates to include in the beam. However, beam search also tends to produce repetitive or generic outputs and can be computationally intensive, particularly for longer sequences. Consequently, its application may be limited in real-time or resource-constrained scenarios. This issue has been extensively discussed in the language generation research of Vijayakumar et al. (2016) [18] and Shao et al. (2017) [19].</p>
<p>In addition to deterministic approaches, stochastic methods can be explored to prevent the output from being too generic. This can be achieved by using the probabilities generated by the softmax function to determine the next token. Random sampling methods are useful for generating more diverse and creative outputs while maintaining a balance between randomness and coherence in the generated text.</p>
<p>Temperature sampling [20] is one such method that adjusts the level of randomness in the generation of text by modifying the temperature hyperparameter of the probability distribution. Specifically, dividing the logits by the temperature value decreases the probability of unlikely tokens while increasing the probability of likely tokens. The resulting values are then passed through the softmax function to obtain the probability distribution across the vocabulary. Lower temperature values encourage more deterministic behavior, whereas higher values promote greater diversity.</p>
<p>Top-K sampling [21] constrains the number of candidate words to the K most probable tokens, which ensures that less probable words are excluded from consideration. However, this method has limitations because it does not account for the shape of the probability distribution and filters the same number of words regardless of the distribution's shape. This constraint can result in nonsensical output for sharp distributions or limit the model's creativity for flat distributions.</p>
<p>The top-p sampling method [22] was introduced to enhance the top-k method by selecting the top tokens with a cumulative probability greater than a certain threshold (p), thus avoiding the generation of low-probability tokens. Lower values of p yield more focused text, while higher values promote exploration and flexibility.</p>
<p>While random sampling techniques can result in less cohesive text and potentially inaccurate responses, they do provide diversity in the generated content. By sampling a model n times, a range of answers may be obtained, including some correct ones.</p>
<p>To assess the accuracy of these responses, evaluation metrics such as top-1 and top-n accuracy can be utilized. Top-1 accuracy refers to the percentage of times the model's topranked output is correct, while top-n accuracy measures the proportion of correct answers within the model's top-n predictions. It has been noted that top-n accuracy is generally higher than top-1 accuracy. This finding suggests that refining the ranking process of multiple model outputs can enhance top-1 accuracy. Nevertheless, challenges remain, such as developing reliable confidence metrics for ranking model outputs.</p>
<p>Processing Astronomical Texts with InstructGPT</p>
<p>Overview</p>
<p>GPT-3 (Generative Pre-trained Transformer) [16] is a state-of-the-art language model developed by OpenAI. It is a Transformer-based model with 175 billion parameters that was trained on a dataset of over 570 GB of text data. GPT-3 is capable of generating humanlike text and can perform a wide range of language tasks, such as language translation, summarization, and question answering, with high accuracy.</p>
<p>InstructGPT is a version of the GPT-3 that was fine-tuned with human feedback by OpenAI [5]. The InstructGPT is superior to GPT-3 in terms of following instructions in English due to the difference in language modeling objectives between these two models. Many LLMs are trained to predict the next token in a given text, while InstructGPT is designed to act in accordance with the user's objective. To achieve this result, OpenAI uses a three-step process. Firstly, they fine-tune a GPT-3 model using a dataset of prompts and human-written outputs. Next, a reward model is trained using human-ranked model sampled outputs. Finally, this reward model is used to fine-tune a supervised baseline using the PPO (Proximal Policy Optimization) algorithm. This approach results in InstructGPT being significantly preferred to GPT-3 when considering how well the model follows the user's instructions.</p>
<p>GPT-3 and, consequently, InstructGPT models have shown an understanding of astronomical terms required for our task during the initial experiments. This fact, along with these models achieving state-of-the-art results on a wide range of natural language processing (NLP) tasks, led us to choose them for this research.</p>
<p>Text-to-Text Transfer Transformer (T5) [4] is an open-source neural network architecture developed by Google Research for a wide range of natural language processing tasks. Unlike GPT-3, which uses a language modeling approach, T5 uses a text-to-text transfer learning framework, where the model is firstly pre-trained on the unsupervised text denoising objective, then fine-tuned to generate text that is a correct answer to a given text input. The task-specific information is encoded in the prompt, allowing the model to adapt to different tasks without changing its architecture. The model architecture consists of a stack of transformer layers, similar to GPT-3, with an encoder-decoder structure where the input text is encoded into a latent representation and then decoded into output text.</p>
<p>Flan-T5 is a version of the T5 neural network that was additionally instructionfinetuned on 1836 tasks that were phrased as instructions and often included chain-ofthought reasoning annotations [23]. Aside from the increase in performance of task-based benchmarks, instruction finetuning on chain-of-thought data makes the model much more capable of zero-shot reasoning. In this paper, we exclusively examine Flan-T5-XXL, the biggest publicly available version of Flan-T5 with 11B parameters.</p>
<p>Zero-Shot InstructGPT 175B and Flan-T5-XXL 11B</p>
<p>Prompt engineering is a naive approach to extracting named entities with LLMs that is the easiest to start with. We construct a prompt for each category of extracted information and then iteratively improve it, using labeled data for evaluation. This approach is the most straightforward and most convenient to start the research with because it does not require extensive training data or complex architectures. It can be performed with a pre-trained model and a small amount of labeled data (used solely for the evaluation of the model), which makes it easy to implement. However, it is worth noting that prompt engineering can also have its limitations. For instance, the quality of the generated text is highly dependent on the quality of the prompts, and if the prompts are not well-written or do not provide enough context, the generated text may not be coherent or relevant.</p>
<p>To refine the model outputs, we tried several different prompting approaches. The resulting prompts can be divided into categories depending on the idea behind them, and the method that was used for creating them.</p>
<p>The first prompt type is a simple task description, in which we ask the model to extract a certain entity from a text (example shown on Figure 2). Due to GPT-3's extensive range of functions that do not require runtime learning, we anticipate that the zero-shot prompts will yield satisfactory outcomes [24]. As the completions tended to contain additional information, we specified at the end of prompts what to include in the answer. While this type of prompt had difficulties discerning between different entities and dealing with absent entities, it has shown good performance.</p>
<p>Extract the observed astronomical object from the text: {Dataset Message} We don't need the name of the object, only its type, so output only one thing − the observed astronomical object: The second type of prompt is based on using examples in prompts, e.g., mentioning possible event types or astronomical object names. It was utilized to show the model the type of entities we wanted to see in the output -to help it differentiate between different entities. However, in case the required entity is absent in the astronomical observation, the model is prone to use one of the entities mentioned in the prompt, leading to the wrong answer. That led to a third type of prompt-"few-shot" prompts. These prompts contained the description of the task and several short astronomical texts with desired answers (example shown on Figure 3). This way, we can give a general description of the desired entity through examples as well as mention some borderline cases, such as the absence of the required entity.  In our experiments, this kind of prompt has allowed us to correctly identify dataset messages that contain no required named entities. However, the overall performance has decreased; thus, we refined them by adding short explanations of why such answers were chosen (example shown in Figure 4). By providing explanations along with the examples, the model can learn to understand the underlying reasoning or intent behind the classification, rather than just memorizing the examples [25]. Existing work in prompt tuning suggests that in-context supplemental information can affect the model performance positively.</p>
<p>A: SGR−like bursts Explanation: Konus−Wind and Helicon−Coronas−F observed the SGR−like bursts. SGR 1806−20 is the source of SGR−like bursts, it is not an event. Therefore, the observed event is SGR−like bursts. The fifth type of prompts was obtained by sampling them from the language model using the Automatic Prompt Engineer (APE) method. The aim was to reduce the human effort in creating the prompts and automate the process so that it would be possible to apply it to new kinds of entities in the future. The meta-prompts for generation and resampling were taken from previous work in this area [26]. We chose 10 as our sample size, as the instructions generated by the model differ little in semantic content, and we are able to find adequate prompts under this constraint. The resulting prompts contain only the description of the task and no examples, compared to the previous prompts (shown in Figure 5). Despite this, zero-shot performed better than few-shot.</p>
<p>Input a description of an astronomical object. Output the object's classification Taking into account the improved performance of zero-shot prompts, we decided to tune this kind of prompt. To direct a GPT-3's inference in the truth-seeking pattern, we employ reasoning in our prompts. Compared to few-shot prompts with explanations, this time, we ask the model to come up with a motivation for such output by asking the model via the prompt "Let's solve this problem by splitting into steps.", as shown in Figure 6. The term "chain-of-thought" prompt has been used to describe this type of prompt [27].</p>
<p>Astronomical event is something we can detect with a telescope. For example, it can be gamma−burst or transient signals. Astronomical events are the results of some physical phenomena, such as supernova explosions or neutron−star binary mergers. Extract the observed astronomical event from the text: {Dataset message}. Let's solve this problem by splitting it into steps. To receive an answer in a closed form that is convenient for the task, we inject "Thus, the correct answer is" after generating model output and query the model again with the new prompt.</p>
<p>Few-Shot GPT-3 175B</p>
<p>As another approach to the extraction of named entities using LLM, we fine-tuned the OpenAI davinci model-GPT-3 with 175B parameters-using OpenAI API. We have tried several approaches to fine-tuning.</p>
<p>The first idea is to retrieve all required entities at once, meaning the output for a given text message from the ATel dataset would look as shown in Figure 7-and it would either contain the corresponding entity if it is present in the text message or a tag 'none'. This method has shown the lowest accuracy, so we decided from this point on to extract only one entity at a time.  Thus, in the second experiment, we fine-tuned the model on training examples consisting of a dataset input example and one type of its associated named entity. This approach is more computably expensive, as we have to fine-tune a model for each of the entities we want to obtain. To address this issue, we changed the training examples for the fine-tuned model to be able to extract all types of entities. For this, we used the prompts with the best performance created in the previous section. The input during the fine-tuning consists of a dataset text message embedded in one of the prompts. The output to this input is chosen according to the type of entity this prompt is designed to extract. This way, to obtain a certain kind of entity during inference, we would have to prepend the astronomical text with the corresponding prompt. The first two datasets are of size 202, while the third one contains 606 message-entity pairs since for the last dataset, we used three prompts (for each entity) to prepend all 202 texts to fine-tune for all entities.</p>
<p>Embedding Ranking for InstructGPT</p>
<p>Due to the probabilistic nature of the sampling process, it is expected of LLMs to not be able to generate an accurate answer at 100% of attempts. Following the evaluation procedure as defined in Ref. [23], we sample our LLMs five times with temperature T = 0.7, and then rank completions based on their perplexity. When ranking LM's sampled outputs, perplexity can be used as a way to evaluate the quality of the generated text. Because perplexity is a measure of how well the model can predict the next word in a sequence, a lower perplexity score for an output indicates that the output is more likely to contain coherent and grammatically correct text. Perplexity is calculated as the exponential of the average negative log-likelihood of the model's predictions. In our case, we expect the answer with the lowest perplexity to be the correct one.</p>
<p>After manually examining each of the generated completion, we found out that quite often, LLMs do generate the correct answer, but they do not necessarily have the best perplexity as shown in Table 1. We define ∆ accuracy as the difference between the highest possible accuracy (if we were always selecting the correct completion, if present) vs. the accuracy obtained by the perplexity ranking. To maximize the accuracy of our NER process, we developed an alternative method of embedding ranking. Table 1. The influence of multiple sampling attempts on the LLM's accuracy is shown by the example of the text-davinci-003 model on the type of object or physical phenomena. Notice that ranking answers by their perplexity does not always lead to the correct prediction (if such a prediction exists). We train a feed-forward network (FFN) that receives the embeddings of sampled answers obtained from a feature extractor network and predicts the embedding of a correct answer (Figure 8). The FFN consists of two linear layers combined with dropout layers. The experiments have shown that the most optimal hyper-parameters for training the FFN on a dataset consisting of 151 messages are the following: 12 epochs, 5 × 10 −4 for the learning rate of Adam optimizer. During the evaluation, we calculate the L2 distance between answer embedding and predicted embedding. The answer with the lowest L2 is assumed to be correct. During the training phase, we also minimize the triple margin loss of the L2 distance between embeddings with regard to the model's weights. This approach achieves the best accuracy compared to those described previously.</p>
<p>Model</p>
<p>It should be noted that although training an FFN model for ranking may be less convenient, its primary advantage lies in having a smaller number of parameters. This results in reduced processing time and computing resources required to generate predictions, making it particularly useful in production environments with limited resources.</p>
<p>Increasing the model size or training dataset may decrease the gap between top-1 and top-n accuracies, as demonstrated in Jared Kaplan et al. (2020) [28]. However, despite this, we use different methods for ranking model outputs to reduce this gap due to recent research by Hyung Won Chung et al. (2022) [23] , which indicates that achieving an accuracy improvement of 38.8% → 58.5% would require a considerable increase in LLM size (more than eight times larger).</p>
<p>Step I </p>
<p>Sample Answers</p>
<p>Step II</p>
<p>Text Message</p>
<p>Embedding: size=768</p>
<p>Feature extractor network</p>
<p>Embedding: size=768</p>
<p>FFN</p>
<p>Step III True Labels</p>
<p>Answers' Embeddings</p>
<p>Update weights</p>
<p>Best answer Step I. Calculate the top three answers and their embeddings for a given text message and a prompt.</p>
<p>Step II. Predict an embedding of a correct answer for a given text message.</p>
<p>Step III. Calculate the L2 distance between answer embedding and predicted embedding. The answer with the lowest L2 is assumed to be correct.</p>
<p>Results</p>
<p>To understand how the model architecture and size affect the quality of output, given a certain prompt, we examine two davinci models available via the OpenAI API, and two FLAN-T5 models available via HuggingFace. To evaluate the quality of the prompts, we prepared a dataset of 202 messages with manually labeled ground truth answers for the following entities: event type; event ID or object name; physical phenomena, or astronomical object. For every text message from the dataset, we query a model five times and compare the predicted entities with the correct answers. Table 1 shows the inefficiency of perplexity-based answer ranking with respect to the accuracy of LLMs, using the example of text-davinci-003 [29]. We define top-5 accuracy as the proportion of text messages in which at least one correct entity is present among the top five predictions. We define top-1 accuracy as the proportion of text messages in which the highest-ranked prediction (based on perplexity or embedding L2 distance) is the correct answer.</p>
<p>Our goal is to maximize top-5 accuracy, then additionally reduce the gap between top-5 and top-1 accuracies, which we label as ∆ Accuracy. To achieve that, we compare two methods to rank sampled answers-perplexity ranking and embedding ranking using FFN. The comparison between best results using top-1 perplexity and embedding ranking is shown in Table 2. While being a common method to rank the LLM predictions (e.g., used by [23]), perplexity ranking produces suboptimal results compared to embeddings ranking.</p>
<p>As for the methods for comparing the model's outputs, the embedding ranking pipeline has shown the best performance compared to previous methods. As shown in Figure 9, we used MSELoss (minimizes squared L2 norm between embeddings of a prediction and correct answer) is represented by the solid line, ContrastiveLoss (minimizes squared L2 norm between embeddings of a prediction and correct answer, maximizes squared L2 norm between embeddings of a prediction and wrong answer) by the dashed line, and TripletMarginLoss (minimizes squared L2 norm between embeddings of a prediction and correct answer, maximizes squared L2 norm between embeddings of a prediction and wrong answer, plus adds a regularizing margin parameter). The highest accuracy is achieved by using TripletMarginLoss during training, with the hidden layers' dimension of 512. The pipeline has shown itself to be suitable for practical tasks: it requires a low amount of labeled data (100-200 text messages) to learn a new entity category. However, it is computationally intensive.  To optimize the amount of computing power used for each prediction, as well as to improve the embedding quality, we tried to use different feature extractors for our embedding ranking pipeline. We considered the following models: GPT-2, GPT-2 trained on data from NASA's Astrophysical Data System (GPT-2 Exo-Machina), Exo-Machina that was fine-tuned on our dataset, GPT-NEO 125M, and InstructGPT. The fine-tuned GPT-2 Exo-Machina, as shown in Table 3, has the best combination of computing cost and accuracy. It was fine-tuned on the dataset of 202 text messages with three prompts per message (606 message-prompt pairs).</p>
<p>We also investigated the impact of fine-tuning LLMs, specifically InstructGPT, on a small dataset. The testing datasets consisted of 31 text messages (15% of human marked-up dataset). Table 4 shows that the models fine-tuned only on one type of named entities have shown the best performance in two types of entities. However, the result is worse compared to zero-shot learning. As for the model fine-tuned to output three entities at once, the generated outputs have shown that the model failed to capture the difference between event type and astronomical objects from the training dataset alone. The model trained on the instructions showed a slight difference in accuracy from the best result and can be instead used for computational efficiency. A comparative study of different prompts is also presented in Appendix A. </p>
<p>Outlook</p>
<p>Large language models have proven to be extremely useful for the task of analyzing astronomical reports that are written in natural language. Through this research, we have already achieved a &gt;95% accuracy rate in the extraction of astronomical object names and event types from our labeled dataset of astronomical reports. We are continuing to work on extracting other entities from the text, such as physical quantities and descriptions of observations, in order to extract and structure the information contained in these reports fully. At the same time, we are also planning to prepare more labeled data to further increase the quality of our LLMs. All the predictions obtained with our best-performing method-embedding ranking pipeline-are publicly available via search API [6] and GitHub repository [30]. Using this API, our functionality can be integrated with multi-messenger brokers and platforms, e.g., with Astro-COLIBRI [31,32], to provide complementary information for human-written messages. In the coming months, we are also planning to release the first version of the web application for searching and crossreferencing ATels and GCN circulars using the data extracted with our LLMs. We are welcoming comments, feedback, and requests from the community, as well as third-party contributions of any kind. Funding: This research received no funding.</p>
<p>Data Availability Statement:</p>
<p>This research is based on the publicly available data from The Astronomer's Telegram (https://astronomerstelegram.org/ (accessed on 28 February 2023)) and NASA GCN circulars (https://gcn.nasa.gov/circulars (accessed on 28 February 2023)). We are gradually releasing the source code and prediction results in our publicly available GitHub repository: https://github.com/JetBrains/lm-astronomy (accessed on 28 February 2023).</p>
<p>Appendix A. Comparative Study of Different Prompts</p>
<p>Tables A1-A3 show the performance of different prompts on mentioned models. As expected, larger models have demonstrated overall better performance. Predictably, on a simpler named entity, such as an astronomical object name or event ID, all models and prompts have shown noticeably higher accuracy compared to the more complex entities that require text comprehension. For the latter entities, such as event type and the type of astronomical object or physical phenomena, instructions containing only the task description have shown better performance. As we noticed while working with this type of instruction, there has to be a clearly stated output format, or else the model output would be contaminated with unnecessary information. Table A1. Accuracy of predicting the event ID or object name entity for different models and prompts. Evaluated on a dataset of 202 text messages. The first value in a column is top-5 accuracy, and the second one is perplexity-ranked top-1 accuracy.  Table A2. Accuracy of predicting the physical phenomena or astronomical object for different models and prompts. Evaluated on a dataset of 202 text messages. The first value in a column is top-5 accuracy, and the second one is perplexity-ranked top-1 accuracy.  Table A3. Accuracy of predicting the event type for different models and prompts. Evaluated on a dataset of 202 text messages. The first value in a column is top-5 accuracy, and the second one is perplexity-ranked top-1 accuracy. </p>
<p>Subjects: Gamma Ray, Request for Observations, AGN, Blazar, Transient. Description: Referred to by ATel #: 8706, 8718, 8783, 8789 On Jan 14, 2016, the Large Area Telescope (LAT), one of the two instruments on the Fermi Gamma−ray Space Telescope, observed strong gamma−ray emission from a new source. The best−fit location of this gamma−ray source (RA=8.91 deg, Dec=61.52 deg, J2000.0) has a 95% containment radius of 0.08 deg (errors are statistical only). This source is not in any published LAT atalog and in the past has not been detected by AGILE or EGRET. The closest candidate counterpart is the radio source 87GB 003232.7+611352 , with coordinates RA=8.8542 deg, Dec=61.5083 deg (J2000.0; Petrov et al. 2006, AJ, 131 1872), at an angular distance of 0.03 deg. Preliminary analysis indicates that on Jan 14, 2016, the daily−averaged flux (E&gt;100MeV) was (5.7+/−1.5)10ˆ−7 photons cmˆ−2 sˆ−1, with a photon index of 1.8+/−0.2 (errors are statistical only).</p>
<p>Figure 1 .
1An example message from ATel. Named entities are marked with color: object name; the type of the object or physical phenomena; event type. Taken from Ref.[1].</p>
<p>Figure 2 .
2An example of description prompt for the type of the object or physical phenomena.</p>
<p>Task: Find the observed event type in the text. T: The Large Area Telescope (LAT), one of two instruments on the Fermi Gamma−ray Space Telescope, has observed increasing gamma−ray emission from a source positionally consistent with the very−high energy peaked BL Lac object 1ES 1215+303 . A: gamma−ray emission T: Subjects: Infra−Red, Supernovae, Transient. Description: The Wide−field Infrared Survey Explorer (WISE; Wright et al. 2010) obtained flux density measurements of five of the six currently known members of the class of objects known as Luminous Red Novae . A: None T: 10 SGR−like bursts were detected by Konus−Wind and Helicon−Coronas−F. The triangulation of several these bursts indicated that their origin is SGR 1806−20 . A: SGR−like bursts T: MASTER−Tunka auto−detection system (Lipunov et al., "MASTER Global Robotic Net", Advances in Astronomy, 2010, 349171) discovered optical transient at (RA, Dec) = 22h 44m 05.86s +43d 45m 32.2s on 2015−11−16.51056 UT. A: optical transient</p>
<p>Figure 3 .
3An example of a "few−shot" prompt for an event type. T. and A. denote input text and expected answer correspondingly. Named entities are marked with color: object name; the type of the object or physical phenomena; event type.</p>
<p>Figure 4 .
4An example of explanation inserted in "few−shot" prompt for the event type. A. denotes the expected answer.</p>
<p>Figure 5 .
5An example of prompt obtained by APE method for the type of the object or physical phenomena.</p>
<p>Figure 6 .
6An example of a prompt for coming up with motivation for an answer.</p>
<p>[
Event ID or object name] [Event type] [Physical phenomena or astronomical object]</p>
<p>Figure 7 .
7An example of output with three entities.</p>
<p>Figure 8 .
8Steps of the embedding ranking pipeline (from up to bottom).</p>
<p>Figure 9 .
9Comparison of FFN validation accuracy during training depending on a loss function and a hidden dimension size. Shown on the example of event type entity and fine-tuned pearsonkyle/gpt2-exomachina embeddings. The MSELoss is represented by the solid line, Con-trastiveLoss by the dashed line, and TripletMarginLoss by the line with a dot.</p>
<p>Author
Contributions: Conceptualization, V.S.; methodology, V.S. and A.C.; software, A.C.; validation, V.S. and A.C.; formal analysis, V.S.; investigation, A.C.; resources, V.S.; data curation, A.C.; writing-original draft preparation, V.S. and A.C.; writing-review and editing, V.S.; visualization, A.C.; supervision, V.S.; project administration, V.S. All authors have read and agreed to the published version of the manuscript.</p>
<p>Table 2 .
2The comparison of models according to their top-1 accuracy. All models, except for the embedding ranking pipeline, have used perplexity ranking.Model 
Event Type 
Event ID or Object 
Name </p>
<p>Physical Phenomena or 
Astronomical Object </p>
<p>text-davinci-003 zero-shot 
0.495 
0.801 
0.648 </p>
<p>Flan-T5-XXL zero-shot 
0.544 
0.861 
0.514 </p>
<p>Fine-tuned GPT-3 
0.677 
0.806 
0.548 </p>
<p>Embedding ranking pipeline 
0.941 
0.980 
0.823 </p>
<p>Table 3 .
3Top-1 accuracy of the embedding ranking pipeline depending on the feature extractor network. Large Language Model that was used for sampling answers-text-davincii-002.Table 4. Fine-tuned GPT models top-1 accuracy on datasets of 31 text messages. * Best prompt results on fine-tuning test datasets.Feature Extractor Network 
Event Type 
Event ID or Object 
Name </p>
<p>Physical Phenomena or 
Astronomical Object </p>
<p>text-embedding-ada-001 
0.882 
0.960 
0.764 </p>
<p>gpt-2 
0.745 
0.901 
0.823 </p>
<p>pearsonkyle/gpt2-exomachina 
0.941 
0.921 
0.784 </p>
<p>pearsonkyle/gpt2-exomachina 
fine-tuned 
0.882 
0.980 
0.823 </p>
<p>gpt-neo-125 
0.823 
0.921 
0.745 </p>
<p>Approach 
Event Type 
Event ID or Object Name 
Physical Phenomena or 
Astronomical Object </p>
<p>Fine-tuning: 3 Entities in 
completion 
0.529 
0.627 
0.392 </p>
<p>Fine-tuning: 1 Entity in 
completion 
0.705 
0.862 
0.588 </p>
<p>Fine-tuning: Prompt in input, 
1 entity in completion 
0.638 
0.793 
0.574 </p>
<p>Zero-shot learning * 
0.625 
0.851 
0.606 </p>
<p>Acknowledgments:We thank S. Golovachev and A. Ivanov for their help with setting up the R&amp;D infrastructure. We express our sincere gratitude to D. Kostunin for providing insightful commentaries. We thank OpenAI for providing the early access to their embedding API.Conflicts of Interest:The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.
The Astronomer's Telegram (ATel). 28Available onlineThe Astronomer's Telegram (ATel). Available online: https://www.astronomerstelegram.org (accessed on 28 February 2023).</p>
<p>GCN: The Gamma-ray Coordinates Network. 28GCN: The Gamma-ray Coordinates Network. Available online: https://gcn.nasa.gov/ (accessed on 28 February 2023).</p>
<p>. Amazon Mechanical Turk, 28Amazon Mechanical Turk. Available online: https://www.mturk.com/ (accessed on 28 February 2023).</p>
<p>C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, arXiv:1910.10683Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv 2019. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P.J. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv 2019, arXiv:1910.10683.</p>
<p>L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2203.02155Training language models to follow instructions with human feedback. arXiv 2022. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. Training language models to follow instructions with human feedback. arXiv 2022, arXiv:2203.02155.</p>
<p>Language Models for Multimessenger Astronomy. Project. Available online. 28Landing Page of theLanding Page of the "Language Models for Multimessenger Astronomy" Project. Available online: https://lm-astronomy.labs.jb. gg/ (accessed on 28 February 2023).</p>
<p>Named Entity Recognition without Gazetteers. A Mikheev, M Moens, C Grover, 10.3115/977035.977037Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics (EACL '99). the Ninth Conference on European Chapter of the Association for Computational Linguistics (EACL '99)Bergen, Norway; Stroudsburg, PA, USAAssociation for Computational LinguisticsMikheev, A.; Moens, M.; Grover, C. Named Entity Recognition without Gazetteers. In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics (EACL '99), Bergen, Norway, 8-12 June 1999; Association for Computational Linguistics: Stroudsburg, PA, USA, 1999; pp. 1-8. [CrossRef]</p>
<p>A Survey of Named Entity Recognition and Classification. Lingvisticae Investig. D Nadeau, S Sekine, 10.1075/li.30.1.03nad30Nadeau, D.; Sekine, S. A Survey of Named Entity Recognition and Classification. Lingvisticae Investig. 2007, 30, 3-26. [CrossRef]</p>
<p>Neural Architectures for Named Entity Recognition. G Lample, M Ballesteros, S Subramanian, K Kawakami, C Dyer, 10.18653/v1/N16-1030Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CA, USA; Stroudsburg, PA, USAAssociation for Computational LinguisticsLample, G.; Ballesteros, M.; Subramanian, S.; Kawakami, K.; Dyer, C. Neural Architectures for Named Entity Recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego, CA, USA, 12-17 June 2016; Association for Computational Linguistics: Stroudsburg, PA, USA, 2016; pp. 260-270. [CrossRef]</p>
<p>Named Entity Recognition using an HMM-based Chunk Tagger. G Zhou, J Su, 10.3115/1073083.1073163Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USA; Stroudsburg, PA, USAAssociation for Computational LinguisticsZhou, G.; Su, J. Named Entity Recognition using an HMM-based Chunk Tagger. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA, 6-12 June 2002; Association for Computational Linguistics: Stroudsburg, PA, USA, 2002; pp. 473-480. [CrossRef]</p>
<p>Maximum Entropy Models for Named Entity Recognition. O Bender, F J Och, H Ney, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. the Seventh Conference on Natural Language Learning at HLT-NAACL 2003Edmonton, AB, CanadaBender, O.; Och, F.J.; Ney, H. Maximum Entropy Models for Named Entity Recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, Edmonton, AB, Canada, 31 May-1 June 2003; pp. 148-151.</p>
<p>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. J D Lafferty, A Mccallum, F C N Pereira, Proceedings of the Eighteenth International Conference on Machine Learning (ICML '01). the Eighteenth International Conference on Machine Learning (ICML '01)Boca Raton, FL, USALafferty, J.D.; McCallum, A.; Pereira, F.C.N. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML '01), Boca Raton, FL, USA, 16-19 December 2001;</p>
<p>The First Corpus in Time-Domain Astrophysics: Analysis and First Experiments on Named Entity Recognition. A K Alkan, C Grouin, F Schussler, P Zweigenbaum, Tdac, Proceedings of the First Workshop on Information Extraction from Scientific Publications. the First Workshop on Information Extraction from Scientific PublicationsStroudsburg, PA, USA, 2022Association for Computational LinguisticsAlkan, A.K.; Grouin, C.; Schussler, F.; Zweigenbaum, P. TDAC, The First Corpus in Time-Domain Astrophysics: Analysis and First Experiments on Named Entity Recognition. In Proceedings of the First Workshop on Information Extraction from Scientific Publications, Online, 20 November 2022; Association for Computational Linguistics: Stroudsburg, PA, USA, 2022; pp. 131-139.</p>
<p>J Devlin, M Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv 2018. Devlin, J.; Chang, M.; Lee, K.; Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv 2018, arXiv:1810.04805.</p>
<p>Large Language Models Struggle to Learn Long-Tail Knowledge. N Kandpal, H Deng, A Roberts, E Wallace, C Raffel, arXiv:2211.08411arXiv 2022Kandpal, N.; Deng, H.; Roberts, A.; Wallace, E.; Raffel, C. Large Language Models Struggle to Learn Long-Tail Knowledge. arXiv 2022, arXiv:2211.08411.</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, arXiv:2005.14165Language Models are Few-Shot Learners. arXiv 2020. Brown, T.B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. Language Models are Few-Shot Learners. arXiv 2020, arXiv:2005.14165.</p>
<p>J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652Finetuned Language Models Are Zero-Shot Learners. arXiv 2021. Wei, J.; Bosma, M.; Zhao, V.Y.; Guu, K.; Yu, A.W.; Lester, B.; Du, N.; Dai, A.M.; Le, Q.V. Finetuned Language Models Are Zero-Shot Learners. arXiv 2021, arXiv:2109.01652.</p>
<p>Diverse Beam Search: Decoding Diverse Solutions from. A K Vijayakumar, M Cogswell, R R Selvaraju, Q Sun, S Lee, D Crandall, D Batra, arXiv:1610.02424Neural Sequence Models. Vijayakumar, A.K.; Cogswell, M.; Selvaraju, R.R.; Sun, Q.; Lee, S.; Crandall, D.; Batra, D. Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models. arXiv 2018, arXiv:1610.02424.</p>
<p>L Shao, S Gouws, D Britz, A Goldie, B Strope, R Kurzweil, arXiv:1701.03185Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models. Shao, L.; Gouws, S.; Britz, D.; Goldie, A.; Strope, B.; Kurzweil, R. Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models. arXiv 2017, arXiv:1701.03185.</p>
<p>A Learning Algorithm for Boltzmann Machines<em>. D H Ackley, G E Hinton, T J Sejnowski, 10.1207/s15516709cog0901_7Cogn. Sci. 9Ackley, D.H.; Hinton, G.E.; Sejnowski, T.J. A Learning Algorithm for Boltzmann Machines</em>. Cogn. Sci. 1985, 9, 147-169. [CrossRef]</p>
<p>. A Fan, M Lewis, Y Dauphin, arXiv:1805.04833Hierarchical Neural Story Generation. Fan, A.; Lewis, M.; Dauphin, Y. Hierarchical Neural Story Generation. arXiv 2018, arXiv:1805.04833.</p>
<p>A Holtzman, J Buys, L Du, M Forbes, Y Choi, arXiv:1904.09751The Curious Case of Neural Text Degeneration. arXiv 2020. Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; Choi, Y. The Curious Case of Neural Text Degeneration. arXiv 2020, arXiv:1904.09751.</p>
<p>H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416Scaling Instruction-Finetuned Language Models. arXiv 2022. Chung, H.W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al. Scaling Instruction-Finetuned Language Models. arXiv 2022, arXiv:2210.11416.</p>
<p>L Reynolds, K Mcdonell, arXiv:2102.07350Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. arXiv 2021. Reynolds, L.; McDonell, K. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. arXiv 2021, arXiv:2102.07350.</p>
<p>. A K Lampinen, I Dasgupta, S C Y Chan, K Matthewson, M H Tessler, A Creswell, J L Mcclelland, J X Wang, F Hill, arXiv:2204.02329Can language models learn from explanations in context? arXiv 2022Lampinen, A.K.; Dasgupta, I.; Chan, S.C.Y.; Matthewson, K.; Tessler, M.H.; Creswell, A.; McClelland, J.L.; Wang, J.X.; Hill, F. Can language models learn from explanations in context? arXiv 2022, arXiv:2204.02329.</p>
<p>Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, arXiv:2211.01910Large Language Models Are Human-Level Prompt Engineers. arXiv 2022. Zhou, Y.; Muresanu, A.I.; Han, Z.; Paster, K.; Pitis, S.; Chan, H.; Ba, J. Large Language Models Are Human-Level Prompt Engineers. arXiv 2022, arXiv:2211.01910.</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv 2022. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; Zhou, D. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv 2022, arXiv:2201.11903.</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, arXiv:2001.08361Amodei, D. Scaling Laws for Neural Language Models. arXiv 2020. Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; Amodei, D. Scaling Laws for Neural Language Models. arXiv 2020, arXiv:2001.08361.</p>
<p>The List of GPT-3 Models Available via the OpenAI API. 28The List of GPT-3 Models Available via the OpenAI API. Available online: https://platform.openai.com/docs/models/gpt-3 (accessed on 28 February 2023).</p>
<p>Language Models for Multimessenger Astronomy. Github, Project. Available online. 28GitHub Repository of the "Language Models for Multimessenger Astronomy" Project. Available online: https://github.com/ JetBrains/lm-astronomy (accessed on 28 February 2023).</p>
<p>Astro-COLIBRI-The COincidence LIBrary for Real-time Inquiry for Multimessenger Astrophysics. P Reichherzer, F Schüssler, V Lefranc, A Yusafzai, A K Alkan, H Ashkar, J B Tjus, 10.3847/1538-4365/ac1517Astrophys. J. Suppl. Ser. 2021, 256, 5. [CrossRefReichherzer, P.; Schüssler, F.; Lefranc, V.; Yusafzai, A.; Alkan, A.K.; Ashkar, H.; Tjus, J.B. Astro-COLIBRI-The COincidence LIBrary for Real-time Inquiry for Multimessenger Astrophysics. Astrophys. J. Suppl. Ser. 2021, 256, 5. [CrossRef]</p>
<p>Astro-COLIBRI 2-An Advanced Platform for Real-Time Multi-Messenger Discoveries. P Reichherzer, F Schüssler, V Lefranc, J Becker Tjus, J Mourier, A K Alkan, 10.3390/galaxies11010022Galaxies 2023, 11, 22. [CrossRefReichherzer, P.; Schüssler, F.; Lefranc, V.; Becker Tjus, J.; Mourier, J.; Alkan, A.K. Astro-COLIBRI 2-An Advanced Platform for Real-Time Multi-Messenger Discoveries. Galaxies 2023, 11, 22. [CrossRef]</p>
<p>The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. Disclaimer/Publisher&apos;s Note, instructions or products referred to in the contentDisclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p>            </div>
        </div>

    </div>
</body>
</html>
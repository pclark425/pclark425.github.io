<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4242 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4242</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4242</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-274899237</p>
                <p><strong>Paper Title:</strong> Data extraction from polymer literature using large language models</p>
                <p><strong>Paper Abstract:</strong> Automated data extraction from materials science literature at scale using arti ﬁ cial intelligence and natural language processing techniques is critical to advance materials discovery. However, this process for large spans of text continues to be a challenge due to the speci ﬁ c nature and styles of scienti ﬁ c manuscripts. In this study, we present a framework to automatically extract polymer-property data from full-text journal articles using commercially available (GPT-3.5) and open-source (LlaMa 2) large language models (LLM), in tandem with the named entity recognition (NER)-based MaterialsBERT model. Leveraging a corpus of ~ 2.4 million full text articles, our method successfully identi ﬁ ed and processed around 681,000 polymer-related articles, resulting in the extraction of over one million records corresponding to 24 properties of over 106,000 unique polymers. We additionally conducted an extensive evaluation of the performance and associated costs of the LLMs used for data extraction, compared to the NER model. We suggest methodologies to optimize costs, provide insights on effective inference via in-context few-shots learning, and illuminate gaps and opportunities for future studies utilizing LLMs for natural language processing in polymer science. The extracted polymer-property data has been made publicly available for the wider scienti ﬁ c community via the Polymer Scholar website.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4242.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4242.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 pipeline (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo-0613 based polymer-property extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A production pipeline that uses GPT-3.5 with one-shot in-context prompting (similarity-selected example) to extract numerical polymer property records from full-text articles, followed by post-processing (unit standardization, range checks) and downstream analysis of pairwise property relationships and correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data extraction from polymer literature using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3.5-based polymer data extraction and relationship discovery pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Input full-text paragraphs were prefiltered using property-specific heuristics and a MaterialsBERT NER filter to ensure presence of material, property, value and unit entities. A similarity-based one-shot in-context example (selected by k-means on MaterialsBERT text embeddings) was prepended to prompts of the form "Extract all <property> values in JSONL format with 'material','property','value','condition' columns." GPT-3.5-turbo-0613 (temperature 0.001) produced structured JSONL outputs which were post-processed to standardize units, enforce value ranges, resolve polymer name membership, and insert into a relational database; pairwise analyses of extracted numeric values were used to reveal empirical relationships between properties.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>likely >200B (as noted by authors, not precisely specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (polymer science)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to a corpus of ~681,000 polymer-related articles (full corpus); benchmarked on a 1,000-article subset (37,434 paragraphs, 6,179 after filters). Extracted 672,449 validated polymer-property records from full text.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical relationships and correlations (pairwise statistical relationships and observed trade-offs) distilled from literature-reported numerical property values (no new closed-form physical laws were proposed).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples of relationships recovered from the extracted numerical datasets include: positive correlation between crystallization temperature and glass transition temperature (Tg); inverse correlation between water contact angle and water uptake (hydrophilicity vs uptake); inverse relationship between bandgap and refractive index at fixed wavelength (optical property trend); negative trade-off between tensile strength and elongation at break; negative impact of water uptake on tensile properties. (No explicit closed-form equations were reported in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text mining with LLM: heuristic property filter -> MaterialsBERT NER filter -> GPT-3.5 one-shot in-context prompt asking for structured JSONL output -> programmatic post-processing (unit conversion, numeric range checks, polymer name cross-referencing). Similarity-based shot selection (MaterialsBERT embeddings + k-means) used to choose the one-shot example.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Performance evaluated on a manually curated benchmark: 630 abstracts with curated Tg and bandgap values (used as shot pool) and a 1,000-article subset for full-text benchmarking; computed entity-and-relationship-level F1 scores by verifying that extracted material, property, numerical value and unit matched ground truth extracted from the provided paragraph; post-processing validation included unit checks and min/max range filters determined from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 scores (entity+relationship level) reported: Tg F1 = 0.67 (GPT-3.5), bandgap F1 = 0.85 (GPT-3.5, Similar-shot selection reported 0.85; Random-shot sometimes 0.87). Quantity: GPT-3.5 extracted 4,706 material-property pairs (Random-shot) and 4,589 (Similar-shot) in the 6,179-paragraph benchmark; from full corpus GPT-3.5 produced 672,449 validated records across 24 properties. Monetary cost: approximately US$1,200 to process ~716,000 paragraphs (authors' estimate) and US$4.48 for the 6,179-paragraph benchmark run; token costs and token vs word correlations were also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No single "law correctness" fraction given; success characterized by F1 values above (Tg 0.67, bandgap 0.85) and by 672,449 post-validated records extracted from the full corpus. On the 1,000-paper benchmark, GPT-3.5 extracted valid data from 47,966 paragraphs where MaterialsBERT failed, and missed 7,311 paragraphs that MaterialsBERT captured.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Hallucinations and variability in LLM outputs; token and monetary costs scale with prompt length and number of shots; difficulty resolving entity relationships spanning multiple paragraphs or sections; inability to reliably parse chemical structures in figures or infer SMILES without manual conversion; inconsistent outputs from conversational LLMs requiring post-processing; errors when a paragraph lacks explicit entities (solved partially by NER prefilter); occasional mis-assignment of related but different temperatures (e.g., confusion between decomposition temperature and Tg) and incomplete identification of composite/blend constituents; tables and supplementary PDFs remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to MaterialsBERT (NER + rule-based relation extraction) and LLaMA-2 (70B) on the same benchmark. GPT-3.5 outperformed MaterialsBERT and LLaMA-2 in quantity of extracted records and in F1 for bandgap and Tg, at the expense of monetary costs and API dependence. MaterialsBERT was faster and cheaper (in-house), LLaMA-2 had slower inference and lower extraction performance in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data extraction from polymer literature using large language models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4242.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4242.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaterialsBERT pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MaterialsBERT (PubMedBERT-derived NER model) with rule-based relationship extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized BERT-based NER model (MaterialsBERT) used to identify named entities (material, property, value, unit) and rule-based heuristics to form relationships and extract polymer property records from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data extraction from polymer literature using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MaterialsBERT NER + heuristic relation-extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MaterialsBERT predicts named entities (POLYMER, MONOMER, PROPERTY_NAME, PROPERTY_VALUE, UNIT, etc.) in paragraphs that passed property-specific heuristic filters. A rule-based relationship module establishes correct associations between identified entities (e.g., matching a property value and unit to a specific material mention) to produce structured records. Subsequent post-processing standardizes units and filters values by literature-informed ranges. The NER filter is also used upstream to limit LLM prompts to paragraphs that contain all required named entities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MaterialsBERT (PubMedBERT-derived domain model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (polymer science)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the same corpus (681,000 polymer-related articles); extracted 390,813 validated polymer-property records from full text in this work; previously used to extract ~31,539 records from abstracts in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical relationships and correlations obtained indirectly by aggregating literature-reported numerical property values (pairwise correlations and trends), not closed-form theoretical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Captured central-region Tg and bandgap values used in pairwise plots (e.g., central clusters in Tg vs bandgap), and contributed data points for correlations such as crystallization temperature vs Tg and mechanical property trade-offs, though tended to miss complex composite/blend contexts and extremes identified by GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>NER-based entity recognition (MaterialsBERT) followed by deterministic rule-based relationship extraction; property-specific heuristic filters before NER; post-processing includes unit conversion and value-range checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Evaluated on 1,000-paper benchmark with manual curation (630 curated abstracts for shot pool); scored by F1 requiring exact presence of material, property, value, unit in the paragraph and correct association to compute entity+relation F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 scores reported: Tg F1 = 0.63 (MaterialsBERT), bandgap F1 = 0.66 (MaterialsBERT). Quantity: extracted 3,631 material-property pairs on the 6,179-paragraph benchmark; processed 6,179 paragraphs in under half an hour in-house (no monetary API cost). From full corpus: 390,813 validated records for 24 properties.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Measured via F1 as above; produced 390,813 post-validated records from full text. On benchmark, MaterialsBERT succeeded on 7,311 paragraphs that GPT-3.5 missed.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Difficulty associating property values to correct materials in texts with many numeric values (over-counting in blends/composites), inability to capture relationships expressed across multiple sentences/paragraphs, sensitivity to non-standard polymer nomenclature (synonyms, common names, acronyms), limited handling of table/figure data, and need for extensive manual curation/rules for relationship formation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-3.5 and LLaMA-2 on the benchmark; MaterialsBERT had lower extraction quantity and (for some properties) lower F1 but far lower computational cost and faster in-house inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data extraction from polymer literature using large language models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4242.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4242.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 (70B) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta AI LLaMA-2-chat 70B instruct-tuned model (evaluated locally)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter open-source instruct-tuned LLaMA-2-chat model evaluated for polymer-property extraction; run quantized locally and benchmarked against MaterialsBERT and GPT-3.5 but not used in final full-corpus extraction due to slower inference and comparatively lower end-to-end performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data extraction from polymer literature using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLaMA-2-chat (70B) local extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An instruct-tuned LLaMA-2-chat (70B) model (GPTQ 4-bit quantized, run on 4x Nvidia Quadro GP100 GPUs) was given the same or similar structured prompting approach (temperature 0.001, top_p 0.95, frequency_penalty 1.1, top_k 1) and produced structured outputs for extraction. No conversation history was maintained; outputs were post-processed similarly. Inference runtime was long due to model size and available hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (polymer science)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on the 1,000-article benchmark (6,179 paragraphs after filters); not used for full-corpus extraction due to performance/time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of numeric material property values to enable discovery of empirical relationships/correlations (no explicit closed-form laws reported).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>LLaMA-2 achieved stronger performance than MaterialsBERT for bandgap extraction in the benchmark (bandgap F1 ~0.77 reported for LLaMA-2 in comparisons), but did not demonstrate additional explicit law discovery beyond extracting numerical values used in pairwise analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted generation with LLaMA-2-chat on prefiltered paragraphs (same heuristic + NER upstream); structured-output prompting with post-processing to standardize units and validate values.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Benchmarking against the manually curated 1,000-article subset with F1 scoring comparable to other methods; compared extracted records to ground truth paragraph-level curated data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On the 6,179-paragraph benchmark: extracted ~3,441 material-property pairs; reported F1 ~0.77 for bandgap, Tg F1 ~0.64 in the text (LLaMA-2 outperformed MaterialsBERT for bandgap in this setup). Inference time was the longest of the three evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Measured by F1 scores as above on the benchmark; not deployed to full corpus due to throughput limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Long inference time on available in-house GPU hardware despite quantization, large memory/compute demands, lower end-to-end throughput which made it impractical for large-scale full-corpus extraction within the project timeline; compared unfavorably on cost/time tradeoffs versus in-house MaterialsBERT and cloud-hosted GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with MaterialsBERT and GPT-3.5 on the 1,000-paper benchmark: LLaMA-2 had intermediate extraction quality for some properties (e.g., bandgap) but slower inference time and was not selected for full-corpus extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data extraction from polymer literature using large language models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4242.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4242.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dagdelen et al. (cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that fine-tuned GPT-3.5 and LLaMA-2 models to extract structured records linking dopants and host metal-organic frameworks (MOFs) from the literature; cited here as an example of LLMs applied to structured IE in materials science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fine-tuned GPT-3.5 and LLaMA-2 for MOF dopant-host extraction (as described in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>According to the citation in this paper, Dagdelen et al. fine-tuned large LLMs (GPT-3.5 and LLaMA-2 variants) to perform structured information extraction linking dopants to MOF hosts; details about prompt engineering, fine-tuning dataset size, and exact pipeline are reported in the cited paper rather than this article.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (fine-tuned), LLaMA-2 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (metal-organic frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Structured relationship extraction (associating dopants with host materials) enabling construction of datasets for downstream analysis; not described as deriving explicit mathematical laws in this citation.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Model fine-tuning of LLMs for information extraction (as reported in the cited work); specifics not provided in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned here as prior art demonstrating LLM fine-tuning for structured extraction; details on limitations are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data extraction from polymer literature using large language models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4242.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4242.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng et al. (ChatGPT MOF synthesis extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited workflow using ChatGPT as a collaborator to extract a large corpus of synthesis parameters from MOF literature, demonstrating LLM-assisted extraction of quantitative experimental parameters at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT-assisted synthesis-parameter extraction workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>According to the citation, ChatGPT was used interactively as part of a human-in-the-loop workflow to extract synthesis parameters (e.g., temperatures, reagents, times) from MOF literature; the present paper cites the result: 26,257 distinct synthesis parameters of ~800 MOFs extracted from 228 articles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (model not specifically versioned in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / materials science (MOF synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>228 articles (as reported in this paper's citation summary)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of quantitative experimental parameters (synthesis conditions) and structured datasets enabling downstream analysis of relationships between synthesis conditions and materials properties; not explicitly described as deriving closed-form laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>26,257 distinct synthesis parameters for ~800 MOFs (quantitative experimental data extraction example cited).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>ChatGPT used in a human-collaborative extraction workflow (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data extraction from polymer literature using large language models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4242.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4242.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polak & Morgan (GPT-4 follow-up questioning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that used follow-up questioning with GPT-4 to verify correctness and mitigate hallucinations when extracting materials data (applied to metallic glasses and high-entropy alloys), illustrating conversational LLM strategies for improving extraction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 conversational extraction with follow-up questioning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>As cited, Polak & Morgan employed GPT-4 in an iterative conversational loop with follow-up questions to confirm and correct outputs (a strategy to reduce hallucinations and improve data accuracy) for materials data extraction tasks. Specific pipeline details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (metallic glasses, high-entropy alloys)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Structured numerical data extraction supporting empirical dataset construction; used follow-up interrogation to validate extracted items rather than deriving explicit analytic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Conversational LLM with iterative follow-up queries to confirm extracted values and relationships (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Follow-up questioning and human oversight to address hallucinations (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited as addressing hallucination and correctness with iterative questioning; further limitations and metrics are in the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data extraction from polymer literature using large language models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4242.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4242.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yang et al. (GPT-4 bandgap extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accurate prediction of experimental band gaps from large language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study that used GPT-4 with repeated questioning to extract/estimate bandgap values from the literature, reporting reduced error rates and producing a dataset more extensive than available human-curated databases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accurate prediction of experimental band gaps from large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 repeated-questioning bandgap extraction workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>As described in this paper's citations, Yang et al. used GPT-4 with a repeated or iterative questioning strategy to extract bandgap values from literature, improving accuracy and producing a dataset larger than some human-curated databases; specifics are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (electronic/optical properties)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of experimental property values (bandgaps) enabling improved empirical datasets and downstream predictive modeling; not framed as deriving new analytic physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Reduction in error rates for extracted bandgap values and generation of a dataset larger than human-curated counterparts (as reported in the citation).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Repeated questioning / iterative prompt strategy with GPT-4 to cross-check and refine extracted values (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Reported reduction in error rates versus prior datasets (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited as demonstrating improved accuracy via iterative querying; detailed limitations are in the original publication.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data extraction from polymer literature using large language models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from scientific text with large language models <em>(Rating: 2)</em></li>
                <li>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Accurate prediction of experimental band gaps from large language model <em>(Rating: 2)</em></li>
                <li>Accelerating materials language processing with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4242",
    "paper_id": "paper-274899237",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 pipeline (this work)",
            "name_full": "OpenAI GPT-3.5-turbo-0613 based polymer-property extraction pipeline",
            "brief_description": "A production pipeline that uses GPT-3.5 with one-shot in-context prompting (similarity-selected example) to extract numerical polymer property records from full-text articles, followed by post-processing (unit standardization, range checks) and downstream analysis of pairwise property relationships and correlations.",
            "citation_title": "Data extraction from polymer literature using large language models",
            "mention_or_use": "use",
            "system_name": "GPT-3.5-based polymer data extraction and relationship discovery pipeline",
            "system_description": "Input full-text paragraphs were prefiltered using property-specific heuristics and a MaterialsBERT NER filter to ensure presence of material, property, value and unit entities. A similarity-based one-shot in-context example (selected by k-means on MaterialsBERT text embeddings) was prepended to prompts of the form \"Extract all &lt;property&gt; values in JSONL format with 'material','property','value','condition' columns.\" GPT-3.5-turbo-0613 (temperature 0.001) produced structured JSONL outputs which were post-processed to standardize units, enforce value ranges, resolve polymer name membership, and insert into a relational database; pairwise analyses of extracted numeric values were used to reveal empirical relationships between properties.",
            "model_name": "GPT-3.5-turbo-0613",
            "model_size": "likely &gt;200B (as noted by authors, not precisely specified)",
            "scientific_domain": "Materials science (polymer science)",
            "number_of_papers": "Applied to a corpus of ~681,000 polymer-related articles (full corpus); benchmarked on a 1,000-article subset (37,434 paragraphs, 6,179 after filters). Extracted 672,449 validated polymer-property records from full text.",
            "law_type": "Empirical relationships and correlations (pairwise statistical relationships and observed trade-offs) distilled from literature-reported numerical property values (no new closed-form physical laws were proposed).",
            "law_examples": "Examples of relationships recovered from the extracted numerical datasets include: positive correlation between crystallization temperature and glass transition temperature (Tg); inverse correlation between water contact angle and water uptake (hydrophilicity vs uptake); inverse relationship between bandgap and refractive index at fixed wavelength (optical property trend); negative trade-off between tensile strength and elongation at break; negative impact of water uptake on tensile properties. (No explicit closed-form equations were reported in the paper.)",
            "extraction_method": "Text mining with LLM: heuristic property filter -&gt; MaterialsBERT NER filter -&gt; GPT-3.5 one-shot in-context prompt asking for structured JSONL output -&gt; programmatic post-processing (unit conversion, numeric range checks, polymer name cross-referencing). Similarity-based shot selection (MaterialsBERT embeddings + k-means) used to choose the one-shot example.",
            "validation_approach": "Performance evaluated on a manually curated benchmark: 630 abstracts with curated Tg and bandgap values (used as shot pool) and a 1,000-article subset for full-text benchmarking; computed entity-and-relationship-level F1 scores by verifying that extracted material, property, numerical value and unit matched ground truth extracted from the provided paragraph; post-processing validation included unit checks and min/max range filters determined from literature.",
            "performance_metrics": "F1 scores (entity+relationship level) reported: Tg F1 = 0.67 (GPT-3.5), bandgap F1 = 0.85 (GPT-3.5, Similar-shot selection reported 0.85; Random-shot sometimes 0.87). Quantity: GPT-3.5 extracted 4,706 material-property pairs (Random-shot) and 4,589 (Similar-shot) in the 6,179-paragraph benchmark; from full corpus GPT-3.5 produced 672,449 validated records across 24 properties. Monetary cost: approximately US$1,200 to process ~716,000 paragraphs (authors' estimate) and US$4.48 for the 6,179-paragraph benchmark run; token costs and token vs word correlations were also reported.",
            "success_rate": "No single \"law correctness\" fraction given; success characterized by F1 values above (Tg 0.67, bandgap 0.85) and by 672,449 post-validated records extracted from the full corpus. On the 1,000-paper benchmark, GPT-3.5 extracted valid data from 47,966 paragraphs where MaterialsBERT failed, and missed 7,311 paragraphs that MaterialsBERT captured.",
            "challenges_limitations": "Hallucinations and variability in LLM outputs; token and monetary costs scale with prompt length and number of shots; difficulty resolving entity relationships spanning multiple paragraphs or sections; inability to reliably parse chemical structures in figures or infer SMILES without manual conversion; inconsistent outputs from conversational LLMs requiring post-processing; errors when a paragraph lacks explicit entities (solved partially by NER prefilter); occasional mis-assignment of related but different temperatures (e.g., confusion between decomposition temperature and Tg) and incomplete identification of composite/blend constituents; tables and supplementary PDFs remain challenging.",
            "comparison_baseline": "Compared directly to MaterialsBERT (NER + rule-based relation extraction) and LLaMA-2 (70B) on the same benchmark. GPT-3.5 outperformed MaterialsBERT and LLaMA-2 in quantity of extracted records and in F1 for bandgap and Tg, at the expense of monetary costs and API dependence. MaterialsBERT was faster and cheaper (in-house), LLaMA-2 had slower inference and lower extraction performance in this setup.",
            "uuid": "e4242.0",
            "source_info": {
                "paper_title": "Data extraction from polymer literature using large language models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MaterialsBERT pipeline",
            "name_full": "MaterialsBERT (PubMedBERT-derived NER model) with rule-based relationship extraction pipeline",
            "brief_description": "A domain-specialized BERT-based NER model (MaterialsBERT) used to identify named entities (material, property, value, unit) and rule-based heuristics to form relationships and extract polymer property records from text.",
            "citation_title": "Data extraction from polymer literature using large language models",
            "mention_or_use": "use",
            "system_name": "MaterialsBERT NER + heuristic relation-extraction pipeline",
            "system_description": "MaterialsBERT predicts named entities (POLYMER, MONOMER, PROPERTY_NAME, PROPERTY_VALUE, UNIT, etc.) in paragraphs that passed property-specific heuristic filters. A rule-based relationship module establishes correct associations between identified entities (e.g., matching a property value and unit to a specific material mention) to produce structured records. Subsequent post-processing standardizes units and filters values by literature-informed ranges. The NER filter is also used upstream to limit LLM prompts to paragraphs that contain all required named entities.",
            "model_name": "MaterialsBERT (PubMedBERT-derived domain model)",
            "model_size": null,
            "scientific_domain": "Materials science (polymer science)",
            "number_of_papers": "Applied to the same corpus (681,000 polymer-related articles); extracted 390,813 validated polymer-property records from full text in this work; previously used to extract ~31,539 records from abstracts in prior work.",
            "law_type": "Empirical relationships and correlations obtained indirectly by aggregating literature-reported numerical property values (pairwise correlations and trends), not closed-form theoretical laws.",
            "law_examples": "Captured central-region Tg and bandgap values used in pairwise plots (e.g., central clusters in Tg vs bandgap), and contributed data points for correlations such as crystallization temperature vs Tg and mechanical property trade-offs, though tended to miss complex composite/blend contexts and extremes identified by GPT-3.5.",
            "extraction_method": "NER-based entity recognition (MaterialsBERT) followed by deterministic rule-based relationship extraction; property-specific heuristic filters before NER; post-processing includes unit conversion and value-range checks.",
            "validation_approach": "Evaluated on 1,000-paper benchmark with manual curation (630 curated abstracts for shot pool); scored by F1 requiring exact presence of material, property, value, unit in the paragraph and correct association to compute entity+relation F1.",
            "performance_metrics": "F1 scores reported: Tg F1 = 0.63 (MaterialsBERT), bandgap F1 = 0.66 (MaterialsBERT). Quantity: extracted 3,631 material-property pairs on the 6,179-paragraph benchmark; processed 6,179 paragraphs in under half an hour in-house (no monetary API cost). From full corpus: 390,813 validated records for 24 properties.",
            "success_rate": "Measured via F1 as above; produced 390,813 post-validated records from full text. On benchmark, MaterialsBERT succeeded on 7,311 paragraphs that GPT-3.5 missed.",
            "challenges_limitations": "Difficulty associating property values to correct materials in texts with many numeric values (over-counting in blends/composites), inability to capture relationships expressed across multiple sentences/paragraphs, sensitivity to non-standard polymer nomenclature (synonyms, common names, acronyms), limited handling of table/figure data, and need for extensive manual curation/rules for relationship formation.",
            "comparison_baseline": "Compared to GPT-3.5 and LLaMA-2 on the benchmark; MaterialsBERT had lower extraction quantity and (for some properties) lower F1 but far lower computational cost and faster in-house inference.",
            "uuid": "e4242.1",
            "source_info": {
                "paper_title": "Data extraction from polymer literature using large language models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLaMA-2 (70B) evaluation",
            "name_full": "Meta AI LLaMA-2-chat 70B instruct-tuned model (evaluated locally)",
            "brief_description": "A 70-billion-parameter open-source instruct-tuned LLaMA-2-chat model evaluated for polymer-property extraction; run quantized locally and benchmarked against MaterialsBERT and GPT-3.5 but not used in final full-corpus extraction due to slower inference and comparatively lower end-to-end performance.",
            "citation_title": "Data extraction from polymer literature using large language models",
            "mention_or_use": "use",
            "system_name": "LLaMA-2-chat (70B) local extraction pipeline",
            "system_description": "An instruct-tuned LLaMA-2-chat (70B) model (GPTQ 4-bit quantized, run on 4x Nvidia Quadro GP100 GPUs) was given the same or similar structured prompting approach (temperature 0.001, top_p 0.95, frequency_penalty 1.1, top_k 1) and produced structured outputs for extraction. No conversation history was maintained; outputs were post-processed similarly. Inference runtime was long due to model size and available hardware.",
            "model_name": "LLaMA-2-chat",
            "model_size": "70B",
            "scientific_domain": "Materials science (polymer science)",
            "number_of_papers": "Evaluated on the 1,000-article benchmark (6,179 paragraphs after filters); not used for full-corpus extraction due to performance/time constraints.",
            "law_type": "Extraction of numeric material property values to enable discovery of empirical relationships/correlations (no explicit closed-form laws reported).",
            "law_examples": "LLaMA-2 achieved stronger performance than MaterialsBERT for bandgap extraction in the benchmark (bandgap F1 ~0.77 reported for LLaMA-2 in comparisons), but did not demonstrate additional explicit law discovery beyond extracting numerical values used in pairwise analyses.",
            "extraction_method": "Prompted generation with LLaMA-2-chat on prefiltered paragraphs (same heuristic + NER upstream); structured-output prompting with post-processing to standardize units and validate values.",
            "validation_approach": "Benchmarking against the manually curated 1,000-article subset with F1 scoring comparable to other methods; compared extracted records to ground truth paragraph-level curated data.",
            "performance_metrics": "On the 6,179-paragraph benchmark: extracted ~3,441 material-property pairs; reported F1 ~0.77 for bandgap, Tg F1 ~0.64 in the text (LLaMA-2 outperformed MaterialsBERT for bandgap in this setup). Inference time was the longest of the three evaluated models.",
            "success_rate": "Measured by F1 scores as above on the benchmark; not deployed to full corpus due to throughput limitations.",
            "challenges_limitations": "Long inference time on available in-house GPU hardware despite quantization, large memory/compute demands, lower end-to-end throughput which made it impractical for large-scale full-corpus extraction within the project timeline; compared unfavorably on cost/time tradeoffs versus in-house MaterialsBERT and cloud-hosted GPT-3.5.",
            "comparison_baseline": "Compared with MaterialsBERT and GPT-3.5 on the 1,000-paper benchmark: LLaMA-2 had intermediate extraction quality for some properties (e.g., bandgap) but slower inference time and was not selected for full-corpus extraction.",
            "uuid": "e4242.2",
            "source_info": {
                "paper_title": "Data extraction from polymer literature using large language models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Dagdelen et al. (cited prior work)",
            "name_full": "Structured information extraction from scientific text with large language models",
            "brief_description": "Prior work that fine-tuned GPT-3.5 and LLaMA-2 models to extract structured records linking dopants and host metal-organic frameworks (MOFs) from the literature; cited here as an example of LLMs applied to structured IE in materials science.",
            "citation_title": "Structured information extraction from scientific text with large language models",
            "mention_or_use": "mention",
            "system_name": "Fine-tuned GPT-3.5 and LLaMA-2 for MOF dopant-host extraction (as described in cited work)",
            "system_description": "According to the citation in this paper, Dagdelen et al. fine-tuned large LLMs (GPT-3.5 and LLaMA-2 variants) to perform structured information extraction linking dopants to MOF hosts; details about prompt engineering, fine-tuning dataset size, and exact pipeline are reported in the cited paper rather than this article.",
            "model_name": "GPT-3.5 (fine-tuned), LLaMA-2 (fine-tuned)",
            "model_size": null,
            "scientific_domain": "Materials science (metal-organic frameworks)",
            "number_of_papers": null,
            "law_type": "Structured relationship extraction (associating dopants with host materials) enabling construction of datasets for downstream analysis; not described as deriving explicit mathematical laws in this citation.",
            "law_examples": null,
            "extraction_method": "Model fine-tuning of LLMs for information extraction (as reported in the cited work); specifics not provided in the present paper.",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Mentioned here as prior art demonstrating LLM fine-tuning for structured extraction; details on limitations are in the cited work.",
            "comparison_baseline": null,
            "uuid": "e4242.3",
            "source_info": {
                "paper_title": "Data extraction from polymer literature using large language models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Zheng et al. (ChatGPT MOF synthesis extraction)",
            "name_full": "ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis",
            "brief_description": "A cited workflow using ChatGPT as a collaborator to extract a large corpus of synthesis parameters from MOF literature, demonstrating LLM-assisted extraction of quantitative experimental parameters at scale.",
            "citation_title": "ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis",
            "mention_or_use": "mention",
            "system_name": "ChatGPT-assisted synthesis-parameter extraction workflow",
            "system_description": "According to the citation, ChatGPT was used interactively as part of a human-in-the-loop workflow to extract synthesis parameters (e.g., temperatures, reagents, times) from MOF literature; the present paper cites the result: 26,257 distinct synthesis parameters of ~800 MOFs extracted from 228 articles.",
            "model_name": "ChatGPT (model not specifically versioned in this paper)",
            "model_size": null,
            "scientific_domain": "Chemistry / materials science (MOF synthesis)",
            "number_of_papers": "228 articles (as reported in this paper's citation summary)",
            "law_type": "Extraction of quantitative experimental parameters (synthesis conditions) and structured datasets enabling downstream analysis of relationships between synthesis conditions and materials properties; not explicitly described as deriving closed-form laws.",
            "law_examples": "26,257 distinct synthesis parameters for ~800 MOFs (quantitative experimental data extraction example cited).",
            "extraction_method": "ChatGPT used in a human-collaborative extraction workflow (details in cited work).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": null,
            "comparison_baseline": null,
            "uuid": "e4242.4",
            "source_info": {
                "paper_title": "Data extraction from polymer literature using large language models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Polak & Morgan (GPT-4 follow-up questioning)",
            "name_full": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "brief_description": "Cited work that used follow-up questioning with GPT-4 to verify correctness and mitigate hallucinations when extracting materials data (applied to metallic glasses and high-entropy alloys), illustrating conversational LLM strategies for improving extraction fidelity.",
            "citation_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "mention_or_use": "mention",
            "system_name": "GPT-4 conversational extraction with follow-up questioning",
            "system_description": "As cited, Polak & Morgan employed GPT-4 in an iterative conversational loop with follow-up questions to confirm and correct outputs (a strategy to reduce hallucinations and improve data accuracy) for materials data extraction tasks. Specific pipeline details are in the cited work.",
            "model_name": "GPT-4 (as cited)",
            "model_size": null,
            "scientific_domain": "Materials science (metallic glasses, high-entropy alloys)",
            "number_of_papers": null,
            "law_type": "Structured numerical data extraction supporting empirical dataset construction; used follow-up interrogation to validate extracted items rather than deriving explicit analytic laws.",
            "law_examples": null,
            "extraction_method": "Conversational LLM with iterative follow-up queries to confirm extracted values and relationships (as cited).",
            "validation_approach": "Follow-up questioning and human oversight to address hallucinations (as cited).",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited as addressing hallucination and correctness with iterative questioning; further limitations and metrics are in the original paper.",
            "comparison_baseline": null,
            "uuid": "e4242.5",
            "source_info": {
                "paper_title": "Data extraction from polymer literature using large language models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Yang et al. (GPT-4 bandgap extraction)",
            "name_full": "Accurate prediction of experimental band gaps from large language model",
            "brief_description": "A cited study that used GPT-4 with repeated questioning to extract/estimate bandgap values from the literature, reporting reduced error rates and producing a dataset more extensive than available human-curated databases.",
            "citation_title": "Accurate prediction of experimental band gaps from large language model",
            "mention_or_use": "mention",
            "system_name": "GPT-4 repeated-questioning bandgap extraction workflow",
            "system_description": "As described in this paper's citations, Yang et al. used GPT-4 with a repeated or iterative questioning strategy to extract bandgap values from literature, improving accuracy and producing a dataset larger than some human-curated databases; specifics are in the cited paper.",
            "model_name": "GPT-4 (as cited)",
            "model_size": null,
            "scientific_domain": "Materials science (electronic/optical properties)",
            "number_of_papers": null,
            "law_type": "Extraction of experimental property values (bandgaps) enabling improved empirical datasets and downstream predictive modeling; not framed as deriving new analytic physical laws.",
            "law_examples": "Reduction in error rates for extracted bandgap values and generation of a dataset larger than human-curated counterparts (as reported in the citation).",
            "extraction_method": "Repeated questioning / iterative prompt strategy with GPT-4 to cross-check and refine extracted values (as cited).",
            "validation_approach": "Reported reduction in error rates versus prior datasets (details in cited work).",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited as demonstrating improved accuracy via iterative querying; detailed limitations are in the original publication.",
            "comparison_baseline": null,
            "uuid": "e4242.6",
            "source_info": {
                "paper_title": "Data extraction from polymer literature using large language models",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from scientific text with large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_scientific_text_with_large_language_models"
        },
        {
            "paper_title": "ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Accurate prediction of experimental band gaps from large language model",
            "rating": 2,
            "sanitized_title": "accurate_prediction_of_experimental_band_gaps_from_large_language_model"
        },
        {
            "paper_title": "Accelerating materials language processing with large language models",
            "rating": 1,
            "sanitized_title": "accelerating_materials_language_processing_with_large_language_models"
        }
    ],
    "cost": 0.0202175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Data extraction from polymer literature using large language models Check for updates</p>
<p>Sonakshi Gupta 
School of Computational Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Akhlak Mahmood 0000-0002-5607-2885
School of Materials Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Pranav Shetty 
School of Computational Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Aishat Adeboye 
School of Chemical and Biomolecular Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Rampi Ramprasad rampi.ramprasad@mse.gatech.edu 0000-0003-4630-1565
School of Materials Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Data extraction from polymer literature using large language models Check for updates
0DA68B74C6456EAD54A2BAE3D6653F2E10.1038/s43246-024-00708-9Received: 25 April 2024; Accepted: 30 November 2024;
Automated data extraction from materials science literature at scale using artificial intelligence and natural language processing techniques is critical to advance materials discovery.However, this process for large spans of text continues to be a challenge due to the specific nature and styles of scientific manuscripts.In this study, we present a framework to automatically extract polymerproperty data from full-text journal articles using commercially available (GPT-3.5) and open-source (LlaMa 2) large language models (LLM), in tandem with the named entity recognition (NER)-based MaterialsBERT model.Leveraging a corpus of ~2.4 million full text articles, our method successfully identified and processed around 681,000 polymer-related articles, resulting in the extraction of over one million records corresponding to 24 properties of over 106,000 unique polymers.We additionally conducted an extensive evaluation of the performance and associated costs of the LLMs used for data extraction, compared to the NER model.We suggest methodologies to optimize costs, provide insights on effective inference via in-context few-shots learning, and illuminate gaps and opportunities for future studies utilizing LLMs for natural language processing in polymer science.The extracted polymer-property data has been made publicly available for the wider scientific community via the Polymer Scholar website.</p>
<p>The field of materials informatics 1,2 suffers from lack of data readiness and data accessibility.Although materials data can be systematically generated through computational and physical experiments, a substantial amount of historical data is trapped in published literature.An ever-growing volume of data is continually released in scientific journal articles, but this data frequently exists in unstructured natural language text formats, posing challenges for immediate utilization by modern informatics that rely on the availability of structured datasets.Natural language processing (NLP) techniques implemented in materials science seek to automatically extract materials insights, materials properties, and synthesis data from a corpus of text documents, and propose hypotheses and designs for new materials [3][4][5] .After acquiring the corpus, a series of complex NLP operations are performed which include turning texts into smaller units called tokens, recognizing key entities (such as materials, characterization methods, or properties) using named entity recognition (NER) methods, creating rulebased algorithms to identify relationships between the entities through dependency parsing, and finally, extracting information and organizing it into a structured format 6 .</p>
<p>With the advent of modern machine learning (ML) and artificial intelligence techniques, deep learning models including recurrent neural networks and long short-term memory architectures have become valuable for NER tasks 7,8 .In recent years, the transformer-based BERT architecture, with its ability to capture contextual and semantic relationships within scientific texts 9 , has especially demonstrated superior performance compared to traditional neural network models 10 .We have previously developed and published MaterialsBERT 11 , a NER model derived from PubMedBERT 12 .This model demonstrated superior performance on publicly available datasets in comparison to other BERT models, including ChemBERT 13 and MatBERT 14 , particularly for materials science-specific data extraction tasks.By employing a MaterialsBERT-based pipeline, we successfully extracted over 300,000 polymer-property records from approximately 130,000 abstracts, the largest such undertaking at that time, with the data made publicly available 15 .This data extraction approach demonstrated the effectiveness of the MaterialsBERT model in processing a substantial volume of abstracts to obtain polymer-property information.The potential for large-scale data extraction using MaterialsBERT from the full texts of journal articles presents a further new opportunity for materials data acquisition.</p>
<p>While NER models excel in identifying named entities within texts, discerning entity relationships across extended passages encompassing multiple sentences solely through recognized named entities continues to be a challenge 16 .This limitation is particularly pronounced in technical and scientific documents, where critical information is often expressed in a nonstandard and complex manner.In the domain of polymer science, NERbased extraction methods encounter additional specific challenges stemming from the expansive chemical design space of the materials and the utilization of non-standard nomenclature, including commonly used names, acronyms, synonyms, and historical terms 17 .</p>
<p>Recently, large language models (LLMs) such as Generative Pretrained Transformer (GPT), Large Language Model Meta AI (LlaMa), Pathways Language Model, etc., have gained significant attention in the field of natural language processing 18,19 .These models have shown remarkable performance in handling various NLP tasks, showcasing their robustness and versatility 20 , especially in high-performance text classification, NER, and extractive question answering with limited datasets 21 .A key factor contributing to the success of the LLMs is the vast amount of 'knowledge' these models gain during semi-supervised pre-training (e.g., using masked language modeling to predict the next token given a set of preceding tokens for context) 22 .In the pre-training phase, LLMs acquire a foundational comprehension of language semantics and contextual understanding through exposure to training datasets, which typically comprise texts from general science and scientific literature 23 .Subsequently, the pre-trained LLMs, also referred to as base models, undergo supervised fine-tuning to produce desired text outputs in response to specific prompts or instructions.Examples include OpenAI Codex and Code LlaMa, both of which are finetuned to generate code snippets based on a given natural language input 24 .Similarly, ChatGPT and LlaMa Chat models are language models finetuned to respond to user prompts or instructions conversationally while maintaining a history of previous interactions for added context for the conversation.A human-like understanding of the language semantics and subsequent instruction tuning thus enable the LLMs to perform in-domain tasks such as information extraction about a specific material class with no (zero-shot) to only a few task-specific examples (few-shots).Such ability offers excellent performance and eliminates the efforts needed to create a labeled dataset of significant volume and train or fine-tune a new model 25 .</p>
<p>Despite the potential for many use cases including data extraction, the improved capabilities of the LLMs depend on access to significant computational resources.Using LLMs for inference incurs significant monetary costs, due to high demands of energy consumption, hardware or cloud computing time, and in terms of the environment, due to the carbon footprint of powering a number of modern tensor processing units 26,27 .Therefore, a data extraction pipeline aiming to efficiently utilize LLMs should extract the maximum amount of high-quality information and at the same time reduce the unnecessary prompting of the LLMs during the processing of millions of full-text scientific articles.</p>
<p>Limited prior works exist on the application of LLMs for data extraction in materials science.Dagdelen et al. fine-tuned GPT-3.5 and LlaMa 2 models to extract useful records of linking dopants and host metal-organic frameworks 28 .Zheng et al. developed a workflow utilizing ChatGPT as a collaborator for human chemists, extracting 26,257 distinct synthesis parameters of approximately 800 metal-organic frameworks from 228 articles 29 .Polak and Morgan proposed a similar workflow for metallic glasses and high entropy alloys, employing follow-up questions to GPT-4 to ensure correctness and address the issues of hallucinations with LLMs 30 .Similarly, Yang et al. used a repeated questioning strategy with GPT-4 for bandgap values, demonstrating reduced error rates and a more extensive dataset than human-curated databases 31 .GPT-based approach offered high-performance text classification, NER, and extractive question answering with limited datasets, and could reduce researcher workload by producing initial labelling sets and verifying human-annotations.</p>
<p>In this contribution, we present an approach to employing LLM-and NER-based pipelines, specifically designed to automate the extraction of property data of polymers from the full-text contents of journal articles.Our data extraction workflow, depicted in Fig. 1, processes a corpus of 2.4 million materials science journal articles published in the last two decades, from which, we identify and concentrate on 681,000 polymer-related articles.Subsequently, the paragraphs of the articles are processed through a dualstage filtering scheme consisting of a 'heuristic filter' and a 'NER filter' to identify the most relevant paragraphs that contain extractable property data.The materials and properties are identified, relationships are established, and the information is extracted in a structured format using Materi-alsBERT and GPT-3.5 models independently.Our pipelines extracted more than one million values of 24 selected properties from the full texts of the polymer-related articles.We have made the extracted data publicly available at polymerscholar.org(henceforth referred to as Polymer Scholar) where researchers can explore the distribution and relationships within the properties of polymers 15 .To identify the most efficient model, with a special focus on optimizing quality and costs, we evaluate three models -Materi- The NER filter identifies paragraphs with extractable named entities.The LlaMa-2 large language model was also evaluated, but was not used in the final data extraction pipeline due to comparatively low performance and long inference time, as described later in the text.</p>
<p>alsBERT, GPT-3.5, and LlaMa 2, across four critical performance categories: quantity, quality, time, and cost of data extraction.Our study undertakes a thorough examination of the capabilities of the LLMs, juxtaposing their performance against MaterialsBERT.We also present results offering insights into optimizing the performance and costs associated with using LLMs for data extraction via in-context few-shots learning and analyzing the general trends, characteristics, and distributions of the extracted full text data.We conclude by addressing the remaining challenges and looking ahead at the future potential of utilizing LLMs for informatics tasks specific to polymer science.</p>
<p>Results and discussion</p>
<p>Overview of the data extraction pipelines</p>
<p>We assembled a corpus comprising more than 2.4 million materials science journal articles published over the last two decades.The articles were initially indexed through the Crossref database, followed by authorized downloads from 11 publishers, including Elsevier, Wiley, Springer Nature, American Chemical Society, and the Royal Society of Chemistry.Further details regarding the articles can be found in the Methods section and in ref. 32.Specifically focusing on polymer-related content, we identified 681,000 documents by searching for the term 'poly' in the title and abstract of the articles.Extracting information from these polymer-related documents involved treating individual paragraphs as text units, resulting in a total of 23.3 million paragraphs.To extract data from the selected paragraphs, we targeted 24 properties of polymers based on their significance and downstream usage.Commonly reported thermal and optical properties were selected for their efficacy in training multi-task ML models, using highly correlated properties as substitutes for less prevalent ones.Additionally, properties that are beneficial for various polymer application areas were included.For instance, the bandgap and refractive index are vital for dielectric aging and breakdown, gas permeability properties are crucial for filtering and distillation applications, and mechanical properties are significant for thermosets and recyclable polymers.A list of the polymer properties selected for extraction can be found in Table 1.</p>
<p>A two-step filtering system was used to avoid unnecessary prompting of LLMs by ignoring texts that do not have extractable and complete data.First, as illustrated in Fig. 1b, each paragraph was passed through propertyspecific heuristic filters to detect paragraphs that mention a target polymer property or its co-referents manually curated via literature review.Approximately 2.6 million paragraphs (~11%) successfully passed the property-specific heuristic filters, indicating relevance to the selected 24 properties of polymers.Subsequently, an additional NER filter was applied to identify paragraphs containing all necessary named entities such as material name, property name, and property value (Fig. 1c) to confirm the existence of a complete extractable record.This refined filtering stage yielded about 716,000 paragraphs (~3%) containing texts relevant to the selected 24 properties.Regardless of the final data extraction model, the NER filter is utilized to verify the presence of 'material', 'property', 'value', and 'unit' entities in the given paragraph because the absence of any of these entities would preclude the extraction of a complete data point by the models.This filter thus assists LLMs in accurately identifying relationships without which they may generate placeholder values such as 'not mentioned', 'n/a', or '-', or even hallucinate false data if an entity is not present in the text.</p>
<p>The texts of the filtered paragraphs are then passed to either Materi-alsBERT for NER-based data extraction, or to the OpenAI API for GPT-3.5baseddata extraction.During the extraction process, the relationship extraction module of MaterialsBERT processes the identified entities to determine and establish correct relationships using heuristic rules.GPT-3.5, on the other hand, automatically identifies relationships between the entities by itself.Finally, the extracted data undergo post-processing, validation, and deposition into a relational database and the data stored in the database is made publicly accessible for visualization via a user-friendly web interface of Polymer Scholar.In total, the pipelines extracted over one million polymerproperty records from the full texts.Data extracted by GPT-3.5 (Materi-alsBERT) from the full text is approximately 21 times (12 times) than what was collected purely from the abstracts in our previous work for the selected 24 properties.Additional details about the different stages of the pipelines are discussed in the Methods section, with specific details on data extraction using MaterialsBERT provided in ref. 11.</p>
<p>Data extraction using large language models</p>
<p>Polymer-related property data from a journal article can be extracted by leveraging the ability of LLMs to understand the specialized semantics of materials entities discussed in the text.However, obtaining the desired output from an LLM poses a challenging task and is presently a subject of active research 33,34 .Even with the same prompt or instruction and text generation parameters, the LLMs can produce varying responses 35 .The effectiveness of simple natural language prompts in eliciting desired results from LLMs is not always straightforward, due to the models' interpretation methods often being non-intuitive 36 .Hence, it is crucial to employ techniques and parameters that can minimize the variability in the generated responses.One of the intriguing capabilities of LLMs, referred to as in-context few-shot learning, is their ability to learn from examples (often termed as 'shots') prepended to the prompt 25 .The response generation can be sufficiently influenced by the shots and the prompt given to the model as inputs.For few-shot learning, we used a pool of examples containing manually curated 595 glass transition temperature (Tg) values and 356 bandgap values from 630 abstracts.Given the necessity for the LLM's response to be presented in a structured format for seamless programmatic extraction of material names and property values, we experimented with different prompts.We finally selected one that directs the model to identify entities of interest, reading: "Extract all <property> values in JSONL format with 'material', 'property', 'value', 'condition' columns."The placeholder <property> is replaced with the desired polymer property names we chose to extract.A sample shot, shown in Fig. 2a, displays glass transition temperature data manually curated from the abstract of ref. 37 and the corresponding prompt.The formatted response in the example adheres to the JSONL structure, serving as a demonstration to the model that its generated response should precisely follow the same format.The shot is followed by the actual prompt containing the input text from which data needs to be extracted.Our similarity-based shot selection method, illustrated in Fig. 2e, provides a way to determine the most suitable example from the pool of examples for inclusion as a shot along with the LLM prompt.We first performed k-means clustering (with k = 10) on the word embeddings of the examples.The embeddings of the examples and input text were determined using the MaterialsBERT text encoder.Subsequently, we selected the example corresponding to the centroid of the cluster closest to the input text which is sent to the LLM for data extraction.This method, as opposed to random selection, allows us to choose an example that closely resembles the text from which data needs to be extracted.</p>
<p>To assess the monetary costs associated with text generation using GPT-3.5, we counted the number of OpenAI tokens in the input prompts and shots while extracting property data from the 630 manually curated abstracts.The tokenization process employed by the LLMs depends on the linguistic characteristics and contextual nuances of the words, numbers, punctuations, and symbols present in a given text.Our initial evaluations nevertheless reveal a direct correlation between the number of tokens and the word count within the selected abstracts, as depicted in Fig. 2f, which provides an approximate but simpler way to understand the effects of the text lengths on computational expenses associated with using GPT-3.5.</p>
<p>Costs during text generation directly increase with the number of words being processed.Furthermore, the introduction of multiple shots as an additional component in the input requires the LLM to consider extended textual inputs during response generation.A linear increase in both token utilization and corresponding API usage can be observed in Fig. 2g with an increasing number of shots used to extract data from the 630 abstracts.</p>
<p>Despite the expectation that the model's performance would improve with an increased number of shots, we consistently observed optimal results when providing only a single shot to GPT-3.5 while prompting for extraction of Tg and bandgap values (see Fig. 2h).A plausible explanation for this phenomenon could be that the model learns the structure of the anticipated output immediately from a single shot and experiences disorientation when additional shots are added to the prompt.Based on these observations, we proceeded to use one shot while prompting the LLM to extract data from a given text.</p>
<p>Performance benchmarking for a labeled subset of the full corpus We employ NER-and LLM-based extraction methods to comparatively assess the validity and reliability of different data extraction pipelines.The primary objective of this assessment is to identify the most effective extraction methods while emphasizing the optimization of computational and monetary costs.Consequently, the evaluation involves a subset of 1000 articles from the larger pool of the 681,000 polymer-related papers.In this subset, we have manually curated data from the abstracts of 630 articles, which reported one or more Tg and bandgap values in their abstracts and were selected randomly.The rest of the 370 articles were randomly chosen from the polymer papers.Bar charts containing the distribution of the selected articles compared to the full corpus are shown in Fig. S1.</p>
<p>The assessment pipeline, depicted in Fig. 3a, involves parsing the full texts of the selected papers into paragraphs, resulting in a total of 37,434 paragraphs.As discussed in the previous section, two filtering stages are employed to select the most relevant paragraphs for a target property and paragraphs containing extractable data.In the first stage, the propertyspecific heuristic filter is applied, resulting in a reduction of the paragraph count to 12 To determine the optimal shot for each prompt in the LLM-based data extraction process, we compared our strategy based on similarity to a random selection of shots from the curated data pool.We termed these approaches as "Similar" and "Random" shot selection methods, respectively.</p>
<p>In terms of quantity, (Fig. 3c, d) GPT-3.5 demonstrates significant superiority over the other models.It extracted the largest amount of data, comprising 4706 material-property value pairs using Random-shot and 4589 pairs using Similar-shot selection from the selected 6179 paragraphs.The NER-based MaterialsBERT pipeline extracted 3631 material-property pairs, slightly outperforming LLaMa, which extracted 3441 data pairs.</p>
<p>To assess extracted data quality, we checked if the extracted material name, property name, and property value (including unit) match the manually curated records (Fig. 3b) to calculate the F 1 scores.Our F 1 score computation methodology necessitates that the extracted data be completely present within the provided text.Thus, to ensure precise data extraction, it is imperative to identify all entities, specifically 'material', 'property', 'value', and 'unit' within the specified paragraph.Furthermore, to verify the correct relationships among these entities, they must correspond accurately with their respective entities in the curated dataset to account for hallucination by the LLMs.The F 1 score calculation method not only verifies the accuracy of the completely extracted data but also confirms their origin from the appropriate source paragraph, because any semantically accurate but fabricated data points produced by LLMs would be absent in the curated ground truth data extracted from the provided text.Additional details about the calculation of the F 1 score are discussed in the Supplementary Discussion.</p>
<p>We found that the performance of the models is contingent on the property being extracted.Both LLMs exhibit superior performance relative to the NER-based MaterialsBERT pipeline in extracting data for bandgap, while accuracy declines when extracting data for Tg (Fig. 3f, g).GPT-3.5 achieved the highest F 1 score of 0.67 for Tg, with the Similar-shot selection method slightly outperforming the Random selection method.Materi-alsBERT and LlaMa obtained F 1 scores of 0.63 and 0.64, respectively.LlaMa 2 outperformed the comparatively lower F 1 score of 0.66 achieved using MaterialsBERT, particularly during the extraction of bandgap, where it achieved an accuracy of 0.77.GPT-3.5 once again secured the highest F 1 scores of 0.87 and 0.85 for the Random and Similar-shot selection methods, respectively.We previously demonstrated the superior performance of MaterialsBERT compared to other existing NER models 11 .The higher F 1 scores compared to the MaterialsBERT obtained in this work thus underscores the advantage of the LLMs over the existing NER models.</p>
<p>Concerning computational efficiency and monetary costs, Materi-alsBERT emerges as the most advantageous choice (Fig. 3e).Operated inhouse, MaterialsBERT processed the 6179 paragraphs in under half hour without incurring any financial costs.LlaMa-2, also hosted locally, imposed no direct monetary costs but demonstrated the longest inference time, attributable to its substantial model size of 70 billion parameters running on four Nvidia Quadro GP 100 GPU cards.In contrast, the commercial LLM, GPT-3.5, required API calls to OpenAI's servers for inference, introducing a direct financial cost of $4.48 (for ~2.9 million tokens) for each of the shot selection methods (Fig. 3h).</p>
<p>This thorough evaluation allowed us to identify the best models for data extraction from the full corpus.Given GPT-3.5'ssuperior performance in both quantity and quality and MaterialsBERT's optimal cost efficiency, we chose to incorporate GPT-3.5 with Similar-shot selection and Materi-alsBERT in our final pipeline to extract data for all the selected properties from the entire corpus of polymer articles.</p>
<p>Data extraction from full texts</p>
<p>Having selected the best-performing models, we extracted data for the 24 selected properties from the full texts of 681, 000 polymer-related journal articles using the NER-based MaterialsBERT and the LLM-based GPT-3.5 pipelines.Given that neither pipeline can achieve perfect accuracy, and manually curating data sets for all 24 properties requires a significant effort, we conducted additional validation of the extracted data in a postprocessing step.After programmatically obtaining the data from the MaterialsBERT pipeline and JSONL responses generated by GPT-3.5, we verified if the property name matched one of the selected 24 property names or their known variations.We standardized the extracted values and units within the extraction pipelines, such as converting kPa or GPa to MPa, K to °C, etc. Subsequently, in the post-processing stage, we checked if the unit of the extracted data matched the unit corresponding to the selected property.Additionally, we assessed if the extracted value for each property fell within a specified minimum and maximum range, that was manually assigned based on literature review.We ignored any extracted data that does not satisfy these post-processing validation criteria.In addition to the polymers, the pipelines also extracted property data for other classes of materials.To identify the polymers, we checked if the extracted material is a valid polymer name by cross-referencing it with a comprehensive, albeit non-exhaustive, list of polymer names manually collected from the literature.</p>
<p>From the GPT-3.5 pipeline, we extracted 672,449 polymer-property records, and from the MaterialsBERT pipeline, we obtained 390,813 records for the 24 selected properties that passed the validation stage (see Table 1).In assessing the number of data points extracted for the selected properties, the GPT-3.5 pipeline demonstrated superior performance compared to the MaterialsBERT pipeline.Specifically, with the full text, the LLM extracted data volume was 72% greater than the data extracted by the NER pipeline, and 21 times the data extracted from abstracts (using the NER pipeline) in ref.</p>
<p>11.Among the extracted data, thermal and mechanical properties were more commonly found in the literature, while data on gas permeability was comparatively sparse.</p>
<p>Figure 4a, b illustrates the distribution of extracted Tg and bandgap data, respectively.For comparative purposes, we have also presented the distributions of Tg and bandgap data extracted from abstracts using MaterialsBERT which show a significantly higher amount of data extracted from the full texts.Though not obvious in the distribution of extracted data from abstracts, a bimodal distribution of Tg values and an elongated tail can be observed for all pipelines, demonstrating the presence of extreme property values.A comparative analysis of full-text extraction using MaterialsBERT reveals 75,722 valid Tg records, which is about 12 times the data extracted from abstracts earlier.This corresponds to 20,511 unique materials.Further, it provides 30,732 valid bandgap records, indicating 13 times the abstract extracted data and corresponds to 10,627 unique materials.In contrast, the GPT-3.5 pipeline provides a higher volume of valid records, amassing 125,585 Tg records, a 65% increase over MaterialsBERT and 20 times the data extracted from abstracts.This pertains to 69,740 unique materials.Similarly, GPT-3.5 yielded 106% more bandgap data than MaterialsBERT (63,361 records), which is 28 times the data obtained from abstracts, for a total of 31,337 unique materials.</p>
<p>Upon comparing the valid data points of Tg and bandgap that passed the post-processing criteria, we observed that each pipeline retrieved data from the source paragraphs where the other pipeline encountered difficulties (Fig. 4c).This supports the previously reported F 1 scores, demonstrating that GPT-3.5 is capable of understanding more intricate relationships and extracting more data.Notably, there is a significant number of paragraphs where MaterialsBERT fails (F 1 = 0.66), in contrast to the success of GPT-3.5 (F 1 = 0.85).Specifically, GPT-3.5 extracted data from an additional 47,966 paragraphs, from which MaterialsBERT failed to extract any valid data.Conversely, MaterialsBERT successfully extracted data from 7311 paragraphs where the GPT-3.5 pipeline did not retrieve any valid data.</p>
<p>The combined data extraction efforts of both pipelines resulted in a total of 113,099 unique materials for Tg and bandgap.Notably, only 11,042 material names exactly matched between the pipelines, as illustrated in Fig. 4d, and parity plots showing overlap between the pipelines in Fig. S2.GPT-3.5 demonstrated its proficiency by extracting an additional 88,128 material names that were not captured by MaterialsBERT.This significant increase in the identification of new material names can be attributed to GPT-3.5's inclination towards detailed extraction of composition and types while extracting material names (see Table S3).It is important to mention that variations in naming and co-referents of polymers were considered correct during the calculation of the F 1 score.Consequently, although there is a low overlap of the names exactly extracted by the methods as depicted in Fig. 4d, a high F 1 score is still maintained.For comparison, similar Venn diagrams for Tg and bandgap data extracted from the 630 abstracts are shown in Fig. S1.Upon further investigation, we found that MaterialsBERT in some cases faced challenges in correctly associating property and material names in texts filled with numerous numbers and values.This was particularly prevalent in cases involving polymer blends or composites, where MaterialsBERT often extracted redundant values for each polymer.As a result, the total amount of data extracted by the MaterialsBERT pipeline is often erroneously inflated for many ordinary polymers, as shown in Fig. 4e.GPT-3.5 exhibited significantly enhanced efficiency in accounting for composite and blend compositions in the sentences.Examples elucidating the F-1 score calulcation method and anomalies of MaterialsBERT are discussed in the Supplementary Discussion.</p>
<p>We plotted the pairwise distributions where values of two properties were available for the same material name and were extracted from the same source article.Fig. 4f depicts the relationship between bandgap and Tg data extracted by the pipelines.GPT-3.5 demonstrated the ability to capture more data including numerous extreme property values, while Materi-alsBERT successfully captured values towards the center of the distribution.Additionally, both methods illustrate the scarcity of materials exhibiting high bandgaps and simultaneously high Tg values.Six data points with high bandgap or Tg values, as determined by the pipelines, are identified and numbered in Fig. 4f.The pipelines correctly extracted points 1, 2, and 3.In the case of point 1, GPT-3.5 managed to extract more detailed information, identifying both 'non-oriented PMMA layer' and 'oriented PMMA layer' instead of merely labeling it as 'PMMA'.Neither GPT-3.5 nor Materi-alsBERT were successful in extracting any bandgap data for points 2 and 3, respectively.With regard to point 4, GPT-3.5 incorrectly identified the thermal decomposition temperature as the Tg value.Meanwhile, Materi-alsBERT was unable to extract the high bandgap value.Points 5 and 6 were identified as a composite and blend, respectively.However, MaterialsBERT only managed to extract the polymer names, failing to recognize the presence and modifications by other materials.Despite the observation that GPT-3.5 typically extracts the compositions of polymer composites and blends, it did not identify the presence of other materials in this instance and only extracted the polymer names due to the complexity of the sentences in the source text.Out of the total 12 values for the designated property pairs, 10 were accurately extracted.Supplementary Information, including references and actual values of the marked points, can be found in Tables S2 and S3.</p>
<p>Correlations between extracted properties</p>
<p>Knowledge of the relationships between distinct material properties can be gained by examining the pairwise distributions among other property pairs.Representative pairwise plots indicate diverse trends, with Fig. 4g highlighting a discernible positive correlation between crystallization temperature and Tg.The extracted data highlights an inverse correlation between water contact angle and water uptake (Fig. 4h), confirming that hydrophilic materials with smaller contact angles tend to absorb greater amounts of water.Additionally, the bandgap determines the energy above which a material remains transparent.As the light wavelength decreases towards the bandgap, there is a corresponding increase in the refractive index.When the light wavelength is held constant, materials with a larger bandgap generally exhibit a smaller refractive index.The inverse trend depicted in Fig. 4i illustrates this relationship between the optical properties of the materials extracted.Another fundamental observation for polymers is the inherent trade-off between mechanical properties.Fig. 4j elucidates the negative trade-off between tensile strength and elongation at break, emphasizing the capacity of GPT-3.5 to capture additional values.The phenomenon of water absorption induces polymer chain swelling, thereby instigating plasticizing effects that can alter the mechanical properties of the material.Although the specific nature of the relation depends on the material in question, Fig. 4k suggests a pervasive negative trend wherein water uptake causes a reduction in the tensile properties of materials.Overall, the extracted data largely follow expected trends and agree with domain knowledge as demonstrated by the pairwise distributions.</p>
<p>The datasets we extracted from literature containing the 24 properties are integral for training downstream ML models.In a previous study, we employed similar datasets to optimize the material system and develop robust predictive models to optimize power conversion efficiency of polymer solar cells and demonstrated a significant reduction in new polymer discovery time 39 .In another study, we assembled a comparable dataset to predict the retrosynthesis pathways for a target polymer 40 .</p>
<p>Outlook</p>
<p>LLMs, such as GPT-3.5 with likely over 200 billion parameters, show a marked advantage in data extraction quality and ease due to their pretraining on extensive text corpora, even without fine-tuning on domainspecific datasets.The efficacy of pre-training is highlighted by GPT-3.5'sproficiency in recognizing material and chemical entities.The LlaMa-2 model, with 70 billion parameters, demonstrates comparatively limited capability in recognizing chemical entities and establishing correct entity relationships, hinting at consideration for potential improvement through fine-tuning on labeled datasets.The challenges specific to polymer literature for NER-based models are marked by the absence of a standardized naming convention for polymers and the requirement for manual efforts to identify entity relationships.</p>
<p>Despite the promising performance of GPT-3.5, various limitations still exist for the extraction of data from polymer literature and their applications in polymer informatics.We discuss some specific issues and our goals for improvement below.</p>
<p>• Manual conversion is necessary to transform the extracted material names into machine-readable formats such as Simplified Molecular Input Line Entry System (SMILES) strings to make the datasets informatics-ready.Despite the incorporation of LLMs into the data extraction pipeline, the parsing of chemical structures from figures remains a significant challenge, particularly in polymer-related studies.This is due largely to the fact that polymer structures are often exclusively presented in figures, which obstructs the direct extraction and conversion of polymer chemistry into machine-readable SMILES strings from the text.In the future, integration of large-scale computer vision models with LLMs to efficiently identify and extract polymer molecules depicted in figures will enable immediate use of the extracted data for training ML models without the need for additional manual processing.• The intricate nature of scientific texts, particularly in introducing material names across different sections and using abbreviations, makes establishing correct relationships between entities mentioned in different paragraphs or even sentences a difficult task.Our current pipelines extract data that is described completely in a specific paragraph by looking for all the required named entities (i.e., 'material', 'property', 'value' and 'unit') to establish correct relationships.However, the properties of polymers often rely on further information, such as molecular weights, temperature, synthesis and processing conditions, and morphology.This additional data also needs to be extracted from multiple paragraphs, while ensuring the preservation of valid relationships.Using a specific example of Fig. 2d, where one of the extracted materials is simply labeled as a 'copolymer,' it becomes challenging to fully extract the actual name or chemistry of the material without inputting the full text of the article into the LLM or correctly identifying the first occurrence of the term using other means.However, feeding the entirety of text contents into larger context lengths of the latest LLMs, even if possible, is fundamentally inefficient and a squandering of computational resources.In addition, the outputs derived from conversational LLMs often exhibit inconsistency, necessitating manual effort for conversion into structured formats.Formulation of a robust chemical entity relationship extraction strategy that leverages both NER and LLM, could markedly augment the quality and application of the extracted data.• Extraction of materials data present in tabular formats can further enhance the utility of comprehensive data extraction tasks.Another significant source of data often originates from the supplementary information published alongside articles.These documents are typically available in portable document format (PDF), which poses a challenge for parsing due to the lack of standardization in document creation 41 .While values with heightened scientific significance are usually mentioned in the main text, the presence of a substantial amount of extractable and relevant data in tabular format and supporting documents has the potential to greatly improve the performance of downstream data-hungry ML models.However, tables are frequently arbitrarily structured, necessitating meticulous filtering, classification, and pre-processing for correct relationship establishment 42,43 .• Extracting property data from literature represents a specific application of NLP in polymer research.Accurate predictions of step-by-step tasks and procedures, such as synthesis recipes, characterization data, measurement conditions, etc., could guide the development of superior polymers through inverse design and suggest specific conditions that researchers could maintain to produce a target material.Synthesis recipes, for example, present a unique challenge due to the need to extract a diverse set of information, including monomers, catalysts, temperature, reaction conditions, and more.Additionally, the chemical reactions must be predicted algorithmically, maintaining proper order of the procedure.Despite these challenges, the capacity of LLMs to comprehend complex procedures offers a promising avenue for systematically extracting such information.</p>
<p>Nevertheless, it is evident that the introduction of LLMs such as GPT-3.5 is a significant leap forward in the field of data extraction, particularly in complex domains like polymer literature.Future work may involve finetuning the model to concurrently handle searching, filtering, NER and data extraction tasks.As advancements in smaller open-source models persist, we anticipate the potential to substitute our entire workflow with a single, fine-tuned, open-source LLM.Such a system could democratize the process of extracting accurate data from literature.However, this transition would require comprehensive analysis and validation by the materials science community.Our focus will not only be on refining data extraction workflows but also on ensuring the availability of the extracted data for inspection through resources such as Polymer Scholar.</p>
<p>Conclusion</p>
<p>In conclusion, this study presents a framework for automated extraction of polymer property data from full-text scientific literature, utilizing a combination of NER-based MaterialsBERT and GPT-3.5 LLMs.The approach has demonstrated a significant improvement in data extraction capabilities, yielding 21 times more data than previously extracted from just the abstracts of journal articles.A comparative analysis of the performance and costs associated with both extraction models was also conducted.The GPT-3.5 and MaterialsBERT models achieved F 1 scores of 0.67 and 0.63 respectively for Tg, and 0.85 and 0.66 respectively for bandgap.While traditional NER models offer speed, the LLMs, although more costly, provide ease of use and require less manual effort, making them an attractive alternative.GPT-3.5 also shows a marked improvement in recognizing materials entities and correct entity relationships, particularly in the context of polymer composites and blends.However, both models encountered difficulties in extracting correct values from texts containing complex discussions about materials.The potential of LLMs lie in their ability to produce not just structured outputs such as property data extraction, but also assistance with property predictions, material design and synthesis recipe recommendations.</p>
<p>Methods</p>
<p>The literature corpus Our literature corpus consists of ~2.4 million documents downloaded from publishers including Elsevier, Wiley, Springer Nature, American Chemical Society, and Royal Society of Chemistry which covers articles published up to the year 2021.Only HTML and XML versions of the documents were processed in this work as formats such as PDF are difficult to parse 44 .Literature published before the year 2000 is often found in PDF format while XML and HTML versions are available from most publishers after 2000.The details of the workflow is discussed in ref. 32.</p>
<p>Full-text extraction</p>
<p>Plain text paragraphs embedded inside the p, span, or similar tags of the HTML and XML documents were extracted using the LXML Python package.Both the abstracts and the full texts available in the body of the documents were collected in the process.Subscripts and superscripts in the text were encoded by the underscore and the caret symbols, respectively.No additional pre-processing was performed since the LLMs are generally able to 'understand' chemical entities, units, and literature references.</p>
<p>Property-specific heuristic filter</p>
<p>The paragraphs extracted from full texts of the journal articles underwent a two-stage filtering process before being sent to the data-extraction pipeline.The heuristic filter assessed the text for the occurrence of the property name or any of its known variations, employing string-matching and dictionary lookup techniques.The property name variations were carefully curated through an extensive literature search.</p>
<p>NER filter NER-based filtering was used to identify the paragraphs that contain data suitable for extraction.First, the named entities present in the paragraph are predicted using MaterialsBERT.The filter then selects the paragraphs that have (1) at least one of the material-related named entities, (i.e., "POLY-MER", "MONOMER", "POLYMER_FAMILY", "ORGANIC", "INOR-GANIC") (2) the "PROPERTY_NAME" entity and (3) the "PROPERTY_VALUE" entity present in the text.If the text does not meet all three conditions, the paragraph fails the filter.This allows the filter to select paragraphs with data for all possible materials and properties known by MaterialsBERT and are suitable for extraction.</p>
<p>LlaMa model</p>
<p>An instruct-tuned 70B LlaMa-2-chat model was used in this work to extract data from texts.The architecture of the LlaMa model was obtained from the HuggingFace hub using the transformers Python package.The corresponding LlaMa 2 weights were requested and obtained from the official website of Meta AI.A 4-bit GPTQ quantized (group size 32, with act order) version of the model was run on four 235W 16GB Nvidia Quadro GP100 GPUs hosted in our in-house computing servers.For text generation parameters, the temperature was set to 0.001, top_p to 0.95, min_p to 0, frequency_penalty to 1.1, and top_k to 1.The maximum output length was automatically computed each time before inference, so the total number of tokens for the prompt and the generated output remains less than the context length of the model, i.e., 4096.The use of a conversational model did not involve incorporating history from prior interactions during text generation.Consequently, the model treated each prompt as a distinct text generation request.</p>
<p>GPT model</p>
<p>The GPT-3.5-turbo-0613 model hosted by OpenAI was used to extract data from text.The OpenAI Python package was used to access the OpenAI API.</p>
<p>The temperature parameter was set to 0.001 for text generation by the model, with all other parameters remaining at their default values.Similar to the LlaMa 2 model, a history of previous interactions was not maintained between the API requests.The manually curated Tg and bandgap data were used as a shot to the LLM regardless of the property to be extracted.The extraction process from the full texts of the 716,000 paragraphs took approximately a month (respecting the guidelines for fair usage of the API server), incurring approximately 1200 US dollars.The API usage costs were calculated assuming 0.0015 and 0.0020 US dollars per one thousand prompt tokens and completion tokens respectively.</p>
<p>Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material.You do not have permission under this licence to share adapted material derived from this article or parts of it.The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.To view a copy of this licence, visit http://creativecommons.org/licenses/bync-nd/4.0/.</p>
<p>Fig. 1 |
1
Fig. 1 | Overall workflow to extract polymer property data.a Polymer-specific documents are selected from a corpus of 2.4 million materials science journal articles.Multiple stages of filtering select the most relevant documents and paragraphs of the documents before performing data extraction by MaterialsBERT and GPT-3.5.Extracted data are finally deposited to a relational database of the Polymer Scholar web interface.b Property-specific paragraphs are selected by a heuristic filter based on string matching and dictionary lookup.cThe NER filter identifies paragraphs with extractable named entities.The LlaMa-2 large language model was also evaluated, but was not used in the final data extraction pipeline due to comparatively low performance and long inference time, as described later in the text.</p>
<p>Fig. 2b provides an illustrative example of such a prompt, comprising a paragraph taken from the full text of ref. 38, along with specific instructions given to the LLM for data extraction.The resulting response from GPT-3.5 is shown in Fig. 2c, revealing three data points of glass transition temperature values extracted by the LLM.</p>
<p>Fig. 2 |
2
Fig. 2 | Data extraction using large language models.a Example of a shot generated from the text of a manually curated dataset containing glass transition temperature and bandgap values.b Prompt used to extract data using LLM.c Response generated by GPT-3.5 and d Tg data extracted from the response text.e Schematic illustration of clusters formed by word embeddings of the texts of 630 manually curated abstracts and choice of the shot most similar to the text from which data is to be extracted.fCorrelation between token count and number of words in 630 abstracts related to polymers.The positive y-intercept is attributable to the average expected number of punctuation marks and symbols typically found in the texts.g Barchart depicting an increase in the number of tokens and the ultimate cost for OpenAI API usage for multiple shots.h Effects of multiple shots on the accuracy of the extracted Tg and bandgap data from the manually curated 630 abstracts.</p>
<p>,817.Subsequently, the second stage utilizes the MaterialsBERTbased NER filter, further narrowing the selection to 6179 paragraphs.The final data extraction process involves three models: MaterialsBERT, incorporating NER and rule-based entity recognition and relationship extraction; the open-source 70 B LlaMa-2 model developed by Meta AI; and the commercially available GPT-3.5 model hosted by OpenAI.</p>
<p>Fig. 3 |
3
Fig. 3 | Performance evaluation of NER and LLM pipelines.a Overview of the pipelines used to measure the performance of MaterialsBERT, LlaMa-2, and GPT-3.5 for data extraction from 1000 polymer documents.b Representation of F 1 score measurement using manually curated data.c, d Total number of materials and property data extracted using the pipelines, e time spent running the pipelines, f, g calculated F 1 scores for Tg and bandgap respectively and h incurred cost due to API usage by the pipelines.</p>
<p>Fig. 4 |
4
Fig. 4 | Comparison of GPT and MaterialsBERT extracted data.a, b Distribution of extracted Tg and bandgap data from the abstracts using MaterialsBERT and full texts of polymer articles using both GPT-3.5 and MaterialsBERT pipelines.c, d Overlap between the source paragraphs and extracted material names in GPT-3.5 and MaterialsBERT extracted Tg and bandgap data.e Barchart showing the total number of data extracted from the full texts for the top 10 polymers using the two pipelines.f Pairwise plot of bandgap and Tg values for the materials extracted from the same source articles, marked points are discussed in the text.g-k Representative pairwise plots showing relationships between selected pairs of properties.The dashed lines are guides for the eye.</p>
<p>Table 1 |
1
List of the selected 24 property names and the corresponding number of property values extracted using the GPT-3.5 and MaterialsBERT pipelines
PropertyGPT-3.5MaterialsBERTMaterialsBERTFull textFull textAbstractGlass transition125,58575,722temperatureMelting temperature76,57741,766Thermal70,28519,817decompositiontemperatureLower critical20,11511,658solution temperatureCrystallization12,8634045temperatureThermal conductivity557410,300Upper critical1486581solution temperatureBandgap63,36130,732Ion exchange31184656capacityRefractive index18,9829785Tensile strength63,01438,773Young's modulus40,14832,207Elongation at break30,75415,072Compressive12,3436879strengthFlexural strength72013543Hardness52711984Water contact angle84,60163,685Water uptake15,9916019Limiting78936606oxygen indexCO 2 permeability29432561Swelling degree18801995Methanol1228852permeabilityO 2 permeability735503H 2 permeability5011072Total672,449390,81331,539
The number of data records extracted from the abstracts alone using MaterialsBERT are taken from ref. 11 for comparison against the full text data extraction.</p>
<p>Communications Materials | (2024) 5:269
© The Author(s) 2024 https://doi.org/10.1038/s43246-024-00708-9
AcknowledgementsThis work was supported by the Office of Naval Research through grants N00014-19-1-2103 and N00014-20-1-2175.Pranav Shetty was partially funded by a fellowship by JPMorgan Chase &amp; Co. that helped to support this research.Any views or opinions expressed herein are solely those of the authors listed, and may differ from the views and opinions expressed by JPMorgan Chase &amp; Co. or its affiliates.Data availabilityThe journal articles used to extract material property data were downloaded through licensing arrangements that the Georgia Institute of Technology has with Elsevier, Wiley, Royal Society of Chemistry, American Chemical Society, Springer Nature, Taylor &amp; Francis, and the American Institute of Physics.The pre-trained language model MaterialsBERT is available in the HuggingFace hub at https://huggingface.co/pranav-s/MaterialsBERT.The material property data extracted in this work can be freely explored through https://polymerscholar.org.Code availabilityThe code used in this work can be found at https://github.com/Ramprasad-Group/PromptDataExtraction.Author contributionsConceptualization: R.R. Data curation: A.A., P.S., and S.G.MaterialsBERT pipeline: P.S. LLM pipelines: A.M., P.S. Database design: A.M., S.G.Polymer Scholar: A.M., P.S. Visualization: A.M. Original draft: A.M., S.G.Review &amp; editing: S.G., A.M., P.S., and R.R.Competing interestsThe authors declare no competing interests.Additional informationSupplementary information The online version contains supplementary material available at https://doi.org/10.1038/s43246-024-00708-9.Correspondence and requests for materials should be addressed to Rampi Ramprasad.Peer review information Communications materials thanks Byungju Lee, Zhiling Zheng and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.Primary Handling Editors: Milica Todorović and Aldo Isidori.A peer review file is available.Reprints and permissions information is available at http://www.nature.com/reprintsPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Machine-learning predictions of polymer properties with Polymer Genome. H Doan Tran, J. Appl. Phys. 1281711042020</p>
<p>Dielectric polymers tolerant to electric field and temperature extremes: integration of phenomenology, informatics, and experimental validation. C Wu, ACS Appl. Mater. Interfaces. 132021</p>
<p>Opportunities and challenges of text mining in materials research. O Kononova, iScience. 241021552021</p>
<p>Automatic extraction of materials and properties from superconductors scientific literature. L Foppiano, Sci. Technol. Adv. Mater.: Methods. 321536332023</p>
<p>PolyIE: a dataset of information extraction from polymer material scientific literature. J J Cheung, 2023</p>
<p>Emerging materials intelligence ecosystems propelled by machine learning. R Batra, L Song, R Ramprasad, Nat. Rev. Mater. 62021</p>
<p>Machine extraction of polymer data from tables using XML versions of scientific articles. H Oka, A Yoshizawa, H Shindo, Y Matsumoto, M Ishii, Sci. Technol. Adv. Mater.: Methods. 12021</p>
<p>Text-mined dataset of inorganic materials synthesis recipes. O Kononova, Sci. Data. 62032019</p>
<p>Deep learning of electrochemical CO2 conversion literature reveals research trends and directions. J Choi, J. Mater. Chem. A. 112023</p>
<p>. I Beltagy, K Lo, A Cohan, Scibert, 2019</p>
<p>A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing. P Shetty, npj Comput Mater. 92023</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, ACM Trans. Comput. Healthc. 32022</p>
<p>ChemBERTa: largescale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, 2020</p>
<p>Quantifying the advantage of domain-specific pretraining on named entity recognition tasks in materials science. A Trewartha, Patterns. 31004882022</p>
<p>Polymer scholar. A Mahmood, G Sonakshi, P Shetty, 2024</p>
<p>Data-driven materials research enabled by natural language processing and information extraction. E A Olivetti, Appl. Phys. Rev. 7413172020</p>
<p>Machine-guided polymer knowledge extraction using natural language processing: the example of named entity normalization. P Shetty, R Ramprasad, J. Chem. Inf. Model. 612021</p>
<p>Llama 2: open foundation and fine-tuned chat models. H Touvron, 2023</p>
<p>PaLM: scaling language modeling with pathways. A Chowdhery, 2022</p>
<p>Assessing the strengths and weaknesses of large language models. S Lappin, 10.1007/s10849-023-09409-xJ. Log. Lang. Inf. 2023</p>
<p>Accelerating materials language processing with large language models. J Choi, B Lee, Commun. Mater. 52024</p>
<p>Pre-trained language models and their applications. H Wang, J Li, H Wu, E Hovy, Y Sun, Engineering. 252023</p>
<p>Y Liu, J Cao, C Liu, K Ding, L Jin, Datasets for large language models: a comprehensive survey. 2024</p>
<p>Code Llama: open foundation models for code. B Rozière, 2023</p>
<p>Language models are few-shot learners. T B Brown, 2020</p>
<p>Energy and policy considerations for deep. E Strubell, A Ganesh, A Mccallum, 2019</p>
<p>Estimating the carbon footprint of BLOOM. A S Luccioni, S Viguier, A.-L Ligozat, 2022</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, Nat. Commun. 1514182024</p>
<p>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, J. Am. Chem. Soc. 1452023</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, Nat. Commun. 1515692024</p>
<p>Accurate prediction of experimental band gaps from large language model. S J Yang, 2023</p>
<p>Automated knowledge extraction from polymer literature using natural language processing. P Shetty, R Ramprasad, 202124101922</p>
<p>ChatGPT and large language models in academia: opportunities and challenges. J G Meyer, BioData Min. 16202023</p>
<p>A simple but effective approach to improve structured language model output for information extraction. Y Li, R Ramprasad, C Zhang, 2024</p>
<p>Qualitative research methods for large language models: conducting semi-structured interviews with ChatGPT and BARD on computer science education. A Dengel, Informatics. 10782023</p>
<p>Large language models are human-level prompt engineers. Y Zhou, 2023</p>
<p>Investigation of glass transition behaviours in aromatic poly(amic acid) precursors with various chain rigidities by oscillating differential scanning calorimetry. S I Kim, S M Pyo, K Kim, M Ree, Polymer. 391998</p>
<p>Synthesis, thermal stability and kinetic decomposition of triblock copolymer polypropylene glycol-poly glycidyl nitrate-polypropylene glycol (PPG-PGN-PPG). T Khanlari, Y Bayat, M Bayat, Polym. Bull. 772020</p>
<p>Accelerating materials discovery for polymer solar cells: data-driven insights enabled by natural language processing. P Shetty, A Adeboye, S Gupta, C Zhang, R Ramprasad, Chem. Mater. 362024</p>
<p>Data-assisted polymer retrosynthesis planning. L Chen, J Kern, J P Lightstone, R Ramprasad, Appl. Phys. Rev. 8314052021</p>
<p>PDFDataExtractor: a tool for reading scientific text and interpreting metadata from the typeset literature in the portable document format. M Zhu, J M Cole, J. Chem. Inf. Model. 622022</p>
<p>Reconstructing the materials tetrahedron: challenges in materials information extraction. K Hira, M Zaki, D Sheth, N M Anoop Krishnan, Digital Discov. 32024</p>
<p>DiSCoMaT: distantly supervised composition extraction from tables in materials science articles. T Gupta, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Challenges in information-mining the materials literature: a case study and perspective. A Smith, V Bhat, Q Ai, C Risko, Chem. Mater. 342022</p>            </div>
        </div>

    </div>
</body>
</html>
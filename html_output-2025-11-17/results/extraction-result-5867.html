<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5867 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5867</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5867</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-07c70ca55793984ffdf31582a05170ef3d62381a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/07c70ca55793984ffdf31582a05170ef3d62381a" target="_blank">Prompt Consistency for Zero-Shot Task Generalization</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work takes advantage of the fact that multiple prompts can be used to specify a single task, and proposes to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts, to improve zero-shot performance.</p>
                <p><strong>Paper Abstract:</strong> One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al., 2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5867.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5867.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Swarm Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Swarm Distillation (Prompt Consistency Regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised prompt-consistency training method that enforces consistent predictions across multiple synonymous prompts by pairwise distillation among prompts; applied as post-adaptation (training-time or test-time) using unlabeled examples and parameter-efficient tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot classification across 11 NLP datasets (NLI, sentence completion, coreference, word sense disambiguation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot prompting: apply prompt templates to inputs and predict discrete labels (examples: RTE, ANLI R1/R2/R3, CB, COPA, HellaSwag, Story Cloze, Winogrande, WSC, WIC).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple synonymous natural-language prompts (each with an input template r_x and an output template r_y plus prompt-specific metadata like 'Choices') are applied to each unlabeled input; model is trained to be consistent via a pairwise distillation loss that treats each prompt's prediction as a pseudo-target for another prompt. Evaluation uses ensemble (averaged output distributions across prompts) and median (single-prompt median) metrics. Two adaptation settings: (1) training-time tuning on unlabeled training split; (2) test-time tuning on unlabeled test inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to: (a) T0 baseline (no unsupervised adaptation), (b) Self-distillation baseline (each prompt distills to itself), (c) Full fine-tuning, (d) parameter-efficient tuning with LoRA and model-selection ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>T0-3B baseline ensemble accuracies vary by dataset (e.g., RTE 64.6%); Swarm Distillation (train) improves many datasets (RTE ensemble 75.2% ±0.8, up +10.6 absolute points; HellaSwag 34.2% ±0.2, up +6.4; WIC 55.4% ±1.1, up +5.1). On T0-11B (test-time tuning) swarm distillation improves HellaSwag from 34.4% to 45.0% (+10.6) and yields gains on RTE, WIC, WSC (see Table 2). Overall, swarm distillation outperforms T0-3B on 9/11 datasets in the main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to self-distillation: swarm distillation outperforms self-distillation on 9/11 datasets (ensemble accuracy) by up to ~10.3 absolute points. Compared to full fine-tuning: full fine-tuning often collapses and hurts performance unless combined with unsupervised model-selection; combining LoRA + model-selection yields similarly large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+10.6 absolute points (largest observed improvement; RTE for T0-3B and HellaSwag for T0-11B). Average Fleiss' kappa (prompt agreement) increased relatively by 14.6% across datasets after swarm distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper hypothesizes and demonstrates that PLMs are sensitive to prompt wording and that regularizing consistency across multiple synonymous prompts encourages robustness: pairwise distillation makes predictions invariant to prompt-level reformatting. Using parameter-efficient tuning (LoRA) mitigates catastrophic forgetting, and Fleiss' kappa as an unsupervised selection metric penalizes trivial collapsed solutions (where all prompts predict the same label). Ensembles over prompts further reduce variance from prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>On WSC swarm distillation produced a large ensemble accuracy drop (≈−10.9 points in one setting) due to model-selection choosing a bad checkpoint; oracle model-selection (using labeled validation) shows that swarm distillation actually had improvements in the middle of training, indicating failures in unsupervised checkpoint selection rather than method failure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5867.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self Distillation (single-prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self Distillation Baseline (prompt self-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline ablation where the same prompt distills its own predictions (r^i = r^j), i.e., pseudo-label self-training per prompt rather than pairwise across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same zero-shot classification tasks as Swarm Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Unsupervised pseudo-labeling where each prompt's own predicted distribution is used as target for itself (no cross-prompt consistency imposed).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single-prompt self-distillation: the prompt formats remain the same but the distillation pairs are identical prompts (no prompt-to-prompt transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared directly against swarm distillation (pairwise prompt distillation) and the T0 baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Self-distillation shows small or negligible improvements over T0 baseline in many datasets (e.g., RTE ensemble 64.9% vs T0 baseline 64.6%), but is outperformed by swarm distillation on 9/11 datasets. See Table 1 (self-distillation columns).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Swarm Distillation outperforms self-distillation on 9/11 datasets by up to ~10.3 absolute points (ensemble accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to ~+10.3 absolute points advantage for pairwise (swarm) vs self-distillation (dataset dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>swarm (multi-prompt) improved over self (single-prompt) ; self-distillation alone generally provided only minor gains.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Pairwise cross-prompt distillation leverages diverse prompt views to propagate stronger pseudo supervision, whereas self-distillation lacks cross-view regularization and thus is weaker at improving robustness to prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5867.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Ensembling (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Ensemble (averaging output distributions across prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>At inference time, average the predicted probability distributions from multiple prompt templates and predict according to the averaged distribution; reported as 'ensemble' accuracy and compared to median single-prompt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same zero-shot classification tasks (11 datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Inference method combining multiple prompt outputs to produce a single prediction for each example.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Apply K prompt templates to each example; average resulting predicted label distributions to obtain ensemble prediction. Also report median of per-prompt accuracies as proxy for single-prompt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared ensemble (Ens.) vs median single-prompt (Med.) across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ensemble accuracy is generally higher than median single-prompt accuracy across most datasets and both baseline and adapted models. Example: T0-3B RTE Ens 64.6% vs Med 64.1% (baseline); swarm-distillation ensemble typically higher than median. Specific dataset values are in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Ensembling improves robustness relative to single-prompt median in most cases (dataset-dependent), and is the preferred evaluation metric in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Dataset dependent; ensemble usually > median (often a few percentage points), exact per-dataset deltas provided in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different prompts produce different biases/noise; averaging (ensembling) reduces variance due to prompt wording and improves expected accuracy over selecting a single prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Not universally larger for every dataset (e.g., some datasets or specific seeds may show median near or exceeding ensemble), but ensemble is generally better overall as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5867.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Count Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation: Effect of Number of Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis varying the number of available prompt templates used for swarm distillation; experiments show performance improves as prompts increase but saturates quickly (around 4 prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>COPA and ANLI R2 (examples used for ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure ensemble accuracy while varying the number of prompts sampled from the dataset's prompt pool (COPA has 8 prompts, ANLI R2 has 15 prompts available).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Subset of prompt templates applied (from 0 to full set); swarm distillation trained on that subset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>0 prompts (baseline, no adaptation) vs 1,2,... up to full K prompts; examine curve and saturation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance increases as number of prompts increases from 0; stabilizes when approximately 4 prompts are available. Exact ensemble accuracy curves in Figure 4a (qualitative improvement visible for both COPA and ANLI R2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Most of the benefit is achieved with a small number of prompts; beyond ~4 prompts marginal gains are small.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative saturation by ~4 prompts; exact numeric improvements depend on dataset (figures show clear early gains).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved up to saturation</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Diverse prompt views provide complementary pseudo-label signals; a small set of diverse prompts suffices to regularize model predictions effectively, so the method is not 'prompt-hungry'.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5867.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unlabeled Data Size Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation: Effect of Number of Unlabeled Examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of how many unlabeled inputs are required for swarm distillation to be effective; shows large gains with very few examples (tens), saturating quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WIC and ANLI R2 (example datasets for ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Randomly sample subsets of the training set (varying counts of unlabeled examples) for unsupervised training and evaluate ensemble accuracy on the validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Vary number of unlabeled examples used for swarm distillation from 0 up through hundreds/10k (limited by dataset); apply multiple prompts per example during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>0 unlabeled examples (baseline) vs small numbers (e.g., 10) vs larger numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Swarm distillation outperforms baseline with as few as 10 unlabeled examples on WIC and ANLI R2; performance typically saturates after tens-to-hundreds of examples (see Figure 4b).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Significant gains with 10 examples relative to 0; further gains smaller beyond tens/hundreds of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative 'large gain' with only ~10 unlabeled examples; no single global absolute value provided (dataset dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved with small unlabeled set; saturates thereafter</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompt-consistency loss effectively propagates supervision from few unlabeled examples due to constraints imposed by multiple prompt views; thus method is sample-efficient for adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5867.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Wording Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Wording Sensitivity (Jiang et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior observation cited: model predictions are empirically very sensitive to the exact wording of task prompts, motivating prompt consistency approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How can we know what language models know?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General observation from prior work that LMs vary responses with prompt wording changes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Different natural-language prompt phrasings for the same task (wording/paraphrases) change model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-sensitive (i.e., prompt wording affects outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Cited as evidence that PLMs are sensitive to prompt wording, which motivates regularizing across multiple prompts to encourage robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5867.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Metadata Variation (Choices)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-specific Metadata / Output-Token Mapping Variation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention that prompts include metadata such as the 'Choices' list (mapping labels to verbalizations), and these can vary across prompts (e.g., [Positive, Negative] vs [Good, Bad]) which changes the formatted target tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment classification example (illustrative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt templates include output templates and a Choices list mapping label ids to strings; different prompts may use different choice tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Changes in output-token verbalization and choice lists across prompts for the same classification labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format affects mapping of model outputs to labels (implied effect on performance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Different verbalizations can alter the model's token probabilities and therefore label assignment; prompt-consistency training must handle differing metadata mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5867.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-time Tuning (prompt-format adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-time Tuning via Prompt Consistency (adapt on unlabeled test inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adapting the PLM at test time on unlabeled test inputs using swarm distillation to make predictions more aligned with the test distribution; yields comparable gains to training-time tuning while using fewer examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same zero-shot classification tasks (11 datasets) evaluated in test-time tuning mode</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Offline test-time tuning: assume access to the entire test (validation) inputs without labels; apply swarm distillation on those inputs before making predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same prompt templates applied to test inputs and used to train via prompt-consistency loss; then ensemble predictions are produced.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to training-time tuning on training split; compared to baseline T0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Test-time tuning performs comparably to training-time tuning in many cases despite using less data. Example: RTE T0-3B swarm-distillation (test) ensemble 75.2% (≈ training-time 75.2%). Overall swarm-distillation (test) improved 9/11 datasets relative to baseline in many settings (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Often similar to training-time tuning; sometimes slightly different per dataset but generally comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Comparable effect sizes to training-time adaptation; eg. RTE +10.6 absolute points (test-time).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Adapting on unlabeled test inputs exposes the model to the test distribution (via prompt-formatted views) and allows the prompt-consistency loss to better align predictions for that distribution, mitigating distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5867.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5867.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parameter-efficient tuning (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoRA (Low-Rank Adaptation) for Prompt-Format Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of LoRA to update only a small low-rank set of parameters during prompt-consistency training to reduce catastrophic forgetting and reduce collapse risk.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same zero-shot classification tasks when applying swarm distillation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Instead of fine-tuning all model parameters, LoRA injects low-rank update matrices into feed-forward weights and only updates those (bottleneck dim b=1 in experiments) during unsupervised adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>No change in prompt format, but a change in which model parameters are trainable during adaptation (parameter-efficient vs full fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared LoRA-only updates vs full fine-tuning (all params) under the swarm-distillation objective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Full fine-tuning often caused collapse and lower accuracy (e.g., RTE 59.6% ensemble under full fine-tuning vs baseline 64.6%). Using LoRA + unsupervised model-selection recovered large gains (LoRA + model selection gave RTE 75.2% ensemble), indicating stability advantage. See Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LoRA + model selection outperformed full fine-tuning and LoRA alone in the unsupervised adaptation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Full fine-tuning could reduce performance by many points (dataset dependent); combining LoRA + model selection yields up to ~+10 absolute points vs baseline on some datasets (see swarm-distillation results).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>LoRA (parameter-efficient tuning) improved stability and enabled prompt-format consistency training to improve performance; full fine-tuning often reduced performance (collapsed) unless combined with model-selection.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Updating only a small set of added parameters preserves the pretrained model's capabilities and prevents catastrophic forgetting that can arise under an unsupervised objective divergent from pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Consistency for Zero-Shot Task Generalization', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How can we know what language models know? <em>(Rating: 2)</em></li>
                <li>Measuring and improving consistency in pretrained language models <em>(Rating: 2)</em></li>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 2)</em></li>
                <li>Promptsource: An integrated development environment and repository for natural language prompts <em>(Rating: 2)</em></li>
                <li>Unsupervised data augmentation for consistency training <em>(Rating: 1)</em></li>
                <li>LoRA: Low-rank adaptation of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5867",
    "paper_id": "paper-07c70ca55793984ffdf31582a05170ef3d62381a",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Swarm Distillation",
            "name_full": "Swarm Distillation (Prompt Consistency Regularization)",
            "brief_description": "An unsupervised prompt-consistency training method that enforces consistent predictions across multiple synonymous prompts by pairwise distillation among prompts; applied as post-adaptation (training-time or test-time) using unlabeled examples and parameter-efficient tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B, 11B",
            "task_name": "Zero-shot classification across 11 NLP datasets (NLI, sentence completion, coreference, word sense disambiguation)",
            "task_description": "Zero-shot prompting: apply prompt templates to inputs and predict discrete labels (examples: RTE, ANLI R1/R2/R3, CB, COPA, HellaSwag, Story Cloze, Winogrande, WSC, WIC).",
            "problem_format": "Multiple synonymous natural-language prompts (each with an input template r_x and an output template r_y plus prompt-specific metadata like 'Choices') are applied to each unlabeled input; model is trained to be consistent via a pairwise distillation loss that treats each prompt's prediction as a pseudo-target for another prompt. Evaluation uses ensemble (averaged output distributions across prompts) and median (single-prompt median) metrics. Two adaptation settings: (1) training-time tuning on unlabeled training split; (2) test-time tuning on unlabeled test inputs.",
            "comparison_format": "Compared to: (a) T0 baseline (no unsupervised adaptation), (b) Self-distillation baseline (each prompt distills to itself), (c) Full fine-tuning, (d) parameter-efficient tuning with LoRA and model-selection ablations.",
            "performance": "T0-3B baseline ensemble accuracies vary by dataset (e.g., RTE 64.6%); Swarm Distillation (train) improves many datasets (RTE ensemble 75.2% ±0.8, up +10.6 absolute points; HellaSwag 34.2% ±0.2, up +6.4; WIC 55.4% ±1.1, up +5.1). On T0-11B (test-time tuning) swarm distillation improves HellaSwag from 34.4% to 45.0% (+10.6) and yields gains on RTE, WIC, WSC (see Table 2). Overall, swarm distillation outperforms T0-3B on 9/11 datasets in the main experiments.",
            "performance_comparison": "Compared to self-distillation: swarm distillation outperforms self-distillation on 9/11 datasets (ensemble accuracy) by up to ~10.3 absolute points. Compared to full fine-tuning: full fine-tuning often collapses and hurts performance unless combined with unsupervised model-selection; combining LoRA + model-selection yields similarly large gains.",
            "format_effect_size": "+10.6 absolute points (largest observed improvement; RTE for T0-3B and HellaSwag for T0-11B). Average Fleiss' kappa (prompt agreement) increased relatively by 14.6% across datasets after swarm distillation.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "The paper hypothesizes and demonstrates that PLMs are sensitive to prompt wording and that regularizing consistency across multiple synonymous prompts encourages robustness: pairwise distillation makes predictions invariant to prompt-level reformatting. Using parameter-efficient tuning (LoRA) mitigates catastrophic forgetting, and Fleiss' kappa as an unsupervised selection metric penalizes trivial collapsed solutions (where all prompts predict the same label). Ensembles over prompts further reduce variance from prompt wording.",
            "counterexample_or_null_result": "On WSC swarm distillation produced a large ensemble accuracy drop (≈−10.9 points in one setting) due to model-selection choosing a bad checkpoint; oracle model-selection (using labeled validation) shows that swarm distillation actually had improvements in the middle of training, indicating failures in unsupervised checkpoint selection rather than method failure.",
            "uuid": "e5867.0",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Self Distillation (single-prompt)",
            "name_full": "Self Distillation Baseline (prompt self-training)",
            "brief_description": "A baseline ablation where the same prompt distills its own predictions (r^i = r^j), i.e., pseudo-label self-training per prompt rather than pairwise across prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B",
            "task_name": "Same zero-shot classification tasks as Swarm Distillation",
            "task_description": "Unsupervised pseudo-labeling where each prompt's own predicted distribution is used as target for itself (no cross-prompt consistency imposed).",
            "problem_format": "Single-prompt self-distillation: the prompt formats remain the same but the distillation pairs are identical prompts (no prompt-to-prompt transfer).",
            "comparison_format": "Compared directly against swarm distillation (pairwise prompt distillation) and the T0 baseline.",
            "performance": "Self-distillation shows small or negligible improvements over T0 baseline in many datasets (e.g., RTE ensemble 64.9% vs T0 baseline 64.6%), but is outperformed by swarm distillation on 9/11 datasets. See Table 1 (self-distillation columns).",
            "performance_comparison": "Swarm Distillation outperforms self-distillation on 9/11 datasets by up to ~10.3 absolute points (ensemble accuracy).",
            "format_effect_size": "Up to ~+10.3 absolute points advantage for pairwise (swarm) vs self-distillation (dataset dependent).",
            "format_effect_direction": "swarm (multi-prompt) improved over self (single-prompt) ; self-distillation alone generally provided only minor gains.",
            "explanation_or_hypothesis": "Pairwise cross-prompt distillation leverages diverse prompt views to propagate stronger pseudo supervision, whereas self-distillation lacks cross-view regularization and thus is weaker at improving robustness to prompt wording.",
            "counterexample_or_null_result": null,
            "uuid": "e5867.1",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Prompt Ensembling (inference)",
            "name_full": "Prompt Ensemble (averaging output distributions across prompts)",
            "brief_description": "At inference time, average the predicted probability distributions from multiple prompt templates and predict according to the averaged distribution; reported as 'ensemble' accuracy and compared to median single-prompt performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B, 11B",
            "task_name": "Same zero-shot classification tasks (11 datasets)",
            "task_description": "Inference method combining multiple prompt outputs to produce a single prediction for each example.",
            "problem_format": "Apply K prompt templates to each example; average resulting predicted label distributions to obtain ensemble prediction. Also report median of per-prompt accuracies as proxy for single-prompt performance.",
            "comparison_format": "Compared ensemble (Ens.) vs median single-prompt (Med.) across prompts.",
            "performance": "Ensemble accuracy is generally higher than median single-prompt accuracy across most datasets and both baseline and adapted models. Example: T0-3B RTE Ens 64.6% vs Med 64.1% (baseline); swarm-distillation ensemble typically higher than median. Specific dataset values are in Table 1.",
            "performance_comparison": "Ensembling improves robustness relative to single-prompt median in most cases (dataset-dependent), and is the preferred evaluation metric in the paper.",
            "format_effect_size": "Dataset dependent; ensemble usually &gt; median (often a few percentage points), exact per-dataset deltas provided in Table 1.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Different prompts produce different biases/noise; averaging (ensembling) reduces variance due to prompt wording and improves expected accuracy over selecting a single prompt.",
            "counterexample_or_null_result": "Not universally larger for every dataset (e.g., some datasets or specific seeds may show median near or exceeding ensemble), but ensemble is generally better overall as reported.",
            "uuid": "e5867.2",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Prompt Count Ablation",
            "name_full": "Ablation: Effect of Number of Prompts",
            "brief_description": "Analysis varying the number of available prompt templates used for swarm distillation; experiments show performance improves as prompts increase but saturates quickly (around 4 prompts).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B",
            "task_name": "COPA and ANLI R2 (examples used for ablation)",
            "task_description": "Measure ensemble accuracy while varying the number of prompts sampled from the dataset's prompt pool (COPA has 8 prompts, ANLI R2 has 15 prompts available).",
            "problem_format": "Subset of prompt templates applied (from 0 to full set); swarm distillation trained on that subset.",
            "comparison_format": "0 prompts (baseline, no adaptation) vs 1,2,... up to full K prompts; examine curve and saturation.",
            "performance": "Performance increases as number of prompts increases from 0; stabilizes when approximately 4 prompts are available. Exact ensemble accuracy curves in Figure 4a (qualitative improvement visible for both COPA and ANLI R2).",
            "performance_comparison": "Most of the benefit is achieved with a small number of prompts; beyond ~4 prompts marginal gains are small.",
            "format_effect_size": "Qualitative saturation by ~4 prompts; exact numeric improvements depend on dataset (figures show clear early gains).",
            "format_effect_direction": "improved up to saturation",
            "explanation_or_hypothesis": "Diverse prompt views provide complementary pseudo-label signals; a small set of diverse prompts suffices to regularize model predictions effectively, so the method is not 'prompt-hungry'.",
            "counterexample_or_null_result": null,
            "uuid": "e5867.3",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Unlabeled Data Size Ablation",
            "name_full": "Ablation: Effect of Number of Unlabeled Examples",
            "brief_description": "Analysis of how many unlabeled inputs are required for swarm distillation to be effective; shows large gains with very few examples (tens), saturating quickly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B",
            "task_name": "WIC and ANLI R2 (example datasets for ablation)",
            "task_description": "Randomly sample subsets of the training set (varying counts of unlabeled examples) for unsupervised training and evaluate ensemble accuracy on the validation set.",
            "problem_format": "Vary number of unlabeled examples used for swarm distillation from 0 up through hundreds/10k (limited by dataset); apply multiple prompts per example during training.",
            "comparison_format": "0 unlabeled examples (baseline) vs small numbers (e.g., 10) vs larger numbers.",
            "performance": "Swarm distillation outperforms baseline with as few as 10 unlabeled examples on WIC and ANLI R2; performance typically saturates after tens-to-hundreds of examples (see Figure 4b).",
            "performance_comparison": "Significant gains with 10 examples relative to 0; further gains smaller beyond tens/hundreds of examples.",
            "format_effect_size": "Qualitative 'large gain' with only ~10 unlabeled examples; no single global absolute value provided (dataset dependent).",
            "format_effect_direction": "improved with small unlabeled set; saturates thereafter",
            "explanation_or_hypothesis": "Prompt-consistency loss effectively propagates supervision from few unlabeled examples due to constraints imposed by multiple prompt views; thus method is sample-efficient for adaptation.",
            "counterexample_or_null_result": null,
            "uuid": "e5867.4",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Prompt Wording Sensitivity",
            "name_full": "Prompt Wording Sensitivity (Jiang et al., 2020)",
            "brief_description": "Prior observation cited: model predictions are empirically very sensitive to the exact wording of task prompts, motivating prompt consistency approaches.",
            "citation_title": "How can we know what language models know?",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": null,
            "task_name": "",
            "task_description": "General observation from prior work that LMs vary responses with prompt wording changes.",
            "problem_format": "Different natural-language prompt phrasings for the same task (wording/paraphrases) change model outputs.",
            "comparison_format": null,
            "performance": "",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "format-sensitive (i.e., prompt wording affects outputs)",
            "explanation_or_hypothesis": "Cited as evidence that PLMs are sensitive to prompt wording, which motivates regularizing across multiple prompts to encourage robustness.",
            "counterexample_or_null_result": null,
            "uuid": "e5867.5",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Prompt Metadata Variation (Choices)",
            "name_full": "Prompt-specific Metadata / Output-Token Mapping Variation",
            "brief_description": "Mention that prompts include metadata such as the 'Choices' list (mapping labels to verbalizations), and these can vary across prompts (e.g., [Positive, Negative] vs [Good, Bad]) which changes the formatted target tokens.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "T0",
            "model_size": "3B",
            "task_name": "Sentiment classification example (illustrative)",
            "task_description": "Prompt templates include output templates and a Choices list mapping label ids to strings; different prompts may use different choice tokens.",
            "problem_format": "Changes in output-token verbalization and choice lists across prompts for the same classification labels.",
            "comparison_format": null,
            "performance": "",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "format affects mapping of model outputs to labels (implied effect on performance)",
            "explanation_or_hypothesis": "Different verbalizations can alter the model's token probabilities and therefore label assignment; prompt-consistency training must handle differing metadata mappings.",
            "counterexample_or_null_result": null,
            "uuid": "e5867.6",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Test-time Tuning (prompt-format adaptation)",
            "name_full": "Test-time Tuning via Prompt Consistency (adapt on unlabeled test inputs)",
            "brief_description": "Adapting the PLM at test time on unlabeled test inputs using swarm distillation to make predictions more aligned with the test distribution; yields comparable gains to training-time tuning while using fewer examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B, 11B",
            "task_name": "Same zero-shot classification tasks (11 datasets) evaluated in test-time tuning mode",
            "task_description": "Offline test-time tuning: assume access to the entire test (validation) inputs without labels; apply swarm distillation on those inputs before making predictions.",
            "problem_format": "Same prompt templates applied to test inputs and used to train via prompt-consistency loss; then ensemble predictions are produced.",
            "comparison_format": "Compared to training-time tuning on training split; compared to baseline T0.",
            "performance": "Test-time tuning performs comparably to training-time tuning in many cases despite using less data. Example: RTE T0-3B swarm-distillation (test) ensemble 75.2% (≈ training-time 75.2%). Overall swarm-distillation (test) improved 9/11 datasets relative to baseline in many settings (see Table 1).",
            "performance_comparison": "Often similar to training-time tuning; sometimes slightly different per dataset but generally comparable.",
            "format_effect_size": "Comparable effect sizes to training-time adaptation; eg. RTE +10.6 absolute points (test-time).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Adapting on unlabeled test inputs exposes the model to the test distribution (via prompt-formatted views) and allows the prompt-consistency loss to better align predictions for that distribution, mitigating distribution shift.",
            "counterexample_or_null_result": null,
            "uuid": "e5867.7",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Parameter-efficient tuning (LoRA)",
            "name_full": "LoRA (Low-Rank Adaptation) for Prompt-Format Adaptation",
            "brief_description": "Use of LoRA to update only a small low-rank set of parameters during prompt-consistency training to reduce catastrophic forgetting and reduce collapse risk.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B, 11B",
            "task_name": "Same zero-shot classification tasks when applying swarm distillation",
            "task_description": "Instead of fine-tuning all model parameters, LoRA injects low-rank update matrices into feed-forward weights and only updates those (bottleneck dim b=1 in experiments) during unsupervised adaptation.",
            "problem_format": "No change in prompt format, but a change in which model parameters are trainable during adaptation (parameter-efficient vs full fine-tuning).",
            "comparison_format": "Compared LoRA-only updates vs full fine-tuning (all params) under the swarm-distillation objective.",
            "performance": "Full fine-tuning often caused collapse and lower accuracy (e.g., RTE 59.6% ensemble under full fine-tuning vs baseline 64.6%). Using LoRA + unsupervised model-selection recovered large gains (LoRA + model selection gave RTE 75.2% ensemble), indicating stability advantage. See Table 6.",
            "performance_comparison": "LoRA + model selection outperformed full fine-tuning and LoRA alone in the unsupervised adaptation setting.",
            "format_effect_size": "Full fine-tuning could reduce performance by many points (dataset dependent); combining LoRA + model selection yields up to ~+10 absolute points vs baseline on some datasets (see swarm-distillation results).",
            "format_effect_direction": "LoRA (parameter-efficient tuning) improved stability and enabled prompt-format consistency training to improve performance; full fine-tuning often reduced performance (collapsed) unless combined with model-selection.",
            "explanation_or_hypothesis": "Updating only a small set of added parameters preserves the pretrained model's capabilities and prevents catastrophic forgetting that can arise under an unsupervised objective divergent from pretraining.",
            "counterexample_or_null_result": null,
            "uuid": "e5867.8",
            "source_info": {
                "paper_title": "Prompt Consistency for Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How can we know what language models know?",
            "rating": 2
        },
        {
            "paper_title": "Measuring and improving consistency in pretrained language models",
            "rating": 2
        },
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 2
        },
        {
            "paper_title": "Promptsource: An integrated development environment and repository for natural language prompts",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised data augmentation for consistency training",
            "rating": 1
        },
        {
            "paper_title": "LoRA: Low-rank adaptation of large language models",
            "rating": 1
        }
    ],
    "cost": 0.01815225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prompt Consistency for Zero-Shot Task Generalization</h1>
<p>Chunting Zhou ${ }^{<em> 1}$, Junxian $\mathrm{He}^{</em> 1}$, Xuezhe $\mathrm{Ma}^{2}$, Taylor Berg-Kirkpatrick ${ }^{3}$, Graham Neubig ${ }^{1}$<br>${ }^{1}$ Language Technologies Institute, Carnegie Mellon University<br>${ }^{2}$ Information Sciences Institute, University of Southern California<br>${ }^{3}$ Department of Computer Science and Engineering, University of California San Diego<br>{chuntinz, junxianh, gneubig}@cs.cmu.edu<br>xuezhema@isi.edu, tberg@eng.ucsd.edu</p>
<h4>Abstract</h4>
<p>One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zeroshot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zeroshot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al., 2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>While the past decade has demonstrated that pretrained language models (PLMs) are powerful tools for improving generalization from training datasets to test datasets (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), more recent work has shown that they can even perform zero-shot generalization to new tasks without any annotated examples (Brown et al., 2020; Wei et al., 2022; Sanh et al., 2022). These systems leverage natural language prompts that specify the task for the model and represent different tasks in a unified format (Liu</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2021b). Zero-shot task generalization suggests a path towards generic systems that perform a wide variety of NLP tasks with no annotated examples. However, while enticing conceptually, zero-shot performance often remains relatively low compared to systems trained using even a small amount of task-specific labeled data.</p>
<p>In this paper, we examine methods to make PLMs better zero-shot learners using unlabeled text. Our work is motivated by consistency training methods that regularize model predictions to be invariant to perturbation (e.g. noise or paraphrasing) of the input examples. Consistency training is widely used in semi-supervised learning literature as an effective technique to utilize unannotated examples (Bachman et al., 2014; Sajjadi et al., 2016; Beyer et al., 2019; Xie et al., 2020a). It is often understood as a type of smoothness regularization or data augmentation (Xie et al., 2020a) and attains strong performance in semi-supervised learning. Instead of example-level consistency, we propose to regularize prompt consistency, where a model is regularized to make the same prediction across a diverse set of synonymous task prompts. Prompt consistency regularization makes sense intuitively since PLMs should be robust across synonymous prompts, whereas it is known that model predictions are empirically very sensitive to the wording of the task prompts (Jiang et al., 2020).</p>
<p>Specifically, we design a pairwise distillation loss that encourages consistency between every pair of prompts (Figure 1). We refer to our method as swarm distillation, and it has the advantage of being fully unsupervised, only requiring unannotated inputs. Notably, unannotated examples are often relatively easy to collect. Drafting several prompts for a task is also far cheaper than annotating labels for each example - in fact, there are already well-designed prompts available for a wide range of NLP tasks (Bach et al., 2022).</p>
<p>Previous work on example-level consistency reg-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of the proposed approach in an sentiment classification task. We apply multiple synonymous prompts to the unlabeled example, then we regularize the consistency of the predictions from different prompts, through our swarm distillation loss as detailed in Eq. 2.</p>
<p>The localization typically minimizes a consistency loss along with a supervised loss in a semi-supervised setting (Miyato et al., 2018; Xie et al., 2020a). Recently, Elazar et al. (2021) performed experiments optimizing a prompt consistency loss in the context of a relation prediction task, also incorporating a supervised version of the masked language model pretraining objective. In contrast, we (1) optimize a novel prompt consistency loss alone, making our approach completely unsupervised and agnostic to the model's pretraining objective, and (2) experiment on and demonstrate the practicality of such an approach for a broad variety of NLP tasks. Notably, this unsupervised setting poses additional learning challenges: without explicit supervision, the model may suffer from catastrophic forgetting and even exhibit a form of collapse where the model always makes the same predictions for any input. To address this issue, we adopt two simple strategies: (1) we utilize parameter-efficient tuning techniques (Houlsby et al., 2019; He et al., 2022) to only update a small number of extra parameters, naturally mitigating catastrophic forgetting by fixing the original PLM parameters; (2) we propose an unsupervised criterion to select the model checkpoint before it falls into a collapsed local optimum.</p>
<p>In experiments, we build our method on top of a state-of-the-art zero-shot task learner, T0 (Sanh et al., 2022), and validate its performance on 11 datasets from 4 NLP tasks: natural language inference, coreference resolution, word sense disambiguation, and sentence completion. We perform experiments under two scenarios: (1) training the model with unlabeled training data; or (2) tuning the model with unlabeled test inputs directly. In both settings, we show that our swarm distillation method improves the accuracy of the 3B-parameter T0 model on 9 out of 11 datasets by up to 10.6 absolute points. We further scale model size up to 11B parameters, and demonstrate that our approach outperforms the 11B-parameter T0 model on 4 out of 4 datasets. Remarkably, analysis implies that these gains are often possible with only tens of examples, suggesting a small computation overhead.</p>
<h1>2 Prompt-based Zero-Shot Task Generalization</h1>
<p>Given a task where the input is denoted as <strong>x</strong> ∈ <strong>X</strong> and the goal is to predict <strong>y</strong> ∈ <strong>Y</strong>, we focus on the zero-shot task generalization setting: we aim to feed a PLM with <strong>x</strong> to predict <strong>y</strong>, where the PLM is never trained on the specific task to be performed. Zero-shot task generalization goes beyond traditional dataset generalization, as the model must generalize to new functions <em>f</em> : <strong>X</strong> → <strong>Y</strong> as opposed to new input examples, <strong>x</strong>. Recently, the development of prompting methods has advanced zero-shot task generalization by representing different tasks in a unified format (Liu et al., 2021b), and several prompt-based approaches have attained reasonable zero-shot performance (Brown et al., 2020; Sanh et al., 2022; Wei et al., 2022).</p>
<p>A prompt <em>r</em> consists of an input template <em>r<sub>x</sub></em>, an output template <em>r<sub>y</sub></em>, and metadata to re-format the original <strong>x</strong> and <strong>y</strong> into new prompt-formatted input and target, <em>r<sub>x</sub></em>(<strong>x</strong>) and <em>r<sub>y</sub></em>(<strong>y</strong>). For example, as shown in Figure 1, in a sentiment classification task where we must predict positive or negative sentiment of the text, the input includes the field <strong>Sentence</strong> and the target consists of the</p>
<p>field Label. An input template could be "Does the following sentence have a positive or negative sentiment? {Sentence]", and the target template is "Choices[{label}]". Here Choices is the metadata that is a list containing [Positive, Negative] to correspond to the numeric label ids. We note that such metadata is prompt-specific and can differ with different prompts for the same task - for instance, in Figure 1 the Choices list of the last prompt on the bottom is [Good, Bad]. In prompt-based approaches the PLM models the conditional probability $q(\mathbf{y} \mid \mathbf{x}, r)$ through $p_{\theta}\left(r_{y}(\mathbf{y}) \mid r_{x}(\mathbf{x})\right)$ where $\theta$ denotes the model parameters. In classification tasks where $\mathcal{Y}$ is a finite label set, $q(\mathbf{y} \mid \mathbf{x}, r)$ is normalized over the possible labels at inference time to predict $\mathbf{y}$ :</p>
<p>$$
q(\mathbf{y} \mid \mathbf{x}, r)=\frac{p_{\theta}\left(r_{y}(\mathbf{y}) \mid r_{x}(\mathbf{x})\right)}{\sum_{\mathbf{y}^{\prime} \in \mathcal{Y}} p_{\theta}\left(r_{y}\left(\mathbf{y}^{\prime}\right) \mid r_{x}(\mathbf{x})\right)}
$$</p>
<p>In generation tasks where $\mathcal{Y}$ is an infinite sequence space, the target template is typically instantiated as the target itself, i.e. $p_{\theta}\left(r_{y}(\mathbf{y}) \mid r_{x}(\mathbf{x})\right)=$ $p_{\theta}\left(\mathbf{y} \mid r_{x}(\mathbf{x})\right)$, then the output can be directly decoded through sequence decoding approaches. Through designing such prompts for each task, all NLP tasks share the same data format, and models trained on one task may generalize to others.</p>
<h2>3 Prompt Consistency Training</h2>
<h3>3.1 Problem Definition</h3>
<p>In this paper, we aim to explore unannotated examples to improve prompt-based zero-shot task generalization. Formally, we are given an unlabeled dataset in the task of interest $\left{\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>}, \cdots, \mathbf{x<em x="x">{N}\right}$, and we assume the dataset has $K$ different prompts, $\left{\left(r</em>)$. Unlabeled inputs are often available in practice, we consider two such scenarios in the paper.}^{(1)}, r_{y}^{(1)}\right), \cdots,\left(r_{x}^{(K)}, r_{y}^{(K)}\right)\right}$. Our goal is to utilize these resources and adapt a PLM to predict $r_{y}(\mathbf{y})$ conditioned on $r_{x}(\mathbf{x</p>
<p>First, we consider the case when unannotated examples from a non-test set are available. For many NLP tasks their inputs are plain text such as reviews, documents, or questions and can be easily collected (less so for other NLP tasks, like natural language inference the inputs are paired hypotheses and premises that can be non-trivial to obtain automatically). In this paper, we test this setting by utilizing the inputs of the training dataset. This is similar to schick2021prioritized, schick2021prioritized where they directly use the inputs of the training split as unlabeled resources to help few-shot learning.</p>
<p>Second is the case when unannotated test inputs are available. This is almost always true for any task. We use the test split to mimic the setting. While the limited number of unlabeled examples could potentially limit the effectiveness of some unsupervised learning methods, we show in $\S 4.4$ that our method is effective even with tens to hundreds of unlabeled examples.</p>
<p>On the other hand, a diverse set of prompts is not exceedingly difficult to collect practically - drafting prompts for each task is easier than annotating labels for many examples. In fact, the community efforts have pushed out a Public Pool of Prompts (P3) that contains thousands of prompts for hundreds of NLP datasets already (Bach et al., 2022).</p>
<h3>3.2 The Prompt Consistency Loss</h3>
<p>Consistency regularization is a method that creates different views (e.g. paraphrases of text) of the input and regularizes the outputs to be close to each other, and has achieved significant success in semi-supervised learning (Clark et al., 2018; Xie et al., 2020a,b). While previous methods use an additional module to perturb each example and then optimize example-level consistency, we propose to optimize prompt-level consistency which (1) is conceptually simple, and (2) can mitigate the fact that the predictions of PLMs are typically inconsistent with different prompts for the same task (Jiang et al., 2020; Elazar et al., 2021). Intuitively, we propose to regularize the predictions of different prompts for a given input to be close to each other, using a pairwise distillation loss to draw the predictions from one prompt closer to those from the other. Concretely, we randomly sample a few pairs of prompts and distill the pseudo target $\hat{\mathbf{y}}$ from one prompt $r^{(i)}$ to the other prompt $r^{(j)}$, as illustrated in Figure 1. The loss function is defined as:</p>
<p>$$
\begin{aligned}
\mathcal{L}=-\mathbb{E}<em d="d">{\mathbf{x} \sim p</em>}(\mathbf{x})} &amp; \mathbb{E<em _hat_mathbf_y="\hat{\mathbf{y">{r^{(i)}, r^{(j)} \sim p(r)} \
&amp; \mathbb{E}</em>)\right)
\end{aligned}
$$}} \sim \hat{q}\left(\mathbf{y} \mid \mathbf{x}, r^{(i)}\right)} \log p_{\theta}\left(r_{y}^{(j)}(\hat{\mathbf{y}}) \mid r_{x}^{(j)}(\mathbf{x</p>
<p>where $p_{d}(\mathbf{x})$ is the empirical data distribution, $p(r)$ is a uniform distribution over possible prompts, and $\hat{q}(\mathbf{y} \mid \mathbf{x}, r)$ is the conditional target distribution defined as in Eq. 1 but with a stopping gradient operator. We do not propagate gradients to $\hat{q}\left(\mathbf{y} \mid \mathbf{x}, r^{(i)}\right)$ following Miyato et al. (2018) and Xie et al. (2020a). ${ }^{2}$ Stopping the gradient of one side</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>in a pairwise consistency loss is also shown to help mitigate the collapse issue where all inputs lead to the same predictions <em>Chen and He (2021)</em>. Different from traditional distillation that distills from a teacher model to a student model <em>Hinton et al. (2015)</em>, or previous consistency training that a single teacher distills to several students <em>Clark et al. (2018); Xie et al. (2020a)</em>, we perform distillation among a swarm of prompts where each prompt is a teacher and student at the same time, thus we term our method as swarm distillation. In our implementation, we approximate the expectation over the paired prompts $(r^{(i)},r^{(j)})$ with $k$ randomly sampled pairs for training efficiency.</p>
<p>Prompt consistency is related to example-level consistency when viewing different prompt-formatted inputs $r_{x}^{(i)}(\mathbf{x})$ as separated views of the same example, thus our swarm distillation approach shares spirit with previous work on example-level consistency training and can be understood similarly from the perspective of unsupervised data augmentation, smoothness regularization, or label propagation <em>Xie et al. (2020a)</em>. In this paper, we focus on classification tasks where $\mathcal{Y}$ is a finite label set, while Eq. 2 can be directly applied to sequence generation tasks as well with sequence distillation <em>Kim and Rush (2016)</em>.</p>
<p>Our approach differs from previous consistency training methods which often combine an unsupervised consistency loss with a supervised loss in a semi-supervised setting <em>Miyato et al. (2018); Clark et al. (2018); Xie et al. (2020a)</em>. <em>Elazar et al. (2021)</em> try to improve prompt consistency for a relation filling task with a pairwise two-sided KL divergence loss, while they also optimize a supervised version of the original PLM objective that turns out to be important. In contrast, our approach minimizes the swarm distillation loss in Eq. 2 alone, and therefore is completely unsupervised and agnostic to the pretraining objective. However, this setting also poses challenges in learning, which we discuss next.</p>
<h3>3.3 Training</h3>
<p>Being trained without explicit supervision, the PLM may forget what it learns during pretraining since the unsupervised consistency loss is different from the pretraining objective. Also, we note that prompt consistency may be achieved with a trivial solution – if the predictions from each example and each prompt collapse to the same label then maximal consistency among prompts can be</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A diagram of LoRA in the FFN sublayer. Only the LoRA parameters, $A$ and $B$, are updated during training.</p>
<p>reached. To mitigate such catastrophic forgetting and collapse issues, we propose two techniques:</p>
<p>Parameter-efficient tuning: It has recently been observed that updating a small number of added parameters in a PLM is able to achieve comparable performance to tuning all the parameters <em>Houlsby et al. (2019); Li and Liang (2021); Hu et al. (2022); He et al. (2022)</em>. Parameter-efficient tuning methods naturally mitigate catastrophic forgetting and collapse through fixing the original PLM parameters. Specifically, we use LoRA <em>Hu et al. (2022)</em>, a low-rank adaptation method for PLMs. As shown in Figure 2, LoRA learns a low-rank approximation of the pretrained matrix updates: given a pretrained weight matrix $W \in \mathbb{R}^{d \times m}$, LoRA learns to update it as $W \leftarrow W + \alpha BA$, where $B \in \mathbb{R}^{d \times b}$, $A \in \mathbb{R}^{b \times m}$ are low-rank matrices and $\alpha$ is a hyperparameter, and only $B$ and $A$ are updated during training. $b \ll d$ is referred to as the <em>bottleneck dimension</em>. Following <em>He et al. (2022)</em>, we apply LoRA to the feed-forward weight matrices of every layer in the pretrained transformer <em>Vaswani et al. (2017)</em> model. In our preliminary experiments, we found that LoRA is less likely to suffer from collapse, while on some datasets the model still collapses in the end even though it learns well in the middle. This motivates us to develop a criterion to select the model checkpoint before it falls into a collapsed local optimum, which we describe next.</p>
<p>Unsupervised model selection criterion: Our zero-shot setting does not have labeled validation data for model selection, and the swarm distillation objective is not an ideal selection criterion since it is minimized at collapse. Therefore, we would like to have an unsupervised criterion that encourages consistency but simultaneously penalizes collapse. With that in mind, we focus on Fleiss' kappa <em>Fleiss (1971)</em>, a commonly used metric to</p>
<p>assess the reliability of agreement. In our setting, Fleiss' kappa expresses the extent to which the amount of agreement among prompts exceeds what would be expected if all prompts made their predictions according to the marginalized distribution of labels. This design computes a notion of "relative consistency" and naturally penalizes collapse. Formally, let $n_{i j}$ be the number of prompts that predict the $j$-th label for the $i$-th example. There are a total of $N K$ predictions where $N$ is the number of examples and $K$ is the number of prompts. Given an example $\mathbf{x}<em i="i">{i}$, the agreement probability $p</em>$ computes the normalized number of agreeing prompt pairs:</p>
<p>$$
p_{i}=\sum_{j} n_{i j}\left(n_{i j}-1\right) /(K(K-1))
$$</p>
<p>then the "absolute consistency" $\bar{P}$ is:</p>
<p>$$
\bar{P}=\sum_{i=1}^{N} p_{i} / N
$$</p>
<p>$\bar{P}$ is maximized in the case of collapse. However, Fleiss' kappa considers the marginalized distribution of labels: how likely are two prompts consistent if they make predictions randomly according to the marginalized label distribution? This chance probability $\bar{P}_{e}$ is:</p>
<p>$$
\bar{P}<em j="j">{e}=\sum</em> /(N K)
$$} q_{j}^{2}, \quad q_{j}=\sum_{i=1}^{N} n_{i j</p>
<p>where $q_{j}$ represents the marginalized distribution of labels, i.e. $p(\mathbf{y}=j) . \bar{P}_{e}$ is large when collapse happens and one label dominates in the entire corpus. Finally, Fleiss' kappa is computed as:</p>
<p>$$
\kappa=\frac{\bar{P}-\bar{P}<em e="e">{e}}{1-\bar{P}</em>
$$}</p>
<p>where $1-\bar{P}<em e="e">{e}$ gives the degree of consistency that is attainable above chance, $\bar{P}-\bar{P}</em>$ We emphasize that we perform validation on the data that the model is trained on and do not require an additional development dataset. We include ablation analysis for both LoRA and the model selection components in Appendix C that shows that they are important for the success of our method.}$ gives the degree of consistency actually achieved above chance. $\kappa$ ranges from -1 to 1. Eq. 6 naturally penalizes collapse, and in our experiments, we always observe a monotonic decrease of $\kappa$ when collapse happens. Therefore, we select the model checkpoint after which $\kappa$ monotonically decreases. ${ }^{3</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>4 Experiments</h2>
<p>Our experiments below are designed to (1) measure whether swarm distillation is able to improve zero-shot task generalization; and (2) analyze how much resource (number of prompts and unlabeled examples) our method demands.</p>
<h3>4.1 General Setup</h3>
<p>Datasets: Following Sanh et al. (2022), we evaluate our method on 11 NLP datasets across 4 unseen tasks. They are (1) natural language inference: ANLI (Nie et al., 2020) (there are three versions of ANLI with different levels of difficulty, which we denote as ANLI R1/R2/R3), CB (De Marneffe et al., 2019), RTE (Wang et al., 2019a); (2) sentence completion: COPA (Roemmele et al., 2011), HellaSwag (Zellers et al., 2019), Story Cloze (Mostafazadeh et al., 2016); (3) coreference resolution: WSC, Winogrande (Levesque et al., 2012); and (4) word sense disambiguation: WIC (Pilehvar and Camacho-Collados, 2019). We access them using Hugging Face Datasets (Lhoest et al., 2021) and most of them are from the SuperGLUE benchmark (Wang et al., 2019a). All of these datasets are classification-based, predicting a discrete label from a finite set. Each of these datasets has a diverse set of prompts provided by the Public Pool of Prompts (Sanh et al., 2022) The number of prompts ranges from 4 to 15 . Please refer to Appendix A for detailed statistics of these datasets.</p>
<p>Setup: We build our method on top of the PLM T0 (Sanh et al., 2022). T0 is an adapted version of the pretrained T5 model (Raffel et al., 2020) that is continually trained on multiple tasks with supervised, prompt-formatted examples. T0 outperforms GPT3 (Brown et al., 2020) and demonstrates state-of-the-art performance in zero-shot task generalization. All the tasks that we are studying are not included in T0's training data. We focus our major study on the T0 model version with 3 billion parameters (T0-3B), while we also include results using the largest T0 model with 11 billion parameters (T0-11B) on some datasets, due to the high computational cost of training T0-11B. The hyperparameters (e.g. the optimization hyperparameters) are tuned on the RTE dataset with its validation set and fixed for all other datasets. We use a bottleneck dimension of 1 for LoRA. Complete setup details can be found in Appendix B.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Dataset</th>
<th>T0-3B</th>
<th></th>
<th>Self Dist. (train)</th>
<th></th>
<th>Swarm Dist. (train)</th>
<th></th>
<th>Swarm Dist. (test)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Ens.</td>
<td>Med.</td>
<td>Ens.</td>
<td>Med.</td>
<td>Ens.</td>
<td>Med.</td>
<td>Ens.</td>
<td>Med.</td>
</tr>
<tr>
<td>NLI</td>
<td>RTE</td>
<td>64.6</td>
<td>64.1</td>
<td>64.9$\pm$6.2</td>
<td>63.8$\pm$0.1</td>
<td>75.2$\pm$0.8 $\uparrow$ 10.6</td>
<td>73.9$\pm$0.8 $\uparrow$ 9.8</td>
<td>75.2$\pm$0.2 $\uparrow$ 10.6</td>
<td>73.5$\pm$6.1 $\uparrow$ 9.4</td>
</tr>
<tr>
<td></td>
<td>CB</td>
<td>46.4</td>
<td>50.0</td>
<td>47.0$\pm$1.0</td>
<td>49.4$\pm$2.7</td>
<td>47.6$\pm$1.0 $\uparrow$ 1.2</td>
<td>48.2$\pm$0.0 $\downarrow$ 1.8</td>
<td>46.4$\pm$0.0 $\uparrow$ 0.0</td>
<td>48.8$\pm$1.0 $\downarrow$ 1.2</td>
</tr>
<tr>
<td></td>
<td>ANLI R1</td>
<td>34.6</td>
<td>33.7</td>
<td>36.1$\pm$0.1</td>
<td>34.7$\pm$0.1</td>
<td>38.4$\pm$0.5 $\uparrow$ 3.8</td>
<td>35.7$\pm$0.4 $\uparrow$ 2.0</td>
<td>38.5$\pm$0.3 $\uparrow$ 3.9</td>
<td>35.7$\pm$0.5 $\uparrow$ 2.0</td>
</tr>
<tr>
<td></td>
<td>ANLI R2</td>
<td>33.7</td>
<td>33.4</td>
<td>35.3$\pm$0.1</td>
<td>33.2$\pm$0.2</td>
<td>37.9$\pm$0.8 $\uparrow$ 4.2</td>
<td>36.6$\pm$0.5 $\uparrow$ 3.2</td>
<td>37.7$\pm$0.2 $\uparrow$ 4.0</td>
<td>35.4$\pm$0.4 $\uparrow$ 2.0</td>
</tr>
<tr>
<td></td>
<td>ANLI R3</td>
<td>34.7</td>
<td>33.3</td>
<td>33.1$\pm$0.0</td>
<td>33.8$\pm$0.2</td>
<td>34.0$\pm$0.3 $\downarrow$ 0.7</td>
<td>34.6$\pm$0.1 $\uparrow$ 1.3</td>
<td>34.1$\pm$0.2 $\downarrow$ 0.6</td>
<td>33.5$\pm$0.0 $\uparrow$ 0.2</td>
</tr>
<tr>
<td>Compl.</td>
<td>COPA</td>
<td>78.0</td>
<td>79.0</td>
<td>82.3$\pm$0.6</td>
<td>78.2$\pm$0.3</td>
<td>82.7$\pm$0.6 $\uparrow$ 4.7</td>
<td>79.0$\pm$0.5 $\uparrow$ 0.0</td>
<td>83.0$\pm$1.0 $\uparrow$ 5.0</td>
<td>79.7$\pm$0.6 $\uparrow$ 0.7</td>
</tr>
<tr>
<td></td>
<td>HellaSwag</td>
<td>27.8</td>
<td>27.5</td>
<td>32.5$\pm$0.2</td>
<td>32.7$\pm$0.3</td>
<td>34.2$\pm$0.2 $\uparrow$ 6.4</td>
<td>33.4$\pm$0.2 $\uparrow$ 0.9</td>
<td>33.7$\pm$0.6 $\uparrow$ 5.9</td>
<td>33.2$\pm$0.3 $\uparrow$ 5.7</td>
</tr>
<tr>
<td></td>
<td>Story Cloze</td>
<td>86.5</td>
<td>85.1</td>
<td>89.6$\pm$0.0</td>
<td>88.7$\pm$0.0</td>
<td>-</td>
<td>-</td>
<td>87.3$\pm$0.1 $\uparrow$ 0.8</td>
<td>86.9$\pm$0.2 $\uparrow$ 1.8</td>
</tr>
<tr>
<td>Coref.</td>
<td>Wino.</td>
<td>50.9</td>
<td>50.5</td>
<td>51.1$\pm$0.1</td>
<td>50.7$\pm$0.1</td>
<td>52.0$\pm$0.3 $\uparrow$ 1.1</td>
<td>51.4$\pm$0.0 $\uparrow$ 0.9</td>
<td>52.1$\pm$0.3 $\uparrow$ 1.2</td>
<td>51.2$\pm$0.2 $\uparrow$ 0.7</td>
</tr>
<tr>
<td></td>
<td>WSC</td>
<td>69.2</td>
<td>64.4</td>
<td>69.2$\pm$0.0</td>
<td>64.6$\pm$0.3</td>
<td>58.3$\pm$1.1 $\downarrow$ 10.9</td>
<td>59.3$\pm$2.0 $\downarrow$ 5.1</td>
<td>57.7$\pm$0.0 $\downarrow$ 11.5</td>
<td>58.8$\pm$0.6 $\downarrow$ 5.6</td>
</tr>
<tr>
<td>WSD</td>
<td>WIC</td>
<td>50.3</td>
<td>50.4</td>
<td>50.3$\pm$0.0</td>
<td>50.3$\pm$0.0</td>
<td>55.4$\pm$1.1 $\uparrow$ 5.1</td>
<td>54.4$\pm$0.7 $\uparrow$ 4.0</td>
<td>55.5$\pm$0.8 $\uparrow$ 5.2</td>
<td>54.8$\pm$0.5 $\uparrow$ 4.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy results on the validation set of 11 NLP datasets based on the T0-3B model. Swarm Distillation (train) and Swarm Distillation (test) use the unlabeled training split and validation split of datasets to train the model respectively, corresponding to training-time and test-time tuning. The Story Cloze dataset does not have a training split and its self distillation results are from tuning on the validation split. We report the mean and std across 3 random runs, and also denote the absolute accuracy change compared to the T0-3B baseline.</p>
<h3>4.2 Evaluation</h3>
<p>Metrics: We use accuracy as the metric for all datasets. We report two different types of accuracy given that we have multiple prompts. The ensemble accuracy (Ens.) averages the output distributions of multiple prompts and makes predictions according to it. Ensembling multiple prompts has been explored before and found superior to using a single prompt <em>Jiang et al. (2020); Qin and Eisner (2021)</em>. The median accuracy (Med.) within the set of prompts serves as a proxy for the expected performance when users specify a single prompt and input a prompt-formatted example. As our approach assumes availability of a set of prompts for the downstream task, and it is relatively cheap to craft several prompts for a task, ensemble prediction is the better option given input $\mathbf{x}$, and it does empirically yield higher accuracy overall than the median for both the baseline and our method. Therefore, we will report both numbers but mainly discuss ensemble accuracy. We report these metrics on the validation split of each dataset. We run the experiments with 3 random seeds and report the mean and standard deviation.</p>
<p>Evaluation scenarios: We provide our methods with different unlabeled sources which lead to two practical scenarios during evaluation: (1) trainingtime tuning: we use the unlabeled training split from the corresponding dataset to train the model. This is similar to traditional settings where training and test data are different; and (2) test-time tuning <em>Sun et al. (2020); Wang et al. (2021)</em>: we directly adapt the PLM on the test data. This setting is reasonable, as we will always have access to the test inputs at test time. Intuitively, the unlabeled test sample $\mathbf{x}$ often provides hints about the distribution it was drawn, suggesting that we may update the model before making the prediction. This scenario is attractive since it alleviates the common distribution mismatch issue when there is a distribution shift between the training and test data. Compared to training-time tuning, test-time tuning typically uses less unlabeled data in our experiments since it uses the validation split itself. In the major experiments, we focus on the offline testtime tuning where we assume access to the entire test data and train our approach on all test examples, while in $\S 4.4$ we will discuss the potential for online adaptation where data arrives in a stream.</p>
<p>Baselines: As far as we know, there is no prior work studying unsupervised approaches for this prompt-based task generalization setting, thus T0 is the main baseline that we compare our approach against. However, we still implement an ablation baseline, self distillation, to separate the improvement from optimizing prompt consistency and pseudo-label distillation. Specifically, self distillation minimizes the same loss as in Eq. 2 but with $r^{(i)}=r^{(j)}$ - instead of pairwise distillation, the prompt always distills its own prediction to itself. This baseline can be viewed as a prompt</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>T0-11B</th>
<th>Swarm Dist.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Ens.</td>
<td>Med.</td>
</tr>
<tr>
<td>WSC</td>
<td>63.5</td>
<td>62.5</td>
</tr>
<tr>
<td>RTE</td>
<td>83.8</td>
<td>82.0</td>
</tr>
<tr>
<td>HellaSwag</td>
<td>34.4</td>
<td>33.6</td>
</tr>
<tr>
<td>WIC</td>
<td>57.2</td>
<td>56.8</td>
</tr>
<tr>
<td>Table 2: Accuracy based on T0-11B.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: Ensemble accuracy on the distribution shift setting based on T0-3B. “No Shift” represents the original setting where we train the swarm distillation loss on the training split from the same dataset as the test examples. “SD on MNLI/QNLI” represents swarm distillation trained on the training split of MNLI/QNLI.
version of self-training, which has proven to effectively utilize unlabeled data [he2020unsupervised; xie2020unsupervised; zhang2020unsupervised]. We report self distillation results in the training-time tuning setting only for simplicity.</p>
<h3>4.3 Results</h3>
<p>How well does swarm distillation work? We first compare swarm distillation against the T0-3B baseline. As shown in Table 1, the ensemble accuracy of swarm distillation exceeds the T0-3B baseline on 9 out of 11 datasets in both trainingand test-time tuning settings. Particularly, our approach improves the zero-shot performance on RTE by around 10 absolute points in all cases. Our approach slightly hurts ensemble accuracy of ANLI R3 and median accuracy of CB, but is overall comparable on these two datasets. Compared to self distillation, swarm distillation outperforms it on 9 out of 11 datasets in terms of ensemble accuracy, by up to 10.3 absolute points. These results further confirm the effectiveness of encouraging prompt consistency. We note that swarm distillation severely fails on WSC with a 10-point accuracy decrease compared to both T0 and self distillation, this is because Fleiss’ kappa selects a bad model checkpoint, while our approach actually improves the performance on WSC in the middle of training as we will discuss more in §4.4. Notably, our approach is helpful on several datasets where T03B only shows nearly chance accuracy, such as ANLI R1/R2/R3 (3 labels), HellaSwag (4 labels), Winogrande (2 labels), and WIC (2 labels). In addition, we observe that swarm distillation in the test-time tuning setting performs comparably well to the training-time one despite using much less training data, as shown in Appendix A. It is worth noting that prompt-based zero-shot task generalization is challenging, for example, T0 with even 11 billion parameters reports a median accuracy of only $\sim 40$ on ANLI R1/R2/R3, 33.7 on HellaSwag, and 57.2 on WIC [sanh2022wisdom]. Our results are surely still far from satisfactory, yet we hope to inspire future research to explore unlabeled data to build better zero-shot learners.</p>
<p>Scaling to 11B parameters: We now evaluate our method based on the largest version of T0 model, T0-11B. T0-11B is a very powerful zeroshot baseline that greatly outperforms GPT3. Due to the expensive computation to train T0-11B, we use one dataset per task, a total of 4 datasets as our benchmark, and only run with one random seed in the test-time tuning setting. Results are shown in Table 2. Swarm distillation outperforms T0-11B on all 4 datasets in terms of ensemble accuracy, and notably, improves the ensemble accuracy on HellaSwag from 34.4 to 45.0 without any annotation. Table 1 and Table 2 demonstrate the effectiveness of swarm distillation across different model sizes.</p>
<p>Robustness of swarm distillation: The main experiments so far train and test on examples from the same dataset, thus the results are based on the assumption that the training and test distributions are similar. To relax this assumption, here we evaluate swarm distillation with a more difficult setting by collecting unlabeled training examples from other datasets that are not listed in $\S 4.1$. Specifically, we focus on the NLI task, train swarm distillation on the unlabeled training split of MNLI [williams2018mnist] and QNLI [wang2019qnli] respectively, and evaluate it on RTE and ANLI. Table 3 shows that our proposed method still performs well in such a distribution shift setup - the models trained on MNLI or QNLI help improve the T0-3B baseline in most cases. The results from MNLI training are generally comparable to the original "No Shift" numbers, while QNLI training causes a mild accuracy drop.</p>
<h3>4.4 Analysis</h3>
<p>Are predictions more consistent across prompts after swarm distillation? We wonder whether</p>
<table>
<thead>
<tr>
<th></th>
<th>RTE</th>
<th>CB</th>
<th>ANLI R1</th>
<th>ANLI R2</th>
<th>ANLI R3</th>
<th>COPA</th>
<th>HS</th>
<th>Story.</th>
<th>Wino.</th>
<th>WSC</th>
<th>WIC</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>T0-3B</td>
<td>0.644</td>
<td>0.440</td>
<td>0.221</td>
<td>0.189</td>
<td>0.170</td>
<td>0.586</td>
<td>0.164</td>
<td>0.765</td>
<td>0.396</td>
<td>0.255</td>
<td>0.398</td>
<td>0.384</td>
</tr>
<tr>
<td>Swarm Dist.</td>
<td>0.662</td>
<td>0.254</td>
<td>0.145</td>
<td>0.156</td>
<td>0.177</td>
<td>0.699</td>
<td>0.402</td>
<td>0.862</td>
<td>0.509</td>
<td>0.462</td>
<td>0.517</td>
<td>$\mathbf{0 . 4 4 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Fleiss' kappa on 11 datasets based on T0-3B. Swarm distillation is trained on the training split of the respective dataset.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Analysis results to compare the model checkpoints selected by the unsupervised criterion Fleiss' kappa with the oracle model checkpoints selected by validation accuracy.
the gains of swarm distillation are attained together with more consistent predictions across different prompts. To this end, we report Fleiss' kappa, a commonly used metric for group agreement as detailed in §3.3. Results are shown in Table 4. Fleiss' kappa on 8 out of 11 datasets increases after swarm distillation, which boosts the averaged Fleiss' kappa of T0-3B by $14.6 \%$ relatively. This implies that swarm distillation facilitates prompt consistency, and potentially improves the robustness of PLMs to different wording of prompts.</p>
<p>Does the unsupervised criterion select the best model checkpoint? In $\S 3.3$, we discussed using Fleiss' kappa to select the best model checkpoint for evaluation, here we report the oracle accuracy numbers obtained by selecting the model checkpoint with the best validation accuracy, and compare it to the one selected by Fleiss' kappa. We compare the ensemble accuracy using T0-3B in the training-time tuning setting, with results in Figure 3. On most of the datasets, Fleiss' kappa is able to achieve numbers close to the best ones. On all 11 datasets, our oracle number outperforms the T0-3B baseline. In Table 1 we show that swarm distillation hurts the performance on WSC a lot, while in Figure 3 swarm distillation (oracle) in fact outperforms T0-3B, implying that the issue lies on model selection. Therefore, swarm distillation could potentially work better if an annotated dev set is available or when it is combined with other tech-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ensemble accuracy of swarm distillation on three example datasets based on T0-3B, demonstrating the effect of prompt size and unlabeled data size.
niques in few-shot learning settings, where good checkpoints may be selected out more easily.</p>
<p>How many prompts do we need? Our approach requires a diverse set of prompts to regularize prompt consistency. Here we perform ablation experiments to understand the effect of the number of prompts on the performance. We take COPA and ANLI R2 as example datasets which have 8 and 15 prompts, respectively. We then vary the number of available prompts by randomly sampling a subset of prompts before training. We report the ensemble accuracy of swarm distillation (train) in Figure 4a. On both COPA and ANLI R2, we observe gains as we increase the number of prompts from 0 (the baseline), yet the performance saturates very quickly and relatively stabilizes when we provide 4 prompts. This implies that swarm</p>
<p>distillation is not prompt-hungry and could work well with a small number of prompts. We note the with one prompt here Eq. 2 degenerates to a weaker version of self distillation compared to the one in Table 1 - self distillation in Table 1 utilizes all prompts during training while we assume access to only one prompt here.</p>
<p>How many unlabeled examples do we need? We measure the effect of unlabeled data size. Specifically, we randomly sample a subset of examples from the train split for training and report results on the entire validation dataset. Results on WIC and ANLI R2 are shown in Figure 4b. Notably, swarm distillation is able to outperform the baselines (#examples=0) by a large margin on both datasets with only 10 unlabeled examples, and the performance starts to saturate quickly afterward. These results suggest that swarm distillation is not data-hungry and works reasonably well with few unlabeled examples, allowing swarm distillation to remain as a relatively light approach while typical unsupervised training (e.g. pretraining) often requires a large amount of data and computation. Also, we argue that the phenomenon demonstrated in the results implies that swarm distillation may be applied to the online setting of test-time tuning, where the batches of test data arrive in a stream. Online test-time tuning is a practical setting in real life, and we leave it as future work to study.</p>
<h2>5 Discussion</h2>
<p>In this paper, we explore prompt consistency regularization to make PLMs better zero-shot learners. Our approach, swarm distillation, utilizes unlabeled examples to attain zero-shot gains. While we use swarm distillation in a post-adaptation manner, it could be potentially combined with the pretraining objectives in the pretraining stage (e.g. the multi-prompt training loss (Sanh et al., 2022; Wei et al., 2022)), or even with annotated data in fewshot learning settings. Combining swarm distillation with these other losses may easily bypass the model collapse issue since the other loss typically discourages the collapsed local optimum.</p>
<h2>Acknowledgement</h2>
<p>We thank Victor Sanh and Colin Raffel for help on using the T0 model. This work was supported in part by the CMU-Portugal MAIA Project, a Baidu PhD Fellowship for Junxian He, and a CMU Presidential Fellowship for Chunting Zhou.</p>
<h2>Limitations</h2>
<p>There are two limitations of our work: (1) Because our method is operated in a fully unsupervised manner, there is no supervised development data for us to either select the best model or tune hyperparameters. Thus, we propose to use Fleiss' Kappa as our unsupervised development metric for model selection, which attains decent performance in most cases. However, we also observe on very few datasets that the proposed metric fails to select the best checkpoints and hurt the model's performance. As discussed in $\S 4.4$, our method can be combined with few-shot learning where a few labeled data are provided, and we believe this can largely alleviate the issues of model selection in the unsupervised setting. (2) The other limitation and at the same time an advantage of our method is that the proposed method can work well even with 10 unlabeled data points. This certainly makes our method a good candidate for the online setting where batches of test data come in a stream. However, as we discussed in $\S 4.4$, the performance of our model saturates quickly as we increase the number of unlabeled data, which means the performance of our method cannot scale well with tons of unlabeled data like self-supervised pretraining. As discussed in $\S 5$, we expect combining our method with few-shot learning setting / pre-training can lead to further improvements as the supervised signals may guide the model to a better local optimum.</p>
<h2>Ethics Statement</h2>
<p>Similar to T0, this work aims to produce an openended system that could perform all text-based tasks through designing different prompts. While the performance of GPT3, T0, and this work is far from the practical level on unseen tasks, we expect a greatly improved prompt-based system in the future could be built to help perform many daily tasks in real life. However, the resulted model in this paper also admits the same ethics concerns that T0 has. For example, the unrestricted use of prompts may easily trigger offensive generations or private information leakage, and how to fix unwanted LM behaviors is still an active research problem (Liu et al., 2021a; Perez et al., 2022).</p>
<h2>References</h2>
<p>Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Ab-</p>
<p>heesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. Promptsource: An integrated development environment and repository for natural language prompts.</p>
<p>Philip Bachman, Ouais Alsharif, and Doina Precup. 2014. Learning with pseudo-ensembles. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3365-3373.</p>
<p>Lucas Beyer, Xiaohua Zhai, Avital Oliver, and Alexander Kolesnikov. 2019. S4L: self-supervised semisupervised learning. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 1476-1485. IEEE.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750-15758.</p>
<p>Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 19141925, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107-124.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031.</p>
<p>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</p>
<p>Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. 2020. Revisiting self-training for neural sequence generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2022. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. ArXiv preprint, abs/1503.02531.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Yoon Kim and Alexander M. Rush. 2016. Sequencelevel knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317-1327, Austin, Texas. Association for Computational Linguistics.</p>
<p>Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations.</p>
<p>Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning.</p>
<p>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175-184, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582-4597, Online. Association for Computational Linguistics.</p>
<p>Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021a. DExperts: Decodingtime controlled text generation with experts and antiexperts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6691-6706, Online. Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ArXiv preprint, abs/2107.13586.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692.</p>
<p>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2018. Virtual adversarial training: a regularization method for supervised and semisupervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):19791993.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,</p>
<p>Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguistics.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association for Computational Linguistics.</p>
<p>Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red teaming language models with language models. ArXiv preprint, abs/2202.03286.</p>
<p>Mohammad Taher Pilehvar and Jose CamachoCollados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267-1273, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. Zerooffload: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series.</p>
<p>Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. 2016. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 510, 2016, Barcelona, Spain, pages 1163-1171.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Timo Schick and Hinrich Schütze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2021b. It's not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352, Online. Association for Computational Linguistics.</p>
<p>Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. 2020. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9229-9248. PMLR.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, $B C$, Canada, pages 3261-3275.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th</p>
<p>International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. 2021. Tent: Fully test-time adaptation by entropy minimization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Qizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong, and Quoc Le. 2020a. Unsupervised data augmentation for consistency training. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and Quoc V. Le. 2020b. Self-training with noisy student improves imagenet classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10684-10695. IEEE.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yu Zhang, James Qin, Daniel S Park, Wei Han, ChungCheng Chiu, Ruoming Pang, Quoc V Le, and Yonghui Wu. 2020. Pushing the limits of semisupervised learning for automatic speech recognition. ArXiv preprint, abs/2010.10504.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Dataset</th>
<th>#train set</th>
<th>#validation set</th>
<th>#labels</th>
<th>#prompts</th>
</tr>
</thead>
<tbody>
<tr>
<td>NLI</td>
<td>RTE</td>
<td>2,490</td>
<td>277</td>
<td>2</td>
<td>10</td>
</tr>
<tr>
<td></td>
<td>CB</td>
<td>250</td>
<td>57</td>
<td>3</td>
<td>15</td>
</tr>
<tr>
<td></td>
<td>ANLI R1</td>
<td>16,946</td>
<td>1000</td>
<td>3</td>
<td>15</td>
</tr>
<tr>
<td></td>
<td>ANLI R2</td>
<td>45,460</td>
<td>1000</td>
<td>3</td>
<td>15</td>
</tr>
<tr>
<td></td>
<td>ANLI R3</td>
<td>100,459</td>
<td>1200</td>
<td>3</td>
<td>15</td>
</tr>
<tr>
<td>Compl.</td>
<td>COPA</td>
<td>400</td>
<td>100</td>
<td>2</td>
<td>8</td>
</tr>
<tr>
<td></td>
<td>HellaSwag</td>
<td>39,905</td>
<td>10,042</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>Story Cloze</td>
<td>-</td>
<td>1,871</td>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>Coref.</td>
<td>Winogrande</td>
<td>40,398</td>
<td>1,267</td>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td></td>
<td>WSC</td>
<td>554</td>
<td>104</td>
<td>2</td>
<td>10</td>
</tr>
<tr>
<td>WSD</td>
<td>WIC</td>
<td>5,428</td>
<td>637</td>
<td>2</td>
<td>10</td>
</tr>
</tbody>
</table>
<p>Table 5: Statistics of the datasets</p>
<h2>Appendix A Datasets</h2>
<p>We present the statistics of the 11 datasets in Table 5. For the training-time tuning scenario, we use up to 10,000 data points from the training set for training if the train set contains more than 10,000 data points.</p>
<h2>Appendix B Experimental Setup</h2>
<h3>B.1 LoRA Setup</h3>
<p>We use LoRA <em>Hu et al. (2022)</em> as our parameterefficient tuning model and set the bottleneck dimension of LoRA weight matrices to be 1 for both 3B and 11B models. We emphasize that the linear mapping matrix $B$ (or $A$) in LoRA needs to be initialized as a zero matrix to ensure the output distribution after adding LoRA layers is the same as the original PLM before training, otherwise, the zero-shot ability of PLMs would be broken upon initialization and there is no supervision to learn it back. For both models, we set the dropout probability for the the LoRA intermediate representations to be 0.3. Let $\alpha$ denote the scaling factor of LoRA that is used to scale the output of the LoRA layer before adding to the hidden states of the pre-trained model. We set $\alpha$ to be 4 and 2 respectively for the 3B and 11B model. The peak learning rates of the 3B and 11B models are set to be 3e-5 and 5e-5 respectively with a warm-up stage of 100 steps and polynomial learning rate scheduler. We train for a maximum of 1,500 steps. Note that the hyperparameters for the 3B model is tuned on the RTE dataset and used for other datasets. We did not tune the hyperparameters of the 11B model.</p>
<h3>B.2 Implementation Details</h3>
<p>The reported T0 baseline numbers are obtained from our own running using the released T0 weights. We are able to reproduce the numbers reported in Sanh et al. <em>Sanh et al. (2022)</em>, except for COPA where our T0 median number is higher than the originally reported one.</p>
<p>During training, at each update we first sample one input example $\mathbf{x}$ and apply all the prompts to reformat it as $r_{x}^{1}(\mathbf{x}),\cdots, r_{x}^{K}(\mathbf{x})$, then we perform inference for them and randomly shuffle the predictions. Next we iterate over them with a batch size of 5/10 (3B/11B) and use the shuffled predictions to supervise them to compute the distillation loss, this implements the swarm distillation mechanism in Eq. 2 and amounts to approximating the expectation over paired prompts with $K$ random pairs. We accumulate the gradients for 16 steps for one update so that each gradient descent is computed from 16 data examples. And we use 1 A40 GPU (45GB memory) to train the 3B model and 4 A40 GPUs with DeepSpeed Zero-2 <em>Ren et al. (2021)</em> to train the 11B model. In general, training converges pretty fast and takes around 1 - 3 GPU hours for the 3B model and 2 - 6 hours for the 11B model depending on early stop points of different datasets. We use Adam <em>Kingma and Ba (2015)</em> as the optimizer with $\beta_{1}=0.9$, $\beta_{2}=0.98$ and $\epsilon=1e-6$.</p>
<p>For the Transformer <em>Vaswani et al. (2017)</em> models with model dimension $d$, the feed-forward intermediate dimension $m$ and number of layers $l$, the additional parameters used in LoRA with bottleneck dimension $b$ is calculated as $b<em>(m+d)</em>2<em>l</em>2$. As we set $b$ to be 1 for both the 3B and 11B models, the additional number of LoRA parameters is 1,671,168 for the T0-3B model ($d=1024,m=$ $16384,l=24$) and 6,389,760 for the T0-11B model ($d=1024,m=65536,l=24$).</p>
<h2>Appendix C Ablation on LoRA and Model Selection</h2>
<p>We report the ablation results on LoRA and unsupervised model selection in Table 6. Full finetuning hurts the T0-3B performance on all datasets – actually it collapses on almost all the datasets when we check its predictions, which could partially explain the low accuracies. Using LoRA alone is able to improve full fine-tuning generally and outperforms the T0-3B baseline sometimes. Moreoever, we find that unsupervised model selection is very effective to mitigate collapse and greatly improves full fine-tuning results. Finally, combining LoRA</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">ANLI R1</th>
<th style="text-align: center;">ANLI R2</th>
<th style="text-align: center;">ANLI R3</th>
<th style="text-align: center;">COPA</th>
<th style="text-align: center;">HS</th>
<th style="text-align: center;">Story.</th>
<th style="text-align: center;">Wino.</th>
<th style="text-align: center;">WSC</th>
<th style="text-align: center;">WIC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T0-3B</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">50.3</td>
</tr>
<tr>
<td style="text-align: left;">Full fine-tuning</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">+ LoRA</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">+ model selection</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: left;">+ LoRA + model selection</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">55.4</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation results on LoRA and unsupervised model selection. The training objective is the swarm distillation loss. Numbers are ensemble accuracy in the training-time tuning setting based on T0-3B. "Full fine-tuning" updates all the model parameters, while " + LoRA" means that we freeze the T0 parameters and only update the LoRA parameters.
and unsupervised model selection gives the best results overall on these 11 datasets.</p>
<p>We clarify that the results in Table 6 are only for analysis purpose to better understand the effect of different components of our method, but were not used by us to make design decisions in our preliminary experiments- as stated in $\S 3.3$, we use LoRA because it collapses less often than full fine-tuning and develop a unsupervised model selection criterion since LoRA still collapses on some datasets. To judge collapse or not, we simply checked the model predictions, to see if the predictions for all the examples are almost the same.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Because the GPU memory sometimes cannot handle all the prompts within one batch.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
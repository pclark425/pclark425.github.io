<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3207 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3207</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3207</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-50f44ef10335d59cec145b15effae20ff22c1fdb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/50f44ef10335d59cec145b15effae20ff22c1fdb" target="_blank">ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper seeks inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning, and proposes a symbolic memory framework instantiated as an LLM and a set of SQL databases.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3207.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3207.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDB (LLM + Databases as Symbolic Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments an LLM controller with an external SQL database as symbolic memory and uses a chain-of-memory (CoM) approach: the LLM generates SQL operations to read/update the database step-by-step to solve multi-hop reasoning tasks reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent composed of an LLM controller (used in experiments: ChatGPT GPT-3.5 Turbo) that issues generated SQL statements to a MySQL database; it uses chain-of-memory to decompose user requests into intermediate memory (SQL) operations and then summarizes results back into natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external database (symbolic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory is implemented as one or more SQL tables in a MySQL database. The LLM generates explicit SQL statements (INSERT/UPDATE/DELETE/SELECT) to store, modify, and query structured historical records. Intermediate results are materialized in the DB and used by subsequent SQL steps (chain-of-memory). The DB executes calculations and returns exact results, which the LLM integrates into the final response.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fruit Shop Dataset (synthetic management / QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A synthetic dataset simulating fruit-shop records (purchases, sales, price changes, returns). The evaluation consists of 50 natural-language questions (15 easy, 35 hard) requiring retrieval, multi-hop reasoning, accurate arithmetic and stateful exclusion of returned transactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-hop reasoning over historical records</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>41 / 50 correct (82% accuracy); Easy: 13 / 15; Hard: 28 / 35</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a symbolic SQL database as external memory with chain-of-memory substantially improves multi-hop reasoning and precise calculations, preventing error accumulation; ChatDB (symbolic memory) achieves 82% vs ChatGPT baseline 22% on the Fruit Shop Dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No explicit empirical limitations of ChatDB reported in experiments; the paper notes schema creation can be manual or LLM-assisted but does not report failure modes specific to ChatDB in the evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3207.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3207.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo) baseline (prompt-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline LLM used in experiments where the entire history of records is provided inside the prompt (standard context window) and the model answers directly without an external symbolic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard ChatGPT (GPT-3.5 Turbo) used with temperature 0, given the dataset records as part of the prompt (within the model's context window) and asked to answer questions directly without issuing SQL or using an external DB.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window / prompt-based (no external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Historical records are concatenated into the prompt (up to context limit) and provided to the model; the model must internally reason over and compute answers from the prompt text without a structured external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fruit Shop Dataset (synthetic management / QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as ChatDB evaluation: answering 50 questions about shop history requiring multi-hop reasoning and arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>11 / 50 correct (22% accuracy); Easy: 10 / 15; Hard: 1 / 35</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When given full records in-prompt, ChatGPT performs reasonably on easy retrieval questions but fails on hard multi-hop calculations; it generates calculation and omission errors leading to severe performance degradation on hard questions (1/35).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Susceptible to arithmetic and multi-step reasoning errors and to token-context length limits; error accumulation and mis-tracking of returned transactions were explicitly observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3207.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3207.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT (tool-using agent / open-source project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An example of an agent that chains LLM prompts and external tools and uses a memory module that stores text prompts to keep track of history; mentioned as a prompt-based memory example in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A tool-using agent framework (community project) that orchestrates LLM calls and external tools; referenced here as an example of systems that store text prompts/history to support multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-based memory (text stored and re-inserted into prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores historical text (prompts/responses) and/or vector embeddings and retrieves relevant content to include in future prompts; memory is used as additional prompt context rather than a symbolic structured DB.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General multi-step tool-using tasks (not evaluated here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step automation and planning via chained LLM calls and external tools (web search, file I/O, etc.). In the paper, Auto-GPT is only cited as an example (no task-specific evaluation provided).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>tool use / multi-step automation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as illustrative of prompt-based memory approaches where text prompts/history are stored and re-used to maintain agent history.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3207.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3207.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that uses a memory module to store textual memory of agent experiences and retrieves them to produce more coherent, history-aware behaviors; cited as related work employing prompt-based memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that store textual memories (events/observations) and retrieve them to simulate long-term coherent behavior and dialogue; used as an example of memory-augmented agent design in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-based textual memory (episodic prompts / retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Keeps a memory buffer of textual events and uses retrieval to surface relevant memories into prompts to influence future responses and planning; memory is text-centric (not a structured symbolic DB).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive simulation of agent behavior (from Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-horizon simulation and interactive behavior requiring recall of past events and context to maintain consistent agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>simulation / dialogue / long-horizon behavior</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example where maintaining history in memory modules helps agents act consistently over time; used as contrast to ChatDB's symbolic DB approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3207.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3207.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer (RMT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer variant that augments inputs with additional memory tokens/matrices to read/write local and global information across long contexts; mentioned as a matrix-based memory baseline in comparison to symbolic DB memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent memory transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recurrent Memory Transformer (RMT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model that introduces persistent memory tokens appended to sequences to store and exchange information across segments, enabling scaling to longer contexts via trainable neural memory structures.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>matrix-based memory (trainable memory tokens / matrices)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Adds additional memory tokens/matrices that are updated by the network during processing; memory operations are learned and non-symbolic (neural-controlled read/write).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context sequence modeling / reasoning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed to process and propagate information across very long token sequences by using recurrent memory tokens; applicable to long-context tasks and multi-segment information exchange.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-context sequence modeling / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Characterized in the paper as a representative matrix-based memory approach: supports read/write but uses non-symbolic execution and has lower interpretability compared to symbolic DB memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Described as non-symbolic and lower interpretability; memory contents/dimensions lack explicit meaning which can complicate precise state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3207.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3207.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented language models / prompt-based memory (e.g., RAG, Retrieval-augmented in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of systems that retrieve relevant documents or embeddings from an external store and insert them into the LLM prompt to augment context; discussed as a common memory augmentation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented LLMs (RAG / retrieval-augmented in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Systems that use a retrieval module to find and return relevant textual passages or vector-embedded content from a memory store; retrieved content is added to the LLM prompt to inform generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented / prompt-based memory (document store + retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory is a corpus of text and/or vector embeddings; a retriever finds relevant entries by similarity and the selected texts are inserted into the model's prompt as context. Operations are non-symbolic and rely on similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive NLP tasks / long-context retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring external factual knowledge or long historical context beyond the model's parametric memory; retrieval augments prompt with targeted facts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / knowledge retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper contrasts retrieval/prompt-based memory with symbolic DB memory: retrieval helps supply relevant text but is non-symbolic, has limited support for updates/deletes, and may not track structured state precisely.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prone to retrieval errors and similarity-metric inaccuracies; limited interpretability and limited explicit state tracking compared to symbolic databases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3207.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3207.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Turing Machines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machines (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early neural architecture that augments RNNs with an external differentiable memory matrix and learns read/write operations via gradient descent; mentioned as prior neural memory work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Turing Machines (Graves et al., 2014)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neural models with a differentiable external memory matrix and attention-based read/write heads, trained end-to-end to learn memory interaction patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trainable external memory matrix (differentiable memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory is a continuous matrix; the model issues soft attention-based read and write operations learned through gradient descent. Memory contents are not symbolic and are distributed across vector dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequence learning tasks demonstrating algorithmic behaviors (original work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed for algorithmic tasks requiring storage and retrieval of intermediate results (e.g., copying, sorting); cited here as classical neural-memory architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>sequence algorithmic tasks / memory-augmented learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a representative neural memory approach that contrasts with symbolic DB memory; neural memory can be powerful but is approximate and may accumulate errors for long multi-step symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper highlights general limitations of neural memory approaches: non-symbolic storage, error accumulation, and lower interpretability for precise multi-hop symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent memory transformer <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Neural turing machines <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
                <li>Binding language models in symbolic languages <em>(Rating: 1)</em></li>
                <li>Sql-palm: Improved large language modeladaptation for text-to-sql <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3207",
    "paper_id": "paper-50f44ef10335d59cec145b15effae20ff22c1fdb",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "ChatDB",
            "name_full": "ChatDB (LLM + Databases as Symbolic Memory)",
            "brief_description": "A framework that augments an LLM controller with an external SQL database as symbolic memory and uses a chain-of-memory (CoM) approach: the LLM generates SQL operations to read/update the database step-by-step to solve multi-hop reasoning tasks reliably.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ChatDB",
            "agent_description": "An agent composed of an LLM controller (used in experiments: ChatGPT GPT-3.5 Turbo) that issues generated SQL statements to a MySQL database; it uses chain-of-memory to decompose user requests into intermediate memory (SQL) operations and then summarizes results back into natural language.",
            "memory_used": true,
            "memory_type": "external database (symbolic memory)",
            "memory_mechanism_description": "Memory is implemented as one or more SQL tables in a MySQL database. The LLM generates explicit SQL statements (INSERT/UPDATE/DELETE/SELECT) to store, modify, and query structured historical records. Intermediate results are materialized in the DB and used by subsequent SQL steps (chain-of-memory). The DB executes calculations and returns exact results, which the LLM integrates into the final response.",
            "task_name": "Fruit Shop Dataset (synthetic management / QA)",
            "task_description": "A synthetic dataset simulating fruit-shop records (purchases, sales, price changes, returns). The evaluation consists of 50 natural-language questions (15 easy, 35 hard) requiring retrieval, multi-hop reasoning, accurate arithmetic and stateful exclusion of returned transactions.",
            "task_type": "question answering / multi-hop reasoning over historical records",
            "performance_with_memory": "41 / 50 correct (82% accuracy); Easy: 13 / 15; Hard: 28 / 35",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Using a symbolic SQL database as external memory with chain-of-memory substantially improves multi-hop reasoning and precise calculations, preventing error accumulation; ChatDB (symbolic memory) achieves 82% vs ChatGPT baseline 22% on the Fruit Shop Dataset.",
            "limitations_or_challenges": "No explicit empirical limitations of ChatDB reported in experiments; the paper notes schema creation can be manual or LLM-assisted but does not report failure modes specific to ChatDB in the evaluation.",
            "uuid": "e3207.0",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ChatGPT-baseline",
            "name_full": "ChatGPT (GPT-3.5 Turbo) baseline (prompt-only)",
            "brief_description": "A baseline LLM used in experiments where the entire history of records is provided inside the prompt (standard context window) and the model answers directly without an external symbolic memory.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ChatGPT (GPT-3.5 Turbo) baseline",
            "agent_description": "Standard ChatGPT (GPT-3.5 Turbo) used with temperature 0, given the dataset records as part of the prompt (within the model's context window) and asked to answer questions directly without issuing SQL or using an external DB.",
            "memory_used": false,
            "memory_type": "context window / prompt-based (no external memory)",
            "memory_mechanism_description": "Historical records are concatenated into the prompt (up to context limit) and provided to the model; the model must internally reason over and compute answers from the prompt text without a structured external memory.",
            "task_name": "Fruit Shop Dataset (synthetic management / QA)",
            "task_description": "Same as ChatDB evaluation: answering 50 questions about shop history requiring multi-hop reasoning and arithmetic.",
            "task_type": "question answering / multi-hop reasoning",
            "performance_with_memory": null,
            "performance_without_memory": "11 / 50 correct (22% accuracy); Easy: 10 / 15; Hard: 1 / 35",
            "has_performance_comparison": true,
            "key_findings": "When given full records in-prompt, ChatGPT performs reasonably on easy retrieval questions but fails on hard multi-hop calculations; it generates calculation and omission errors leading to severe performance degradation on hard questions (1/35).",
            "limitations_or_challenges": "Susceptible to arithmetic and multi-step reasoning errors and to token-context length limits; error accumulation and mis-tracking of returned transactions were explicitly observed.",
            "uuid": "e3207.1",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Auto-GPT",
            "name_full": "Auto-GPT (tool-using agent / open-source project)",
            "brief_description": "An example of an agent that chains LLM prompts and external tools and uses a memory module that stores text prompts to keep track of history; mentioned as a prompt-based memory example in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Auto-GPT",
            "agent_description": "A tool-using agent framework (community project) that orchestrates LLM calls and external tools; referenced here as an example of systems that store text prompts/history to support multi-step tasks.",
            "memory_used": true,
            "memory_type": "prompt-based memory (text stored and re-inserted into prompts)",
            "memory_mechanism_description": "Stores historical text (prompts/responses) and/or vector embeddings and retrieves relevant content to include in future prompts; memory is used as additional prompt context rather than a symbolic structured DB.",
            "task_name": "General multi-step tool-using tasks (not evaluated here)",
            "task_description": "Multi-step automation and planning via chained LLM calls and external tools (web search, file I/O, etc.). In the paper, Auto-GPT is only cited as an example (no task-specific evaluation provided).",
            "task_type": "tool use / multi-step automation (mentioned)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Mentioned as illustrative of prompt-based memory approaches where text prompts/history are stored and re-used to maintain agent history.",
            "limitations_or_challenges": null,
            "uuid": "e3207.2",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive Simulacra of Human Behavior",
            "brief_description": "A referenced system that uses a memory module to store textual memory of agent experiences and retrieves them to produce more coherent, history-aware behaviors; cited as related work employing prompt-based memory.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al., 2023)",
            "agent_description": "Agents that store textual memories (events/observations) and retrieve them to simulate long-term coherent behavior and dialogue; used as an example of memory-augmented agent design in related work.",
            "memory_used": true,
            "memory_type": "prompt-based textual memory (episodic prompts / retrieval)",
            "memory_mechanism_description": "Keeps a memory buffer of textual events and uses retrieval to surface relevant memories into prompts to influence future responses and planning; memory is text-centric (not a structured symbolic DB).",
            "task_name": "Interactive simulation of agent behavior (from Park et al.)",
            "task_description": "Long-horizon simulation and interactive behavior requiring recall of past events and context to maintain consistent agent behavior.",
            "task_type": "simulation / dialogue / long-horizon behavior",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Cited as an example where maintaining history in memory modules helps agents act consistently over time; used as contrast to ChatDB's symbolic DB approach.",
            "limitations_or_challenges": null,
            "uuid": "e3207.3",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "RMT",
            "name_full": "Recurrent Memory Transformer (RMT)",
            "brief_description": "A transformer variant that augments inputs with additional memory tokens/matrices to read/write local and global information across long contexts; mentioned as a matrix-based memory baseline in comparison to symbolic DB memory.",
            "citation_title": "Recurrent memory transformer",
            "mention_or_use": "mention",
            "agent_name": "Recurrent Memory Transformer (RMT)",
            "agent_description": "Model that introduces persistent memory tokens appended to sequences to store and exchange information across segments, enabling scaling to longer contexts via trainable neural memory structures.",
            "memory_used": true,
            "memory_type": "matrix-based memory (trainable memory tokens / matrices)",
            "memory_mechanism_description": "Adds additional memory tokens/matrices that are updated by the network during processing; memory operations are learned and non-symbolic (neural-controlled read/write).",
            "task_name": "Long-context sequence modeling / reasoning (general)",
            "task_description": "Designed to process and propagate information across very long token sequences by using recurrent memory tokens; applicable to long-context tasks and multi-segment information exchange.",
            "task_type": "long-context sequence modeling / reasoning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Characterized in the paper as a representative matrix-based memory approach: supports read/write but uses non-symbolic execution and has lower interpretability compared to symbolic DB memory.",
            "limitations_or_challenges": "Described as non-symbolic and lower interpretability; memory contents/dimensions lack explicit meaning which can complicate precise state tracking.",
            "uuid": "e3207.4",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Retrieval-augmented LLMs",
            "name_full": "Retrieval-augmented language models / prompt-based memory (e.g., RAG, Retrieval-augmented in-context learning)",
            "brief_description": "A class of systems that retrieve relevant documents or embeddings from an external store and insert them into the LLM prompt to augment context; discussed as a common memory augmentation approach.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented LLMs (RAG / retrieval-augmented in-context learning)",
            "agent_description": "Systems that use a retrieval module to find and return relevant textual passages or vector-embedded content from a memory store; retrieved content is added to the LLM prompt to inform generation.",
            "memory_used": true,
            "memory_type": "retrieval-augmented / prompt-based memory (document store + retriever)",
            "memory_mechanism_description": "Memory is a corpus of text and/or vector embeddings; a retriever finds relevant entries by similarity and the selected texts are inserted into the model's prompt as context. Operations are non-symbolic and rely on similarity metrics.",
            "task_name": "Knowledge-intensive NLP tasks / long-context retrieval",
            "task_description": "Tasks requiring external factual knowledge or long historical context beyond the model's parametric memory; retrieval augments prompt with targeted facts.",
            "task_type": "question answering / knowledge retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Paper contrasts retrieval/prompt-based memory with symbolic DB memory: retrieval helps supply relevant text but is non-symbolic, has limited support for updates/deletes, and may not track structured state precisely.",
            "limitations_or_challenges": "Prone to retrieval errors and similarity-metric inaccuracies; limited interpretability and limited explicit state tracking compared to symbolic databases.",
            "uuid": "e3207.5",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Neural Turing Machines",
            "name_full": "Neural Turing Machines (NTM)",
            "brief_description": "An early neural architecture that augments RNNs with an external differentiable memory matrix and learns read/write operations via gradient descent; mentioned as prior neural memory work.",
            "citation_title": "Neural turing machines",
            "mention_or_use": "mention",
            "agent_name": "Neural Turing Machines (Graves et al., 2014)",
            "agent_description": "Neural models with a differentiable external memory matrix and attention-based read/write heads, trained end-to-end to learn memory interaction patterns.",
            "memory_used": true,
            "memory_type": "trainable external memory matrix (differentiable memory)",
            "memory_mechanism_description": "Memory is a continuous matrix; the model issues soft attention-based read and write operations learned through gradient descent. Memory contents are not symbolic and are distributed across vector dimensions.",
            "task_name": "Sequence learning tasks demonstrating algorithmic behaviors (original work)",
            "task_description": "Designed for algorithmic tasks requiring storage and retrieval of intermediate results (e.g., copying, sorting); cited here as classical neural-memory architecture.",
            "task_type": "sequence algorithmic tasks / memory-augmented learning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Mentioned as a representative neural memory approach that contrasts with symbolic DB memory; neural memory can be powerful but is approximate and may accumulate errors for long multi-step symbolic reasoning.",
            "limitations_or_challenges": "Paper highlights general limitations of neural memory approaches: non-symbolic storage, error accumulation, and lower interpretability for precise multi-hop symbolic reasoning.",
            "uuid": "e3207.6",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent memory transformer",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 2
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        },
        {
            "paper_title": "Binding language models in symbolic languages",
            "rating": 1
        },
        {
            "paper_title": "Sql-palm: Improved large language modeladaptation for text-to-sql",
            "rating": 1
        }
    ],
    "cost": 0.015347499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatDB: Augmenting LLMs with Databases as Their SYMBOLIC MEMORY</h1>
<p>Chenxu Hu ${ }^{1 *}$ Jie Fu ${ }^{2 * \dagger}$ Chenzhuang Du ${ }^{1}$ Simian Luo ${ }^{1}$ Junbo Zhao ${ }^{3}$ Hang Zhao ${ }^{1 \dagger}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Beijing Academy of Artificial Intelligence ${ }^{3}$ Zhejiang University<br>fujie@baai.ac.cn hangzhao@mail.tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) with memory are computationally universal (Schuurmans, 2023). However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https: //chatdatabase.github.io/.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall workflow of ChatDB. The LLM controller controls the read and write operations to the memory. The memory stores historical information and provides relevant historical information to assist in responding to user input. In ChatDB, we focus on augmenting LLMs with databases as their symbolic memory.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Large language models (LLMs), such as GPT-4 <em>(OpenAI, 2023)</em> and PaLM 2 <em>(Anil et al., 2023)</em>, have increasingly become an essential component of modern artificial intelligence (AI) systems, revolutionizing our understanding of natural language processing (NLP) and transforming various industries <em>(Hao et al., 2023; Wang et al., 2023)</em>. While LLMs have made significant strides in understanding and generating contextually relevant responses, they have limitations <em>(Chen et al., 2023)</em>. One of the main challenges is that multi-turn interactions with language models generate a large number of tokens, which can easily exceed the input token limit of LLMs. For example, GPT-4 (32K) can only handle 32,000 tokens. As the interaction progresses, the LLMs must maintain contextual information (e.g., user inputs, and previous responses) and generate responses based on the accumulated data. Simply concatenating all contextual information and cramming it into LLMs, however, can easily exceed the processing capabilities of LLMs and accumulate errors, causing the model to lose track of the conversation and produce less accurate responses.</p>
<p>Some neural memory mechanisms have been explored <em>(Wu et al., 2022a; Khattab et al., 2022; Zhong et al., 2022)</em> to overcome the limited token input issue of LLMs. The memory components serve as a storage and retrieval system for relevant information from previous interactions. However, augmenting LLMs with conventional neural memory usually leads to difficulties in storing, retrieving, and manipulating historical information in memory, especially for tasks requiring complex multi-hop reasoning. Two main causes are (a) They do not store historical information in a structured form; (b) Their manipulation of the information stored in memory is not symbolic, as they all rely on some vector similarity calculations, which can be inaccurate, thus leading to the accumulation of errors.</p>
<p>To address the aforementioned issues, we propose using databases as novel symbolic memory for LLMs. The whole framework is named ChatDB. As shown in Figure 1, ChatDB consists of two components: an LLM controller and its memory. The LLM controller can be any commonly used LLM <em>(OpenAI, 2023; Touvron et al., 2023; Du et al., 2022; Zeng et al., 2022)</em> and is responsible for controlling the read and write operations to the memory. The memory of LLMs, which can be symbolic or non-symbolic, or a combination of both, is responsible for storing historical information and providing information when needed to assist the LLM in responding to user input. In ChatDB, we focus on using databases as symbolic memory, which allows for the structured storage of historical information through the execution of a symbolic language, namely SQL statements. These SQL statements are generated by an LLM. Incorporating a database as symbolic memory is particularly useful in scenarios requiring precise recording, modification, querying, deletion, and analysis of historical data. For example, a store manager needs to maintain daily sales records, where using plain text or matrices as memory is unsuitable <em>(Chen et al., 2023)</em>. However, using a database as external symbolic memory is highly suitable. The database enables accurate operations, including data insertion, deletion, update, and selection, using SQL statements. Thus, employing databases as external symbolic memory ensures precision and efficiency in managing and manipulating historical data, significantly enhancing the performance of LLMs in scenarios that require high accuracy and long-term data recording and processing.</p>
<p>In the ChatDB framework, we propose the chain-of-memory (CoM) approach to manipulate the external symbolic memory more effectively, thereby further enhancing the reasoning capabilities of LLMs. The chain-of-memory approach transforms user input into a series of intermediate memory operation steps that lead to final results. Through the chain-of-memory approach, a complex problem is decomposed into multiple steps of memory operations, significantly reducing the complexity of problem-solving. In ChatDB, each intermediate step involves one or more SQL statements.</p>
<p>Our ChatDB makes several contributions to the field of LLMs. Firstly, we propose augmenting LLMs with databases as their external symbolic memory, allowing for structured storage of historical data and enabling symbolic and complex data operations using SQL statements. Secondly, our chain-of-memory approach enables effective memory manipulation by converting user input into multi-step intermediate memory operations, which enhance the performance of ChatDB, enabling it to handle complex, multi-table database interactions with improved accuracy and stability. Finally, our experiments demonstrate that augmenting LLMs with symbolic memory improves multi-hop reasoning capabilities and prevents error accumulation, thereby enabling ChatDB to significantly outperform ChatGPT on a synthetic dataset.</p>
<h2>2 Related Work</h2>
<p>Memory-Augmented Large Language Models (LLMs). LLMs, such as GPT-4 <em>(OpenAI, 2023)</em> and PaLM 2 <em>(Anil et al., 2023)</em>, have demonstrated powerful reasoning and decision-making abilities. However, LLMs are often hindered by their limited context window sizes (e.g., GPT-4 can only handle 32K tokens). Memory-augmented LLMs <em>(Wu et al., 2022a, b; Zhong et al., 2022; Lewis et al., 2020; Guu et al., 2020; Park et al., 2023; Khattab et al., 2022; Izacard et al., 2022)</em> incorporate a memory module that prevents the model from forgetting crucial information and allows it to handle long text inputs that exceed the context window size. Retrieval-augmented in-context learning <em>(Khattab et al., 2022)</em></p>
<p>uses retrieval models (RM) to retrieve relevant information that can be inserted into the LLM as a prompt. For example, Auto-GPT ${ }^{3}$ and Generative Agents (Park et al., 2023) utilize a memory module to store the text prompt directly, allowing the agent to keep track of its history. The past and current prompts are then input into the LLM for processing. Neural Turing Machines (NMT) (Graves et al., 2014), which incorporate the recurrent neural network (RNN) with external trainable memory resources and learn to interact with the memory module with gradient descent. Gated Graph Sequence Neural Network (GGS-NN) (Johnson, 2017) constructs and modifies graphs and utilizes the graphs to produce reasonable outputs. Recurrent Memory Transformer (RMT) (Bulatov et al., 2022) introduces additional memory tokens to the input and output sequences to store, process and exchange local and global information between segments of long sequences, and then train the model to control both memory operation and sequence representations processing.
Reasoning with LLMs. LLMs are known to struggle in complex reasoning tasks. Previous methods focus on incorporating specially designed supervisory signals or fine-tuning to enhance the reasoning ability of language models (Pikos et al., 2021; Ran et al., 2019; Andor et al., 2019; Cobbe et al., 2021; Chen et al., 2022). Recent methods mainly improve the reasoning ability of language models through In-Context Learning (Brown et al., 2020; Lester et al., 2021; Wei et al., 2021, 2022; Wang et al., 2022). The most representative of these is Chain-of-Thought (CoT) (Wei et al., 2022), which presents the intermediate reasoning process of solving sample problems to the language model, greatly enhancing its reasoning capabilities.
LLMs with DBs. LLMs have demonstrated an impressive capability in generating code, including Python code, execution commands for Excel, and Structured Query Language (SQL) for databases (OpenAI, 2023). ChatExcel ${ }^{4}$ uses LLMs to generate the Excel execution command, simplifying the user interaction process. BINDER (Cheng et al., 2022) proposes a framework that maps task inputs to executable programs in a programming language (e.g., Python code) bound with an API to call LLMs to perform a wide range of functionalities. SQL-PALM (Sun et al., 2023) proposes an LLM-based Text-to-SQL model, using the execution-based self-consistent prompting approach, and outperforms previous Text-2-SQL methods by a large margin. While previous works involve databases to some extent, our proposed ChatDB system differs significantly from these methods. In specific, ChatDB views the databases as the external symbolic memory module for the LLM, and then leverages the database for reading and writing essential data information to enhance the reasoning process via chain-of-memory, leading to more accurate reasoning results.
Tool-using LLMs. From the tool-using perspective, ChatDB can also be seen as an LLM utilizing DBs as a tool (Schick et al., 2023; Shen et al., 2023; Surs et al., 2023; Paranjape et al., 2023). Toolformer (Schick et al., 2023), through a series of demonstrations, instructs the language model that it can invoke some APIs to utilize external tools to solve the current problem. Another representative work is Auto-GPT ${ }^{5}$, which enables the language models to complete a series of impressive tasks using a search engine. The advantage of ChatDB, which uses a database as an external tool, is that it allows the language model to maintain more accurate records and use historical data, thereby addressing more complex problems, especially those that require accurate historical data for reasoning.</p>
<h1>3 ChatDB</h1>
<p>In this section, we first briefly introduce the task definition and setting. We then describe the overall framework of our proposed ChatDB. Finally, we delve into the details of the chain-of-memory approach, which is the main component of ChatDB.</p>
<h3>3.1 Task Definition</h3>
<p>Given a user input in natural language and details of existing tables in the database (not required if there are no existing tables), the goal is to manipulate the symbolic memory, i.e., the external database, to fulfill the user's request. For instance, if the user (e.g., a store manager) command is to record, modify, query, and delete specific data, the corresponding SQL operation should be to insert, update, select, and delete the relevant data in the appropriate tables, respectively. These operations often involve multiple tables in the database.</p>
<h3>3.2 Framework Overview</h3>
<p>ChatDB framework consists of three main stages: input processing, chain-of-memory, and response summary, as illustrated in Figure 2. Algorithm 1 provides a detailed illustration of the entire algorithmic procedure of ChatDB to respond to the user input.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ChatDB framework. The red arrow lines represent the process flow of chain-of-memory, indicating the connection between multiple memory operations. The red arrow lines between database tables represent the reference relationships between primary keys and foreign keys, which start from primary keys to foreign keys. Only the first four columns of each table are shown for brevity. This example showcases the process of returning goods purchased on 2023-01-02 by a customer with the phone number 823451.</p>
<p>Input Processing. ChatDB generates a series of intermediate steps to manipulate the symbolic memory by utilizing LLMs if responding to the user input requires the use of memory. Otherwise, we use LLMs directly to generate a reply.
Chain-of-Memory. ChatDB executes a series of intermediate memory operation steps to interact with symbolic memory. ChatDB manipulates the symbolic memory in sequence according to a series of previously generated SQL statements, including operations such as insert, update, select, delete, etc. The external database executes the corresponding SQL statements, updates the database, and returns the results. It is worth noting that ChatDB decides whether to update the memory operation step based on the results of previous SQL statements before executing this operation. ChatDB executes the next step following the same procedure until all operations on the memory are completed.
Response Summary. ChatDB summarizes the final response to the user based on the results of a series of chain-ofmemory steps.</p>
<h1>3.3 Chain-of-Memory</h1>
<p>Chain-of-thought (Wei et al., 2022) emphasizes decomposing complex reasoning into a series of intermediate steps. Chain-of-memory (CoM) can be seen as a way to augment chain-of-thought by providing a symbolic memory mechanism to support the storage associated with those intermediate steps.
The purpose of chain-of-memory is to enhance the reasoning capabilities and robustness of LLMs when manipulating symbolic memory. The approach involves converting the user input into a sequence of intermediate memory operations, enabling LLMs to more accurately and effectively manipulate the memory in a symbolic way. The ability to manipulate symbolic memory is particularly valuable for real-world applications that involve complex and accurate interactions with historical data, such as record-keeping and data analysis in a management setting.
To enhance the performance and stability of our approach, we employ in-context learning (Brown et al., 2020), providing prompt exemplars of several sequences of chain-of-memory steps, and chain-of-thought prompting. A robust and accurate chain-of-memory process enables LLMs to better reason over symbolic memory and handle more complex scenarios.</p>
<p>The advantages of chain-of-memory are twofold. Firstly, it enables LLMs to perform complex database manipulations with greater accuracy, enhancing their multi-hop reasoning ability over symbolic memory. Secondly, by decomposing complex operations into a sequence of intermediate memory operations, the chain-of-memory approach enhances the capability of LLMs when handling complex, multi-table interactions. This approach enables LLMs to handle edge cases and unexpected scenarios better, making it a promising method for real-world applications.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">algorithm</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">ChatDB</span>
<span class="nv">Input</span>:<span class="w"> </span><span class="nv">userInput</span>,<span class="w"> </span><span class="nv">dataBase</span>
<span class="nv">Output</span>:<span class="w"> </span><span class="nv">reply</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">manipulate</span><span class="w"> </span><span class="nv">memory</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">respond</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">user</span><span class="w"> </span><span class="nv">Input</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nv">memOps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">getSteps</span><span class="ss">(</span><span class="nv">userInput</span><span class="ss">)</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="nv">reply</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="ss">(</span><span class="nv">userInput</span><span class="ss">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">reply</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="nv">sqlResults</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[]
<span class="w">    </span><span class="nv">newMemOps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[]
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">memOp</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">memOps</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">update</span><span class="w"> </span><span class="nv">memOp</span><span class="w"> </span><span class="nv">based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">sqlResults</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nv">newMemOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">updateOperation</span><span class="ss">(</span><span class="nv">memOp</span>,<span class="w"> </span><span class="nv">sqlResults</span><span class="ss">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="nv">newMemOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">memOp</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="nv">sqlResult</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">executeOperation</span><span class="ss">(</span><span class="nv">newMemOp</span>,<span class="w"> </span><span class="nv">dataBase</span><span class="ss">)</span>
<span class="w">        </span><span class="nv">sqlResults</span>.<span class="nv">append</span><span class="ss">(</span><span class="nv">sqlResult</span><span class="ss">)</span>
<span class="w">        </span><span class="nv">newMemOps</span>.<span class="nv">append</span><span class="ss">(</span><span class="nv">newMemOp</span><span class="ss">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="nv">reply</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">summary</span><span class="ss">(</span><span class="nv">userInput</span>,<span class="w"> </span><span class="nv">newMemOps</span>,<span class="w"> </span><span class="nv">sqlResults</span><span class="ss">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">reply</span>
</code></pre></div>

<p>$\triangleright$ Begin of Input Processing
$\triangleright$ Generate intermediate steps using LLMs
$\triangleright$ Generate reply using LLMs directly
$\triangleright$ Begin of Chain-of-Memory</p>
<h1>3.4 Comparison with previous memory-augmented LLMs</h1>
<p>Table 1: Comparison with prompt-based memory and matrix-based memory.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Memory format</th>
<th style="text-align: center;">Supported operations</th>
<th style="text-align: center;">Memory storage</th>
<th style="text-align: center;">Memory execution</th>
<th style="text-align: center;">Interpretability</th>
<th style="text-align: center;">State tracking</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">ChatDB</td>
<td style="text-align: center;">Symbolic storage <br> (e.g., database)</td>
<td style="text-align: center;">Insert, Delete Update, Select</td>
<td style="text-align: center;">Structured</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-based</td>
<td style="text-align: center;">Auto-GPT</td>
<td style="text-align: center;">Content and its vector embeddings</td>
<td style="text-align: center;">Insert, Select</td>
<td style="text-align: center;">Semi-Structured</td>
<td style="text-align: center;">Non-Symbolic</td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">Matrix-based</td>
<td style="text-align: center;">RMT</td>
<td style="text-align: center;">Memory tokens/ memory matrices</td>
<td style="text-align: center;">Read, Write</td>
<td style="text-align: center;">Semi-Structured</td>
<td style="text-align: center;">Non-Symbolic</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Yes</td>
</tr>
</tbody>
</table>
<p>In this subsection, we provide a comprehensive comparison between ChatDB and recent approaches that augment Transformer-based language models with memory module. The memory modules of language models proposed in previous work can be broadly categorized into two types. The first type of memory stores context and uses a retrieval model to find content from past interactions that's most relevant to the current conversation, and then uses it as a prompt for the language model (Khattab et al., 2022). We refer to this type of memory as prompt-based memory. The second type of approach utilizes additional memory tokens or memory matrices as memory (Bulatov et al., 2022), which we refer to as matrix-based memory. We compare ChatDB with these approaches based on the following aspects:</p>
<ol>
<li>Memory Format. This aspect relates to the format used for storing memory. ChatDB utilizes databases as its memory. Prompt-based memory (Park et al., 2023) stores relevant interaction content and/or their corresponding vector embeddings. Matrix-based memory employs additional trainable memory tokens (Bulatov et al., 2022, 2023) or trainable memory matrices (Graves et al., 2014).</li>
<li>Supported Operations. This aspect refers to the operations supported for manipulating memory. ChatDB supports operations such as insertion, deletion, update, and selection of data in the database memory. The prompt-based memory primarily supports insertion and selection operations but lacks complete support for updates and deletions. Matrix-based memory supports reading (selection) and writing (insertion, updating, deletion) operations. However, the exact operations performed by the neural network are not explicitly known.</li>
<li>Memory Storage. This aspect refers to the format in which data is stored in memory, specifically whether it is structured or not. ChatDB stores memory in a structured format using databases, while both the prompt-based memory</li>
</ol>
<p>and matrix-based memory are viewed as semi-structured. The reason is that vector embeddings and memory matrices have specific dimensions and sizes, but each individual dimension does not carry a specific and explicit meaning.
4. Memory Execution. This aspect focuses on how memory operations are executed, specifically whether they are symbolic or not. ChatDB executes operations on its database memory using SQL, which is a symbolic language, thus making it inherently symbolic. Prompt-based memory performs selections based on a similarity metric using vector embeddings and employs a language encoder to obtain vector embeddings for insertions. Both of these are considered non-symbolic executions. In matrix-based memory augmentated LLMs, memory operations are entirely controlled by the neural network, resulting in non-symbolic executions as well.
5. Interpretability. This aspect refers to the degree of interpretability of memory. In ChatDB, memory is stored in a structured and explicit format, and its operations are symbolic, leading to a high level of interpretability. In the prompt-based memory, interpretability is generally limited due to inherent challenges in interpreting vector embeddings. For matrix-based memory approach, the degree of interpretability is low since memory is completely controlled by the neural network implicitly.
6. State Tracking. This aspect refers to whether the memory effectively tracks the current state of the LLMs. In the case of ChatDB, its memory accurately tracks the current state of the LLMs. The fruit shop experiment serves as a demonstration, where after processing each record, ChatDB's database memory is updated to reflect the latest state of the fruit shop. This showcases how ChatDB's memory effectively tracks its current state. Thanks to the symbolic memory execution, ChatDB's memory allows for easy rollback to any desired timestamp, providing greater flexibility and controllability. In matrix-based memory approach, memory is continuously updated and altered by the model itself, enabling it to track the current state of the LLMs. However, the prompt-based memory approach simply stores the historical context and only knows what has happened in the past without a clear understanding of the current state.
By examining these aspects, we observe the distinctive features and capabilities of ChatDB in comparison to existing approaches. The superiority of ChatDB highlights the advantages of using symbolic memory to augment LLMs.</p>
<h1>4 Evaluation</h1>
<p>In this section, we conduct experiments to evaluate the effectiveness of augmenting LLMs with databases as their symbolic memory. Our experiment results demonstrate that ChatDB significantly outperforms the baseline model ChatGPT, highlighting the advantages of symbolic memory integration.</p>
<h3>4.1 Experimental Setup</h3>
<p>As previously mentioned, using a database as symbolic memory is particularly suitable for scenarios that require precise recording and processing of historical information, such as various data management scenarios. To adapt to the use case of ChatDB and enable quantitative comparisons with other models, we constructed a synthetic dataset simulating the management of a fruit shop.
Furthermore, in order to evaluate the performance of the models, we collected a set of 50 questions with annotated standard answers. These questions varied in difficulty, ranging from hard ones that required multi-hop reasoning to easy ones that only required retrieving information from historical data. There are 15 easy questions and 35 hard questions. Each question is independently answered by the models.</p>
<h3>4.1.1 Model Configuration</h3>
<p>ChatDB. The LLM used in ChatDB is ChatGPT (GPT-3.5 Turbo), and the hyperparameter temperature is set to 0 . We used the MySQL database as the external symbolic memory.
Baseline. We use ChatGPT (GPT-3.5 Turbo) as the baseline model with a maximum token length of 4096. Similar to ChatDB, we set the temperature to 0 .</p>
<h3>4.1.2 Dataset</h3>
<p>We synthesize a dataset of fruit shop management records, referred to as the "Fruit Shop Dataset". This dataset simulates four common operations in a shop: purchasing, selling, changing prices, and goods returns. We ensure that all historical records are valid and do not encounter issues such as negative inventory. We generate 70 records arranged in chronological order, totaling approximately 3.3 k tokens, which is within the maximum token length limit of ChatGPT (4096 tokens).</p>
<p>(a) Purchasing goods: the shop purchases goods from suppliers.</p>
<p>Record: We restocked our store on 2023-01-01 with a new supply of fruits from "ABC" (abc_sup@example.com, 10080). The purchased quantities include 24 kg cherry, at unit prices of 0.8 . Our intended selling price of cherry is 1.3 dollars per unit.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Insert supplier 'ABC' if not exists
INSERT INTO suppliers (supplier_name, contact_number, email)
SELECT 'ABC', "10080", 'abc_sup@example.com'
WHERE NOT EXISTS (SELECT 1 FROM suppliers WHERE supplier_name $\sim$ - "ABC");
Step2: Insert fruit (set the selling price to NULL and stock quantity to 0 ) if not exists</p>
<p>INSERT INTO fruits (fruit_name, selling_price, stock_quantity, $\sim$ fruit_type, shelf_life)
SELECT 'cherry', NULL, 0, NULL, NULL
WHERE NOT EXISTS (SELECT 1 FROM fruits WHERE fruit_name $\sim$ 'cherry');
Step3: Insert purchase
INSERT INTO purchases (supplier_id, purchase_date, $\sim$ total_cost)
VALUES (SELECT supplier_id FROM suppliers WHERE
$\sim$ supplier_name - "ABC"), "2023-01-01", 24 - 0.8);
Step4: Insert purchase item
INSERT INTO purchase_items (purchase_id, fruit_id,
$\sim$ quantity_purchased, cost_per_item, item_total_cost)
VALUES (SELECT MAX(purchase_id) FROM purchases), (SELECT
$\sim$ fruit_id FROM fruits WHERE fruit_name - "cherry"), 24,
$\sim 0.8,24$ - 0.8);
Step5: Update the stock quantity of cherry
UPDATE fruits
SET stock_quantity = stock_quantity + 24
WHERE fruit_name = "cherry";
Step6: Update the selling price of cherry if given new selling price
UPDATE fruits
SET selling_price $=1.3$
WHERE fruit_name = "cherry";
(c) Return of goods: customers bring back purchased items to the shop.</p>
<p>Record: Because the customer returned their purchase, we are required to undo the sales transaction made by customer 'John Doe' (phone: 123-4567890, email: john.doe@example.com) on 2023-01-05.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Find the sale_id for this customer on this date
SELECT sale_id FROM sales
WHERE customer_id = (SELECT customer_id FROM customers WHERE
$\sim$ phone_number = "123-456-7890" AND email =
"john.doe@example.com") AND sale_date = "2023-01-05";
Database response:
$\qquad$
| sale_id |
$+$............
1 9
$+$............
Step2: Get all the fruit_id and quantity_sold for this sale, replace <sale_id> with the results from the previous queries
SELECT fruit_id, quantity_sold FROM sale_items
WHERE sale_id = 9 [<sale_id>];
Database response:
$+$.............
| fruit_id | quantity_sold |
$+$.............
| 7 | 10
1 0
$+$............
$\square$</p>
<h2>(b) Selling goods: customers buy goods from the shop</h2>
<p>Record: A sale was made on 2023-01-02 to 'Bob Smith' (contact details: 123-456-7893, bob.smith@example.com). The items purchased were 9 kg apple, 4 kg cherry.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Insert customer 'Bob Smith' if not exists
INSERT INTO customers (first_name, last_name, phone_number, $\sim$ email)
SELECT 'Bob', 'Smith', "123-456-7893",
$\sim$ 'bob.smith@example.com'
WHERE NOT EXISTS (SELECT 1 FROM customers WHERE phone_number $\sim$ = "123-456-7893");
Step2: Insert sale
INSERT INTO sales (customer_id, sale_date, total_price)
VALUES (SELECT customer_id FROM customers WHERE phone_number
$\sim$ = "123-456-7893"), "2023-01-02", (SELECT selling_price
$\sim$ FROM fruits WHERE fruit_name = "apple") + 9 + (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "cherry") + $\sim$ 4);</p>
<p>Step3: Insert sale item
INSERT INTO sale_items (sale_id, fruit_id, quantity_sold,
$\sim$ price_per_item, item_total_price)
VALUES (SELECT MAX(sale_id) FROM sales), (SELECT fruit_id
$\sim$ FROM fruits WHERE fruit_name = "apple"), 9, (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "apple"),
$\sim$ (SELECT selling_price FROM fruits WHERE fruit_name =
$\sim$ "apple") + 9),
((SELECT MAX(sale_id) FROM sales), (SELECT fruit_id FROM
$\sim$ fruits WHERE fruit_name = "cherry"), 4, (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "cherry"),
$\sim$ (SELECT selling_price FROM fruits WHERE fruit_name =
$\sim$ "cherry") + 4);
Step4: Update the stock quantity of apple and cherry
UPDATE fruits
SET stock_quantity = CASE
WHERE fruit_name = "apple" THEN stock_quantity = 9
WHERE fruit_name = "cherry" THEN stock_quantity = 4
ELSE stock_quantity
END
WHERE fruit_name IN ("apple", "cherry");</p>
<p>Step3: Increase the stock_quantity for each fruit sold in this sale, replace <quantity_sold> <fruit_id> with the results from the previous queries
UPDATE fruits
SET stock_quantity = stock_quantity + 10 [<quantity_sold>]
WHERE fruit_id = 7 [<fruit_id>];
UPDATE fruits
SET stock_quantity = stock_quantity + 9 [<quantity_sold>]
WHERE fruit_id = 6 [<fruit_id>];
Step4: Delete the sale items for this sale, replace <sale_id> with the results from the previous queries
DELETE FROM sale_items WHERE sale_id = 9 [<sale_id>];
Step5: Delete the sale record, replace <sale_id> with the results from the previous queries
DELETE FROM sales WHERE sale_id = 9 [<sale_id>];
(d) Changing prices: modify the price of the goods.</p>
<p>Record: On 2023-01-05, the sale price of pear in the store was changed to 1.6 dollar per unit.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Update the selling price of pear
UPDATE fruits
SET selling_price $=1.6$
WHERE fruit_name = "pear";</p>
<p>Figure 3: Examples showcasing ChatDB's response to the four common operations in the records of the Fruit Shop Dataset. " 9 [<sale_id>]" represents that <sale_id> is replaced with 9 in the updated SQL statement. Due to page size limitations, some unnecessary details and ChatDB's summary are omitted.</p>
<p>Suppose you are a fruit shop manager and good at analyzing history records.
The fruit shop newly opened on January 1, 2023. Given the history records for the fruit shop in January 2023, which include customer names, transaction dates, fruit prices, quantities purchased, and whether the items were returned, you need to answer some questions.
By default, exclude the sales transactions that have been returned when performing calculations.
Here are the historical records of the fruit shop, which are arranged in chronological order based on the occurrence time, surrounded by triple backticks:
$\cdots$
${$ records $}$
$\cdots$
Based on the history records, answer the question about the fruit shop:
${$ question $}$
Figure 4: Prompt for ChatGPT to answer questions in the Fruit Shop Dataset. The placeholders "records" and "question" will be replaced by specific details when it is actually used.</p>
<p>Why do we limit the token length of the dataset? If the token length of the dataset exceeds the maximum token length of ChatGPT, memory becomes necessary. However, the mainstream memory retrieval methods based on vector embedding are prone to errors. This inevitably leads to a decline in the performance of ChatGPT, which is not desired. Therefore, we deliberately design the token length of the dataset to be within the maximum token length of ChatGPT to avoid using memory and maximize the model's performance. Note that ChatDB's performance is generally unaffected by the token length of the dataset. Thus, if ChatDB outperforms ChatGPT when the dataset is small, it indicates that ChatDB is also superior to memory-augmented ChatGPT when the dataset is large.</p>
<h1>4.1.3 Processing records</h1>
<p>For ChatDB, the first step is to initialize the database. We need to generate a reasonable database schema for the specific task scenario and create tables in the database. The generation of the database schema can be done manually or using LLMs. Next, for each record in the dataset, ChatDB processes them one by one. Using the LLM controller, ChatDB manipulates the external database (i.e., symbolic memory) following Algorithm 1. We provide examples of ChatDB's response to the four common operations in the Fruit Shop Dataset, namely purchasing, selling, changing prices, and goods returns, as shown in Figure 3. It is worth emphasizing that ChatDB processes record one by one, so it is not sensitive to the total number of records. Furthermore, each step of the database operation in ChatDB is symbolic and free from errors. Therefore, in theory, ChatDB can handle an infinite number of historical records without sacrificing performance. However, for ChatGPT or existing memory-augmented LLMs, excessively long historical records can significantly degrade performance. In this experiment, as for the ChatGPT baseline, since the records are not long, we simply treat them as part of the prompt.</p>
<h3>4.1.4 Answering questions</h3>
<p>When answering questions, ChatDB no longer requires records to be part of the prompt. After processing the records, the information is stored in symbolic memory. Following Algorithm 1, ChatDB utilizes SQL statements to perform a series of database queries (including calculations) in order to answer the question. On the other hand, ChatGPT includes records as part of the prompt and directly asks the question. The prompt template is shown in Figure 4.</p>
<h3>4.2 Results</h3>
<p>The experimental results are presented in Table 2, which clearly demonstrate that ChatDB outperforms ChatGPT with significantly higher accuracy. While ChatGPT is capable of answering easy questions, it falls short in handling hard questions that necessitate multi-hop reasoning and precise calculations. Consequently, ChatGPT exhibits a low accuracy rate for these hard questions. In contrast, ChatDB exhibits a notably high accuracy rate, underscoring the advantages of utilizing a database as symbolic memory. This approach not only prevents error accumulation but also enhances the multi-hop reasoning and precise calculation capabilities of LLMs.
We present several examples of the two models answering questions in Figure 5 for comparison. In all these examples, ChatDB correctly answers the questions while ChatGPT fails. ChatGPT often exhibits errors in calculating the total price of each sale transaction, as observed in Figure 5(a). Sometimes, the formulas are correct but the calculations are wrong, while other times, even the formulas are incorrect. In addition, ChatGPT struggles to find all valid sale</p>
<p>(a) Question: What was the total revenue for January 2023?</p>
<h2>ChatGPT:</h2>
<p>To calculate the total revenue for January 2023, we need to sum up the revenue generated from each sales transaction. We can use the following formula to calculate the revenue for each transaction: revenue $=$ quantity $*$ selling price We also need to exclude the sales transactions that have been returned. Based on the history records, the following sales transactions have been returned:</p>
<ul>
<li>Alice Smith on 2023-01-17 - Bob Smith on 2023-01-05 [...]</li>
</ul>
<p>Therefore, we need to exclude these transactions when calculating the total revenue. Here is the calculation for each sales transaction:</p>
<ul>
<li>Bob Smith on 2023-01-02: $(9 \mathrm{~kg} * 3.8 \mathrm{USD} / \mathrm{kg})+(4 \mathrm{~kg} * 1.3 \mathrm{USD} / \mathrm{kg})=39.1$ USD [WRONG]: calculation error, should be 39.4</li>
<li>Sue Davis on 2023-01-02: $1 \mathrm{~kg} * 3.8$ USD/kg $=3.8$ USD</li>
<li>Bob Smith on 2023-01-05: $(5 \mathrm{~kg} * 1.8 \mathrm{USD} / \mathrm{kg})+(2 \mathrm{~kg} * 3.0 \mathrm{USD} / \mathrm{kg})+(6 \mathrm{~kg}$ * 2.8 USD/kg) $=47.6$ USD [WRONG]: calculation error and this transaction has been returned.
[...] [WRONG]: most of the sale transactions are calculated incorrectly.
[STOP]: Because it exceeds the maximum token length.
(b) Question: Which day in January 2023 had the highest revenue?</li>
</ul>
<h2>ChatGPT:</h2>
<p>To calculate the revenue for each day, we need to sum up the total sales for that day (excluding any returned items) and multiply by the selling price. We can then compare the revenue for each day to determine which day had the highest revenue.
Here is the breakdown of sales for each day:</p>
<ul>
<li>January 2: 9 kg apple at $3.8=34.2,4 \mathrm{~kg}$ cherry at $1.3=5.2,3 \mathrm{~kg}$ cherry at $1.3=$ $3.9,5 \mathrm{~kg}$ orange at $2.1=6.3$, total sales $=49.6$ [WRONG]: should be 43.2</li>
<li>January 4: 2 kg apple at $1.5=3,4 \mathrm{~kg}$ pear at $1.6=6.4,7 \mathrm{~kg}$ cherry at $1.3=9.1$, total sales $=18.5$ [WRONG]: should be 27.9</li>
<li>January 5: 5 kg cherry at $1.9=9.5,10 \mathrm{~kg}$ orange at $1.8=18,1 \mathrm{~kg}$ cherry at 1.9 $=1.9,1 \mathrm{~kg}$ grape at $3.2=3.2$, total sales $=32.6$ [WRONG]: should be 83</li>
<li>January 6: 3 kg apple at $1.5=4.5,10 \mathrm{~kg}$ grape at $3.2=32$, total sales $=36.5$ [WRONG]: should be 29.4</li>
<li>January 7: 4 kg pear at $1.6=6.4$, total sales $=6.4$ [WRONG]: should be 37.2</li>
<li>January 8: 6 kg peach at $3=18$, total sales $=18$ [WRONG]: should be 21.6
[...] [WRONG]: most of the daily revenues are calculated incorrectly.
[STOP]: Because it exceeds the maximum token length.</li>
</ul>
<h2>ChatDB:</h2>
<p>Begin to interact with the external database (symbolic memory).
Step1: Calculate the total revenue for January 2023
SELECT SUM (total_price) AS total_revenue
FROM sales
WHERE sale_date &gt;= <em>2023-01-01</em> AND sale_date &lt; <em>2023-02-01</em>;
Database response:
{
\total_revenue
$\mid$
707.8
}</p>
<h2>ChatDB:</h2>
<p>Begin to interact with the external database (symbolic memory).
Step1: Calculate the total revenue for each day in January 2023
SELECT sale_date, SUM (total_price) AS revenue
FROM sales
WHERE sale_date BETWEEN <em>2023-01-01</em> AND <em>2023-01-31</em> GROUP BY sale_data;</p>
<p>Database response:
{
\sale_date | revenue
}
2023-01-02 43.2
2023-01-04 27.9
Step2: Find the day with the highest revenue
SELECT sale_date, SUM (total_price) AS revenue
FROM sales
WHERE sale_date BETWEEN <em>2023-01-01</em> AND <em>2023-01-31</em> GROUP BY sale_date ORDER BY revenue DESC LIMIT 1;</p>
<p>Database response:
}
\sale_date | revenue
}
2023-01-30 119.7
}
(c) Question: What was the average weight of fruit per sales transaction in January 2023?</p>
<p>ChatGPT: To calculate the average weight of fruit per sales transaction, we need to first identify all the sales transactions in January 2023 and calculate the total weight of fruit sold in those transactions. Then, we can divide the total weight by the number of sales transactions to get the average weight per transaction. Here are the sales transactions in January 2023:</p>
<ul>
<li>Bob Smith on 2023-01-02: 9 kg apple, 4 kg cherry</li>
<li>Sue Davis on 2023-01-02: 1 kg apple</li>
<li>Bob Smith on 2023-01-05: 5 kg orange, 2 kg peach, 6 kg pear [WRONG]: this transaction has been returned.</li>
<li>Sue Davis on 2023-01-05: 5 kg pear, 10 kg orange, 1 kg cherry</li>
<li>Alice Smith on 2023-01-05: 5 kg cherry
[...] [WRONG]: not find out all valid sale transactions.
To calculate the total weight of fruit sold, we can add up the quantities of each fruit sold in the above transactions:</li>
<li>Apples: 16 kg - Cherries: 12 kg - Oranges: 10 kg - Pears: 18 kg - Kiwis: 12 kg Grapes: 18 kg - Bananas: 27 kg - Strawberries: 7 kg - Peaches: 9 kg - Plums: 8 kg
The total weight of fruit sold in January 2023 is 127 kg . [WRONG]: It should be 278. Even according to the calculation above, the total should be 137, not 127. To calculate the average weight per sales transaction, we need to divide the total weight by the number of sales transactions. There are 18 sales transactions in January 2023, [WRONG]: It should be 28 valid sale transactions in total. so the average weight [STOP]: Because it exceeds the maximum token length.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">ChatDB:</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Begin to interact with the external database (symbolic memory).</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Step1: Calculate the total weight of fruit sold in January 2023</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SELECT SUM(si.quantity_sold) as</td>
<td style="text-align: left;">Database response:</td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$ total_weight</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FROM sale_items si</td>
<td style="text-align: left;">total_weight</td>
</tr>
<tr>
<td style="text-align: left;">JOIN sales $x$ ON si.sale_id = s.sale_id</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WHERE s.sale_date BETWEEN <em>2023-01-01</em></td>
<td style="text-align: left;">278</td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$ AND <em>2023-01-31</em></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Step2: Count the number of sales transactions in January 2023
SELECT COUNT (DISTINCT s.sale_id) as
$\rightarrow$ num_sales
FROM sales $x$
WHERE s.sale_date BETWEEN <em>2023-01-01</em> $\rightarrow$ AND <em>2023-01-31</em></p>
<div class="codehilite"><pre><span></span><code>\s avg_weight_per_sale
</code></pre></div>

<p>Step3: Calculate the average weight of fruit per sales transaction in January 2023, replace <total_weight> and <num_sales> with the results from the previous queries</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SELECT 278</td>
<td style="text-align: left;"><total_weight></td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$</td>
<td style="text-align: left;">$&lt;$ <num_sales></td>
</tr>
<tr>
<td style="text-align: left;">as avg_weight_per_sale</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">9.93</td>
</tr>
</tbody>
</table>
<p>Figure 5: Examples of ChatDB and ChatGPT answering questions. While ChatGPT fails to answer any of the questions correctly, ChatDB successfully answers all of them. The issues with ChatGPT are highlighted in red. Some irrelevant details and ChatDB's summary are omitted for brevity. The gray ellipsis [...] indicates that the response is trimmed.</p>
<p>Table 2: The experimental results of answering questions in the Fruit Shop Dataset. There are 50 questions in total, with 15 being easy questions and 35 being hard questions.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Easy</th>
<th>Hard</th>
<th>All</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>$10 / 15$</td>
<td>$1 / 35$</td>
<td>$11 / 50$</td>
<td>$22 \%$</td>
</tr>
<tr>
<td>ChatDB (ours)</td>
<td>$13 / 15$</td>
<td>$28 / 35$</td>
<td>$41 / 50$</td>
<td>$82 \%$</td>
</tr>
</tbody>
</table>
<p>transactions, leading to errors in its answering process. This issue is commonly encountered and evident in all these examples. Furthermore, ChatGPT tends to make sequential errors, resulting in significant error accumulation.</p>
<p>In contrast, ChatDB performs quite well in these examples. During the initial processing of records, symbolic operations (i.e., SQL operations) are applied to manipulate the database (i.e., symbolic memory), ensuring that all information is stored in a structured form within the database. When answering questions, ChatDB generates SQL statements to query the database. The three examples demonstrate the effectiveness of ChatDB in solving problems requiring one, two, and three chain-of-memory steps, respectively. We can observe that ChatDB accurately answers the questions, and the execution logic of the chain-of-memory is clear, with each step tightly connected and approaching the ultimate answer. From these examples, the advantages of ChatDB are evident in two aspects:</p>
<ol>
<li>Through the chain-of-memory approach, complex problems are decomposed into multiple steps of memory operations, simplifying the problem's complexity. Each step's result is accurately stored as an intermediate outcome and used in subsequent steps, which greatly assists in complex reasoning.</li>
<li>Symbolic memory enables precise operations and calculations. ChatDB delegates many calculational tasks to the external database by executing SQL statements, ensuring the accuracy of each step and preventing error accumulation.</li>
</ol>
<p>In summary, by leveraging external databases as symbolic memory, ChatDB significantly outperforms ChatGPT in this experiment.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we introduce ChatDB, a framework that augments LLMs with symbolic memory in the form of databases. We demonstrate the advantages and capabilities of symbolic memory and chain-of-memory approach in enhancing complex reasoning and preventing error accumulation. By providing a precise storage mechanism for intermediate results, symbolic memory enables accurate and reliable operations. Moreover, the use of symbolic languages, such as SQL, allows symbolic computation and manipulation of stored information. Through experimental evaluation, we observe a significant improvement in performance with ChatDB compared to ChatGPT. The integration of symbolic memory in ChatDB substantially enhances the model's ability to handle various queries and reasoning tasks in management settings. This improvement highlights the benefits and effectiveness of leveraging symbolic memory in LLMs.</p>
<h2>References</h2>
<p>Andor, D., He, L., Lee, K., and Pitler, E. (2019). Giving bert a calculator: Finding operations and arguments with reading comprehension. arXiv preprint arXiv:1909.00109.</p>
<p>Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. (2023). Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Bulatov, A., Kuratov, Y., and Burtsev, M. (2022). Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079-11091.</p>
<p>Bulatov, A., Kuratov, Y., and Burtsev, M. S. (2023). Scaling transformer to 1m tokens and beyond with rmt. arXiv preprint arXiv:2304.11062.</p>
<p>Chen, A., Phang, J., Parrish, A., Padmakumar, V., Zhao, C., Bowman, S. R., and Cho, K. (2023). Two failures of self-consistency in the multi-step reasoning of llms. arXiv preprint arXiv:2305.14279.</p>
<p>Chen, W., Ma, X., Wang, X., and Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L., et al. (2022). Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. (2022). Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. (2020). Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR.</p>
<p>Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. (2023). Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992.</p>
<p>Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. (2022). Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Johnson, D. D. (2017). Learning graphical state transitions. In International conference on learning representations.
Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022). Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024.</p>
<p>Lester, B., Al-Rfou, R., and Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kttler, H., Lewis, M., Yih, W.-t., Rocktschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>OpenAI (2023). Gpt-4 technical report.
Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M. T. (2023). Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.</p>
<p>Park, J. S., OBrien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Pikos, P., Michalewski, H., and Malinowski, M. (2021). Measuring and improving bert's mathematical abilities by predicting the order of reasoning. arXiv preprint arXiv:2106.03921.</p>
<p>Ran, Q., Lin, Y., Li, P., Zhou, J., and Liu, Z. (2019). Numnet: Machine reading comprehension with numerical reasoning. arXiv preprint arXiv:1910.06701.</p>
<p>Schick, T., Dwivedi-Yu, J., Dess, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Schuurmans, D. (2023). Memory augmented large language models are computationally universal. arXiv preprint arXiv:2301.04589.</p>
<p>Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023). Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Sun, R., Arik, S. O., Nakhost, H., Dai, H., Sinha, R., Yin, P., and Pfister, T. (2023). Sql-palm: Improved large language modeladaptation for text-to-sql.</p>
<p>Surs, D., Menon, S., and Vondrick, C. (2023). Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozire, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al. (2023). Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. (2022a). Memorizing transformers. arXiv preprint arXiv:2203.08913.</p>
<p>Wu, Y., Zhao, Y., Hu, B., Minervini, P., Stenetorp, P., and Riedel, S. (2022b). An efficient memory-augmented transformer for knowledge-intensive nlp tasks. arXiv preprint arXiv:2210.16773.</p>
<p>Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. (2022). Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Zhong, Z., Lei, T., and Chen, D. (2022). Training language models with memory augmentation. arXiv preprint arXiv:2205.12674.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/Significant-Gravitas/Auto-GPT
${ }^{4}$ https://chatexcel.com/
${ }^{5}$ https://github.com/Significant-Gravitas/Auto-GPT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
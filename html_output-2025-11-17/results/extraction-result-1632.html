<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1632 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1632</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1632</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-3707478</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1710.06537v1.pdf" target="_blank">Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</a></p>
                <p><strong>Paper Abstract:</strong> Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this"reality gap."By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1632.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1632.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM + Dynamics Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent (LSTM) policy trained with Dynamics Randomization, HER, and Recurrent Deterministic Policy Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network control policy (LSTM) trained entirely in MuJoCo with randomized dynamics and Hindsight Experience Replay (HER) using RDPG; deployed zero-shot on a real 7-DOF Fetch arm to perform non-prehensile puck-pushing tasks with high success despite simulator–real-world mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Fetch Robotics 7-DOF arm (puck-pushing setup)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-degree-of-freedom robotic manipulator (Fetch arm) used to push a planar puck on a table; puck pose tracked by a PhaseSpace motion-capture system; real puck mass ≈ 0.2 kg and radius ≈ 0.065 m.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic manipulation (non-prehensile pushing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>MuJoCo physics engine simulating rigid-body dynamics, joint actuators/controllers, contact interactions between the arm, puck, and table, and providing state observations (joint positions/velocities, puck pose/velocities).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate physics simulation (MuJoCo) with intentionally randomized dynamics parameters — not claimed to be high-fidelity; low-fidelity simulation shown sufficient when combined with randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, joint dynamics, contact interactions (puck-table, gripper-puck), masses, joint damping, friction coefficients (puck), actuator controller gains, control timestep/latency model, observation (sensor) noise.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No photorealistic rendering or vision in experiments (state-based inputs used); contact and friction models are approximate and subject to calibration error; many real-world sources of variability (spatially varying surface friction, unmodeled micro-contacts, exact actuator dynamics) are not explicitly modeled beyond randomized parameter ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical Fetch 7-DOF arm pushing a real puck on a table; puck tracked by PhaseSpace mocap; episodes run with 200 timesteps; minimal calibration between simulator and robot was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Non-prehensile object pushing: move a puck from random initial positions to a target location on the table within a tolerance (goal satisfaction).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (off-policy) using Recurrent Deterministic Policy Gradient (RDPG) with Deep Deterministic Policy Gradient style actor-critic, Hindsight Experience Replay (HER), and ADAM optimizer; recurrent policy (LSTM) and omniscient recurrent critic (provided dynamics µ during training).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate = fraction of episodes where puck is within target distance at episode end (binary goal reward).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>0.91 ± 0.03 (success rate in simulation for LSTM, evaluated over 100 randomized-dynamics episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>0.89 ± 0.06 (success rate on real Fetch arm for LSTM, reported over 28 trials)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>95 randomized parameters sampled per episode, including: mass of each robot link; damping of each joint; puck mass, friction, and damping; table height; position-controller gains; timestep between actions (models latency) sampled per-step from t0 + Exp(λ) with λ fixed per episode; observation noise (Gaussian per-feature, mean 0, std 5% of running stddev); some parameters sampled logarithmically, others uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Modeling / calibration error between simulator and real robot (different joint trajectories for same commands), controller latency differences, sensor noise, inaccuracies in contact/friction modeling, and unmodeled real-world variability in contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training across a wide distribution of randomized dynamics (dynamics randomization); recurrent policy architecture (LSTM) that can implicitly perform online system identification from histories; use of HER to handle sparse binary rewards; omniscient critic during training to stabilize learning; explicitly randomizing action timestep (latency) and observation noise which were found critical in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High-fidelity simulation is not required; instead, covering relevant variability matters. In particular, modeling (and randomizing) controller latency (action timestep) and sensor noise was identified as especially important; accurate modeling of contact dynamics (friction) also matters — removing randomization of friction or masses reduced real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamics randomization combined with a memory-based policy (LSTM) enables zero-shot sim-to-real transfer for a challenging contact-rich pushing task; LSTM policies trained with randomized dynamics match simulation performance when deployed on the real robot (≈0.9 success rate) without real-world fine-tuning; randomizing action timestep (latency) and observation noise are particularly important for transfer; feedforward (memoryless) policies perform worse even with randomization, demonstrating the value of recurrent policies for implicit system identification and adaptation to unseen real dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Robotic Control with Dynamics Randomization', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Transfer from simulation to real world through learning deep inverse dynamics model <em>(Rating: 2)</em></li>
                <li>Preparing for the unknown: Learning a universal policy with online system identification <em>(Rating: 2)</em></li>
                <li>Reinforcement learning for pivoting task <em>(Rating: 2)</em></li>
                <li>Cad2rl: Real single-image flight without a single real image <em>(Rating: 1)</em></li>
                <li>Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1632",
    "paper_id": "paper-3707478",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "LSTM + Dynamics Randomization",
            "name_full": "Recurrent (LSTM) policy trained with Dynamics Randomization, HER, and Recurrent Deterministic Policy Gradient",
            "brief_description": "A recurrent neural network control policy (LSTM) trained entirely in MuJoCo with randomized dynamics and Hindsight Experience Replay (HER) using RDPG; deployed zero-shot on a real 7-DOF Fetch arm to perform non-prehensile puck-pushing tasks with high success despite simulator–real-world mismatch.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Fetch Robotics 7-DOF arm (puck-pushing setup)",
            "agent_system_description": "A 7-degree-of-freedom robotic manipulator (Fetch arm) used to push a planar puck on a table; puck pose tracked by a PhaseSpace motion-capture system; real puck mass ≈ 0.2 kg and radius ≈ 0.065 m.",
            "domain": "robotic manipulation (non-prehensile pushing)",
            "virtual_environment_name": "MuJoCo",
            "virtual_environment_description": "MuJoCo physics engine simulating rigid-body dynamics, joint actuators/controllers, contact interactions between the arm, puck, and table, and providing state observations (joint positions/velocities, puck pose/velocities).",
            "simulation_fidelity_level": "Approximate physics simulation (MuJoCo) with intentionally randomized dynamics parameters — not claimed to be high-fidelity; low-fidelity simulation shown sufficient when combined with randomization.",
            "fidelity_aspects_modeled": "Rigid-body dynamics, joint dynamics, contact interactions (puck-table, gripper-puck), masses, joint damping, friction coefficients (puck), actuator controller gains, control timestep/latency model, observation (sensor) noise.",
            "fidelity_aspects_simplified": "No photorealistic rendering or vision in experiments (state-based inputs used); contact and friction models are approximate and subject to calibration error; many real-world sources of variability (spatially varying surface friction, unmodeled micro-contacts, exact actuator dynamics) are not explicitly modeled beyond randomized parameter ranges.",
            "real_environment_description": "Physical Fetch 7-DOF arm pushing a real puck on a table; puck tracked by PhaseSpace mocap; episodes run with 200 timesteps; minimal calibration between simulator and robot was performed.",
            "task_or_skill_transferred": "Non-prehensile object pushing: move a puck from random initial positions to a target location on the table within a tolerance (goal satisfaction).",
            "training_method": "Reinforcement learning (off-policy) using Recurrent Deterministic Policy Gradient (RDPG) with Deep Deterministic Policy Gradient style actor-critic, Hindsight Experience Replay (HER), and ADAM optimizer; recurrent policy (LSTM) and omniscient recurrent critic (provided dynamics µ during training).",
            "transfer_success_metric": "Success rate = fraction of episodes where puck is within target distance at episode end (binary goal reward).",
            "transfer_performance_sim": "0.91 ± 0.03 (success rate in simulation for LSTM, evaluated over 100 randomized-dynamics episodes)",
            "transfer_performance_real": "0.89 ± 0.06 (success rate on real Fetch arm for LSTM, reported over 28 trials)",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "95 randomized parameters sampled per episode, including: mass of each robot link; damping of each joint; puck mass, friction, and damping; table height; position-controller gains; timestep between actions (models latency) sampled per-step from t0 + Exp(λ) with λ fixed per episode; observation noise (Gaussian per-feature, mean 0, std 5% of running stddev); some parameters sampled logarithmically, others uniformly.",
            "sim_to_real_gap_factors": "Modeling / calibration error between simulator and real robot (different joint trajectories for same commands), controller latency differences, sensor noise, inaccuracies in contact/friction modeling, and unmodeled real-world variability in contact dynamics.",
            "transfer_enabling_conditions": "Training across a wide distribution of randomized dynamics (dynamics randomization); recurrent policy architecture (LSTM) that can implicitly perform online system identification from histories; use of HER to handle sparse binary rewards; omniscient critic during training to stabilize learning; explicitly randomizing action timestep (latency) and observation noise which were found critical in ablations.",
            "fidelity_requirements_identified": "High-fidelity simulation is not required; instead, covering relevant variability matters. In particular, modeling (and randomizing) controller latency (action timestep) and sensor noise was identified as especially important; accurate modeling of contact dynamics (friction) also matters — removing randomization of friction or masses reduced real-world performance.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Dynamics randomization combined with a memory-based policy (LSTM) enables zero-shot sim-to-real transfer for a challenging contact-rich pushing task; LSTM policies trained with randomized dynamics match simulation performance when deployed on the real robot (≈0.9 success rate) without real-world fine-tuning; randomizing action timestep (latency) and observation noise are particularly important for transfer; feedforward (memoryless) policies perform worse even with randomization, demonstrating the value of recurrent policies for implicit system identification and adaptation to unseen real dynamics.",
            "uuid": "e1632.0",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization",
                "publication_date_yy_mm": "2017-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "rating": 2,
            "sanitized_title": "transfer_from_simulation_to_real_world_through_learning_deep_inverse_dynamics_model"
        },
        {
            "paper_title": "Preparing for the unknown: Learning a universal policy with online system identification",
            "rating": 2,
            "sanitized_title": "preparing_for_the_unknown_learning_a_universal_policy_with_online_system_identification"
        },
        {
            "paper_title": "Reinforcement learning for pivoting task",
            "rating": 2,
            "sanitized_title": "reinforcement_learning_for_pivoting_task"
        },
        {
            "paper_title": "Cad2rl: Real single-image flight without a single real image",
            "rating": 1,
            "sanitized_title": "cad2rl_real_singleimage_flight_without_a_single_real_image"
        },
        {
            "paper_title": "Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids",
            "rating": 1,
            "sanitized_title": "ensemblecio_fullbody_dynamic_motion_planning_that_transfers_to_physical_humanoids"
        }
    ],
    "cost": 0.009299499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</p>
<p>Xue Bin Peng 
Marcin Andrychowicz 
Wojciech Zaremba 
Pieter Abbeel 
Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</p>
<p>Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap." By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.</p>
<p>I. INTRODUCTION</p>
<p>Deep reinforcement learning (DeepRL) has been shown to be an effective framework for solving a rich repertoire of complex control problems. In simulated domains, agents have been developed to perform a diverse array of challenging tasks [1], [2], [3]. Unfortunately, many of the capabilities demonstrated by simulated agents have often not been realized by their physical counterparts. Many of the modern DeepRL algorithms, which have spurred recent breakthroughs, pose high sample complexities, therefore often precluding their direct application to physical systems. In addition to sample complexity, deploying RL algorithms in the real world also raises a number of safety concerns both for the agent and its surroundings. Since exploration is a key component of the learning process, an agent can at times perform actions that endanger itself or its environment. Training agents in simulation is a promising approach that circumvents some of these obstacles. However, transferring policies from simulation to the real world entails challenges in bridging the "reality gap", the mismatch between the simulated and real world environments. Narrowing this gap has been a subject of intense interest in robotics, as it offers the potential of applying powerful algorithms that have so far been relegated to simulated domains. While significant efforts have been devoted to building higher fidelity simulators, we show that dynamics randomization using low fidelity simulations can also be an effective 1 OpenAI 2 UC Berkeley, Department of Electrical Engineering and Computer Science approach to develop policies that can be transferred directly to the real world. The effectiveness of our approach is demonstrated on an object pushing task, where a policy trained exclusively in simulation is able to successfully perform the task with a real robot without additional training on the physical system.</p>
<p>II. RELATED WORK</p>
<p>Recent years have seen the application of deep reinforcement learning to a growing repertoire of control problems. The framework has enabled simulated agents to develop highly dynamic motor skills [4], [5], [6], [7]. But due to the high sample complexity of RL algorithms and other physical limitations, many of the capabilities demonstrated in simulation have yet to be replicated in the physical world. Guided Policy Search (GPS) [8] represents one of the few algorithms capable of training policies directly on a real robot. By leveraging trajectory optimization with learned linear dynamics models, the method is able to develop complex manipulation skills with relatively few interactions with the environment. The method has also been extended to learning vision-based manipulation policies [9]. Researchers have also explored parallelizing training across multiple robots [10]. Nonetheless, successful examples of training policies directly on physical robots have so far been demonstrated only on relatively restrictive domains.</p>
<p>A. Domain Adaptation</p>
<p>The problem of transferring control policies from simulation to the real world can be viewed as an instance of domain adaptation, where a model trained in a source domain is transfered to a new target domain. One of the key assumptions behind these methods is that the different domains share common characteristics such that representations and behaviours learned in one will prove useful for the other. Learning invariant features has emerged as a promising approach of taking advantage of these commonalities [11], [12]. Tzeng et al. [11] and Gupta et al. [13] explored using pairwise constraints to encourage networks to learn similar embeddings for samples from different domains that are labeled as being similar. Daftry et al. [14] applied a similar approach to transfer policies for controlling aerial vehicles to different environments and vehicle models. In the context of RL, adversarial losses have been used to transfer policies between different simulated domains, by encouraging agents to adopt similar behaviours across the various environments [15]. Alternatively, progressive networks have also been used to transfer policies for a robotic arm from simulation to the real world [16]. By reusing features learned in simulation, their method was able to significantly reduce the amount of data needed from the physical system. Christiano et al. [17] transfered policies from simulation to a real robot by training an inverse-dynamics model from real world data. While promising, these methods nonetheless still require data from the target domain during training.</p>
<p>B. Domain Randomization</p>
<p>Domain randomization is a complementary class of techniques for adaptation that is particularly well suited for simulation. With domain randomization, discrepancies between the source and target domains are modeled as variability in the source domain. Randomization in the visual domain has been used to directly transfer vision-based policies from simulation to the real world without requiring real images during training [18], [19]. Sadeghi and Levine [18] trained vision-based controllers for a quadrotor using only synthetically rendered scenes, and Tobin et al. [19] demonstrated transferring image-based object detectors. Unlike previous methods, which sought to bridge the reality gap with high fidelity rendering [20], their systems used only low fidelity rendering and modeled differences in visual appearance by randomizing scene properties such as lighting, textures, and camera placement. In addition to randomizing the visual features of a simulation, randomized dynamics have also been used to develop controllers that are robust to uncertainty in the dynamics of the system. Mordatch et al. [21] used a trajectory optimizer to plan across an ensemble of dynamics models, to produce robust trajectories that are then executed on a real robot. Their method allowed a Darwin robot to perform a variety of locomotion skills. But due to the cost of the trajectory optimization step, the planning is performed offline. Other methods have also been proposed to develop robust policies through adversarial training schemes [22], [23]. Yu et al. [24] trained a system identification module to explicitly predict parameters of interest, such as mass and friction. The predicted parameters are then provided as input to a policy to compute the appropriate controls. While the results are encouraging, these methods have so far only been demonstrated on transfer between different simulators.</p>
<p>The work most reminiscent to our proposed method is that of Antonova et al. [25], where randomized dynamics was used to transfer manipulation policies from simulation to the real world. By randomizing physical parameters such as friction and latency, they were able to train policies in simulation for pivoting objects held by a gripper, and later transfer the policies directly to a Baxter robot without requir-ing additional fine-tuning on the physical system. However their policies were modeled using memoryless feedforward networks, and while the policies developed robust strategies, the lack of internal state limits the feedforward policies' ability to adapt to mismatch between the simulated and real environment. We show that memory-based policies are able to cope with greater variability during training and also better generalize to the dynamics of the real world. Unlike previous methods which often require meticulous calibration of the simulation to closely conform to the physical system, our policies are able to adapt to significant calibration error.</p>
<p>C. Non-prehensile Manipulation</p>
<p>Pushing, a form of non-prehensile manipulation, is an effective strategy for positioning and orienting objects that are too large or heavy to be grasped [26]. Though pushing has attracted much interest from the robotics community [27], [28], [29], it remains a challenging skill for robots to adopt. Part of the difficulty stems from accurately modeling the complex contact dynamics between surfaces. Characteristics such as friction can vary significantly across the surface of an object, and the resulting motions can be highly sensitive to the initial configuration of the contact surfaces [26]. Models have been proposed to facilitate planning algorithms [27], [30], [28], but they tend to rely on simplifying assumptions that are often violated in practice. More recently, deep learning methods have been applied to train predictive models for pushing [31]. While data-driven methods overcome some of the modeling challenges faced by previous frameworks, they require a large corpus of real world data during training. Such a dataset can be costly to collect, and may become prohibitive for more complex tasks. Clavera et al. demonstrated transferring pushing policies trained in simulation to a real PR2 [32]. Their approach took advantage of shaped reward functions and careful calibration to ensure that the behaviour of the simulation conforms to that of the physical system. In contrast, we will show that adaptive policies can be trained exclusively in simulation and using only sparse rewards. The resulting policies are able accommodate large calibration errors when deployed on a real robot and also generalize to variability in the dynamics of the physical system.</p>
<p>III. BACKGROUND</p>
<p>In this section we will provide a review of the RL framework and notation used in the following sections. We consider a standard RL problem where an agent interacts with an environment according to a policy in order to maximize a reward. The state of the environment at timestep t is denoted by s t ∈ S. For simplicity, we assume that the state is fully observable. A policy π(a|s) defines a distribution over the action space A given a particular state s, where each query to the policy samples an action a from the conditional distribution. The reward function r : S × A → R provides a scalar signal that reflects the desirability of performing an action at a given state. For convenience, we denote r t = r(s t , a t ). The goal of the agent is to maximize
the multi-step return R t = T t =t γ t −t r t , where γ ∈ [0, 1]
is a discount factor and T is the horizon of each episode.</p>
<p>The objective during learning is to find an optimal policy π * that maximize the expected return of the agent J(π) π * = arg max π J(π)</p>
<p>If each episode starts in a fixed initial state, expected return can be rewritten as the expected return starting at the first step
J(π) = E[R 0 |π] = E τ ∼p(τ |π) T −1 t=0 r(s t , a t )
where p(τ |π) represents the likelihood of a trajectory τ = (s 0 , a 0 , s 1 , ..., a T −1 , s T ) under the policy π,
p(τ |π) = p(s 0 ) T −1 t=0 p(s t+1 |s t , a t )π(s t , a t )
with the state transition model p(s t+1 |s t , a t ) being determined by the dynamics of the environment. The dynamics is therefore of crucial importance, as it determines the consequences of the agent's actions, as well as the behaviours that can be realized.</p>
<p>A. Policy Gradient Methods</p>
<p>For a parametric policy π θ with parameters θ, the objective is to find the optimal parameters θ * that maximizes the expected return θ * = arg max θ J(π θ ). Policy gradient methods [33] is a popular class of algorithms for learning parametric policies, where an estimate of the gradient of the objective θ J(π θ ) is used to perform gradient ascent to maximize the expected return. While the previous definition of a policy is suitable for tasks where the goal is common across all episodes, it can be generalized to tasks where an agent is presented with a different goal every episode by constructing a universal policy [34]. A universal policy is a simple extension where the goal g ∈ G is provided as an additional input to the policy π(a|s, g). The reward is then also dispensed according to the goal r(s t , a t , g). In our framework, a random goal will be sampled at the start of each episode, and held fixed over the course the episode. For the pushing task, the goal specifies the target location for an object.</p>
<p>B. Hindsight Experience Replay</p>
<p>During training, RL algorithms often benefit from carefully shaped reward functions that help guide the agent towards fulling the overall objective of a task. But designing a reward function can be challenging for more complex tasks, and may bias the policy towards adopting less optimal behaviours. An alternative is to use a binary reward r(s, g) that only indicates if a goal is satisfied in a given state,
r(s, g) = 0, if g is satisfied in s −1, otherwise
Learning from a sparse binary reward is known to be challenging for most modern RL algorithms. We will therefore leverage a recent innovation, Hindsight Experience Relay (HER) [35], to train policies using sparse rewards. Consider an episode with trajectory τ ∈ (s 0 , a 0 , ..., a T −1 , s T ), where the goal g was not satisfied over the course the trajectory. Since the goal was not satisfied, the reward will be −1 at every timestep, therefore providing the agent with little information on how to adjust its actions to procure more rewards. But suppose that we are provided with a mapping m : S → G, that maps a state to the corresponding goal satisfied in the given state. For example, m(s T ) = g represents the goal that is satisfied in the final state of the trajectory. Once a new goal has been determined, rewards can be recomputed for the original trajectory under the new goal g . While the trajectory was unsuccessful under the original goal, it becomes a successful trajectory under the new goal. Therefore, the rewards computed with respect to g will not be −1 for every timestep. By replaying past experiences with HER, the agent can be trained with more successful examples than is available in the original recorded trajectories. So far, we have only considered replaying goals from the final state of a trajectory. But HER is also amenable to other replay strategies, and we refer interested readers to the original paper [35] for more details.</p>
<p>IV. METHOD</p>
<p>Our objective is to train policies that can perform a task under the dynamics of the real world p * (s t+1 |s t , a t ). Since sampling from the real world dynamics can be prohibitive, we instead train a policy using an approximate dynamics modelp(s t+1 |s t , a t ) ≈ p * (s t+1 |s t , a t ) that is easier to sample from. For all of our experiments,p assumes the form of a physics simulation. Due to modeling and other forms of calibration error, behaviours that successfully accomplish a task in simulation may not be successful once deployed in the real world. Furthermore, it has been observed that DeepRL policies are prone to exploiting idiosyncrasies of the simulator to realize behaviours that are infeasible in the real world [2], [7]. Therefore, instead of training a policy under one particular dynamics model, we train a policy that can perform a task under a variety of different dynamics models. First we introduce a set of dynamics parameters µ that parameterizes the dynamics of the simulationp(s t+1 |s t , a t , µ). The objective is then modified to maximize the expected return across a distribution of dynamics models ρ µ ,
E µ∼ρµ E τ ∼p(τ |π,µ) T −1 t=0 r(s t , a t )
By training policies to adapt to variability in the dynamics of the environment, the resulting policy might then better generalize to the dynamics of real world.</p>
<p>A. Tasks</p>
<p>Our experiments are conducted on a puck pushing task using a 7-DOF Fetch Robotics arm. Images of the real robot  Figure 2. The goal g for each episode specifies a random target position on the table that the puck should be moved to. The reward is binary with r t = 0 if the puck is within a given distance of the target, and r t = −1 otherwise. At the start of each episode, the arm is initialized to a default pose and the initial location of the puck is randomly placed within a fixed bound on the table.</p>
<p>B. State and Action</p>
<p>The state is represented using the joint positions and velocities of the arm, the position of the gripper, as well as the puck's position, orientation, linear and angular velocities. The combined features result in a 52D state space. Actions from the policy specify target joint angles for a position controller. This yields a 7D action space.</p>
<p>C. Dynamics Randomization</p>
<p>During training, rollouts are organized into episodes of a fixed length. At the start of each episode, a random set of dynamics parameters µ are sampled according to ρ µ and held fixed for the duration of the episode. The parameters which we randomize include:</p>
<p>• Mass of each link in the robot's body • Damping of each joint • Mass, friction, and damping of the puck • Height of the table • Gains for the position controller • Timestep between actions • Observation noise which results in a total of 95 randomized parameters. The timestep between actions specifies the amount of time an action is applied before the policy is queried again to sample a new action. This serves as a simple model of the latency exhibited by the physical controller. The observation noise models uncertainty in the sensors and is implemented as independent Gaussian noise applied to each state feature. While parameters such as mass and damping are constant over the course of an episode, the action timestep and the observation noise varies randomly each timestep.</p>
<p>D. Adaptive Policy</p>
<p>Manipulation tasks, such as pushing, have a strong dependency on the physical properties of the system (e.g. mass, friction, and characteristics of the actuators). In order to determine the appropriate actions, a policy requires some means of inferring the underlying dynamics of its environment. While the dynamics parameters are readily available in simulation, the same does not hold once a policy has been deployed in the real world. In the absence of direct knowledge of the parameters, the dynamics can be inferred from a history of past states and actions. System identification using a history of past trajectories has been previously explored by Yu et al. [24]. Their system incorporates an online system identification module φ(s t , h t ) =μ, which utilizes a history of past states and actions h t = [a t−1 , s t−1 , a t−2 , s t−2 , ...] to predict the dynamics parameters µ. The predicted parameters are then used as inputs to a universal policy that samples an action according to the current state and inferred dynamics π(a t |s t ,μ). However, this decomposition requires identifying the dynamics parameters of interest to be predicted at runtime, which may be difficult for more complex systems. Constructing such a set of parameters necessarily requires some structural assumptions about the dynamics of a system, which may not hold in the real world. Alternatively, SysID can be implicitly embedded into a policy by using a recurrent model π(a t |s t , z t , g), where the internal memory z t = z(h t ) acts as a summary of past states and actions, thereby providing a mechanisms with which the policy can use to infer the dynamics of the system. This model can then be trained end-to-end and the representation of the internal memory can be learned without requiring manual identification of a set of dynamics parameters to be inferred at runtime.</p>
<p>E. Recurrent Deterministic Policy Gradient</p>
<p>Since HER augments the original training data recorded from rollouts of the policy with additional data generated from replayed goals, it requires off-policy learning. Deep Deterministic Policy Gradient (DDPG) [2] is a popular offpolicy algorithm for continuous control. Its extension to recurrent policies, Recurrent Deterministic Policy Gradient (RDPG) [36], provides a method to train recurrent policies with off-policy data. To apply RDPG, we denote a deterministic policy as π(s t , z t , g) = a t . In additional to the policy, we will also model a recurrent universal value function as Q(s t , a t , y t , g, µ), where y t = y(h t ) is the value function's internal memory. Since the value function is used only during training and the dynamics parameters µ of the simulator are known, µ is provided as an additional input to the value function but not to the policy. We will refer to a value function with knowledge of the dynamics parameters as an omniscient critic. This follows the approach of [37], [38], where additional information is provided to the value function during training in order to reduce the variance of the policy gradients and allow the value function to provide more meaningful feedback for improving the policy.</p>
<p>Algorithm 1 summarizes the training procedure, where M represents a replay buffer [2], and θ and ϕ are the parameters for the policy and value function respectively. We also incorporate target networks [2], but they are excluded for brevity. Algorithm 1 Dynamics Randomization with HER and RDPG 1: θ ← random weights 2: ϕ ← random weights 3: while not done do 4: g ∼ ρ g sample goal 5: µ ∼ ρ µ sample dynamics 6: Generate rollout τ = (s 0 , a 0 , ..., s T )with dynamics µ 7:</p>
<p>for each s t , a t in τ do 8: r t ← r(s t , g) 9:</p>
<p>end for 10: Store (τ, {r t }, g, µ) in M 11:</p>
<p>Sample episode (τ, {r t }, g, µ) from M</p>
<p>12:</p>
<p>with probability k 13: g ← replay new goal with HER 14: r t ← r(s t , g) for each t 15:</p>
<p>endwith 16: for each t do 17: Compute memories z t and y t 18:â t+1 ← π θ (s t+1 , z t+1 , g) 19:â t ← π θ (s t , z t , g) 20: q t ← r t + γQ ϕ (s t+1 ,â t+1 , y t+1 , g, µ) 21: q t ← q t − Q ϕ (s t , a t , y t , g, µ) 22: end for 23: Update value function and policy with θ and ϕ 26: end while
ϕ = 1 T t q t ∂Qϕ(st,</p>
<p>F. Network Architecture</p>
<p>A schematic illustrations of the policy and value networks are available in Figure 4. The inputs to the network consist of the current state s t and previous action a t−1 , and the internal memory is updated incrementally at every step. Each network consists of a feedforward branch and recurrent branch, with the latter being tasked with inferring the dynamics from past observations. The internal memory is modeled using a layer of LSTM units and is provided only with information required to infer the dynamics (e.g. s t and a t−1 ). The recurrent branch consists of an embedding layer of 128 fullyconnected units followed by 128 LSTM units. The goal g does not hold any information regarding the dynamics of the system, and is therefore processed only by the feedforward branch. Furthermore, since the current state s t is of particular importance for determining the appropriate action for the current timestep, a copy is also provided as input to the feedforward branch. This presents subsequent layers with more direct access to the current state, without requiring information to filter through the LSTM. The features computed by both branches are then concatenated and processed by 2 additional fully-connected layers of 128 units each. The value network Q(s t , a t , a t−1 , g, µ) follows a similar architecture, with the query action a t and parameters µ being processed by the feedforward branch. ReLU activations are used after each hidden layer (apart from the LSTM). The output layer of Q consists of linear units, while π consists of tanh output units scaled to span the bounds of each action parameter.</p>
<p>V. EXPERIMENTS</p>
<p>Results are best seen in the supplemental video youtu.be/HEhXX03KmE0. Snapshots of policies deployed on the real robot are available in Figure 3. All simulations are performed using the MuJoCo physics engine [39] with a simulation timestep of 0.002s. 20 simulation timesteps are performed for every control timestep. Each episode consists of 100 control timestep, corresponding to approximately 4 seconds per episode, but may vary as a result of the random timesteps between actions. Table I details the range of values for each dynamics parameter. At the start of each episode, a new set of parameters µ is sampled by drawing values for each parameter from their respective range. Parameters such as mass, damping, friction, and controller gains are logarithmically sampled, while other parameters are uniformly sampled. The timestep t between actions varies every step according to t ∼ t 0 + Exp(λ), where t 0 = 0.04s is the default control timestep, and Exp(λ) is an exponential distribution with rate parameter λ. While   t varies every timestep, λ is fixed within each episode. In addition to randomizing the physical properties of the simulated environment, we also simulate sensor noise by applying gaussian noise to the observed state features at every step. The noise has a mean of zero and a standard deviation of 5% of the running standard deviation of each feature. Gaussian action exploration noise is added at every step with a standard deviation of 0.01rad.</p>
<p>The real puck has a mass of approximately 0.2kg and a radius of 0.065m. The goal is considered satisfied if the puck is within 0.07m of the target. The location of the puck is tracked using the PhaseSpace mocap system. When evaluating performance on the physical system, each episode consists of 200 timesteps. Little calibration was performed to ensure that the behaviour of the simulation closely conforms to that of the real robot. While more extensive calibration will likely improve performance, we show that our policy is nonetheless able to adapt to the physical system despite poor calibration. To illustrate the discrepancies between the dynamics of the real world and simulation we executed the same target trajectory on the real and simulated robot, and recorded the resulting joint trajectories. Figure 5 illustrates the recorded trajectories. Given the same target trajectory, the pose trajectories of the simulated and real robot differ significantly, with varying degrees of mismatch across joints.</p>
<p>During training, parameter updates are performed using the ADAM optimizer [40] with a stepsize of 5 × 10 −4 for both the policy and value function. Updates are performed using batches of 128 episodes with 100 steps per episode. New goals are sampled using HER with a probability of k = 0.8. Each policy is trained for approximately 8000 update iterations, requiring about 8 hours on a 100 core cluster.</p>
<p>A. Comparison of Architectures</p>
<p>To evaluate the impact of different architectural choices, we compared policies modeled using different architectures and tested their performance in simulation and on the real robot. The first is an LSTM policy following the architecture illustrated in Figure 4. Next we consider a memoryless feedforward network (FF) that receives only the current state s t and goal g as input. As a baseline, we also trained a memoryless feedforward network without randomization (FF no Rand), then evaluated the performance with randomization. To provide the feedforward network with more information to infer the dynamics, we augmented the inputs with a history of the 8 previously observed states and actions (FF + Hist). The success rate is determined as the portion of episodes where the goal is fulfilled at the end of the episode. In simulation, performance of each policy is evaluated over 100 episodes, with randomized dynamics parameters for each episode. Learning curves comparing the performance of different model architectures in simulation are available in Figure 6. Four policies initialized with different random seeds are trained for each architecture. The LSTM learns faster while also converging to a higher success rate than  the feedforward models. The feedforward network trained without randomization is unable to cope with unfamiliar dynamics during evaluation. While training a memoryless policy with randomization improves robustness to random dynamics, it is still unable to perform the task consistently.</p>
<p>Next, we evaluate the performance of the different models when deployed on the real Fetch arm. Figure 7 compares the performance of the final policies when deployed in simulation and the real world. Table II summarizes the performance of the models. The target and initial location of the puck is randomly placed within a 0.3m × 0.3m bound. While the performance of LSTM and FF + Hist policies are comparable in simulation, the LSTM is able to better generalize to the dynamics of the physical system. The feedforward network trained without randomization is unable to perform the task under the real world dynamics.</p>
<p>B. Ablation</p>
<p>To evaluate the effects of randomizing the various dynamics parameters, we trained policies with subsets of the parameters held fixed. A complete list of the dynamics parameters are available in Table I. The configurations we consider include training with a fixed timestep between actions, training without observation noise, or with fixed mass for each link. Table III summarizes the performance of the resulting policies when deployed on the real robot. Disabling randomization of the action timestep, observation noise, link mass, and friction impairs the policies' ability to adapt to the physical environment. Policies trained without randomizing the action timestep and observation noise show particularly noticeable drops in performance. This suggests that coping with the latency of the controller and sensor noise are important factors in adapting to the physical system.</p>
<p>C. Robustness</p>
<p>To evaluate the robustness of the LSTM policy to different dynamics when deployed on the real robot, we experimented with changing the contact dynamics of the physical system by attaching a packet of chips to the bottom of the puck. The texture of the bag reduces the friction between the puck and the table, while the contents of the bag further alters the contact dynamics. Nonetheless, the LSTM policy achieves a success rate of 0.91 ± 0.04, which is comparable to the success rate without the attachment 0.89 ± 0.06. The policy also develops clever strategies to make fine adjustments to position the puck over the target. One such strategy involves pressing on one side of the puck in order to partially upend it before sliding it to the target. Other strategies including manipulating the puck from the top or sides depending on the required adjustments, and correcting for case where the puck overshoots the target. These behaviours emerged naturally from the learning process using only a sparse binary reward.</p>
<p>VI. CONCLUSIONS</p>
<p>We demonstrated the use of dynamics randomization to train recurrent policies that are capable of adapting to unfamiliar dynamics at runtime. Training policies with randomized dynamics in simulation enables the resulting policies to be deployed directly on a physical robot despite poor calibrations. By training exclusively in simulation, we are able to leverage simulators to generate a large volume of training data, thereby enabling us to use powerful RL techniques that are not yet feasible to apply directly on a physical system. Our experiments with a real world pushing tasks showed comparable performance to simulation and the ability to adapt to changes in contact dynamics. We also evaluated the importance of design decisions pertaining to choices of architecture and parameters which to randomize during training. We intend to extend this work to a richer repertoire tasks and incorporate more modalities such as vision. We hope this approach will open more opportunities for developing skillful agents in simulation that are then able to be deployed in the physical world.</p>
<p>VII. ACKNOWLEDGEMENT</p>
<p>Fig. 1 .
1A recurrent neural network policy trained for a pushing task in simulation is deployed directly on a Fetch Robotics arm. The red marker indicates the target location for the puck.</p>
<p>Fig. 2 .
2Our experiments are conducted on a 7-DOF Fetch Robotics arm. Left: Real robot. Right: Simulated MuJoCo model. and simulated model is available in</p>
<p>Fig. 3 .
3LSTM policy deployed on the Fetch arm. Bottom: The contact dynamics of the puck was modified by attaching a packet of chips to the bottom.</p>
<p>Fig. 4 .
4Schematic illustrations of the policy network (top), and value network (bottom). Features that are relevant for inferring the dynamics of the environment are processed by the recurrent branch, while the other inputs are processed by the feedforward branch.</p>
<p>Fig. 5 .
5Joint trajectories recorded from the simulated and real robot when executing the same target trajectories. The joints correspond to the shoulder, elbow, and wrist of the Fetch arm.</p>
<p>Fig. 6 .
6Learning curves of different network architectures. Four policies are trained for each architecture with different random seeds. Performance is evaluated over 100 episodes in simulation with random dynamics.</p>
<p>Fig. 7 .
7Performance of different models when deployed on the simulated and real robot for the pushing task. Policies are trained using only data from simulation.</p>
<p>TABLE I DYNAMICS
IPARAMETERS AND THEIR RESPECTIVE RANGES.</p>
<p>TABLE II PERFORMANCE OF THE POLICIES WHEN DEPLOYED ON THE SIMULATED AND REAL ROBOT. PERFORMANCE IN SIMULATION IS EVALUATED OVER 100 TRIALS WITH RANDOMIZED DYNAMICS PARAMETERS.TABLE III PERFORMANCE OF LSTM POLICIES ON THE REAL ROBOT, WHERE THE POLICIES ARE TRAINED WITH SUBSETS OF PARAMETERS HELD FIXED.Model 
Success (Sim) Success (Real) Trials (Real) 
LSTM 
0.91 ± 0.03 
0.89 ± 0.06 
28 
FF no Rand 
0.51 ± 0.05 
0.0 ± 0.0 
10 
FF 
0.83 ± 0.04 
0.67 ± 0.14 
12 
FF + Hist 
0.87 ± 0.03 
0.70 ± 0.10 
20 </p>
<p>Model 
Success 
Trials 
all 
0.89 ± 0.06 
28 
fixed action timestep 
0.29 ± 0.11 
17 
no observation noise 
0.25 ± 0.12 
12 
fixed link mass 
0.64 ± 0.10 
22 
fixed puck friction 
0.48 ± 0.10 
27 </p>
<p>We would like to thank Ankur Handa, Vikash Kumar, Bob McGrew, Matthias Plappert, Alex Ray, Jonas Schneider, and Peter Welinder for their support and feedback on this project.
Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, 10.1038/nature14236Nature. 5187540V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, "Human-level control through deep reinforcement learning," Nature, vol. 518, no. 7540, pp. 529-533, 02 2015. [Online]. Available: http://dx.doi.org/10.1038/nature14236</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, abs/1509.02971CoRR. T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," CoRR, vol. abs/1509.02971, 2015. [Online].</p>
<p>Benchmarking deep reinforcement learning for continuous control. Y Duan, X Chen, R Houthooft, J Schulman, P Abbeel, abs/1604.06778CoRR. Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, "Benchmarking deep reinforcement learning for continuous control," CoRR, vol. abs/1604.06778, 2016. [Online]. Available: http://arxiv. org/abs/1604.06778</p>
<p>Terrain-adaptive locomotion skills using deep reinforcement learning. X B Peng, G Berseth, M Van De Panne, Proc. SIGGRAPH 2016). SIGGRAPH 2016)35X. B. Peng, G. Berseth, and M. van de Panne, "Terrain-adaptive loco- motion skills using deep reinforcement learning," ACM Transactions on Graphics (Proc. SIGGRAPH 2016), vol. 35, no. 4, 2016.</p>
<p>Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. X B Peng, G Berseth, K Yin, M Van De Panne, Proc. SIGGRAPH 2017). SIGGRAPH 2017)36X. B. Peng, G. Berseth, K. Yin, and M. van de Panne, "Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning," ACM Transactions on Graphics (Proc. SIGGRAPH 2017), vol. 36, no. 4, 2017.</p>
<p>Learning to schedule control fragments for physics-based characters using deep q-learning. L Liu, J Hodgins, http:/doi.acm.org/10.1145/3083723ACM Trans. Graph. 363L. Liu and J. Hodgins, "Learning to schedule control fragments for physics-based characters using deep q-learning," ACM Trans. Graph., vol. 36, no. 3, pp. 29:1-29:14, Jun. 2017. [Online]. Available: http://doi.acm.org/10.1145/3083723</p>
<p>Emergence of locomotion behaviours in rich environments. N Heess, D Tb, S Sriram, J Lemmon, J Merel, G Wayne, Y Tassa, T Erez, Z Wang, S M A Eslami, M A Riedmiller, D Silver, abs/1707.02286CoRR. N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, S. M. A. Eslami, M. A. Riedmiller, and D. Silver, "Emergence of locomotion behaviours in rich environments," CoRR, vol. abs/1707.02286, 2017. [Online]. Available: http://arxiv.org/abs/1707.02286</p>
<p>Learning contactrich manipulation skills with guided policy search. S Levine, N Wagener, P Abbeel, abs/1501.05611CoRR. S. Levine, N. Wagener, and P. Abbeel, "Learning contact- rich manipulation skills with guided policy search," CoRR, vol. abs/1501.05611, 2015. [Online]. Available: http://arxiv.org/abs/1501. 05611</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, P Abbeel, abs/1504.00702CoRR. S. Levine, C. Finn, T. Darrell, and P. Abbeel, "End-to-end training of deep visuomotor policies," CoRR, vol. abs/1504.00702, 2015. [Online]. Available: http://arxiv.org/abs/1504.00702</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, D Quillen, abs/1603.02199CoRR. S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection," CoRR, vol. abs/1603.02199, 2016. [Online]. Available: http://arxiv.org/abs/1603.02199</p>
<p>Adapting deep visuomotor representations with weak pairwise constraints. E Tzeng, C Devin, J Hoffman, C Finn, X Peng, S Levine, K Saenko, T Darrell, abs/1511.07111CoRR. E. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine, K. Saenko, and T. Darrell, "Adapting deep visuomotor representations with weak pairwise constraints," CoRR, vol. abs/1511.07111, 2015. [Online]. Available: http://arxiv.org/abs/1511.07111</p>
<p>Domain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, J. Mach. Learn. Res. 171Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky, "Domain-adversarial training of neural networks," J. Mach. Learn. Res., vol. 17, no. 1, pp. 2096-2030, Jan. 2016. [Online]. Available: http: //dl.acm.org/citation.cfm?id=2946645.2946704</p>
<p>Learning invariant feature spaces to transfer skills with reinforcement learning. A Gupta, C Devin, Y Liu, P Abbeel, S Levine, abs/1703.02949CoRR. A. Gupta, C. Devin, Y. Liu, P. Abbeel, and S. Levine, "Learning invariant feature spaces to transfer skills with reinforcement learning," CoRR, vol. abs/1703.02949, 2017. [Online]. Available: http://arxiv.org/abs/1703.02949</p>
<p>Learning transferable policies for monocular reactive MAV control. S Daftry, J A Bagnell, M Hebert, abs/1608.00627CoRR. S. Daftry, J. A. Bagnell, and M. Hebert, "Learning transferable policies for monocular reactive MAV control," CoRR, vol. abs/1608.00627, 2016. [Online]. Available: http://arxiv.org/abs/1608.00627</p>
<p>Mutual alignment transfer learning. M Wulfmeier, I Posner, P Abbeel, abs/1707.07907CoRR. M. Wulfmeier, I. Posner, and P. Abbeel, "Mutual alignment transfer learning," CoRR, vol. abs/1707.07907, 2017. [Online]. Available: http://arxiv.org/abs/1707.07907</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerik, T Rothörl, N Heess, R Pascanu, R Hadsell, abs/1610.04286CoRR. A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell, "Sim-to-real robot learning from pixels with progressive nets," CoRR, vol. abs/1610.04286, 2016. [Online].</p>
<p>Transfer from simulation to real world through learning deep inverse dynamics model. P Christiano, Z Shah, I Mordatch, J Schneider, T Blackwell, J Tobin, P Abbeel, W Zaremba, abs/1610.03518CoRR. P. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin, P. Abbeel, and W. Zaremba, "Transfer from simulation to real world through learning deep inverse dynamics model," CoRR, vol. abs/1610.03518, 2016. [Online]. Available: http://arxiv.org/abs/ 1610.03518</p>
<p>Cad2rl: Real single-image flight without a single real image. F Sadeghi, S Levine, abs/1611.04201CoRR. F. Sadeghi and S. Levine, "Cad2rl: Real single-image flight without a single real image," CoRR, vol. abs/1611.04201, 2016. [Online].</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, abs/1703.06907CoRR. J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," CoRR, vol. abs/1703.06907, 2017. [Online]. Available: http://arxiv.org/abs/1703.06907</p>
<p>3d simulation for robot arm control with deep q-learning. S James, E Johns, abs/1609.03759CoRR. S. James and E. Johns, "3d simulation for robot arm control with deep q-learning," CoRR, vol. abs/1609.03759, 2016. [Online].</p>
<p>Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids. I Mordatch, K Lowrey, E Todorov, 10.1109/IROS.2015.73541262015 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2015. Hamburg, GermanyI. Mordatch, K. Lowrey, and E. Todorov, "Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids," in 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2015, Hamburg, Germany, September 28 -October 2, 2015, 2015, pp. 5307-5314. [Online]. Available: https://doi.org/10.1109/IROS.2015.7354126</p>
<p>Epopt: Learning robust neural network policies using model ensembles. A Rajeswaran, S Ghotra, S Levine, B Ravindran, abs/1610.01283CoRR. A. Rajeswaran, S. Ghotra, S. Levine, and B. Ravindran, "Epopt: Learning robust neural network policies using model ensembles," CoRR, vol. abs/1610.01283, 2016. [Online]. Available: http://arxiv. org/abs/1610.01283</p>
<p>Robust adversarial reinforcement learning. L Pinto, J Davidson, R Sukthankar, A Gupta, abs/1703.02702CoRR. L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, "Robust adversarial reinforcement learning," CoRR, vol. abs/1703.02702, 2017. [Online]. Available: http://arxiv.org/abs/1703.02702</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, C K Liu, G Turk, abs/1702.02453CoRR. W. Yu, C. K. Liu, and G. Turk, "Preparing for the unknown: Learning a universal policy with online system identification," CoRR, vol. abs/1702.02453, 2017. [Online]. Available: http://arxiv.org/abs/1702. 02453</p>
<p>Reinforcement learning for pivoting task. R Antonova, S Cruciani, C Smith, D Kragic, abs/1703.00472CoRR. R. Antonova, S. Cruciani, C. Smith, and D. Kragic, "Reinforcement learning for pivoting task," CoRR, vol. abs/1703.00472, 2017. [Online]. Available: http://arxiv.org/abs/1703.00472</p>
<p>More than a million ways to be pushed: A high-fidelity experimental data set of planar pushing. K Yu, M Bauzá, N Fazeli, A Rodriguez, abs/1604.04038CoRR. K. Yu, M. Bauzá, N. Fazeli, and A. Rodriguez, "More than a million ways to be pushed: A high-fidelity experimental data set of planar pushing," CoRR, vol. abs/1604.04038, 2016. [Online]. Available: http://arxiv.org/abs/1604.04038</p>
<p>Stable pushing: Mechanics, controllability, and planning. K M Lynch, M T Mason, The International Journal of Robotics Research. 156K. M. Lynch and M. T. Mason, "Stable pushing: Mechanics, controlla- bility, and planning," The International Journal of Robotics Research, vol. 15, no. 6, pp. 533-556, 1996.</p>
<p>A framework for push-grasping in clutter. M Dogar, S Srinivasa, Robotics: Science and Systems VII. Pittsburgh, PAMIT PressM. Dogar and S. Srinivasa, "A framework for push-grasping in clutter," in Robotics: Science and Systems VII. Pittsburgh, PA: MIT Press, July 2011.</p>
<p>Parameter and contact force estimation of planar rigid-bodies undergoing frictional contact. N Fazeli, R Kolbert, R Tedrake, A Rodriguez, The International Journal of Robotics Research. 000278364917698749N. Fazeli, R. Kolbert, R. Tedrake, and A. Rodriguez, "Parameter and contact force estimation of planar rigid-bodies undergoing frictional contact," The International Journal of Robotics Research, vol. 0, no. 0, p. 0278364917698749, 2016.</p>
<p>Posing polygonal objects in the plane by pushing. S Akella, M T Mason, The International Journal of Robotics Research. 171S. Akella and M. T. Mason, "Posing polygonal objects in the plane by pushing," The International Journal of Robotics Research, vol. 17, no. 1, pp. 70-88, 1998.</p>
<p>Unsupervised learning for physical interaction through video prediction. C Finn, I J Goodfellow, S Levine, abs/1605.07157CoRR. C. Finn, I. J. Goodfellow, and S. Levine, "Unsupervised learning for physical interaction through video prediction," CoRR, vol. abs/1605.07157, 2016. [Online]. Available: http://arxiv.org/abs/1605. 07157</p>
<p>Policy transfer via modularity. D H Ignasi Clavera, P , IROS. IEEED. H. Ignasi Clavera and P. Abbeel, "Policy transfer via modularity," in IROS. IEEE, 2017.</p>
<p>Policy gradient methods for reinforcement learning with function approximation. R S Sutton, D Mcallester, S Singh, Y Mansour, Advances in Neural Information Processing Systems 12. MIT PressR. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour, "Policy gradient methods for reinforcement learning with function approximation," in In Advances in Neural Information Processing Systems 12. MIT Press, 2000, pp. 1057-1063.</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, PMLR, 07-09Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning. Research, F. Bach and D. Bleithe 32nd International Conference on Machine Learning, ser. Machine LearningLille, France37T. Schaul, D. Horgan, K. Gregor, and D. Silver, "Universal value function approximators," in Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, F. Bach and D. Blei, Eds., vol. 37. Lille, France: PMLR, 07-09 Jul 2015, pp. 1312-1320. [Online]. Available: http://proceedings.mlr.press/v37/schaul15.html</p>
<p>Hindsight experience replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, Advances in Neural Information Processing Systems. M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welin- der, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, "Hindsight experience replay," in Advances in Neural Information Processing Systems, 2017.</p>
<p>Memory-based control with recurrent neural networks. N Heess, J J Hunt, T P Lillicrap, D Silver, abs/1512.04455CoRR. N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver, "Memory-based control with recurrent neural networks," CoRR, vol. abs/1512.04455, 2015. [Online]. Available: http://arxiv.org/abs/1512.04455</p>
<p>Learning to communicate with deep multi-agent reinforcement learning. J N Foerster, Y M Assael, N Freitas, S Whiteson, abs/1605.06676CoRR. J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, "Learning to communicate with deep multi-agent reinforcement learning," CoRR, vol. abs/1605.06676, 2016. [Online]. Available: http://arxiv.org/abs/1605.06676</p>
<p>Multi-agent actor-critic for mixed cooperativecompetitive environments. R Lowe, Y Wu, A Tamar, J Harb, P Abbeel, I Mordatch, abs/1706.02275CoRR. R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, "Multi-agent actor-critic for mixed cooperative- competitive environments," CoRR, vol. abs/1706.02275, 2017. [Online]. Available: http://arxiv.org/abs/1706.02275</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, IROS. IEEEE. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model-based control." in IROS. IEEE, 2012, pp. 5026-5033.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, abs/1412.6980CoRR. D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," CoRR, vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412.6980</p>            </div>
        </div>

    </div>
</body>
</html>
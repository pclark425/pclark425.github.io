<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2862 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2862</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2862</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-15b91292ba80adaa87361a0e8894e47899f02f1d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/15b91292ba80adaa87361a0e8894e47899f02f1d" target="_blank">Learning Dynamic Belief Graphs to Generalize on Text-Based Games</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics.</p>
                <p><strong>Paper Abstract:</strong> Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of 24.2%.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2862.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2862.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Aided Transformer Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based RL agent that infers and maintains a continuous, multi-relational belief graph as an explicit memory to plan and act in partially observable text-based games (TextWorld). The graph updater is pre-trained with self-supervised objectives and an RNN-based graph operation function carries memory across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GATA consists of two modules: (1) a graph updater that constructs/updates a continuous belief graph G_t (real-valued adjacency tensor) from the previous belief graph, the last action, and the current text observation; (2) an action selector that encodes text (transformer) and graph (R-GCN) representations, fuses them with a bi-directional attention aggregator, and scores candidate actions via self-attention + MLP. The graph updater outputs a graph-difference Δg_t via f_Δ (an attention-based aggregator over text and graph encodings), feeds Δg_t into an RNN (GRU-style) recurrent state h_t which acts as memory, and decodes h_t with f_d (MLP + tanh) to a continuous adjacency tensor G_t. The graph encoder uses an R-GCN conditioned on learned relation embeddings; the text encoder is a custom transformer block initialized with fastText embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Custom transformer encoder/decoder (not a large off-the-shelf LLM); word embeddings initialized from 300-d fastText (frozen). Model uses transformer blocks (self-attention + conv layers + MLP) but is not GPT/GPT-like pretrained LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>continuous multi-relational belief graph (structured knowledge-graph style memory) plus recurrent hidden state (RNN) as temporal memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Belief graph represented as a continuous adjacency tensor G ∈ [-1,1]^{R × N × N} (R relations, N entities). GATA uses f_Δ to produce Δg_t from h_{G_{t-1}} (graph encoder outputs), h_{O_t} (text encoder outputs), and h_{A_{t-1}}; Δg_t is input to an RNN producing hidden state h_t which f_d decodes to G_t. The graph encoder is a 6-layer R-GCN (hidden size 64) augmented with learned relation embeddings concatenated into message passing. The action selector retrieves relevant graph info via a bi-directional attention aggregator between token embeddings and node embeddings; R-GCN message passing acts as local/global retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Configured with R=10 relation types and N=99 entities (so the adjacency tensor is 10 × 99 × 99 entries; implementation stores both relation and inverse-relation slices so effectively 2R × N × N). Internal hidden sizes: graph encoder hidden size H=64, node embedding size 100, relation embedding size 32; recurrent state h_t dimension aligned to H.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Attention-based retrieval: bi-directional trilinear attention between text tokens and graph node embeddings; R-GCN message passing over the adjacency tensor to compute node representations; self-attention in scorer reinforces token-token and node-node dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Memory (belief graph) is updated every step: the updater computes Δg_t from current observation O_t, previous action A_{t-1}, and prior graph; Δg_t is fed to an RNN to produce h_t and decoded to G_t by an MLP (f_d). The graph updater is pre-trained with self-supervised Observation Generation (OG) and Contrastive Observation Classification (COC) on FTWP transitions and is then frozen during RL (the action selector is trained with Double DQN).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>TextWorld (500+ unique games from FTWP; multi-difficulty levels; family of cooking/recipe games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Choice-based cooking games with partial observability: randomized maps (1–6 locations depending on level), randomized recipes (1–3 ingredients depending on level), sparse rewards for collecting/processing ingredients, some games require cutting/cooking tools, varying verbosity and combinatorial action spaces; training/test split emphasizes generalization to unseen game configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Best GATA variant (combined pre-training and with text observations) outperforms text-only baselines by an average of +24.2% normalized test score across difficulty levels on the TextWorld suite (reported on 500+ unique games). Representative normalized test scores (100 training games, GATA variant pre-trained with both self-supervised tasks and using observations): Level1 62.5%, Level2 33.0%, Level3 46.7%, Level4 25.9%, Level5 33.4% (table entry labeled with highest-performing GATA variant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Text-only baseline Tr-DQN (transformer encoder, no explicit graph memory) normalized test scores (100 training games): Level1 62.5%, Level2 32.0%, Level3 38.3%, Level4 17.7%, Level5 34.6%. Tr-DRQN (transformer + recurrent implicit memory) shows modest improvements over Tr-DQN (average relative improvements reported ≈ +3.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Ablations compare: (a) pre-training objectives (OG vs COC vs both), (b) adding text observations to the action scorer, (c) continuous belief graphs vs discrete updater, (d) use of ground-truth full graphs. Key findings: combining pre-training objectives (OG+COC) yields better downstream RL performance; providing text observations to the scorer further improves performance (helps counteract accumulated graph errors); continuous-valued belief graphs with learned update function outperform the discrete graph-updater variant (GATA-GTP) in many settings; access to ground-truth full graphs (GATA-GTF) sets an upper bound with substantially higher performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared continuous graph memory to (i) discrete graph updater (GATA-GTP) and (ii) ground-truth full graph input (GATA-GTF). Continuous learned graphs (GATA) generally outperformed the discrete command-generation updater (GATA-GTP). Ground-truth full graphs (GATA-GTF) outperformed all and represent an upper bound (e.g., normalized scores up to 95% on some levels), showing that accurate full-state memory greatly aids performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Structured, graph-structured memory improves planning and generalization in partially observable text games compared to text-only agents; pre-training belief-graph construction via self-supervised objectives (OG and contrastive COC) is useful; combining graph memory with attention to current observations reduces error accumulation; continuous-valued graph representations and learned, recurrent update functions are more robust than discrete update commands.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2862.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2862.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA-GTP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATA with Ground-Truth Pre-trained Discrete Graph Updater</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of GATA that pre-trains a discrete graph-updater using ground-truth KGs (command-generation discrete operations) and maintains a discrete multi-relational belief graph during play; action selector identical to GATA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA-GTP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same action selector as GATA; graph updater is discrete: it generates sequences of add/delete commands (e.g., add(node1,node2,relation)) as Δg_t using a transformer Seq2Seq model trained on FTWP ground-truth graph differences. During RL the discrete updater is run to update a discrete belief graph which the action selector encodes via R-GCN.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Transformer Seq2Seq used for discrete command generation; transformer text encoder (same custom transformer with fastText embeddings) for action scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>discrete knowledge-graph style memory (partial observed KGs represented as discrete edges)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Discrete belief graph maintained as a set of symbolic edges; updater generates token sequences of add/delete operations (command-generation), sorted for consistent output ordering; action selector uses an R-GCN over the discrete graph.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Uses the TextWorld provided graph vocabulary (discrete nodes/relations); capacity inherently bounded by game-specific node/relation sets (in experiments up to the same N and R as GATA, but represented discretely).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>R-GCN encoding of discrete graph + bi-directional attention with text tokens in the aggregator (same as GATA action selector).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>At each step the Seq2Seq command generator outputs k≥0 add/delete operations; updater is pre-trained on FTWP transitions with teacher-forcing and then used (detached) during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>TextWorld (FTWP)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same TextWorld cooking/recipe games as used for GATA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GATA-GTP sometimes improves over text-only baselines when text observations are also provided; example (100 training games, with text observations) normalized scores: Level1 62.5%, Level2 32.0%, Level3 51.7%, Level4 21.8%, Level5 23.5% (table entry 'diamond★'); overall its average improvement over Tr-DQN is smaller than GATA's.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>When not augmented with text observations, or when using only discrete belief graphs, performance can be worse than continuous GATA and in some settings worse than Tr-DQN (discrete operations are vulnerable to error accumulation and discretization/rounding issues).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Paper finds discrete updater (GATA-GTP) is more prone to error accumulation and less robust than continuous belief-graph updater; adding text observations helps GATA-GTP but it still underperforms the continuous GATA in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Direct comparison: discrete updater (GATA-GTP) vs continuous belief graph (GATA) — continuous variant often performs better; both are outperformed by ground-truth full-graph access (GATA-GTF).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>While using ground-truth graphs to pre-train a discrete updater gives supervision, discrete update operations are brittle; continuous learned belief graphs with learned update functions generalize more robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2862.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2862.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA-GTF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATA with Ground-Truth Full Graph Input (upper bound)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An upper-bound agent that feeds the full ground-truth game graph (fully observable, exact state) directly into the action selector (no graph updater); this measures the value of perfect structured memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA-GTF</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Removes the graph-updater; at each step the action selector receives the full ground-truth graph G^{full} from TextWorld, encodes it with the R-GCN and combines with text (when provided) for action scoring. Used to measure performance upper bound when memory is perfect/full-state.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Same custom transformer-based text encoder + R-GCN graph encoder; no Seq2Seq updater.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>ground-truth full knowledge graph (perfect full-state memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Direct use of TextWorld-provided discrete full-state graphs as R-GCN input; no update errors or partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Full game-state KGs (complete set of entities and relations for each step) as provided by TextWorld; capacity varies with game but contains complete state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Direct: graph encoder (R-GCN) encodes the ground-truth graph; aggregator attends between graph nodes and text tokens as in GATA.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>N/A (graph provided by environment each step; no learned updates).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>TextWorld (FTWP)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same multi-difficulty, cooking/recipe text-based games; using ground-truth graphs removes partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Strong upper-bound performance. Example (100 training games): normalized test scores up to Level1 95.0%, Level2 95.0%, Level3 70.0%, Level4 37.3%, Level5 52.8%; reported average relative improvement over Tr-DQN of +99.0% (for some training regimes) and an overall average improvement reported +81.6% in table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Using ground-truth full graphs dramatically increases scores compared to inferred belief graphs and text-only baselines, indicating that accurate full-state memory removes partial-observability bottlenecks and provides a significant performance ceiling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Ground-truth full graphs > continuous learned belief graphs (GATA) > discrete belief graphs (GATA-GTP) > text-only baselines (Tr-DQN).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Perfect, fully-observable graph memory yields the best performance and demonstrates that structured state information (if accurate) is extremely beneficial for generalization and efficient reward optimization in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2862.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2862.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tr-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based Deep Recurrent Q-Network (DRQN variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-only baseline that replaces LSTM with a transformer encoder and uses a recurrent policy (DRQN) to provide implicit memory; used for comparison to explicit graph memory agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tr-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline agent using transformer-based text encoder (same architecture as GATA's text encoder) feeding into a recurrent policy (DRQN) that maintains an implicit RNN hidden state as memory; trained with reinforcement learning (Double DQN variants). No explicit structured graph memory; uses recurrent hidden state to integrate observations over time.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Custom transformer encoder (same fastText initialization) + recurrent Q-network (DRQN). Not a pre-trained large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit recurrent (policy RNN) working memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Transformer encodes observations; outputs fed into an RNN (DRQN) whose hidden state carries past information; no structured external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>RNN hidden-state dimensionality as implemented in DRQN (paper does not give exact hidden-dim here for Tr-DRQN; baseline uses standard DRQN sizes comparable to GATA's hidden sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency via RNN hidden state; no explicit retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>RNN state updated at each time-step as in standard recurrent policies.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>TextWorld (FTWP)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same as GATA experiments (multiple difficulty levels, partial observability).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Tr-DRQN outperforms Tr-DQN modestly: reported relative improvement ≈ +3.9% (aggregate) over Tr-DQN in some settings. Example (20 training games row): Tr-DRQN scores sometimes higher than Tr-DQN on specific levels (e.g., level 3 improved).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Tr-DQN (transformer encoder without recurrent memory) scores are provided as baseline: e.g., 100-training Tr-DQN normalized test scores: Level1 62.5%, Level2 32.0%, Level3 38.3%, Level4 17.7%, Level5 34.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared implicitly with explicit structured belief-graph memory (GATA); GATA's explicit graph memory substantially outperforms Tr-DRQN on generalization across games.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Implicit recurrent memory provides some benefit over memoryless transformer policies, but structured graph memories provide larger gains for generalization and planning in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2862.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2862.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tr-DRQN+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer DRQN with Episodic Counting Bonus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer+DRQN baseline augmented with an episodic count-based exploration bonus (counting memory) to encourage exploration; used to compare count-based memory/exploration with structured graph memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tr-DRQN+</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same as Tr-DRQN but augmented with episodic counting bonus as in prior work [Yuan et al.]; the counting mechanism acts as an episodic exploration memory to encourage visiting novel states.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Custom transformer encoder + DRQN policy; counting is an episodic count-based bonus component (not a retrieval LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic count-based memory (counts of visited states/observations used as intrinsic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Counting mechanism that tracks visits (episodic) to encourage exploration; integrated with DRQN RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency/occurrence counts used to compute exploration bonus; not retrieval for action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Counts updated on state/observation visits (episodically).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>TextWorld (FTWP)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same set of cooking text games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Tr-DRQN+ shows modest improvements over Tr-DQN in some settings (e.g., relative improvements reported +10.7% in some 20-game rows in table but mixed results across training sizes). Overall still substantially below GATA in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Tr-DQN baseline as above.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Counting-based episodic memory improves exploration somewhat but is outperformed by explicit graph-structured memory (GATA) for generalization across many unseen games.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Episodic counting helps exploration, but does not substitute for structured, semantic memory (belief graphs) required for planning and generalization in more complex TextWorld games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2862.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2862.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior mentions: counting / memory-networks / KG-based agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Related prior agents using memory (cited in paper): counting memory (Yuan et al.), memory networks (Urbanek et al.), KG-based agents (Ammanabrolu & Riedl / Ammanabrolu & Hausknecht)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites several prior approaches that use memory-like mechanisms for text games: episodic counting bonuses, memory networks for dialogue/adventure tasks, and (often rule-based) knowledge-graph construction used as memory to inform policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Prior-memory agents (mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited works include: (a) Yuan et al. (2018) 'Counting to explore and generalize' — count-based episodic memory for exploration; (b) Urbanek et al. (2019) — memory networks and ranking systems for adventure dialog tasks; (c) Ammanabrolu & Riedl / Ammanabrolu & Hausknecht — build KGs via rule-based heuristics to feed RL agents. The current paper positions GATA as a learned, end-to-end graph-memory alternative to these heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>mentions include episodic counting, memory networks, hand-crafted knowledge graphs (KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>TextWorld and other interactive fiction/adventure/dialog tasks referenced in citations</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Varied; some prior work trains/tests on single games, some target multi-game generalization; tasks include adventure-style interactive fiction and dialog-grounded action tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Paper references these works to motivate that memory (counting, memory nets, KGs) helps exploration and planning; but stresses prior reliance on hand-crafted heuristics and emphasizes GATA's learned graph-based memory as a more general approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Building dynamic knowledge graphs from text-based games <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 2)</em></li>
                <li>Exploration based language learning for text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2862",
    "paper_id": "paper-15b91292ba80adaa87361a0e8894e47899f02f1d",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [
        {
            "name_short": "GATA",
            "name_full": "Graph-Aided Transformer Agent",
            "brief_description": "A transformer-based RL agent that infers and maintains a continuous, multi-relational belief graph as an explicit memory to plan and act in partially observable text-based games (TextWorld). The graph updater is pre-trained with self-supervised objectives and an RNN-based graph operation function carries memory across steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GATA",
            "agent_description": "GATA consists of two modules: (1) a graph updater that constructs/updates a continuous belief graph G_t (real-valued adjacency tensor) from the previous belief graph, the last action, and the current text observation; (2) an action selector that encodes text (transformer) and graph (R-GCN) representations, fuses them with a bi-directional attention aggregator, and scores candidate actions via self-attention + MLP. The graph updater outputs a graph-difference Δg_t via f_Δ (an attention-based aggregator over text and graph encodings), feeds Δg_t into an RNN (GRU-style) recurrent state h_t which acts as memory, and decodes h_t with f_d (MLP + tanh) to a continuous adjacency tensor G_t. The graph encoder uses an R-GCN conditioned on learned relation embeddings; the text encoder is a custom transformer block initialized with fastText embeddings.",
            "base_llm": "Custom transformer encoder/decoder (not a large off-the-shelf LLM); word embeddings initialized from 300-d fastText (frozen). Model uses transformer blocks (self-attention + conv layers + MLP) but is not GPT/GPT-like pretrained LLM.",
            "uses_memory": true,
            "memory_type": "continuous multi-relational belief graph (structured knowledge-graph style memory) plus recurrent hidden state (RNN) as temporal memory",
            "memory_architecture": "Belief graph represented as a continuous adjacency tensor G ∈ [-1,1]^{R × N × N} (R relations, N entities). GATA uses f_Δ to produce Δg_t from h_{G_{t-1}} (graph encoder outputs), h_{O_t} (text encoder outputs), and h_{A_{t-1}}; Δg_t is input to an RNN producing hidden state h_t which f_d decodes to G_t. The graph encoder is a 6-layer R-GCN (hidden size 64) augmented with learned relation embeddings concatenated into message passing. The action selector retrieves relevant graph info via a bi-directional attention aggregator between token embeddings and node embeddings; R-GCN message passing acts as local/global retrieval.",
            "memory_capacity": "Configured with R=10 relation types and N=99 entities (so the adjacency tensor is 10 × 99 × 99 entries; implementation stores both relation and inverse-relation slices so effectively 2R × N × N). Internal hidden sizes: graph encoder hidden size H=64, node embedding size 100, relation embedding size 32; recurrent state h_t dimension aligned to H.",
            "memory_retrieval_method": "Attention-based retrieval: bi-directional trilinear attention between text tokens and graph node embeddings; R-GCN message passing over the adjacency tensor to compute node representations; self-attention in scorer reinforces token-token and node-node dependencies.",
            "memory_update_strategy": "Memory (belief graph) is updated every step: the updater computes Δg_t from current observation O_t, previous action A_{t-1}, and prior graph; Δg_t is fed to an RNN to produce h_t and decoded to G_t by an MLP (f_d). The graph updater is pre-trained with self-supervised Observation Generation (OG) and Contrastive Observation Classification (COC) on FTWP transitions and is then frozen during RL (the action selector is trained with Double DQN).",
            "text_game_benchmark": "TextWorld (500+ unique games from FTWP; multi-difficulty levels; family of cooking/recipe games)",
            "game_characteristics": "Choice-based cooking games with partial observability: randomized maps (1–6 locations depending on level), randomized recipes (1–3 ingredients depending on level), sparse rewards for collecting/processing ingredients, some games require cutting/cooking tools, varying verbosity and combinatorial action spaces; training/test split emphasizes generalization to unseen game configurations.",
            "performance_with_memory": "Best GATA variant (combined pre-training and with text observations) outperforms text-only baselines by an average of +24.2% normalized test score across difficulty levels on the TextWorld suite (reported on 500+ unique games). Representative normalized test scores (100 training games, GATA variant pre-trained with both self-supervised tasks and using observations): Level1 62.5%, Level2 33.0%, Level3 46.7%, Level4 25.9%, Level5 33.4% (table entry labeled with highest-performing GATA variant).",
            "performance_without_memory": "Text-only baseline Tr-DQN (transformer encoder, no explicit graph memory) normalized test scores (100 training games): Level1 62.5%, Level2 32.0%, Level3 38.3%, Level4 17.7%, Level5 34.6%. Tr-DRQN (transformer + recurrent implicit memory) shows modest improvements over Tr-DQN (average relative improvements reported ≈ +3.9%).",
            "has_ablation_study": true,
            "memory_ablation_results": "Ablations compare: (a) pre-training objectives (OG vs COC vs both), (b) adding text observations to the action scorer, (c) continuous belief graphs vs discrete updater, (d) use of ground-truth full graphs. Key findings: combining pre-training objectives (OG+COC) yields better downstream RL performance; providing text observations to the scorer further improves performance (helps counteract accumulated graph errors); continuous-valued belief graphs with learned update function outperform the discrete graph-updater variant (GATA-GTP) in many settings; access to ground-truth full graphs (GATA-GTF) sets an upper bound with substantially higher performance.",
            "comparison_with_other_memory_types": "Compared continuous graph memory to (i) discrete graph updater (GATA-GTP) and (ii) ground-truth full graph input (GATA-GTF). Continuous learned graphs (GATA) generally outperformed the discrete command-generation updater (GATA-GTP). Ground-truth full graphs (GATA-GTF) outperformed all and represent an upper bound (e.g., normalized scores up to 95% on some levels), showing that accurate full-state memory greatly aids performance.",
            "key_findings_about_memory_effectiveness": "Structured, graph-structured memory improves planning and generalization in partially observable text games compared to text-only agents; pre-training belief-graph construction via self-supervised objectives (OG and contrastive COC) is useful; combining graph memory with attention to current observations reduces error accumulation; continuous-valued graph representations and learned, recurrent update functions are more robust than discrete update commands.",
            "uuid": "e2862.0",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "GATA-GTP",
            "name_full": "GATA with Ground-Truth Pre-trained Discrete Graph Updater",
            "brief_description": "A variant of GATA that pre-trains a discrete graph-updater using ground-truth KGs (command-generation discrete operations) and maintains a discrete multi-relational belief graph during play; action selector identical to GATA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GATA-GTP",
            "agent_description": "Same action selector as GATA; graph updater is discrete: it generates sequences of add/delete commands (e.g., add(node1,node2,relation)) as Δg_t using a transformer Seq2Seq model trained on FTWP ground-truth graph differences. During RL the discrete updater is run to update a discrete belief graph which the action selector encodes via R-GCN.",
            "base_llm": "Transformer Seq2Seq used for discrete command generation; transformer text encoder (same custom transformer with fastText embeddings) for action scoring.",
            "uses_memory": true,
            "memory_type": "discrete knowledge-graph style memory (partial observed KGs represented as discrete edges)",
            "memory_architecture": "Discrete belief graph maintained as a set of symbolic edges; updater generates token sequences of add/delete operations (command-generation), sorted for consistent output ordering; action selector uses an R-GCN over the discrete graph.",
            "memory_capacity": "Uses the TextWorld provided graph vocabulary (discrete nodes/relations); capacity inherently bounded by game-specific node/relation sets (in experiments up to the same N and R as GATA, but represented discretely).",
            "memory_retrieval_method": "R-GCN encoding of discrete graph + bi-directional attention with text tokens in the aggregator (same as GATA action selector).",
            "memory_update_strategy": "At each step the Seq2Seq command generator outputs k≥0 add/delete operations; updater is pre-trained on FTWP transitions with teacher-forcing and then used (detached) during RL.",
            "text_game_benchmark": "TextWorld (FTWP)",
            "game_characteristics": "Same TextWorld cooking/recipe games as used for GATA experiments.",
            "performance_with_memory": "GATA-GTP sometimes improves over text-only baselines when text observations are also provided; example (100 training games, with text observations) normalized scores: Level1 62.5%, Level2 32.0%, Level3 51.7%, Level4 21.8%, Level5 23.5% (table entry 'diamond★'); overall its average improvement over Tr-DQN is smaller than GATA's.",
            "performance_without_memory": "When not augmented with text observations, or when using only discrete belief graphs, performance can be worse than continuous GATA and in some settings worse than Tr-DQN (discrete operations are vulnerable to error accumulation and discretization/rounding issues).",
            "has_ablation_study": true,
            "memory_ablation_results": "Paper finds discrete updater (GATA-GTP) is more prone to error accumulation and less robust than continuous belief-graph updater; adding text observations helps GATA-GTP but it still underperforms the continuous GATA in many settings.",
            "comparison_with_other_memory_types": "Direct comparison: discrete updater (GATA-GTP) vs continuous belief graph (GATA) — continuous variant often performs better; both are outperformed by ground-truth full-graph access (GATA-GTF).",
            "key_findings_about_memory_effectiveness": "While using ground-truth graphs to pre-train a discrete updater gives supervision, discrete update operations are brittle; continuous learned belief graphs with learned update functions generalize more robustly.",
            "uuid": "e2862.1",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "GATA-GTF",
            "name_full": "GATA with Ground-Truth Full Graph Input (upper bound)",
            "brief_description": "An upper-bound agent that feeds the full ground-truth game graph (fully observable, exact state) directly into the action selector (no graph updater); this measures the value of perfect structured memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GATA-GTF",
            "agent_description": "Removes the graph-updater; at each step the action selector receives the full ground-truth graph G^{full} from TextWorld, encodes it with the R-GCN and combines with text (when provided) for action scoring. Used to measure performance upper bound when memory is perfect/full-state.",
            "base_llm": "Same custom transformer-based text encoder + R-GCN graph encoder; no Seq2Seq updater.",
            "uses_memory": true,
            "memory_type": "ground-truth full knowledge graph (perfect full-state memory)",
            "memory_architecture": "Direct use of TextWorld-provided discrete full-state graphs as R-GCN input; no update errors or partial observability.",
            "memory_capacity": "Full game-state KGs (complete set of entities and relations for each step) as provided by TextWorld; capacity varies with game but contains complete state.",
            "memory_retrieval_method": "Direct: graph encoder (R-GCN) encodes the ground-truth graph; aggregator attends between graph nodes and text tokens as in GATA.",
            "memory_update_strategy": "N/A (graph provided by environment each step; no learned updates).",
            "text_game_benchmark": "TextWorld (FTWP)",
            "game_characteristics": "Same multi-difficulty, cooking/recipe text-based games; using ground-truth graphs removes partial observability.",
            "performance_with_memory": "Strong upper-bound performance. Example (100 training games): normalized test scores up to Level1 95.0%, Level2 95.0%, Level3 70.0%, Level4 37.3%, Level5 52.8%; reported average relative improvement over Tr-DQN of +99.0% (for some training regimes) and an overall average improvement reported +81.6% in table.",
            "performance_without_memory": null,
            "has_ablation_study": true,
            "memory_ablation_results": "Using ground-truth full graphs dramatically increases scores compared to inferred belief graphs and text-only baselines, indicating that accurate full-state memory removes partial-observability bottlenecks and provides a significant performance ceiling.",
            "comparison_with_other_memory_types": "Ground-truth full graphs &gt; continuous learned belief graphs (GATA) &gt; discrete belief graphs (GATA-GTP) &gt; text-only baselines (Tr-DQN).",
            "key_findings_about_memory_effectiveness": "Perfect, fully-observable graph memory yields the best performance and demonstrates that structured state information (if accurate) is extremely beneficial for generalization and efficient reward optimization in text games.",
            "uuid": "e2862.2",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Tr-DRQN",
            "name_full": "Transformer-based Deep Recurrent Q-Network (DRQN variant)",
            "brief_description": "A text-only baseline that replaces LSTM with a transformer encoder and uses a recurrent policy (DRQN) to provide implicit memory; used for comparison to explicit graph memory agents.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Tr-DRQN",
            "agent_description": "Baseline agent using transformer-based text encoder (same architecture as GATA's text encoder) feeding into a recurrent policy (DRQN) that maintains an implicit RNN hidden state as memory; trained with reinforcement learning (Double DQN variants). No explicit structured graph memory; uses recurrent hidden state to integrate observations over time.",
            "base_llm": "Custom transformer encoder (same fastText initialization) + recurrent Q-network (DRQN). Not a pre-trained large language model.",
            "uses_memory": true,
            "memory_type": "implicit recurrent (policy RNN) working memory",
            "memory_architecture": "Transformer encodes observations; outputs fed into an RNN (DRQN) whose hidden state carries past information; no structured external memory.",
            "memory_capacity": "RNN hidden-state dimensionality as implemented in DRQN (paper does not give exact hidden-dim here for Tr-DRQN; baseline uses standard DRQN sizes comparable to GATA's hidden sizes).",
            "memory_retrieval_method": "Recency via RNN hidden state; no explicit retrieval mechanism.",
            "memory_update_strategy": "RNN state updated at each time-step as in standard recurrent policies.",
            "text_game_benchmark": "TextWorld (FTWP)",
            "game_characteristics": "Same as GATA experiments (multiple difficulty levels, partial observability).",
            "performance_with_memory": "Tr-DRQN outperforms Tr-DQN modestly: reported relative improvement ≈ +3.9% (aggregate) over Tr-DQN in some settings. Example (20 training games row): Tr-DRQN scores sometimes higher than Tr-DQN on specific levels (e.g., level 3 improved).",
            "performance_without_memory": "Tr-DQN (transformer encoder without recurrent memory) scores are provided as baseline: e.g., 100-training Tr-DQN normalized test scores: Level1 62.5%, Level2 32.0%, Level3 38.3%, Level4 17.7%, Level5 34.6%.",
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": "Compared implicitly with explicit structured belief-graph memory (GATA); GATA's explicit graph memory substantially outperforms Tr-DRQN on generalization across games.",
            "key_findings_about_memory_effectiveness": "Implicit recurrent memory provides some benefit over memoryless transformer policies, but structured graph memories provide larger gains for generalization and planning in text-based games.",
            "uuid": "e2862.3",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Tr-DRQN+",
            "name_full": "Transformer DRQN with Episodic Counting Bonus",
            "brief_description": "A transformer+DRQN baseline augmented with an episodic count-based exploration bonus (counting memory) to encourage exploration; used to compare count-based memory/exploration with structured graph memory.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Tr-DRQN+",
            "agent_description": "Same as Tr-DRQN but augmented with episodic counting bonus as in prior work [Yuan et al.]; the counting mechanism acts as an episodic exploration memory to encourage visiting novel states.",
            "base_llm": "Custom transformer encoder + DRQN policy; counting is an episodic count-based bonus component (not a retrieval LLM).",
            "uses_memory": true,
            "memory_type": "episodic count-based memory (counts of visited states/observations used as intrinsic reward)",
            "memory_architecture": "Counting mechanism that tracks visits (episodic) to encourage exploration; integrated with DRQN RL training.",
            "memory_capacity": null,
            "memory_retrieval_method": "Recency/occurrence counts used to compute exploration bonus; not retrieval for action generation.",
            "memory_update_strategy": "Counts updated on state/observation visits (episodically).",
            "text_game_benchmark": "TextWorld (FTWP)",
            "game_characteristics": "Same set of cooking text games.",
            "performance_with_memory": "Tr-DRQN+ shows modest improvements over Tr-DQN in some settings (e.g., relative improvements reported +10.7% in some 20-game rows in table but mixed results across training sizes). Overall still substantially below GATA in many settings.",
            "performance_without_memory": "Tr-DQN baseline as above.",
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": "Counting-based episodic memory improves exploration somewhat but is outperformed by explicit graph-structured memory (GATA) for generalization across many unseen games.",
            "key_findings_about_memory_effectiveness": "Episodic counting helps exploration, but does not substitute for structured, semantic memory (belief graphs) required for planning and generalization in more complex TextWorld games.",
            "uuid": "e2862.4",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Prior mentions: counting / memory-networks / KG-based agents",
            "name_full": "Related prior agents using memory (cited in paper): counting memory (Yuan et al.), memory networks (Urbanek et al.), KG-based agents (Ammanabrolu & Riedl / Ammanabrolu & Hausknecht)",
            "brief_description": "The paper cites several prior approaches that use memory-like mechanisms for text games: episodic counting bonuses, memory networks for dialogue/adventure tasks, and (often rule-based) knowledge-graph construction used as memory to inform policies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Prior-memory agents (mentions)",
            "agent_description": "Cited works include: (a) Yuan et al. (2018) 'Counting to explore and generalize' — count-based episodic memory for exploration; (b) Urbanek et al. (2019) — memory networks and ranking systems for adventure dialog tasks; (c) Ammanabrolu & Riedl / Ammanabrolu & Hausknecht — build KGs via rule-based heuristics to feed RL agents. The current paper positions GATA as a learned, end-to-end graph-memory alternative to these heuristics.",
            "base_llm": "",
            "uses_memory": null,
            "memory_type": "mentions include episodic counting, memory networks, hand-crafted knowledge graphs (KGs)",
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "TextWorld and other interactive fiction/adventure/dialog tasks referenced in citations",
            "game_characteristics": "Varied; some prior work trains/tests on single games, some target multi-game generalization; tasks include adventure-style interactive fiction and dialog-grounded action tasks.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Paper references these works to motivate that memory (counting, memory nets, KGs) helps exploration and planning; but stresses prior reliance on hand-crafted heuristics and emphasizes GATA's learned graph-based memory as a more general approach.",
            "uuid": "e2862.5",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Building dynamic knowledge graphs from text-based games",
            "rating": 2
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 2
        },
        {
            "paper_title": "Exploration based language learning for text-based games",
            "rating": 1
        }
    ],
    "cost": 0.021898249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Dynamic Belief Graphs to Generalize on Text-Based Games</h1>
<p>Ashutosh Adhikari ${ }^{\dagger <em>}$ Xingdi Yuan ${ }^{\text { }}$ * Marc-Alexandre Côte ${ }^{\text { } </em>}$ * Mikuláš Zelinka ${ }^{\dagger}$ Marc-Antoine Rondeau ${ }^{\circ}$<br>$\underset{\text { Adam Trischler }{ }^{\circ}}{\text { Romain Laroche }}$ William L. Hamilton ${ }^{\circ}$<br>${ }^{\dagger}$ University of Waterloo ${ }^{\circ}$ Microsoft Research, Montréal ${ }^{\dagger}$ Charles University<br>${ }^{\text {Mila }} \quad{ }^{\circ}$ McGill University ${ }^{\text { }}$ HEC Montréal ${ }^{\ddagger}$ Vector Institute eric.yuan@microsoft.com</p>
<h4>Abstract</h4>
<p>Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of $24.2 \%$.</p>
<h2>1 Introduction</h2>
<p>Text-based games are complex, interactive simulations in which the game state is described with text and players act using simple text commands (e.g., light torch with match). They serve as a proxy for studying how agents can exploit language to comprehend and interact with the environment. Text-based games are a useful challenge in the pursuit of intelligent agents that communicate with humans (e.g., in customer service systems).</p>
<p>Solving text-based games requires a combination of reinforcement learning (RL) and natural language processing (NLP) techniques. However, inherent challenges like partial observability, long-term dependencies, sparse rewards, and combinatorial action spaces make these games very difficult. ${ }^{2}$ For instance, Hausknecht et al. [16] show that a state-of-the-art model achieves a mere $2.56 \%$ of the total possible score on a curated set of text-based games for human players [5]. On the other hand, while text-based games exhibit many of the same difficulties as linguistic tasks like open-ended dialogue, they are more structured and constrained.</p>
<p>To design successful agents for text-based games, previous works have relied largely on heuristics that exploit games' inherent structure. For example, several works have proposed rule-based components</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: GATA playing a text-based game by updating its belief graph. In response to action $A_{t-1}$, the environment returns text observation $O_{t}$. Based on $O_{t}$ and $\mathcal{G}<em t="t">{t-1}$, the agent updates $\mathcal{G}</em>$. In the figure, blue box with squares is the game engine, green box with diamonds is the graph updater, red box with slashes is the action selector.
that prune the action space or shape the rewards according to a priori knowledge of the game dynamics [50, 24, 1, 48]. More recent approaches take advantage of the graph-like structure of textbased games by building knowledge graph (KG) representations of the game state: Ammanabrolu and Riedl [4], Ammanabrolu and Hausknecht [3], for example, use hand-crafted heuristics to populate a KG that feeds into a deep neural agent to inform its policy. Despite progress along this line, we expect more general, effective representations for text-based games to arise in agents that learn and scale more automatically, which replace heuristics with learning [37].
This work investigates how we can learn graph-structured state representations for text-based games in an entirely data-driven manner. We propose the graph aided transformer agent (GATA) ${ }^{3}$ that, in lieu of heuristics, learns to construct and update graph-structured beliefs ${ }^{4}$ and use them to further optimize rewards. We introduce two self-supervised learning strategies—based on text reconstruction and mutual information maximization-which enable our agent to learn latent graph representations without direct supervision or hand-crafted heuristics.
We benchmark GATA on 500+ unique games generated by TextWorld [9], evaluating performance in a setting that requires generalization across different game configurations. We show that GATA outperforms strong baselines, including text-based models with recurrent policies. In addition, we compare GATA to agents with access to ground-truth graph representations of the game state. We show that GATA achieves competitive performance against these baselines even though it receives only partial text observations of the state. Our findings suggest, promisingly, that graph-structured representations provide a useful inductive bias for learning and generalizing in text-based games, and act as a memory enabling agents to optimize rewards in a partially observed setting.}$ and selects a new action $A_{t</p>
<h1>2 Background</h1>
<p>Text-based Games: Text-based games can be formally described as partially observable Markov decision processes (POMDPs) [9]. They are environments in which the player receives text-only observations $O_{t}$ (these describe the observable state, typically only partially) and interacts by issuing short text phrases as actions $A_{t}$ (e.g., in Figure 1, go west moves the player to a new location). Often, the end goal is not clear from the start; the agent must infer the objective by earning sparse rewards for completing subgoals. Text-based games have a variety of difficulty levels determined mainly by the environment's complexity (i.e., how many locations in the game, and how many objects are interactive), the game length (i.e., optimally, how many actions are required to win), and the verbosity (i.e., how much text information is irrelevant to solving the game).</p>
<p>Problem Setting: We use TextWorld [9] to generate unique choice-based games of varying difficulty. All games share the same overarching theme: an agent must gather and process cooking ingredients, placed randomly across multiple locations, according to a recipe it discovers during the game. The agent earns a point for collecting each ingredient and for processing it correctly. The game is won</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>upon completing the recipe. Processing any ingredient incorrectly terminates the game (e.g., slice carrot when the recipe asked for a diced carrot). To process ingredients, an agent must find and use appropriate tools (e.g., a knife to slice, dice, or chop; a stove to fry, an oven to roast).</p>
<p>We divide generated games, all of which have unique recipes and map configurations, into sets for training, validation, and test. Adopting the supervised learning paradigm for evaluating generalization, we tune hyperparameters on the validation set and report performance on a test set of previously unseen games. Testing agents on unseen games (within a difficulty level) is uncommon in prior RL work, where it is standard to train and test on a single game instance. Our approach enables us to measure the robustness of learned policies as they generalize (or fail to) across a "distribution" of related but distinct games. Throughout the paper, we use the term generalization to imply the ability of a single policy to play a distribution of related games (within a particular difficulty level).</p>
<p>Graphs and Text-based Games: We expect graph-based representations to be effective for textbased games because the state in these games adheres to a graph-like structure. The essential content in most observations of the environment corresponds either to entity attributes (e.g., the state of the carrot is sliced) or to relational information about entities in the environment (e.g., the kitchen is north_of the bedroom). This information is naturally represented as a dynamic graph $\mathcal{G}<em t="t">{t}=\left(\mathcal{V}</em>}, \mathcal{E<em t="t">{t}\right)$, where the vertices $\mathcal{V}</em>$ represent relations between entities (e.g., north_of, in, is) that hold at a particular time-step $t$. By design, in fact, the full state of any game generated by TextWorld can be represented explicitly as a graph of this type [53]. The aim of our model, GATA, is to estimate the game state by learning to build graph-structured beliefs from raw text observations. In our experiments, we benchmark GATA against models with direct access to the ground-truth game state rather than GATA's noisy estimate thereof inferred from text.}$ represent entities (including the player, objects, and locations) and their current conditions (e.g., closed, fried, sliced), while the edges $\mathcal{E}_{t</p>
<h1>3 Graph Aided Transformer Agent (GATA)</h1>
<p>In this section, we introduce GATA, a novel transformer-based neural agent that can infer a graphstructured belief state and use that state to guide action selection in text-based games. As shown in Figure 2, the agent consists of two main modules: a graph updater and an action selector. ${ }^{3}$ At game step $t$, the graph updater extracts relevant information from text observation $O_{t}$ and updates its belief graph $\mathcal{G}<em t="t">{t}$ accordingly. The action selector issues action $A</em>$. Figure 1 illustrates the interaction between GATA and a text-based game.}$ conditioned on $O_{t}$ and the belief graph $\mathcal{G}_{t</p>
<h3>3.1 Belief Graph</h3>
<p>We denote by $\mathcal{G}$ a belief graph representing the agent's belief about the true game state according to what it has observed so far. We instantiate $\mathcal{G} \in[-1,1]^{\mathcal{R} \times \mathcal{N} \times \mathcal{N}}$ as a real-valued adjacency tensor, where $\mathcal{R}$ and $\mathcal{N}$ indicate the number of relation types and entities. Each entry ${r, i, j}$ in $\mathcal{G}$ indicates the strength of an inferred relationship $r$ from entity $i$ to entity $j$. We select $\mathcal{R}=10$ and $\mathcal{N}=99$ to match the maximum number of relations and entities in our TextWorld-generated games. In other words, we assume that GATA has access to the vocabularies of possible relations and entities but it must learn the structure among these objects, and their semantics, from scratch.</p>
<h3>3.2 Graph Updater</h3>
<p>The graph updater constructs and updates the dynamic belief graph $\mathcal{G}$ from text observations $O_{t}$. Rather than generating the entire belief graph at each step $t$, we generate a graph update, $\Delta g_{t}$, that represents the change of the agent's belief after receiving a new observation. This is motivated by the fact that observations $O_{t}$ typically communicate only incremental information about the state's change from time step $t-1$ to $t$. The relation between $\Delta g_{t}$ and $\mathcal{G}$ is given by</p>
<p>$$
\mathcal{G}<em t-1="t-1">{t}=\mathcal{G}</em>
$$} \oplus \Delta g_{t</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: GATA in detail. The coloring scheme is same as in Figure 1. The graph updater first generates $\Delta g_{t}$ using $\mathcal{G}<em t="t">{t-1}$ and $O</em>}$. Afterwards the action selector uses $O_{t}$ and the updated graph $\mathcal{G<em t="t">{t}$ to select $A</em>$. Purple dotted line indicates a detached connection (i.e., no back-propagation through such connection).}$ from the list of action candidates $C_{t</p>
<p>where $\oplus$ is a graph operation function that produces the new belief graph $\mathcal{G}<em t-1="t-1">{t}$ given $\mathcal{G}</em>$. We formulate the graph operation function $\oplus$ using a recurrent neural network (e.g., a GRU [8]) as:}$ and $\Delta g_{t</p>
<p>$$
\begin{aligned}
\Delta g_{t} &amp;= f_{\Delta}(h_{\mathcal{G}<em O__t="O_{t">{t-1}}, h</em>); \
h_{t} &amp;= R N N(\Delta g_{t}, h_{t-1}); \
\mathcal{G}}}, h_{A_{t-1}<em d="d">{t} &amp;= f</em>).
\end{aligned}
$$}(h_{t</p>
<p>The function $f_{\Delta}$ aggregates the information in $\mathcal{G}<em t-1="t-1">{t-1}$, $A</em>}$, and $O_{t}$ to generate the graph update $\Delta g_{t}$. $h_{\mathcal{G<em t-1="t-1">{t-1}}$ denotes the representation of $\mathcal{G}</em>}$ from the graph encoder. $h_{O_{t}}$ and $h_{A_{t-1}}$ are outputs of the text encoder (refer to Figure 2, left part). The vector $h_{t}$ is a recurrent hidden state from which we decode the adjacency tensor $\mathcal{G<em t="t">{t}$; $h</em>$). We elaborate on each of the sub-modules in Appendix A.}$ acts as a memory that carries information across game steps—a crucial function for solving POMDPs [15]. The function $f_{d}$ is a multi-layer perceptron (MLP) that decodes the recurrent state $h_{t}$ into a real-valued adjacency tensor (i.e., the belief graph $\mathcal{G}_{t</p>
<p><strong>Training the Graph Updater:</strong> We pre-train the graph updater using two self-supervised training regimes to learn structured game dynamics. After pre-training, the graph updater is fixed during GATA's interaction with games; at this time it provides belief graphs $\mathcal{G}$ to the action selector. We train the action selector subsequently via RL. Both pre-training tasks share the same goal: to ensure that $\mathcal{G}_{t}$ encodes sufficient information about the environment state at game step $t$. For training data, we gather a collection of transitions by following walkthroughs in <em>FTWP</em> games. 6 To ensure variety in the training data, we also randomly sample trajectories off the optimal path. Next we describe our pre-training approaches for the graph updater.</p>
<ul>
<li><strong>Observation Generation (OG):</strong> Our first approach to pre-train the graph updater involves training a decoder model to reconstruct text observations from the belief graph. Conditioned on the belief graph, $\mathcal{G}<em t-1="t-1">{t}$, and the action performed at the previous game step, $A</em>}$, the observation generation task aims to reconstruct $O_{t} = {O_{t}^{1}, \ldots, O_{t}^{L_{O_{t}}}}$ token by token, where $L_{O_{t}}$ is the length of $O_{t}$. We formulate this task as a sequence-to-sequence (Seq2Seq) problem and use a transformer-based model [43] to generate the output sequence. Specifically, conditioned on $\mathcal{G<em t-1="t-1">{t}$ and $A</em>}$. We train the Seq2Seq model using teacher-forcing to optimize the negative log-likelihood loss:}$, the transformer decoder predicts the next token $O_{t}^{i}$ given ${O_{t}^{1}, \ldots, O_{t}^{i-1</li>
</ul>
<p>$$
\mathcal{L}<em i="1">{\text{OG}} = -\sum</em>}^{L_{O_{t}}} \log p_{\text{OG}}(O_{t}^{i} \mid O_{t}^{1}, \ldots, O_{t}^{i-1}, \mathcal{G<em t-1="t-1">{t}, A</em>),
$$</p>
<p>where $p_{\text{OG}}$ is the conditional distribution parametrized by the observation generation model.</p>
<ul>
<li><strong>Contrastive Observation Classification (COC):</strong> Inspired by the literature on contrastive representation learning [41, 19, 44, 7], we reformulate OG mentioned above as a contrastive prediction task. We use contrastive learning to maximize mutual information between the predicted $\mathcal{G}<em t="t">{t}$ and the text observations $O</em>}$. Specifically, we train the model to differentiate between representations corresponding to true observations $O_{t}$ and "corrupted" observations $\widetilde{O<em t="t">{t}$, conditioned on $\mathcal{G}</em>$. To obtain corrupted observations, we sample randomly from the set of all collected observations across our pre-training data. We use a noise-contrastive objective and minimize the binary cross-entropy (BCE) loss given by}$ and $A_{t-1</li>
</ul>
<p>$$
\mathcal{L}<em t="1">{\text{COC}} = \frac{1}{K} \sum</em>}^{K} \left( \mathbb{E<em O__t="O_{t">{O} \left[ \log \mathcal{D}(h</em>}}, h_{\mathcal{G<em _widetilde_O="\widetilde{O">{t}}) \right] + \mathbb{E}</em>}} \left[ \log (1 - \mathcal{D}(h_{\widetilde{O<em _mathcal_G="\mathcal{G">{t}}, h</em>)) \right] \right).
$$}_{t}</p>
<p><sup>6</sup>This is an independent and unique set of TextWorld games [39]. Details are provided in Appendix F.</p>
<p>Here, $K$ is the length of a trajectory as we sample a positive and negative pair at each step and $\mathcal{D}$ is a discriminator that differentiates between positive and negative samples. The motivation behind contrastive unsupervised training is that one does not require to train complex decoders. Specifically, compared to OG, the COC’s objective relaxes the need for learning syntactical or grammatical features and allows GATA to focus on learning the semantics of the $O_{t}$.</p>
<p>We provide further implementation level details on both these self-supervised objectives in Appendix B.</p>
<h3>3.3 Action Selector</h3>
<p>The graph updater discussed in the previous section defines a key component of GATA that enables the model to maintain a structured belief graph based on text observations. The second key component of GATA is the action selector, which uses the belief graph $\mathcal{G}<em t="t">{t}$ and the text observation $O</em>$ at each time-step to select an action. As shown in Figure 2, the action selector consists of four main components: the text encoder and graph encoder convert text inputs and graph inputs, respectively, into hidden representations; a representation aggregator fuses the two representations using an attention mechanism; and a scorer ranks all candidate actions based on the aggregated representations.</p>
<ul>
<li>Graph Encoder: GATA’s belief graphs, which estimate the true game state, are multi-relational by design. Therefore, we use relational graph convolutional networks (R-GCNs) [32] to encode the belief graphs from the updater into vector representations. We also adapt the R-GCN model to use embeddings of the available relation labels, so that we can capture semantic correspondences among relations (e.g., east_of and west_of are reciprocal relations). We do so by learning a vector representation for each relation in the vocabulary that we condition on the word embeddings of the relation’s name. We concatenate the resulting vector with the standard node embeddings during R-GCN’s message passing phase. Our R-GCN implementation uses basis regularization [32] and highway connections [36] between layers for faster convergence. Details are given in Appendix A.1.</li>
<li>Text Encoder: We adopt a transformer encoder [43] to convert text inputs from $O_{t}$ and $A_{t-1}$ into contextual vector representations. Details are provided in Appendix A.2.</li>
<li>Representation Aggregator: To combine the text and graph representations, GATA uses a bi-directional attention-based aggregator [49, 33]. Attention from text to graph enables the agent to focus more on nodes that are currently observable, which are generally more relevant; attention from nodes to text enables the agent to focus more on tokens that appear in the graph, which are therefore connected with the player in certain relations. Details are provided in Appendix A.3.</li>
<li>Scorer: The scorer consists of a self-attention layer cascaded with an MLP layer. First, the self-attention layer reinforces the dependency of every token-token pair and node-node pair in the aggregated representations. The resulting vectors are concatenated with the representations of action candidates $C_{t}$ (from the text encoder), after which the MLP generates a single scalar for every action candidate as a score. Details are provided in Appendix A.4.</li>
</ul>
<p>Training the Action Selector: We use Q-learning [45] to optimize the action selector on reward signals from the training games. Specifically, we use Double DQN [42] combined with multi-step learning [38] and prioritized experience replay [31]. To enable GATA to scale and generalize to multiple games, we adapt standard deep Q-Learning by sampling a new game from the set of training games to collect an episode. Consequently, the replay buffer contains transitions from episodes of different games. We provide further details on this training procedure in Appendix E.2.</p>
<h3>3.4 Variants Using Ground-Truth Graphs</h3>
<p>In GATA, the belief graph is learned entirely from text observations. However, the TextWorld API also provides access to the underlying graph states for games, in the format of discrete KGs. Thus, for comparison, we also consider two models that learn from or encode ground-truth graphs directly.</p>
<p>GATA-GTP: Pre-training a discrete graph updater using ground-truth graphs. We first consider a model that uses ground-truth graphs to pre-train the graph updater, in lieu of self-supervised methods. GATA-GTP uses ground-truth graphs from FTWP during pre-training, but infers belief graphs from the raw text during RL training of the action selector to compare fairly against GATA. Here, the belief graph $\mathcal{G}_{t}$ is a discrete multi-relational graph. To pre-train a discrete graph updater, we adapt the</p>
<p>command generation approach proposed by Zelinka et al. [53]. We provide details of this approach in Appendix C.</p>
<p>GATA-GTF: Training the action selector using ground-truth graphs. To get a sense of the upper bound on performance we might obtain using a belief graph, we also train an agent that uses the full ground-truth graph $\mathcal{G}^{\text {full }}$ during action selection. This agent requires no graph updater module; we simply feed the ground-truth graphs into the action selector (via the graph encoder). The use of ground-truth graphs allows GATA-GTF to escape the error cascades that may result from inferred belief graphs. Note also that the ground-truth graphs contain full state information, relaxing partial observability of the games. Consequently, we expect more effective reward optimization for GATAGTF compared to other graph-based agents. GATA-GTF's comparison with text-based agents is a sanity check for our hypothesis-that structured representations help learning general policies.</p>
<h1>4 Experiments and Analysis</h1>
<p>We conduct experiments on generated text-based games (Section 2) to answer two key questions: Q1: Does the belief-graph approach aid GATA in achieving high rewards on unseen games after training? In particular, does GATA improve performance compared to SOTA text-based models?
Q2: How does GATA compare to models that have access to ground-truth graph representations?</p>
<h3>4.1 Experimental Setup and Baselines</h3>
<p>We divide the games into four subsets with one difficulty level per subset. Each subset contains 100 training, 20 validation, and 20 test games, which are sampled from a distribution determined by their difficulty level. To elaborate on the diversity of games: for easier games, the recipe might only require a single ingredient and the world is limited to a single location, whereas harder games might require an agent to navigate a map of 6 locations to collect and appropriately process up to three ingredients. We also test GATA's transferability across difficulty levels by mixing the four difficulty levels to build level 5 . We sample 25 games from each of the four difficulty levels to build a training set. We use all validation and test games from levels 1 to 4 for level 5 validation and test. In all experiments, we select the top-performing agent on validation sets and report its test scores; all validation and test games are unseen in the training set. Statistics of the games are shown in Table 1.</p>
<p>Table 1: Games statistics (averaged across all games within a difficulty level).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: center;">Recipe Size</th>
<th style="text-align: center;">#Locations</th>
<th style="text-align: center;">Max Score</th>
<th style="text-align: center;">Need Cut</th>
<th style="text-align: center;">Need Cook</th>
<th style="text-align: center;">#Action Candidates</th>
<th style="text-align: center;">#Objects</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Mixture of levels $[1,2,3,4]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>As baselines, we use our implementation of LSTM-DQN [29] and LSTM-DRQN [50], both of which use only $O_{t}$ as input. Note that LSTM-DRQN uses an RNN to enable an implicit memory (i.e., belief); it also uses an episodic counting bonus to encourage exploration [50]. This draws an interesting comparison with GATA, wherein the belief is extracted and updated dynamically, in the form of a graph. For fair comparison, we replace the LSTM-based text encoders with a transformer-based text encoder as in GATA. We denote those agents as Tr-DQN and Tr-DRQN respectively. We denote a Tr-DRQN equipped with the episodic counting bonus as Tr-DRQN+. These three text-based baselines are representative of the current top-performing neural agents on text-based games.</p>
<p>Additionally, we test the variants of GATA that have access to ground-truth graphs (as described in Section 3.4). Comparing with GATA, the GATA-GTP agent also maintains its belief graphs throughout the game; however, its graph updater is pre-trained on FTWP using ground-truth graphsa stronger supervision signal. GATA-GTF, on the other hand, does not have a graph updater. It directly uses ground-truth graphs as input during game playing.</p>
<p>Table 2: Agents' normalized test scores and averaged relative improvement ( $\%$ †) over Tr-DQN across difficulty levels. An agent m's relative improvement over Tr-DQN is defined as $\left(\mathrm{R}<em _mathrm_Tr="\mathrm{Tr">{\mathrm{m}}-\right.$ $\left.\mathrm{R}</em>}-\mathrm{DQN}}\right) / \mathrm{R<em t="t">{\mathrm{Tr}-\mathrm{DQN}}$ where R is the score. All numbers are percentages. $\diamond$ represents ground-truth full graph; represents discrete $\mathcal{G}</em>$ generated by GATA, when the graph updater is pre-trained with OG and COC tasks, respectively.}$ generated by GATA-GTP; represents $O_{t}$. $\cdot$ and $\infty$ are continuous $G_{t</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">20 Training Games</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">100 Training Games</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Difficulty Level</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\%$ †</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\%$ †</td>
<td style="text-align: center;">$\%$ †</td>
</tr>
<tr>
<td style="text-align: center;">Agent</td>
<td style="text-align: center;">Text-based Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Tr-DQN</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Tr-DRQN</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">$+10.3$</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">$-2.6$</td>
<td style="text-align: center;">$+3.9$</td>
</tr>
<tr>
<td style="text-align: center;">Tr-DRQN+</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">$+10.7$</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">$-3.4$</td>
<td style="text-align: center;">$+3.6$</td>
</tr>
<tr>
<td style="text-align: center;">Input</td>
<td style="text-align: center;">GATA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\cdot$</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">$-0.2$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">$+16.1$</td>
<td style="text-align: center;">$+8.0$</td>
</tr>
<tr>
<td style="text-align: center;">$\cdot \star$</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">$+24.8$</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">$+16.1$</td>
<td style="text-align: center;">$+20.4$</td>
</tr>
<tr>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">$+27.1$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$+13.2$</td>
<td style="text-align: center;">$+20.2$</td>
</tr>
<tr>
<td style="text-align: center;">$\infty \star$</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">$+34.9$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$+13.6$</td>
<td style="text-align: center;">$+24.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GATA-GTP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\diamond$</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">$+16.6$</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">$-18.9$</td>
<td style="text-align: center;">$-1.2$</td>
</tr>
<tr>
<td style="text-align: center;">$\diamond \star$</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">$+24.6$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">$+5.2$</td>
<td style="text-align: center;">$+14.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GATA-GTF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\cdot$</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">$+64.2$</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">$+99.0$</td>
<td style="text-align: center;">$+81.6$</td>
</tr>
</tbody>
</table>
<h1>Q1: Performance of GATA compared to text-based baselines</h1>
<p>In Table 2, we show the normalized test scores achieved by agents trained on either 20 or 100 games for each difficulty level. Equipped with belief graphs, GATA significantly outperforms all text-based baselines. The graph updater pre-trained on both of the self-supervised tasks (Section 3.2) leads to better performance than the baselines ( $\cdot$ and $\infty$ ). We observe further improvements in GATA's policies when the text observations $(\boldsymbol{\Delta})$ are also available. We believe the text observations guide GATA's action scorer to focus on currently observable objects through the bi-attention mechanism. The attention may further help GATA to counteract accumulated errors from the belief graphs. In addition, we observe that Tr-DRQN and Tr-DRQN+ outperform Tr-DQN, with $3.9 \%$ and $3.6 \%$ relative improvement $(\% \uparrow)$. This suggests the implicit memory of the recurrent components improves performance. We also observe GATA substantially outperforms Tr-DQN when trained on 100 games, whereas the DRQN agents struggle to optimize rewards on the larger training sets.</p>
<h2>Q2: Performance of GATA compared to models with access to the ground-truth graph</h2>
<p>Table 2 also reports test performance for GATA-GTP ( $\boldsymbol{\Delta}$ ) and GATA-GTF ( $\boldsymbol{\Delta}$ ). Consistent with GATA, we find GATA-GTP also performs better when given text observations $(\boldsymbol{\Delta})$ as additional input to the action scorer. Although GATA-GTP outperforms Tr-DQN by $14.9 \%$ when text observations are available, its overall performance is still substantially poorer than GATA. Although the graph updater in GATA-GTP is trained with ground-truth graphs, we believe the discrete belief graphs and the discrete operations for updating them (Appendix C.1) make this approach vulnerable to an accumulation of errors over game steps, as well as errors introduced by the discrete nature of the predictions (e.g., round-off error). In contrast, we suspect that the continuous belief graph and the learned graph operation function (Eqn. 2) are easier to train and recover more gracefully from errors.
Meanwhile, GATA-GTF, which uses ground-truth graphs $\mathcal{G}^{\text {full }}$ during training and testing, obtains significantly higher scores than does GATA and all other baselines. Because $\mathcal{G}^{\text {full }}$ turns the game environment into a fully observable MDP and encodes accurate state information with no error accumulation, GATA-GTF represents the performance upper-bound of all the $\mathcal{G}_{t}$-based baselines. The scores achieved by GATA-GTF reinforce our intuition that belief graphs improve text-based game</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: Training curves on 20 level 2 games (averaged over 3 seeds). Right: Density comparison between a ground-truth graph (binary) and a belief graph G generated by the COC pre-training procedure. Both matrices are slices of adjacency tensors corresponding the is relation.</p>
<p>agents. At the same time, the performance gap between GATA and GATA-GTF invites investigation into better ways to learn accurate graph representations of text.</p>
<h3>Additional Results</h3>
<p>We also show the agents' training curves and examples of the belief graphs G generated by GATA. Figure 3 (<strong>Left</strong>) shows an example of all agents' training curves. We observe consistent trends with the testing results of Table 2 — GATA outperforms the text-based baselines and GATA-GTP, but a significant gap exists between GATA and GATA-GTF (which uses ground-truth graphs as input to the action scorer). Figure 3 (<strong>Right</strong>) highlights the sparsity of a ground-truth graph compared to that of a belief graph G. Since generation of G is unsupervised by any ground-truth graphs, we do not expect G to be interpretable nor sparse. Further, since the self-supervised models learn belief graphs directly from text, some of the learned features may correspond to the underlying grammar or other features useful for the self-supervised tasks, rather than only being indicative of relationships between objects. However, we show G encodes useful information for a relation prediction probing task in Appendix D.5.</p>
<p>Given space limitations, we only report a representative selection of our results in this section. Appendix D provides an exhaustive set of results including training curves, training scores, and test scores for all experimental settings introduced in this work. We also provide a detailed qualitative analysis including hi-res visualizations of the belief graphs. We encourage readers to refer to it.</p>
<h1>5 Related Work</h1>
<p><strong>Dynamic graph extraction:</strong> Numerous recent works have focused on constructing graphs to encode structured representations of raw data, for various tasks. Kipf et al. [23] propose contrastive methods to learn latent structured world models (C-SWMs) as state representations for vision-based environments. Their work, however, does not focus on learning policies to play games or to generalize across varying environments. Das et al. [10] leverage a machine reading comprehension mechanism to query for entities and states in short text passages and use a dynamic graph structure to track changing entity states. Fan et al. [12] propose to encode graph representations by linearizing the graph as an input sequence in NLP tasks. Johnson [21] construct graphs from text data using gated graph transformer neural networks. Yang et al. [46] learn transferable latent relational graphs from raw data in a self-supervised manner. Compared to the existing literature, our work aims to infer multi-relational KGs dynamically from partial text observations of the state and subsequently use these graphs to inform general policies. Concurrently, Srinivas et al. [35] propose to learn state representations with contrastive learning methods to facilitate RL training. However, they focus on vision-based environments and they do not investigate generalization.</p>
<p>More generally, we want to note that compared to traditional knowledge base construction (KBC) works, our approach is more related to the direction of neural relational inference [22]. In particular, we seek to generate task-specific graphs, which tend to be dynamic, contextual and relatively small, whereas traditional KBC focus on generating large, static graphs.</p>
<p><strong>Playing Text-based Games:</strong> Recent years have seen a host of work on playing text-based games. Various deep learning agents have been explored [29, 17, 14, 51, 20, 3, 52, 47]. Fulda et al. [13] use pre-trained embeddings to reduce the action space. Zahavy et al. [51], Seurin et al. [34], and</p>
<p>Jain et al. [20] explicitly condition an agent's decisions on game feedback. Most of this literature trains and tests on a single game without considering generalization. Urbanek et al. [40] use memory networks and ranking systems to tackle adventure-themed dialog tasks. Yuan et al. [50] propose a count-based memory to explore and generalize on simple unseen text-based games. Madotto et al. [26] use GoExplore [11] with imitation learning to generalize. Adolphs and Hofmann [1] and Yin and May [48] also investigate the multi-game setting. These methods rely either on reward shaping by heuristics, imitation learning, or rule-based features as inputs. We aim to minimize hand-crafting, so our action selector is optimized only using raw rewards from games while other components of our model are pre-trained on related data. Recently, Ammanabrolu and Riedl [4], Ammanabrolu and Hausknecht [3], Yin and May [48] leverage graph structure by using rule-based, untrained mechanisms to construct KGs to play text-based games.</p>
<h1>6 Conclusion</h1>
<p>In this work, we investigate how an RL agent can play and generalize within a distribution of textbased games using graph-structured representations inferred from text. We introduce GATA, a novel neural agent that infers and updates latent belief graphs as it plays text-based games. We use a combination of RL and self-supervised learning to teach the agent to encode essential dynamics of the environment in its belief graphs. We show that GATA achieves good test performance, outperforming a set of strong baselines including agents pre-trained with ground-truth graphs. This evinces the effectiveness of generating graph-structured representations for text-based games.</p>
<h2>7 Broader Impact</h2>
<p>Our work's immediate aim-improved performance on text-based games-might have limited consequences for society; however, taking a broader view of our work and where we'd like to take it forces us to consider several social and ethical concerns. We use text-based games as a proxy to model and study the interaction of machines with the human world, through language. Any system that interacts with the human world impacts it. As mentioned previously, an example of language-mediated, human-machine interaction is online customer service systems.</p>
<ul>
<li>In these systems, especially in products related to critical needs like healthcare, providing inaccurate information could result in serious harm to users. Likewise, failing to communicate clearly, sensibly, or convincingly might also cause harm. It could waste users' precious time and diminish their trust.</li>
<li>The responses generated by such systems must be inclusive and free of bias. They must not cause harm by the act of communication itself, nor by making decisions that disenfranchise certain user groups. Unfortunately, many data-driven, free-form language generation systems currently exhibit bias and/or produce problematic outputs.</li>
<li>Users' privacy is also a concern in this setting. Mechanisms must be put in place to protect it. Agents that interact with humans almost invariably train on human data; their function requires that they solicit, store, and act upon sensitive user information (especially in the healthcare scenario envisioned above). Therefore, privacy protections must be implemented throughout the agent development cycle, including data collection, training, and deployment.</li>
<li>Tasks that require human interaction through language are currently performed by people. As a result, advances in language-based agents may eventually displace or disrupt human jobs. This is a clear negative impact.</li>
</ul>
<p>Even more broadly, any systems that generate convincing natural language could be used to spread misinformation.</p>
<p>Our work is immediately aimed at improving the performance of RL agents in text-based games, in which agents must understand and act in the world through language. Our hope is that this work, by introducing graph-structured representations, endows language-based agents with greater accuracy and clarity, and the ability to make better decisions. Similarly, we expect that graph-structured representations could be used to constrain agent decisions and outputs, for improved safety. Finally, we believe that structured representations can improve neural agents' interpretability to researchers and users. This is an important future direction that can contribute to accountability and transparency</p>
<p>in AI. As we have outlined, however, this and future work must be undertaken with awareness of its hazards.</p>
<h1>8 Acknowledgements</h1>
<p>We thank Alessandro Sordoni and Devon Hjelm for the helpful discussions about the probing task. We also thank David Krueger, Devendra Singh Sachan, Harm van Seijen, Harshita Sahijwani, Jacob Miller, Koustuv Sinha, Loren Lugosch, Meng Qu, Travis LaCroix, and the anonymous ICML 2020 and NeurIPS 2020 reviewers and ACs for their insightful comments on an earlier draft of this work. The work was funded in part by an academic grant from Microsoft Research, an NSERC Discovery Grant RGPIN-2019-05123, an IVADO Fundamental Research Project Grant PRF-2019-3583139727, as well as Canada CIFAR Chairs in AI, held by Prof. Hamilton, Prof. Poupart and Prof. Tang.</p>
<h2>References</h2>
<p>[1] Adolphs, L. and Hofmann, T. (2019). Ledeepchef: Deep reinforcement learning agent for families of text-based games. CoRR, abs/1909.01646.
[2] Alain, G. and Bengio, Y. (2017). Understanding intermediate layers using linear classifier probes. ArXiv, abs/1610.01644.
[3] Ammanabrolu, P. and Hausknecht, M. (2020). Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations.
[4] Ammanabrolu, P. and Riedl, M. (2019). Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565, Minneapolis, Minnesota. Association for Computational Linguistics.
[5] Atkinson, T., Baier, H., Copplestone, T., Devlin, S., and Swan, J. (2018). The text-based adventure ai competition. IEEE Transactions on Games, 11:260-266.
[6] Ba, L. J., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. CoRR, abs/1607.06450.
[7] Bachman, P., Hjelm, R. D., and Buchwalter, W. (2019). Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pages $15509-15519$.
[8] Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar. Association for Computational Linguistics.
[9] Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Tao, R. Y., Hausknecht, M., Asri, L. E., Adada, M., Tay, W., and Trischler, A. (2018). Textworld: A learning environment for text-based games. CoRR, abs/1806.11532.
[10] Das, R., Munkhdalai, T., Yuan, X., Trischler, A., and McCallum, A. (2019). Building dynamic knowledge graphs from text using machine reading comprehension. In International Conference on Learning Representations.
[11] Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and Clune, J. (2019). Go-explore: a new approach for hard-exploration problems. ArXiv, abs/1901.10995.
[12] Fan, A., Gardent, C., Braud, C., and Bordes, A. (2019). Using local knowledge graph construction to scale Seq2Seq models to multi-document inputs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4186-4196, Hong Kong, China. Association for Computational Linguistics.</p>
<p>[13] Fulda, N., Ricks, D., Murdoch, B., and Wingate, D. (2017). What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045.
[14] Hausknecht, M., Ammanabrolu, P., Marc-Alexandre, C., and Yuan, X. (2020). Interactive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence.
[15] Hausknecht, M. and Stone, P. (2015). Deep recurrent q-learning for partially observable mdps. In AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents (AAAI-SDMIA15).
[16] Hausknecht, M. J., Loynd, R., Yang, G., Swaminathan, A., and Williams, J. D. (2019). Nail: A general interactive fiction agent. CoRR, abs/1902.04259.
[17] He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. (2016). Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany. Association for Computational Linguistics.
[18] Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence.
[19] Hjelm, D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. (2019). Learning deep representations by mutual information estimation and maximization. In ICLR 2019. ICLR.
[20] Jain, V., Fedus, W., Larochelle, H., Precup, D., and Bellemare, M. G. (2020). Algorithmic improvements for deep reinforcement learning applied to interactive fiction. In Thirty-Fourth AAAI Conference on Artificial Intelligence.
[21] Johnson, D. D. (2017). Learning graphical state transitions. In International Conference on Learning Representations (ICLR).
[22] Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. (2018). Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687.
[23] Kipf, T., van der Pol, E., and Welling, M. (2020). Contrastive learning of structured world models. In International Conference on Learning Representations.
[24] Lima, P. (2019). First textworld challenge - first place solution.
[25] Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. (2020). On the variance of the adaptive learning rate and beyond. In Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020).
[26] Madotto, A., Namazifar, M., Huizinga, J., Molino, P., Ecoffet, A., Zheng, H., Papangelis, A., Yu, D., Khatri, C., and Tur, G. (2020). Exploration based language learning for text-based games.
[27] Meng, R., Yuan, X., Wang, T., Brusilovsky, P., Trischler, A., and He, D. (2019). Does order matter? an empirical study on generating multiple keyphrases as a sequence. CoRR.
[28] Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and Joulin, A. (2018). Advances in pre-training distributed word representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).
[29] Narasimhan, K., Kulkarni, T., and Barzilay, R. (2015). Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Linguistics.
[30] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in pytorch. In NIPS-W.
[31] Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In International Conference on Learning Representations, Puerto Rico.</p>
<p>[32] Schlichtkrull, M., Kipf, T. N., Bloem, P., Van Den Berg, R., Titov, I., and Welling, M. (2018). Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593-607. Springer.
[33] Seo, M. J., Kembhavi, A., Farhadi, A., and Hajishirzi, H. (2017). Bidirectional attention flow for machine comprehension. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
[34] Seurin, M., Preux, P., and Pietquin, O. (2019). "i'm sorry dave, i'm afraid i can't do that" deep q-learning from forbidden action. CoRR, abs/1910.02078.
[35] Srinivas, A., Laskin, M., and Abbeel, P. (2020). Curl: Contrastive unsupervised representations for reinforcement learning. arXiv preprint arXiv:2004.04136.
[36] Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway networks. CoRR, abs/1505.00387.
[37] Sutton, R. (2019). The Bitter Lesson.
[38] Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9-44.
[39] Trischler, A., Côté, M.-A., and Lima, P. (2019). First TextWorld Problems, the competition: Using text-based games to advance capabilities of AI agents.
[40] Urbanek, J., Fan, A., Karamcheti, S., Jain, S., Humeau, S., Dinan, E., Rocktäschel, T., Kiela, D., Szlam, A., and Weston, J. (2019). Learning to speak and act in a fantasy text adventure game. CoRR, abs/1903.03094.
[41] van den Oord, A., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictive coding. CoRR, abs/1807.03748.
[42] van Hasselt, H., Guez, A., and Silver, D. (2015). Deep reinforcement learning with double q-learning. In AAAI.
[43] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems 30, pages 5998-6008. Curran Associates, Inc.
[44] Veličković, P., Fedus, W., Hamilton, W. L., Liò, P., Bengio, Y., and Hjelm, R. D. (2019). Deep Graph Infomax. In International Conference on Learning Representations.
[45] Watkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8(3):279-292.
[46] Yang, Z., Zhao, J., Dhingra, B., He, K., Cohen, W. W., Salakhutdinov, R. R., and LeCun, Y. (2018). Glomo: Unsupervised learning of transferable relational graphs. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31, pages 8950-8961. Curran Associates, Inc.
[47] Yin, X. and May, J. (2019a). Comprehensible context-driven text game playing. In 2019 IEEE Conference on Games (CoG), pages 1-8. IEEE.
[48] Yin, X. and May, J. (2019b). Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games. CoRR, abs/1908.04777.
[49] Yu, A. W., Dohan, D., Luong, M., Zhao, R., Chen, K., Norouzi, M., and Le, Q. V. (2018). Qanet: Combining local convolution with global self-attention for reading comprehension. In International Conference on Learning Representations.
[50] Yuan, X., Côté, M.-A., Sordoni, A., Laroche, R., Combes, R. T. d., Hausknecht, M., and Trischler, A. (2018). Counting to explore and generalize in text-based games. arXiv preprint arXiv:1806.11525.</p>
<p>[51] Zahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J., and Mannor, S. (2018). Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573.
[52] Zelinka, M. (2018). Baselines for reinforcement learning in text games. 2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI), pages 320-327.
[53] Zelinka, M., Yuan, X., Cote, M.-A., Laroche, R., and Trischler, A. (2019). Building dynamic knowledge graphs from text-based games. arXiv preprint arXiv:1910.09532.</p>
<h1>Contents in Appendices:</h1>
<ul>
<li>In Appendix A, we describe each of the components in GATA in detail.</li>
<li>In Appendix B, we provide detailed information on how we pre-train GATA's graph updater with the two proposed methods (i.e., OG and COC).</li>
<li>In Appendix C, we provide detailed information on GATA-GTP, the discrete version of GATA. Since the action scorer module is the same as in GATA, this appendix elaborates on how a discrete graph updater works and how to pre-train the discrete graph updater.</li>
<li>In Appendix D, we provide additional results and discussions. This includes training curves, training scores, testing scores, and high-res examples of the belief graphs learned by GATA. We provide a set of probing experiments to show that the belief graphs learned by GATA can capture useful information for relation classification tasks. We also provide qualitative analysis on GATA's OG task, which also suggests the belief graphs contain useful information for reconstructing the text observation $O_{t}$.</li>
<li>In Appendix E, we provide implementation details for all our experiments.</li>
<li>In Appendix F, we show examples of graphs in TextWorld games.</li>
</ul>
<h2>A Details of GATA</h2>
<h2>Notations</h2>
<p>In this section, we use $O_{t}$ to denote text observation at game step $t, C_{t}$ to denote a list of action candidate provided by a game, and $\mathcal{G}_{t}$ to denote a belief graph that represents GATA's belief to the state.
We use $L$ to refer to a linear transformation and $L^{f}$ means it is followed by a non-linear activation function $f$. Brackets $[\cdot ; \cdot]$ denote vector concatenation. Overall structure of GATA is shown in Figure 2.</p>
<h2>A. 1 Graph Encoder</h2>
<p>As briefly mentioned in Section 3.3, GATA utilizes a graph encoder which is based on R-GCN [32].
To better leverage information from relation labels, when computing each node's representation, we also condition it on a relation representation $E$ :</p>
<p>$$
\tilde{h}<em _in="\in" _mathcal_R="\mathcal{R" r="r">{i}=\sigma\left(\sum</em>}} \sum_{j \in \mathcal{N<em r="r">{i}^{r}} W</em>\right]\right)
$$}^{l}\left[h_{j}^{l} ; E_{r}\right]+W_{0}^{l}\left[h_{i}^{l} ; E_{r</p>
<p>in which, $l$ denotes the $l$-th layer of the R-GCN, $\mathcal{N}<em r="r">{i}^{r}$ denotes the set of neighbor indices of node $i$ under relation $r \in \mathcal{R}, \mathcal{R}$ indicates the set of different relations, $W</em>}^{l}$ and $W_{0}^{l}$ are trainable parameters. Since we use continuous graphs, $\mathcal{N<em r="r">{i}^{r}$ includes all nodes (including node $i$ itself). To stabilize the model and preventing from the potential explosion introduced by stacking R-GCNs with continuous graphs, we use Sigmoid function as $\sigma$ (in contrast with the commonly used ReLU function).
As the initial input $h^{0}$ to the graph encoder, we concatenate a node embedding vector and the averaged word embeddings of node names. Similarly, for each relation $r, E</em>$ is the concatenation of a relation embedding vector and the averaged word embeddings of $r$ 's label. Both node embedding and relation embedding vectors are randomly initialized and trainable.
To further help our graph encoder to learn with multiple layers of R-GCN, we add highway connections [36] between layers:</p>
<p>$$
\begin{aligned}
g &amp; =L^{\text {sigmoid }}\left(\tilde{h}<em i="i">{i}\right) \
h</em>}^{l+1} &amp; =g \odot \tilde{h<em i="i">{i}+(1-g) \odot h</em>
\end{aligned}
$$}^{l</p>
<p>where $\odot$ indicates element-wise multiplication.
We use a 6-layer graph encoder, with a hidden size $H$ of 64 in each layer. The node embedding size is 100 , relation embedding size is 32 . The number of bases we use is 3 .</p>
<h1>A. 2 Text Encoder</h1>
<p>We use a transformer-based text encoder, which consists of a word embedding layer and a transformer block [43]. Specifically, word embeddings are initialized by the 300-dimensional fastText [28] word vectors trained on Common Crawl ( 600 B tokens) and kept fixed during training in all settings.
The transformer block consists of a stack of 5 convolutional layers, a self-attention layer, and a 2-layer MLP with a ReLU non-linear activation function in between. In the block, each convolutional layer has 64 filters, each kernel's size is 5 . In the self-attention layer, we use a block hidden size $H$ of 64 , as well as a single head attention mechanism. Layernorm [6] is applied after each component inside the block. Following standard transformer training, we add positional encodings into each block's input.
We use the same text encoder to process text observation $O_{t}$ and the action candidate list $C_{t}$. The resulting representations are $h_{O_{t}} \in \mathbb{R}^{L_{O_{t}} \times H}$ and $h_{C_{t}} \in \mathbb{R}^{N_{C_{t}} \times L_{C_{t}} \times H}$, where $L_{O_{t}}$ is the number of tokens in $O_{t}, N_{C_{t}}$ denotes the number of action candidates provided, $L_{C_{t}}$ denotes the maximum number of tokens in $C_{t}$, and $H=64$ is the hidden size.</p>
<h2>A. 3 Representation Aggregator</h2>
<p>The representation aggregator aims to combine the text observation representations and graph representations together. Therefore this module is activated only when both the text observation $O_{t}$ and the graph input $\mathcal{G}<em _mathcal_G="\mathcal{G">{t}$ are provided. In cases where either of them is absent, for instance, when training the agent with only $\mathcal{G}^{\text {belief }}$ as input, the aggregator will be deactivated and the graph representation will be directly fed into the scorer.
For simplicity, we omit the subscript $t$ denoting game step in this subsection. At any game step, the graph encoder processes graph input $\mathcal{G}$, and generates the graph representation $h</em>$ denotes the number of tokens in $O$.
We adopt a standard representation aggregation method from question answering literature [49] to combine the two representations using attention mechanism.
Specifically, the aggregator first uses an MLP to convert both $h_{\mathcal{G}}$ and $h_{O}$ into the same space, the resulting tensors are denoted as $h_{\mathcal{G}}^{\prime} \in \mathbb{R}^{N_{\mathcal{G}} \times H}$ and $h_{O}^{\prime} \in \mathbb{R}^{L_{O} \times H}$. Then, a trilinear similarity function [33] is used to compute the similarities between each token in $h_{O}^{\prime}$ with each node in $h_{\mathcal{G}}^{\prime}$. The similarity between $i$ th token in $h_{O}^{\prime}$ and $j$ th node in $h_{\mathcal{G}}^{\prime}$ is thus computed by:}} \in \mathbb{R}^{N_{\mathcal{G}} \times H}$. The text encoder processes text observation $O$ to generate text representation $h_{O} \in \mathbb{R}^{L_{O} \times H} . N_{\mathcal{G}}$ denotes the number of nodes in the graph $\mathcal{G}, L_{O</p>
<p>$$
\operatorname{Sim}(i, j)=W\left(h_{O_{i}}^{\prime}, h_{\mathcal{G}<em O__i="O_{i">{j}}^{\prime}, h</em>\right)
$$}}^{\prime} \oplus h_{\mathcal{G}_{j}}^{\prime</p>
<p>where $W$ is trainable parameters in the trilinear function. By applying the above computation for each pair of $h_{O}^{\prime}$ and $h_{\mathcal{G}}^{\prime}$, a similarity matrix $S \in \mathbb{R}^{L_{O} \times N_{\mathcal{G}}}$ is resulted.
Softmax of the similarity matrix $S$ along both dimensions (number of nodes $N_{\mathcal{G}}$ and number of tokens $L_{O}$ ) are computed, producing $S_{\mathcal{G}}$ and $S_{O}$. The information contained in the two representations are then aggregated by:</p>
<p>$$
\begin{aligned}
h_{O \mathcal{G}} &amp; =\left[h_{O}^{\prime} ; P ; h_{O}^{\prime} \oplus P ; h_{O}^{\prime} \oplus Q\right] \
P &amp; =S_{\mathcal{G}} h_{\mathcal{G}}^{\prime \top} \
Q &amp; =S_{\mathcal{G}} S_{O}^{\top} h_{O}^{\prime \top}
\end{aligned}
$$</p>
<p>where $h_{O \mathcal{G}} \in \mathbb{R}^{L_{O} \times 4 H}$ is the aggregated observation representation, each token in text is represented by the weighted sum of graph representations. Similarly, the aggregated graph representation $h_{\mathcal{G} O} \in$ $\mathbb{R}^{N_{\mathcal{G}} \times 4 H}$ can also be obtained, where each node in the graph is represented by the weighted sum of text representations. Finally, a linear transformation projects the two aggregated representations to a space with size $H$ of 64 :</p>
<p>$$
\begin{aligned}
&amp; h_{\mathcal{G} O}=L\left(h_{\mathcal{G} O}\right) \
&amp; h_{O \mathcal{G}}=L\left(h_{O \mathcal{G}}\right)
\end{aligned}
$$</p>
<h1>A. 4 Scorer</h1>
<p>The scorer consists of a self-attention layer, a masked mean pooling layer, and a two-layer MLP. As shown in Figure 2 and described above, the input to the scorer is the action candidate representation $h_{C_{t}}$, and one of the following game state representation:</p>
<p>$$
s_{t}= \begin{cases}h_{\mathcal{G}<em O__t="O_{t">{t}} &amp; \text { if only graph input is available, } \ h</em>
$$}} &amp; \text { if only text observation is available, this degrades GATA to a Tr-DQN, } \ h_{\mathcal{G} O_{t}}, h_{O \mathcal{G}_{t}} &amp; \text { if both are available. }\end{cases</p>
<p>First, a self-attention is applied to the game state representation $s_{t}$, producing $\hat{s_{t}}$. If $s_{t}$ includes graph representations, this self-attention mechanism will reinforce the connection between each node and its related nodes. Similarly, if $s_{t}$ includes text representation, the self-attention mechanism strengthens the connection between each token and other related tokens. Further, masked mean pooling is applied to the self-attended state representation $\hat{s_{t}}$ and the action candidate representation $h_{C_{t}}$, this results in a state representation vector and a list of action candidate representation vectors. We then concatenate the resulting vectors and feed them into a 2-layer MLP with a ReLU non-linear activation function in between. The second MLP layer has an output dimension of 1, after squeezing the last dimension, the resulting vector is of size $N_{C_{t}}$, which is the number of action candidates provided at game step $t$. We use this vector as the score of each action candidate.</p>
<h2>A. 5 The $\mathrm{f}_{\Delta}$ Function</h2>
<p>As mentioned in Eqn. 2, $\mathrm{f}<em t-1="t-1">{\Delta}$ is an aggregator that combines information in $\mathcal{G}</em>$.
In specific, $\mathrm{f}_{\Delta}$ uses the same architecture as the representation aggregator described in Appendix A.3. Denoting the aggregator as a function Aggr:}, A_{t-1}$, and $O_{t}$ to generate the graph difference $\Delta g_{t</p>
<p>$$
h_{P Q}, h_{Q P}=\operatorname{Aggr}\left(h_{P}, h_{Q}\right)
$$</p>
<p>$\mathrm{f}<em O__t="O_{t">{\Delta}$ takes text observation representations $h</em>}} \in \mathbb{R}^{L_{O_{t}} \times H}$, belief graph representations $h_{\mathcal{G<em _mathcal_G="\mathcal{G">{t-1}} \in$ $\mathbb{R}^{N</em>$ is the number of nodes in the graph; $H$ is hidden size of the input representations.
We first aggregate $h_{O_{t}}$ with $h_{\mathcal{G}}} \times H}$, and action representations $h_{A_{t-1}} \in \mathbb{R}^{L_{A_{t-1}} \times H}$ as input. $L_{O_{t}}$ and $L_{A_{t-1}}$ are the number of tokens in $O_{t}$ and $A_{t-1}$, respectively; $N_{\mathcal{G}<em A__t-1="A_{t-1">{t-1}}$, then similarly $h</em>$ :}}$ with $h_{\mathcal{G}_{t-1}</p>
<p>$$
\begin{aligned}
&amp; h_{O \mathcal{G}}, h_{\mathcal{G} O}=\operatorname{Aggr}\left(h_{O_{t}}, h_{\mathcal{G}<em A="A" _mathcal_G="\mathcal{G">{t-1}}\right) \
&amp; h</em>\right)
\end{aligned}
$$}}, h_{\mathcal{G} A}=\operatorname{Aggr}\left(h_{A_{t-1}}, h_{\mathcal{G}_{t-1}</p>
<p>The output of $\mathrm{f}_{\Delta}$ is:</p>
<p>$$
\Delta g_{t}=\left[h_{O \mathcal{G}} ; h_{\mathcal{G} O} ; h_{A \mathcal{G}} ; h_{\mathcal{G} A}\right]
$$</p>
<p>where $\bar{X}$ is the masked mean of $X$ on the first dimension. The resulting concatenated vector $\Delta g_{t}$ has the size of $\mathbb{R}^{4 H}$.</p>
<h2>A. 6 The $\mathrm{f}_{\mathrm{d}}$ Function</h2>
<p>$\mathrm{f}<em t="t">{\mathrm{d}}$ is a decoder that maps a hidden graph representation $h</em>$.
Specifically, $\mathrm{f}_{\mathrm{d}}$ consists of a 2-layer MLP:} \in \mathbb{R}^{H}$ (generated by the RNN) into a continuous adjacency tensor $\mathcal{G} \in[-1,1]^{2 \mathcal{R} \times \mathcal{N} \times \mathcal{N}</p>
<p>$$
\begin{aligned}
&amp; h_{1}=L_{1}^{\mathrm{ReLU}}\left(h_{t}\right) \
&amp; h_{2}=L_{2}^{\text {tanh }}\left(h_{1}\right)
\end{aligned}
$$</p>
<p>In which, $h_{1} \in \mathbb{R}^{H}, h_{2} \in[-1,1]^{\mathcal{R} \times \mathcal{N} \times \mathcal{N}}$. To better facilitate the message passing process of R-GCNs used in GATA's graph encoder, we explicitly use the transposed $h_{2}$ to represent the inversed relations in the belief graph. Thus, we have $\mathcal{G}$ defined as:</p>
<p>$$
\mathcal{G}=\left[h_{2} ; h_{2}^{T}\right]
$$</p>
<p>The transpose is performed on the last two dimensions (both of size $\mathcal{N}$ ), the concatenation is performed on the dimension of relations.</p>
<p>The tanh activation function on top of the second layer of the MLP restricts the range of our belief graph $\mathcal{G}$ within $[-1,1]$. Empirically we find it helpful to keep the input of the multi-layer graph neural networks (the R-GCN graph encoder) in this range.</p>
<h1>B Details of Pre-training Graph Updater for GATA</h1>
<p>As briefly described in Section 3.2, we design two self-supervised tasks to pre-train the graph updater module of GATA. As training data, we gather a collection of transitions from the FTWP dataset. Here, we denote a transition as a 3-tuple $\left(O_{t-1}, A_{t-1}, O_{t}\right)$. Specifically, given text observation $O_{t-1}$, an action $A_{t-1}$ is issued; this leads to a new game state and $O_{t}$ is returned from the game engine. Since the graph updater is recurrent (we use an RNN as its graph operation function), the set of transitions are stored in the order they are collected.</p>
<h2>B. 1 Observation Generation (OG)</h2>
<p>As shown in Figure 4, given a transition $\left(O_{t-1}, A_{t-1}, O_{t}\right)$, we use the belief graph $\mathcal{G}<em t-1="t-1">{t}$ and $A</em>}$ to reconstruct $O_{t} . \mathcal{G<em t-1="t-1">{t}$ is generated by the graph updater, conditioned on the recurrent information $h</em>$ carried over from previous data point in the transition sequence.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Observation generation model.</p>
<h2>B.1.1 Observation Generator Layer</h2>
<p>The observation generator is a transformer-based decoder. It consists of a word embedding layer, a transformer block, and a projection layer.</p>
<p>Similar to the text encoder, the embedding layer is frozen after initializing with the pre-trained fastText [28] word embeddings. Inside the transformer block, there is one self attention layer, two attention layers and a 3-layer MLP with ReLU non-linear activation functions in between. Taking word embedding vectors and the two aggregated representations produced by the representation aggregator as input, the self-attention layer first generates a contextual encoding vectors for the words. These vectors are then fed into the two attention layers to compute attention with graph representations and text observation representations respectively. The two resulting vectors are thus concatenated, and they are fed into the 3-layer MLP. The block hidden size of this transformer is $H=64$.</p>
<p>Finally, the output of the transformer block is fed into the projection layer, which is a linear transformation with output size same as the vocabulary size. The resulting logits are then normalized by a softmax to generate a probability distribution over all words in vocabulary.</p>
<p>Following common practice, we also use a mask to prevent the decoder transformer to access "future" information during training.</p>
<h2>B. 2 Contrastive Observation Classification (COC)</h2>
<p>The contrastive observation classification task shares the same goal of ensuring the generated belief graph $\mathcal{G}<em t="t">{t}$ encodes the necessary information describing the environment state at step $t$. However, instead of generating $O</em>}$ from $\mathcal{G<em t="t">{t}$, it requires a model to differentiate the real $O</em>$ that are randomly sampled from other data points. In this task, the belief graph does not need to encode}$ from some $\tilde{O}_{t</p>
<p>the syntactical information as in the observation generation task, rather, a model can use its full capacity to learn the semantic information of the current environmental state.</p>
<p>We illustrate our contrastive observation classification model in Figure 5. This model shares most components with the previously introduced observation generation model, except replacing the observation generator module by a discriminator.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Contrastive observation classification model.</p>
<h1>B. 3 Reusing Graph Encoder in Action Scorer</h1>
<p>Both of the graph updater and action selector modules rely heavily on the graph encoder layer. It is natural to reuse the graph updater's graph encoder during the RL training of action selector. Specifically, we use the pre-trained graph encoder (and all its dependencies such as node embeddings and relation embeddings) from either the above model to initialize the graph encoder in action selector. In such settings, we fine-tune the graph encoders during RL training. In Appendix D, we compare GATA's performance between reusing the graph encoders with randomly initialize them.</p>
<h2>C GATA-GTP and Discrete Belief Graph</h2>
<p>As mentioned in Section 3.4, since the TextWorld API provides ground-truth (discrete) KGs that describe game states at each step, we provide an agent that utilizes this information, as a strong baseline to GATA. To accommodate the discrete nature of KGs provided by TextWorld, we propose GATA-GTP, which has the same action scorer with GATA, but equipped with a discrete graph updater. We show the overview structure of GATA-GTP in Figure 6.</p>
<h2>C. 1 Discrete Graph Updater</h2>
<p>In the discrete graph setting, we follow [53], updating $\mathcal{G}<em t-1="t-1">{t}$ with a set of discrete update operations that act on $\mathcal{G}</em>$ as a set of update operations, wherein each update operation is a sequence of tokens. We define the following two elementary operations so that any graph update can be achieved in $k \geq 0$ such operations:}$. In particular, we model the (discrete) $\Delta g_{t</p>
<ul>
<li>add(node1, node2, relation): add a directed edge, named relation, between node1 and node2.</li>
<li>delete(node1, node2, relation): delete a directed edge, named relation, between node1 and node2. If the edge does not exist, ignore this command.</li>
</ul>
<p>Given a new observation string $O_{t}$ and $\mathcal{G}_{t-1}$, the agent generates $k \geq 0$ such operations to merge the newly observed information into its belief graph.</p>
<p>Table 3: Update operations matching the transition in Figure 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;s&gt;</span><span class="w"> </span>add<span class="w"> </span>player<span class="w"> </span>shed<span class="w"> </span>at<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>shed<span class="w"> </span>backyard<span class="w"> </span>west_of<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>wooden<span class="w"> </span>door<span class="w"> </span>shed
east_of<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>toolbox<span class="w"> </span>shed<span class="w"> </span>in<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>toolbox<span class="w"> </span>closed<span class="w"> </span>is<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>workbench
shed<span class="w"> </span>in<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>delete<span class="w"> </span>player<span class="w"> </span>backyard<span class="w"> </span>at<span class="w"> </span><span class="nt">&lt;/s&gt;</span>
</code></pre></div>

<p>We formulate the update generation task as a sequence-to-sequence (Seq2Seq) problem and use a transformer-based model [43] to generate token sequences for the operations. We adopt the decoding strategy from [27], where given an observation sequence $O_{t}$ and a belief graph $\mathcal{G}_{t-1}$, the agent</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: GATA-GTP in detail. The coloring scheme is same as in Figure 1. The discrete graph updater first generates $\Delta g_{t}$ using $\mathcal{G}<em t="t">{t-1}$ and $O</em>}$. Afterwards the action selector uses $O_{t}$ and the updated graph $\mathcal{G<em t="t">{t}$ to select $A</em>$. Purple dotted line indicates a detached connection (i.e., no back-propagation through such connection).
generates a sequence of tokens that contains multiple graph update operations as subsequences, separated by a delimiter token &lt;|&gt;.
Since Seq2Seq set generation models are known to learn better with a consistent output ordering [27], we sort the ground-truth operations (e.g., always add before delete) for training. For the transition shown in Figure 1, the generated sequence is shown in Table 3.}$ from the list of action candidates $C_{t</p>
<h1>C. 2 Pre-training Discrete Graph Updater</h1>
<p>As described above, we frame the discrete graph updating behavior as a language generation task. We denote this task as command generation (CG). Similar to the continuous version of graph updater in GATA, we pre-train the discrete graph updater using transitions collected from the FTWP dataset. It is worth mentioning that despite requiring ground-truth KGs in FTWP dataset, GATA-GTP does not require any ground-truth graph in the RL game to train and evaluate the action scorer.
For training discrete graph updater, we use the $\mathcal{G}^{\text {seen }}$ type of graphs provided by the TextWorld API. Specifically, at game step $t, \mathcal{G}_{t}^{\text {seen }}$ is a discrete partial KG that contains information the agent has observed from the beginning until step $t$. It is only possible to train an agent to generate belief about the world it has seen and experienced.</p>
<p>In the collection FTWP transitions, every data point contains two consecutive graphs, we convert the difference between the graphs to ground-truth update operations (i.e., add and delete commands). We use standard teacher forcing technique to train the transformer-based Seq2Seq model. Specifically, conditioned on the output of representation aggregator, the command generator is required to predict the $k^{\text {th }}$ token of the target sequence given all the ground-truth tokens up to time step $k-1$. The command generator module is transformer-based decoder, similar to the observation generator described in Appendix B.1.1. Negative log-likelihood is used as loss function for optimization. An illustration of the command generation model is shown in Figure 7.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Command Generation Model.
During the RL training of action selector, the graph updater is detached without any back-propagation performed. It generates token-by-token started by a begin-of-sequence token, until it generates an end-of-sequence token, or hitting the maximum sequence length limit. The resulting tokens are consequently used to update the discrete belief graph.</p>
<h2>C. 3 Pre-training a Discrete Graph Encoder for Action Scorer</h2>
<p>In the discrete graph setting, we take advantage of the accessibility of the ground-truth graphs. Therefore we also consider various pre-training approaches to improve the performance of the graph encoder in the action selection module. Similar to the training of graph updater, we use transitions collected from the FTWP dataset as training data.</p>
<p>In particular, here we define a transition as a 6-tuple $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}\right.$. Specifically, given $\mathcal{G<em t-1="t-1">{t-1}$ and $O</em>}$, an action $A_{t-1}$ is selected from the candidate list $C_{t-1}$; this leads to a new game state $\mathcal{S<em t="t">{t}$, thus $\mathcal{G}</em>}$ and $O_{t}$ are returned. Note that $\mathcal{G<em t="t">{t}$ in transitions can either be $\mathcal{G}</em>$ that describes the part of state that the agent has experienced.}^{\text {full }}$ that describes the full environment state or $\mathcal{G}_{t}^{\text {new }</p>
<p>In this section, we start with providing details of the pre-training tasks and their corresponding models, and then show these models' performance for each of the tasks.</p>
<h1>C.3.1 Action Prediction (AP)</h1>
<p>Given a transition $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}, r_{t-1}\right)$, we use $A_{t-1}$ as positive example and use all other action candidates in $C_{t-1}$ as negative examples. A model is required to identify $A_{t-1}$ amongst all action candidates given two consecutive graphs $\mathcal{G<em t="t">{t-1}$ and $\mathcal{G}</em>$.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Action Prediction Model.
We use a model with similar structure and components as the action selector of GATA. As illustrated in Figure 8, the graph encoder first converts the two input graphs $\mathcal{G}<em t="t">{t-1}$ and $\mathcal{G}</em>$ and all negative examples) are fed into the text encoder to generate action candidate representations. The scorer thus takes these representations and the aggregated graph representations as input, and it outputs a ranking over all action candidates.}$ into hidden representations, the representation aggregator combines them using attention mechanism. The list of action candidates (which includes $A_{t-1</p>
<p>In order to achieve good performance in this setting, the bi-directional attention between $\mathcal{G}<em t="t">{t-1}$ and $\mathcal{G}</em>}$ in the representation aggregator needs to effectively determine the difference between the two sparse graphs. To achieve that, the graph encoder has to extract useful information since often the difference between $\mathcal{G<em t="t">{t-1}$ and $\mathcal{G}</em>$ is minute (e.g., before and after taking an apple from the table, the only change is the location of the apple).</p>
<h2>C.3.2 State Prediction (SP)</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: State Prediction Model.
Given a transition $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}, r_{t-1}\right)$, we use $\mathcal{G<em t-1="t-1">{t}$ as positive example and gather a set of game states by issuing all other actions in $C</em>}$ except $A_{t-1}$. We use the set of graphs representing the resulting game states as negative samples. In this task, a model is required to identify $\mathcal{G<em t="t">{t}$ amongst all graph candidates $G C</em>}$ given the previous graph $\mathcal{G<em t-1="t-1">{t-1}$ and the action taken $A</em>$.</p>
<p>As shown in Figure 9, a similar model is used to train both the SP and AP tasks.</p>
<h2>C.3.3 Deep Graph Infomax (DGI)</h2>
<p>This is inspired by Velickovic et al., [44]. Given a transition $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}, r_{t-1}\right)$, we map the graph $\mathcal{G<em t="t">{t}$ into its node embedding space. The node embedding vectors of $\mathcal{G}</em>$.}$ is denoted as $H$. We randomly shuffle some of the node embedding vectors to construct a "corrupted" version of the node representations, denoted as $\tilde{H</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Deep Graph Infomax Model.</p>
<p>Given node representations $H=\left{\overrightarrow{h_{1}}, \overrightarrow{h_{2}}, \ldots, \overrightarrow{h_{N}}\right}$ and corrupted representations of these nodes $\tilde{H}=\left{\overrightarrow{h_{1}}, \overrightarrow{h_{2}}, \ldots, \overrightarrow{h_{N}}\right}$, where $N$ is the number of vertices in the graph, a model is required to discriminate between the original and corrupted representations of nodes. As shown in Figure 10, the model is composed of a graph encoder and a discriminator. Specifically, following [44], we utilize a noise-contrastive objective with a binary cross-entropy (BCE) loss between the samples from the joint (positive examples) and the product of marginals (negative examples). To enable the discriminator to discriminate between $\mathcal{G}_{t}$ and the negative samples, the graph encoder must learn useful graph representations at both global and local level.</p>
<h1>C.3.4 Performance on Graph Encoder Pre-training Tasks</h1>
<p>We provide test performance of all the models described above for graph representation learning. We fine-tune the models on validation set and report their performance on test set.
Additionally, as mentioned in Section 3.3 and Appendix A, we adapt the original R-GCN to condition the graph representation on additional information contained by the relation labels. We show an ablation study for this in Table 4, where R-GCN denotes the original R-GCN [32] and R-GCN w/ R-Emb denotes our version that considers relation labels.</p>
<p>Note, as mentioned in previous sections, the dataset to train, valid and test these four pre-training tasks are extracted from the FTWP dataset. There exist unseen nodes (ingredients in recipe) in the validation and test sets of FTWP, it requires strong generalizability to get decent performance on these datasets.</p>
<p>From Table 4, we show the relation label representation significantly boosts the generalization performance on these datasets. Compared to AP and SP, where relation label information has significant effect, both models perform near perfectly on the DGI task. This suggests the corruption function we consider in this work is somewhat simple, we leave this for future exploration.</p>
<p>Table 4: Test performance of models on all pre-training tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Graph Type</th>
<th style="text-align: center;">R-GCN</th>
<th style="text-align: center;">R-GCN w/ R-Emb</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 1}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 3}$</td>
</tr>
<tr>
<td style="text-align: center;">SP</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">$\mathbf{0 . 9 7 1}$</td>
</tr>
<tr>
<td style="text-align: center;">DGI</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
</tr>
</tbody>
</table>
<h2>D Additional Results and Discussions</h2>
<h2>D. 1 Training Curves</h2>
<p>We report the training curves of all our mentioned experiment settings. Figure 11 shows the GATA's training curves. Figure 12 shows the training curves of the three text-based baseline (Tr-DQN, Tr-DRQN, Tr-DRQN+). Figure 13 shows the training curve of GATA-GTF (no graph updater, the action scorer takes ground-truth graphs as input) and GATA-GTP (graph updater is trained using ground-truth graphs from the FTWP dataset, the trained graph updater maintains a discrete belief graph throughout the RL training).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The graph updater and action selector share some structures but not their parameters (unless specified).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
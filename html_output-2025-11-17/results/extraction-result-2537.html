<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2537 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2537</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2537</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-2b6fe3e433707b5521ed2a50274c27ea8750b40f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b6fe3e433707b5521ed2a50274c27ea8750b40f" target="_blank">SpecGen: Automated Generation of Formal Program Specifications via Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Software Engineering</p>
                <p><strong>Paper TL;DR:</strong> Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon.</p>
                <p><strong>Paper Abstract:</strong> In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2537.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2537.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpecGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpecGen: Automated Generation of Formal Program Specifications via Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system that leverages a large language model (gpt-3.5-turbo) together with a JML verifier, mutation operators, and a heuristic selector to generate verifiable formal program specifications for Java programs via an iterative conversation + mutation workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SpecGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SpecGen is a two-phase system for automatic formal specification generation. Phase 1 (conversation-driven generation) queries an LLM with an initial prompt (system role + few-shot examples + program) to produce JML specifications; verification failures reported by a verifier (OpenJML) are fed back as natural-language guidance into subsequent LLM prompts in a multi-turn conversation (up to 10 rounds). Phase 2 (mutation-based generation) takes LLM-produced specifications that failed verification, exhaustively mutates operator types (predicative, logical, comparative, arithmetic) to produce variant candidates, and uses a heuristic selector (weighted scoring over mutation types) to iteratively choose mutated candidates to present to the verifier until a verified set is obtained or no candidates remain.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>4 (interpreted modular components: LLM generator, verifier, mutator, selector)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>LLM generator agent: generates candidate JML specifications from program source using few-shot prompts and conversational feedback; Verifier agent: OpenJML checks candidate specifications and returns one verification failure message per attempt; Mutator agent: applies four mutation operator classes (predicative, logical, comparative, arithmetic) to failing template specifications to produce variant families; Selector agent: heuristic selection component that scores mutated candidates based on counts of applied mutation types and predefined weights and selects variants to re-submit to the verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Specification synthesis and validation (idea/description generation => implementation of specs as JML annotations => execution of formal verification => iterative refinement via feedback and mutation). In other terms: generation, verification (evaluation), and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Sequential, hybrid pipeline with iterative loops: a centralized orchestration where the LLM and verifier are engaged in a conversational loop (generator -> verifier -> feedback -> generator), and, if conversation fails, the pipeline switches to a mutation-selection loop where the selector coordinates replacement of refuted specs from mutated families and re-invokes the verifier until convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language prompt/response API exchanges between the orchestrator and the LLM (OpenAI API to gpt-3.5-turbo-1106) with structured prompt components (system role, few-shot examples, program, and inserted verifier error guidance). Verifier-to-orchestrator communication is via verifier error messages (configured to report a single failure message), which are then converted into natural-language guidance inserted into subsequent LLM prompts. The mutation and selector components operate on internal structured representations of JML specifications (program text / spec templates) and pass candidate spec text to the verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Iterative verification feedback loop: the verifier returns a single verification failure message which is incorporated into the next prompt as guidance for the LLM (conversation-driven refinement). In mutation mode, verifier refutations identify refuted specifications; the selector replaces refuted specs with mutated variants from the same family and re-submits the selected set to the verifier (iterative replacement until all selected specs verify or no candidates remain).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand and iterative: LLM <-> verifier communication occurs after each generation attempt (conversation rounds up to 10). In mutation mode, selection and verifier calls iterate repeatedly: replace refuted specs and re-run verifier until convergence. Typical max conversation rounds = 10; mutation-selection runs until E_refuted is empty or candidates exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software engineering / program verification: automatic generation of formal JML specifications (pre-conditions, post-conditions, loop invariants) for Java programs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Number of programs with verified specifications: 279 / 385 total across two datasets (SV-COMP Java category + SpecGenBench). On SV-COMP (265 programs): SpecGen verified 179 programs. On SpecGenBench (120 programs): SpecGen verified 100? (implied from totals; overall total 279). Defects4J (50 real-world files): SpecGen handled 38 / 50 with average success probability 55.20%. User-study semantic quality (Likert 1–5): SpecGen average = 4.54 vs oracle 4.83 and Houdini/Daikon 2.32. Efficiency: average verifier calls per successful run (total): heuristic selection 15.51 vs random 18.44; SV-COMP: heuristic 8.91 vs random 9.41 (5.30% improvement); SpecGenBench: heuristic 28.51 vs random 36.20 (21.23% improvement). Ablation of mutation types: disabling comparative mutation reduced total handled programs to 223 (most important); disabling logical reduced to 235; disabling predicative or arithmetic reduced less (259 and 262 respectively). Overall success: SpecGen 279 vs AutoSpec 247 vs Houdini 98 (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to multiple baselines: Houdini (template-based) and Daikon (dynamic), and LLM-based methods (few-shot, conversational, AutoSpec). SpecGen outperforms all baselines in number of programs with verified specifications (279 / 385), beating AutoSpec (247) and Houdini (98). User ratings show SpecGen (4.54) close to oracle (4.83) and substantially above Houdini/Daikon (2.32).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Iterative generator-verifier feedback improves correctness of LLM outputs (conversation-driven phase increases number of verified programs versus few-shot LLM); mutation + selector increases coverage for hard cases where LLM alone fails. Quantified benefits: overall higher number of verified programs (279) and higher semantic quality rating (4.54); heuristic selector reduces verifier calls by 5.3% (SV-COMP) and 21.2% (SpecGenBench) compared to random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>LLM sometimes produces near-correct but unverifiable specs requiring exhaustive mutation exploration; verifier limitations (OpenJML incompleteness, timeouts) can falsely refute correct specs; large candidate space from exhaustive mutation imposes verification cost (necessitating heuristic selection); conversation feedback can be limited by abstract/generalized verifier error messages which LLMs may struggle to interpret for complex programs. Timeouts: single verification timeout set to 30 minutes to mitigate non-response.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes. Ablation on mutation types shows comparative mutation is most critical (SpecGen w/o comparative: 223 handled programs vs full SpecGen 279), logical mutation second-most important (235), predicative and arithmetic less (259 and 262). Also selection strategy ablation: heuristic selection vs random shows fewer verifier calls (15.51 vs 18.44 overall) and faster convergence, especially on loop-heavy programs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Empirically chosen settings reported: LLM model gpt-3.5-turbo-1106; temperature = 0.4; 4 few-shot examples in the initial prompt; max conversation rounds = 10; mutation weights set to comparative = -1, logical = -2, arithmetic = -4, predicative = -4 (negative weights to prefer candidates with fewer mutations). These settings were selected to balance diversity, accuracy, and verification efficiency per the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SpecGen: Automated Generation of Formal Program Specifications via Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conversational automated program repair <em>(Rating: 2)</em></li>
                <li>Enchanting program specification synthesis by large language models using static analysis and program verification <em>(Rating: 2)</em></li>
                <li>Ranking llm-generated loop invariants for program verification <em>(Rating: 2)</em></li>
                <li>Finding inductive loop invariants using large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2537",
    "paper_id": "paper-2b6fe3e433707b5521ed2a50274c27ea8750b40f",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "SpecGen",
            "name_full": "SpecGen: Automated Generation of Formal Program Specifications via Large Language Models",
            "brief_description": "An automated system that leverages a large language model (gpt-3.5-turbo) together with a JML verifier, mutation operators, and a heuristic selector to generate verifiable formal program specifications for Java programs via an iterative conversation + mutation workflow.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SpecGen",
            "system_description": "SpecGen is a two-phase system for automatic formal specification generation. Phase 1 (conversation-driven generation) queries an LLM with an initial prompt (system role + few-shot examples + program) to produce JML specifications; verification failures reported by a verifier (OpenJML) are fed back as natural-language guidance into subsequent LLM prompts in a multi-turn conversation (up to 10 rounds). Phase 2 (mutation-based generation) takes LLM-produced specifications that failed verification, exhaustively mutates operator types (predicative, logical, comparative, arithmetic) to produce variant candidates, and uses a heuristic selector (weighted scoring over mutation types) to iteratively choose mutated candidates to present to the verifier until a verified set is obtained or no candidates remain.",
            "number_of_agents": "4 (interpreted modular components: LLM generator, verifier, mutator, selector)",
            "agent_specializations": "LLM generator agent: generates candidate JML specifications from program source using few-shot prompts and conversational feedback; Verifier agent: OpenJML checks candidate specifications and returns one verification failure message per attempt; Mutator agent: applies four mutation operator classes (predicative, logical, comparative, arithmetic) to failing template specifications to produce variant families; Selector agent: heuristic selection component that scores mutated candidates based on counts of applied mutation types and predefined weights and selects variants to re-submit to the verifier.",
            "research_phases_covered": "Specification synthesis and validation (idea/description generation =&gt; implementation of specs as JML annotations =&gt; execution of formal verification =&gt; iterative refinement via feedback and mutation). In other terms: generation, verification (evaluation), and iterative refinement.",
            "coordination_mechanism": "Sequential, hybrid pipeline with iterative loops: a centralized orchestration where the LLM and verifier are engaged in a conversational loop (generator -&gt; verifier -&gt; feedback -&gt; generator), and, if conversation fails, the pipeline switches to a mutation-selection loop where the selector coordinates replacement of refuted specs from mutated families and re-invokes the verifier until convergence.",
            "communication_protocol": "Natural-language prompt/response API exchanges between the orchestrator and the LLM (OpenAI API to gpt-3.5-turbo-1106) with structured prompt components (system role, few-shot examples, program, and inserted verifier error guidance). Verifier-to-orchestrator communication is via verifier error messages (configured to report a single failure message), which are then converted into natural-language guidance inserted into subsequent LLM prompts. The mutation and selector components operate on internal structured representations of JML specifications (program text / spec templates) and pass candidate spec text to the verifier.",
            "feedback_mechanism": "Iterative verification feedback loop: the verifier returns a single verification failure message which is incorporated into the next prompt as guidance for the LLM (conversation-driven refinement). In mutation mode, verifier refutations identify refuted specifications; the selector replaces refuted specs with mutated variants from the same family and re-submits the selected set to the verifier (iterative replacement until all selected specs verify or no candidates remain).",
            "communication_frequency": "On-demand and iterative: LLM &lt;-&gt; verifier communication occurs after each generation attempt (conversation rounds up to 10). In mutation mode, selection and verifier calls iterate repeatedly: replace refuted specs and re-run verifier until convergence. Typical max conversation rounds = 10; mutation-selection runs until E_refuted is empty or candidates exhausted.",
            "task_domain": "Software engineering / program verification: automatic generation of formal JML specifications (pre-conditions, post-conditions, loop invariants) for Java programs.",
            "performance_metrics": "Number of programs with verified specifications: 279 / 385 total across two datasets (SV-COMP Java category + SpecGenBench). On SV-COMP (265 programs): SpecGen verified 179 programs. On SpecGenBench (120 programs): SpecGen verified 100? (implied from totals; overall total 279). Defects4J (50 real-world files): SpecGen handled 38 / 50 with average success probability 55.20%. User-study semantic quality (Likert 1–5): SpecGen average = 4.54 vs oracle 4.83 and Houdini/Daikon 2.32. Efficiency: average verifier calls per successful run (total): heuristic selection 15.51 vs random 18.44; SV-COMP: heuristic 8.91 vs random 9.41 (5.30% improvement); SpecGenBench: heuristic 28.51 vs random 36.20 (21.23% improvement). Ablation of mutation types: disabling comparative mutation reduced total handled programs to 223 (most important); disabling logical reduced to 235; disabling predicative or arithmetic reduced less (259 and 262 respectively). Overall success: SpecGen 279 vs AutoSpec 247 vs Houdini 98 (as reported).",
            "baseline_comparison": "Compared to multiple baselines: Houdini (template-based) and Daikon (dynamic), and LLM-based methods (few-shot, conversational, AutoSpec). SpecGen outperforms all baselines in number of programs with verified specifications (279 / 385), beating AutoSpec (247) and Houdini (98). User ratings show SpecGen (4.54) close to oracle (4.83) and substantially above Houdini/Daikon (2.32).",
            "coordination_benefits": "Iterative generator-verifier feedback improves correctness of LLM outputs (conversation-driven phase increases number of verified programs versus few-shot LLM); mutation + selector increases coverage for hard cases where LLM alone fails. Quantified benefits: overall higher number of verified programs (279) and higher semantic quality rating (4.54); heuristic selector reduces verifier calls by 5.3% (SV-COMP) and 21.2% (SpecGenBench) compared to random selection.",
            "coordination_challenges": "LLM sometimes produces near-correct but unverifiable specs requiring exhaustive mutation exploration; verifier limitations (OpenJML incompleteness, timeouts) can falsely refute correct specs; large candidate space from exhaustive mutation imposes verification cost (necessitating heuristic selection); conversation feedback can be limited by abstract/generalized verifier error messages which LLMs may struggle to interpret for complex programs. Timeouts: single verification timeout set to 30 minutes to mitigate non-response.",
            "ablation_studies": "Yes. Ablation on mutation types shows comparative mutation is most critical (SpecGen w/o comparative: 223 handled programs vs full SpecGen 279), logical mutation second-most important (235), predicative and arithmetic less (259 and 262). Also selection strategy ablation: heuristic selection vs random shows fewer verifier calls (15.51 vs 18.44 overall) and faster convergence, especially on loop-heavy programs.",
            "optimal_configurations": "Empirically chosen settings reported: LLM model gpt-3.5-turbo-1106; temperature = 0.4; 4 few-shot examples in the initial prompt; max conversation rounds = 10; mutation weights set to comparative = -1, logical = -2, arithmetic = -4, predicative = -4 (negative weights to prefer candidates with fewer mutations). These settings were selected to balance diversity, accuracy, and verification efficiency per the paper's experiments.",
            "uuid": "e2537.0",
            "source_info": {
                "paper_title": "SpecGen: Automated Generation of Formal Program Specifications via Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conversational automated program repair",
            "rating": 2
        },
        {
            "paper_title": "Enchanting program specification synthesis by large language models using static analysis and program verification",
            "rating": 2
        },
        {
            "paper_title": "Ranking llm-generated loop invariants for program verification",
            "rating": 2
        },
        {
            "paper_title": "Finding inductive loop invariants using large language models",
            "rating": 2
        }
    ],
    "cost": 0.01163825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SpecGen: Automated Generation of Formal Program Specifications via Large Language Models</h1>
<p>Lezhi $\mathrm{Ma}^{1}$, Shangqing Liu ${ }^{1 \dagger}$, Yi $\mathrm{Li}^{2}$, Xiaofei Xie ${ }^{3}$, and Lei $\mathrm{Bu}^{1 \dagger}$<br>${ }^{1}$ State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China<br>${ }^{2}$ Nanyang Technological University, Singapore<br>${ }^{3}$ Singapore Management University, Singapore<br>lezhima@hotmail.com, shangqingliu666@gmail.com, yi_li@ntu.edu.sg, xfxie@smu.edu.sg, bulei@nju.edu.cn</p>
<h4>Abstract</h4>
<p>In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs.</p>
<p>To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.</p>
<p>Index Terms-program verification, specification inference, large language model</p>
<h2>I. INTRODUCTION</h2>
<p>Formal specifications play a central role in describing, understanding, and reasoning about program behaviors. They capture the intended or actual program behaviors, in terms of formal languages, with precise semantics. Formal specifications may take various forms, such as procedure pre-/postconditions, loop invariants, and assertions at specific program locations. They are essential in a variety of software quality assurance tasks, including software testing [1, 2], model checking $[3,4]$, and program verification $[5,6]$.
${ }^{\dagger}$ Corresponding author.</p>
<p>Yet, a practical challenge is the absence of documented formal specifications in most real-world software projects, since manually writing high-quality specifications is highly nontrivial. To alleviate the burden on software developers, several tools have been introduced for generating program specifications automatically [7, 8, 9], including Houdini [7] and Daikon [9], two most representative ones for Java programs. However, these tools rely heavily on predefined templates or grammars during the specification generation process. As claimed by Molina et al. [10], the fixed templates involved result in a limited range of specifications covered, usually yielding overly simplistic specifications that struggle to capture the complex behaviors and functionalities of real-world programs accurately. This phenomenon poses non-negligible limitations for these tools, consequently hindering their applications in the actual software development process [11, 12, 13].</p>
<p>To address this challenge, we introduce SpecGen, an automated technique for Java program specification generation based on the Large Language Models (LLMs). With the rise of LLMs, extensive research has attempted to apply them in software engineering and LLMs exhibit outstanding performance in various tasks [14, 15, 16, 17, 18], where LLMs have demonstrated remarkable capabilities on code comprehension and summarization [19]. Inspired by this insight, we believe that LLMs can serve as a potent solution to overcome the limitations of existing program specification generation methods. The core idea of this work is to leverage LLMs to generate specifications that accurately capture the real behaviors of input programs, thus imbuing these specifications with richer semantics for further practical use.</p>
<p>The workflow of SpecGen comes in two phases. In the first phase, conversation-driven specification generation, we aim to query the output specifications by conducting a conversation with the LLM. To start the conversation, a prompt is constructed with several few-shot examples for the initial query. During the conversation process, we utilize the verification failure information from the specification verifier as the feedback prompt for the next round of the conversation. In this way, LLMs receive more cues, facilitating them to better generate accurate specifications. Nevertheless, despite the powerful code understanding and generation capabilities of large language models, they still struggle to handle complex programs effectively i.e., generating accurate specifications for complex programs. Through our repeated observation and</p>
<p>testing of the model-generated results, we found that although the generated content is not highly accurate, it is already very close to the oracle, which motivates us to design the second phase, mutation-based specification generation. It focuses on generating accurate specifications where the LLM fails to provide verifiable results. Specifically, given a verification failure result by the LLM, SpecGen endeavors to combine four different kinds of mutation operators to modify it and obtain all potential variants. A selector adopting a heuristic selection strategy by assigning different weights of variants further repeatedly chooses a subset of these mutated variants deemed most likely to pass the verification until the results are successfully verified.</p>
<p>To evaluate SpecGen, we conduct experiments on two datasets. We first evaluate SpecGen on the benchmark for the Java category of SV-COMP [20]. To further evaluate the performance of SpecGen on different kinds of programs, we constructed another dataset containing 120 Java programs with manually written ground-truth specifications. The selected programs are highly representative, encompassing different control-flow structures and various data structures to avoid any bias in our evaluation. We compared the performance of SpecGen on the dataset against multiple baselines. The results of our evaluation demonstrate that SpecGen significantly outperforms the baseline methods. SpecGen successfully generated verifiable specifications for 279 out of the total 385 programs, outweighing 247 for AutoSpec [21], the best-performing LLM-based approach, and 98 for Houdini, the best-performing non-LLM method. An ablation study on mutations was also conducted, proving the effectiveness of all four types of mutation operators. Additionally, the results of evaluations on the heuristic selection strategy suggested that our strategy effectively improves the efficiency of SpecGen compared to the random selection strategy. Furthermore, a user study was conducted to evaluate the semantic quality of the generated specifications, illustrating the ability of SpecGen to accurately and comprehensively characterize program behaviors. The main contributions are summarized as follows:</p>
<ul>
<li>A novel approach for formal program specification generation and corresponding prototype tool [22], leveraging the Large Language Models to generate accurate and comprehensive specifications to describe program behaviors. Benefiting from the code comprehension ability of LLMs, our approach is capable of generating specifications with high quality, overcoming the limitations of existing methods in generating simplistic and basic specifications.</li>
<li>A mutation-based generation approach to enrich the diversity of the LLM output, consisting of a set of mutation operators and a novel heuristic selection strategy proposed to improve the efficiency of the verification that existing works fail to consider.</li>
<li>A dataset named SpecGenBench, with hand-written specifications by experts, facilitating follow-up research. Other than the established benchmark SV-COMP, we collected programs on a more diverse spectrum for deeper insights.</li>
<li>A comprehensive evaluation to evaluate our approach in all aspects. We compare SpecGen against Purely LLM-based approaches and representative non-LLM approaches. SpecGen succeeds in 279 out of the 385 programs, significantly outperforming the baseline approaches.</li>
</ul>
<h2>II. BACKGROUND AND MOTIVATION</h2>
<h2>A. Specification Generation and Verification</h2>
<p>Program specifications encompass precise statements that describe the intended or actual behaviors of a particular program, either in its entirety or in distinct parts. In this work, we focus on generating specifications for the actual behaviors of input programs. A large proportion of program specifications are expressed in formal languages, such as mathematical expressions to describe the constraints on the behaviors of a program. There are different kinds of specifications such as pre-conditions, which establish constraints on function parameters, ensuring proper execution of the function, post-conditions, which delineate the properties of a set of variables that persist after a function is executed, and loop invariants, which represent a specialized form of specification, detailing properties that consistently hold before executing the loop body. For different programming languages, the specifications may have different implementation forms. For example, in Java, the specifications can be expressed in Java Modeling Language (i.e., JML) [23] where requires statements denote the pre-conditions of a function, ensures statements represent the post-conditions of a function, and maintaining statements specify the loop invariants.</p>
<p>A series of automated program specification generation tools have been developed $[7,8,9,24,25]$ to reduce the burden on software developers. Two representative works are Houdini [7] and Daikon [9]. Both rely on templates defined by human experts to generate a massive amount of candidate specifications, which verifiers then filter to eliminate incorrect candidate specifications until the remaining candidates are successfully verified. In particular, the template usually involves two or three variables and their corresponding operators i.e., <var> <op> <var>, where <var> should be filled in with variable names and <op> should be an operator. For example, if a function contains two integer parameters x and y and an integer return value, Houdini may generate candidate preconditions and post-conditions for this function in the form of $\mathrm{x}&gt;\mathrm{y}, \mathrm{x}&lt;\mathrm{y}, \backslash$ result $&gt;=0, \backslash$ result $&lt;=0$, etc. where \result is the defined variable denoting the return value in JML. For a program, all available variables within the scope such as class members, function parameters, and return values are taken to generate candidate specifications based on the defined templates and instrumented into corresponding points of the input program to verify the correctness of the specifications. The main difference between Houdini and Daikon lies in the design of the verifier where Houdini adopts a JML specification verifier, OpenJML [26], which is designed from constraint solving [27] for verifying. Yet, Daikon is based on the runtime checking which compares each candidate specification with the runtime execution traces.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: An example program and corresponding specifications generated by SpecGen, for which existing tools cannot generate comprehensive specifications to describe the program behaviors.</p>
<h3><em>B. Motivation</em></h3>
<p>The existing automated program specification generation tools have limitations hindering real-world deployment and application. They rely on templates defined by human experts to generate specifications, which results in simple and trivial specifications. We present an example program on the left of Figure 1 for illustration. This program aims to search in the given integer array for the indexes of two separated elements of which the sum is exactly the given target value, and is implemented in two nested loops. If there do not exist such elements in the array, the program returns an empty array. To fully articulate the behaviors of the program, such properties must be specified, where Daikon and Houdini fail. In particular, both Houdini and Daikon can only generate trivial post-conditions such as <code>nums != null</code> and <code>\result[i] &gt;= 0</code> for the method <code>TwoSum()</code> as a whole. As for the outer-layer loop, only some simple loop invariants are generated, describing trivial numerical relationships between variables, such as <code>i &gt;= 0</code> and <code>i &lt; arr.length</code>. For the inner-layer loop, the generated specifications are <code>j &gt;= 1</code>, <code>i &lt; j</code> and <code>j &lt; nums.length</code>, which are similar to the out-layer. The generated specifications are too trivial, without detailed information to accurately capture the program's functionality.</p>
<p>Recently, large language models (i.e., LLMs) [28, 29] have exhibited powerful capacities in coding [18, 19]. The emergence of these models may greatly compensate for the limitations of traditional software analysis tools in code understanding. A significant amount of work attempts to leverage large language models in software engineering [30, 31, 32, 33, 34] and we have witnessed substantial progress brought about by the introduction of LLMs. Inspired by these works, in this work, we aim to leverage the large language models in the automated generation of formal program specifications to address the limitations of conventional template-based approaches. From this perspective, we innovate our approach SpecGen, which generated three parts of specifications for the example presented in the right part of Figure 1. The first part is to describe the method <code>TwoSum()</code> as a whole, specifying its pre-conditions and post-conditions. The specifications in lines a and b claim that the input array must not be null before and after the method is executed. The post-condition in line c specifies that the target value equals the sum of the two elements corresponding to the indexes stored in the returned array. The post-condition at line d specifies that there does not exist such a pair of elements that satisfies the constraint when the length of the returned array is zero. These generated specifications can fully articulate the functionality of method <code>TwoSum()</code>. Furthermore, the loop invariants in the second and third parts specify corresponding constraints that must be met within a certain range in the array. These specifications generated by SpecGen comprehensively describe the semantics of this function and their correctness is verifiable.</p>
<h3>III. APPROACH</h3>
<h4><em>A. Overview</em></h4>
<p>The overview of SpecGen is presented in Figure 2, which consists of two components i.e., conversation-driven specification generation and mutation-based specification generation. The former is designed to communicate with the large language model to query the output in a conversational manner. In particular, a prompt is constructed with some few-shot examples for the initial query. The verification failure information provided by the verifier is further used as the prompt for the next round of conversation if the model-generated results are incorrect. The conversation will be repeated iteratively until the generated specifications successfully pass the verifier or a maximum number of iterations is reached. The latter aims at generating the specifications of a program that the large language model fails to generate. Four kinds of mutation operators are adopted to mutate the specification that failed verification by the verifier and obtain all potential variants. A heuristic selector is further designed to efficiently choose a set of mutated variants most likely to pass verification.</p>
<h4><em>B. Conversation-Driven Specification Generation</em></h4>
<p>Engaging in conversation with large models can fully leverage their capabilities, better assisting them in generating the desired content and avoiding potential errors [35]. Inspired by Xia et al. [15], we propose our conversation-driven specification generation in SpecGen to interact with the LLM conversationally to generate specifications. There are two main benefits: firstly, the conversational manner aids the large model in automatically correcting potential syntax errors in the generated content, secondly, providing the model with the verification failures information by the verifier in the conversation helps it generate more accurate specifications. The design mainly consists of two sequential components: initial prompt construction, which pre-defined an initial prompt to prepare for querying with the LLM, and conversational</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Overview of our SpecGen.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Conversational generation.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Illustration of the initial prompt construction.</p>
<p>Specification generation, which communicates to the LLM by incorporating verification failure information produced by the verifier in the conversation manner to generate verifiable specifications. The conversation will be repeated iteratively until the generated specifications pass the verifier or a maximum number of iterations is reached.</p>
<p><em>1) Initial Prompt Construction:</em> We need to define the initial prompt to query with LLM to obtain the model output. After multiple attempts to assess the impact of different prompts on the quality of generated program specifications, we ultimately chose to follow Xia et al. [36] in designing our prompt. The prompt is presented in Fig. 4 illustrating the components of the initial prompt, which consists of three different parts: the system role, few-shot examples, and the queried program. The system's role aims to inform LLMs about our application scenario, which is to generate JML specifications. We further add some few-shot examples. The reasons are two-fold. On one hand, few-shot examples can help the model to generate more accurate outputs [37]. On the other hand, LLMs can generate the desired output format that is learned from these examples. Each example is a pair of a program and its corresponding specifications. We randomly select it from our collected dataset to construct the few-shot examples. The last component is the queried program which requires the model to generate the output.</p>
<p><em>2) Conversational Specification Generation:</em> Given the initial prompt, LLM can obtain the initial output for the input program. As the output of the LLM in the first attempt may not successfully pass the validation of the verifier, we interleave the process of specification generation with verification failure feedback to prompt future generation in a conversational manner which is illustrated in Fig. 3. In particular, each generated specification by the model is verified by a JML verifier to test whether the generated result can pass the verifier. If the verification fails, we construct feedback information using the reported error message from the verifier as the prompt for the next generation. The verification error message can help the model understand the reason for failure and provide guidance for generating correct specifications. In addition, to avoid the verifier providing excessively long error messages, we configure the verifier to report only one verification failure message per attempt. Furthermore, through a massive amount of experiments, we summarize several types of common verification failures reported by the verifier. For each kind of error, we provide guidance in the natural language to facilitate the model in generating correct specifications. Upon encountering these types of verification failures reported by the verifier, we will insert corresponding guidance information into the prompt e.g., <Guidance> in Fig. 3 to assist the model in resolving the issues. The conversation will be repeated iteratively until the specifications are successfully verified or a maximum number of iterations is reached.</p>
<h3><em>C. Mutation-based Specification Generation</em></h3>
<p>In the conversation-driven generation process, some few-shot examples are provided in the initial prompt to start the query with the LLM. To further stimulate the potential of the LLM, multi-turn conversation continually guides the LLM in approaching the accurate specification more closely. Yet, they still struggle to generate fully correct specifications for some complex programs. The reasons are two-fold. On the one hand, in comparison to code generation [37, 38, 39], specification generation poses greater challenges for LLMs due to the limited corpora related to program specifications for the model to learn from. On the other hand, although the verification failure information provided by the verifier can assist LLMs in providing higher-quality responses to some extent, as the error messages are highly abstract and generalized, LLMs still struggle to accurately understand the semantic information within error messages for complex programs. While LLMs may not accurately generate specifications for complex programs, the generated results are already highly close to the oracle, inspiring us to design mutation-based generation.</p>
<p>In particular, the mutation-based specification generation component takes the output generated by the large language model that fails to pass the verifier through the multi-round conversation as the input. We further define a set of mutation operators to modify these generated outputs to obtain more</p>
<p>diverse results. Then a heuristic strategy is adopted for efficient verification. The workflow is presented in Algorithm 1. Specifically, we define the specifications generated by LLM that fail verification as the set of template specifications $E_{t}$, which consists of different specifications generated for different locations in a program, and a set of mutation operators as $M$. The MutationBasedGen takes $E_{t}$ and $M$ as the input and outputs a set of correct specifications as $E$. The function SpecMutation corresponds to the mutation operation of $E_{t}$, where each kind of mutation operator will be performed through the mutation function Mutate() (Section III-C1) on a template specification $e\left(e \in E_{t}\right)$ to obtain a set of candidates $E_{\text {mutated }}$ (lines 7 and line 8).</p>
<p>After the mutation operations are performed, we further design the specification selection algorithm to select a subset $E_{\text {selected }}$ of mutated specifications that can pass the verification. The selected subset $E_{\text {selected }}$ is initialized with $E_{t}$. We then iteratively require the verifier to check the correctness of $E_{\text {selected }}$ and obtain a set of refuted specifications denoted as $E_{\text {refuted }}$ from $E_{\text {selected }}$ that the verifier fails to verify. After that, we need to replace the failed specifications from $E_{\text {refuted }}$ with another mutated variant for the next iteration of verification. The ReSelect function is presented from line 18 to line 25. For each refuted specification $e_{r} \in E_{\text {refuted }}$, we first remove it from the mutation set $E_{\text {mutated }}$ and the selected set $E_{\text {selected }}$. Then we replace $e_{r}$ with another mutated variant $e$ that comes from the same family by a heuristic selection strategy (Section III-C2) from line 22 to line 23. Here the family refers to a set of mutated specifications $E_{f}$ that come from the same template specification. Finally, we add $e$ to the selected set $E_{\text {selected }}$ to prepare for the next iteration of verification. The above process will be repeated until all candidates in $E_{\text {selected }}$ are successfully verified i.e., $E_{\text {refuted }}$ is empty (line 16). Note that if all candidates are refuted, the process will finally select an empty set of candidates, guaranteeing the termination of the whole process.</p>
<p>1) Template Specification Mutation: As shown in Table I, we define four kinds of mutation operators including predicative, logical, comparative, and arithmetic. Each type of mutation corresponds to one type of operator supported by JML. LLMs perform well in formulating the overall syntactical structure of specifications, but they often make mistakes in grasping the fine-grained relationships between variables, resulting in incorrect operators used to describe the relationships between variables, which is why our mutation design is centered around the operators. A mutation operation substitutes the operators of the corresponding type in the specification with another of the same type. For example, after applying a predicative mutation, a \exists predicate within a specification may be substituted with \forall. Note that the mutation for a certain type of operator does not necessarily create only one mutated candidate. For example, the expression $\mathrm{a}&lt;=\mathrm{b}$ may be mutated to $\mathrm{a}&lt;\mathrm{b}$ or $\mathrm{a}-1&lt;=\mathrm{b}$. If multiple mutations can be applied to a specification at the same time, we try to exhaust each combination of different types of mutations to get all potential variants. Since the set of all</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Mutation-based</span><span class="w"> </span><span class="nt">Specification</span><span class="w"> </span><span class="nt">Generation</span>
<span class="w">    </span><span class="nt">Input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Set</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">template</span><span class="w"> </span><span class="nt">specification</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">mutations</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">M</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="nt">Set</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">verified</span><span class="w"> </span><span class="nt">specifications</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Function</span><span class="w"> </span><span class="nt">MutationBasedGen</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">SpecMutation</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">SpecSelection</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Function</span><span class="w"> </span><span class="nt">SpecMutation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">emptyset</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">e</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">Mutate</span><span class="p">}</span><span class="o">(</span><span class="nt">e</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Function</span><span class="w"> </span><span class="nt">SpecSelection</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="nt">E_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{refuted</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">emptyset</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">repeat</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{refuted</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">Verify</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">ReSelect</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{refuted</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">until</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{refuted</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mu</span><span class="w"> </span><span class="err">\</span><span class="nt">emptyset</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Function</span><span class="w"> </span><span class="nt">ReSelect</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{refuted</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">e_</span><span class="p">{</span><span class="err">r</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{refuted</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">left</span><span class="err">\</span><span class="nt">backslash</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">e_{r</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">left</span><span class="err">\</span><span class="nt">backslash</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">e_{r</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">f</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">GetFamilyOf</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">e_</span><span class="p">{</span><span class="err">r</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mutated</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">e</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">SelectByHeuristic</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">f</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">M</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="p">{</span><span class="err">e\</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{selected</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
</code></pre></div>

<p>TABLE I: The defined mutation operators.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Mutation Type</th>
<th style="text-align: center;">Original Operator</th>
<th style="text-align: center;">Mutated Operators</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Predicative</td>
<td style="text-align: center;">\forall</td>
<td style="text-align: center;">\exists</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\exists</td>
<td style="text-align: center;">\forall</td>
</tr>
<tr>
<td style="text-align: center;">Logical</td>
<td style="text-align: center;">$\underline{\underline{\mathbf{A}}}$</td>
<td style="text-align: center;">$|$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$|$</td>
<td style="text-align: center;">$\underline{\underline{\mathbf{A}}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$&lt;\infty&gt;$</td>
<td style="text-align: center;">$&lt;\infty, \infty&gt;$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\infty&gt;$</td>
<td style="text-align: center;">$&lt;\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$&lt;\infty$</td>
<td style="text-align: center;">$\infty&gt;$</td>
</tr>
<tr>
<td style="text-align: center;">Comparative</td>
<td style="text-align: center;">$&lt;=$</td>
<td style="text-align: center;">$&lt;-1&lt;=$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$&gt;=$</td>
<td style="text-align: center;">$&gt;,"+1&gt;="$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$&lt;$</td>
<td style="text-align: center;">$&lt;=$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$&gt;$</td>
<td style="text-align: center;">$&gt;=$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$!=$</td>
</tr>
<tr>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">$!=$</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$+$</td>
</tr>
</tbody>
</table>
<p>potential variants of a certain template is determined, the exhaustive searching process is deterministic. For instance, the expression $\mathrm{x}&lt;\mathrm{n}+1$ can be mutated to $\mathrm{x}&lt;=\mathrm{n}+1$ from the comparative type, $\mathrm{x}&lt;\mathrm{n}-1$ from the arithmetic type, or $\mathrm{x}&lt;=\mathrm{n}-1$ by combining them.
2) Mutated Specification Selection: Typically, for a program, Houdini [7] verifies all generated specifications at one time. However, similar practice cannot be applied in SpecGen as we exhaust all potential combinations of mutations for a template specification. The set of the obtained specifications for verification is considerably large, posing a much greater burden for the verifier within a single verification process. To address this challenge, we innovate a heuristic selection strategy to improve the stability and efficiency of verification.</p>
<p>In general, the heuristic selection algorithm finds a specification $\hat{e}$ such that</p>
<p>$$
\hat{e}=\underset{e \in E_{f}}{\arg \max } \sum_{m \in M}\left(\operatorname{times}\left(m, e, e_{t}\right) \cdot \operatorname{weight}(m)\right)
$$</p>
<p>where $E_{f}$ denotes a family of mutated specifications that come</p>
<p>from the same template specification $e_{t}$, and $M$ denotes the set of all mutations. Given $E_{f}$, $e_{t}$, and $M$, we design the heuristic selection logic to prioritize selecting important candidates for verification. In particular, we assign scores for each mutated candidate $e \in E_{f}$ and select the candidate with the highest score as the output. To calculate the score of a candidate $e$, for all types of mutations $m \in M$, we sum up all the values of $\operatorname{times}\left(m, e, e_{t}\right)$ multiplied by $\operatorname{weight}(m)$, where $\operatorname{times}\left(m, e, e_{t}\right)$ calculates how many times the mutation $m$ is performed when $e_{t}$ mutates into $e$, and $\operatorname{weight}(m)$ denotes the corresponding weight of $m$.</p>
<h2>IV. EXPERIMENTAL SETUP</h2>
<p>We design the following four research questions for evaluation:</p>
<ul>
<li>RQ1: How does SpecGen compare with the baseline approaches?</li>
<li>RQ2: How does each type of mutation contribute to the effectiveness of SpecGen?</li>
<li>RQ3: How do different candidate selection strategies affect the efficiency of SpecGen?</li>
<li>RQ4: To what extent can the generated specification contain the semantic information of the input program?</li>
</ul>
<h2>A. Implementation</h2>
<p>We use the API provided by OpenAI [40] to communicate with the large language model of gpt-3.5-turbo-1106 for the experiments. Temperature is set to 0.4 to balance the diversity and rigorousness of the outputs of GPT. 4 fewshot examples are used during the prompt construction to balance the input length and response time. The maximum number of rounds of conversation is set to 10 . The verifier is OpenJML [26], the most recent JML specification verification tool to check the consistency between Java source code and JML specifications. Due to the incompleteness in the implementations of OpenJML, we set a timeout limit of 30 minutes for a single verification in our implementation to avoid unexpected situations, such as the non-responding of OpenJML. All experiments are conducted on an 8-core workstation with Intel Core i7-12700H CPU @2.30GHz and 32GB RAM, running Ubuntu 22.04.3 LTS. The version of OpenJDK is 1.8.0_371 for all experiments except for Houdini, which has to run under OpenJDK 1.6.0_45. We set the weight of comparative, logical, arithmetic, and predicative mutation to $-1,-2,-4$, and -4 respectively as the comparative mutation is more likely to pass the verification followed by the logical mutation. The predicative and arithmetic mutations are the least important through our observations from extensive experiments. Note that the weights are defined with negative values, leading to negative calculated scores as well. The reason for the design is to prioritize the specification candidates with fewer mutations.</p>
<h2>B. Dataset</h2>
<p>To comprehensively evaluate the effectiveness of SpecGen, following previous work [41], we first use an established dataset, the benchmark of SV-COMP [20], for evaluation.
Specifically, we used 265 class definitions in the Java category of SV-COMP benchmark and conducted the necessary modifications on part of these programs (referred to as Dataset SV-COMP hereinafter) for ease of evaluation. The remaining data in the benchmark cannot be applied for specification generation even with our modification. We made minimal modifications to SV-COMP programs to ensure they can be executed outside the SV-COMP environment. Specifically, the programs destined to trigger false assertions have to be modified so that the programs can exit properly. Also, those library calls specific to the competition settings (e.g. Verifier.nondetInt()) are replaced with equivalent Java library calls so that they can be successfully compiled. It is ensured that the semantics of the modified programs remain unchanged. As calculated by tool JaCoCo [42], these programs have an average line of code (LoC) of 22.51, along with an average cyclomatic complexity (CC) of 6.18. However, after a deep analysis of the characteristics of the data from SV-COMP, we find that $88.7 \%$ programs are loop-free, indicating that the samples with more complex program structures cannot be covered by this dataset, inducing limitations on the evaluation. Also, very few datasets have been established specifically for specification generation tasks so far.</p>
<p>To remedy this gap, we further collect another dataset, SpecGenBench, containing 120 samples as a supplement where 20 programs (including the corresponding specifications) from the dataset constructed by Nilizadeh et al. [43] and 100 programs from LeetCode [44]. The selected programs are assured of the feasibility of expressing their behaviors as JML-specified verifiable specifications. These programs involve a variety of control flow structures and encompass multiple data structures such as arrays, strings, and other data structures supported by the Java library. They also cover a diverse set of specifications including post-conditions and loop invariants, involving both linear and nonlinear relationships between variables, making them representative of a broad spectrum of scenarios. They can be categorized into five categories according to their types of control flow structures [45]. Specifically, Sequential denotes the programs without branches or loops. Branched represents the loop-free programs that will contain branches like if-else or switch structures. Single-path Loop contains the simplest type of loop, with only one layer of loop structure without branches in their loop bodies. In contrast, Multi-path Loop denotes the loops that have branches in the loop bodies. Lastly, Nested Loop denotes the programs with multiple layers of loop structure where each layer may have a branch. The quantity of programs for Sequential, Branched, Single-path Loop, Multi-path Loop and Nested Loop is 26, 23, 24, 26, and 21, respectively. Programs in SpecGenBench have an average LoC of 20.77 and an average CC of 6.60 .</p>
<p>To obtain the ground truth specifications for the 100 Java programs from LeetCode, we follow a similar procedure in Nilizadeh et al. [43] with the help of human experts. Three experts with rich experience in formal verification were employed, to manually write specifications for each program. Each expert is required to write the specifications that can be</p>
<p>successfully verified to describe the functionality and behavior of the program as accurately and comprehensively as possible. For a single program, if multiple experts have written verifiable specifications, another expert is responsible for selecting one of them as the ground truth.</p>
<p>As Daikon [9] requires a set of test suites instrumented into the source code to execute the code, we manually write these test suites for it. A small test suite is initialized first, on which Daikon is invoked to generate specifications. If the results fail verification, the counterexample produced by the verifier will be added to the test suite. The procedure is repeated until no new specifications are generated. The test suites achieve an average instruction coverage of 90.16%, branch coverage of 87.98%, and line coverage of 91.36%. Lastly, we instrument dummy function calls at the top of each loop body in a program to ensure Houdini and Daikon can generate the specifications at these program points.</p>
<h2>C. Baselines</h2>
<p>We select two conventional approaches and several LLMbased approaches as the baselines for comparison.
Houdini [7]. It is a template-based JML annotation generator that relies on a series of pre-defined templates to generate candidate specifications. Given the input program, it first generates candidate specifications by filling in the templates with available variables and all kinds of operators. Afterward, It iteratively invokes a JML specification verifier to check their correctness and removes the refuted ones. The process will be repeated until the remaining candidates are all verified.
Daikon [9]. It is a classic tool for the dynamic detection of program specifications which relies on the dynamic execution trace of the target program to infer likely specifications. Given the input program, it first instruments the target program to trace certain variables and extracts execution traces. Then the inference engine reads the trace data and infers the potential invariants with a generate-and-check algorithm. Daikon supports dynamic detection for Java, C/C++, C#, and Perl programs along with various formats such as DBC format [46], JML format [23], and CSharpContract format [47].</p>
<p>Apart from the conventional approaches for specification generation, we further add the LLM-based approaches.
Few-shot LLM. They refer to the large language model i.e., gpt-3.5 with few-shot settings in our work to generate specifications. Under the few-shot settings, the LLM is queried only once to obtain the final result. We set 0-shot, 2-shot, and 4-shot for few-shot comparison.
Conversational. It refers to the generation technique described in Section III-B. Conversational generation iteratively queries the LLM to refine its results, with error information provided to the LLM as feedback on each iteration. The conversational setting is based on 4-shot examples. Other settings remain the same with SpecGen.
AutoSpec [21]. It is a recent technique for specification generation combining LLMs and static analysis. AutoSpec first decomposes the input program into its components, upon which a hierarchy graph is built. For each component, AutoSpec queries the LLM for corresponding specifications respectively. Eventually, specifications for all components are combined to obtain the overall result, which is presented to a verifier for correctness validation. The progress is repeated iteratively until the result is successfully verified.</p>
<h2>D. Evaluation Metrics</h2>
<p>Following the previous works [41, 48], we use the metric of Number of Passes for assessment. We further add more metrics for comprehensive evaluation.
Number of Passes. It defines the number of programs for which the generated specifications of an approach pass the validation by the verifier. For a program, we consider the specifications that pass the verifier as the correct specifications.
Success Probability. It is used to evaluate the model-based approaches. The randomness inherent in the content generated by large language models may introduce a certain level of contingency in a successful generation. Thus, we use the success probability for the measurement. For a test program, it is calculated by $\frac{N_{\text {success }}}{N_{\text {attempt }}}$ where $N_{\text {success }}$ denotes the number of successful generations of verifiable specifications and $N_{\text {attemp }}$ denotes a fixed number of trials in total (10 times in SpecGen).
Number of Verifier Calls. It is used to evaluate the efficiency of our approach. We propose a heuristic selection algorithm to prioritize the important candidates for the verifier to verify. To evaluate the effectiveness of the proposed selection algorithm, we use the number of verifier calls as the evaluation metric.
User Rating. It aims to measure the semantic quality of the generated specifications. We invited 15 Ph.D. students who are experts in Java programming language to rate the specifications generated by different approaches. The research is conducted using the Likert Scale [49], where students are required to give a rating from one point to five points for each case according to a reference rating criteria.</p>
<h2>V. EXPERIMENTAL RESULTS</h2>
<h2>A. RQ1: Comparison with Baselines</h2>
<p>The experimental results are presented in Table II where Num. denotes the number of successfully handled programs, and Prob. denotes the average success probability. Each LLMbased approach is granted 10 trials for each program.
Performance on SV-COMP. From Table II, we can find that on the SV-COMP dataset (265 programs in total), Houdini handled 56 programs, which is more than Daikon. However, both underperform LLM-based approaches even in LLM’s simplest setting i.e. 0-shot setting, which handled 81 programs, demonstrating the feasibility of employing LLMs for formal program specification generation. Among the LLM-based approaches with few-shot settings, as the number of given fewshot examples increases, the number of programs that LLM can generate verifiable specifications is also increased, substantiating the effectiveness of the few-shot examples. Based on the 4-shots examples in the initial prompt, the multi-turn conversational manner (in Section III-B) can further improve the performance, with 146 programs handled, compared to 94 programs of 4-shot LLM. Combining LLM-generated results</p>
<p>TABLE II: Number of programs that successfully pass the verifier and average success probability.</p>
<p>| Approach | | SV-COMP
(265) | | SpecGenBench | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>TABLE III: Effectiveness of different types of mutations.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>SpecGenBench</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Sequential</td>
<td>Branched</td>
<td>Single-path</td>
<td>Multi-path</td>
<td>Nested</td>
<td>SV-COMP</td>
<td>Total</td>
</tr>
<tr>
<td></td>
<td>(26)</td>
<td>(23)</td>
<td>(24)</td>
<td>(26)</td>
<td>(21)</td>
<td>(265)</td>
<td>(385)</td>
</tr>
<tr>
<td>w/o Predicative</td>
<td>24</td>
<td>20</td>
<td>20</td>
<td>19</td>
<td>9</td>
<td>167</td>
<td>259</td>
</tr>
<tr>
<td>w/o Logical</td>
<td>24</td>
<td>18</td>
<td>14</td>
<td>18</td>
<td>10</td>
<td>151</td>
<td>235</td>
</tr>
<tr>
<td>w/o Comparative</td>
<td>24</td>
<td>19</td>
<td>13</td>
<td>12</td>
<td>7</td>
<td>148</td>
<td>223</td>
</tr>
<tr>
<td>w/o Arithmetic</td>
<td>23</td>
<td>19</td>
<td>18</td>
<td>21</td>
<td>11</td>
<td>170</td>
<td>262</td>
</tr>
<tr>
<td>SpecGen</td>
<td>24</td>
<td>20</td>
<td>23</td>
<td>20</td>
<td>13</td>
<td>179</td>
<td>279</td>
</tr>
</tbody>
</table>
<h3>-B RQ2: Ablation Study on Mutation Types</h3>
<p>We conduct an ablation study to evaluate the effectiveness of different mutation types in SpecGen. The results are shown in Table III where w/o $\left{^{*}\right}$ denotes the disabled mutation type.</p>
<p>We can find that SpecGen successfully generates verifiable specifications for 279 out of a total of 385 programs. SpecGen w/o Comparative addresses the least number of programs i.e., 223, indicating that the comparative mutation is the most important in the defined mutation operations. The main reason is the frequent usage of numerical variables in programs and the recurring need to bound their range in the specifications. SpecGen w/o Logical has the second least number of programs i.e., 235, indicating that the logical operators are also important to generate verifiable specifications. This is due to the necessity of combining two or more expressions for different properties with logical operations when specifying complex behaviors. SpecGen w/o Predicative and SpecGen w/o Arithmetic have the most number of programs (259 and 262 respectively), which means both of them are less important than the comparative and logical mutation. We still consider them as they are applicable in certain situations. In some complex programs, specifications generated by the LLM are prone to have predicate errors. Hence, the predicative mutation will be useful. Similar cases exist when there are complicated numerical constraints on variables, where mutations on arithmetic operators turn out to be helpful.</p>
<p>Further analyzing the effectiveness of the mutation type for different kinds of programs, we can find that SpecGen w/o Comparative handles an especially lower number of programs in the loop category including single-path, multi-path, and nested loop. It is due to the rigid demand of scope bounding for loop variables when loops are involved. Scope bounding for loop variables is an intricate work where LLMs frequently make mistakes, substantiating the importance of comparative mutations. The performance of SpecGen w/o Predicative also drops on programs with nested loop, because of the relatively higher quantity and complexity of \forall and \exists statements involved in these nested programs.</p>
<p>RQ2: Each type of mutations contributes differently to SpecGen. The comparative mutation contributes the most to the performance while the predicative and arithmetic are less important. When combining them together, SpecGen achieves the best performance.</p>
<h3>-C RQ3: Effectiveness of Selection Strategy</h3>
<p>In Section III-C2, we design the heuristic selection strategy to improve the efficiency of verification. We also conduct an</p>
<p>TABLE IV: Average numbers of verifier calls in a single run under different specification selection strategies in Section III-C2.</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>SV-COMP</th>
<th>SpecGenBench</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Sequential</td>
<td>Branched</td>
<td>Single-path</td>
<td>Multi-path</td>
<td>Nested</td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>9.41</td>
<td>2.76</td>
<td>2.32</td>
<td>32.62</td>
<td>55.16</td>
<td>41.97</td>
<td>18.44</td>
</tr>
<tr>
<td>Heuristic</td>
<td>8.91</td>
<td>2.59</td>
<td>2.16</td>
<td>24.21</td>
<td>43.58</td>
<td>34.99</td>
<td>15.51</td>
</tr>
</tbody>
</table>
<p>experiment to compare with the random selection strategy. Specifically, when a candidate specification is refuted by the verifier, we randomly select another specification from $E_{f}$ for replacement. To compare with different strategies, for each program that successfully generates verifiable specifications by SpecGen, we run SpecGen 5 times to obtain the average number of verifier calls as the evaluation metric.
Performance on SV-COMP. Using the random selection strategy makes 9.41 verifier calls on average while the heuristic selection strategy takes 8.91 calls, resulting in an improvement of 5.30\%. The improvement is relatively modest and the values denote that only less than 10 verifier calls on average are used to generate verifiable specifications for programs in SVCOMP. The main reason is that the number of rounds for conversation is set to 10 in SpecGen, the specifications for these programs tend to be successfully generated within the conversation module (Section III-B). It is before the selection strategy used in the mutation-based specification generation (Section III-C) comes into effect.
Performance on SpecGenBench. Using the random selection strategy takes 36.20 verifier calls on average while the heuristic selection strategy takes 28.51 calls in terms of five categories in SpecGenBench. Hence, the heuristic selection strategy achieves an improvement of 21.23\%. Furthermore, we can observe that the improvements in different categories of programs vary significantly. The improvements in the loop categories including single-path, multi-path, and nested loop are more significant than sequential and branched. The main reason is that generating specifications for loop-containing programs is challenging, and usually requires more iterations to obtain verifiable specifications. In this case, a good selection strategy often highlights advantages more effectively. However, the improvements in sequential and branched categories are fewer. The reason is similar to SV-COMP, where the high efficiency of conversational generation on sequential and branched programs makes the selection strategies invalid. Nevertheless, loop structures are common in programs, thus a heuristic selection strategy to improve the validation efficiency is still helpful and necessary.</p>
<p>RQ3: The heuristic selection strategy effectively improves the efficiency of SpecGen. It is especially useful when generating specifications for programs with more complex structures such as loops.</p>
<h3>-D RQ4: User Study on the Quality of Specifications</h3>
<p>A user study is conducted to evaluate the semantic quality of the generated specifications. 15 Ph.D. students are invited</p>
<p>TABLE V: Average rating scores on the generated specifications by different approaches.</p>
<table>
<thead>
<tr>
<th>Test case</th>
<th>Houdini</th>
<th>Daikon</th>
<th>SpecGen</th>
<th>Oracle</th>
</tr>
</thead>
<tbody>
<tr>
<td>Absolute</td>
<td>3.50</td>
<td>3.36</td>
<td>4.85</td>
<td>5.00</td>
</tr>
<tr>
<td>AddLoop</td>
<td>2.40</td>
<td>1.33</td>
<td>4.57</td>
<td>5.00</td>
</tr>
<tr>
<td>Conjunction</td>
<td>4.50</td>
<td>3.50</td>
<td>5.00</td>
<td>5.00</td>
</tr>
<tr>
<td>ConvertTemperature</td>
<td>2.33</td>
<td>2.50</td>
<td>5.00</td>
<td>5.00</td>
</tr>
<tr>
<td>Disjunction</td>
<td>2.50</td>
<td>3.50</td>
<td>5.00</td>
<td>5.00</td>
</tr>
<tr>
<td>FireBuzz</td>
<td>2.63</td>
<td>2.86</td>
<td>5.00</td>
<td>5.00</td>
</tr>
<tr>
<td>IsCommonFactor</td>
<td>2.00</td>
<td>4.13</td>
<td>4.14</td>
<td>4.71</td>
</tr>
<tr>
<td>IsPalindrome</td>
<td>1.83</td>
<td>1.17</td>
<td>4.75</td>
<td>5.00</td>
</tr>
<tr>
<td>IsSubsequence</td>
<td>2.43</td>
<td>1.13</td>
<td>4.14</td>
<td>4.00</td>
</tr>
<tr>
<td>IsSuffix</td>
<td>2.20</td>
<td>1.50</td>
<td>4.33</td>
<td>4.63</td>
</tr>
<tr>
<td>MulLoop</td>
<td>1.88</td>
<td>1.25</td>
<td>3.33</td>
<td>5.00</td>
</tr>
<tr>
<td>MySqrt</td>
<td>2.00</td>
<td>2.80</td>
<td>3.75</td>
<td>4.25</td>
</tr>
<tr>
<td>Perimeter</td>
<td>1.00</td>
<td>2.80</td>
<td>4.78</td>
<td>5.00</td>
</tr>
<tr>
<td>SmallestEvenMal</td>
<td>2.57</td>
<td>1.00</td>
<td>4.50</td>
<td>5.00</td>
</tr>
<tr>
<td>Swap</td>
<td>1.00</td>
<td>2.00</td>
<td>5.00</td>
<td>4.88</td>
</tr>
<tr>
<td>Average</td>
<td>2.32</td>
<td>2.32</td>
<td>4.54</td>
<td>4.83</td>
</tr>
</tbody>
</table>
<p>to rate the specifications generated by different approaches. A detailed description of the rating process is given in Section IV-D. We selected the 15 programs from the dataset of SpecGenBench that can be handled by all of Houdini, Daikon, and SpecGen. Apart from the specifications generated by these approaches, we also add the ground truth as a reference. The specifications are kept anonymous to the students, disclosing no information about the sources of the specifications. The rating scores are presented in Table V, where the score for full marks is 5.</p>
<p>We can observe that the ground truth specifications (oracle) receive an average rating score of 4.83 , indicating that the semantics of these programs can be described comprehensively through JML specifications. Furthermore, the specifications generated by SpecGen received a rating score of 4.54 , which is close to the oracle, indicating that the generated specifications by SpecGen can also describe the real behaviors of the input program more fully. Among the 15 programs, all rating scores given to SpecGen are above 3, with the lowest rating being 3.33, meaning that in the worst case, SpecGen can still generate non-trivial specifications about the properties of the input program. In comparison, the specifications generated by Houdini and Daikon received an average rating score of 2.32, reflecting the semantic weakness in these specifications. Houdini and Daikon rely on pre-defined templates, which are in fact independent from the input program and can only cover a limited number of specification patterns. Consequently, the specifications produced are often simplistic and trivial, involving only a narrow range of variables and operators, making it difficult to capture the actual behavior and functionality of the input program precisely. Unlike traditional approaches that rely on a fixed set of templates, SpecGen utilizes the code comprehension capabilities of LLMs, which can cover a larger range of scenarios and generate targeted specifications that more closely match the semantics of the input program.</p>
<p>RQ4: SpecGen received an average rating score of 4.54 , which is close to the 4.83 of the oracle specifications, demonstrating the ability to accurately characterize the real program behaviors and generate specifications with comprehensive program semantics.</p>
<h2>VI. DISCUSSION</h2>
<h2>A. Performance on Real-world Programs</h2>
<p>To further evaluate the performance of SpecGen on real-world programs, we collect programs involved in Defects4J [50], a well-known dataset of reproducible bugs within open-source repositories. During the collection process, we only consider individual files with no dependency on thirdparty libraries or other files in the repository. This ensures that all the collected files can be properly executed and verified outside the repository. Eventually, 50 Java source files from 9 repositories are collected. The average line of code and cyclomatic complexity of the collected programs are 374.78 and 18.29 , respectively. We follow the same experimental settings in Section IV-A for experiments. Note that we only aim to evaluate the verifiability of the generated specifications in the same way Section V-A does, so the ground truth specifications of the programs are not prepared.</p>
<p>Table VI shows the performance of SpecGen and other baseline methods on the programs extracted from Defects4J. Although Daikon underperforms the LLM-based approaches, it still exhibits certain abilities in processing real-world programs. This is due to the existence of some simplistic methods within real-world class definitions, such as those retrieving the value of a certain class member without doing anything else, which Daikon is capable of handling. Compared to Daikon, the LLM-based approach with the simplest setting, i.e. 4shot, achieved 5 more programs handled. Based on the fewshot learning technique, the conversational approach further achieved 28 programs handled. Lastly, SpecGen succeeds in handling 38 out of the 50 programs, with an average success probability of $55.20 \%$, displaying decent capabilities in handling real-world programs.</p>
<h2>B. Threats to Validity</h2>
<p>Internal Validity. First, the prompts we used to communicate with the LLM may affect our results. To mitigate it, we refer to Xia et al. [15] to design the prompt. We plan to investigate the effect of different prompts in the future. Second, a potential threat lies in the risk of data leakage. Our constructed dataset SpecGenBench consists of 100 programs with expert-written specifications and 20 programs with their corresponding specifications from Nilizadeh et al. [43]. The former does not have the issue of data leakage as the specifications are written by experts in our research. However, since gpt-3.5 does not release its model as well as the training data, the latter 20 programs from the existing dataset may have the risk. The used dataset SV-COMP also has this risk. Nevertheless, through our observation of SpecGen on these programs, we have never spotted a situation where the output of SpecGen is the same as the existing oracle. Hence, we believe this threat is limited. Furthermore, even if we remove these potentially risky programs, SpecGen still successfully handles 87 programs in the remaining 100 programs, which is also the best.
External Validity. One of the external threats lies in the accuracy of the verifier (OpenJML). Due to the implementation flaws in OpenJML, there may be cases where some correct</p>
<p>TABLE VI: Performance on programs collected from 9 repositories in Defects4J.</p>
<p>| Approaches | chart <br> (7) |  | cll <br> (5) |  | codes <br> (4) |  | compress <br> (6) |  | jackson <br> (7) |  | jypath <br> (6) |  | lang <br> (7) |  | math <br> (4) |  | time <br> (4) |  | Total <br> (50) |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | Num. | Prob. | Num. | Prob. | Num. | Prob. | Num. | Prob. | Num. | Prob. | Num. | Prob. | Num. | Prob. | Num. | Prob. | Num. | Prob. | Num.  |
|  Darkon | 3 | - | 3 | - | 0 | - | 1 | - | 3 | - | 2 | - | 2 | - | 1 | - | 0 | - | 15  |
|  4-shot LLM | 1 | 7.14\% | 2 | 7.33\% | 2 | 9.71\% | 3 | 11.67\% | 3 | 9.52\% | 1 | 2.78\% | 4 | 11.67\% | 3 | 17.50\% | 1 | 5.00\% | 20  |
|  Conversational | 4 | 47.62\% | 4 | 60.00\% | 3 | 41.67\% | 1 | 11.11\% | 2 | 19.05\% | 5 | 55.56\% | 3 | 28.57\% | 3 | 66.67\% | 3 | 41.67\% | 28  |
|  SpecGen | 6 | 68.57\% | 4 | 68.00\% | 4 | 65.00\% | 4 | 36.67\% | 4 | 42.86\% | 5 | 63.33\% | 5 | 65.71\% | 3 | 45.00\% | 3 | 35.00\% | 38  |</p>
<p>specifications fail to pass verification. This is an inevitable problem that other verifiers [51] have to face as well. The reason lies in the undecidability of automatic software verification [52, 53]. Even in such a situation, SpecGen achieves impressive performance with the majority of testcases successfully generated verifiable specifications. Another threat is the potential bias of the hand-written specifications by experts. To mitigate this, we follow the procedure in Nilizadeh et al. [43]. First, the selected experts should have rich experience in writing specifications. Second, the initially chosen specifications should be verifiable by the verifier. Last, if multiple experts have written the specifications that pass the verifier, another expert is responsible for selecting one.</p>
<h2>VII. Related Work</h2>
<p>Large Language Models. With the advancement of generative AI, Large Language Models (LLMs) have emerged as a formidable force and have quickly found widespread applications. LLMs are characterized by their immense parameter scale and training dataset size [54]. An important feature of LLMs is their ability for in-context learning [37], which enhances the coherence between the context and the output of LLMs. The learning ability gives rise to a unique usage of LLMs known as prompting [55], where a natural language description of the intended downstream task is provided to the LLM before assigning it the task. LLMs initially demonstrated remarkable capabilities in the field of Natural Language Processing (NLP) [56], excelling in tasks such as document classification [57], text summarization [58], and machine translation [59]. They are also widely deployed in various software engineering tasks [14, 18], including software testing [30, 31], code generation [17, 33] and code summarization [16]. Compared with these works, our goal is to employ LLMs for the automated generation of program specifications, which is important in formal methods. Program Specification Generation. The research on program specification generation can be categorized into two types: natural language specification generation and formal specification generation. Natural language specification generation primarily manifests as code summarization [60], a process of automatically generating accurate, human-readable descriptions of code functionality. Numerous efforts have been made to utilize machine learning methods for code summarization [61, 62, 63, 64]. Formal specification generation primarily takes the form of the generation of program invariants, the formal language representations of properties that a program is guaranteed to satisfy at a certain program point. In invariant generation, a large amount of research focuses on the generation of loop invariants [65, 66, 67, 68, 69, 70, 71], while the rest of the works attempt to generate invariants of other forms, e.g. pre-conditions [48, 72], post-conditions [10, 13, 41, 73, 74], assertion-based invariants [75] and finite automata [76, 77, 78]. With the development of Large Language Models, there have also been efforts employing LLMs to generate program specifications. Wen et al. [21] combine LLMs with static analysis techniques, including code decomposition, to generate verifiable program specifications. Pei et al. [79] utilize fine-tuning to enhance the performance of LLMs on specification generation tasks. Concerning the difficulty of selecting correct specifications from the massive LLM-generated results, Chakraborty et al. [69] propose a ranking algorithm that can distinguish correct inductive invariants from incorrect attempts based on the problem definition. Among these works, artifacts of Ghosal et al. [48] and Pei et al. [79] are not publicly available, the artifact of Alshnakat et al. [41] is built for C code and FramaC contracts, and the grammar for specifications of Molina et al. $[10,74]$ involves specific features, which cannot be trivially translated into equivalent JML, thus we cannot include them for comparison. Compared to these works, SpecGen utilizes the code comprehension capability of LLMs for program specification generation in a conversational manner, further followed by the mutation-based approach for enhancement.</p>
<h2>VIII. CONCLUSION</h2>
<p>In this paper, we introduced SpecGen, a novel approach that utilizes the Large Language Model for formal program specification generation. Leveraging the code comprehension ability of LLMs as well as the well-designed mutation-based specification generation component, our approach is capable of accurately capturing the behaviour and functionality of input programs to generate accurate specifications. A comprehensive evaluation between SpecGen and other baselines is conducted on two different datasets, the benchmark for the Java category of SV-COMP, and a more diverse and manually constructed dataset containing 120 programs. The extensive experimental results haver demonstrated that our approach significantly outperforms the baseline approaches, with the ability to effectively articulate program behaviors.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>We are grateful for the constructive feedback of all the anonymous reviewers to improve this manuscript. The authors from Nanjing University are supported in part by the Leadingedge Technology Program of Jiangsu Natural Science Foundation (No. BK20202001), the National Natural Science Foundation of China (No. 62232008, 62172200), and the Postgraduate Research \&amp; Practice Innovation Program of Jiangsu Province (No. KYCX24_0237).</p>
<h2>REFERENCES</h2>
<p>[1] A. Mesbah, A. van Deursen, and D. Roest, "Invariant-based automatic testing of modern web applications," IEEE Transactions on Software Engineering, vol. 38, no. 1, pp. 35-53, 2012.
[2] T.-H. Nguyen and D.-H. Dang, "Tc4mt: A specification-driven testing framework for model transformations," International Journal of Software Engineering and Knowledge Engineering, pp. 1-39, 2023.
[3] G. Cabodi, S. Nocco, and S. Quer, "Strengthening model checking techniques with inductive invariants," IEEE transactions on computeraided design of integrated circuits and systems, vol. 28, no. 1, pp. 154158, 2008.
[4] D. Beyer, M. Dangl, and P. Wendler, "Boosting k-induction with continuously-refined invariants," in International Conference on Computer Aided Verification. Springer, 2015, pp. 622-640.
[5] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, and R. Stata, "Extended static checking for java," in Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation, 2002, pp. 234-245.
[6] E. Rodríguez-Carbonell and D. Kapur, "Program verification using automatic generation of invariants," in International Colloquium on Theoretical Aspects of Computing. Springer, 2004, pp. 325-340.
[7] C. Flanagan and K. R. M. Leino, "Houdini, an annotation assistant for esc/java," in International Symposium of Formal Methods Europe. Springer, 2001, pp. 500-517.
[8] J. W. Nimmer and M. D. Ernst, "Automatic generation of program specifications," ACM SIGSOFT Software Engineering Notes, vol. 27, no. 4, pp. 229-239, 2002.
[9] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant, C. Pacheco, M. S. Tschantz, and C. Xiao, "The daikon system for dynamic detection of likely invariants," Science of computer programming, vol. 69, no. 1-3, pp. 35-45, 2007.
[10] F. Molina, M. d'Amorim, and N. Aguirre, "Fuzzing class specifications," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 1008-1020.
[11] Z. Y. Ding, Y. Lyu, C. Timperley, and C. Le Goues, "Leveraging program invariants to promote population diversity in search-based automatic program repair," in 2019 IEEE/ACM International Workshop on Genetic Improvement (GI). IEEE, 2019, pp. 2-9.
[12] F. Rahman and Y. Labiche, "A comparative study of invariants generated by daikon and user-defined design contracts," in 2014 14th International Conference on Quality Software. IEEE, 2014, pp. 174-183.
[13] Y. Wei, C. A. Furia, N. Kazmin, and B. Meyer, "Inferring better contracts," in Proceedings of the 33rd International Conference on Software Engineering, 2011, pp. 191-200.
[14] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy, and H. Wang, "Large language models for software engineering: A systematic literature review," arXiv preprint arXiv:2308.10620, 2023.
[15] C. S. Xia and L. Zhang, "Conversational automated program repair," arXiv preprint arXiv:2301.13246, 2023.
[16] T. Ahmed and P. Devanbu, "Few-shot training llms for project-specific code-summarization," in Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, pp. 1-5.
[17] Z. Zeng, H. Tan, H. Zhang, J. Li, Y. Zhang, and L. Zhang, "An extensive study on pre-trained models for program understanding and generation," in Proceedings of the 31st ACM SIGSOFT international symposium on software testing and analysis, 2022, pp. 39-51.
[18] W. Ma, S. Liu, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, and Y. Liu, "The scope of chatgpt in software engineering: A thorough investigation," arXiv preprint arXiv:2305.12138, 2023.
[19] Z. Yuan, J. Liu, Q. Zi, M. Liu, X. Peng, and Y. Lou, "Evaluating instruction-tuned large language models on code comprehension and generation," arXiv preprint arXiv:2308.01240, 2023.
[20] sosy lab, "Sv-comp - international competition on software verification," 2024, https://sites.google.com/view/specgen.
[21] C. Wen, J. Cao, J. Su, Z. Xu, S. Qin, M. He, H. Li, S.-C. Cheung, and C. Tian, "Enchanting program specification synthesis by large language models using static analysis and program verification," arXiv preprint arXiv:2404.00762, 2024.
[22] Github, "Specgen-artifact," 2024, https://github.com/Lezhi-Ma/ SpecGen-Artifact.
[23] L. Burdy, Y. Cheon, D. R. Cok, M. D. Ernst, J. R. Kiniry, G. T. Leavens, K. R. M. Leino, and E. Poll, "An overview of jml tools and applications," International journal on software tools for technology transfer, vol. 7, pp. 212-232, 2005.
[24] E. I. Leonard and C. L. Heitmeyer, "Automatic program generation from formal specifications using apts," Automatic Program Development: A Tribute to Robert Paige, pp. 93-113, 2008.
[25] M. Pradel and T. R. Gross, "Automatic generation of object usage specifications from large method traces," in 2009 IEEE/ACM International Conference on Automated Software Engineering. IEEE, 2009, pp. 371382.
[26] D. R. Cok, "Openjml: Jml for java 7 by extending openjdk," in NASA Formal Methods: Third International Symposium, NFM 2011, Pasadena, CA, USA, April 18-20, 2011. Proceedings 3. Springer, 2011, pp. 472479.
[27] C. Barrett and C. Tinelli, Satisfiability modulo theories. Springer, 2018.
[28] OpenAI, "Gpt-3.5," 2023, https://platform.openai.com/docs/models/ gpt-3-5.
[29] ——, "Chatgpt," 2023, https://chat.openai.com/.
[30] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, "Large language models are edge-case generators: Crafting unusual programs for fuzzing deep learning libraries," in 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE). IEEE Computer Society, 2023, pp. 830-842.
[31] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang, "Universal fuzzing via large language models," arXiv preprint arXiv:2308.04748, 2023.
[32] X. Jiang, Y. Dong, L. Wang, Q. Shang, and G. Li, "Selfplanning code generation with large language model," arXiv preprint arXiv:2303.06689, 2023.
[33] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation," arXiv preprint arXiv:2305.01210, 2023.
[34] Y. Wei, C. S. Xia, and L. Zhang, "Copiloting the copilots: Fusing large language models with completion engines for automated program repair," in Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2023, pp. 172-184.
[35] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen et al., "Siren's song in the ai ocean: A survey on hallucination in large language models," arXiv preprint arXiv:2309.01219, 2023.
[36] C. S. Xia and L. Zhang, "Keep the conversation going: Fixing 162 out of 337 bugs for $\$ 0.42$ each using chatgpt," arXiv preprint arXiv:2304.00385, 2023.
[37] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[38] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021.
[39] OpenAI, "Gpt-4 technical report," 2023.
[40] ——, "Api reference - openai api," 2023, https://platform.openai.com/ docs/api-reference.
[41] A. Alshnakat, D. Gurov, C. Lidström, and P. Rümmer, "Constraintbased contract inference for deductive verification," Deductive Software Verification: Future Perspectives: Reflections on the Occasion of 20 Years of KeY, pp. 149-176, 2020.
[42] EclEmma, "Eck Emma - jacoco java code coverage library," 2024, https: //www.eclemma.org/jacoco/.
[43] A. Nilizadeh, G. T. Leavens, X.-B. Le, C. S. Pasareanu, and D. Cok, "Exploring true text overfitting in dynamic automated program repair using formal methods (in press)," in 2021 14th IEEE Conference on Software Testing, Validation and Verification (ICST). IEEE, 2021.
[44] LeetCode, "The world's leading online programming learning platform," 2023, https://leetcode.com/.
[45] X. Xie, B. Chen, L. Zou, Y. Liu, W. Le, and X. Li, "Automatic loop summarization via path dependency analysis," IEEE Transactions on Software Engineering, vol. 45, no. 6, pp. 537-557, 2017.
[46] Parasoft, "Ai-powered java testing tool," 2023, https://www.parasoft. com/products/parasoft-jtest/.</p>
<p>[47] Microsoft, "Code contracts - microsoft research," 2023, https://www. microsoft.com/en-us/research/project/code-contracts/.
[48] S. Ghosal, B. Jonsson, and P. Rümmer, "An active learning approach to synthesizing program contracts," in International Conference on Software Engineering and Formal Methods. Springer, 2023, pp. 126144 .
[49] T. Nemoto and D. Beglar, "Likert-scale questionnaires," in JALT 2013 conference proceedings, 2014, pp. 1-8.
[50] R. Just, D. Jalali, and M. D. Ernst, "Defects4j: A database of existing faults to enable controlled testing studies for java programs," in Proceedings of the 2014 international symposium on software testing and analysis, 2014, pp. 437-440.
[51] W. Ahrendt, T. Baar, B. Beckert, R. Bubel, M. Giese, R. Hähnle, W. Menzel, W. Mostowski, A. Roth, S. Schlager et al., "The key tool: integrating object oriented design and formal verification," Software \&amp; Systems Modeling, vol. 4, pp. 32-54, 2005.
[52] P. A. Abdulla and B. Jonsson, "Undecidable verification problems for programs with unreliable channels," Information and Computation, vol. 130, no. 1, pp. 71-90, 1996.
[53] U. Mathur, P. Madhusudan, and M. Viswanathan, "What's decidable about program verification modulo axioms?" in International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, 2020, pp. 158-177.
[54] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., "A survey of large language models," arXiv preprint arXiv:2303.18223, 2023.
[55] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing," ACM Computing Surveys, vol. 55, no. 9, pp. 1-35, 2023.
[56] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, "Recent advances in natural language processing via large pre-trained language models: A survey," ACM Computing Surveys, vol. 56, no. 2, pp. 1-40, 2023.
[57] S. Hegselmann, A. Buendia, H. Lang, M. Agrawal, X. Jiang, and D. Sontag, "Tabllm: Few-shot classification of tabular data with large language models," in Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning Research, F. Ruiz, J. Dy, and J.-W. van de Meent, Eds., vol. 206. PMLR, 25-27 Apr 2023, pp. 5549-5581. [Online]. Available: https://proceedings.mlr.press/v206/hegselmann23a.html
[58] X. Yang, Y. Li, X. Zhang, H. Chen, and W. Cheng, "Exploring the limits of chatgpt for query or aspect-based text summarization," arXiv preprint arXiv:2302.08081, 2023.
[59] B. Zhang, B. Haddow, and A. Birch, "Prompting large language model for machine translation: A case study," arXiv preprint arXiv:2301.07069, 2023.
[60] Y. Zhu and M. Pan, "Automatic code summarization: A systematic literature review," arXiv preprint arXiv:1909.04352, 2019.
[61] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, "Summarizing source code using a neural attention model," in 54th Annual Meeting of the Association for Computational Linguistics 2016. Association for Computational Linguistics, 2016, pp. 2073-2083.
[62] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, "A transformer-based approach for source code summarization," arXiv preprint arXiv:2005.00653, 2020.
[63] S. Liu, Y. Chen, X. Xie, J. Siow, and Y. Liu, "Retrieval-augmented generation for code summarization via hybrid gnn," arXiv preprint arXiv:2006.05405, 2020.
[64] P. Fernandes, M. Allamanis, and M. Brockschmidt, "Structured neural summarization," arXiv preprint arXiv:1811.01824, 2018.
[65] P. Garg, D. Neider, P. Madhusudan, and D. Roth, "Learning invariants using decision trees and implication counterexamples," ACM Sigplan Notices, vol. 51, no. 1, pp. 499-512, 2016.
[66] G. Ryan, J. Wong, J. Yao, R. Gu, and S. Jana, "Cln2inv: learning loop invariants with continuous logic networks," arXiv preprint arXiv:1909.11542, 2019.
[67] X. Si, H. Dai, M. Raghothaman, M. Naik, and L. Song, "Learning loop invariants for program verification," Advances in Neural Information Processing Systems, vol. 31, 2018.
[68] C. Janßen, C. Richter, and H. Wehrheim, "Can chatgpt support software verification?" arXiv preprint arXiv:2311.02433, 2023.
[69] S. Chakraborty, S. K. Lahiri, S. Fukhoury, M. Musuvathi, A. Lal, A. Rastogi, A. Senthilnathan, R. Sharma, and N. Swamy, "Ranking
llm-generated loop invariants for program verification," arXiv preprint arXiv:2310.09342, 2023.
[70] A. Kamath, A. Senthilnathan, S. Chakraborty, P. Deligiannis, S. K. Lahiri, A. Lal, A. Rastogi, S. Roy, and R. Sharma, "Finding inductive loop invariants using large language models," arXiv preprint arXiv:2311.07948, 2023.
[71] V. J. Hellendoorn, P. T. Devanbu, O. Polozov, and M. Marron, "Are my invariants valid? a learning approach," arXiv preprint arXiv:1903.06089, 2019.
[72] P. Cousot, R. Cousot, M. Fähndrich, and F. Logozzo, "Automatic inference of necessary preconditions," in International Workshop on Verification, Model Checking, and Abstract Interpretation. Springer, 2013, pp. 128-148.
[73] Y. Moy and C. Marché, "Modular inference of subprogram contracts for safety checking," Journal of Symbolic Computation, vol. 45, no. 11, pp. $1184-1211,2010$.
[74] F. Molina, P. Ponzio, N. Aguirre, and M. Frias, "Evospex: An evolutionary algorithm for learning postconditions," in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1223-1235.
[75] V. Terragni, G. Jahangirova, P. Tonella, and M. Pezzè, "Evolutionary improvement of assertion oracles," in Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020, pp. 1178-1189.
[76] M. Christodorescu, S. Jha, and C. Kruegel, "Mining specifications of malicious behavior," in Proceedings of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering, 2007, pp. 5-14.
[77] F. Aarts, F. Heidarian, H. Kappens, P. Olsen, and F. Vaandrager, "Automata learning through counterexample guided abstraction refinement," in FM 2012: Formal Methods: 18th International Symposium, Paris, France, August 27-31, 2012. Proceedings 18. Springer, 2012, pp. 1027.
[78] V. Murali, S. Chaudhuri, and C. Jermaine, "Bayesian specification learning for finding api usage errors," in Proceedings of the 2017 11th joint meeting on foundations of software engineering, 2017, pp. 151162.
[79] K. Pei, D. Bieber, K. Shi, C. Sutton, and P. Yin, "Can large language models reason about program invariants?" in International Conference on Machine Learning. PMLR, 2023, pp. 27 496-27 520.</p>            </div>
        </div>

    </div>
</body>
</html>
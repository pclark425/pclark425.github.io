<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-761 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-761</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-761</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-218486872</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2005.00811v1.pdf" target="_blank">Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we consider the recent trend of evaluating progress on reinforcement learning technology by using text-based environments and games as evaluation environments. This reliance on text brings advances in natural language processing into the ambit of these agents, with a recurring thread being the use of external knowledge to mimic and better human-level performance. We present one such instantiation of agents that use commonsense knowledge from ConceptNet to show promising performance on two text-based environments.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e761.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e761.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief+KG (KG Full / KG Evolve)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-aware RL agent combining a dynamic belief graph with ConceptNet commonsense knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based RL agent that maintains a dynamic symbolic belief graph from observations and augments it with subgraphs extracted from the ConceptNet knowledge graph, encoding the aggregated graph with pretrained KG embeddings and GCNs and selecting actions with a learned A2C policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief+KG (KG Full / KG Evolve)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The agent encodes observations and admissible actions via hierarchical GRU encoders (GloVe embeddings). It maintains a dynamic belief graph generated from textual observations and links entity mentions to an external commonsense KB (ConceptNet) to extract a commonsense subgraph. The belief and commonsense graphs are merged into an aggregated graph G_t; node features are initialized with Numberbatch embeddings and updated via stacked GCN layers to produce node vectors Z_t and a pooled graph encoding g_t. Action selection concatenates the state embedding s_t, graph encoding g_t, and encoded admissible actions into r_t and passes it through an MLP + softmax to produce action probabilities; the policy is trained with Advantage Actor-Critic (A2C). Two KG delivery regimes are evaluated: KG Full (entire commonsense subgraph available at start) and KG Evolve (incremental exposure of commonsense links as entities are observed).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>TextWorld is a text-adventure benchmark expressed as a POMDP: at each turn the agent receives a textual observation (sequence of tokens) and a set of admissible textual actions; the underlying true state is not directly observable, creating partial observability, large combinatorial action spaces, and hard exploration challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet knowledge graph (commonsense KB) for extracting subgraphs related to observed entities; Numberbatch pretrained KG embeddings for node features; GloVe pretrained word embeddings for textual encoding. (TextWorld itself provides admissible actions but is the environment, not a tool.)</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph data: nodes (entities/concepts) and typed edges (relations) forming a subgraph of ConceptNet; precomputed dense embedding vectors for nodes (Numberbatch); (textual observations remain as token sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Symbolic dynamic belief graph constructed from textual observations (entities and relations); this belief graph is merged with the commonsense subgraph to form G_t. Node features come from Numberbatch embeddings and are refined by message passing through stacked GCN layers; the pooled graph vector g_t is used as the graph-level belief encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each time step t the observation o_t is parsed for entity mentions; the belief graph is updated (nodes added/removed) from observed text; the agent links observed entities to ConceptNet to extract a commonsense subgraph and merges it with the belief graph based on shared entity mentions. The merged graph's node features are initialized with Numberbatch embeddings and iteratively updated via L stacked GCN message-passing layers to produce updated node vectors Z_t; averaging Z_t columns yields the graph encoding g_t used downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (model-free actor-critic): A2C optimizes an action-selection network that conditions on encoded observations, admissible actions, and the aggregated graph encoding; the graph is used to prune/guide the action space rather than performing explicit search-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregating a dynamic belief graph with commonsense knowledge from ConceptNet can reduce exploration and improve agent efficiency versus a text-only baseline; incremental exposure to commonsense (KG Evolve) outperforms providing the full commonsense subgraph upfront (KG Full) because full KG can overwhelm the agent with noisy relations; however, in some tasks (e.g., simple single-room recipe retrieval) the ground-truth full belief graph can outperform commonsense augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e761.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e761.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Aided Transformer Agent (GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that constructs a dynamic belief graph of the game state from observations and uses that graph to guide action selection and generalization in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GATA represents the environment state as a dynamic belief graph constructed during exploration; this symbolic graph is used to inform the agent's policy and prune the action space, improving planning and generalization. In this paper GATA is used as a baseline (both GATA Full and GATA Evolve variants) for comparison in the cooking recipe task.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (recipe games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same TextWorld setting: partially observable textual game instances; the recipe tasks involve collecting ingredients possibly across rooms, with the belief graph representing observed entities/relations and (when full) the ground-truth state graph used internally by TextWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>None (GATA uses a belief graph derived from observations; the full belief graph corresponds to TextWorld's internal ground-truth state but is not an external KB).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic belief graph constructed from observed textual descriptions (entity nodes and relations); available either as full ground-truth belief graph (GATA Full) or as an evolve/incrementally-built belief graph (GATA Evolve).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The evolve-belief graph is incrementally generated from observations; the full belief graph (when provided) is the ground-truth TextWorld state graph. The graph is used to prune the action set and to produce state encodings for policies.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Graph-guided learned policy (uses belief graph to prune actions and guide exploration); training details per original GATA work (used here as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GATA Full (ground-truth full belief graph) can outperform commonsense-augmented agents in simple recipe tasks where the full state graph provides directly useful information; belief graphs help prune action spaces and improve exploration efficiency, and incremental (evolve) graphs can be preferable to exposing full graphs depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e761.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e761.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-DQN (Knowledge-Graph DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that learns a dynamic knowledge/belief graph from text-game observations and uses it to prune the action space and guide a DQN policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KG-DQN constructs a belief graph from textual observations (entities and relations) and uses that graph representation within a DQN framework to select actions and eliminate implausible actions, reducing exploration complexity in text-adventure games. It is cited as prior work that represents game state as a belief graph learned during exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / TextWorld (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-adventure environments treated as POMDPs where observations are textual and the true state is partially observable; large discrete action spaces and long-horizon planning make exploration difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>No external KB is required; relies on learned belief graph derived from observations.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic belief graph built from observed text; symbolic graph used to represent state and prune actions.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Belief graph is updated from incoming textual observations as the agent explores; the graph acts as the agent's memory/state representation used by the DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned value-based policy (DQN) augmented with action pruning based on the belief graph; not explicit search-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior work demonstrates that dynamic belief graphs learned from text can reduce the action space and improve exploration efficiency in text-adventure games; such graph-based state representations inspire the paper's approach of combining belief and external commonsense graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e761.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e761.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeDeepChef</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LeDeepChef</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep RL agent for families of text-based recipe games that uses a supervised list of common food items from Freebase to help generalize to unseen recipes and ingredients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ledeepchef: Deep reinforcement learning agent for families of text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LeDeepChef</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LeDeepChef augments RL training with supervision using a curated list of common food items (from Freebase) to enable generalization to unseen recipes and ingredients across different house environments in text-based cooking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based cooking/recipe games (TextWorld-like)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Recipe-based text games where the agent must collect ingredients and perform cooking actions; partial observability arises from local textual observations and unseen portions of the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Freebase (list of common food items) as an external knowledge resource to supervise and assist generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured list of food entities (symbolic items).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Supervised use of common-item lists rather than a dynamic symbolic belief graph (as described in the LeDeepChef reference).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy augmented with supervised priors from an external KB (Freebase) to aid generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supervising agents with external KB-derived priors (common food items) can improve generalization to unseen recipes/environments; this is complementary to the belief-graph + commonsense approach studied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic knowledge graphs to generalize on textbased games. <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Ledeepchef: Deep reinforcement learning agent for families of text-based games. <em>(Rating: 2)</em></li>
                <li>Building dynamic knowledge graphs from text using machine reading comprehension. <em>(Rating: 2)</em></li>
                <li>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-761",
    "paper_id": "paper-218486872",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "Belief+KG (KG Full / KG Evolve)",
            "name_full": "Knowledge-aware RL agent combining a dynamic belief graph with ConceptNet commonsense knowledge",
            "brief_description": "A text-based RL agent that maintains a dynamic symbolic belief graph from observations and augments it with subgraphs extracted from the ConceptNet knowledge graph, encoding the aggregated graph with pretrained KG embeddings and GCNs and selecting actions with a learned A2C policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief+KG (KG Full / KG Evolve)",
            "agent_description": "The agent encodes observations and admissible actions via hierarchical GRU encoders (GloVe embeddings). It maintains a dynamic belief graph generated from textual observations and links entity mentions to an external commonsense KB (ConceptNet) to extract a commonsense subgraph. The belief and commonsense graphs are merged into an aggregated graph G_t; node features are initialized with Numberbatch embeddings and updated via stacked GCN layers to produce node vectors Z_t and a pooled graph encoding g_t. Action selection concatenates the state embedding s_t, graph encoding g_t, and encoded admissible actions into r_t and passes it through an MLP + softmax to produce action probabilities; the policy is trained with Advantage Actor-Critic (A2C). Two KG delivery regimes are evaluated: KG Full (entire commonsense subgraph available at start) and KG Evolve (incremental exposure of commonsense links as entities are observed).",
            "environment_name": "TextWorld",
            "environment_description": "TextWorld is a text-adventure benchmark expressed as a POMDP: at each turn the agent receives a textual observation (sequence of tokens) and a set of admissible textual actions; the underlying true state is not directly observable, creating partial observability, large combinatorial action spaces, and hard exploration challenges.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet knowledge graph (commonsense KB) for extracting subgraphs related to observed entities; Numberbatch pretrained KG embeddings for node features; GloVe pretrained word embeddings for textual encoding. (TextWorld itself provides admissible actions but is the environment, not a tool.)",
            "tool_output_types": "Structured graph data: nodes (entities/concepts) and typed edges (relations) forming a subgraph of ConceptNet; precomputed dense embedding vectors for nodes (Numberbatch); (textual observations remain as token sequences).",
            "belief_state_mechanism": "Symbolic dynamic belief graph constructed from textual observations (entities and relations); this belief graph is merged with the commonsense subgraph to form G_t. Node features come from Numberbatch embeddings and are refined by message passing through stacked GCN layers; the pooled graph vector g_t is used as the graph-level belief encoding.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each time step t the observation o_t is parsed for entity mentions; the belief graph is updated (nodes added/removed) from observed text; the agent links observed entities to ConceptNet to extract a commonsense subgraph and merges it with the belief graph based on shared entity mentions. The merged graph's node features are initialized with Numberbatch embeddings and iteratively updated via L stacked GCN message-passing layers to produce updated node vectors Z_t; averaging Z_t columns yields the graph encoding g_t used downstream.",
            "planning_approach": "Learned policy (model-free actor-critic): A2C optimizes an action-selection network that conditions on encoded observations, admissible actions, and the aggregated graph encoding; the graph is used to prune/guide the action space rather than performing explicit search-based planning.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Aggregating a dynamic belief graph with commonsense knowledge from ConceptNet can reduce exploration and improve agent efficiency versus a text-only baseline; incremental exposure to commonsense (KG Evolve) outperforms providing the full commonsense subgraph upfront (KG Full) because full KG can overwhelm the agent with noisy relations; however, in some tasks (e.g., simple single-room recipe retrieval) the ground-truth full belief graph can outperform commonsense augmentation.",
            "uuid": "e761.0",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "GATA",
            "name_full": "Graph Aided Transformer Agent (GATA)",
            "brief_description": "An agent that constructs a dynamic belief graph of the game state from observations and uses that graph to guide action selection and generalization in text-based games.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "mention_or_use": "use",
            "agent_name": "GATA",
            "agent_description": "GATA represents the environment state as a dynamic belief graph constructed during exploration; this symbolic graph is used to inform the agent's policy and prune the action space, improving planning and generalization. In this paper GATA is used as a baseline (both GATA Full and GATA Evolve variants) for comparison in the cooking recipe task.",
            "environment_name": "TextWorld (recipe games)",
            "environment_description": "Same TextWorld setting: partially observable textual game instances; the recipe tasks involve collecting ingredients possibly across rooms, with the belief graph representing observed entities/relations and (when full) the ground-truth state graph used internally by TextWorld.",
            "is_partially_observable": true,
            "external_tools_used": "None (GATA uses a belief graph derived from observations; the full belief graph corresponds to TextWorld's internal ground-truth state but is not an external KB).",
            "tool_output_types": null,
            "belief_state_mechanism": "Dynamic belief graph constructed from observed textual descriptions (entity nodes and relations); available either as full ground-truth belief graph (GATA Full) or as an evolve/incrementally-built belief graph (GATA Evolve).",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "The evolve-belief graph is incrementally generated from observations; the full belief graph (when provided) is the ground-truth TextWorld state graph. The graph is used to prune the action set and to produce state encodings for policies.",
            "planning_approach": "Graph-guided learned policy (uses belief graph to prune actions and guide exploration); training details per original GATA work (used here as baseline).",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "GATA Full (ground-truth full belief graph) can outperform commonsense-augmented agents in simple recipe tasks where the full state graph provides directly useful information; belief graphs help prune action spaces and improve exploration efficiency, and incremental (evolve) graphs can be preferable to exposing full graphs depending on task.",
            "uuid": "e761.1",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "KG-DQN (Knowledge-Graph DQN)",
            "brief_description": "An approach that learns a dynamic knowledge/belief graph from text-game observations and uses it to prune the action space and guide a DQN policy.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "KG-DQN constructs a belief graph from textual observations (entities and relations) and uses that graph representation within a DQN framework to select actions and eliminate implausible actions, reducing exploration complexity in text-adventure games. It is cited as prior work that represents game state as a belief graph learned during exploration.",
            "environment_name": "Text-based games / TextWorld (general)",
            "environment_description": "Text-adventure environments treated as POMDPs where observations are textual and the true state is partially observable; large discrete action spaces and long-horizon planning make exploration difficult.",
            "is_partially_observable": true,
            "external_tools_used": "No external KB is required; relies on learned belief graph derived from observations.",
            "tool_output_types": null,
            "belief_state_mechanism": "Dynamic belief graph built from observed text; symbolic graph used to represent state and prune actions.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Belief graph is updated from incoming textual observations as the agent explores; the graph acts as the agent's memory/state representation used by the DQN.",
            "planning_approach": "Learned value-based policy (DQN) augmented with action pruning based on the belief graph; not explicit search-based planning.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Prior work demonstrates that dynamic belief graphs learned from text can reduce the action space and improve exploration efficiency in text-adventure games; such graph-based state representations inspire the paper's approach of combining belief and external commonsense graphs.",
            "uuid": "e761.2",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "LeDeepChef",
            "name_full": "LeDeepChef",
            "brief_description": "A deep RL agent for families of text-based recipe games that uses a supervised list of common food items from Freebase to help generalize to unseen recipes and ingredients.",
            "citation_title": "Ledeepchef: Deep reinforcement learning agent for families of text-based games.",
            "mention_or_use": "mention",
            "agent_name": "LeDeepChef",
            "agent_description": "LeDeepChef augments RL training with supervision using a curated list of common food items (from Freebase) to enable generalization to unseen recipes and ingredients across different house environments in text-based cooking tasks.",
            "environment_name": "Text-based cooking/recipe games (TextWorld-like)",
            "environment_description": "Recipe-based text games where the agent must collect ingredients and perform cooking actions; partial observability arises from local textual observations and unseen portions of the environment.",
            "is_partially_observable": true,
            "external_tools_used": "Freebase (list of common food items) as an external knowledge resource to supervise and assist generalization.",
            "tool_output_types": "Structured list of food entities (symbolic items).",
            "belief_state_mechanism": "Supervised use of common-item lists rather than a dynamic symbolic belief graph (as described in the LeDeepChef reference).",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": null,
            "planning_approach": "Learned policy augmented with supervised priors from an external KB (Freebase) to aid generalization.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Supervising agents with external KB-derived priors (common food items) can improve generalization to unseen recipes/environments; this is complementary to the belief-graph + commonsense approach studied in this paper.",
            "uuid": "e761.3",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "rating": 2,
            "sanitized_title": "learning_dynamic_knowledge_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Ledeepchef: Deep reinforcement learning agent for families of text-based games.",
            "rating": 2,
            "sanitized_title": "ledeepchef_deep_reinforcement_learning_agent_for_families_of_textbased_games"
        },
        {
            "paper_title": "Building dynamic knowledge graphs from text using machine reading comprehension.",
            "rating": 2,
            "sanitized_title": "building_dynamic_knowledge_graphs_from_text_using_machine_reading_comprehension"
        },
        {
            "paper_title": "Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning.",
            "rating": 1,
            "sanitized_title": "go_for_a_walk_and_arrive_at_the_answer_reasoning_over_paths_in_knowledge_bases_using_reinforcement_learning"
        },
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning.",
            "rating": 1,
            "sanitized_title": "learn_what_not_to_learn_action_elimination_with_deep_reinforcement_learning"
        }
    ],
    "cost": 0.0136625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge
2 May 2020</p>
<p>Keerthiram Murugesan keerthiram.murugesan@ibm.com 
Pushkar Shukla pushkarshukla@ttic.edu 
Mrinmaya Sachan mrinmaya@ttic.edu 
Pavan Kapanipathi kapanipa@us.ibm.com 
Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge
2 May 2020AD650EF8DCF26DE441D89D4F14C87F68arXiv:2005.00811v1[cs.AI]
In this paper, we consider the recent trend of evaluating progress on reinforcement learning technology by using text-based environments and games as evaluation environments.This reliance on text brings advances in natural language processing into the ambit of these agents, with a recurring thread being the use of external knowledge to mimic and better human-level performance.We present one such instantiation of agents that use commonsense knowledge from ConceptNet to show promising performance on two text-based environments.</p>
<p>Introduction</p>
<p>Over the years, simulation environments and games have been used extensively to showcase and drive advances in reinforcement learning technology.A recent environment that has received much focus is TextWorld (TW) (Côté et al., 2018), where an agent must interact with an external environment to achieve goals while maximizing reward -all of this using only the modality of text.TextWorld and similar text-based tasks seek to bring advances in natural language processing (NLP) and question answering solutions to agent-based reinforcement learning techniques, and vice-versa.</p>
<p>A common thread inherent in solutions to some of the NLP tasks is that mere text-based techniques cannot achieve or beat the human-level performance and that NLP systems must instead learn how to utilize additional knowledge from external sources such as knowledge bases (KBs) and knowledge graphs (KGs) to improve their overall performance.Figure 1 presents a running example that illustrates this: in the figure, the additional knowledge that must be utilized effectively by the * Both student authors contributed equally.</p>
<p>agent is presented in the bottom left corner under the ConceptNet heading.</p>
<p>In general, the use of external knowledge to improve the accuracy of NLP tasks has garnered significant attention from the community.Specifically, for tasks like natural language inference (NLI), recent work (Kapanipathi et al., 2020;Wang et al., 2019) has shown that while external knowledge can bring in useful information, this must be balanced by the context-specific relevance of the new information fed into the system.If this is not done properly, there is a very high risk of overwhelming the agent/algorithm with too much information, leading to poor decisions and performance.</p>
<p>In this paper, we present a novel approach to the use of external knowledge from the Concept-Net (Liu and Singh, 2004;Speer et al., 2017) knowledge graph to reduce the exploration space for a Reinforcement Learning (RL) agent.Specifically, we consider an RL based agent that is able to model the world around it at two levels -a local, or belief, graph that describes its current belief of the state of the world; and a global or commonsense graph of entities that are related to that state -and the interaction between those two levels.The belief graph provides the agent with a symbolic way to represent its current perception of the world, which can be easily combined with symbolic commonsense knowledge from the commonsense graph.This two-level representation of the world and the knowledge about it follows the model proposed in the Graph Aided Transformer Agent (GATA) (Adhikari et al., 2020) framework.</p>
<p>Using this model, we are able to show a significant improvement in the performance of an RL agent in a kitchen cleanup task that is set in the TextWorld setting.An example of such a kitchen cleanup task is shown in Figure 1: the agent is given an initial observation (which is used to pro-</p>
<p>Goal</p>
<p>Clean up the kitchen</p>
<p>ConceptNet</p>
<p>Apple</p>
<p>Refrigerator AtLocation</p>
<p>Plate</p>
<p>Cabinet AtLocation</p>
<p>Agent</p>
<p>Best action trajectory 1.Take the apple from the table 2. Take the plate from the table 3. Open the refrigerator 4. Put the apple in the refrigerator 5. Open the cabinet 6.Put the plate in the cabinet Plausible Actions 1. Open the cabinet 2. Eat the apple 3. Put the apple in the cabinet 4. ... duce the first iteration of the agent's belief graph), with the final goal of cleaning up the kitchen.The agent has to produce the list of actions that are necessary to achieve this goal: that list is given on the right hand side.Finally, the additional external knowledge from the ConceptNet knowledge graph -which makes up the global graph for our agent -is shown at the bottom left.In the case of this running example, the agent may discover from ConceptNet that apples are usually located in refrigerators, and plates are located in cabinets.We will use this kitchen cleanup instance as a running example throughout the paper.</p>
<p>By evaluating our approach on two different tasks -a kitchen cleanup task as above, and an additional cooking recipe task -we can show that the interaction between the belief and commonsense graphs can reduce the exploration of the RL agent in comparison to the purely text-based model.However, we are also able to demonstrate a more nuanced point: merely providing an agent with commonsense knowledge is not sufficient to improve its performance.Indeed, oftentimes it is detrimental to the agent's performance.We show that this is due to the agent being overwhelmed with too much commonsense knowledge, and discuss how different tasks and settings have different demands on the knowledge that is used by an agent.</p>
<p>Related Work</p>
<p>We start out with a look at work that is related to our focus area, which we categorize into three primary areas below.Our work sits at the intersection of knowledge graphs and the use of commonsense (and external) knowledge to make reinforcement learning more efficient; and our improvements are showcased in TextWorld and adjacent text-based domains.</p>
<p>Knowledge Graphs</p>
<p>Graphs have become a common way to represent knowledge.These knowledge graphs consist of a set of concepts (nodes) connected by relationships (edges).Well-known knowledge graphs (KGs) that are openly available include Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007), WordNet (Miller, 1995), and ConceptNet (Speer et al., 2017).Each of these KGs comprises different types of knowledge.For the tasks that our work considers, we found that the commonsense knowledge available in ConceptNet is more suitable than the encyclopedic knowledge from DBpedia or Freebase -we hence focus on this.Since our approach considers the KG as a generic graph structure, it is amenable to the use of any of the KGs mentioned here.</p>
<p>Knowledge graphs have been used to perform reasoning to improve performance in various domains, particularly within the NLP community.In particular, KGs have been leveraged for tasks such as Entity Linking (Hoffart et al., 2012), Question Answering (Sun et al., 2018;Das et al., 2017;Atzeni and Atzori, 2018), Sentiment Analysis (Recupero et al., 2015;Atzeni et al., 2018) and Natural Language Inference (Kapanipathi et al., 2020).Different techniques have been explored for their use.In most cases, knowledge graph embeddings such as TransH (Wang et al., 2014) and Com-plEx (Trouillon et al., 2016) are used to vectorize the concepts and relationships in a KG as input to a learning framework.Reinforcement learning has also been used to find relevant paths in a knowledge graph for knowledge base question answering (Das et al., 2017).Sun et al. (2018) and Kapanipathi et al. (2020) find sub-graphs from the corresponding KGs and encode them using a graph convolutional networks (Kipf and Welling, 2016) for question answering and natural language inference respectively.</p>
<p>External Knowledge for Sample Efficient Reinforcement Learning</p>
<p>A key challenge for current reinforcement learning (RL) technology is the low sample efficiency (Kaelbling et al., 1998).RL techniques require a large amount of interaction with the environment which can be very expensive.This has prevented the use of RL in real-world decision-making problems.In contrast, humans possess a wealth of commonsense knowledge which helps them solve problems in the face of incomplete information.Inspired by this, there have been a few recent attempts on adding prior or external knowledge to RL approaches.Notably, Garnelo et al. (2016) propose Deep Symbolic RL, which combines aspects of symbolic AI with neural networks and reinforcement learning as a way to introduce common sense priors.However, their work is mainly theoretical.There has also been some work on policy transfer (Bianchi et al., 2015), which studies how knowledge acquired in one environment can be re-used in another environment; and experience replay (Wang et al., 2016;Lin, 1992Lin, , 1993) ) which studies how an agent's previous experiences can be stored and then later reused.In contrast to the above, in this paper, we explore the use of commonsense knowledge stored in knowledge graphs such as ConceptNet as a way to improve sample efficiency in text-based RL agents.To the best of our knowledge, there is no prior work that explores how commonsense knowledge can be used to make RL agents more efficient.</p>
<p>RL Environments and TextWorld</p>
<p>Games are a rich domain for studying grounded language and how information from the text can be utilized in controlled applications.Notably, in this line of research, Branavan et al. ( 2012) builds an RL-based game player that utilizes text manuals to learn strategies for Civilization II; and Narasimhan et al. (2015) build an RL-based game player for multi-user Dungeon games.In both cases, the text is analyzed and control strategies are learned jointly using feedback from the gaming environment.Similarly, in the vision domain, there has been work on building automatic video game players (Koutník et al., 2013;Mnih et al., 2016).</p>
<p>Our work builds on a recently introduced text-based game TextWorld (Côté et al., 2018).TextWorld is a sandbox learning environment for training and evaluating RL-based agents on textbased games.Since its introduction and other such tools, there has been a large body of work devoted to improving performance on this benchmark.One interesting line of work on TextWorld is on learning symbolic (typically graphical) representations of the agent's belief of the state of the world.Notably, Ammanabrolu and Riedl (2019) proposed KG-DQN and Adhikari et al. ( 2020) proposed GATA; both represent the game state as a belief graph learned during exploration.This graph is used to prune the action space, enabling more efficient exploration.Similar approaches for building dynamic belief graphs have also been explored in the context of machine comprehension of procedural text (Das et al., 2018).In our work, we also represent the world as a belief graph.Moreover, we also explore how the belief graph can be used with commonsense knowledge for efficient exploration.</p>
<p>The LeDeepChef system (Adolphs and Hofmann, 2019), which investigates the generalization capabilities of text-based RL agents as they learn to transfer their cooking skills to neverbefore-seen recipes in unfamiliar house environments, is also related to our work.They achieve transfer by additionally supervising the model with a list of the most common food items in Freebase, allowing their agent to generalize to hitherto unseen recipes and ingredients.</p>
<p>Finally, Zahavy et al. (2018) propose the Action-Elimination Deep Q-Network (AE-DQN) which learns to predict invalid actions in the textadventure game Zork, and eliminates them using contextual bandits.This allows the model to efficiently handle the large action space.The use of common sense knowledge in our work potentially has the same effect of down-weighting implausible actions.</p>
<p>TextWorld as a POMDP</p>
<p>Text-based games can be seen as partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998) where the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state.As an agent interacts with a TextWorld game instance, at each turn, several lines of text describe the state of the game; and the player can issue a text command to change the state in some desirable way (typically, in order to move towards a goal).</p>
<p>Formally, let (S, T, A, Ω, O, R, γ) denote the underlying TextWorld POMDP.Here, S denotes the set of states, A denotes the action space, T denotes the state transition probabilities, Ω denotes the set of observations, O denotes the set of conditional observation probabilities, and γ ∈ [0, 1] is the discount factor.The agent's observation o t at time step t depends on the current state s t and the previous action a t−1 .The agent receives a reward at time step t: r t = R(s t , a t ) and the agent's goal is to maximize the expected discounted sum of rewards:
E[ ∑ t γ t r t ].
TextWorld allows the agent to perceive and interact with the environment via the modality of text.Thus, the observation o t is presented by the environment as a sequence of tokens (o t = {o 1 t , . . .o N t }).Similarly, each action a is also denoted as a sequence of tokens {a 1 , . . ., a M }.</p>
<p>Model Description</p>
<p>In order to solve the above POMDP, we design a model that can leverage commonsense knowledge and learn a graph-structured representation of its belief of the world state.The high-level architecture of the model contains three major components, namely the input encoder, a graphbased knowledge extractor, and the action prediction module.The input encoding layers are used for encoding the observation at time step t and the list of admissible actions.The graph-based knowledge extractor tries to extract knowledge from two different sources.First, it makes use of external commonsense knowledge to improve the ability of the agent to select the correct action at each time step.Secondly, the belief about the environment (world state) perceived by the agent is also captured by a belief graph that is generated dynamically from the textual observations from the game.The information from both sources is then aggregated together in a single graph.The actionprediction module takes as input the encoded ob-servation states, the encoded list of admissible actions and the encoded aggregated graph, and predicts an action for each step.Figure 2 provides a compact visualization of our approach.We describe the various components of our model below.</p>
<p>Input Encoder</p>
<p>At any time step t, the agent observes a textual description of the current state provided as a sequence of tokens o t = (o 1 t , . . ., o N t ).Given the current observation o t , we use pre-trained GloVe embeddings (Pennington et al., 2014)
i = (a 1 i , . . . , a M i ) ∈ A t as a sequence of d-dimensional pretrained GloVe embeddings c 1 i , . . . , c M i .
The agent relies on a hierarchical encoder architecture to model the current state as a vector s t , based on o t and the previous observations.First, a GRU-based encoder is used to process the sequence x 1 t , . . ., x N t of the GloVe embeddings associated with o t .This allows representing the current observation as a single h-dimensional vector o t ∈ R h , where h is the output dimensionality of the GRU.Formally, o t is computed as o t = h N t , with h k t = GRU(h k−1 t , x k t ), for k = 1, . . ., N. In the previous equation, GRU(•) refers to the forward propagation of a gated recurrent unit (Cho et al., 2014).Then, the sequence of previous observations up to o t is encoded in a similar way into a vector s t = GRU(s t−1 , o t ) ∈ R h .We do the same to represent each admissible action a i as
a i = a M i , with a k i = GRU(a k−1 i , c k i ), for k = 1, . . . , M.</p>
<p>Graph-based Knowledge Integration</p>
<p>We enhance our text-based RL agent by allowing it to access a graph that captures both commonsense knowledge and the agent's current belief of the world state.Formally, we assume that, at each time step t, the agent has access to a graph G t = (V t , E t ), where V t is the set of nodes and E t ⊆ V 2 t denotes the edges of the graph.The graph is updated dynamically at each time step t and new nodes are either added or deleted based on the textual observation o t .</p>
<p>As mentioned, G t encodes both commonsense knowledge and the belief of the world state.Commonsense knowledge is extracted from the history of the observations by linking the entities mentioned in the text to an external KG.This allows extracting a commonsense knowledge graph, which is a subgraph of the external source of knowledge providing information about the entities of interest.In our experiments, we use Con-ceptNet (Speer et al., 2017) as the external knowledge graph.On the other hand, the observation o t is also used to update a dynamically generated belief graph as in recent work by Adhikari et al. (2020).The graph aggregation is performed by merging the belief and commonsense knowledge graphs based on the entity mentions.This helps to reduce the noise extracted from updating both the belief and the commonsense graphs.As shown in Figure 2, the commonsense knowledge graph, and the belief graph are updated based on the observations, and then they are aggregated to form a single graph G t .The graph G t at time step t is processed by a graph encoder as follows.First, pretrained KG embeddings are used to map the set of nodes V t into a feature matrix
E t = [e 1 t , . . . , e |V t | t ] ∈ R f ×|V t | ,
where each column e i t ∈ R f is the embedding of node i ∈ V t .We use Numberbatch embeddings (Speer et al., 2017) to create the matrix E t .Such feature matrix provides initial node embeddings that are iteratively updated by message passing between the nodes of G t , using L stacked GCN (graph convolutional network) layers (Kipf and Welling, 2016), where L is an hyperparameter of the model.The output of this process is an updated matrix
Z t = [z 1 t , . . . , z |V | t ] ∈ R h×|V t | .
We then compute a graph encoding g t for G t by simply averaging over the columns of Z t , namely:
g t = 1 |V t | |V t | ∑ i=1 z i t .
In our experiments, we use the updated KG embeddings to create a graph-based encoding vector for each action as described in Section 4.1, in addition to the graph encoding g t .This approach has shown to be a better integration of the knowledge graph at each time step.</p>
<p>Action Prediction</p>
<p>The action ât selected by the agent at time step t is computed based on the state embedding s t , the graph embedding g t and the action candidates a 1 , . . ., a |A t | .First, all these vectors are concatenated together into a single vector r t = [g t ; s t ; a 1 ; . . .; a |A t | ].Then, we compute a vector p t ∈ R |A t | with a probability score for each action a i ∈ A t as:
p t = so f tmax(W 1 • ReLU(W 2 • r t + b 2 ) + b 1 )
where W 1 ,W 2 , b 1 , and b 2 are learnable parameters of the model.The final action chosen by the agent is then given by the one with the maximum probability score, namely ât = arg max i p t,i .</p>
<p>Learning</p>
<p>Following the winning strategy in the First-TextWorld competition (Adolphs and Hofmann, 2019), we use the Advantage Actor-Critic (A2C) framework (Mnih et al., 2016) to train the agent and optimize the action selector on reward signals from training games.</p>
<p>Experiments</p>
<p>In this section, we report on experiments to study the role of commonsense knowledge-based RL agents in the TextWorld environment.We evaluate and compare our agent on two sets of game instances: 1) Kitchen Cleanup Task, and 2) Cooking Recipe Task.</p>
<p>Kitchen Cleanup Task</p>
<p>First, we use TextWorld (Côté et al., 2018) to generate a game/task to assess the performance gain using commonsense knowledge graphs such as ConceptNet.We generate the game with 10 objects relevant to the game, and 5 distractor objects spread across the room.The goal of the agent is to tidy the room (kitchen) by putting the objects in the right place.We create a set of realistic kitchen cleanup goals for the agent: for instance, take apple from the table and put apple inside the refrigerator.Since information on concepts that map to the objects in the room is explicitly provided in ConceptNet (Apple → AtLocation → Refrigerator), the main hypothesis underlying the creation of this game is that leveraging the commonsense knowledge could allow the agent to achieve a higher reward while reducing the number of interactions with the environment.</p>
<p>The agent is presented with the textual description of a kitchen, consisting of the location of different objects in the kitchen and their spatial relationship to the other objects.The agent uses this information to select the next action to perform in the environment.Whenever the agent takes an object and puts it in the target location, it receives a reward and its total score goes up by one point.The maximum score that can be achieved by the agent in this kitchen cleanup task is equal to 10.In addition to the textual description, we extract the commonsense knowledge graph from ConceptNet based on the text description.Figure 3 shows an instance of the commonsense knowledge graph created during the agent's interaction with the environment.Note that even for the simple kitchen cleanup task that we model (see Figure 1 for details), the commonsense knowledge graph contains more than 20 entities (nodes) and a similar number of relations (edges).This visualization is useful, as it lends a basis for our upcoming discussion on agents being overwhelmed with too much commonsense knowledge.</p>
<p>Results on Kitchen Cleanup</p>
<p>We compare our knowledge-aware RL agents (KG Full and KG Evolve) against two baselines for performance comparison: Random, where the agent chooses an action randomly at each step; and Simple, where the agent chooses the next action using the text description alone and ignores the commonsense knowledge graph.The knowledgeaware RL agents, on the other hand, use the commonsense knowledge graph to choose the next action.The graph is provided in either full-graph setting where all the commonsense relationships between the objects are given at the beginning of the game (KG Full); or evolve-graph setting where only the commonsense relationship between the objects seen/interacted by the agent until the current steps are revealed (KG Evolve).We record the average score achieved by each agent and the average number of interactions (moves) with the environment as our evaluation metrics.Figure 4 shows the results for the kitchen cleanup task averaged over 5 runs, with 500 episodes per run.</p>
<p>Discussion of Kitchen Cleanup</p>
<p>As expected, we see that agents that use the textual description and additionally the commonsense knowledge outperform the baseline random agent.We are also able to demonstrate clearly that the knowledge-aware agent outperforms the simple agent with the help of commonsense knowledge.The knowledge-aware agent with the evolve-graph setting outperforms both the simple agent as well as the agent with the full-graph setting.We believe that when an agent has access to the full commonsense knowledge graph at the beginning of the game, the agent gets overwhelmed by the amount of knowledge given; and is prone to making noisy explorations in the environment.On the other hand, feeding the commonsense knowledge gradually during the agent's learning process provides more focus to the exploration, and drives it toward the concepts related to the rest of the goals.These results can also be seen as an RL-centric agent-based validation of similar results shown in the broader NLP literature by the work of (Kapanipathi et al., 2020).</p>
<p>Cooking Recipe Task</p>
<p>Next, we evaluate the performance of our agent on the cooking recipe task by using 20 different games generated by (Adhikari et al., 2020).These games follow a recipe-based cooking theme, with a single ingredient in a single room (difficulty level 1).The goal is to collect that specific ingredient to prepare a meal from a given recipe.As in our previous task, we compare our agent with the Simple agent.In addition to the simple agent, we compare our agent with the GATA agent (Adhikari et al., 2020) which uses the belief graph for effective planning and generalization.As used throughout this paper, the belief graph represents the state of the current game based on the textual description from the environment.Similar to commonsense knowledge, the belief graph can be fed to the agent as a full-graph (GATA Full) or an evolve-graph (GATA Evolve) and then aggregated as the current graph.It is worth noting that the full belief graph is considered as the ground truth state information in the TextWorld environment: it is the graph used by the TextWorld environment internally to modify the state information and the list of admissible actions.On the other hand, the evolve-belief graph is generated based on the observed state information.</p>
<p>Results on Cooking Recipe</p>
<p>We compare both the simple and GATA agents against our agent which uses the commonsense knowledge extracted from ConceptNet.As before, we consider a full-graph setting and evolvegraph setting where either the full commonsense knowledge graph is available at the beginning of the game, or it is fed incrementally as the game proceeds, respectively.For this task, we aggregate the commonsense knowledge graph with the belief graph (Belief+KG Full and Belief+KG Evolve).</p>
<p>Figure 5 shows the results for the Cooking recipe task averaged over 5 runs and 20 games, with 100 episodes per run.As before, all the agents outperform the simple agent, which shows that using different state representations such as the belief graph and additional information such as commonsense knowledge improves the performance of an agent.</p>
<p>Discussion of Cooking Recipe</p>
<p>We observe that the evolve-graph setting for both the GATA and Belief+KG performs better than the Belief+KG Full, as feeding more information can lead to noisy exploration as observed in the earlier task.More interestingly, we observe that GATA Full performs significantly better than the other agents.We believe that the reason for this result is the difficulty of the task at hand, and the process via which these cooking games are generated.Since the cooking recipe task (difficulty level 1) entails retrieving a single ingredient from the same room that the agent is present in, there are no meaningful concepts related to the current state that can be leveraged from the commonsense knowledge for better exploration.Even with the difficult task setting in this game environment (dif-ficulty level 10 with 3 ingredients spread across 6 rooms), the ingredients are randomly chosen and spread across the rooms.In such a game setting, the ground truth full belief graph is more beneficial than the commonsense knowledge graph.This is an interesting negative result, in that it shows that there can still be scenarios and domains where commonsense knowledge may not necessarily help an agent.We are actively exploring further settings of the cooking recipe task in order to understand and frame this effect better.</p>
<p>Conclusion</p>
<p>Previous approaches for text-based games like TextWorld primarily focused on text understanding and reinforcement learning for learning control policies and were thus sample inefficient.In contrast, humans utilize their commonsense knowledge to efficiently act in the world.As a step towards bridging this gap, we investigated the novel problem of using commonsense knowledge to build efficient RL agents for text-based games.</p>
<p>We proposed a technique that symbolically represents the agent's belief of the world, and then combines that belief with commonsense knowledge from the ConceptNet knowledge graph in order to act in the world.We evaluated our approach on multiple tasks and environments and showed that commonsense knowledge can help the agent act efficiently and accurately.We also showcased some interesting negative results with respect to agents being overwhelmed with too much commonsense knowledge.We are currently actively studying this problem, and future work will report in more detail on this phenomenon.</p>
<p>Figure 1 :
1
Figure 1: An illustration of our Kitchen Cleanup game.The agent perceives the world via text and has been given the goal of cleaning up the kitchen.As shown here, the agent can leverage commonsense knowledge from ConceptNet to reduce the exploration and achieve the goal.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Example of the commonsense knowledge graph extracted from ConceptNet for the Kitchen Cleanup Task</p>
<p>Figure 5 :
5
Figure 5: Comparison of agents for the Cooking Recipe task with belief graph and/or commonsense knowledge graph (averaged over 5 runs).</p>
<p>Observation</p>
<p>You've entered a kitchen.You see a closed cabinet and a refrigerator.Here's a dining table.You see a plate and an apple on the table.</p>
<p>to represent o t as a sequence of d-dimensional vectors x 1 t , . . ., x N t , where each x k t ∈ R d is the glove embedding of the k-th observed token o k t , k = 1, . . ., N. Similarly, given the set A t of admissible actions at time step t, we represent each action a</p>
<p>AcknowledgmentsWe thank Sadhana Kumaravel, Gerald Tesauro, and Murray Campbell for their feedback and help with this work.
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L Hamilton, arXiv:2002.09127Learning dynamic knowledge graphs to generalize on textbased games. 2020arXiv preprint</p>
<p>Ledeepchef: Deep reinforcement learning agent for families of text-based games. Leonard Adolphs, Thomas Hofmann, ArXiv, abs/1909.016462019</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark Riedl, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>What is the cube root of 27? Question answering over codeontology. Mattia Atzeni, Maurizio Atzori, The Semantic Web -ISWC 2018 -17th International Semantic Web Conference. Lecture Notes in Computer Science. Springer2018</p>
<p>Using frame-based resources for sentiment analysis within the financial domain. Mattia Atzeni, Amna Dridi, Diego Reforgiato Recupero, Progress in AI. 742018</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The semantic web. 2007</p>
<p>Transferring knowledge as heuristics in reinforcement learning: A case-based approach. Reinaldo Ac Bianchi, Luiz A CelibertoJr, Paulo E Santos, Jackson P Matsuura, Ramon Lopez De Mantaras, Artificial Intelligence. 2262015</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of dataAcM2008</p>
<p>Learning to win by reading manuals in a monte-carlo framework. David Srk Branavan, Regina Silver, Barzilay, Journal of Artificial Intelligence Research. 432012</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, C ¸aglar Gülc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 10.3115/v1/d14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, CoRR, abs/1806.115322018</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Workshop on Computer Games. Springer2018</p>
<p>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew Mccallum, arXiv:1711.058512017arXiv preprint</p>
<p>Building dynamic knowledge graphs from text using machine reading comprehension. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, Andrew Mccallum, CoRR, abs/1810.056822018</p>
<p>Marta Garnelo, Kai Arulkumaran, Murray Shanahan, arXiv:1609.05518Towards deep symbolic reinforcement learning. 2016arXiv preprint</p>
<p>Kore: keyphrase overlap relatedness for entity disambiguation. Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, Gerhard Weikum, Proceedings of the 21st ACM international conference on Information and knowledge management. the 21st ACM international conference on Information and knowledge management2012</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Infusing knowledge into the textual entailment task using graph convolutional networks. Pavan Kapanipathi, Veronika Thost, Sankalp Siva, Spencer Patel, Ibrahim Whitehead, Avinash Abdelaziz, Maria Balakrishnan, Kshitij Chang, Chulaka Fadnis, Bassem Gunasekara, Nicholas Makni, Mattei, 2020AAAIKartik Talamadupula, and Achille Fokoue</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Evolving largescale neural networks for vision-based reinforcement learning. Jan Koutník, Giuseppe Cuccu, Jürgen Schmidhuber, Faustino Gomez, Proceedings of the 15th annual conference on Genetic and evolutionary computation. the 15th annual conference on Genetic and evolutionary computation2013</p>
<p>Self-improving reactive agents based on reinforcement learning, planning and teaching. Long-Ji Lin, Machine learning. 83-41992</p>
<p>Reinforcement learning for robots using neural networks. Long-Ji Lin, 1993Carnegie-Mellon Univ Pittsburgh PA School of Computer ScienceTechnical report</p>
<p>Conceptnet-a practical commonsense reasoning tool-kit. Hugo Liu, Push Singh, BT technology journal. 2242004</p>
<p>Wordnet: a lexical database for english. George A Miller, Communications of the ACM. 38111995</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. 2016</p>
<p>Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.08941Language understanding for textbased games using deep reinforcement learning. 2015arXiv preprint</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, 10.3115/v1/d14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Sentilo: Frame-based sentiment analysis. Diego Reforgiato Recupero, Valentina Presutti, Sergio Consoli, Aldo Gangemi, Andrea Giovanni Nuzzolese, 10.1007/s12559-014-9302-zCognitive Computation. 722015</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robert Speer, Joshua Chin, Catherine Havasi, AAAI. 2017</p>
<p>Open domain question answering using early fusion of knowledge bases and text. Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018Ruslan Salakhutdinov, and William Cohen</p>
<p>Complex embeddings for simple link prediction. Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, Guillaume Bouchard, International Conference on Machine Learning. 2016</p>
<p>Improving natural language inference using external knowledge in the science questions domain. Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Knowledge graph embedding by translating on hyperplanes. Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen, AAAI. 2014</p>
<p>Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando De Freitas, arXiv:1611.01224Sample efficient actorcritic with experience replay. 2016arXiv preprint</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, Advances in Neural Information Processing Systems. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
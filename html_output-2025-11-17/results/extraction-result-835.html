<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-835 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-835</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-835</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-265506147</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.18751v3.pdf" target="_blank">Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web</a></p>
                <p><strong>Paper Abstract:</strong> Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise of small-scale finetuned and transferred models for task compositionality, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e835.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e835.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive Criticism and Improvement (RCI) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompted language-model agent that generates open-loop plans from few-shot exemplars and uses iterative self-criticism (explicit RCI) and grounding/refinement loops (implicit RCI) to improve plans before stepwise execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can solve computer tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>RCI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting-based agent pipeline: few-shot plan generation, explicit self-criticism/improvement loop (ERCI), and implicit grounding/format-refinement loop (IRCI) before stepwise execution. Uses a backbone LLM for planning and criticism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MiniWoB (base tasks) and CompWoB (compositional web automation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web automation / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>MiniWoB: ~90.6% (reported for RCI with standard setup); CompWoB (compositional): 28.7% (RCI with combined exemplars, gpt-3.5 backbone) ; with gpt-4 backbone RCI improved to ~56.0% on compositional tasks (reported aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Self-refinement (ERCI) and grounding/format refinement (IRCI) loops; few-shot exemplar conditioning; plan-then-execute pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / few-shot in-context learning (no finetuning reported for RCI in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (self-criticism iterations) and scaling to stronger backbone LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated varying numbers of ERCI and IRCI loops (explicit and implicit self-improvement iterations). Also evaluated using stronger backbone LLMs (gpt-4) instead of gpt-3.5-turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Self-improvement loop counts affect per-task optimality (recommended ERCI=1, IRCI=3). Using gpt-4 improved RCI compositional success from ~28.7% (gpt-3.5) to ~56.0% on CompWoB; oracle exemplars plus gpt-4 achieved ~82.9% on 20 two-way tasks (sanity check).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Prompted RCI overfits exemplar styles (linear 'left-to-right' instruction formats); long/complex instructions and deeper HTML trees increase planning difficulty; prompting methods lack robustness to novel task compositions and instruction permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e835.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e835.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdaPlanner (Adaptive planning from feedback with language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompted agent that synthesizes a plan as executable code (Python function) and adaptively re-generates plans in closed-loop based on runtime/environmental feedback (assertion/functional errors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaplanner: Adaptive planning from feedback with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>AdaPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted LMA that uses LLM program synthesis to produce an open-loop plan encoded as a Python function; monitors runtime feedback (assertions, errors) and regenerates plan for remaining steps (closed-loop).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MiniWoB (base tasks) and CompWoB (compositional web automation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web automation / sequential decision-making / programmatic planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>MiniWoB: ~92.9% (reported); CompWoB: ~20.6% average (reported for prompted AdaPlanner with gpt-3.5 backbone); improved but still low with stronger LLMs (text-davinci-003 used as coder backbone yielded some gains in other experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Program-synthesis-based planner (code-as-plan), closed-loop replanning on runtime errors, uses code-aware LLMs for better plan fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / few-shot in-context learning (no additional finetuning reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (program synthesis) and backbone selection (code-capable LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>AdaPlanner relies on LLMs with stronger code generation ability (e.g., text-davinci-003 performed better than gpt-3.5-turbo in prior reports); evaluated using provided exemplars for included base tasks and zero-shot otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>AdaPlanner attains strong base-task performance but experiences large degradation on compositional CompWoB tasks (from ~92.9% on MiniWoB down to ~20% on CompWoB); using stronger code-capable LLMs gives some improvement but does not close the compositionality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Program-synthesis plans can be brittle when tasks compose unexpectedly; exemplars and program conditioning bias linear workflows and do not generalize to novel compositions or instruction permutations; long instructions and HTML structure increase brittleness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e835.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e835.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synapse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synapse (structured prompting LMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompted agent using task-specific structured prompts (instruction/HTML reformulations and state-conditional exemplars) and retrieval to produce open-loop plans, aiming to reduce translation/hallucination errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synapse: Leveraging few-shot exemplars for human-level computer control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Synapse</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted pipeline with structured task reformulation categories, translation of HTML/instructions, and state-conditional decomposition into exemplars; selects prompts/exemplars per task and predicts open-loop plans.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MiniWoB (base tasks) and CompWoB (compositional web automation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web automation / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>MiniWoB: ~98.5% (reported best among prompted LMAs on base tasks); CompWoB: ~15-25% (individual reported numbers vary; Synapse showed a large drop on compositional tasks, e.g., ~17.8% reported in aggregates).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Structured prompting with multiple reformulation types, translation of instruction/HTML, state-conditional exemplars and retrieval, open-loop planning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / few-shot in-context learning (no finetuning in this evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompt engineering / structured prompting and exemplar retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Uses a taxonomy of reformulation strategies per task and state-conditional decomposition to reduce error accumulation; assumes optimal exemplar retrieval for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Highly effective on base MiniWoB tasks but performance collapses on unseen compositional CompWoB tasks (large generalization gap). Using stronger LLM backbones (gpt-4) improves but does not eliminate the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Overfitting of structured prompts and exemplars to base tasks and linear instruction formats; difficulty translating longer instructions and deeper HTML leads to errors (XPath hallucination, incorrect action types) in compositional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e835.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e835.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HTML-T5++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HTML-T5++ (data-rebalanced finetuned LMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuned, transferred LMA based on HTML-T5-XL that uses balanced demonstration data (data-rebalancing) to improve generalization to compositional tasks, achieving super-human MiniWoB performance and strong zero-shot CompWoB transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>HTML-T5++</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Finetuned encoder-decoder model (HTML-T5-XL backbone) with local and global attention in encoder and long-span denoising pretraining; closed-loop policy predicting next action tokens from HTML and action history.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>â‰ˆ3B (HTML-T5-XL backbone; paper states tractable-size models up to ~3B used for transferred LMAs)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MiniWoB (base tasks) and CompWoB (compositional web automation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web automation / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>MiniWoB (after data rebalancing/finetuning): 95.2% average success (super-human); CompWoB (zero-shot): 61.5% average success (best zero-shot among methods in this study). Reverse-order instructions: average success dropped from 54.8% to 31.0% for transferred LMA group (HTML-T5++ part of that group).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Encoder with local+global attention, mixture of long-span denoising pretraining, closed-loop action prediction (text-format actions)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised finetuning on demonstration datasets (agent-generated and human), with a data-rebalancing strategy to mitigate task-data imbalance</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training-method intervention (data rebalancing / supervised finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Collected additional demonstrations (77K) and applied multiple data-mixing strategies removing redundant/easy-task episodes to balance training distribution by task difficulty; finetuned HTML-T5-XL on the rebalanced dataset to produce HTML-T5++.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Data rebalancing improved MiniWoB success from ~85.6% to 95.2% (HTML-T5++). On zero-shot CompWoB, HTML-T5++ achieved 61.5%, substantially better than prompted LMAs (24.9% average), reducing the generalization gap compared to prompted agents.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Finetuned models generalize better to novel compositions because supervised exposure teaches compositional patterns; remaining gaps are attributed to instruction length and HTML subtree depth which increase planning ambiguity and sparsity, and to distributional mismatch for reverse-order or out-of-distribution instruction formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e835.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e835.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HTML-T5 (original)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HTML-T5 (pretrained and finetuned LMA baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-derived model adapted to HTML inputs (tokenized HTML) with local and global attention, used as a finetuned closed-loop policy for web automation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding html with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>HTML-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained text-to-text transformer adapted for HTML understanding via specialized pretraining; used as a finetuned agent that maps HTML+history to next action text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XL (prior work HTML-T5-XL; model in this line is ~3B reported elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MiniWoB (base tasks) and CompWoB (compositional tasks as transfer target)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web automation / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Prior HTML-T5 reported competitive MiniWoB performance (e.g., 85.4% reported as transferred LMA baseline in aggregate); after rebalancing finetuning (HTML-T5++), performance improved to 95.2% on MiniWoB and 61.5% on CompWoB.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Local+global attention encoder tailored to HTML representation, text decoder outputting action sequences</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Pretraining (long-span denoising) then supervised finetuning on web automation demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>finetuning / data rebalancing (in HTML-T5++ extension)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>HTML-T5 serves as base; data-rebalancing strategies described in this paper (removing overrepresented easy-task episodes and adding missing demonstrations) improved performance when finetuned to produce HTML-T5++.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Original HTML-T5 performance improved substantially after dataset rebalancing and finetuning (see HTML-T5++ entry).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Original finetuned models trained on imbalanced datasets underperform on certain tasks; balancing training data and increasing coverage of harder tasks improves compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e835.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e835.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XXL (finetuned example from Appendix K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-XXL (finetuned model used in reasoning compositionality experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-finetuned T5 variant (11B) used in Appendix K to demonstrate that a finetuned LLM can outperform prompted LLMs on a compositional reasoning benchmark (Last Letter Concatenation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Flan-T5-XXL (11B, finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned encoder-decoder model (11B) finetuned on supervised compositional reasoning examples (Last Letter Concatenation) to evaluate compositional generalization versus prompted models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Last Letter Concatenation (compositional natural-language reasoning benchmark; not interactive/web automation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step compositional reasoning (natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Finetuned Flan-T5-XXL achieved average exact-match performance ~76% on Last Letter Concatenation (both in-distribution and out-of-distribution), outperforming few-shot prompted Flan-PaLM-540B (~44% exact match) in the paper's appendix experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Instruction-tuned encoder-decoder transformer (T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised finetuning on many compositional examples (2000 examples per n-letter up to 8 letters)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training-method (supervised finetuning) used to improve compositional generalization versus prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Finetuned on extensive supervised compositional examples (Last Letter Concatenation) while prompted LLMs used few-shot CoT exemplars. This tests the hypothesis that finetuning improves compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Finetuned model (Flan-T5-XXL) outperformed prompted models (Flan-PaLM family) both in-distribution and out-of-distribution (2-8 vs 9-12 letters), supporting that finetuning can reduce compositional generalization gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Prompting-only methods (few-shot) may not learn the compositional mapping as robustly as supervised finetuning across many exemplars; finetuning imparts stronger inductive bias for compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e835.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e835.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (backbone LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more capable generalist large language model used as a backbone LLM for prompted LMAs; improves base and compositional task performance but does not fully close the compositionality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 Technical Report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>gpt-4 (backbone LLM used in prompted LMAs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large, general-purpose LLM used for planning, code-generation, and criticism in prompted LMA pipelines (RCI, Synapse etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>CompWoB (compositional web automation tasks) as backbone for prompted LMAs</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web automation / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Using gpt-4 as backbone improved prompted agent performance on CompWoB (e.g., RCI improved from ~28.7% with gpt-3.5 to ~56.0% with gpt-4 in reported aggregates), but still fell short of base-task performance and finetuned transferred models.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>backbone model scaling (using a stronger LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Replacing gpt-3.5-turbo backbone with gpt-4 in prompted LMAs (RCI, Synapse) and evaluating compositional task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Noticeable improvements in prompted LMA compositional success but insufficient to match finetuned LMA performance; for RCI, compositional success rose to ~56% with gpt-4 (from ~28.7%). Reverse-order instruction robustness also improved but remained poor (e.g., RCI reverse-order from ~19.2% to ~43.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Even stronger generalist LLMs lack sufficient inductive biases or task-specific training to generalize reliably to novel sequential compositions and instruction permutations; scaling helps but does not remove brittleness from prompting pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e835.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e835.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted vs Finetuned LMAs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted LMAs (in-context prompting) vs Transferred/Finetuned LMAs (supervised finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate finding: prompted LMAs (RCI, AdaPlanner, Synapse) achieve human-level performance on base MiniWoB tasks via few-shot prompting, but suffer large degradation on compositional CompWoB tasks; finetuned/transferred LMAs (HTML-T5++) generalize better to compositional tasks, especially after data-rebalancing finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Prompted LMAs (RCI/AdaPlanner/Synapse) vs Transferred (HTML-T5++)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Comparison between prompting-based agent pipelines (few-shot, exemplar-conditioned, self-critique/program-synthesis/structured prompts) and supervised finetuned LLM agents trained on demonstration datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MiniWoB (base tasks) and CompWoB (compositional tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web automation / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Prompted LMAs: MiniWoB avg ~94.0% (base) but CompWoB avg ~24.9% (large drop). Transferred/finetuned LMAs: MiniWoB avg ~85.4% (pre-rebalance) improved to 95.2% with rebalancing (HTML-T5++), and CompWoB zero-shot avg ~54.8% (transferred) with HTML-T5++ at 61.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Prompted LMAs: exemplar-conditioned planning, self-critique loops, code-as-plan modules, structured reformulation. Finetuned LMAs: encoder-decoder with HTML-aware attention, closed-loop action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting/few-shot for prompted LMAs; supervised finetuning with demonstration datasets (and data rebalancing) for transferred LMAs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training-method intervention (data rebalancing/finetuning) and prompting interventions (self-critique loops, program-synthesis prompting, structured prompts); backbone scaling (gpt-4) also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Paper evaluates prompting variants (RCI ERCI/IRCI loop counts, exemplar selection strategies), program-synthesis prompting (AdaPlanner), structured prompting (Synapse), and a training intervention (dataset rebalancing + finetuning to produce HTML-T5++). Also tests backbone scaling to gpt-4/text-davinci-003 and oracle exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Prompting methods obtain high base-task scores but very low zero-shot compositional performance (~24.9%); finetuning with data rebalancing (HTML-T5++) achieves super-human base-task performance (95.2%) and substantially better zero-shot compositional performance (61.5%). Backbone scaling (gpt-4) partially improves prompting results but does not eliminate compositional failures.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Prompting methods rely on exemplar/task-format priors and linear instruction biases that do not capture combinatorial task structures; finetuning provides stronger inductive bias for compositionality but still struggles when instruction order or HTML complexity diverge (long instructions, deep HTML trees).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can solve computer tasks <em>(Rating: 2)</em></li>
                <li>Adaplanner: Adaptive planning from feedback with language models <em>(Rating: 2)</em></li>
                <li>Synapse: Leveraging few-shot exemplars for human-level computer control <em>(Rating: 2)</em></li>
                <li>Understanding html with large language models <em>(Rating: 2)</em></li>
                <li>World of bits: An open-domain platform for web-based agents <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-835",
    "paper_id": "paper-265506147",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "RCI",
            "name_full": "Recursive Criticism and Improvement (RCI) agent",
            "brief_description": "A prompted language-model agent that generates open-loop plans from few-shot exemplars and uses iterative self-criticism (explicit RCI) and grounding/refinement loops (implicit RCI) to improve plans before stepwise execution.",
            "citation_title": "Language models can solve computer tasks",
            "mention_or_use": "use",
            "model_or_agent_name": "RCI",
            "model_description": "Prompting-based agent pipeline: few-shot plan generation, explicit self-criticism/improvement loop (ERCI), and implicit grounding/format-refinement loop (IRCI) before stepwise execution. Uses a backbone LLM for planning and criticism.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MiniWoB (base tasks) and CompWoB (compositional web automation tasks)",
            "interactive_task_type": "web automation / sequential decision-making",
            "interactive_performance": "MiniWoB: ~90.6% (reported for RCI with standard setup); CompWoB (compositional): 28.7% (RCI with combined exemplars, gpt-3.5 backbone) ; with gpt-4 backbone RCI improved to ~56.0% on compositional tasks (reported aggregate)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Self-refinement (ERCI) and grounding/format refinement (IRCI) loops; few-shot exemplar conditioning; plan-then-execute pipeline",
            "training_method": "Prompting / few-shot in-context learning (no finetuning reported for RCI in this paper)",
            "intervention_type": "prompting strategy (self-criticism iterations) and scaling to stronger backbone LLMs",
            "intervention_description": "Evaluated varying numbers of ERCI and IRCI loops (explicit and implicit self-improvement iterations). Also evaluated using stronger backbone LLMs (gpt-4) instead of gpt-3.5-turbo.",
            "intervention_effect": "Self-improvement loop counts affect per-task optimality (recommended ERCI=1, IRCI=3). Using gpt-4 improved RCI compositional success from ~28.7% (gpt-3.5) to ~56.0% on CompWoB; oracle exemplars plus gpt-4 achieved ~82.9% on 20 two-way tasks (sanity check).",
            "hypothesized_cause_of_gap": "Prompted RCI overfits exemplar styles (linear 'left-to-right' instruction formats); long/complex instructions and deeper HTML trees increase planning difficulty; prompting methods lack robustness to novel task compositions and instruction permutations.",
            "uuid": "e835.0",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "AdaPlanner",
            "name_full": "AdaPlanner (Adaptive planning from feedback with language models)",
            "brief_description": "A prompted agent that synthesizes a plan as executable code (Python function) and adaptively re-generates plans in closed-loop based on runtime/environmental feedback (assertion/functional errors).",
            "citation_title": "Adaplanner: Adaptive planning from feedback with language models",
            "mention_or_use": "use",
            "model_or_agent_name": "AdaPlanner",
            "model_description": "Prompted LMA that uses LLM program synthesis to produce an open-loop plan encoded as a Python function; monitors runtime feedback (assertions, errors) and regenerates plan for remaining steps (closed-loop).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MiniWoB (base tasks) and CompWoB (compositional web automation tasks)",
            "interactive_task_type": "web automation / sequential decision-making / programmatic planning",
            "interactive_performance": "MiniWoB: ~92.9% (reported); CompWoB: ~20.6% average (reported for prompted AdaPlanner with gpt-3.5 backbone); improved but still low with stronger LLMs (text-davinci-003 used as coder backbone yielded some gains in other experiments).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Program-synthesis-based planner (code-as-plan), closed-loop replanning on runtime errors, uses code-aware LLMs for better plan fidelity",
            "training_method": "Prompting / few-shot in-context learning (no additional finetuning reported here)",
            "intervention_type": "prompting strategy (program synthesis) and backbone selection (code-capable LLMs)",
            "intervention_description": "AdaPlanner relies on LLMs with stronger code generation ability (e.g., text-davinci-003 performed better than gpt-3.5-turbo in prior reports); evaluated using provided exemplars for included base tasks and zero-shot otherwise.",
            "intervention_effect": "AdaPlanner attains strong base-task performance but experiences large degradation on compositional CompWoB tasks (from ~92.9% on MiniWoB down to ~20% on CompWoB); using stronger code-capable LLMs gives some improvement but does not close the compositionality gap.",
            "hypothesized_cause_of_gap": "Program-synthesis plans can be brittle when tasks compose unexpectedly; exemplars and program conditioning bias linear workflows and do not generalize to novel compositions or instruction permutations; long instructions and HTML structure increase brittleness.",
            "uuid": "e835.1",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Synapse",
            "name_full": "Synapse (structured prompting LMA)",
            "brief_description": "A prompted agent using task-specific structured prompts (instruction/HTML reformulations and state-conditional exemplars) and retrieval to produce open-loop plans, aiming to reduce translation/hallucination errors.",
            "citation_title": "Synapse: Leveraging few-shot exemplars for human-level computer control",
            "mention_or_use": "use",
            "model_or_agent_name": "Synapse",
            "model_description": "Prompted pipeline with structured task reformulation categories, translation of HTML/instructions, and state-conditional decomposition into exemplars; selects prompts/exemplars per task and predicts open-loop plans.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MiniWoB (base tasks) and CompWoB (compositional web automation tasks)",
            "interactive_task_type": "web automation / sequential decision-making",
            "interactive_performance": "MiniWoB: ~98.5% (reported best among prompted LMAs on base tasks); CompWoB: ~15-25% (individual reported numbers vary; Synapse showed a large drop on compositional tasks, e.g., ~17.8% reported in aggregates).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Structured prompting with multiple reformulation types, translation of instruction/HTML, state-conditional exemplars and retrieval, open-loop planning",
            "training_method": "Prompting / few-shot in-context learning (no finetuning in this evaluation)",
            "intervention_type": "prompt engineering / structured prompting and exemplar retrieval",
            "intervention_description": "Uses a taxonomy of reformulation strategies per task and state-conditional decomposition to reduce error accumulation; assumes optimal exemplar retrieval for evaluation.",
            "intervention_effect": "Highly effective on base MiniWoB tasks but performance collapses on unseen compositional CompWoB tasks (large generalization gap). Using stronger LLM backbones (gpt-4) improves but does not eliminate the gap.",
            "hypothesized_cause_of_gap": "Overfitting of structured prompts and exemplars to base tasks and linear instruction formats; difficulty translating longer instructions and deeper HTML leads to errors (XPath hallucination, incorrect action types) in compositional settings.",
            "uuid": "e835.2",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "HTML-T5++",
            "name_full": "HTML-T5++ (data-rebalanced finetuned LMA)",
            "brief_description": "A finetuned, transferred LMA based on HTML-T5-XL that uses balanced demonstration data (data-rebalancing) to improve generalization to compositional tasks, achieving super-human MiniWoB performance and strong zero-shot CompWoB transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "HTML-T5++",
            "model_description": "Finetuned encoder-decoder model (HTML-T5-XL backbone) with local and global attention in encoder and long-span denoising pretraining; closed-loop policy predicting next action tokens from HTML and action history.",
            "model_size": "â‰ˆ3B (HTML-T5-XL backbone; paper states tractable-size models up to ~3B used for transferred LMAs)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MiniWoB (base tasks) and CompWoB (compositional web automation tasks)",
            "interactive_task_type": "web automation / sequential decision-making",
            "interactive_performance": "MiniWoB (after data rebalancing/finetuning): 95.2% average success (super-human); CompWoB (zero-shot): 61.5% average success (best zero-shot among methods in this study). Reverse-order instructions: average success dropped from 54.8% to 31.0% for transferred LMA group (HTML-T5++ part of that group).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Encoder with local+global attention, mixture of long-span denoising pretraining, closed-loop action prediction (text-format actions)",
            "training_method": "Supervised finetuning on demonstration datasets (agent-generated and human), with a data-rebalancing strategy to mitigate task-data imbalance",
            "intervention_type": "training-method intervention (data rebalancing / supervised finetuning)",
            "intervention_description": "Collected additional demonstrations (77K) and applied multiple data-mixing strategies removing redundant/easy-task episodes to balance training distribution by task difficulty; finetuned HTML-T5-XL on the rebalanced dataset to produce HTML-T5++.",
            "intervention_effect": "Data rebalancing improved MiniWoB success from ~85.6% to 95.2% (HTML-T5++). On zero-shot CompWoB, HTML-T5++ achieved 61.5%, substantially better than prompted LMAs (24.9% average), reducing the generalization gap compared to prompted agents.",
            "hypothesized_cause_of_gap": "Finetuned models generalize better to novel compositions because supervised exposure teaches compositional patterns; remaining gaps are attributed to instruction length and HTML subtree depth which increase planning ambiguity and sparsity, and to distributional mismatch for reverse-order or out-of-distribution instruction formats.",
            "uuid": "e835.3",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "HTML-T5 (original)",
            "name_full": "HTML-T5 (pretrained and finetuned LMA baseline)",
            "brief_description": "A T5-derived model adapted to HTML inputs (tokenized HTML) with local and global attention, used as a finetuned closed-loop policy for web automation tasks.",
            "citation_title": "Understanding html with large language models",
            "mention_or_use": "mention",
            "model_or_agent_name": "HTML-T5",
            "model_description": "Pretrained text-to-text transformer adapted for HTML understanding via specialized pretraining; used as a finetuned agent that maps HTML+history to next action text.",
            "model_size": "XL (prior work HTML-T5-XL; model in this line is ~3B reported elsewhere)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MiniWoB (base tasks) and CompWoB (compositional tasks as transfer target)",
            "interactive_task_type": "web automation / sequential decision-making",
            "interactive_performance": "Prior HTML-T5 reported competitive MiniWoB performance (e.g., 85.4% reported as transferred LMA baseline in aggregate); after rebalancing finetuning (HTML-T5++), performance improved to 95.2% on MiniWoB and 61.5% on CompWoB.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Local+global attention encoder tailored to HTML representation, text decoder outputting action sequences",
            "training_method": "Pretraining (long-span denoising) then supervised finetuning on web automation demonstrations",
            "intervention_type": "finetuning / data rebalancing (in HTML-T5++ extension)",
            "intervention_description": "HTML-T5 serves as base; data-rebalancing strategies described in this paper (removing overrepresented easy-task episodes and adding missing demonstrations) improved performance when finetuned to produce HTML-T5++.",
            "intervention_effect": "Original HTML-T5 performance improved substantially after dataset rebalancing and finetuning (see HTML-T5++ entry).",
            "hypothesized_cause_of_gap": "Original finetuned models trained on imbalanced datasets underperform on certain tasks; balancing training data and increasing coverage of harder tasks improves compositional generalization.",
            "uuid": "e835.4",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Flan-T5-XXL (finetuned example from Appendix K)",
            "name_full": "Flan-T5-XXL (finetuned model used in reasoning compositionality experiments)",
            "brief_description": "A large instruction-finetuned T5 variant (11B) used in Appendix K to demonstrate that a finetuned LLM can outperform prompted LLMs on a compositional reasoning benchmark (Last Letter Concatenation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Flan-T5-XXL (11B, finetuned)",
            "model_description": "Instruction-finetuned encoder-decoder model (11B) finetuned on supervised compositional reasoning examples (Last Letter Concatenation) to evaluate compositional generalization versus prompted models.",
            "model_size": "11B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Last Letter Concatenation (compositional natural-language reasoning benchmark; not interactive/web automation)",
            "interactive_task_type": "multi-step compositional reasoning (natural language)",
            "interactive_performance": "Finetuned Flan-T5-XXL achieved average exact-match performance ~76% on Last Letter Concatenation (both in-distribution and out-of-distribution), outperforming few-shot prompted Flan-PaLM-540B (~44% exact match) in the paper's appendix experiments.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Instruction-tuned encoder-decoder transformer (T5 family)",
            "training_method": "Supervised finetuning on many compositional examples (2000 examples per n-letter up to 8 letters)",
            "intervention_type": "training-method (supervised finetuning) used to improve compositional generalization versus prompting",
            "intervention_description": "Finetuned on extensive supervised compositional examples (Last Letter Concatenation) while prompted LLMs used few-shot CoT exemplars. This tests the hypothesis that finetuning improves compositional generalization.",
            "intervention_effect": "Finetuned model (Flan-T5-XXL) outperformed prompted models (Flan-PaLM family) both in-distribution and out-of-distribution (2-8 vs 9-12 letters), supporting that finetuning can reduce compositional generalization gaps.",
            "hypothesized_cause_of_gap": "Prompting-only methods (few-shot) may not learn the compositional mapping as robustly as supervised finetuning across many exemplars; finetuning imparts stronger inductive bias for compositional tasks.",
            "uuid": "e835.5",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4 (backbone LLM)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A more capable generalist large language model used as a backbone LLM for prompted LMAs; improves base and compositional task performance but does not fully close the compositionality gap.",
            "citation_title": "GPT-4 Technical Report",
            "mention_or_use": "use",
            "model_or_agent_name": "gpt-4 (backbone LLM used in prompted LMAs)",
            "model_description": "Large, general-purpose LLM used for planning, code-generation, and criticism in prompted LMA pipelines (RCI, Synapse etc.).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "CompWoB (compositional web automation tasks) as backbone for prompted LMAs",
            "interactive_task_type": "web automation / sequential decision-making",
            "interactive_performance": "Using gpt-4 as backbone improved prompted agent performance on CompWoB (e.g., RCI improved from ~28.7% with gpt-3.5 to ~56.0% with gpt-4 in reported aggregates), but still fell short of base-task performance and finetuned transferred models.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "backbone model scaling (using a stronger LLM)",
            "intervention_description": "Replacing gpt-3.5-turbo backbone with gpt-4 in prompted LMAs (RCI, Synapse) and evaluating compositional task performance.",
            "intervention_effect": "Noticeable improvements in prompted LMA compositional success but insufficient to match finetuned LMA performance; for RCI, compositional success rose to ~56% with gpt-4 (from ~28.7%). Reverse-order instruction robustness also improved but remained poor (e.g., RCI reverse-order from ~19.2% to ~43.5%).",
            "hypothesized_cause_of_gap": "Even stronger generalist LLMs lack sufficient inductive biases or task-specific training to generalize reliably to novel sequential compositions and instruction permutations; scaling helps but does not remove brittleness from prompting pipelines.",
            "uuid": "e835.6",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Prompted vs Finetuned LMAs (aggregate)",
            "name_full": "Prompted LMAs (in-context prompting) vs Transferred/Finetuned LMAs (supervised finetuning)",
            "brief_description": "Aggregate finding: prompted LMAs (RCI, AdaPlanner, Synapse) achieve human-level performance on base MiniWoB tasks via few-shot prompting, but suffer large degradation on compositional CompWoB tasks; finetuned/transferred LMAs (HTML-T5++) generalize better to compositional tasks, especially after data-rebalancing finetuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Prompted LMAs (RCI/AdaPlanner/Synapse) vs Transferred (HTML-T5++)",
            "model_description": "Comparison between prompting-based agent pipelines (few-shot, exemplar-conditioned, self-critique/program-synthesis/structured prompts) and supervised finetuned LLM agents trained on demonstration datasets.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MiniWoB (base tasks) and CompWoB (compositional tasks)",
            "interactive_task_type": "web automation / sequential decision-making",
            "interactive_performance": "Prompted LMAs: MiniWoB avg ~94.0% (base) but CompWoB avg ~24.9% (large drop). Transferred/finetuned LMAs: MiniWoB avg ~85.4% (pre-rebalance) improved to 95.2% with rebalancing (HTML-T5++), and CompWoB zero-shot avg ~54.8% (transferred) with HTML-T5++ at 61.5%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Prompted LMAs: exemplar-conditioned planning, self-critique loops, code-as-plan modules, structured reformulation. Finetuned LMAs: encoder-decoder with HTML-aware attention, closed-loop action prediction.",
            "training_method": "Prompting/few-shot for prompted LMAs; supervised finetuning with demonstration datasets (and data rebalancing) for transferred LMAs.",
            "intervention_type": "training-method intervention (data rebalancing/finetuning) and prompting interventions (self-critique loops, program-synthesis prompting, structured prompts); backbone scaling (gpt-4) also evaluated",
            "intervention_description": "Paper evaluates prompting variants (RCI ERCI/IRCI loop counts, exemplar selection strategies), program-synthesis prompting (AdaPlanner), structured prompting (Synapse), and a training intervention (dataset rebalancing + finetuning to produce HTML-T5++). Also tests backbone scaling to gpt-4/text-davinci-003 and oracle exemplars.",
            "intervention_effect": "Prompting methods obtain high base-task scores but very low zero-shot compositional performance (~24.9%); finetuning with data rebalancing (HTML-T5++) achieves super-human base-task performance (95.2%) and substantially better zero-shot compositional performance (61.5%). Backbone scaling (gpt-4) partially improves prompting results but does not eliminate compositional failures.",
            "hypothesized_cause_of_gap": "Prompting methods rely on exemplar/task-format priors and linear instruction biases that do not capture combinatorial task structures; finetuning provides stronger inductive bias for compositionality but still struggles when instruction order or HTML complexity diverge (long instructions, deep HTML trees).",
            "uuid": "e835.7",
            "source_info": {
                "paper_title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can solve computer tasks",
            "rating": 2,
            "sanitized_title": "language_models_can_solve_computer_tasks"
        },
        {
            "paper_title": "Adaplanner: Adaptive planning from feedback with language models",
            "rating": 2,
            "sanitized_title": "adaplanner_adaptive_planning_from_feedback_with_language_models"
        },
        {
            "paper_title": "Synapse: Leveraging few-shot exemplars for human-level computer control",
            "rating": 2,
            "sanitized_title": "synapse_leveraging_fewshot_exemplars_for_humanlevel_computer_control"
        },
        {
            "paper_title": "Understanding html with large language models",
            "rating": 2,
            "sanitized_title": "understanding_html_with_large_language_models"
        },
        {
            "paper_title": "World of bits: An open-domain platform for web-based agents",
            "rating": 1,
            "sanitized_title": "world_of_bits_an_opendomain_platform_for_webbased_agents"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.02193025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
31 Dec 2024</p>
<p>Hiroki Furuta furuta@weblab.t.u-tokyo.ac.jp 
Google DeepMind</p>
<p>The University of Tokyo</p>
<p>Yutaka Matsuo matsuo@weblab.t.u-tokyo.ac.jp 
The University of Tokyo</p>
<p>Aleksandra Faust 
Google DeepMind</p>
<p>Izzeddin Gur izzeddin@google.com 
Google DeepMind</p>
<p>Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
31 Dec 20246DD3E5FBCF30730D9E4BC69EB2E3BBBFarXiv:2311.18751v3[cs.LG]
Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents.Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored.In this work, we introduce a new benchmark, called CompWoB -50 new compositional web automation tasks reflecting more realistic assumptions.We show that while existing prompted LMAs (gpt-3.5-turboor gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks.On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%.By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%).While these highlight the promise of small-scale finetuned and transferred models for task compositionality, their performance further degrades under different instruction compositions changing combinational order.In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment.</p>
<p>Introduction</p>
<p>Based on the exceptional capability of large language models (LLMs) (OpenAI, 2023;Anil et al., 2023;Touvron et al., 2023) in commonsense understanding (Brown et al., 2020;Chowdhery et al., 2022), multistep reasoning (Wei et al., 2022;Kojima et al., 2022), program synthesis (Chen et al., 2021) and selfimprovement (Shinn et al., 2023;Madaan et al., 2023;To et al., 2023), language model agents (LMAs) have recently emerged to tackle various decision making problems, such as robotics (Huang et al., 2022a;Ahn et al., 2022), information retrieval (Nakano et al., 2021;Yao et al., 2022b), and external tool use (Wu et al., 2023;Shen et al., 2023).Especially, web automation (Shi et al., 2017) has attracted attention a lot as an promising application of LMAs, because LMAs with prompting (Kim et al., 2023;Sun et al., 2023;Zheng et al., 2023) outperform humans and other learning-based agents, such as imitation (Furuta et al., 2023) or reinforcement learning (Humphreys et al., 2022).</p>
<p>Despite their human-level proficiency in MiniWoB (Shi et al., 2017), a standard web automation benchmark, it is still unclear whether LMAs could deal with challenges in the real world; such as complex observation (Gur et al., 2023), domain generalization (Deng et al., 2023), and ambiguity of instructions (Zhou et al., 2023b).These challenges are exacerbated due to the open-ended nature of real-world tasks, making it infeasible for the agent system to prepare exemplars and prompts for any unseen task in advance.Moreover, it is reasonable and practically important to assume the zero-shot evaluation of the off-the-shelf agents at deployment.This is an orthogonal assumption to the recent progress in LMAs learnable from execution/self-feedback (Shinn selection of exemplars for decomposed sub-problems (Drozdov et al., 2023) or model scaling (Qiu et al., 2022) could help generalization.While those are focused on static tasks, our paper deals with task compositionaliy in interactive decision making, especially, in web automation where the task may have more explicitly decomposable structures (Gur et al., 2021) than natural language tasks.</p>
<p>Preliminaries</p>
<p>Web automation could be described as a deterministic sequential decision making problem, which consists of a state space S, action space A, deterministic transition function T : S Ã— A âˆ’ â†’ S, a set of instructions G, a set of contextual information (i.e.prompts for LLM) C, and episodic reward function (i.e.success criteria) r : S Ã— G Ã— A âˆ’ â†’ {0, 1}.At each time step t, the language model agent Ï€ infers the action conditioned on the prompt, instruction, current state, and previous actions Ï€ : S Ã—A Ã—t Ã—C Ã—G â†’ A, and moves to the next state: s t+1 = T (s t , a t ).When the agent reaches the terminal state (e.g.Login button is clicked) or the max time step is exceeded, the episode is marked as a success if the instruction g is satisfied (i.e.r(s t , g, a t ) = 1).The state s t âˆˆ S is a raw HTML, and we assume the programmatic action space: function(selector, text).function is either click, move or type, selector is an integer index or XPath that can uniquely specify the element, and text is a text input for type function.</p>
<p>Task Compositionality Web automation tasks can be decomposed into a set of primitive base tasks.For instance, (1) clicking several checkboxes, (2) fulfilling the password form, and (3) closing the dialog window.Such a combination could be open-ended and have some dependencies.In this work, we assume that the task Ïˆ âˆˆ Î¨ is characterized by a corresponding subtree of HTML (S Ïˆ âŠ‚ S) and instructions (G Ïˆ âŠ‚ G), and can be dependently combined each other as long as the compositional task is executable.</p>
<p>Language Model Agents for Web Automation</p>
<p>To be self-contained, we here review the representative LMAs, which are selected based on their superior performance and novelty in using LLMs for web automation; RCI (Section 4.1), AdaPlanner (Section 4.2), Synapse (Section 4.3).These are characterized with different base LLMs, prompting and pipeline.To clarify their methodological difference, we provide the pseudo code in Appendix B.Moreover, we resolve the sub-optimal performance of transferred LMAs (Section 4.4) with data-rebalanced finetuning (Section 4.5).</p>
<p>RCI</p>
<p>The agent with Recursive Criticism and Improvement (RCI) prompting (Kim et al., 2023) first generates an open-loop plan to follow a given instruction using few-shot demonstrations.Next, it uses a prompt-based critic to identify the errors in the plan and improves it by reflecting on self-criticized outputs, which is referred as an explicit RCI (ERCI) loop.After ERCI, the agent follows the self-improved plan step-by-step.Before executing the action at each step, the agent grounds the action to the current state (i.e.HTML, open-loop plan, and previous actions) and refines its formatting to be parsable, which increases the feasibility and reduces hallucinations.These final steps are referred as an implicit RCI (IRCI) loop without the self-criticism.All of those play complementary roles to achieve human-level proficiency.While 1 ERCI and 3 IRCI loops are recommended, we observe that the optimal number of self-improvement iterations may differ across the tasks (see Appendix C.1).</p>
<p>AdaPlanner</p>
<p>In contrast to other LMAs, AdaPlanner (Sun et al., 2023) leverages the capability of program synthesis in LLMs to mitigate the hallucination in a plan.Conditioning on the instruction, the description of permissible actions in the web environments, and few-shot demonstrations, the agent first generates an open-loop plan in a Python function, where each snippet corresponds to the action.Once the agent receives environmental feedback at each step, such as assertion errors in the code, other functional errors, or "ask" action to LLMs, it adaptively re-generates the plan for the remaining steps in a closed-loop manner.LLMs more capable of code have performed better, such as text-davinci-003 than gpt-3.5-turbo.RCI (Kim et al., 2023) 90.6% AdaPlanner (Sun et al., 2023) 92.9% Human 93.5% CC-Net (Humphreys et al., 2022) 93.5% RCI (gpt-4) (Kim et al., 2023) 94.0% Synapse (Zheng et al., 2023) 98.5%</p>
<p>Synapse</p>
<p>Finetuned and Transferred Language Model Agents</p>
<p>In addition to the prompted LMAs, LMAs finetuned on base tasks have also been developed (Gur et al., 2022;Furuta et al., 2023;Shaw et al., 2023), which are built on pre-trained language models, such as T5 (Raffel et al., 2020), Flan-T5 (Chung et al., 2022), HTML-T5 (Gur et al., 2023), or Pix2Struct (Lee et al., 2023), with web automation demonstrations.Those LMAs take HTML (or screenshots) and previous actions as inputs and predict the text-format next actions in a closed-loop manner.Since pre-trained language models have a sufficient inductive bias for web environments and instruction-following, finetuned LMAs can data-efficiently achieve competitive performance to the RL-finetuned agents trained from scratch with domain-specific architectures (Humphreys et al., 2022;Liu et al., 2018).Compared to the prompted LMAs relying on private LLM API, it is possible to build on-premise agents based on tractable-size models (at most 3 billion parameters), which may reduce inference time and costs.However, prior works have pointed out that finetuned LMAs struggle to the sub-optimal performance (Zheng et al., 2023) and they require demonstrations on the order of hundreds of thousands while prompted LMAs just need hundreds of episodes (Kim et al., 2023).In this paper, we extensively evaluate such finetuned LMAs in zero-shot transfer settings; LMAs are finetuned only with base task demonstrations and should deal with unseen compositional tasks.We call those transferred LMAs in the later sections.</p>
<p>Data-Rebalancing Improves Finetuned Language Model Agents</p>
<p>Furuta et al. ( 2023) utilized agent-driven data collection instead of humans to improve the performance of finetuned LMAs further.For each task, 10k demonstrations are collected and filtered based on task success, which resulted in challenging tasks having much less than 10k demonstrations due to the sub-optimal performance of LMA on these tasks.We identify that by fixing the data-imbalance problem, the performance of finetuned LMAs can be significantly improved, achieving super-human performance on MiniWoB.We first run Synapse (Zheng et al., 2023) on MiniWoB and collect 77K additional demonstrations across 16 tasks on top of 347K demonstrations (Furuta et al., 2023) to compensate for the lack of data in specific tasks.We then estimate the "brute-force" task difficulty averaging success rates for representative web automation agents.Based on those proximal measures, we classify 65 base tasks into three categories, such as easy (0.8 -1.0), medium (0.6 -0.8), and hard (0.0 -0.6) (see Appendix D).We then balance the number of episodes based on the task difficulty, where we gradually reduce the ratio of easier tasks to focus more on challenging tasks.For instance, we remove X% episodes from top-k tasks in easy group.We heuristically design the following data-mixing strategies: â€¢ Original dataset released by Furuta et al. (2023) (347K episodes).</p>
<p>â€¢ Adding 77K episodes from 16 tasks suffering the lack of data (424K episodes).</p>
<p>â€¢ Removing 50% episodes from top-10 easy tasks (424K -73K = 351K episodes)</p>
<p>â€¢ Removing 80% episodes from top-10 easy tasks (424K -102K = 322K episodes)</p>
<p>â€¢ Removing 50% episodes from easy tasks (424K -142K = 282K episodes)</p>
<p>â€¢ Removing 80% episodes from top-15 easy tasks and removing 50% episodes from other 11 easy tasks (424K -183K = 241K episodes)</p>
<p>See Appendix E for further details.The automation of data mixture design (Xie et al., 2023) would be an important future direction.</p>
<p>We finetune HTML-T5-XL (Gur et al., 2023), a pre-trained language model with local and global attention in the encoder and a mixture of long-span denoising, on these rebalanced datasets.Table 1 shows that all the data-rebalance strategies improve the success rate, and reducing 50% episodes from easy tasks (finally 282K episodes in total) is the most effective rebalancing strategy.This suggests that finetuned LMAs can be as capable as prompted LMAs in decision making tasks.Moreover, due to the inherent compositional nature in challenging web automation tasks, data rebalancing may improve the capability of dealing with unseen task compositions.We include HTML-T5++ as a baseline in the later sections.</p>
<p>Designing CompWoB Controlling Task Complexity and Compositionality</p>
<p>Even though MiniWoB includes a spectrum of simulated environments, they have still focused on narrow and single-task instances.We need more advanced environments to measure the generalization to the various functional challenges in real-world web automation, such as complex observation (Deng et al., 2023), instruction (Zhou et al., 2023b), and task compositionality (Gur et al., 2021).We design CompWoB (i.e.compositional MiniWoB), as a general test bed for LMAs, by leveraging the high composability of simulated web environments (Figure 1).In CompWoB, we systematically combine several base tasks (from 2 to 8) in the original MiniWoB, which LMAs can already solve, into a single task (e.g.click-checkboxes-transfer + enter-password + click-dialog â†’ click-checkboxes-transfer_enter-password_click-dialog).This allows us to control the complexity of HTML and instructions, ensuring novel tasks are solvable to some extent.Moreover, CompWoB can work as a hold-out test environment accompanying with MiniWoB as a train environment.While tasks are visually simplified, CompWoB reflects a broader spectrum of functionalities in real-world websites without sacrificing controllability.For instance, the task in Figure 1 sketches the structure of a login form with agreement checkboxes and a pop-up window, like Internet banking.We also designed tasks with page transitions, such as from the login form to the email browser.</p>
<p>Details of Task Design</p>
<p>As mentioned in Section 4.5, we first calculate the average success rates among representative web automation agents as brute-force task complexity, and classify 65 primitive tasks in MiniWoB into 3 categories (easy, medium, hard) based on those scores.The details of classification are described in Appendix D. We randomly select base tasks from easy group, filter those combinations by their feasibility and reality, and make 50 compositional tasks.We divide those into five categories: two-way tasks (20), three-way tasks (10), n-way tasks (5), transition tasks (5), and easy-medium two-way tasks (10).In n-way tasks, we combine from 4 to 8 tasks sequentially, and in transition tasks, we implement explicit page transition, for instance, transiting from the login form to the email browser.We also sample several tasks from medium group to construct easy-medium two-way tasks.You can find the full list of tasks in Appendix H.For the implementation, we stitch the instructions with "and then" and put each HTML on the same depth.LMAs should satisfy the given instructions sequentially, such as from task A, B, to C.Moreover, to test whether LMAs can deal with complex and ambiguous instructions, we propose reverse-order instruction settings, where the instruction is provided upside down while its task order is semantically the same (e.g.solve task B and C, after solving A).These simple yet controllable strategies make the analysis of LMA's behaviors tractable while reflecting compositional aspects of real-world tasks.</p>
<p>Number of Tasks</p>
<p>MiniWoB has around 104 base tasks in total; all the two-way and three-way combinations of these tasks would give 185,460 compositional tasks.It is infeasible to manually curate all the two-way/three-way combinations.Unfortunately, it is also nontrivial to automate this process due to the locality of the environment implementations in MiniWoB.Each environment is mostly self-contained, making it nontrivial to modularize the whole benchmark/codebase so that every combination can be automatically generated.</p>
<p>Alternatively, we outline a set of design principles -solvability, feasibility, and reality, which we follow to manually curate 50 compositional tasks and 50 reverse-order instruction tasks that could cover a wide variety of difficulty, and compositionality.We believe these guiding principles would be applicable to other compositional generalization problems such as robotic navigation.Based on these design principles, our compositional benchmark can easily be extended in the future to study even more challenging and compositional web automation problems.We would also like to highlight that, because previous works were tested around 50 -60 MiniWoB tasks (Kim et al., 2023;Sun et al., 2023;Zheng et al., 2023;Gur et al., 2022;Furuta et al., 2023), 50 compositional tasks is a decent number of tasks to evaluate the agents.Our addition of a hold-out compositional test set doubles the number of tasks for the community to study.</p>
<p>Inherent Compositionality and Sub-Task Dependency in Web Automation</p>
<p>While many of the real-world tasks have inherent compositionality to some degree, these tasks are not explicitly designed for compositionality, making it difficult to systematically investigate the generalization gap.This can be analyzed with our CompWoB by separating the difficulty of the base tasks themselves and the difficulty of the task compositions.</p>
<p>Real-world websites have dependencies between sub-tasks.For instance, an email client requires a successful login to proceed to writing an email or reading social media posts of a specific user requires navigating to the personal page of that user.In CompWoB, we have implemented these kinds of dependencies in the reward function using logical AND operations.For instance, while we allowed agents to continue to "write an email" sub-task without successful login, the episode is flagged as a failure (i.e.zero reward) even if the "write an email" sub-task is successful.We inform agents of these dependencies using connectors in instructions such as "solve B after A" or "solve A then B".Our analysis in Section 6.4 also suggests that, in addition to task compositionality, long instructions and deep HTML sub-tree can lead to challenging tasks.</p>
<p>Comparison to Other Web Automation Benchmarks</p>
<p>We design CompWoB on top of MiniWoB, which enables us to test the human-level LMAs (Kim et al., 2023;Sun et al., 2023;Zheng et al., 2023;Gur et al., 2023) as strong baselines.Because the success of in-domain tasks is ensured, we could focus on the analysis of generalization to unknown task compositions and their functionality in a controllable way.In contrast, other benchmarks copying real websites fail to obtain decent agents (for instance, the episode success rate of Mind2Web (Deng et al., 2023) is around 5-10% at most; around 15% success at most in WebArena (Zhou et al., 2023b)) and it seems to be harder to identify the attribution of errors due to the complex nature of real websites.Moreover, while realistic benchmarks (Deng et al., 2023;Zhou et al., 2023b)</p>
<p>Results</p>
<p>Evaluation Methodology We evaluate both transferred and prompted LMAs with base MiniWoB demonstrations on the unseen compositional tasks in a "zero-shot" manner; i.e. we do not provide any demonstrations on the compositional tasks for the training corpus and exemplars to measure the generalization.We test 50 compositional tasks and run 100 episodes per task.We adopt gpt-3.5-turboas a backbone LLM, unless otherwise mentioned.We assume the optimal exemplar retriever throughout experiments and always provide the pre-defined prompts to LMAs.We borrow hyper-parameters and prompt templates from respective papers with minimal change to respect our zero-shot transfer setting.As a sanity check for the quality of environments and baseline agents, we also test LMAs with oracle demonstrations in Appendix I.</p>
<p>RCI</p>
<p>We test 4 prompting strategies: (1) zero-shot (without any exemplars), few-shot with (2) first-task exemplars, (3) second-task exemplars, and (4) combination exemplars (i.e. both first and second tasks).For consistency and limited context length, we always consider the first two tasks even if the number of primitive tasks is more than two, and fix the number of self-improvement iterations to 1 explicit RCI and 3 implicit RCI as recommended in the original paper.The exemplars we use are provided by Kim et al. (2023).</p>
<p>AdaPlanner Following the original code, we use the same exemplars provided by Sun et al. (2023) for tasks where those base tasks are included, such as enter-text and click-widget (see Appendix C.2). Otherwise, the agents are prompted in a zero-shot manner.</p>
<p>Synapse</p>
<p>We test 3 prompting strategies: few-shot with (1) first-task, (2) second-task exemplars, and (3) best exemplars (i.e.maximum score between (1) and ( 2)).Because prompts and modules are quite different among tasks, we do not merge the prompts and use proper hyper-parameters corresponding to the given exemplars designed by Zheng et al. (2023).</p>
<p>Language Model Agents Struggle to Handle Task Compositionality</p>
<p>Figure 2 shows that, in CompWoB, all the LMAs face performance degradation.Among those, transferred LMAs achieve better success rate (54.8%) than prompted LMAs (24.9%) on average.In particular, HTML-T5++ achieves the best generalization, suppressing the performance drop from 95.2% to 61.5%.This might be because finetuned LLMs are better at natural language compositional tasks prompted LLMs in general.</p>
<p>WebGUM 54.8 31.0 (-23.8)24.9 18.0 (-6.9)</p>
<p>CompWoB</p>
<p>Reverse-Order Instructions</p>
<p>Figure 3: Average success rate of language model agents in reverse-order instruction settings.We use gpt-3.5-turboas the backbone LLM for prompted LMAs (RCI, AdaPlanner, Synapse), and transferred LMAs with 3-billion parameters.Notably, most LMAs significantly degrade the success rate when reverse-order instructions are provided (the performance gap is highlighted with a red number).This trend is more remarkable in transferred LMA (54.8% â†’ 31.0% on average) than prompted LMA (24.9% â†’ 18.0% on average), which suggests that any kind of LMAs are susceptible to the order of compositional instructions.The capability as general language models might be important to parse semantically complex instructions into the correct plan.</p>
<p>See Appendix K for further details.In contrast, prompted LMAs degrade their performance drastically; even the best RCI with few-shot combination exemplars (comb) just degrades the success rate to 28.7% from 90.6% in base MiniWoB.These results indicate that LMAs suffer from generalization to task compositionality, and transferred LMAs can relatively deal with that better than prompted LMAs, which is an opposite trend to base MiniWoB performance.Among prompted LMAs, RCI performs better than AdaPlanner and Synapse, which suggests that multiple iterations of self-criticism and improvement might be more robust to out-of-domain decision making from the exemplars than program synthesis with feedback or structured prompting with state translations.</p>
<p>In the failure episodes (Table 3), LMAs often miss necessary steps, common to all the prompted LMAs.Since the instructions get long in compositional settings, LMAs may skip important intermediate steps to satisfy the instructions.In addition, they predict incorrect action types and XPath: for instance, hallucination in XPath (RCI) and mixing up click and type action (Synapse).Without oracle exemplars, LMAs often struggle to parse compositional task instructions and understand complex HTML.Appendix H also provides average success rates per category.</p>
<p>Reverse-Order Instructions Degrade Language Model Agents</p>
<p>As shown in Figure 3, all the LMAs significantly degrade the success rate when reverse-order instructions are provided.This trend is more remarkable in transferred LMAs dropping from 54.8% to 31.0%than prompted LMAs from 24.9% to 18.0% on average, which suggests that any kind of LMAs is susceptible to the order of compositional instructions and that transferred LMAs may not generalize well to diverse instructions beyond the dataset distribution.As opposed to Section 6.1, the performance differences among prompted LMAs are marginal, which, however, implies that existing prompting methods, even with self-improvement, may not handle complex task instructions enough.The stronger capability as general-purpose language models or other prompting methods might be important to parse semantically complex instructions into the executable sequential plan.</p>
<p>Compared to Section 6.1, RCI and Synapse cannot parse reverse-order instructions into plans correctly (Table 4).This can be because base-task exemplars in the prompt just have "left-to-right" instructions RCI (Kim et al., 2023) AdaPlanner (Sun et al., 2023) Synapse (Zheng et al., 2023) Click button ONE, then click button TWO, and then select whX, 1Nk, Enter the password "UBKR" into both text fields, and then Select yE, and then enter "Juan" into the text field and fUK3 and click Submit select KwpUv and click Submit press Submit from base MiniWoB tasks; strongly conditioning LMAs to process instructions in a linear order while compositionality could imply a non-linear processing of instructions.The mismatch between the prompt and tasks could cause the performance drops.LMAs still fail to select correct action types (AdaPlanner) or XPath (Synapse), and they also predict unnecessary actions (RCI).</p>
<p>Do Advanced LLMs Solve Compositional Tasks?</p>
<p>Figure 4 presents the results when we adopt other advanced LLMs, than gpt-3.5-turbo,as a backbone of each LMA.The more capable models, such as gpt-4 in a generalist aspect and text-davinci-003 in a code generation, can improve the success rate of all the prompted LMAs.However, even gpt-4 is still far from the generalization in compositional tasks (from 28.7% to 56.0% by RCI) or from dealing with reverse-order instructions (from 19.2% to 43.5% by RCI).This indicates that we need much better LLMs to realize deployable systems in complex real-world decision-making tasks.We provide failure examples in Appendix G.</p>
<p>What Determines Task Complexity on the Web?</p>
<p>Figure 5 visualizes the correlation between the success rate averaged across WebGUM, HTML-T5, RCI, AdaPlanner, and Synapse (y-axis) and each statistic of compositional tasks (x-axis), such as synthesized success rate -a product of base task success rates among compositional tasks -the number of instruction tokens, and max depth of HTML subtrees.Synthesized success rate positively correlates with an average success rate (R = 0.691), indicating that compositional task difficulty takes over base task difficulties.In addition, the number of instruction tokens (R = âˆ’0.579)and the max depth of HTML subtrees (R = âˆ’0.433)show negative correlations.All those are statistically significant in paired t-test with p &lt; 0.01.In contrast, other task statistics, such as synthesized success rate with human performance, the number of HTML tokens, and elements in HTML, just show relatively weaker correlations (see Appendix F for the details).This analysis suggests that HTML with larger depth and long instructions make generalizing compositional tasks challenging.The complexity of HTML is determined by its depth rather than its length or the number of  Figure 5: 2D-scatter plots between success rate averaged among LMAs (y-axis) and each statistic of compositional task (x-axis), such as success rate synthesized with a product of base task success rate, the number of instruction tokens, and max depth of HTML subtrees.Synthesized success rate positively correlates with an average success rate (R = 0.691, statistically significant in paired t-test with p &lt; 0.01), indicating that base task difficulty may determine compositional task difficulty.In addition, the number of instruction tokens (R = âˆ’0.579;p &lt; 0.01) and the max depth of HTML subtrees (R = âˆ’0.433;p &lt; 0.01) show negative correlations, which suggests the high complexity of observation and long instructions make the compositional tasks hard to resolve.</p>
<p>elements.This might come from the hierarchical nature of HTML; in deeper HTML subtrees, the elements near the root tend to be distant from each other after the traversal.Such sparsity may cause confusion during planning.We also report the individual characteristics of each agent in Appendix J.</p>
<p>Discussion</p>
<p>Generalizable Prompting Methods The results of Synapse and RCI in Figure 2 imply that those prompted LMAs have "over-fitting" trends to the base MiniWoB tasks.While the robustness across the prompts has been investigated in natural language tasks (Wei et al., 2022;Kojima et al., 2022), it is not well understood in decision making.Because we will not be able to prepare the optimal self-improvement iterations or decomposed prompts for all the possible instructions and task compositions, even if using optimal exemplar retrievers, we should care more about the generalization of prompting methods for the agent systems.</p>
<p>Agent-Specialized Large Language Models As shown in Figure 4, the more capable LLMs, such as gpt-4, can improve the performance of LMAs in CompWoB.However, it has not reached the base MiniWoB yet (e.g. from 90.6% to 56.0% in RCI, and from 98.5% to 41.4% in Synapse).Similarly, as described in Section 4.4, transferred LMAs can perform better if the training dataset has a good balance and coverage, but it is far from sufficient generalization to compositionality and instructions.The current pre-trained LLMs may still not be sufficient to generalize to complex decision making tasks, and then, in addition to prompting methods, the development of agent-specialized LLMs with enhanced reasoning and generalization through the instruction-tuning (Zeng et al., 2023) or RLHF/AIF (Furuta et al., 2024) would be expected.</p>
<p>Parsing Complex Instructions to Executable Plan Section 6.2 highlights that LMAs are fragile when we increase the complexity of instruction even by the most straightforward reverse-order instructions.This may not be preferable for the real-world application since the instructions might not be easy-to-parse and the users should carefully and concisely tell what they would like to do, which hinders the user's experience.It would be an interesting future direction to investigate better planning modules that could parse complex instructions to correct and executable plans.</p>
<p>Conclusion</p>
<p>The robustness and generalization of LMAs are important aspects for real-world deployment.We extensively examine how much existing LMAs, via transferring and prompting, can deal with a set of compositional web automation environments, CompWoB, that consists of easily-resolvable base primitive tasks.Our evaluation implies the contrary conclusion to the prior works (Table 5); the prompted LMAs are strong solver for primitive web automation tasks but significantly drop their performance in unknown task compositionality.The transferred LMAs often show sub-optimal performance in basic tasks but can deal with compositional problems much better.Our detailed analysis also highlights that LMAs also face catastrophic degradation when they receive complex, even in the simplest reversed-order instructions, and that the challenges in compositional tasks might come from instruction length and the depth of HTML subtree.We hope this inspires the community to build robust and generalizable LMAs to task compositionality toward real-world application.</p>
<p>Ethics Statements</p>
<p>This paper systematically evaluates the performance of existing language model agents for web automation in realistic compositional tasks, and unveils remaining challenges towards broader generalization and real-world deployments.This technique could realize capable AI assistant tools on digital devices (e.g.computers or smartphones), and improve productivity and accessibility for society.</p>
<p>While we anticipate the positive aspects of autonomous agents, for responsible development, we should also consider the potential harmful applications and unintended consequences.The misuse of web automation would threaten cyber security, and the users may get scammed.To reduce these risks, it is essential for researchers, policymakers, and industry to discuss concrete guidelines and regulations.</p>
<p>Appendix A Details of LLM API</p>
<p>We used OpenAI API to call LLM inference in our experiments.Table 6 shows the API used for each method.</p>
<p>We did most of our experiments from 2023/07 to 2023/09.We use the official implementations and prompts released by the authors 234 .We spent about $3.6K for the experiments in total.</p>
<p>Methods</p>
<p>B Pseudo Code for Prompted Language Model Agents</p>
<p>We explicitly distinguish LLM itself and LMA (combination of LLM, prompting and pipeline) in this paper.We provide the pseudo code for prompted LMAs to clarify their methodological difference.RCI (Kim et al., 2023) is the first to use prompting in a self-refinement loop, outperforming imitation-or reinforcement-learned agents on MiniWoB benchmark that requires millions of demonstrations to work.AdaPlanner (Sun et al., 2023) and Synapse (Zheng et al., 2023) are the follow-up works outperforming RCI via code generation from environmental feedback or via well-designed decomposed prompts with retrieval.</p>
<p>Algorithm 1 Prompted Language Model Agents: RCI, AdaPlanner, Synapse Input: prompt P , LMA Ï€, task Ïˆ, environment Env, large language model LLM, number of ERCI N ERCI , number of IRCI N IRCI 1: s, g â† âˆ’ Env.reset(Ïˆ) 2: s, g â† âˆ’ LLM(â€¢|P syn , s, g) â–· Task Reformulation (Synapse) 3: history â† âˆ’ {} 4: while Env is not terminated do 5:
{a 1 , ..., a T } â† âˆ’ Ï€(â€¢|P Ï€ , s, g) â–· Planning 6:
for i in range(N ERCI ) do
7: criticism â† âˆ’ LLM(â€¢|P rci , {a 1 , ..., a T }) â–· Criticism (RCI) 8: {a 1 , ..., a T } â† âˆ’ Ï€(â€¢|P Ï€ , s, g, {a 1 , ..., a T }, criticism) â–· Improvement (RCI) 9:
end for 10:</p>
<p>for a in {a 1 , ..., a T } do 11: end for 18: end while
for j in range(N IRCI ) do 12: a â† âˆ’ Ï€(â€¢|P Ï€ , s,</p>
<p>C Details of Hyper-parameters C.1 RCI</p>
<p>As we described in Section 4.1, RCI has two important hyper-parameters to control the number of selfimprovement iterations.In Explicit RCI (ERCI) loop, LLMs criticize their own generated plans to identify the problem and then improve it, reflecting self-criticism.In Implicit RCI (IRCI) loop, LLMs ground the action to the current state (i.e.HTML) and refine its formatting to be parsable without self-criticism, which may reduce hallucinations or tiny errors.We here test how many self-improvement loops RCI requires (IRCI: 1-4, ERCI: 0-2).Table 7 shows that the optimal number of inference loops is different among tasks, while the recommendations are ERCI = 1 and IRCI = 3.These two hyper-parameters might need to be adjusted for each task.</p>
<p>(ERCI, IRCI)
Tasks (0,1) (0,2) (0,3) (0,4) (1,1) (1,2) (1,3) (1,4) (2,1) (2,2) (2,3) (2,</p>
<p>C.2 AdaPlanner</p>
<p>We use the demonstrations of these 13 tasks where they are included in the task composition:
â€¢ enter-text â€¢ click-widget â€¢ navigate-tree â€¢ login-user-popup â€¢ email-inbox-forward-nl-turk â€¢ click-checkboxes-large â€¢ click-tab-2-hard â€¢ click-dialog-2 â€¢ search-engine â€¢ click-checkboxes-soft â€¢ use-autocomplete â€¢ enter-date â€¢ click-dialog-2</p>
<p>C.3 Synapse</p>
<p>As we explained in Section 4.3, Synapse has several hyper-parameters to construct optimal structured prompts per task to specify whether LLMs translate the instruction or HTML.</p>
<p>Table 8 summarizes the type of reformulation into 7 categories and clarifies which transformed inputs are used for predicting open-loop plans.For instance, Task only requires translated instructions (and few-shot planning exemplars), although Obs takes raw instruction, HTML, and translated HTML as inputs.For the tasks that require temporal abstraction, it also employs state-conditional decomposition, which factorizes demonstrations into a set of exemplars conditioned on the environmental states, and can reduce error accumulation over the time step.</p>
<p>Table 9 provides the detailed values for state-filtering and task reformulation, which is quite different across the tasks.These well-designed structured prompts could be the source of the best performance in base MiniWoB.However, in compositional settings, it is challenging to modify them for any combinations.Instead, we assume the optimal retriever always picks up the exemplars for one of the base tasks, and we compute the maximum score among the results with given prompts.</p>
<p>D Ranking Base MiniWoB Tasks</p>
<p>To ensure the solvability of CompWoB to some extent and to identify the data-redundant tasks for finetuned LMAs, we estimate the brute-force task difficulty (Furuta et al., 2021) (Table 10).We compute the average success rate for each task across representative previous web automation agents, such as CC-Net (SL, SL+RL) (Humphreys et al., 2022), WGE (Liu et al., 2018), WebN-T5 (Gur et al., 2022), WebGUM (Furuta et al., 2023), HTML-T5 (Gur et al., 2023), RCI (Kim et al., 2023), AdaPlanner (Sun et al., 2023), Pix2Act (BC, RL) (Shaw et al., 2023), and Synapse (Zheng et al., 2023).Based on those proxy difficulty measures, we classify 65 tasks into three categories (Kim et al., 2023): easy (from 0.8 to 1.0), medium (from 0.6 to 0.8), and hard (from 0.0 to 0.6).Table 10: Brute-force task complexity and difficulty classification of MiniWoB.We split 65 tasks into the three category based on the task complexity: easy (0.8 -1.0), medium (0.6 -0.8), and hard (0.0 -0.6).</p>
<p>Category</p>
<p>E Details of MiniWoB Dataset</p>
<p>To resolve the data-imbalance problem, we first run Synapse (Zheng et al., 2023) on MiniWoB and collect 77K additional demonstrations across 16 tasks on top of 347K demonstrations (Furuta et al., 2023) to compensate for the lack of data in specific tasks (Strategy A).We use PaLM 2-L (Anil et al., 2023) as a backbone LLM for Synapse.We then reduce the number of demonstrations for the tasks the agents can solve to focus on more challenging tasks.Based on brute-force task complexity (Appendix D), we consider the following four strategies as shown in Table 11: â€¢ Removing 50% episodes from top-10 easy tasks (Strategy B; -73K)</p>
<p>â€¢ Removing 80% episodes from top-10 easy tasks (Strategy C; -102K)</p>
<p>â€¢ Removing 50% episodes from easy tasks (Strategy D; -142K)</p>
<p>â€¢ Removing 80% episodes from top-15 easy tasks and removing 50% episodes from other 11 easy tasks (Strategy E; -183K)</p>
<p>We hold out 21K episodes (5% of 347K + 77K = 424K) as a validation split, and after the convergence, we choose the top-5 checkpoints that achieve higher offline validation accuracy, run those checkpoints online on MiniWoB benchmarks, and then report the best success rate.Through the empirical evaluations (Section 4.4), we find that Strategy D realizes a well-balanced dataset to improve the performance.Except for these dataset mixtures, we follow Gur et al. (2023) for other training hyper-parameters of HTML-T5++.We have used cloud TPU-v3, which has a 32 GiB HBM memory space, with 128 cores.Each finetuning experiment takes about 1-2 days.Figure 6: 2D-scatter plots between success rate averaged among LMAs (y-axis) and each statistic of compositional task (x-axis), such as success rate synthesized with a product of base task success rate, the number of instruction tokens, max depth of HTML subtrees, success rate synthesized with a product of base task human performances, the number of elements in HTML, and the number of HTML tokens.Synthesized success rate positively correlates with an average success rate (R = 0.691, statistically significant in paired t-test with p &lt; 0.01), indicating that base task difficulty may determine compositional task difficulty.In addition, the number of instruction tokens (R = âˆ’0.579;p &lt; 0.01) and the max depth of HTML subtrees (R = âˆ’0.433;p &lt; 0.01) show negative correlations, which suggests the high complexity of observation and long instructions make the compositional tasks hard to resolve.In contrast, synthesized success rate from human performance, the number of HTML tokens, and elements in HTML just show relatively weaker correlations.</p>
<p>G Failure Examples with Advanced Language Models</p>
<p>We provide several failure episodes with advanced language models such as gpt-4 and text-davinci-003 in Table 12.The left columns have correct plans, and the right columns have failure plans.Qualitatively, LMAs based on advanced models manage to reduce the ratio of failure modes in gpt-3.5-turbo(Table 3), such as skipping necessary intermediate steps and hallucination of unnecessary actions.However, they tend to suffer from tiny errors more such as capitalization (RCI, AdaPlanner) or attributes in HTML (Synapse).</p>
<p>H Per-Task Performance on MiniWoB and CompWoB</p>
<p>Task HTML-T5 (Gur et al., 2023)  Table 13: Per-task average success rate on 56 tasks from MiniWoB++.We refer to Gur et al. (2023) for the baseline performance.</p>
<p>I Prompted Language Model Agents with Oracle Exemplars</p>
<p>In this paper, we assume, as a realistic and important constraint, that: (1) prompted or finetuned LMAs are deployed as a service in the real world;</p>
<p>(2) users are only allowed to interact with the agents via providing task instruction, and (3) not allowed to intervene the backend system or prompts.</p>
<p>Providing exemplars or preparing finetuning demonstrations for every compositional problem is infeasible given the huge space of web automation problems.Moreover, it significantly hinders the user experience that the agents would be good at some specific format of instructions and that their violation of those affects the performance even if semantically the same.</p>
<p>As a sanity check of the environments and baseline agents, we here demonstrate that prompted LMAs can change their behaviors adaptively by modifying their prompts and demonstrations.We evaluate the performance of RCI with gpt-4 and oracle exemplars on 20 two-way tasks in CompWoB.We provide two demonstrations per task in the prompts.Figure 7 shows that RCI with gpt-4 and oracle exemplars achieves 82.9% success rate, which is the best among the baselines, such as HTML-T5++ (73.9%),RCI with gpt-3.5-turboand oracle exemplars (56.8%),RCI with combination exemplars (gpt-3.5-turbo:46.9%, gpt-4: 71.5%).See Table 14 for other baselines.This ensures that the tasks are feasible, and that if a prompt includes how to perform on compositional tasks, the performance gets better.In contrast, while oracle demonstrations help improve performance, they could not fully resolve the issues from reverse-order instructions.</p>
<p>Through the experiments in Section 6, we have intended to shed light on their zero-shot generalization capabilities of dealing with unknown sequential-task compositions.For reliable and robust web automation agents, we might need to leverage both prompting and finetuning approaches at the different development stages.For instance, prompted LMAs might work well as adaptive automated data collectors to the novel domains and finetuned LMAs as deployed agents for service.</p>
<p>HTML-T5</p>
<p>F</p>
<p>Table 1
1: Average success rate of finetunedLMAs in 56 tasks on MiniWoB. Adding 77Kepisodes and reducing redundant thousands ofepisodes, HTML-T5++ achieves competitiveperformance to prompted LMAs, RL-finetunedagents, and humans, while improving the suc-cess rate from 85.6% to 95.2%.</p>
<h1>of Episode Steps # of Instruction Tokens Benchmark Average 50th 90th 95th Max Average 50th 90th 95th Max</h1>
<p>MiniWoB (Shi et al., 2017)3.64.35.55.87.08.514.830.512.923.2Mind2Web (Deng et al., 2023) 7.76.014.017.037.019.216.033.042.084.0CompWoB (Ours)7.26.812.814.217.837.334.255.257.9100.0Table 2: The statistics (Mean, Max, 50/90/95th percentiles) of episode steps and instruction tokens from MiniWoB (Shiet al., 2017), Mind2Web (Deng et al., 2023), and CompWoB. CompWoB successfully increases task complexity fromMiniWoB, and has sufficient difficulties the same as Mind2Web, a representative benchmark from real websites.</p>
<p>(Zheng et al., 2023) of LMAs in 50 CompWoB tasks.The light color represents the performance in the original MiniWoB, and the dark color for CompWoB.We use gpt-3.5-turboas the backbone LLM for prompted LMAs (RCI(Kim et al., 2023), AdaPlanner(Sun et al., 2023), Synapse(Zheng et al., 2023)), and transferred LMAs with 3-billion parameters.Transferred LMA, especially HTML-T5++, achieves the best generalization in compositional tasks, suppressing the performance degradation (from 95.2% to 61.5%).On the contrary, prompted LMAs drop their performance significantly; even the best RCI that uses combined task prompts in the composition just achieves 28.7% success (from 90.6% in base tasks).This indicates, in contrast to the base task performances, prompted LMAs are more vulnerable to, and transferred LMAs can deal with unknown task compositionality better than expected.
20 40 60 80 100 Avg. Success Rate (%) 0WebGUM HTML-T5 HTML-T5 + + RCI (zero)(1st) (2nd)(comb) AdaPlanner Synapse(1st)(2nd)(best) Transferred (Ave.) 75.5 85.6 95.2 46.3 56.6 61.5 90.6 17.9 22.5 25.8 28.7 92.9 20.6 98.5 17.8 15.4 25.4 85.4 54.8 MiniWoB CompWoBPrompted 94.0 24.9 (Ave.)Figure 2:
(Shi et al., 2017)rministic instructions and observations, CompWoB has randomized instructions generated from a set of templates, as well as element-randomized HTML.For the task complexity compared to other benchmarks, we provide the statistics (mean, max, percentiles) of episode steps and instruction tokens from MiniWoB(Shi et al., 2017), Mind2Web, and CompWoB in Table2.In terms of episode length and instruction length, CompWoB successfully increases task complexity from MiniWoB, and has sufficient difficulties the same as Mind2Web, a representative benchmark from real websites.</p>
<p>Table 3 :
3
Failure examples in CompWoB.The left columns have correct plans and the right have failure plans.LMAs often ignore necessary intermediate steps or predict incorrect action types and XPath.Figure4: Average success rate of large language models with advanced LLMs (gpt-4 for RCI and Synapse, and text-davinci-003 for AdaPlanner).The lighter color represents the performance in MiniWoB, the medium color does in CompWoB with gpt-3.5-turbo,and the darker color does with gpt-4 or text-davinci-003.The more capable models (gpt-4 as a generalist and text-davinci-003 as a coder) can improve the success rate of prompted LMAs but still struggle to generalize to compositional tasks (e.g.56.0% by RCI) or to deal with reverse-order instructions (e.g.43.5% by RCI).This may indicate that we need much better LLMs to realize deployable agents in the complex real world.
20 40 60 80 100 Avg. Success Rate (%) 0RCI (comb) w/ gpt-4 AdaPlanner w/ text-davinci-003 90.6 28.7 56.0 92.9 20.6 29.5 MiniWoBSynapse (1st) w/ gpt-4 (2nd) 98.5 17.8 15.4 29.5 22.5 CompWoB gpt-4/text-davinci-003 25.4 (best) 41.4RCI (comb, reverse) 19.2 43.5 w/ gpt-4</p>
<p>Table 4 :
4
Failure examples in CompWoB with reverse-order instructions.LMAs often fail to parse the instruction into the correct-order plan, and hallucinate unnecessary actions (e.g.type).</p>
<p>Table 5 :
5
Summary of average / max success rate in web automation.
BaseReverse-OrderAdvancedMiniWoBCompWoBInstructionsModelsPrompted LMA94.0% / 98.5%24.9% / 28.7%18.0% / 19.2%42.3% / 56.0%Transferred LMA85.4% / 95.2%54.8% / 61.5% 31.0% / 34.3%-</p>
<p>Table 6 :
6
List of LLM API used in this paper.We did those experiments from 2023/07 to 2023/09.
APICost (input/output; /1K tokens) Context LengthRCI (Kim et al., 2023)gpt-3.5-turbo$0.0015 / $0.0024K tokensgpt-4$0.03 / $0.068K tokensAdaPlanner (Sun et al., 2023) gpt-3.5-turbo$0.0015 / $0.0024K tokenstext-davinci-003 $0.02 / $0.024K tokensSynapse (Zheng et al., 2023)gpt-3.5-turbo$0.0015 / $0.0024K tokensgpt-4$0.03 / $0.068K tokens</p>
<p>g, {a, ..., a T }, history) â–· Improvement (RCI)
13:end for14:s, r, info â† âˆ’ Env.step(a)15:{a, ..., a T } â† âˆ’ Ï€(â€¢|P Ï€ , s, g, {a, ..., a T }, history, info)â–· Replanning (AdaPlanner)16:history â† âˆ’ history âˆª {a}17:</p>
<p>Table 7 :
7
The success rate of RCI with different hyper-parameters.The optimal parameters differ in each task, while the recommended one is (1,3).
4)</p>
<p>Table 11 :
11
Task ratio in rebalanced dataset for HTML-T5++.</p>
<ul>
<li>
<ul>
<li>RCI (comb) RCI (oracle)Figure7: Average success rate of LMAs in 20 two-way tasks from CompWoB.RCI with gpt-4 achieves the best performance when the oracle exemplars are provided (82.9%) in the prompt.While oracle demonstrations help improve performance, they could not fully resolve the issues from reverse-order instructions (for instance, 82.9% â†’ 71.8% in RCI with gpt-4).
40 60 80 Avg. Success Rate (%)73.946.9 CompWoB 56.8 71.582.9HTML-T5 + + RCI (comb) RCI (oracle) 40 60 80 36.4 33.6 49.4 57.3 71.8 Avg. Success Rate (%) Reverse-Ordergpt-3.5-turbogpt-4w/ oracle exemplars
https://github.com/google-research/google-research/tree/master/compositional_rl
Published in Transactions on Machine LearningResearch (12/2024) <br />
RCI(Kim et al., 2023) AdaPlanner(Sun et al., 2023) Synapse(Zheng et al., 2023) Select rJ and click Submit, after clicking on the "yes" button Select OkRi7, and click Submit, after clicking on the "previous" button Select 2ld1 and click Submit, after entering the password "Zy4XI" into both text fields
https://github.com/posgnu/rci-agent
https://github.com/haotiansun14/AdaPlanner
https://github.com/ltzheng/Synapse
AcknowledgmentsWe thank Mustafa Safdari, Yingjie Miao, Yusuke Iwasawa, Heiga Zen, and Doina Precup for the help on this work.HF was supported by JSPS KAKENHI Grant Number JP22J21582.(Zheng et al., 2023). Raw Task Only is specified with Task as Reformation flag, and Reformulation is specified with Reformat Input flag in the original imprementation.RCI(Kim et al., 2023)AdaPlanner(Sun et al., 2023)Synapse(Zheng et al., 2023)Click on the link "adipiscing", and then click on the "submit" button Click on the link "Augue", and then click on a "button" widget Select 0Qm9EUt, and then enter the username "cristin" and the password "M5" into the text fields and press loginJ Task Complexity Analysis with Language Model AgentsFollowing the recent work in deep reinforcement learning literature(Furuta et al., 2021), we measure average performance as a proxy of oracle task solvability.If many kinds of agents perform poorly, such tasks are regarded as challenging.This can shed light on"task" or "environment" themselves, rather than "language model agents" or "prompting methods", while such an analysis has been overlooked so far.Additionally, while the trend with average might be the same, distributional characteristics for each language model agent could be different.We extend our analysis by reporting the individual performances of each agent in Figure8.Our results still indicate that while all the language model agents (HTML-T5++, RCI, AdaPlanner, Synapse) often show negative correlations between the success rate and instruction tokens or max subtree depth with statistical significance, the trends for other statistics may differ among the methods.Figure8: 2D-scatter plots between the success rate for each LMA (y-axis) and each statistic of compositional task (x-axis), such as the number of instruction tokens, max depth of HTML subtrees, the number of elements in HTML, and the number of HTML tokens.The results imply that while all the language model agents (HTML-T5++, RCI, AdaPlanner, Synapse) show negative correlations between the success rate and instruction tokens with statistical significance, the trends for other statistics may differ among the methods.K Finetuning May Generally Outperform Prompting in Compositional TasksIn this section, we examine the capability of finetuned LLMs and prompted LLMs for compositional generalization in natural language reasoning tasks, rather than agentic tasks.We employ Last Letter Concatenation(Wei et al., 2022;Zhou et al., 2023a)as a benchmark, where if LLMs take "hans, structure" as a question, LLMs are asked to answer it as The last letter of "hans" is "s".The last letter of "structure" is "e".Considering "s", "e" leads to "se".So, "hans, structure" outputs "se".This reasoning problem exhibits a compositional nature as we increase the number of words provided as a question.For finetuned LLMs, we adopt 11B-parameter Flan-T5-XXL(Chung et al., 2022;Raffel et al., 2020)as a base model, and prepare 2000 examples per n-letter for a training dataset.For prompted LLMs, we employ Flan-PaLM-8B, 62B, and 540B(Chung et al., 2022;Chowdhery et al., 2022), and prepare randomized 4-shot CoT prompts from the same training dataset.We randomly use one of them during inference.Furthermore, we use up to 8 letters as a training dataset and few-shot exemplars and also use 9-12 letters as a hold-out test dataset to investigate the compositional generalization in natural language reasoning tasks.Table17shows that finetuned LLM (Flan-T5-XXL) almost always outperforms prompted LLM (Flan-U-PaLM-540B) on average (76% v.s.44% in an exact match) and also in both in-distribution (2-8 letters) and out-of-distribution (9-12 letters) settings.This implies that finetuned LLMs may generally outperform prompted LLMs in compositional tasks, which can also lead to the better performance of finetuned LMAs than prompted LMAs in sequential web automation tasks, as shown in the main text.(Wei et al., 2022;Zhou et al., 2023a).We measure the performance with an exact match.We use up to 8 letters as a training dataset and few-shot exemplars and also use 9-12 letters as a hold-out test dataset to investigate the compositional generalization in natural language reasoning tasks.The results show that finetuned LLM (Flan-T5-XXL) almost always outperforms prompted LLM (Flan-U-PaLM-540B) in both in-distribution (2-8 letters) and out-of-distribution (9-12 letters) settings.# of
References Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Nicolas Sermanet, Clayton Sievers, Alexander Tan, Vincent Toshev, Fei Vanhoucke, Ted Xia, Peng Xiao, Sichun Xu, Mengyuan Xu, Andy Yan, Zeng, arxiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</li>
</ul>
</li>
</ul>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, ClÃ©ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark DÃ­az, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Daniel Smilkov, David R So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, arXiv:2305.10403Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone,2023Slav PetrovarXiv preprintand Yonghui Wu. Palm 2 technical report</p>
<p>Plasma: Making small language models better procedural knowledge models for (counterfactual) planning. Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D Hwang, Lorraine Xiang, Hirona J Li, Soumya Arai, Keisuke Sanyal, Xiang Sakaguchi, Yejin Ren, Choi, 2023</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020arXiv preprint</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, arXiv:2305.17126Large language models as tool makers. 2023arXiv preprint</p>
<p>Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, Jianshu Chen, arXiv:2308.00304Skills-incontext prompting: Unlocking compositionality in large language models. 2023arXiv preprint</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374Felipe Petroski Such. Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021arXiv preprintEvaluating large language models trained on code</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, arXiv:2204.02311Scaling language modeling with pathways. David Dohan, Shivani Agrawal, Mark Omernick, Andrew M Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov; Palm2022arXiv preprintand Noah Fiedel</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, arxiv:2210.114162022arXiv preprint</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, arXiv:2306.06070Mind2web: Towards a generalist agent for the web. 2023arXiv preprint</p>
<p>Alexandre Drouin, Maxime Gasse, Massimo Caccia, H Issam, Laradji, Del Manuel, Tom Verme, LÃ©o Marty, Megh Boisvert, Quentin Thakkar, David Cappart, Vazquez, arxiv:2403.07718Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?. 2024arXiv preprint</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael SchÃ¤rli, Ekin AkyÃ¼rek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, International Conference on Learning Representations. 2023</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, arxiv:2305.18654Faith and fate: Limits of transformers on compositionality. 2023arXiv preprint</p>
<p>Compositional generalization in semantic parsing: Pre-training vs. Daniel Furrer, Nathan Marc Van Zee, Nathanael Scales, SchÃ¤rli, arXiv:2007.089702021specialized architectures. arXiv preprint</p>
<p>Policy information capacity: Information-theoretic measure for task complexity in deep reinforcement learning. Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo, Sergey Levine, Ofir Nachum, Shixiang Shane Gu, International Conference on Machine Learning. 2021</p>
<p>Multimodal web navigation with instruction-finetuned foundation models. Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, Izzeddin Gur, arxiv:2305.118542023arXiv preprint</p>
<p>Geometric-averaged preference optimization for soft preference labels. Hiroki Furuta, Kuang-Huei Lee, Shixiang Shane Gu, Yutaka Matsuo, Aleksandra Faust, Heiga Zen, Izzeddin Gur, arxiv:2409.066912024arXiv preprint</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2211.10435Pal: Program-aided language models. 2023arXiv preprint</p>
<p>Learning to navigate the web. Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, Dilek Hakkani-Tur, International Conference on Learning Representations. 2019</p>
<p>Environment generation for zero-shot compositional reinforcement learning. Izzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook Choi, Manoj Tiwari, Honglak Lee, Aleksandra Faust, Advances in neural information processing systems. 2021</p>
<p>Understanding html with large language models. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust, arxiv:2210.039452022arXiv preprint</p>
<p>A real-world webagent with planning, long context understanding, and program synthesis. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, arxiv:2307.128562023arXiv preprint</p>
<p>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu, arxiv:2305.115542023arXiv preprint</p>
<p>Tool documentation enables zero-shot tool-usage with large language models. Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister, arXiv:2308.006752023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, arXiv:2201.072072022aarXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, arxiv:2207.056082022barXiv preprint</p>
<p>A data-driven approach for learning to control computers. David Peter C Humphreys, Toby Raposo, Gregory Pohlen, Rachita Thornton, Alistair Chhaparia, Josh Muldal, Petko Abramson, Alex Georgiev, Adam Goldin, Timothy Santoro, Lillicrap, International Conference on Machine Learning. 2022</p>
<p>DOM-q-NET: Grounded RL on structured language. Sheng Jia, Jamie Ryan Kiros, Jimmy Ba, International Conference on Learning Representations. 2019</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arxiv:2303.174912023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances In Neural Information Processing Systems. 2022</p>
<p>Pix2struct: Screenshot parsing as pretraining for visual language understanding. Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova, arxiv:2210.033472023arXiv preprint</p>
<p>Api-bank: A benchmark for tool-augmented llms. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, arXiv:2304.082442023arXiv preprint</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, arXiv:2209.07753Code as policies: Language model programs for embodied control. 2023arXiv preprint</p>
<p>Reinforcement learning on web interfaces using workflow-guided exploration. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Percy Liang, International Conference on Learning Representations. 2018</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, arxiv:2308.03688Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. 2023aarXiv preprint</p>
<p>Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, arXiv:2308.05960Caiming Xiong, and Silvio Savarese. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. 2023barXiv preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Laser: Llm agent with state-space exploration for web navigation. Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Dong Yu, arxiv:2309.081722023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, arxiv:2303.176512023arXiv preprint</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, arXiv:2112.09332Browser-assisted question-answering with human feedback. Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, Webgpt, 2021arXiv preprint</p>
<p>Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling. Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox, International Conference on Machine Learning. 2023</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, arxiv:2203.021552022arXiv preprint</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, arXiv:2304.08354Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. 2023aarXiv preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, arXiv:2307.16789Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023barXiv preprint</p>
<p>Evaluating the impact of model scale for compositional generalization in semantic parsing. Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, Kristina Toutanova, arXiv:2205.122532022arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, arXiv:2308.03427Tptu: Task planning and tool usage of large language model-based ai agents. 2023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Compositional generalization and natural language variation: Can a semantic parsing approach handle both?. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova, arXiv:2010.127252021arXiv preprint</p>
<p>Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina Toutanova, arXiv:2306.00245From pixels to ui actions: Learning to follow instructions via graphical user interfaces. 2023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>World of bits: An open-domain platform for web-based agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, International Conference on Machine Learning. 2017</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arxiv:2303.113662023arXiv preprint</p>
<p>Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, Sujian Li, Restgpt, arXiv:2306.06624Connecting large language models with real-world restful apis. 2023arXiv preprint</p>
<p>Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, Shuyan Zhou, arxiv:2305.14257Hierarchical prompting assists large language model on web navigation. 2023arXiv preprint</p>
<p>Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, arXiv:2305.16653Adaplanner: Adaptive planning from feedback with language models. 2023arXiv preprint</p>
<p>Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, Le Sun, arXiv:2306.05301Conference on Robot Learning. Jie Yu, Heiga Tan, Aleksandra Zen, Tatsuya Faust, Harada, 2023a. 2023barXiv preprintSaytap: Language to quadrupedal locomotion</p>
<p>Better language models of code through self-improvement. Hung Quoc To, D Q Nghi, Jin Bui, Tien N Guo, Nguyen, arxiv:2304.012282023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arxiv:2302.13971Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, arXiv:2308.11432A survey on large language model based autonomous agents. 2023barXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, International Conference on Machine Learning. 2023c</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.046712023arXiv preprint</p>
<p>Doremi: Optimizing data mixtures speeds up language model pretraining. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Tengyu Quoc V Le, Adams Wei Ma, Yu, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, arXiv:2305.165042023arXiv preprint</p>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, arXiv:2305.18752Gpt4tools: Teaching large language model to use tools via self-instruction. 2023arXiv preprint</p>
<p>Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, arxiv:2207.01206Webshop: Towards scalable real-world web interaction with grounded language agents. 2022aarXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022barXiv preprint</p>
<p>Parsel: Algorithmic reasoning with language models by composing decompositions. Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, Nick Haber, arxiv:2212.105612023arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arxiv:2310.128232023arXiv preprint</p>
<p>Synapse: Leveraging few-shot exemplars for human-level computer control. Longtao Zheng, Rundong Wang, Bo An, arXiv:2306.078632023arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, International Conference on Learning Representations. 2023a</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig, arxiv:2307.13854Webarena: A realistic web environment for building autonomous agents. 2023barXiv preprint</p>
<p>Failure examples in CompWoB with advanced models. 12@id="subbtn"] Table. gpt-4, text-davinci-003</p>            </div>
        </div>

    </div>
</body>
</html>
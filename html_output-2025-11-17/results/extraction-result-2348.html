<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2348 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2348</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2348</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-46d71d947231f86e1f9d4581e61212385debbe14</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/46d71d947231f86e1f9d4581e61212385debbe14" target="_blank">OpenML: networked science in machine learning</a></p>
                <p><strong>Paper Venue:</strong> SKDD</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems.</p>
                <p><strong>Paper Abstract:</strong> Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2348.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2348.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenML (networked experiment database and collaboration platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open, versioned online platform to share, organize and query machine learning datasets, task definitions, algorithm implementations (flows) and experimental runs to improve reproducibility, enable large-scale meta-analysis and speed up scientific discovery in ML and its applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Machine learning research and applied data analysis (cross-domain: e.g., astronomy, biology, general supervised learning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Organize and make reproducible the data, code and experimental results of machine learning studies so researchers can compare methods, reuse results, run large-scale benchmarks and mine combined experiments for insights.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant and growing: OpenML indexes many public datasets (links to UCI, SDSS and others) and accepts uploads/URLs; most datasets are labeled supervised/tabular (ARFF required initially). Data are versioned and attributed; availability depends on contributor licensing. The paper emphasizes many datasets are large and too detailed for papers.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Primarily structured tabular data (ARFF) for classification/regression; metadata/characteristics computed (70+ measures); planned extension to streams, graphs, text, multi-label, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: experiments involve large datasets, many algorithms and hyperparameter sweeps, complex models and large-scale evaluations that are difficult to summarize in papers; reproducibility and comparability across many tasks add complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature in the sense of many established algorithms and benchmarks (UCI, StatLog, MLcomp), but evolving in practices around reproducibility and experiment sharing; OpenML aims to push more standardized, networked scientific practice.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Platform-enabled comparative supervised learning (benchmarking, meta-learning, ensemble evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>OpenML itself is not a single learning algorithm but a system that (1) defines task types (classification/regression/learning curves/streams), (2) stores datasets and their computed characteristics (including landmarkers), (3) stores flows (algorithm implementations, parameter settings) and runs (predictions + evaluations), and (4) provides APIs, visualizations and SQL access to mine aggregated results for meta-analysis and algorithm selection.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>infrastructure for supervised learning, benchmarking and meta-learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for reproducible ML experimentation and cross-dataset meta-analysis; integrates with Weka, R, MOA, RapidMiner, KNIME for easy usage. Limitations initially include restricted input formats (ARFF) and support only for a subset of task types, to be extended.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively demonstrated to enable faster comparisons across methods and tasks, enable designed serendipity and dynamic division of labor (e.g., detect unexpected effects such as nonmonotonic random forest behavior); reduces redundant effort and makes many research questions answerable in minutes rather than days.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: improves reproducibility, accelerates discovery by making experiments queryable and comparable, supports meta-learning and automated algorithm selection, increases visibility and credit for contributors, and enables collaborative benchmark studies and citizen/student contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to other resources (UCI, mldata, MLOSS, MLcomp, Kaggle, experiment database predecessors). OpenML emphasizes richer metadata, easier contribution, collaborative features, and broader queryability compared to prior systems which either required server-specific code, were competition/adversarial focused, or lacked rich experiment organization.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Integration with popular ML tools (Weka, R, MOA, RapidMiner, KNIME), task standardization (train/test splits, evaluation measures), automatic computation of dataset characteristics, versioning and attribution, and REST/API access enabling automated uploads/downloads.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>A structured, versioned platform for sharing datasets, algorithms and runs enables large-scale, reproducible comparative studies and meta-learning that uncover dataset-dependent algorithm behavior and accelerate scientific progress.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenML: networked science in machine learning', 'publication_date_yy_mm': '2014-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2348.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2348.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RandomForest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random forest classifier (ensemble of decision trees)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble learning method that builds multiple decision trees and aggregates their predictions; used on OpenML tasks to evaluate supervised classification performance and study parameter effects such as number of trees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Supervised classification across multiple application domains (benchmarks aggregated on OpenML, e.g., UCI-like datasets, domain datasets linked by contributors such as those from astronomy or biology)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict class labels for instances in tabular datasets using ensemble tree-based models; assess performance across many datasets and parameter settings (e.g., number of trees).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Many labeled datasets available on OpenML (indexed from UCI and other sources); datasets vary in size and quality; labeled supervised instances are the norm for classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular data (ARFF), numeric and nominal features; sometimes missing values; datasets characterized with >70 measures including skewness, mutual information and landmarkers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Varies across datasets; can be high due to high dimensionality, varying class counts, missing data and need to tune hyperparameters (e.g., number of trees). Multiple datasets and parameter sweeps drive large search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature: random forests are well-established as a strong baseline in supervised classification; practices around parameter tuning and benchmarking are established.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Random forest (ensemble decision trees)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Ensemble of decision trees where each tree is trained on bootstrap samples and splits are chosen among random subsets of features; paper discusses varying the number of trees and visualizes performance across many tasks and parameter settings via OpenML (color-coding dots by number of trees).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (ensemble methods)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for tabular supervised classification and is frequently used as a baseline; OpenML shows it's straightforward to apply across many tasks and to compare parameter effects. Limitations include dataset-specific behavior and computational costs for large forests.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Generally, performance increases or plateaus with more trees, but OpenML aggregated results revealed cases where performance decreases as trees increase, indicating dataset-dependent anomalies or interactions with other factors.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for robust classification across diverse datasets; using OpenML to aggregate many runs helps detect unexpected behaviors and informs parameter selection and meta-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly across many flows in OpenML visualizations (rows per flow), enabling comparison to other classifiers and parameterizations; no single quantitative comparison provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large-scale benchmarking across many datasets and parameter settings enabled by OpenML, visualization of results per-task and per-flow, and availability of dataset characteristics to contextualize performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Aggregating many runs across datasets reveals that algorithm performance and parameter trends (e.g., effect of number of trees) are dataset-dependent, so networked experiment repositories are essential to detect nontrivial, nonmonotonic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenML: networked science in machine learning', 'publication_date_yy_mm': '2014-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2348.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2348.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaBoostM1_DecisionStump</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdaBoost.M1 with Decision Stump base learner (Weka implementation shown)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A boosting ensemble that iteratively trains weak learners (decision stumps) and aggregates them into a strong classifier; demonstrated in an OpenML run on the 'anneal' classification task with reported evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Supervised classification (example run on the 'anneal' dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Train a boosted ensemble to predict class labels on a tabular dataset using 10-fold cross-validation, submit predictions and evaluations to OpenML for objective comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Dataset 'anneal' provided as an OpenML dataset (versioned); task specifies labeled data and fixed cross-validation folds; in general OpenML hosts many such labeled datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular data (ARFF) with a specified target attribute 'class' and predefined train/test splits for cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: standard supervised classification with cross-validation; complexity arises in parameter settings (e.g., number of iterations) and handling class distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature: boosting algorithms like AdaBoost are established methods in classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>AdaBoost.M1 (boosting) with DecisionStump base learner</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Boosting algorithm (AdaBoost.M1) that combines many weak learners (decision stumps) weighted by their performance; run on OpenML task 1 with specific hyperparameters shown in the run (e.g., parameters 75_I, 75_P, 75_S, 75_W correspond to Weka parameter settings). Outputs included confusion matrix, f_measure, kappa, kb_relative_information_score and mean absolute error as recorded by OpenML.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (ensemble boosting)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Directly applicable to the supervised classification task as defined by OpenML (10-fold CV, labeled data).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported run metrics (as shown in paper Figure 4): f_measure ~ 0.3172 (with numeric formatting as displayed), kappa 0.4347 (with noted variability), mean_absolute_error 0.1201; confusion matrix entries are reported for class-wise results. (Values taken directly from the run display in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>The run completed successfully and produced detailed per-fold and per-class evaluations; no broader claims about superiority were made in the paper â€” metrics are shown as an example of recorded run outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Demonstrates how OpenML captures detailed run outputs enabling reproducible evaluation and comparison across methods and parameter settings; potential impact is improved transparency and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>The paper shows that OpenML visualizations allow comparing this flow with others on the same task, but no explicit numeric comparisons for this run to alternative algorithms are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Standardized task definitions (fixed CV splits), ability to upload predictions and metadata, and server-side computation of multiple evaluation measures enabling objective comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Standardized task definitions and automatic recording of detailed run evaluations let researchers compare and reproduce specific algorithm runs (e.g., AdaBoost with decision stumps) across datasets, improving benchmarking rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenML: networked science in machine learning', 'publication_date_yy_mm': '2014-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2348.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2348.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-learning / Landmarkers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-learning with dataset characteristics and landmarkers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using computed dataset characteristics (statistical, information-theoretic measures and landmarkers) to learn how algorithm performance relates to data properties for algorithm selection and automated meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Algorithm selection and performance prediction across diverse supervised learning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict which algorithms or hyperparameter settings will perform well on a new dataset by relating prior performance observations to computed dataset characteristics (meta-features, landmarkers).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>OpenML provides many datasets and computes >70 dataset characteristics automatically for tabular data; prior experiment results (runs) are stored and queryable, enabling meta-learning datasets to be assembled.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular datasets with associated meta-features (summary statistics, information-theoretic measures, landmarkers).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Meta-learning involves high-dimensional meta-feature spaces and heterogeneous datasets; complexity arises from variations across datasets, algorithm-hyperparameter interactions, and the need to generalize across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established research area (references to StatLog, MetaL and literature on meta-learning and landmarkers), but active and evolving with needs for richer experiment repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Meta-learning using dataset characteristics and landmarkers</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Collect performance results for many algorithms on many datasets, compute dataset meta-features (e.g., number of instances, skewness, mutual information, landmarkers), and build meta-models linking meta-features to algorithm performance for algorithm selection or predicting expected performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / meta-learning (modeling algorithm performance)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and explicitly encouraged: OpenML computes dataset characteristics and exposes runs to enable such studies; suitable for algorithm selection and understanding algorithm-data interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Framed as promising: meta-learning can yield insights into which techniques suit certain applications and improve algorithm selection, but the paper does not present quantitative meta-learning results itself.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to automate algorithm selection, speed up model development, and provide principled guidance for practitioners by mining aggregated experiment repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to prior meta-learning projects (StatLog, MetaL) and benchmarking services; OpenML aims to lower barriers and provide richer queryable experiment records to improve meta-learning workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of large, diverse collections of labeled datasets with computed meta-features and linked algorithm performance results; standardized tasks and replicable runs recorded on OpenML.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Automatically computed dataset characteristics combined with a repository of algorithm performances enable scalable meta-learning and algorithm-selection studies that were previously costly or infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenML: networked science in machine learning', 'publication_date_yy_mm': '2014-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2348.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2348.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DataStreamClassification (MOA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data stream classification (MOA integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Support for online/streaming classification tasks via integration with MOA (Massive Online Analysis), enabling evaluation and benchmarking of algorithms that learn from potentially infinite or time-varying data streams.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Streaming/online machine learning for time-evolving data (e.g., sensor data, social media feeds)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Classify instances arriving in a stream where data distributions may change over time; tasks require specialized evaluation protocols and algorithms that can learn incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Streams may be dynamic and sometimes non-repeatable (e.g., Twitter feeds); OpenML notes that some dynamic data won't be repeatable but supports stream-type tasks and integrates with MOA which provides streaming datasets and algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-ordered instance streams; may include tabular instances but with temporal component and possible concept drift.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: requires handling nonstationarity (concept drift), bounded memory/time per instance, continuous evaluation; complexity not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Active research area with dedicated tools (MOA) and growing interest; OpenML integrates to support community needs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Stream classification algorithms via MOA</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>MOA provides algorithms and evaluation methods for incremental learning on data streams; OpenML integration allows tasks of type 'data stream classification' and the computation/storage of results for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised/online learning (data stream mining)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for problems with streaming data; OpenML supports defining such task types and linking to MOA implementations, but notes repeatability limitations for dynamic sources.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Mentioned as supported via integration, enabling stream experiments to be shared; no empirical effectiveness results are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Facilitates reproducible benchmarking and collaborative development of stream learning methods, particularly when integrated with MOA and OpenML's task/run infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Integration with established streaming framework (MOA), ability to define stream-specific task types and store results, and planned support for server-side evaluations when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supporting stream-specific task types and tool integrations (e.g., MOA) in an experiment repository allows the streaming ML community to share reproducible evaluations despite challenges with dynamic, non-repeatable data sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenML: networked science in machine learning', 'publication_date_yy_mm': '2014-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2348.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2348.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LearningCurveAnalysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning curve analysis (task type)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task type supported by OpenML which analyzes how model performance evolves with increasing training set size (learning curves), enabling assessment of data-size effects and generalization behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Empirical evaluation of supervised learning models and study of sample complexity</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Measure model performance as a function of training set size to understand learning dynamics, saturation points and to compare algorithms' data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires datasets with sufficient instance counts to generate meaningful learning curves; OpenML hosts many datasets that can be used for such analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular datasets suitable for repeated subsampling or incremental training set growth.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: requires repeated training and evaluation over varying sample sizes which can be computationally expensive for large datasets and complex models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established empirical evaluation technique in ML; OpenML supports it as a task type to standardize workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Learning curve experiments (empirical sample-complexity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Define tasks that require runs over multiple training set sizes, record performance per split/size, and visualize learning curves on the OpenML platform; server-side visualizations planned to facilitate comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>empirical evaluation / supervised learning analysis</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Useful for diagnosing effects of dataset size on runtime and performance, selecting suitable algorithms given data availability, and for fair comparison across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Paper states such questions (e.g., 'What is the effect of data set size on runtime?') can be answered in minutes with OpenML due to shared runs and data, highlighting practical usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables scalable study of sample complexity across many datasets and algorithms, informing practitioners about data needs and algorithm choices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Standardized task definitions for learning curve experiments, aggregated runs stored on OpenML, and visualization tools that present learning curves across flows and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Standardizing and aggregating learning-curve experiments across many datasets lets researchers rapidly evaluate data-efficiency of algorithms and answer practical sample-size questions that were previously time-consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenML: networked science in machine learning', 'publication_date_yy_mm': '2014-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Experiment databases. A new way to share, organize and learn from experiments. <em>(Rating: 2)</em></li>
                <li>Metalearning: Applications to data mining <em>(Rating: 2)</em></li>
                <li>OpenML: A collaborative science platform <em>(Rating: 2)</em></li>
                <li>MOA: Massive Online Analysis <em>(Rating: 2)</em></li>
                <li>The WEKA data mining software: An update <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2348",
    "paper_id": "paper-46d71d947231f86e1f9d4581e61212385debbe14",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "OpenML",
            "name_full": "OpenML (networked experiment database and collaboration platform)",
            "brief_description": "An open, versioned online platform to share, organize and query machine learning datasets, task definitions, algorithm implementations (flows) and experimental runs to improve reproducibility, enable large-scale meta-analysis and speed up scientific discovery in ML and its applications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Machine learning research and applied data analysis (cross-domain: e.g., astronomy, biology, general supervised learning benchmarks)",
            "problem_description": "Organize and make reproducible the data, code and experimental results of machine learning studies so researchers can compare methods, reuse results, run large-scale benchmarks and mine combined experiments for insights.",
            "data_availability": "Abundant and growing: OpenML indexes many public datasets (links to UCI, SDSS and others) and accepts uploads/URLs; most datasets are labeled supervised/tabular (ARFF required initially). Data are versioned and attributed; availability depends on contributor licensing. The paper emphasizes many datasets are large and too detailed for papers.",
            "data_structure": "Primarily structured tabular data (ARFF) for classification/regression; metadata/characteristics computed (70+ measures); planned extension to streams, graphs, text, multi-label, etc.",
            "problem_complexity": "High: experiments involve large datasets, many algorithms and hyperparameter sweeps, complex models and large-scale evaluations that are difficult to summarize in papers; reproducibility and comparability across many tasks add complexity.",
            "domain_maturity": "Mature in the sense of many established algorithms and benchmarks (UCI, StatLog, MLcomp), but evolving in practices around reproducibility and experiment sharing; OpenML aims to push more standardized, networked scientific practice.",
            "mechanistic_understanding_requirements": null,
            "ai_methodology_name": "Platform-enabled comparative supervised learning (benchmarking, meta-learning, ensemble evaluation)",
            "ai_methodology_description": "OpenML itself is not a single learning algorithm but a system that (1) defines task types (classification/regression/learning curves/streams), (2) stores datasets and their computed characteristics (including landmarkers), (3) stores flows (algorithm implementations, parameter settings) and runs (predictions + evaluations), and (4) provides APIs, visualizations and SQL access to mine aggregated results for meta-analysis and algorithm selection.",
            "ai_methodology_category": "infrastructure for supervised learning, benchmarking and meta-learning",
            "applicability": "Highly applicable for reproducible ML experimentation and cross-dataset meta-analysis; integrates with Weka, R, MOA, RapidMiner, KNIME for easy usage. Limitations initially include restricted input formats (ARFF) and support only for a subset of task types, to be extended.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively demonstrated to enable faster comparisons across methods and tasks, enable designed serendipity and dynamic division of labor (e.g., detect unexpected effects such as nonmonotonic random forest behavior); reduces redundant effort and makes many research questions answerable in minutes rather than days.",
            "impact_potential": "High: improves reproducibility, accelerates discovery by making experiments queryable and comparable, supports meta-learning and automated algorithm selection, increases visibility and credit for contributors, and enables collaborative benchmark studies and citizen/student contributions.",
            "comparison_to_alternatives": "Compared qualitatively to other resources (UCI, mldata, MLOSS, MLcomp, Kaggle, experiment database predecessors). OpenML emphasizes richer metadata, easier contribution, collaborative features, and broader queryability compared to prior systems which either required server-specific code, were competition/adversarial focused, or lacked rich experiment organization.",
            "success_factors": "Integration with popular ML tools (Weka, R, MOA, RapidMiner, KNIME), task standardization (train/test splits, evaluation measures), automatic computation of dataset characteristics, versioning and attribution, and REST/API access enabling automated uploads/downloads.",
            "key_insight": "A structured, versioned platform for sharing datasets, algorithms and runs enables large-scale, reproducible comparative studies and meta-learning that uncover dataset-dependent algorithm behavior and accelerate scientific progress.",
            "uuid": "e2348.0",
            "source_info": {
                "paper_title": "OpenML: networked science in machine learning",
                "publication_date_yy_mm": "2014-06"
            }
        },
        {
            "name_short": "RandomForest",
            "name_full": "Random forest classifier (ensemble of decision trees)",
            "brief_description": "An ensemble learning method that builds multiple decision trees and aggregates their predictions; used on OpenML tasks to evaluate supervised classification performance and study parameter effects such as number of trees.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Supervised classification across multiple application domains (benchmarks aggregated on OpenML, e.g., UCI-like datasets, domain datasets linked by contributors such as those from astronomy or biology)",
            "problem_description": "Predict class labels for instances in tabular datasets using ensemble tree-based models; assess performance across many datasets and parameter settings (e.g., number of trees).",
            "data_availability": "Many labeled datasets available on OpenML (indexed from UCI and other sources); datasets vary in size and quality; labeled supervised instances are the norm for classification tasks.",
            "data_structure": "Structured tabular data (ARFF), numeric and nominal features; sometimes missing values; datasets characterized with &gt;70 measures including skewness, mutual information and landmarkers.",
            "problem_complexity": "Varies across datasets; can be high due to high dimensionality, varying class counts, missing data and need to tune hyperparameters (e.g., number of trees). Multiple datasets and parameter sweeps drive large search spaces.",
            "domain_maturity": "Mature: random forests are well-established as a strong baseline in supervised classification; practices around parameter tuning and benchmarking are established.",
            "mechanistic_understanding_requirements": null,
            "ai_methodology_name": "Random forest (ensemble decision trees)",
            "ai_methodology_description": "Ensemble of decision trees where each tree is trained on bootstrap samples and splits are chosen among random subsets of features; paper discusses varying the number of trees and visualizes performance across many tasks and parameter settings via OpenML (color-coding dots by number of trees).",
            "ai_methodology_category": "supervised learning (ensemble methods)",
            "applicability": "Appropriate for tabular supervised classification and is frequently used as a baseline; OpenML shows it's straightforward to apply across many tasks and to compare parameter effects. Limitations include dataset-specific behavior and computational costs for large forests.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Generally, performance increases or plateaus with more trees, but OpenML aggregated results revealed cases where performance decreases as trees increase, indicating dataset-dependent anomalies or interactions with other factors.",
            "impact_potential": "High for robust classification across diverse datasets; using OpenML to aggregate many runs helps detect unexpected behaviors and informs parameter selection and meta-learning.",
            "comparison_to_alternatives": "Compared implicitly across many flows in OpenML visualizations (rows per flow), enabling comparison to other classifiers and parameterizations; no single quantitative comparison provided in the paper.",
            "success_factors": "Large-scale benchmarking across many datasets and parameter settings enabled by OpenML, visualization of results per-task and per-flow, and availability of dataset characteristics to contextualize performance.",
            "key_insight": "Aggregating many runs across datasets reveals that algorithm performance and parameter trends (e.g., effect of number of trees) are dataset-dependent, so networked experiment repositories are essential to detect nontrivial, nonmonotonic behavior.",
            "uuid": "e2348.1",
            "source_info": {
                "paper_title": "OpenML: networked science in machine learning",
                "publication_date_yy_mm": "2014-06"
            }
        },
        {
            "name_short": "AdaBoostM1_DecisionStump",
            "name_full": "AdaBoost.M1 with Decision Stump base learner (Weka implementation shown)",
            "brief_description": "A boosting ensemble that iteratively trains weak learners (decision stumps) and aggregates them into a strong classifier; demonstrated in an OpenML run on the 'anneal' classification task with reported evaluation metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Supervised classification (example run on the 'anneal' dataset)",
            "problem_description": "Train a boosted ensemble to predict class labels on a tabular dataset using 10-fold cross-validation, submit predictions and evaluations to OpenML for objective comparison.",
            "data_availability": "Dataset 'anneal' provided as an OpenML dataset (versioned); task specifies labeled data and fixed cross-validation folds; in general OpenML hosts many such labeled datasets.",
            "data_structure": "Structured tabular data (ARFF) with a specified target attribute 'class' and predefined train/test splits for cross-validation.",
            "problem_complexity": "Moderate: standard supervised classification with cross-validation; complexity arises in parameter settings (e.g., number of iterations) and handling class distributions.",
            "domain_maturity": "Mature: boosting algorithms like AdaBoost are established methods in classification tasks.",
            "mechanistic_understanding_requirements": null,
            "ai_methodology_name": "AdaBoost.M1 (boosting) with DecisionStump base learner",
            "ai_methodology_description": "Boosting algorithm (AdaBoost.M1) that combines many weak learners (decision stumps) weighted by their performance; run on OpenML task 1 with specific hyperparameters shown in the run (e.g., parameters 75_I, 75_P, 75_S, 75_W correspond to Weka parameter settings). Outputs included confusion matrix, f_measure, kappa, kb_relative_information_score and mean absolute error as recorded by OpenML.",
            "ai_methodology_category": "supervised learning (ensemble boosting)",
            "applicability": "Directly applicable to the supervised classification task as defined by OpenML (10-fold CV, labeled data).",
            "effectiveness_quantitative": "Reported run metrics (as shown in paper Figure 4): f_measure ~ 0.3172 (with numeric formatting as displayed), kappa 0.4347 (with noted variability), mean_absolute_error 0.1201; confusion matrix entries are reported for class-wise results. (Values taken directly from the run display in the paper.)",
            "effectiveness_qualitative": "The run completed successfully and produced detailed per-fold and per-class evaluations; no broader claims about superiority were made in the paper â€” metrics are shown as an example of recorded run outputs.",
            "impact_potential": "Demonstrates how OpenML captures detailed run outputs enabling reproducible evaluation and comparison across methods and parameter settings; potential impact is improved transparency and benchmarking.",
            "comparison_to_alternatives": "The paper shows that OpenML visualizations allow comparing this flow with others on the same task, but no explicit numeric comparisons for this run to alternative algorithms are provided in the text.",
            "success_factors": "Standardized task definitions (fixed CV splits), ability to upload predictions and metadata, and server-side computation of multiple evaluation measures enabling objective comparison.",
            "key_insight": "Standardized task definitions and automatic recording of detailed run evaluations let researchers compare and reproduce specific algorithm runs (e.g., AdaBoost with decision stumps) across datasets, improving benchmarking rigor.",
            "uuid": "e2348.2",
            "source_info": {
                "paper_title": "OpenML: networked science in machine learning",
                "publication_date_yy_mm": "2014-06"
            }
        },
        {
            "name_short": "Meta-learning / Landmarkers",
            "name_full": "Meta-learning with dataset characteristics and landmarkers",
            "brief_description": "Using computed dataset characteristics (statistical, information-theoretic measures and landmarkers) to learn how algorithm performance relates to data properties for algorithm selection and automated meta-analysis.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Algorithm selection and performance prediction across diverse supervised learning datasets",
            "problem_description": "Predict which algorithms or hyperparameter settings will perform well on a new dataset by relating prior performance observations to computed dataset characteristics (meta-features, landmarkers).",
            "data_availability": "OpenML provides many datasets and computes &gt;70 dataset characteristics automatically for tabular data; prior experiment results (runs) are stored and queryable, enabling meta-learning datasets to be assembled.",
            "data_structure": "Structured tabular datasets with associated meta-features (summary statistics, information-theoretic measures, landmarkers).",
            "problem_complexity": "Meta-learning involves high-dimensional meta-feature spaces and heterogeneous datasets; complexity arises from variations across datasets, algorithm-hyperparameter interactions, and the need to generalize across domains.",
            "domain_maturity": "Established research area (references to StatLog, MetaL and literature on meta-learning and landmarkers), but active and evolving with needs for richer experiment repositories.",
            "mechanistic_understanding_requirements": null,
            "ai_methodology_name": "Meta-learning using dataset characteristics and landmarkers",
            "ai_methodology_description": "Collect performance results for many algorithms on many datasets, compute dataset meta-features (e.g., number of instances, skewness, mutual information, landmarkers), and build meta-models linking meta-features to algorithm performance for algorithm selection or predicting expected performance.",
            "ai_methodology_category": "supervised learning / meta-learning (modeling algorithm performance)",
            "applicability": "Applicable and explicitly encouraged: OpenML computes dataset characteristics and exposes runs to enable such studies; suitable for algorithm selection and understanding algorithm-data interactions.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Framed as promising: meta-learning can yield insights into which techniques suit certain applications and improve algorithm selection, but the paper does not present quantitative meta-learning results itself.",
            "impact_potential": "High potential to automate algorithm selection, speed up model development, and provide principled guidance for practitioners by mining aggregated experiment repositories.",
            "comparison_to_alternatives": "Compared conceptually to prior meta-learning projects (StatLog, MetaL) and benchmarking services; OpenML aims to lower barriers and provide richer queryable experiment records to improve meta-learning workflows.",
            "success_factors": "Availability of large, diverse collections of labeled datasets with computed meta-features and linked algorithm performance results; standardized tasks and replicable runs recorded on OpenML.",
            "key_insight": "Automatically computed dataset characteristics combined with a repository of algorithm performances enable scalable meta-learning and algorithm-selection studies that were previously costly or infeasible.",
            "uuid": "e2348.3",
            "source_info": {
                "paper_title": "OpenML: networked science in machine learning",
                "publication_date_yy_mm": "2014-06"
            }
        },
        {
            "name_short": "DataStreamClassification (MOA)",
            "name_full": "Data stream classification (MOA integration)",
            "brief_description": "Support for online/streaming classification tasks via integration with MOA (Massive Online Analysis), enabling evaluation and benchmarking of algorithms that learn from potentially infinite or time-varying data streams.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Streaming/online machine learning for time-evolving data (e.g., sensor data, social media feeds)",
            "problem_description": "Classify instances arriving in a stream where data distributions may change over time; tasks require specialized evaluation protocols and algorithms that can learn incrementally.",
            "data_availability": "Streams may be dynamic and sometimes non-repeatable (e.g., Twitter feeds); OpenML notes that some dynamic data won't be repeatable but supports stream-type tasks and integrates with MOA which provides streaming datasets and algorithms.",
            "data_structure": "Time-ordered instance streams; may include tabular instances but with temporal component and possible concept drift.",
            "problem_complexity": "High: requires handling nonstationarity (concept drift), bounded memory/time per instance, continuous evaluation; complexity not quantified in the paper.",
            "domain_maturity": "Active research area with dedicated tools (MOA) and growing interest; OpenML integrates to support community needs.",
            "mechanistic_understanding_requirements": null,
            "ai_methodology_name": "Stream classification algorithms via MOA",
            "ai_methodology_description": "MOA provides algorithms and evaluation methods for incremental learning on data streams; OpenML integration allows tasks of type 'data stream classification' and the computation/storage of results for comparison.",
            "ai_methodology_category": "supervised/online learning (data stream mining)",
            "applicability": "Appropriate for problems with streaming data; OpenML supports defining such task types and linking to MOA implementations, but notes repeatability limitations for dynamic sources.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Mentioned as supported via integration, enabling stream experiments to be shared; no empirical effectiveness results are provided in the paper.",
            "impact_potential": "Facilitates reproducible benchmarking and collaborative development of stream learning methods, particularly when integrated with MOA and OpenML's task/run infrastructure.",
            "comparison_to_alternatives": null,
            "success_factors": "Integration with established streaming framework (MOA), ability to define stream-specific task types and store results, and planned support for server-side evaluations when needed.",
            "key_insight": "Supporting stream-specific task types and tool integrations (e.g., MOA) in an experiment repository allows the streaming ML community to share reproducible evaluations despite challenges with dynamic, non-repeatable data sources.",
            "uuid": "e2348.4",
            "source_info": {
                "paper_title": "OpenML: networked science in machine learning",
                "publication_date_yy_mm": "2014-06"
            }
        },
        {
            "name_short": "LearningCurveAnalysis",
            "name_full": "Learning curve analysis (task type)",
            "brief_description": "A task type supported by OpenML which analyzes how model performance evolves with increasing training set size (learning curves), enabling assessment of data-size effects and generalization behavior.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Empirical evaluation of supervised learning models and study of sample complexity",
            "problem_description": "Measure model performance as a function of training set size to understand learning dynamics, saturation points and to compare algorithms' data efficiency.",
            "data_availability": "Requires datasets with sufficient instance counts to generate meaningful learning curves; OpenML hosts many datasets that can be used for such analyses.",
            "data_structure": "Structured tabular datasets suitable for repeated subsampling or incremental training set growth.",
            "problem_complexity": "Moderate: requires repeated training and evaluation over varying sample sizes which can be computationally expensive for large datasets and complex models.",
            "domain_maturity": "Well-established empirical evaluation technique in ML; OpenML supports it as a task type to standardize workflows.",
            "mechanistic_understanding_requirements": null,
            "ai_methodology_name": "Learning curve experiments (empirical sample-complexity analysis)",
            "ai_methodology_description": "Define tasks that require runs over multiple training set sizes, record performance per split/size, and visualize learning curves on the OpenML platform; server-side visualizations planned to facilitate comparisons.",
            "ai_methodology_category": "empirical evaluation / supervised learning analysis",
            "applicability": "Useful for diagnosing effects of dataset size on runtime and performance, selecting suitable algorithms given data availability, and for fair comparison across methods.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Paper states such questions (e.g., 'What is the effect of data set size on runtime?') can be answered in minutes with OpenML due to shared runs and data, highlighting practical usefulness.",
            "impact_potential": "Enables scalable study of sample complexity across many datasets and algorithms, informing practitioners about data needs and algorithm choices.",
            "comparison_to_alternatives": null,
            "success_factors": "Standardized task definitions for learning curve experiments, aggregated runs stored on OpenML, and visualization tools that present learning curves across flows and tasks.",
            "key_insight": "Standardizing and aggregating learning-curve experiments across many datasets lets researchers rapidly evaluate data-efficiency of algorithms and answer practical sample-size questions that were previously time-consuming.",
            "uuid": "e2348.5",
            "source_info": {
                "paper_title": "OpenML: networked science in machine learning",
                "publication_date_yy_mm": "2014-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Experiment databases. A new way to share, organize and learn from experiments.",
            "rating": 2
        },
        {
            "paper_title": "Metalearning: Applications to data mining",
            "rating": 2
        },
        {
            "paper_title": "OpenML: A collaborative science platform",
            "rating": 2
        },
        {
            "paper_title": "MOA: Massive Online Analysis",
            "rating": 2
        },
        {
            "paper_title": "The WEKA data mining software: An update",
            "rating": 1
        }
    ],
    "cost": 0.0171245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>OpenML: networked science in machine learning</h1>
<p>Joaquin Vanschoren ${ }^{1}$, Jan N. van Rijn ${ }^{2}$, Bernd Bischl ${ }^{3}$, and Luis Torgo ${ }^{4}$<br>${ }^{1}$ Eindhoven University of Technology - j.vanschoren@tue.nl<br>${ }^{2}$ Leiden University - j.n.van.rijn@liacs.leidenuniv.nl<br>${ }^{3}$ Technische UniversitÃ¤t Dortmund - bischl@statistik.tu-dortmund.de<br>${ }^{4}$ INESC Tec / University of Porto - ltorgo@dcc.fc.up.pt</p>
<h4>Abstract</h4>
<p>Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.</p>
<h2>1. INTRODUCTION</h2>
<p>When Galileo Galilei discovered the rings of Saturn, he did not write a scientific paper. Instead, he wrote his discovery down, jumbled the letters into an anagram, and sent it to his fellow astronomers. This was common practice among respected scientists of the age, including Leonardo, Huygens and Hooke.
The reason was not technological. The printing press was well in use those days and the first scientific journals already existed. Rather, there was little personal gain in letting your rivals know what you were doing. The anagrams ensured that the original discoverer alone could build on his ideas, at least until someone else made the same discovery and the solution to the anagram had to be published in order to claim priority.
This behavior changed gradually in the late 17th century. Members of the Royal Society realized that this secrecy was holding them back, and that if they all agreed to publish their findings openly, they would all do better [19]. Under the motto "take nobody's word for it", they established that scientists could only claim a discovery if they published it first, if they detailed their experimental methods so that results could be verified, and if they explicitly gave credit to all prior work they built upon.
Moreover, wealthy patrons and governments increasingly funded science as a profession, and required that findings be published in journals, thus maximally benefiting the public, as well as the public image of the patrons. This effectively created an economy based on reputation [19; 28]. By publishing their findings, scientists were seen as trustworthy by their peers and patrons, which in turn led to better collaboration, research funding, and scientific jobs. This new culture continues to this day and has created a body of shared
knowledge that is the basis for much of human progress. Today, however, the ubiquity of the internet is allowing new, more scalable forms of scientific collaboration. We can now share detailed observations and methods (data and code) far beyond what can be printed in journals, and interact in real time with many people at once, all over the world.
As a result, many sciences have turned to online tools to share, structure and analyse scientific data on a global scale. Such networked science is dramatically speeding up discovery because scientists are now capable to build directly on each other's observations and techniques, reuse them in unforeseen ways, mine all collected data for patterns, and scale up collaborations to tackle much harder problems. Whereas the journal system still serves as our collective long-term memory, the internet increasingly serves as our collective short-term working memory [29], collecting data and code far too extensive and detailed to be comprehended by a single person, but instead (re)used by many to drive much of modern science.
Many challenges remain, however. In the spirit of the journal system, these online tools must also ensure that shared data is trustworthy so that others can build on it, and that it is in individual scientists' best interest to share their data and ideas.
In this paper, we discuss how other sciences have succeeded in building successful networked science tools that led to important discoveries, and build on these examples to introduce OpenML, a collaboration platform through which scientists can automatically share, organize and discuss machine learning experiments, data, and algorithms.
First, we explore how to design networked science tools in Section 2. Next, we discuss why networked science would be particularly useful in machine learning in Section 3, and describe OpenML in Section 4. In Section 5, we describe how OpenML benefits individual scientists, students, and machine learning research as a whole, before discussing future work in Section 6. Section 7 concludes.</p>
<h2>2. NETWORKED SCIENCE</h2>
<p>Networked science tools are changing the way we make discoveries in several ways. They allow hundreds of scientists to discuss complex ideas online, they structure information from many scientists into a coherent whole, and allow anyone to reuse all collected data in new and unexpected ways. In this section we discuss how to design such online tools, and how several sciences have used them to make important breakthroughs.</p>
<h3>2.1 Designing networked science</h3>
<p>Nielsen [29] reviews many examples of networked science, and explains their successes by the fact that, through the interaction of many minds, there is a good chance that someone has just the right expertise to contribute at just the right time:</p>
<p>Designed serendipity Because many scientists have complementary expertise, any shared idea, question, observation, or tool may be noticed by someone who has just the right (micro)expertise to spark new ideas, answer questions, reinterpret observations, or reuse data and tools in unexpected new ways. By scaling up collaborations, such 'happy accidents' become ever more likely and frequent.</p>
<p>Dynamic division of labor Because each scientist is especially adept at certain research tasks, such as generating ideas, collecting data, mining data, or interpreting results, any seemingly hard task may be routine for someone with just the right skills, or the necessary time or resources to do so. This dramatically speeds up progress.
Designed serendipity and a dynamic division of labor occur naturally when ideas, questions, data, or tools are broadcast to a large group of people in a way that allows everyone in the collaboration to discover what interests them, and react to it easily and creatively. As such, for online collaborations to scale, online tools must make it practical for anybody to join and contribute any amount at any time. This can be expressed in the following 'design patterns' [29]:</p>
<ul>
<li>Encourage small contributions, allowing scientists to contribute in (quasi) real time. This allows many scientists to contribute, increasing the cognitive diversity and range of available expertise.</li>
<li>Split up complex tasks into many small subtasks that can be attacked (nearly) independently. This allows many scientists to contribute individually and according to their expertise.</li>
<li>Construct a rich and structured information commons, so that people can efficiently build on prior knowledge. It should be easy to find previous work, and easy to contribute new work to the existing body of knowledge.</li>
<li>Human attention doesn't scale infinitely. Scientists only have a limited amount of attention to devote to the collaboration, and should thus be able to focus on their interests and filter out irrelevant contributions.</li>
<li>Establish accepted methods for participants to interact and resolve disputes. This can be an 'honor code' that encourages respectable and respectful behavior, deters academic dishonesty, and protects the contributions of individual scientists.</li>
</ul>
<p>Still, even if scientists have the right expertise or skill to contribute at the right time, they typically also need the right incentive to do so.
As discussed, scientists actually solved this problem centuries ago by establishing a reputation system implemented using the best medium for sharing information of the day, the journal. Today, the internet and networked science tools provide a much more powerful medium, but they also need to make sure that sharing data, code and ideas online is in scientists' best interest.</p>
<p>The key to do this seems to lie in extending the reputation system [29]. Online tools should allow everyone to see exactly who contributed what, and link valuable contributions to increased esteem amongst the users of the tools and the scientific community at large. The traditional approach to do this is to link useful online contributions to authorship in ensuing papers, or to link the reuse of shared data to citation of associated papers or DOFs. ${ }^{1}$
Moreover, beyond bibliographic measures, online tools can define new measures to demonstrate the scientific (and societal) impact of contributions. These are sometimes called altmetrics [35] or article-level metrics ${ }^{2}$. An interesting example is $\mathrm{ArXiv}^{3}$, an online archive of preprints (unpublished manuscripts) with its own reference tracking system (SPIRES). In physics, preprints that are referenced many times have a high status among physicists. They are added to resumes and used to evaluate candidates for scientific jobs. This illustrates that what gets measured, gets rewarded, and what gets rewarded, gets done [29; 30]. If scholarly tools define useful new measures and track them accurately, scientists will use them to assess their peers.</p>
<h3>2.2 Massively collaborative science</h3>
<p>Online tools can scale up scientific collaborations to any number of participants. In mathematics, Fields medalist Tim Gowers proposed ${ }^{4}$ to solve several problems that have eluded mathematicians for decades by uniting many minds in an online discussion. Each of these Polymath projects state a specific, unsolved math problem, is hosted on a blog ${ }^{5}$ or wiki ${ }^{6}$, and invites anybody who has anything to say about the problem to chip in by posting new ideas and partial progress.
Designed serendipity plays an important role here. Each idea, even if just a hunch, may spark daughter ideas with those who happen to have just the right background. Indeed, several polymaths "found themselves having thoughts they would not have had without some chance remark of another contributor". ${ }^{7}$ There is also a clear dynamic division of labor, with many mathematicians throwing out ideas, criticizing them, synthesizing, coordinating, and reformulating the problem to different subfields of mathematics.
Blogs and wikis are ideally suited as tools, because they are designed to scale up conversations. They ensure that each contribution is clearly visible, stored and indexed, so that anybody can always see exactly what and how much you contributed. ${ }^{8}$ Moreover, everyone can make quick, small contributions by posting comments, all ideas are organized into threads or pages, and new threads or pages can be opened to focus on subproblems. In addition, anybody can quickly scan or search the whole discussion for topics of interest.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Protected by a set of ground rules, individual scientists also receive rewards for sharing their ideas:</p>
<p>Authorship Each successful polymath project resulted in several papers, linked to the original discussion. Via a self-reporting process, participants put forward their own names for authorship if they made important scientific contributions, or to be mentioned in the acknowledgements for smaller contributions.</p>
<p>Visibility Making many useful contributions may earn you the respect of notable peers, which is valuable in future collaborations or grant and job applications.</p>
<p>Productivity A scientist's time and attention is limited. It is profoundly enjoyable to contribute to many projects where you have a special insight or advantage, while the collaboration dynamically picks up other tasks.</p>
<p>Learning Online discussions are very engaging. Nascent ideas are quickly developed, or discarded, often leading to new knowledge or insight into the thought patterns of others. You are also encouraged to share an idea before someone else gets the same idea.</p>
<h3>2.3 Open data</h3>
<p>Online tools also collect and organize massive amounts of scientific data which can be mined for interesting patterns. For instance, the Sloan Digital Sky Survey (SDSS) is a collaboration of astronomers operating a telescope that systematically maps the sky, producing a stream of photographs and spectra that currently covers more than a quarter of the sky and more than 930,000 galaxies. ${ }^{9}$ Although for a limited time, the data is only available to members of the collaboration, the SDSS decided to share it afterwards with the entire worldwide community of astronomers through an online interface [39]. ${ }^{10}$ Since then, thousands of new and important discoveries have been made by analysing the data in many new ways [7].
These discoveries are again driven by designed serendipity. Whereas the Polymath projects broadcast a question hoping that many minds may find a solution, the SDSS broadcasts data in the belief that many minds will ask unanticipated questions that lead to new discoveries. Indeed, because the telescope collects more data than a single person can comprehend, it becomes more of a question of asking the right questions than making a single 'correct' interpretation of the data. Moreover, there is also a clear dynamic division of labor: the astronomers who ask interesting questions, the SDSS scientists who collect high quality observations, and the astroinformaticians who mine the data all work together doing what they know best.
Moreover, making the data publicly available is rewarding for the SDSS scientists are well.</p>
<p>Citation Publishing data openly leads to more citation because other scientists can more easily build on them. In this case, other astronomers will use it to answer new questions, and credit the SDSS scientists. In fact, each data release easily collects thousands of citations.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Funding Sharing the data increases the value of the program to the community as a whole, thus making it easier to secure continued funding. Indeed, if the data was not shared, reviewers may deem that the money is better spent elsewhere [29]. In fact, journals and grant agencies are increasingly expecting all data from publicly funded science to be published after publication.</p>
<p>The evolution towards more open data is not at all limited to astronomy. We are 'mapping and mining' just about every complex phenomenon in nature, including the brain [21; 15], the ocean [18], gene sequences, genetic variants in humans [12], and gene functions [31]. In many of these projects, the data is produced piece by piece by many different scientists, and gathered in a central database which they all can access.</p>
<h3>2.4 Citizen science</h3>
<p>Online tools are also enhancing the relationship between science and society. In citizen science [36], the public is actively involved in scientific research. One example is Galaxy Zoo [24], where citizen scientists are asked to classify the galaxies from the SDSS and other sources such as the Hubble Space Telescope. Within a year, Galaxy Zoo received over 50 million classifications contributed by more than 150,000 people. These classifications led to many new discoveries, and the public data releases are cited hundreds of times.
Once again, designed serendipity occurs naturally. Unexpected observations are reported and discussed on an online forum, and have already resulted in the serendipitous discovery of the previously unknown 'green pea' galaxies [9], 'passive red spirals' [26], and other objects such as 'Hanny's Object' [23], named after the volunteer who discovered it. Moreover, in a dynamic division of labor, citizen scientists take over tasks that are too time-consuming for professional astronomers. In fact, the overall classifications proved more accurate than the classifications made by a single astronomer, and obtained much faster. More engaged volunteers also participate in online discussions, or hunt for specific kinds of objects.
Galaxy Zoo and similar tools are also designed for scalability. The overall task is split up in many small, easy to learn tasks, each volunteer classifies as many galaxies as she wants, and classifications from different users are combined and organized into a coherent whole.
Finally, there are many different reasons for citizen scientists to dedicate their free time [36]. Many are excited to contribute to scientific research. This can be out of a sense of discovery, e.g., being the first to see a particular galaxy, or because they believe in the goal of the project, such as fighting cancer. Many others view it as a game, and find it fun to classify many images. Some citizen science projects explicitly include a gamification component [11], providing leaderboards and immediate feedback to volunteers. Finally, many volunteers simply enjoy learning more about a specific subject, as well as meeting new people with similar interests. Citizen science is being employed in many more scientific endeavors ${ }^{11}$, including protein folding [11], planet hunting [37], classifying plankton ${ }^{12}$, and fighting cancer ${ }^{13}$. Many of them are collecting large amounts of valid scientific data, and have yielded important discoveries.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>3. MACHINE LEARNING</h2>
<p>Machine learning is a field where a more networked approach would be particularly valuable. Machine learning studies typically involve large data sets, complex code, large-scale evaluations and complex models, none of which can be adequately represented in papers. Still, most work is only published in papers, in highly summarized forms such as tables, graphs and pseudo-code. Oddly enough, while machine learning has proven so crucial in analysing large amounts of observations collected by other scientists, such as the SDSS data discussed above, the outputs of machine learning research are typically not collected and organized in any way that allows others to reuse, reinterpret, or mine these results to learn new things, e.g., which techniques are most useful in a given application.</p>
<h3>3.1 Reusability and reproducibility</h3>
<p>This makes us duplicate a lot of effort, and ultimately slows down the whole field of machine learning [43; 14]. Indeed, without prior experiments to build on, each study has to start from scratch and has to rerun many experiments. This limits the depth of studies and the interpretability and generalizability of their results $[1 ; 14]$. It has been shown that studies regularly contradict each other because they are biased toward different datasets [20], or because they don't take into account the effects of dataset size, parameter optimization and feature selection [33; 17]. This makes it very hard, especially for other researchers, to correctly interpret the results. Moreover, it is often not even possible to rerun experiments because code and data are missing, or because space restrictions imposed on publications make it practically infeasible to publish many details of the experiment setup. This lack of reproducibility has been warned against repeatedly $[20 ; 38 ; 32]$, and has been highlighted as one of the most important challenges in data mining research [16].</p>
<h3>3.2 Prior work</h3>
<p>Many machine learning researchers are well aware of these issues, and have worked to alleviate them. To improve reproducibility, there exist repositories to publicly share benchmarking datasets, such as UCI [2], $\mathrm{LDC}^{14}$ and mldata ${ }^{15}$. Moreover, software can be shared on the MLOSS website ${ }^{16}$. There also exists an open source software track in the Journal for Machine Learning Research (JMLR) where short descriptions of useful machine learning software can be submitted. Also, some major conferences have started checking submissions for reproducibility [25], or issue open science awards for submissions that are reproducible. ${ }^{17}$
Moreover, there also exist experiment repositories. First, meta-learning projects such as StatLog [27] and MetaL [8], and benchmarking services such as MLcomp ${ }^{18}$ run many algorithms on many datasets on their servers. This makes benchmarks comparable, and even allows the building of meta-models, but it does require that code be rewritten to run on their servers. Moreover, the results are not organized to be easily queried and reused.
Second, data mining challenge platforms such as Kaggle [10]</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and TunedIT [44] collect results obtained by different competitors. While they do scale and offer monetary incentives, they are adversarial rather than collaborative. For instance, code is typically not shared during a competition.
Finally, we previously introduced the experiment database for machine learning $[6 ; 43]$, which organizes results from different users and makes them queryable through an online interface. Unfortunately, it doesn't allow collaborations to scale easily. It requires researchers to transcribe their experiments into XML, and only covers classification experiments. While all these tools are very valuable in their own right, and we will build on them in this paper, they fail many of the requirements for scalable collaboration discussed above. It can be quite hard for scientists to contribute, there is often no online discussion, and they are heavily focused on benchmarking, not on sharing other results such as models.</p>
<h2>4. OPENML</h2>
<p>OpenML ${ }^{19}$ is a place where machine learning researchers can automatically share data in fine detail and organize it to work more effectively and collaborate on a global scale. It allows anyone to challenge the community with new data to analyze, and everyone able to mine that data to share their code and results (e.g., models, predictions, and evaluations). ${ }^{20}$ OpenML makes sure that each (sub)task is clearly defined, and that all shared results are stored and organized online for easy access, reuse and discussion.
Moreover, OpenML links to data available anywhere online, and is being integrated [41] in popular data mining platforms such as Weka [13], R [5; 40], MOA [4], RapidMiner [42] and KNIME [3]. This means that anyone can easily import the data into these tools, pick any algorithm or workflow to run, and automatically share all obtained results. The OpenML website provides easy access to all collected data and code, compares all results obtained on the same data or algorithms, builds data visualizations, and supports online discussions.
Finally, it is an open source project, inviting scientists to extend it in ways most useful to them.</p>
<h3>4.1 How OpenML works</h3>
<p>OpenML offers various services to share and find data sets, to download or create scientific tasks, to share and find implementations (called flows), and to share and organize results. These services are available through the OpenML website, as well as through a REST API for integration with software tools. ${ }^{21}$</p>
<h3>4.1.1 Data sets</h3>
<p>Anyone can challenge the community with new data sets to analyze. Figure 1 shows how this is done through the website. To be able to analyse the data, OpenML accepts a limited number of formats. For instance, currently it requires the ARFF ${ }^{22}$ format for tabular data, although more formats will be added over time.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Uploading data to OpenML.</p>
<p>The data can either be uploaded or referenced by a URL. This URL may be a landing page with further information or terms of use, or it may be an API call to large repositories of scientific data such as the SDSS. ${ }^{23}$ OpenML will automatically version each newly added data set. Optionally, a user-defined version name can be added for reference. Next, authors can state how the data should be attributed, and which (creative commons) licence they wish to attach to it. Authors can also add a reference for citation, and a link to a paper. Finally, extra information can be added, such as the (default) target attribute(s) in labeled data, or the row-id attribute for data where instances are named.
For known data formats, OpenML will then compute an array of data characteristics. For tabular data, OpenML currently computes more than 70 characteristics ${ }^{24}$, including simple measures (e.g., the number of features, instances, classes, missing values), statistical and information-theoretic measures (e.g., skewness, mutual information) and landmarkers [34]. Some characteristics are specific to subtypes of data, such as data streams. These characteristics are useful to link the performance of algorithms to data characteristics, or for meta-learning [43] and algorithm selection [22].
OpenML indexes all data sets and allows them to be searched through a standard keyword search and search filters. Each data set has its own page with all known information. ${ }^{25}$ This includes the general description, attribution information, and data characteristics, but also statistics of the data distribution and, for each task defined on this data (see below), all results obtained for that task. As will be discussed below, this allows you to quickly see which algorithms (and parameters) are best suited for analysing the data, and who achieved these results. It also includes a discussion section where the data set and results can be discussed.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Task 1</h2>
<p>View XML</p>
<h2>Supervised Classification</h2>
<p>In supervised classification, you are given an input dataset in which instances are labeled with a certain class. The goal is to build a model that predicts the class for future unlabeled instances. The model is evaluated using a train-test procedure, e.g. cross-validation.</p>
<p>To make results by different users comparable, you are given the exact train-test folds to be used, and you need to return at least the predictions generated by your model for each of the test instances. OpenML will use these predictions to calculate a range of evaluation measures on the server.</p>
<p>You can also upload your own evaluation measures, provided that the code for doing so is available from the implementation used. For extremely large datasets, it may be infeasible to upload all predictions. In those cases, you need to compute and provide the evaluations yourself.</p>
<p>Optionally, you can upload the model trained on all the input data. There is no restriction on the file format, but please use a well-known format or PMML.</p>
<h2>Given inputs</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">source_data</th>
<th style="text-align: left;">1 - anneal (1:0)</th>
<th style="text-align: left;">Dataset (required)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">estimation_procedure</td>
<td style="text-align: left;">1 - 10-fold Cross-validation</td>
<td style="text-align: left;">EstimationProcedure <br> (required)</td>
</tr>
<tr>
<td style="text-align: left;">evaluation_misassess</td>
<td style="text-align: left;">predictive_accuracy</td>
<td style="text-align: left;">String (optional)</td>
</tr>
<tr>
<td style="text-align: left;">target_feature</td>
<td style="text-align: left;">class</td>
<td style="text-align: left;">String (required)</td>
</tr>
<tr>
<td style="text-align: left;">data_aptita</td>
<td style="text-align: left;">http://www.openml.org/api_aptita/get/1/1/Task_1_anneal_aptita.anf?</td>
<td style="text-align: left;">TrainTestOptioFile (hidden)</td>
</tr>
</tbody>
</table>
<p>Required outputs</p>
<table>
<thead>
<tr>
<th style="text-align: left;">model</th>
<th style="text-align: left;">A file containing the model built on all the input data. Please use a well-known format or <br> PMML.</th>
<th style="text-align: left;">File (optional)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">evaluations</td>
<td style="text-align: left;">A list of user-defined evaluations of the task as key-value pairs. The code used to calculate <br> the evaluations must be documented in the used flow.</td>
<td style="text-align: left;">Keynote <br> (optional)</td>
</tr>
<tr>
<td style="text-align: left;">predictions</td>
<td style="text-align: left;">ARFF file with the predictions generated by the model. See the format specification for the <br> exact ARFF structure to be used.</td>
<td style="text-align: left;">PredictionsFile <br> (optional)</td>
</tr>
</tbody>
</table>
<h2>Runs</h2>
<p>9 completed runs
See results for anneal (1)
Run details
$62,237,369,413,500,517,540,559,614$</p>
<p>Figure 2: An OpenML task (of task type classification).</p>
<h3>4.1.2 Task types</h3>
<p>Obviously, a data set alone does not constitute a scientific challenge. We must first agree on what types of results are expected to be shared. This is expressed in task types: they define what types of inputs are given, which types of output are expected to be returned, and what scientific protocols should be used. For instance, classification tasks should include well-defined cross-validation procedures, labeled input data, and require predictions as outputs. ${ }^{26}$
OpenML currently covers classification, regression, learning curve analysis and data stream classification. Task types are created by machine learning (sub)communities through the website, and express what they think should ideally be shared. In some cases, additional support may be required, such as running server-side evaluations. Such support will be provided upon request.
${ }^{26}$ Complete description: http://www.openml.org/t/type/1</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Uploading flows to OpenML.</p>
<h3>4.1.3 Tasks</h3>
<p>If scientists want to perform, for instance, classification on a given data set, they can create a new machine learning task online. Tasks are instantiations of task types with specific inputs (e.g., data sets). Tasks are created once, and then downloaded and solved by anyone.
Such a task is shown in Figure 2. In this case, it is a classification task defined on data set 'anneal' version 1. Next to the data set, the task includes the target attribute, the evaluation procedure (here: 10 -fold cross-validation) and a file with the data splits for cross-validation. The latter ensures that results from different researchers can be objectively compared. For researchers doing an (internal) hyperparameter optimization, it also states the evaluation measure to optimize for. The required outputs for this task are the predictions for all test instances, and optionally, the models built and evaluations calculated by the user. However, OpenML will also compute a large range of evaluation measures on the server to ensure objective comparison. ${ }^{27}$
Finally, each task has its own numeric id, a machine-readable XML description, as well as its own web page including all runs uploaded for that task, see Figure 2.</p>
<h3>4.1.4 Flows</h3>
<p>Flows are implementations of single algorithms, workflows, or scripts designed to solve a given task. They are uploaded to OpenML as shown in Figure 3. Again, one can upload the actual code, or reference it by URL. The latter is especially useful if the code is hosted on an open source platform such as GitHub or CRAN. Flows can be updated as often as needed. OpenML will again version each uploaded flow, while users can provide their own version name for reference. Ideally, what is uploaded is software that takes a task id as</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Run 540</h2>
<p>View XML</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Run Details</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: left;">Task 1 (Supervised <br> Classification)</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Input <br> data</td>
<td style="text-align: left;">anneal (1)</td>
<td style="text-align: left;">Start time $2014-04-0707: 18: 14$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Status</td>
<td style="text-align: left;">OK</td>
</tr>
</tbody>
</table>
<p>Flow</p>
<table>
<thead>
<tr>
<th style="text-align: left;">weka.AdaBoostM1_DecisionStump(1)</th>
<th style="text-align: left;">Yoav Freund, Robert E. Schapire. <br> Experiments with a new boosting <br> algorithm. In: Thirteenth International <br> Conference on Machine Learning, San <br> Francisco, 148-156, 1996.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">75_I</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">75_P</td>
<td style="text-align: left;">100</td>
</tr>
<tr>
<td style="text-align: left;">75_S</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">75_W</td>
<td style="text-align: left;">weka.classifiers.trees.DecisionStump</td>
</tr>
</tbody>
</table>
<p>Evaluations</p>
<table>
<thead>
<tr>
<th style="text-align: left;">confusion_matrix</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">$[0,0,8,0,0,0],[0,0,99,0,0,0]$, <br> $[0,0,684,0,0,0],[0,0,0,0,0,0]$, <br> $[0,0,0,0,67,0],[0,0,40,0,0,0]]$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">f_measure</td>
<td style="text-align: left;">$0.3172 \mathrm{~s}$ <br> 0.0005</td>
<td style="text-align: left;">$[0,0,0.90297,0,1,0]$</td>
</tr>
<tr>
<td style="text-align: left;">kappa</td>
<td style="text-align: left;">$0.4347 \mathrm{~s}$ <br> 0.0208</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">kb_relative_information_score</td>
<td style="text-align: left;">$220.8146$ <br> $\pm 2.3923$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">mean_absolute_error</td>
<td style="text-align: left;">$0.1201 \mathrm{~s}$ <br> 0.0016</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 4: An OpenML run.
input and then produces the required outputs. This can be a wrapper around a more general implementation. If not, the description should include instructions detailing how users can run an OpenML task (e.g., to verify submitted results). Attribution information is similar to that provided for data sets, although with a different set of licences. Finally, it is encouraged to add descriptions for the (hyper)parameters of the flow, and a range of recommended values.
It is also possible to annotate flows with characteristics, such as whether it can handle missing attributes, (non)numeric features and (non)numeric targets. As with data sets, each flow has its own page which combines all known information and all results obtained by running the flow on OpenML tasks, as well as a discussion section.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Portion of the page for data set 'anneal'. It compares, for a classification task, the results obtained by different flows, for multiple parameter settings.</p>
<h3>4.1.5 Runs</h3>
<p>Runs are applications of flows on a specific task. They are submitted by uploading the required outputs (e.g. predictions) together with the task id, the flow id, and any parameter settings. There is also a flag that indicates whether these parameter settings are default values, part of a parameter sweep, or optimized internally. Each run also has its own page with all details and results, shown partially in Figure 4. In this case, it is a classification run. Note that OpenML stores the distribution of evaluations per fold (shown here as standard deviations), and details such as the complete confusion matrix and per-class results. We plan to soon add graphical measures such as ROC curves. Runtimes and details on hardware are provided by the user.
Moreover, because each run is linked to a specific task, flow, and author, OpenML will aggregate and visualize results accordingly.
For instance, Figure 5 shows a comparison of results obtained on a specific classification task. Each row represents a flow and each dot represents the performance obtained in a specific run (for different parameter settings). Hovering over a dot reveals more information, while clicking on it will pull up all information about the run. Users can also switch between different performance metrics.
Conversely, Figure 6 shows a comparison of results obtained by a specific flow on all tasks it has run on. Each row represents a task (and data set), and each dot the obtained performance. Additionally, it is possible to color-code the results with parameter values. Here, it shows the number of trees used in a random forest classifier from small (blue, left) to large (red, right). Again, clicking each dot brings up all run details. As such, it is easy to find out when, how, and by whom a certain result was obtained.
OpenML also provides several other task-specific visualizations such as learning curves. Moreover, it provides an SQL endpoint so that users can (re)organize results as they wish by writing their own queries. All results can be downloaded from the website for further study, and all visualizations can be exported. The database can also be queried programmatically through the API.</p>
<p>Performance evaluation
Evaluation measure: $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot$ $\cdot</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: WEKA integration of OpenML.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: MOA integration of OpenML.</p>
<h2>5. NETWORKED MACHINE LEARNING</h2>
<p>Through OpenML, we can initiate a fully networked approach to machine learning. In this section we compare OpenML to the networked science tools described before, and describe how it helps scientists make new discoveries, how it allows collaborations to scale, and how it benefits individual scientists, students and a more general audience.</p>
<h3>5.1 OpenML and networked science</h3>
<p>By sharing and organizing machine learning data sets, code and experimental results at scale, we can stimulate designed serendipity and a dynamic division of labor.</p>
<h3>5.1.1 Designed serendipity</h3>
<p>Similar to the SDSS, by organizing and 'broadcasting' all data, code and experiments, many minds may reuse them in novel, unforeseen ways.
First, new discoveries could by made simply by querying all combined experiments to answer interesting questions. These question may have been nearly impossible to answer before, but are easily answered if a lot of data is already available. In addition, with readily available data, it becomes a routine part of research to answer questions such as "What is the effect of data set size on runtime?" or "How important is it to tune hyperparameter P?" With OpenML, we can answer these questions in minutes, instead of having to spend days setting up and running new experiments [43]. This means that more such questions will be asked, possibly leading to more discoveries.
Second, we can mine all collected results and data characteristics for patterns in algorithm performance. Such metalearning studies could yield insight into which techniques are most suited for certain applications, or to better understand and improve machine learning techniques [43].
Third, anyone could run into unexpected results by browsing through all collected data. An example of this is shown in Figure 10, which is a continuation of the results shown in Figure 6: while the performance of a random forest classifier should increase (or stagnate) when more trees are added to the forest (red dots), it sometimes happens that it decreases. As in Galaxy Zoo, such serendipitous discoveries can be discussed online, combining many minds to explore several possible explanations.
Finally, beyond experiments, data sets and flows can also be reused in novel ways. For instance, an existing technique may prove extremely useful for analysing a new data set, bringing about new applications.</p>
<h3>5.1.2 Dynamic division of labor</h3>
<p>OpenML also enables a dynamic division of labor: largescale studies could be undertaken as a team, or hard questions could be tackled collaboratively, with many scientists contributing according to their specific skills, time or resources.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 10: An unexpected result: the performance of a random forest decreases as more trees are added to the forest.</p>
<p>Scientists, possibly from other domains, can focus the attention of the community on an important problem. This can be done by adding new data sets (and tasks) to OpenML and collaborating with the machine learning community to analyse it. Some may suggest techniques that would otherwise not be considered, while others are especially skilled at designing custom-built workflows, running large-scale experiments, or improving code. Conversely, the scientists that contributed the data can provide direct feedback on the practical utility of suggested approaches, interpret the generated models, and otherwise guide the collaboration to the desired outcome. Such collaborations can scale to any number of scientists. OpenML also helps to coordinate the effort, e.g., by organizing all results per task (see Figure 6), so that everybody can track each other's progress, and discuss ideas and results online.
Another case is that of benchmark studies. While it is important that a new algorithm be compared against the state of the art, it is often time-consuming to hunt down their implementations and to figure out how to run them. On OpenML, each researcher that invents a new algorithm can focus on experimenting with that algorithm alone, knowing best how to apply it on different tasks. Next, she can instantly reuse the results from all other shared algorithms, ran by their original authors. As such, a very complete overview of the state of the art emerges spontaneously.
Finally, students and citizen scientists can also contribute to research simply by using the OpenML plugins to experiment while they learn about machine learning techniques.</p>
<h3>5.2 Scaling up collaboration</h3>
<p>As discussed in section 2.1, these benefits emerge faster if online collaborations are allowed to scale.
First of all, it is easy to make small contributions to OpenML. When using any of the OpenML plugins, you can easily import a task, run any algorithm or workflow, and automatically export the results to OpenML. You can just perform a single run, a few, or thousands without much effort. Moreover, scientists who know of new interesting data sets or algorithms can easily add them through the website, and watch how others start experimenting with them. It is even easier to browse through the discussions running on OpenML, and leave a comment or suggestion. Alternatively, one can browse the results shared on the website, and draw attention to unexpected results that are worth investigating. Even contributing a single run, data set or comment can be valuable to the community, and may stimulate more work in that direction. More committed scientists can contribute in many other ways, such as creating new tasks and task types, adding new data characterizations or evaluation measures, and integrating OpenML in new tools and environments. Moreover, OpenML tasks naturally split up complex studies into tasks which can be run independently by many scientsists according to their skills, as discussed in Section 5.1. Tasks also split the machine learning community into smaller subcommunities (e.g., clustering) which focus on a single task type, or subgroups focusing on a single task (e.g. galaxy clustering). Designed serendipity and dynamic division of labor also occur in small but active subcommunities. They are not held back if other communities are less active. Next, OpenML constructs a rich and structured information commons, building a database of all data sets, flows, tasks, runs, results, scientists, and discussions. OpenML also ag-
gregates results in different ways, e.g., visualizing results per task and flow (see Figures 5 and 6). Keyword searches and filters make it easy to find resources, and more complex questions can be answered through the SQL interface, or by downloading data and analysing it using other tools.
As a result, all information is also open but easily filtered. The website organizes all results per task, data set and flow, so that researchers can focus on what interests them most, without being distracted by the activity of other scientists. In future work, we also aim to filter results by their authors. Finally, OpenML establishes, and in some cases enforces, a scientific approach to sharing results. Indeed, OpenML tasks set a certain standard of scientific quality and trustworthiness by defining how experiments must be run and what must be reported. Because the code is shared when uploading runs, it is possible for others to verify results, and the server-side evaluations makes results objectively comparable. OpenML also makes clear who contributed what (and when), and how it is licenced. Every shared algorithm, flow, run or comment can be attributed to a specific person, and this information is always shown when someone views them online.</p>
<h3>5.3 Benefits for scientists</h3>
<p>How do you, as an individual scientist, benefit from sharing your experiments, data and code on OpenML?</p>
<h3>5.3.1 More time</h3>
<p>First, you gain more time. OpenML assists in most of the routine and tedious duties in running experiments: finding data sets, finding implementations, setting up experiments, and organizing all experiments for further analysis. Moreover, when running benchmark experiments on OpenML, you can directly compare them with the state of the art, reusing other, comparable results. In addition, you can answer routine research question in minutes by tapping into all shared data, instead of losing days setting up new experiments. Finally, having your experiments stored and organized online means they are available any place, any time, through any browser (including mobile devices), so you can access them when it is convenient.</p>
<h3>5.3.2 More knowledge</h3>
<p>Second, you gain more knowledge. Linking your results to everybody else's has a large potential for new discoveries. This was discussed in Section 5.1: you can answer previously impossible questions, mine all combined data, and run into unexpected results. It also makes it easy to check whether certain observations in your data are echoed in the observations of others. Next, with OpenML you can interact with other minds on a global scale. Not only can you start discussions to answer your own questions, you can also help others, and in doing so, learn about other interesting studies, and forge new collaborations. Finally, by reusing prior results, you can launch larger, more generalizable studies that are practically impossible to run on your own.</p>
<h3>5.3.3 More reputation</h3>
<p>Third, OpenML helps you build reputation by making your work more visible to a wider group of people, by bringing you closer to new collaborators, and by making sure that others know how to credit you if they build on any of your work.</p>
<p>Citation OpenML makes sure that all your contributions, every data set, flow and run, are clearly attributed to you. If others wish to reuse your results, OpenML will tell them how you wish to be credited (e.g., through citation). Moreover, OpenML makes your shared resources easy to find, thus making frequent citations more likely.</p>
<p>Altmetrics OpenML will also automatically track how often your data or code is reused in experiments (runs), and how often your experiments are reused in studies (see below). These are clear measures of the impact of your work.</p>
<p>Productivity OpenML allows you to contribute efficiently to many studies. This increases your scientific productivity, which translates to more publications.</p>
<p>Visibility You can increase your visibility by contributing to many studies, thus earning the respect of new peers. Also, if you design flows that outperform many others, OpenML will show these at the top of each data set page, as shown in Figure 5. You can also post links to your online results in blogs or tweets.</p>
<p>Funding Open data sharing is increasingly becoming a requirement in grant proposals, and uploading your research to OpenML is a practical and convincing way to share your data with others.</p>
<p>No publication bias Most journals have a publication bias: even if the findings are valuable, it is hard to publish them if the outcome is not positive. Through OpenML, you can still share such results and receive credit for them.</p>
<h3>5.4 Benefits for students</h3>
<p>OpenML can also substantially help students in gaining a better understanding of machine learning. Browsing through organized results online is much more accessible than browsing through hundreds of papers. It provides a clear overview of the state of the art, interesting new techniques and open problems. As discussed before, students can contribute in small or big ways to ongoing research, and in doing so, learn more about how to become a machine learning researcher. Online discussions may point to new ideas or point out mistakes, so they can learn to do it better next time. In short, it gives students and young scientists a large playground to learn more quickly about machine learning and discover where they can make important contributions.</p>
<h2>6. FUTURE WORK</h2>
<p>In this section, we briefly discuss some of the key suggestions that have been offered to improve OpenML, and we aim to implement these changes as soon as possible. In fact, as OpenML is an open source project, everyone is welcome to help extend it, or post new suggestions through the website.</p>
<h3>6.1 OpenML studies</h3>
<p>One typically runs experiments as part of a study, which ideally leads to a publication. Scientists should therefore be able to create online studies on OpenML, that combine all
relevant info on one page. Such studies reference all runs of interest, either generated for this study or imported from other OpenML studies, and all data sets and flows underlying these runs. Additionally, textual descriptions can be added to explain what the study is about, and any supplementary materials, such as figures, papers or additional data, can be uploaded and attached to it.
If the study is published, a link to this online study can be added in the paper, so that people can find the original experiments, data sets and flows, and build on them. As such, it becomes the online counterpart of a published paper, and you could tell people to cite the published paper if they reuse any of the data. An additional benefit of online studies is that they can be extended after publication.
Moreover, based on the underlying runs, OpenML can automatically generate a list of references (citations) for all underlying data sets, flows and other studies. This helps authors to properly credit data that they reused from other OpenML scientists. Similar to arXiv, OpenML can also automatically keep track of this so that authors can instantly view in which studies their contributions are being reused. Finally, similar to the polymath projects, such studies could be massively collaborative studies, aimed at solving a hard problem, and driven by many people providing ideas, experiments, data or flows. As such, each study should have a discussion section where questions can be asked, suggestions can be made and progress can be discussed. Similar to the polymath studies, it also makes sense if studies link to other studies that tackle specific subproblems, and if the study was linked to a wiki page for collaborative writing. To keep focus on the results of a given study, it can act as a filter, hiding all other results from the website so that only the contents of that study are visible.</p>
<h3>6.2 Visibility and social sharing</h3>
<p>There may be cases where you want all of OpenML's benefits, but do not want to make your data public before publication. On the other hand, you may want to share that data with trusted colleagues for collaboration or feedback, or allow friends to edit your studies, e.g., to add new runs to it. It therefore makes sense if, for a limited amount of time, studies, data sets or flows can be flagged as private or 'friends only'. Still, scientists should agree that this is a temporary situation. When you wish to be attributed for your work, you must first make it publicly available.
In addition, in highly experimental settings, most results may not be interesting as such. Scientists are always able to delete those results or mark them as deprecated.</p>
<h3>6.3 Collaborative leaderboards</h3>
<p>When many scientists work together to design a flow for a specific task, it is useful to show which flows are currently performing best, so that others can build on and improve those flows. However, it is also important to show which contributions had the biggest impact while such a flow was constructed collaboratively. In those cases, it is useful to implement a leaderboard that does not only show the current best solution, but instead credits the authors who contributed solutions that were in, say, the top 3 at any point in time. Alternatively, this can be visualized in a graph of performance versus time, so that it is clear who caused the bigger performance 'jumps'. This is useful to later credit the people who made the most important contributions.</p>
<h3>6.4 Broader data support</h3>
<p>To build a well-structured information space, OpenML needs to be able to correctly interpret the uploaded data. For instance, to calculate data characteristics and build train-test splits, more information is needed about the structure of data sets. Therefore, we have initially focused on ARFF data. However, many types of data, such as graphs, can not always be adequately expressed as ARFF, and we will add support for new data types according to researchers' needs. Moreover, for some types of tasks, additional types of results (run outputs) may need to be defined so that OpenML can properly interpret them.
In the short term, we aim to add support for Graph Mining, Clustering, Recommender Systems, Survival Analysis, Multi-label Classification, Feature selection, Semi-Supervised Learning, and Text Mining. Moreover, we will extend the website to make it easy to propose and work collaboratively on support for new task types.</p>
<h2>7. CONCLUSIONS</h2>
<p>In many sciences, networked science tools are allowing scientists to make discoveries much faster than was ever possible before. Hundreds of scientists are collaborating to tackle hard problems, individual scientists are building directly on the observations of all others, and students and citizen scientists are effectively contributing to real science.
To bring these same benefits to machine learning researchers, we introduce OpenML, an online service to share, organize and reuse data, code and experiments. Following best practices observed in other sciences, OpenML allows collaborations to scale effortlessly and rewards scientists for sharing their data more openly.
We believe that this new, networked approach to machine learning will allow scientists to work more productively, make new discoveries faster, be more visible, forge many new collaborations, and start new types of studies that were practically impossible before.</p>
<h2>8. ACKNOWLEDGEMENTS</h2>
<p>This work is supported by grant 600.065.120.12N150 from the Dutch Fund for Scientific Research (NWO), and by the IST Programme of the European Community, under the Harvest Programme of the PASCAL2 Network of Excellence, IST-2007-216886.
We also wish to thank Hendrik Blockeel, Simon Fisher and Michael Berthold for their advice and comments.</p>
<h2>9. REFERENCES</h2>
<p>[1] D. W. Aha. Generalizing from case studies: a case study. In Proceedings of the ninth international workshop on Machine learning, pages 1-10, San Francisco, CA, USA, 1992. Morgan Kaufmann Publishers Inc.
[2] A. Asuncion and D. Newman. UCI machine learning repository. University of California, School of Information and Computer Science, 2007.
[3] M. Berthold, N. Cebron, F. Dill, T. Gabriel, T. Kotter, T. Meini, P. Ohl, C. Sieb, K. Thiel, and B. Wiswedel. KNIME: The Konstanz information miner. Studies in Classification, Data Analysis, and Knowledge Organization, 5:319-326, 2008.
[4] A. Bifet, G. Holmes, R. Kirkby, and B. Pfahringer. MOA: Massive Online Analysis. Journal of Machine Learning Research, 11:1601-1604, 2010.
[5] B. Bischl. mlr: Machine Learning in R., 2013. R package version 1.1-18.
[6] H. Blockeel and J. Vanschoren. Experiment databases: Towards an improved experimental methodology in machine learning. Lecture Notes in Computer Science, $4702: 6-17,2007$.
[7] T. A. Boroson and T. R. Lauer. A candidate subparsec supermassive binary black hole system. Nature, 458(7234):53-55, 2009.
[8] P. Brazdil, C. Giraud-Carrier, C. Soares, and R. Vilalta. Metalearning: Applications to data mining. Springer, 2009.
[9] C. Cardamone, K. Schawinski, M. Sarzi, S. P. Bamford, N. Bennert, C. Urry, C. Lintott, W. C. Keel, J. Parejko, R. C. Nichol, et al. Galaxy zoo green peas: discovery of a class of compact extremely star-forming galaxies. Monthly Notices of the Royal Astronomical Society, 399(3):1191-1205, 2009.
[10] J. Carpenter. May the best analyst win. Science, 331(6018):698-699, 2011.
[11] S. Cooper, F. Khatib, A. Treuille, J. Barbero, J. Lee, M. Beenen, A. Leaver-Fay, D. Baker, Z. PopoviÄ‡, et al. Predicting protein structures with a multiplayer online game. Nature, 466(7307):756-760, 2010.
[12] K. A. Frazer, D. G. Ballinger, D. R. Cox, D. A. Hinds, L. L. Stuve, R. A. Gibbs, J. W. Belmont, A. Boudreau, P. Hardenbol, S. M. Leal, et al. A second generation human haplotype map of over 3.1 million SNPs. Nature, 449(7164):851-861, 2007.
[13] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. Witten. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10-18, 2009.
[14] D. Hand. Classifier technology and the illusion of progress. Statistical Science, 21(1):1-14, 2006.
[15] M. J. Hawrylycz, E. S. Lein, A. L. Guillozet-Bongaarts, E. H. Shen, L. Ng, J. A. Miller, L. N. van de Lagemaat, K. A. Smith, A. Ebbert, Z. L. Riley, et al. An anatomically comprehensive atlas of the adult human brain transcriptome. Nature, 489(7416):391-399, 2012.
[16] H. Hirsh. Data mining research: Current status and future opportunities. Statistical Analysis and Data Mining, 1(2):104-107, 2008.
[17] V. Hoste and W. Daelemans. Comparing learning approaches to coreference resolution. There is more to it than bias. In Proceedings of the ICML'05 Workshop on Meta-learning, pages 20-27, 2005.
[18] A. R. Isern. The ocean observatories initiative: Wiring the ocean for interactive scientific discovery. IEEE, 2006.</p>
<p>[19] T. Kealey. Sex, science and profits. Random House, 2010 .
[20] E. Keogh and S. Kasetty. On the need for time series data mining benchmarks: A survey and empirical demonstration. Data Mining and Knowledge Discovery, $7(4): 349-371,2003$.
[21] E. S. Lein, M. J. Hawrylycz, N. Ao, M. Ayres, A. Bensinger, A. Bernard, A. F. Boe, M. S. Boguski, K. S. Brockway, E. J. Byrnes, et al. Genome-wide atlas of gene expression in the adult mouse brain. Nature, $445(7124): 168-176,2007$.
[22] R. Leite, P. Brazdil, and J. Vanschoren. Selecting classification algorithms with active testing. In Machine Learning and Data Mining in Pattern Recognition, pages 117-131. Springer, 2012.
[23] C. J. Lintott, K. Schawinski, W. Keel, H. Van Arkel, N. Bennert, E. Edmondson, D. Thomas, D. J. Smith, P. D. Herbert, M. J. Jarvis, et al. Galaxy Zoo:Hanny's Voorwerp, a quasar light echo? Monthly Notices of the Royal Astronomical Society, 399(1):129-140, 2009.
[24] C. J. Lintott, K. Schawinski, A. Slosar, K. Land, S. Bamford, D. Thomas, M. J. Raddick, R. C. Nichol, A. Szalay, D. Andreescu, et al. Galaxy Zoo: morphologies derived from visual inspection of galaxies from the sloan digital sky survey. Monthly Notices of the Royal Astronomical Society, 389(3):1179-1189, 2008.
[25] I. Manolescu, L. Afanasiev, A. Arion, J. Dittrich, S. Manegold, N. Polyzotis, K. Schnaitter, P. Senellart, and S. Zoupanos. The repeatability experiment of SIGMOD 2008. ACM SIGMOD Record, 37(1):39-45, 2008.
[26] K. L. Masters, M. Mosleh, A. K. Romer, R. C. Nichol, S. P. Bamford, K. Schawinski, C. J. Lintott, D. Andreescu, H. C. Campbell, B. Crowcroft, et al. Galaxy Zoo: passive red spirals. Monthly Notices of the Royal Astronomical Society, 405(2):783-799, 2010.
[27] D. Michie, D. Spiegelhalter, and C. Taylor. Machine learning, neural and statistical classification. Ellis Horwood, Upper Saddle River, NJ, USA, 1994.
[28] M. Nielsen. The future of science: Building a better collective memory. APS Physics, 17(10), 2008.
[29] M. Nielsen. Reinventing discovery: the new era of networked science. Princeton University Press, 2012.
[30] E. Ostrom. Collective action and the evolution of social norms. The Journal of Economic Perspectives, pages $137-158,2000$.
[31] H. Parkinson, M. Kapushesky, M. Shojatalab, N. Abeygunawardena, R. Coulson, A. Farne, E. Holloway, N. Kolesnykov, P. Lilja, M. Lukk, et al. ArrayExpress: A public database of microarray experiments and gene expression profiles. Nucleic acids research, 35(suppl 1):D747-D750, 2007.
[32] T. Pedersen. Empiricism is not a matter of faith. Computational Linguistics, 34:465-470, 2008.
[33] C. Perlich, F. Provost, and J. Simonoff. Tree induction vs. logistic regression: A learning-curve analysis. Journal of Machine Learning Research, 4:211-255, 2003.
[34] B. Pfahringer, H. Bensusan, and C. Giraud-Carrier. Meta-learning by landmarking various learning algorithms. Proceedings of the International Conference on Machine Learning (ICML), 951(2000):743-750, 2000.
[35] J. Priem, P. Groth, and D. Taraborelli. The Altmetrics Collection. PLoS ONE, 11(7):e48753, 2012.
[36] M. J. Raddick, G. Bracey, P. L. Gay, C. J. Lintott, P. Murray, K. Schawinski, A. S. Szalay, and J. Vandenberg. Galaxy zoo: Exploring the motivations of citizen science volunteers. Astronomy Education Review, $9(1): 010103,2010$.
[37] M. E. Schwamb, C. J. Lintott, D. A. Fischer, M. J. Giguere, S. Lynn, A. M. Smith, J. M. Brewer, M. Parrish, K. Schawinski, and R. J. Simpson. Planet hunters: Assessing the kepler inventory of short-period planets. The Astrophysical Journal, 754(2):129, 2012.
[38] S. Sonnenburg, M. Braun, C. Ong, S. Bengio, L. Bottou, G. Holmes, Y. LeCun, K. Muller, F. Pereira, C. Rasmussen, G. Ratsch, B. Scholkopf, A. Smola, P. Vincent, J. Weston, and R. Williamson. The need for open source software in machine learning. Journal of Machine Learning Research, 8:2443-2466, 2007.
[39] A. S. Szalay, J. Gray, A. R. Thakar, P. Z. Kunszt, T. Malik, J. Raddick, C. Stoughton, and J. vandenBerg. The sdss skyserver: public access to the sloan digital sky server data. In Proceedings of the 2002 ACM SIGMOD international conference on Management of data, pages 570-581. ACM, 2002.
[40] L. Torgo. Data Mining with R: Learning with Case Studies. Chapman \&amp; Hall/CRC, 1st edition, 2010.
[41] J. N. van Rijn, B. Bischl, L. Torgo, B. Gao, V. Umaashankar, S. Fischer, P. Winter, B. Wiswedel, M. R. Berthold, and J. Vanschoren. OpenML: A collaborative science platform. In Proceedings of ECMLPKDD 2013, pages 645-649, 2013.
[42] J. N. van Rijn, V. Umaashankar, S. Fischer, B. Bischl, L. Torgo, B. Gao, P. Winter, B. Wiswedel, M. R. Berthold, and J. Vanschoren. A RapidMiner extension for open machine learning. In RapidMiner Community Meeting and Conference 2013, pages 59-70, 2013.
[43] J. Vanschoren, H. Blockeel, B. Pfahringer, and G. Holmes. Experiment databases. A new way to share, organize and learn from experiments. Machine Learning, 87(2):127-158, 2012.
[44] M. Wojnarski, S. Stawicki, and P. Wojnarowski. Tunedit.org: System for automated evaluation of algorithms in repeatable experiments. Lecture Notes in Computer Science, 6086:20-29, 2010.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{27}$ The evaluation measures and the exact code can be found on http://www.openml.org/a.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{19}$ OpenML is available on http://www.openml.org
${ }^{20}$ In this sense, OpenML is similar to data mining challenge platforms, except that it allows users to work collaboratively, building on each other's work.
${ }^{21}$ In this paper, we only discuss the web interfaces. API details can be found on http://www.openml.org/api
${ }^{22} \mathrm{http}: / /$ www.cs.waikato.ac.nz/ml/weka/arff.html&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
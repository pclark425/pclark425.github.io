<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-20e6909ce6c5f12b61e5c9022d97134137360273</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/20e6909ce6c5f12b61e5c9022d97134137360273" target="_blank">Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This approach outperforms both goal-image specifications and language conditioned imitation techniques by more than 25%, and is able to perform visuomotor tasks from natural language, such as"open the right drawer" and"move the stapler", on a Franka Emika Panda robot.</p>
                <p><strong>Paper Abstract:</strong> We study the problem of learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. In order to accomplish this, humans need easy and effective ways of specifying tasks to the robot. Goal images are one popular form of task specification, as they are already grounded in the robot's observation space. However, goal images also have a number of drawbacks: they are inconvenient for humans to provide, they can over-specify the desired behavior leading to a sparse reward signal, or under-specify task information in the case of non-goal reaching tasks. Natural language provides a convenient and flexible alternative for task specification, but comes with the challenge of grounding language in the robot's observation space. To scalably learn this grounding we propose to leverage offline robot datasets (including highly sub-optimal, autonomously collected data) with crowd-sourced natural language labels. With this data, we learn a simple classifier which predicts if a change in state completes a language instruction. This provides a language-conditioned reward function that can then be used for offline multi-task RL. In our experiments, we find that on language-conditioned manipulation tasks our approach outperforms both goal-image specifications and language conditioned imitation techniques by more than 25%, and is able to perform visuomotor tasks from natural language, such as"open the right drawer"and"move the stapler", on a Franka Emika Panda robot.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>distilBERT (used in LOReL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>distilBERT-base-uncased sentence encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed pretrained sentence encoder (distilBERT) is used to embed natural language instructions to 768-d vectors which are concatenated with visual embeddings and fed to a binary reward classifier; this pretrained language model enables zero-shot generalization to unseen rephrasings of instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>distilBERT (as language encoder within LOReL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pretrained transformer-based sentence encoder (distilbert-base-uncased) producing 768-dimensional sentence embeddings; used frozen in LOReL to encode language commands which are concatenated with image embeddings and input to a binary reward classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>natural language (large-scale text corpora / sentence-level masked-language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper uses the standard distilbert-base-uncased sentence encoder (15-layer transformer, 768-d output) as provided by the HuggingFace model zoo; this paper does not re-specify the original pretraining corpora or sizes (see the DistilBERT reference for those details).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Simulated Meta-World manipulation tasks and real Franka Emika Panda desk tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Simulation: a Sawyer arm on a tabletop with a drawer, a faucet, and two mugs; six tasks (open/close drawer, turn faucet left/right, push black mug right, push white mug down). Real robot: Franka Emika Panda over IKEA desk with two drawers and a cabinet and diverse objects; five tasks (open left/right drawer, move stapler, reach marker, reach cabinet). Observations are RGB images (64x64), actions are delta end-effector controls.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for distilBERT pretraining (the model encodes raw text sentences/tokens; no action space in text pretraining is used inside this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous delta end-effector control (sim: delta EE control, episode horizon 20 timesteps; real: delta EE control up to 7cm and optional grasping though grasping not used in evaluations; MPC plans sequences of length 20 in sim and 5 on real robot).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No direct word→motor mapping; distilBERT embeddings condition a binary reward classifier R_theta(s0, s, l) that predicts whether a state change satisfies the instruction; that learned reward is combined with a learned visual dynamics model and CEM-based visual model predictive control to produce low-level continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB vision (64x64) in simulation; real robot uses 4 RGB camera views (each 64x64) including wrist and bird's-eye views; visual dynamics model trained on these images.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using distilBERT within LOReL, the method outperforms language-conditioned behavior cloning and offline Q-learning by >25% on the 6 simulated tasks; in real-robot evaluations LOReL (with distilBERT) attains an average success of 66% across 5 tasks (10 trials each) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>The paper reports that removing the pretrained language encoder degrades performance: seen-instruction performance is worse and generalization to unseen rephrasings drops more (stated as drops up to 23% without pretrained LM vs up to 10% with it); exact per-task numeric baselines without pretraining are not fully enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Simulation training used 50,000 episodes (1M frames) collected by a random policy; the classifier and models trained on that dataset. Real-robot experiments used a 3000-episode (150k frame) dataset borrowed from prior work, with 6000 crowd-sourced annotations filtered to ~1.6k episodes used for training; with the pretrained distilBERT encoder, LOReL learned effective rewards and policies from these dataset sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not precisely quantified; paper states models without the pretrained LM require more data to generalize and suffer larger drops on unseen language (up to 23% drop reported) but no direct sample counts to reach parity are given.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative: pretrained distilBERT enables robust zero-shot generalization to unseen rephrasings with only the available offline datasets (sim: 50k episodes; real: ~1.6k annotated episodes), reducing the drop in success on unseen language to ≤~10% versus up to ~23% without the pretrained encoder. The paper does not report a numeric fold-change in sample count.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Semantic priors in the pretrained language encoder that map varied phrasings to similar embeddings; freezing a high-quality sentence encoder reduces the need to learn language semantics from limited robot-labeled data; concatenation of language embedding with visual embeddings and data augmentation (embedding noise) improves robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper notes remaining limitations: pretrained language encoder does not address perception-action gaps (visual grounding still needed); LOReL cannot capture path-dependent behaviors; small/biased real datasets and noisy annotations can limit reward learning despite pretrained language features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a frozen pretrained sentence encoder (distilBERT) to embed natural language instructions into a learned visual reward classifier substantially improves generalization to unseen instruction phrasings and enables effective transfer of language understanding into embodied visuomotor control via a reward+MPC pipeline; this enables strong zero-shot robustness to rephrasings with relatively modest offline datasets (50k sim episodes; ~1.6k labeled real episodes), although exact quantitative sample-efficiency gains are reported qualitatively rather than as precise multipliers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1819.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1819.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hill et al. (transfer from text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human instruction-following with deep reinforcement learning via transfer-learning from text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that studies transfer-learning from text to instruction-following RL agents; cited as an example where text-models can help instruction-following, but only mentioned in related work of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human instruction-following with deep reinforcement learning via transfer-learning from text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Hill et al. transfer-learning from text (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Mentioned as prior work that uses transfer-learning from text for instruction-following with deep RL; this paper cites it for the idea that pretrained text models can aid learning grounded language policies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>text (transfer-learning from text is the motivating approach in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Cited as demonstrating that pretrained language knowledge can aid grounded instruction following; specific factors not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This paper references Hill et al. to support the claim that pretraining on text can provide useful ungrounded language priors that simplify learning of grounded language in embodied settings; the current paper empirically shows a related effect by using a pretrained sentence encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human instruction-following with deep reinforcement learning via transfer-learning from text <em>(Rating: 2)</em></li>
                <li>Grounding language in play <em>(Rating: 2)</em></li>
                <li>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter <em>(Rating: 2)</em></li>
                <li>From language to goals: Inverse reinforcement learning for vision-based instruction following <em>(Rating: 1)</em></li>
                <li>Language-conditioned imitation learning for robot manipulation tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1819",
    "paper_id": "paper-20e6909ce6c5f12b61e5c9022d97134137360273",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "distilBERT (used in LOReL)",
            "name_full": "distilBERT-base-uncased sentence encoder",
            "brief_description": "A fixed pretrained sentence encoder (distilBERT) is used to embed natural language instructions to 768-d vectors which are concatenated with visual embeddings and fed to a binary reward classifier; this pretrained language model enables zero-shot generalization to unseen rephrasings of instructions.",
            "citation_title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "mention_or_use": "use",
            "model_agent_name": "distilBERT (as language encoder within LOReL)",
            "model_agent_description": "A pretrained transformer-based sentence encoder (distilbert-base-uncased) producing 768-dimensional sentence embeddings; used frozen in LOReL to encode language commands which are concatenated with image embeddings and input to a binary reward classifier.",
            "pretraining_data_type": "natural language (large-scale text corpora / sentence-level masked-language pretraining)",
            "pretraining_data_details": "The paper uses the standard distilbert-base-uncased sentence encoder (15-layer transformer, 768-d output) as provided by the HuggingFace model zoo; this paper does not re-specify the original pretraining corpora or sizes (see the DistilBERT reference for those details).",
            "embodied_task_name": "Simulated Meta-World manipulation tasks and real Franka Emika Panda desk tasks",
            "embodied_task_description": "Simulation: a Sawyer arm on a tabletop with a drawer, a faucet, and two mugs; six tasks (open/close drawer, turn faucet left/right, push black mug right, push white mug down). Real robot: Franka Emika Panda over IKEA desk with two drawers and a cabinet and diverse objects; five tasks (open left/right drawer, move stapler, reach marker, reach cabinet). Observations are RGB images (64x64), actions are delta end-effector controls.",
            "action_space_text": "N/A for distilBERT pretraining (the model encodes raw text sentences/tokens; no action space in text pretraining is used inside this paper)",
            "action_space_embodied": "Continuous delta end-effector control (sim: delta EE control, episode horizon 20 timesteps; real: delta EE control up to 7cm and optional grasping though grasping not used in evaluations; MPC plans sequences of length 20 in sim and 5 on real robot).",
            "action_mapping_method": "No direct word→motor mapping; distilBERT embeddings condition a binary reward classifier R_theta(s0, s, l) that predicts whether a state change satisfies the instruction; that learned reward is combined with a learned visual dynamics model and CEM-based visual model predictive control to produce low-level continuous actions.",
            "perception_requirements": "RGB vision (64x64) in simulation; real robot uses 4 RGB camera views (each 64x64) including wrist and bird's-eye views; visual dynamics model trained on these images.",
            "transfer_successful": true,
            "performance_with_pretraining": "Using distilBERT within LOReL, the method outperforms language-conditioned behavior cloning and offline Q-learning by &gt;25% on the 6 simulated tasks; in real-robot evaluations LOReL (with distilBERT) attains an average success of 66% across 5 tasks (10 trials each) as reported in Table 2.",
            "performance_without_pretraining": "The paper reports that removing the pretrained language encoder degrades performance: seen-instruction performance is worse and generalization to unseen rephrasings drops more (stated as drops up to 23% without pretrained LM vs up to 10% with it); exact per-task numeric baselines without pretraining are not fully enumerated in the paper.",
            "sample_complexity_with_pretraining": "Simulation training used 50,000 episodes (1M frames) collected by a random policy; the classifier and models trained on that dataset. Real-robot experiments used a 3000-episode (150k frame) dataset borrowed from prior work, with 6000 crowd-sourced annotations filtered to ~1.6k episodes used for training; with the pretrained distilBERT encoder, LOReL learned effective rewards and policies from these dataset sizes.",
            "sample_complexity_without_pretraining": "Not precisely quantified; paper states models without the pretrained LM require more data to generalize and suffer larger drops on unseen language (up to 23% drop reported) but no direct sample counts to reach parity are given.",
            "sample_complexity_gain": "Qualitative: pretrained distilBERT enables robust zero-shot generalization to unseen rephrasings with only the available offline datasets (sim: 50k episodes; real: ~1.6k annotated episodes), reducing the drop in success on unseen language to ≤~10% versus up to ~23% without the pretrained encoder. The paper does not report a numeric fold-change in sample count.",
            "transfer_success_factors": "Semantic priors in the pretrained language encoder that map varied phrasings to similar embeddings; freezing a high-quality sentence encoder reduces the need to learn language semantics from limited robot-labeled data; concatenation of language embedding with visual embeddings and data augmentation (embedding noise) improves robustness.",
            "transfer_failure_factors": "Paper notes remaining limitations: pretrained language encoder does not address perception-action gaps (visual grounding still needed); LOReL cannot capture path-dependent behaviors; small/biased real datasets and noisy annotations can limit reward learning despite pretrained language features.",
            "key_findings": "Using a frozen pretrained sentence encoder (distilBERT) to embed natural language instructions into a learned visual reward classifier substantially improves generalization to unseen instruction phrasings and enables effective transfer of language understanding into embodied visuomotor control via a reward+MPC pipeline; this enables strong zero-shot robustness to rephrasings with relatively modest offline datasets (50k sim episodes; ~1.6k labeled real episodes), although exact quantitative sample-efficiency gains are reported qualitatively rather than as precise multipliers.",
            "uuid": "e1819.0",
            "source_info": {
                "paper_title": "Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Hill et al. (transfer from text)",
            "name_full": "Human instruction-following with deep reinforcement learning via transfer-learning from text",
            "brief_description": "Referenced prior work that studies transfer-learning from text to instruction-following RL agents; cited as an example where text-models can help instruction-following, but only mentioned in related work of this paper.",
            "citation_title": "Human instruction-following with deep reinforcement learning via transfer-learning from text",
            "mention_or_use": "mention",
            "model_agent_name": "Hill et al. transfer-learning from text (referenced)",
            "model_agent_description": "Mentioned as prior work that uses transfer-learning from text for instruction-following with deep RL; this paper cites it for the idea that pretrained text models can aid learning grounded language policies.",
            "pretraining_data_type": "text (transfer-learning from text is the motivating approach in the cited work)",
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Cited as demonstrating that pretrained language knowledge can aid grounded instruction following; specific factors not detailed in this paper.",
            "transfer_failure_factors": null,
            "key_findings": "This paper references Hill et al. to support the claim that pretraining on text can provide useful ungrounded language priors that simplify learning of grounded language in embodied settings; the current paper empirically shows a related effect by using a pretrained sentence encoder.",
            "uuid": "e1819.1",
            "source_info": {
                "paper_title": "Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human instruction-following with deep reinforcement learning via transfer-learning from text",
            "rating": 2
        },
        {
            "paper_title": "Grounding language in play",
            "rating": 2
        },
        {
            "paper_title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "rating": 2
        },
        {
            "paper_title": "From language to goals: Inverse reinforcement learning for vision-based instruction following",
            "rating": 1
        },
        {
            "paper_title": "Language-conditioned imitation learning for robot manipulation tasks",
            "rating": 1
        }
    ],
    "cost": 0.014310249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</h1>
<p>Suraj Nair ${ }^{1}$, Eric Mitchell ${ }^{1}$, Kevin Chen ${ }^{1}$, Brian Ichter ${ }^{2}$, Silvio Savarese ${ }^{1}$, Chelsea Finn ${ }^{1,2}$<br>${ }^{1}$ Stanford University, ${ }^{2}$ Robotics at Google</p>
<h4>Abstract</h4>
<p>We study the problem of learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. In order to accomplish this, humans need easy and effective ways of specifying tasks to the robot. Goal images are one popular form of task specification, as they are already grounded in the robot's observation space. However, goal images also have a number of drawbacks: they are inconvenient for humans to provide, they can over-specify the desired behavior leading to a sparse reward signal, or under-specify task information in the case of non-goal reaching tasks. Natural language provides a convenient and flexible alternative for task specification, but comes with the challenge of grounding language in the robot's observation space. To scalably learn this grounding we propose to leverage offline robot datasets (including highly sub-optimal, autonomously collected data) with crowd-sourced natural language labels. With this data, we learn a simple classifier which predicts if a change in state completes a language instruction. This provides a language-conditioned reward function that can then be used for offline multi-task RL. In our experiments, we find that on language-conditioned manipulation tasks our approach outperforms both goalimage specifications and language conditioned imitation techniques by more than $25 \%$, and is able to perform visuomotor tasks from natural language, such as "open the right drawer" and "move the stapler", on a Franka Emika Panda robot.</p>
<p>Keywords: Natural Language, Offline RL, Visuomotor Manipulation</p>
<h2>1 Introduction</h2>
<p>We are motivated by the goal of generalist robots which can be commanded to complete a diverse range of manipulation tasks. Doing so requires humans to be able to effectively specify tasks for the robot to solve. One popular approach to task specification is through goal-states, which by definition are grounded in the robot's observation space, making them a natural choice for self-supervised techniques [1, 2, 3, 4]. However, goal-state specification comes with a number of drawbacks, including (a) human effort required in generating a goal state to provide the robot, (b) task overspecification resulting in a sparse reward signal (e.g. a goal image for the task of pushing a single object also specifies positions of all other objects and the robot itself), and (c) task under-specification (e.g. moving to the right indefinitely). Natural language presents a promising alternative form of specification, providing an easy way for humans to communicate tasks. Moreover, natural language can flexibly represent non-goal reaching tasks and tasks with varying degrees of specificity, such
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We learn language-conditioned visuomotor policies using sub-optimal offline data, crowd-sourced annotation, and pre-trained language models, enabling a real robot to complete language-specified tasks while being robust to complex rephrasings of the task description.</p>
<p>as grasping any marker from a desk with several markers, while a single goal image could only capture one instance of success. To this end, we study the problem of learning language-conditioned visuomotor manipulation skills from offline datasets of robotic interaction.</p>
<p>Despite the abundant benefits of being able to command robots with natural language, such agents have remained out of reach. A major challenge in acquiring such agents is that the language instructions need to be grounded in the agent's high-dimensional observation space. Learning this grounding is difficult, and requires diverse interaction data paired with language annotations. Recent works have made progress towards learning such grounding by annotating data collected by humans [5, 6, 7]; however, collecting many human teleoperated trajectories on real robots can be costly and time consuming, and is thus difficult to scale to a broad set of language conditioned behaviors.</p>
<p>Our key insight is that a practical and scalable way to ground language is to combine autonomouslycollected offline datasets of robotic interaction with post-hoc crowd-sourced natural language labels. Unlike prior work, we do not assume this data comes from a human expert or contains optimal actions, allowing the agent to leverage a wide range of data sources such as autonomous exploration data (e.g. random, scripted, intrinsically motivated), replay buffers of trained RL agents, human expert data (e.g. demonstrations, human play), and data without action labels. Given such pre-collected data, we can then use crowd-sourcing to scalably label trajectories with natural language labels describing the behaviors in the data. To learn from this sub-optimal data with noisy annotations, we learn a classifier which takes as input a natural language instruction and an initial and final image, and predicts whether or not the transition completes the instruction. This learned classifier can then be used as a language-conditioned reward for offline RL to learn language-conditioned behaviors.</p>
<p>Concretely, in this work we propose to learn language conditioned skills from vision using suboptimal, autonomously-collected offline data and crowd-sourced annotations (See Figure 1). We present a simple technique to learn language-conditioned rewards from this data, which we call Language-conditioned Offline Reward Learning (LORel), and combine it with visual modelpredictive control to complete language conditioned tasks (See Figure 2). In our experiments in simulation, we observe that even with data collected by a random policy our proposed method solves language-conditioned object manipulation tasks 25% more effectively than language conditioned imitation learning techniques, as well as $\sim 30\%$ more effectively than goal-image conditioned comparisons which over-specify the task. Additionally, we observe that by virtue of leveraging pretrained language models our learned reward is capable of generalizing from scripted language instructions to unseen natural language zero-shot, suggesting that knowledge in pretrained language models can enable more efficient learning of grounded language as observed in prior work [6, 8]. Finally, we leverage an existing real robot dataset of sub-optimal data, label the dataset using crowd-sourcing, and use it to complete five visuomotor tasks specified by natural language, such as “open the right drawer” or “move the stapler” on a real Franka Emika Panda robot.</p>
<h2>2 Related Work</h2>
<p>There is a rich literature of work which studies interactive agents, and grounding their behaviors in language [9, 10, 11, 12]. Many prior works have studied this problem in the context of instruction following, where an agent aims to complete a task specified by formal language/programs [13, 14, 15, 16, 17, 18] or natural language [10, 11, 19, 20, 21, 22]. While these approaches have been largely studied in simulated spatial games [19, 23, 24, 25] or in object-directed visual navigation in simulated robots [26, 27, 28, 29, 30, 31, 25] some of which include high-level object interaction [32], in this work we focus on the domain of learning control for vision-based robotic manipulation.</p>
<p>Early works have approached instruction following with strategies like semantic parsing mapped to motion primitives or pre-defined actions to execute tasks in virtual domains [33, 34, 35, 36] and on mobile robots [37, 38]. Like our approach these methods don't require expert demonstrations; however unlike these approaches, we directly learn robotic control from images and natural language instructions, and don't assume any predefined motion primitives. More recently, end-to-end deep learning has been used to condition agents on natural language instructions [39, 26, 40, 29, 41, 6, 7, 42], which are then trained under an imitation and/or reinforcement learning objective. In the reinforcement learning setting, works have adopted a range of strategies, from language-conditioned reinforcement learning while leveraging environment rewards [23, 43, 44, 45, 46, 47] to using language as a reward bonus to densify the environment reward and aid in exploration [48, 49, 50, 51, 52]. In contrast, we do not assume any environment provided reward signal, and rather aim to learn effective language-conditioned rewards from annotated data of interaction.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Language-conditioned Offline Reward Learning (LOReL). We propose a technique to learn language-conditioned behavior from offline datasets of robot interaction (left). To do so, we crowd-source natural language annotations describing the behavior in the offline data, and use it to learn a language-conditioned reward function (middle). We then combine this reward and a learned visual dynamics model through model predictive control to complete language specified tasks form vision on a real robot (right).</p>
<p>Numerous prior works have also studied learning language conditioned rewards online from demonstrations or examples of successful completion of tasks and language annotations [5, 24, 41, 51]. These works then use the learned reward to optimize policies through online RL, often using the agents own online experience to train the reward [24, 53]. While these works also learn language-conditioned rewards, and in some cases also use discriminative techniques to learn the reward [24, 51], running language-conditioned online RL on a physical robot can be prohibitively time consuming. Our work aims to learn language-conditioned behaviors from entirely offline datasets (which may be highly sub-optimal), making it feasible to learn language-conditioned behaviors on real robots.</p>
<p>Other works have studied using offline data in the form of demonstrations [7] or human teleoperated trajectories (i.e. "play data") [6], to learn language-conditioned robotic agents in simulation. Most related is Lynch and Sermanet [6] who also use crowd-sourcing to annotate play data with natural language instructions. Critically, these works treat the offline data as near optimal, to the extent that behavior cloning techniques can be used to learn language-conditioned policies. Unlike these works, we don't make any assumptions about the optimality of the actions in the collected data, allowing the agent to learn from broader offline datasets, including autonomously collected data, which can be considerably easier to collect at scale on a real robot than human tele-operation data [54, 55]. Moreover, we observe in Section 5.1 that our proposed approach outperforms imitation learning techniques on such data, and in Section 5.3 that our method is effective on a real robot.</p>
<p>Many prior works have studied how robots can learn to complete a wide range of tasks from vision. While many approaches have been taken to task-specification, including task IDs [56, 57], robot and human demonstrations [58, 59, 60], and meta-learning from rewards [61], a common approach is goal-conditioned learning [62, 63, 2, 1], where an agent learns to reach particular goal states or distributions [64]. Many approaches have been applied to this domain, ranging from goal-conditioned model-free learning [2, 65, 57, 66] with goal relabeling [63], model-based planning with a learned visual dynamics model [67, 68], to methods which combine the both [69]. Unlike these works, the focus of this work is multi-task visuomotor learning from natural language specifications. Furthermore, we find in Section 5.1 that using our language-conditioned reward we can more effectively complete tasks than leveraging a goal image specification, while requiring less human effort to specify the task.</p>
<h2>3 Preliminaries</h2>
<p>In this work we consider an interactive agent which aims to complete $K$ tasks ${\mathcal{T}<em 1="1">{k}}</em>}^{K} \subset \mathcal{T}$ where $\mathcal{T}$ denotes the space of all tasks. For each task $\mathcal{T<em i="i">{i} \in \mathcal{T}$ the agent operates in a Markov decision process $\operatorname{MDP} \mathcal{M}=\left(\mathcal{S}, \mathcal{A}, p, \mathcal{R}</em> \rightarrow{0,1}$}, T\right)$ where $\mathcal{S}$ is the state space (in our case RGB images), $\mathcal{A}$ is the robot's action space, $p\left(s_{t+1} \mid s_{t}, a_{t}\right)$ is the robot environment's stochastic dynamics, $\mathcal{R}_{i}: \mathcal{S} \times \mathcal{S</p>
<p>indicates the binary reward at state $s$ for completing task $\mathcal{T}<em 0="0">{i}$ from initial state $s</em>}$, and $T$ is the episode horizon. Lastly, let $\mathcal{L}$ denote the set of all natural language, and let $\mathcal{L<em i="i">{i} \subset \mathcal{L}$ denote the set of language instructions which describe task $i$. Note that there can exist many instructions $l \in \mathcal{L}</em>}$ which describe a task $\mathcal{T<em i="i">{i}$ (e.g. "pick up the blue marker" and "grasp and lift the blue marker"), and any particular instruction $l \in \mathcal{L}$ can describe multiple tasks (e.g. "pick up the marker" can describe the task of picking up the blue marker or the green marker).
In this work we assume that the true reward function $\mathcal{R}</em>}$ for each task $\mathcal{T<em n="n">{i}$ is unobserved, and must be inferred from natural language. Concretely, we assume access to an offline dataset $\mathcal{D}$ of $N$ trajectories, where each trajectory $\tau</em>}$ is a tuple containing a sequence of states and actions, and a single natural language instruction $\tau_{n}=\left(\left[\left(s_{0}, a_{0}\right),\left(s_{1}, a_{1}\right), \ldots,\left(s_{T}\right)\right], l_{n}\right)$. We assume that $l_{n} \in \mathcal{L<em i="i">{i}$ for at least one task $\mathcal{T}</em>} \in \mathcal{T}$ for which $\mathcal{R<em 0="0">{i}\left(s</em>}, s_{T}\right)=1$. Note $\mathcal{T<em k="k">{i}$ does not need to be in $\left{\mathcal{T}</em>\right}<em _theta="\theta">{1}^{K}$, meaning the offline data/annotations can consist of tasks unrelated to the robot's target tasks (e.g. "doing nothing"). Our goal then is to learn a parametrized reward model $\mathcal{R}</em>}: \mathcal{S} \times \mathcal{S} \times \mathcal{L} \rightarrow[0,1]$ which conditioned on a language instruction $l$, initial state $s_{0}$, and state $s$ infers the true reward function $\mathcal{R<em 0="0">{i}\left(s</em>}, s\right)$ for some task $\mathcal{T<em i="i">{i}$ for which $l \in \mathcal{L}</em>}$. Given $\mathcal{R<em t="0">{\theta}$, we aim to instantiate a stochastic language-conditioned policy $\pi: \mathcal{S} \times \mathcal{S} \times L \rightarrow \mathcal{A}$, which conditioned on a natural language instruction $l$ produces actions to maximize the expected sum of rewards $\sum</em>}^{T} \mathcal{R<em 0="0">{i}\left(s</em>$. Note that this formulation captures tasks that are reflected in a change of state, but not tasks which are path dependent (e.g. "close the drawer slowly").}, s_{t}\right)$ for the inferred task $\mathcal{T}_{i</p>
<h1>4 Language-conditioned Offline Reward Learning (LOReL)</h1>
<p>Now we describe how we go about learning our parametrized language-conditioned reward $\mathcal{R}_{\theta}$ from $\mathcal{D}$, as well as how we instantiate our language-conditioned policy $\pi$ to maximize the learned reward, also shown in Figure 2. Our key idea is that while we cannot make assumptions about the optimality of the behavior in $\mathcal{D}$, we can use the the initial and final states of each trajectory and provided language annotations to ground what changes in state correspond to successful completion of language instructions. Then to learn control we can leverage all of the data in $\mathcal{D}$ to learn a global task-agnostic model of the dynamics of the robots environment, which can be combined with the learned reward via model-predictive control (MPC) to complete language conditioned tasks.</p>
<h3>4.1 Learning the Reward Function</h3>
<p>Given the provided dataset $\mathcal{D}=\left[\tau_{1}, \ldots, \tau_{N}\right]$ of $N$ trajectories $\tau_{n}=\left(\left[\left(s_{0}, a_{0}\right),\left(s_{1}, a_{1}\right), \ldots,\left(s_{T}\right)\right], l_{n}\right)$, how might we go about learning our reward function $\mathcal{R}<em n="n">{\theta}$ ? Critically, the behavior policy which collected this data could be sub-optimal. Therefore, we cannot assume the optimality of any particular action taken. However, because the human provided annotations describe the task being completed in the video, the assumption we can make about the data is that going from the start to the end of the trajectory constitutes completion of $l</em>}$. Therefore, we implement our reward function as a binary classifier $\mathcal{R<em 0="0">{\theta}\left(s</em>$, current state $s$, and language instruction $l$ and predicts if going from the initial state to the current state satisfies the language instruction.
Training the reward function in this manner has numerous favorable properties. First, unlike explicitly predicting a single instance of a successful goal state for a language instruction or vice-versa, a classifier can easily capture the many-to-many mapping that exists between language instructions and tasks. Doing so allows it to capture the full space of successful behavior even in cases where there exists many possible language instructions $l$ which can describe completing a task $\mathcal{T}}, s, l\right)$ which looks at the initial state $s_{0<em 0="0">{i}$ and many possible pairs of initial and final states $\left(s</em>$ (described in detail in the next sections). We then minimize the binary cross entropy loss:}, s_{T}\right)$ which can constitute successfully completing any given instruction $l$. Second, by virtue of being context-dependent on the initial state, the reward function can be used in closed loop planning to perform behaviors indefinitely without additional specification (i.e. the reward for "move right" is relative to the agent's current position, so applying it iteratively will encourage the agent to continuously move right). Lastly, unlike other works which use classifiers for single-task reward learning [70, 71, 55] on robots, a languageconditioned reward classifier can flexibly represent many tasks with an easy to provide form of task-specification. Concretely, we sample positive examples $\left(s_{0}, s_{T}, l_{t}\right) \in \tau_{n} \sim \mathcal{D}$ from the annotated dataset which constitute successfully completing an instruction and generate negative examples which don't complete instructions $\left(s_{0}^{\prime}, s_{T}^{\prime}, l^{\prime}\right) \sim \mathcal{N}$ also from $\mathcal{D</p>
<p>$$
\mathcal{J}(\theta)=\mathbb{E}<em 0="0">{\left(s</em>}, s_{T}, l\right) \sim \mathcal{D}}\left[\log \left(\mathcal{R<em 0="0">{\theta}\left(s</em>}, s_{T}, l\right)\right)\right)+\mathbb{E<em 0="0">{\left(s</em>}^{\prime}, s_{T}^{\prime}, l^{\prime}\right) \sim \mathcal{N}}\left[\log \left(1-\mathcal{R<em 0="0">{\theta}\left(s</em>\right)\right)\right]
$$}^{\prime}, s_{T}^{\prime}, l^{\prime</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Training LOReL. We train LOReL on balanced batches of positive examples where the initial/final image transition satisfies the language command (left), negative examples where the initial/final states satisfy a different instruction (middle), and negative examples where the initial and final image are reversed (right).</p>
<p>Positive Selection. Selecting positive examples for the classifier is straightforward, as we know the initial and final state in an episode labeled with instruction <em>l</em> satisfy that command. However, it is also highly likely that there are other states near the beginning and end of the episode for which the instruction is satisfied. Therefore we employ a noisy labeling scheme where we label any (<em>s</em><sup><em>i</em></sup>, <em>s</em><sup><em>j</em></sup>, <em>l</em>) for which <em>i</em> ≤ <em>αT</em> and <em>j</em> ≥ (1−<em>α</em>)<em>T</em> as a positive. Higher values of <em>α</em> may occasionally include false positives, but also significantly increase the set of positives which can be used for training.</p>
<p>Negative Selection. First, we choose initial and final states from episodes with different language instructions as negatives. Specifically, for language command <em>l</em>, we select negatives <em>s</em><sup><em>l</em></sup><sub><em>0</em></sub>, <em>s</em><sup><em>l</em></sup><sub><em>T</em></sub> by selecting any (<em>s</em><sup><em>0</em></sup>, <em>s</em><sup><em>T</em></sup>, <em>l</em><sup><em>l</em></sup>) ∼ <em>D</em> where <em>l</em><sup><em>l</em></sup> ≠ <em>l</em>. Note that since there may be instructions <em>l</em><sup><em>l</em></sup> ≠ <em>l</em> which describe the same task, this may occasionally yield false negatives, however like prior work [70, 55] we find that we can learn an effective reward despite noisy negatives. Second, to encourage the reward to capture temporal progress (as opposed to focusing on spurious visual features), we also include the example (<em>s</em><sup><em>T</em></sup>, <em>s</em><sup><em>0</em></sup>, <em>l</em>) as a negative in training <em>R</em><sub><em>θ</em></sub>. Ultimately, we train on balanced batches of positive examples and both types of negative examples (See Figure 3).</p>
<p>Data Augmentation. Reward functions trained using classifiers have been shown to be prone to over-fitting creating a sparse or incorrect reward signal [55]. This issue is further exacerbated by the fact that we only have limited positive examples per episode. To combat this, we use visual data augmentation in the form of affine transformations and color jitter as well as uniform noise in the embedding space of language instructions to prevent classifier over-fitting.</p>
<p>Leveraging Pre-Trained Language Models. Finally, learning the meaning of raw natural language while simultaneously grounding the robot's actions using only crowd-sourced data of a few thousand robot episodes with language annotations poses a significant challenge. Therefore we leverage a fixed pre-trained distilBERT sentence encoder [72], to encode the natural language commands into a fixed length vector in ℝ<sup>768</sup> before they go into the classifier. We find in Section 5.2 that by using the pre-trained model we can generalize to unseen natural language commands from synthetic data.</p>
<h3>4.2 Learning Language Conditioned Policies with Visual Model Predictive Control</h3>
<p>Once trained, the learned reward function <em>R</em><sub><em>θ</em></sub> in principle could be used with any form of offline reinforcement learning to learn language-conditioned policies. In this work, we aim to learn visuomotor control on real robots from large datasets of sub-optimal or even random offline data. Model-based RL techniques have been particularly effective in this endeavor [67, 60], and in our case all offline data can be used to train a single task-agnostic visual dynamics model. We then use this model with planning to maximize the learned language-conditioned reward <em>R</em><sub><em>θ</em></sub>. Specifically, we learn a forward visual dynamics model <em>s</em><sub><em>t</em></sub>+1 ∼ <em>p</em><sub><em>θ</em></sub>(<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>) which is trained on the entire offline dataset <em>D</em>, and does not use language annotations. We leverage off-the-shelf action-conditioned video prediction frameworks for learning this model [73, 74], which we describe in detail in the supplement.</p>
<p>Given the learned dynamics model <em>p</em><sub><em>θ</em></sub> and the learned reward function <em>R</em><sub><em>θ</em></sub>, we then use model predictive control to instantiate a policy to complete language-conditioned tasks. Specifically, given a language instruction <em>l</em> and initial state <em>s</em><sup><em>0</em></sup>, we sample <em>M</em> different actions sequences of length <em>H</em>, which we feed through <em>p</em><sub><em>θ</em></sub> to get a predicted future state <em>ŝ</em><sup><em>t</em></sup><sub><em>t</em></sub>+<em>H</em><sup><em>t</em></sup>. For each prediction we compute the reward as <em>R</em><sub><em>θ</em></sub>(<em>s</em><sup><em>0</em></sup>, <em>ŝ</em><sup><em>t</em></sup><sub><em>t</em></sub>+<em>H</em><sup><em>t</em></sup>, <em>l</em>). Action sequences are optimized to maximize reward using the cross-entropy method (CEM) [75], until the best action sequence is applied in the environment (Figure 4).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Executing Language-Conditioned Policies with LOReL. To execute language-conditioned behavior, we perform model predictive control with a learned visual dynamics model and LOReL. Specifically, from the initial state we predict many future states for different action sequences (left/middle). We then rank those sequences according to the LOReL reward for the user specified natural language instruction (middle). After multiple iterations, the best action sequence is stepped in the environment executing the task (right).</p>
<h1>5 Experiments</h1>
<p>In our experiments we aim to study three main questions. (1) How does our proposed method for learning language conditioned policies from offline data compare to both language-conditioned and goal-image conditioned prior methods? (2) By virtue of using pre-trained language models, to what extent can our method generalize to unseen natural language commands? (3) Can our method be used to solve visuomotor tasks on a real robot using crowd-sourced annotations? We study experiments (1) and (2) in simulation, and experiment (3) on a Franka Emika Panda robot positioned in front of a desk. For qualitative results and videos, please see https://sites.google.com/view/robotlore1.</p>
<h2>Simulated Domain</h2>
<p>We study our first two experimental questions in a simulated domain developed on top of the Meta-World [56] environment, where a simulated sawyer robot interacts on a tabletop with a drawer, a faucet, and two mugs (see Figure 5 (left)). In this domain, we collect an offline dataset of 50,000 episodes by running a random policy in the environment, and label episodes procedurally using the true environment state yielding 2311 unique instructions (see Figure 5 (right)). After training on this data, we evaluate on 6 seen tasks which involve (1) closing the drawer, (2) opening the drawer, (3) turning the faucet left and (4) right, and (5) pushing the black mug right, and (6) pushing the white mug down.</p>
<h3>5.1 Does LOReL enable effective language-conditioned behavior compared to prior work?</h3>
<p>In this experiment we aim to evaluate how LOReL compares to prior techniques for learning language and goal image conditioned behavior on the 6 target tasks described previously.</p>
<p>Comparisons. We compare LOReL (Ours) to language-conditioned behavior-cloning (LCBC), which imitates the behavior in the offline dataset conditioned on the language instruction label, which is reflective of prior works that use imitation learning to learn language-conditioned behavior [6, 7]. We also compare to language-conditioned RL (LCRL), which labels the final state in each episode as having reward 1 for the annotated language instruction and 0 elsewhere, and trains a language conditioned-policy using offline Q learning, which reflects a fully offline version of the low-level policy used in [43]. Furthermore, we compare to using a goal-image as the task specification instead of language, and provide the agent with a ground truth goal-image of the object in its desired position, with which we use either L2 pixel distance (Pixel) or LPIPS [76] similarity (LPIPS) as a planning cost, reflective of prior work in visual MPC [67, 77]. Finally, we include an (Oracle) which uses the ground truth dynamics model and ground truth reward indicating the upper bound on the performance of the CEM planner, as well as the performance of a (Random) policy. All comparisons use the same architecture and data where possible; see the supplement for further details.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Comparison to Prior Work. On 6 simulated language-conditioned tasks, we find that LOReL (Ours) outperforms language-conditioned imitation learning (LCBC) and Q-learning (LCRL) as well as goal-image task specification (LPIPS/Pixel) by over $25 \%$. Success rates/standard error computed over 3 seeds of 100 trials. Results. Figure 6 shows the success rates over 3 seeds of 100 trials, ordered by legend. We observe first that our proposed approach outperforms the next best method, language-conditioned behavior cloning, by more than $25 \%$. By learning a language-conditioned reward and planning over it, the robot executes the tasks more effectively than what it observed in the data. On the other hand, because the data is sub-optimal, language-conditioned imitation learning is only able to learn coarse directions associated with each task, and as a result fails on tasks that require more fine-grained motion like "turn faucet left". Second, we observe that language-conditioned RL with a binary reward struggles to learn at all, indicating the difficulty in jointly learning language-grounding and control. Finally, we find that using goal-images with a pixel cost also fails, performing comparably with a random policy. We observe that the agent tries to match the arm position in the goal instead of the interacting with the objects, highlighting the limitation of goal-images in their tendency to over-specify the task. Using LPIPS similarity improves performance, however is still $\sim 30 \%$ worse than our method.</p>
<h1>5.2 Can LOReL generalize zero-shot to unseen natural language commands?</h1>
<p>In our second experiment, we study our methods ability to generalize to unseen instructions by nature of using pre-trained language models. Specifically, for the six target tasks, we test our method with a rephrased instruction, which was completely unseen during training. We evaluate task performance on the Original commands, on the commands with an Unseen Verb (e.g. "turn faucet left" $\rightarrow$ "rotate faucet left"), on the command with an Unseen Noun (e.g. "move black mug right" $\rightarrow$ "move dark cup right"), and Unseen Verb+Noun (e.g. "close drawer" $\rightarrow$ "shut cabinet"). Finally, we also test on Unseen Natural Language commands collected from 9 human volunteers who were asked to rephrase the command in a creative way, for example "turn faucet left" $\rightarrow$ "Spin nozzle left". The full set of unseen instructions for each task is in the supplement.</p>
<p>In Table 1, we see that on average when changing the verb or noun, we only see a drop in success rate of $5 \%$, and when changing both or using human provided natural language, we see at most a $10 \%$ drop in success rate. Furthermore, we compare performance with and without using the pre-trained language model, and observe that without the pre-trained language model performance is worse on seen instructions and drops significantly more on unseen instructions (up to $23 \%$ vs $10 \%$ ), suggesting that the pre-trained language model is essential to learning and generalization, consistent with results in prior work on language-conditioned imitation [6]. This result also suggests that the ungrounded knowledge in large language models may enable learning language groundings from small datasets or entirely programmatic language which can generalize to natural language.</p>
<h3>5.3 Can LOReL be used to learn language-conditioned visuomotor skills on a real robot?</h3>
<p>Finally, we study the efficacy of our method in learning language-conditioned behavior on a real robot using sub-optimal offline data and crowd-sourced annotation. We consider a Franka Emika</p>
<p>Panda robot mounted over an IKEA desk with two drawers and a cabinet, which can hold a range of objects (Figure 7). The robot's observation space consists of 4 camera viewpoints, each providing $64 \times 64$ RGB images. The robot's action space is delta end-effector control.</p>
<p>Data and Annotation. We use an offline 3000 episode ( 150000 frame) dataset without any modification from concurrent work [78], which trains policies for different behaviors on the IKEA desk using online RL. As a result, our dataset consists of diverse behaviors, but is also sub-optimal in that it comes from the replay buffer of a learning policy which will often not complete any task or will complete tasks in highly sub-optimal ways. To annotate the data, we leverage crowdsourcing, specifically Amazon Mechanical Turk. We ask human annotators to describe the behavior, if any, that the robot is doing, and to phrase it as a command without any pre-specified template. We collect 6000 annotations, two per episode, containing a total of 1699 unique instructions, examples of which can be seen in Figure 7. We filter out episodes for which annotators wrote the robot did nothing or indicated they could not understand what the robot was doing. See supplement for details about the environment, data, tasks, distribution of annotations, and annotator interface.</p>
<p>Results. We find in Table 2 that, by training LOReL on this annotated dataset and using it for visual MPC with a learned dynamics model in this domain, the robot can complete 5 language-conditioned skills with a $66 \%$ success rate on average across skills. Additionally, we find that removing negative training examples with the initial/final state flipped (LOReL (-FN)) reduces performance by $30 \%$, suggesting that such negatives are important for the reward to capture temporal progress and prevent over-fitting to objects.</p>
<p>Finally, we test LOReL's robustness to more complex vocabulary and instruction length, by replacing the commands for opening the left drawer and moving the stapler with "Open the small black and white drawer on the left fully" and "Push the small gray stapler around on top of the black desk" respectively. We find that LOReL is still able to succeed $\mathbf{7 / 1 0}$ and $\mathbf{5 / 1 0}$ times respectively, providing evidence that it is robust to instruction phrasing, consistent with the results in Section 5.2.</p>
<h2>6 Limitations and Future Work</h2>
<p>We have presented LOReL, a technique for learning language conditioned behavior from offline data and crowd sourced annotation that is effective for visuomotor control on real robots and is capable of generalizing to unseen language instructions. However, a number of limitations remain. First, in its current form LOReL can only capture tasks which are reflected in some change of state, but cannot capture tasks which are path dependent (e.g. "move in a circle slowly"). One exciting direction for future work to address this is to train LOReL on full video clips. Second, while in this work we have focused on learning short-horizon skills from language, composing these skills to solve long-horizon language-specified tasks is important for making robots useful in the real world. Data sources with long-horizon behaviors, and more powerful planners and visual dynamics models are necessary to enabling these longer horizon tasks. Finally, while we have presented language specification as an alternative to goal images, goal images maintain the benefit of being self-supervised and in some cases can be effective task specification. Unifying both forms of specification for robots would be a valuable future direction.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task (10 Trials Each)</th>
<th style="text-align: right;">LOReL</th>
<th style="text-align: right;">LOReL (-FN)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"Open the left drawer"</td>
<td style="text-align: right;">$\mathbf{9 0 \%}$</td>
<td style="text-align: right;">$30 \%$</td>
</tr>
<tr>
<td style="text-align: left;">"Open the right drawer"</td>
<td style="text-align: right;">$\mathbf{4 0 \%}$</td>
<td style="text-align: right;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">"Move the stapler"</td>
<td style="text-align: right;">$\mathbf{5 0 \%}$</td>
<td style="text-align: right;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">"Reach the marker"</td>
<td style="text-align: right;">$\mathbf{7 0 \%}$</td>
<td style="text-align: right;">$\mathbf{7 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">"Reach the cabinet"</td>
<td style="text-align: right;">$\mathbf{8 0 \%}$</td>
<td style="text-align: right;">$\mathbf{8 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Average over tasks</td>
<td style="text-align: right;">$\mathbf{6 6 \%}$</td>
<td style="text-align: right;">$36 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Real Robot Results. Using LOReL we are able to complete 5 language-conditioned skills on a robot specified by language with a $66 \%$ success rate.</p>
<h1>Acknowledgments</h1>
<p>The authors would like to thank Bohan Wu for valuable input throughout the project and assistance in conducting the robot experiments. The authors would also like to thank members of the IRIS and RAIL labs for providing valuable feedback. Suraj Nair is funded in part by an NSF GRFP. Eric Mitchell is funded by a Knight-Hennessy Fellowship. The authors would also like to thank the AMT annotators who helped label the robot data. Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.</p>
<h2>References</h2>
<p>[1] C. Finn and S. Levine. Deep visual foresight for planning robot motion. In IEEE International Conference on Robotics and Automation (ICRA), 2017.
[2] A. V. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine. Visual reinforcement learning with imagined goals. In Advances in Neural Information Processing Systems, 2018.
[3] C. Colas, P. Fournier, O. Sigaud, and P.-Y. Oudeyer. Curious: Intrinsically motivated multi-task, multi-goal reinforcement learning. ArXiv, abs/1810.06284, 2018.
[4] B. Ichter, P. Sermanet, and C. Lynch. Broadly-exploring, local-policy trees for long-horizon task planning. ArXiv, abs/2010.06491, 2020.
[5] J. Fu, A. Balan, S. Levine, and S. Guadarrama. From language to goals: Inverse reinforcement learning for vision-based instruction following. ArXiv, abs/1902.07742, 2019.
[6] C. Lynch and P. Sermanet. Grounding language in play. ArXiv, abs/2005.07648, 2020.
[7] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor. Language-conditioned imitation learning for robot manipulation tasks. ArXiv, abs/2010.12083, 2020.
[8] F. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep reinforcement learning via transfer-learning from text. ArXiv, abs/2005.09382, 2020.
[9] T. Winograd. Understanding natural language. Cognitive Psychology, 3(1):1-191, 1972. ISSN 0010-0285. doi:https://doi.org/10.1016/0010-0285(72)90002-3. URL https://www. sciencedirect.com/science/article/pii/0010028572900023.
[10] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. 012006.
[11] T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In HRI 2010, 2010.
[12] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foerster, J. Andreas, E. Grefenstette, S. Whiteson, and T. Rocktäschel. A survey of reinforcement learning informed by natural language. In IJCAI, 2019.
[13] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy sketches. ArXiv, abs/1611.01796, 2017.
[14] M. Denil, S. G. Colmenarejo, S. Cabi, D. Saxton, and N. D. Freitas. Programmable agents. ArXiv, abs/1706.06383, 2017.
[15] S.-H. Sun, T.-L. Wu, and J. J. Lim. Program guided agent. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=BkxUvnEYDH.
[16] Y. Yang, J. Inala, O. Bastani, Y. Pu, A. Solar-Lezama, and M. Rinard. Program synthesis guided reinforcement learning. ArXiv, abs/2102.11137, 2021.
[17] B. Gangopadhyay, H. Soora, and P. Dasgupta. Hierarchical program-triggered reinforcement learning agents for automated driving. ArXiv, abs/2103.13861, 2021.
[18] E. Brooks, J. Rajendran, R. L. Lewis, and S. Singh. Reinforcement learning of implicit and explicit control flow in instructions. In ICML, 2021.</p>
<p>[19] S. I. Wang, P. Liang, and C. D. Manning. Learning language games through interaction. ArXiv, abs/1606.02447, 2016.
[20] D. Arumugam, S. Karamcheti, N. Gopalan, L. Wong, and S. Tellex. Accurately and efficiently interpreting human-robot instructions of varying granularities. Robotics: Science and Systems XIII, Jul 2017. doi:10.15607/rss.2017.xiii.056. URL http://dx.doi.org/10.15607/RSS. 2017.XIII. 056.
[21] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. ArXiv, abs/1706.05064, 2017.
[22] D. Arumugam, S. Karamcheti, N. Gopalan, E. C. Williams, M. Rhee, L. L. S. Wong, and S. Tellex. Grounding natural language instructions to semantic goal representations for abstraction and generalization. Autonomous Robots, 43:449-468, 2019.
[23] M. Jänner, K. Narasimhan, and R. Barzilay. Representation learning for grounded spatial reasoning. Transactions of the Association for Computational Linguistics, 6:49-61, 2018.
[24] D. Bahdanau, F. Hill, J. Leike, E. Hughes, S. Hosseini, P. Kohli, and E. Grefenstette. Learning to understand goal specifications by modelling reward. In $I C L R, 2019$.
[25] K. Chen, J. Chen, J. Chuang, M. V'azquez, and S. Savarese. Topological planning with transformers for vision-and-language navigation. ArXiv, abs/2012.05292, 2020.
[26] K. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, M. Wainwright, C. Apps, D. Hassabis, and P. Blunsom. Grounded language learning in a simulated 3d world. ArXiv, abs/1706.06551, 2017.
[27] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV), 2017.
[28] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[29] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov. Gated-attention architectures for task-oriented language grounding. In AAAI, 2018.
[30] H. Chen, A. Suhr, D. K. Misra, N. Snavely, and Y. Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12530-12539, 2019.
[31] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Vision-andlanguage navigation in continuous environments. 2020.
[32] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. URL https://arxiv.org/abs/1912.01734.
[33] G. Kuhlmann, P. Stone, R. J. Mooney, and Shavlik. Guiding a reinforcement learner with natural language advice: Initial results in robocup soccer. In AAAI 2004, 2004.
[34] D. L. Chen and R. J. Mooney. Learning to interpret natural language navigation instructions from observations. pages 859-865, August 2011. URL http://nn.cs.utexas.edu/?chen: aaai11.
[35] Y. Artzi and L. Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49-62, 2013. doi:10.1162/tacl_a_00209. URL https://www.aclweb.org/anthology/Q13-1005.</p>
<p>[36] J. Andreas and D. Klein. Alignment-based compositional semantics for instruction following. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1165-1174, Lisbon, Portugal, Sept. 2015. Association for Computational Linguistics. doi:10.18653/v1/D15-1138. URL https://www.aclweb.org/anthology/D15-1138.
[37] T. Kollar, S. Tellex, D. Roy, and N. Roy. Grounding verbs of motion in natural language commands to robots. Springer Tracts in Advanced Robotics, 79, 01 2010. doi:10.1007/ 978-3-642-28572-1_3.
[38] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. volume 2, 01 2011.
[39] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI, 2016.
[40] D. K. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017.
[41] V. Blukis, Y. Terme, E. Niklasson, R. A. Knepper, and Y. Artzi. Learning to map natural language instructions to physical quadcopter control using simulated flight. In CoRL, 2019.
[42] A. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, and O. Sigaud. Grounding Language to Autonomously-Acquired Skills via Goal Generation. In ICLR 2021 - Ninth International Conference on Learning Representation, Vienna / Virtual, Austria, May 2021. URL https: //hal.inria.fr/hal-03121146.
[43] Y. Jiang, S. Gu, K. Murphy, and C. Finn. Language as an abstraction for hierarchical deep reinforcement learning. In NeurIPS, 2019.
[44] H. Chan, Y. Wu, J. Kiros, S. Fidler, and J. Ba. Actrce: Augmenting experience via teacher's advice for multi-goal reinforcement learning. ArXiv, abs/1902.04546, 2019.
[45] J. D. Co-Reyes, A. Gupta, S. Sanjeev, N. Altieri, J. DeNero, P. Abbeel, and S. Levine. Guiding policies with language via meta-learning. ArXiv, abs/1811.07882, 2019.
[46] M. Chevalier-Boisvert, D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, T. Nguyen, and Y. Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. In ICLR, 2019.
[47] J. D. Kanu, E. Dessalene, X. Lin, C. Fermuller, and Y. Aloimonos. Following instructions by imagining and reaching visual goals. ArXiv, abs/2001.09373, 2020.
[48] R. Kaplan, C. Sauer, and A. Sosa. Beating atari with natural language guided reinforcement learning. ArXiv, abs/1704.05539, 2017.
[49] P. Goyal, S. Niekum, and R. Mooney. Using natural language for reward shaping in reinforcement learning. ArXiv, abs/1903.02020, 2019.
[50] X. E. Wang, Q. Huang, A. Çelikyilmaz, J. Gao, D. Shen, Y. Wang, W. Y. Wang, and L. Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6622-6631, 2019.
[51] S. P. Mirchandani, S. Karamcheti, and D. Sadigh. Ella: Exploration through learned language abstraction. ArXiv, abs/2103.05825, 2021.
[52] P. Goyal, S. Niekum, and R. Mooney. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. ArXiv, abs/2007.15543, 2020.
[53] G. Cideron, M. Seurin, F. Strub, and O. Pietquin. Self-educated language agent with hindsight experience replay for instruction following. ArXiv, abs/1910.09451, 2019.
[54] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn. Robonet: Large-scale multi-robot learning. In Conference on Robot Learning, 2019.</p>
<p>[55] A. S. Chen, H. Nam, S. Nair, and C. Finn. Batch exploration with examples for scalable robotic reinforcement learning. IEEE Robotics and Automation Letters, 2021.
[56] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, 2020.
[57] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. ArXiv, abs/2104.08212, 2021.
[58] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via meta-learning. In CoRL, 2017.
[59] T. Yu, C. Finn, S. Dasari, A. Xie, T. Zhang, P. Abbeel, and S. Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018.
[60] A. S. Chen, S. Nair, and C. Finn. Learning generalizable robotic reward functions from "in-the-wild" human videos. ArXiv, abs/2103.16817, 2021.
[61] T. Zhao, A. Nagabandi, K. Rakelly, C. Finn, and S. Levine. Meld: Meta-reinforcement learning from images via latent state models. ArXiv, abs/2010.13957, 2020.
[62] L. P. Kaelbling. Learning to achieve goals. In IN PROC. OF IJCAI-93, pages 1094-1098. Morgan Kaufmann, 1993.
[63] M. Andrychowicz, D. Crow, A. Ray, J. Schneider, R. H. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. ArXiv, abs/1707.01495, 2017.
[64] S. Nasiriany, V. H. Pong, A. Nair, A. Khazatsky, G. Berseth, and S. Levine. Disco rl: Distributionconditioned reinforcement learning for general-purpose policies. ArXiv, abs/2104.11707, 2021.
[65] B. Eysenbach, R. Salakhutdinov, and S. Levine. Search on the replay buffer: Bridging planning and reinforcement learning. In NeurIPS, 2019.
[66] Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan, B. Eysenbach, R. C. Julian, C. Finn, and S. Levine. Actionable models: Unsupervised offline reinforcement learning of robotic skills. ArXiv, abs/2104.07749, 2021.
[67] F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv:1812.00568, 2018.
[68] Y.-C. Lin, M. Bauzá, and P. Isola. Experience-embedded visual foresight. ArXiv, abs/1911.05071, 2019.
[69] S. Tian, S. Nair, F. Ebert, S. Dasari, B. Eysenbach, C. Finn, and S. Levine. Model-based visual planning with self-supervised functional distances. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=UcoXdfrORC.
[70] J. Fu, A. Singh, D. Ghosh, L. Yang, and S. Levine. Variational inverse control with events: A general framework for data-driven reward definition. In Advances in Neural Information Processing Systems, 2018.
[71] A. Singh, L. Yang, C. Finn, and S. Levine. End-to-end robotic reinforcement learning without reward engineering. In Proceedings of Robotics: Science and Systems, FreiburgimBreisgau, Germany, June 2019.
[72] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019.
[73] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine. Stochastic variational video prediction. arXiv:1710.11252, 2017.</p>
<p>[74] B.-H. Wu, S. Nair, R. Martín-Martín, L. Fei-Fei, and C. Finn. Greedy hierarchical variational autoencoders for large-scale video prediction. ArXiv, abs/2103.04174, 2021.
[75] R. Y. Rubinstein and D. P. Kroese. The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning. Springer Science \&amp; Business Media, 2013.
[76] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.
[77] S. Nair and C. Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. ArXiv, abs/1909.05829, 2020.
[78] B. Wu, S. Nair, L. Fei-Fei, and C. Finn. Example-driven model-based reinforcement learning for solving long-horizon visuomotor tasks. In 5th Annual Conference on Robot Learning, 2021. URL https://openreview.net/forum?id=_daqOuh6yXr.</p>
<h1>A Method Implementation Details</h1>
<h2>A. 1 LOReL</h2>
<p>We begin by describing the method implementation details for our method LOReL.</p>
<h2>A.1.1 Reward Function</h2>
<p>The reward function is trained as a binary classifier which takes as input the initial state $s_{0}$, the current state $s$, and language instruction $l$, and outputs a scalar [0,1] prediction.</p>
<p>Architecture. The network first concatenates the initial image $(64,64,3)$ and goal image $(64,64,3)$ channel wise. The resulting image is passed through a convolutional image encoder with ReLU activations with the following architecture [channels, kernel size, stride] (all with padding 1): [[32, 4, 2], [32, 4, 1], [64, 4, 2], [64, 4, 2], [128, 4, 2], [128, 4, 1], [256, 4, 2], [256, 4, 1]]. The output is flattened and passed through fully connected layers of size $[512,512,512,512, L]$ where $L$ is a hyperparameter for the image embedding size. In our real robot experiments the observations have 4 camera views, each $(64,64,3)$ so we concatenate each initial image and goal per view, then use the same convolutional architecture but with 4 groups.</p>
<p>Simultaneously, the language instruction is passed through a pretrained distilbert-base-uncased sentence encoder [72], available at https://huggingface.co/distilbert-base-uncased. The network performs sub-word tokenization using the distilber tokenizer, then passes the sentences through the 15 layer transformer network, outputting a real valued vector of size 768.</p>
<p>The resulting image encoding of size $L$ and the sentence encoding of size 768 are concatenated, and fed through a fully connected network of size $[L, L, L, 1]$ where the first three layers have ReLU activations and a 0.2 dropout, and the final scalar prediction goes through a Sigmoid activation.</p>
<p>Hyper-parameters. In our experiments we train the reward with $L=128$, batchsize of 32 , and Adam optimizer with learning rate 0.00001 . In our simulated experiments we use $\alpha=0$, that is, we only use the initial and final states of the episode for positives. In our real robot experiments we use $\alpha=0.25$, selecting positives with initial state in the first quarter of the episode and final states from the last quarter of the episode.</p>
<p>Augmentation. To prevent the classifier from over-fitting, we use visual data augmentation in the form of color jitter (brightness=0.02, contrast=0.02, saturation=0.02, hue=0.02) and affine transformations of up to 20 pixels on the images (translate $=(0.1,0.1)$, scale $=(0.9,1.1)$ ). We also add uniform random noise in $[-0.1,0.1]^{768}$ to the instruction embeddings.</p>
<h2>A.1.2 Visual Dynamics Model (Sim)</h2>
<p>In our simulation experiments, we train an action-conditioned video prediction model using Stochastic Variational Video Prediction [73]. We train it on the full simulated dataset of 1M frames for 300000 iterations with all default hyperparameters using the codebase https://github.com/tensorflow/ tensor2tensor.</p>
<h2>A.1.3 Visual Dynamics Model (Real Robot)</h2>
<p>On the real robot setup we leverage a pre-trained visual dynamics model on the robot desk setup using the GHVAE [74] architecture, that trains a VAE, encodes each of the 4 camera views, and learns a predictive model in the latent space.</p>
<h2>A.1.4 MPC Planner</h2>
<p>Finally for choosing actions with the learned reward and visual dynamics model we use model predictive control. Specifically, in simulation we optimize a single sequence of 20 actions which are stepped in the environment. We sample 200 action sequences of length 20, which are fed through the video prediction model and ranked according to the reward of their final state. The top $10 \%$ are used to refit the action sampling distribution, and the process is repeated 3 times before the best action sequence is stepped in the environment.</p>
<p>On the real robot, we optimize action sequences of length 5, using again 3 iterations of CEM, now with 48 sampled action sequences per iteration. After stepping each 5 step action sequence we step the agent in the environment, then repeat for the full 30 timestep episode. On the real robot, we use the same initial state $\mathrm{T}=0$ as the initial state throughout the full episode.</p>
<h1>A. 2 Language Conditioned Imitation Learning</h1>
<p>The language conditioned imitation learning agent takes in the current state $s$ and language instruction $l$ from that episode and is trained to minimize the mean squared error to the action taken in the data.
Architecture. This agent uses an identical image encoder, and identical distilbert sentence encoder as LOReL. The resulting embeddings are again concatenated and fed through a fully connected network of size $[L, L, L, A]$ where the first three layers have ReLU activations and a 0.2 dropout, and the final prediction outputs the $A$ dimensional action. Hyper-parameters. This agent is trained with $L=128$, batch-size of 32 and an Adam optimizer with learning rate 0.0001 . Augmentation. The BC agent uses the same visual augmentation as LOReL. Unlike LOReL which struggles with over-fitting to language instructions, the BC agent struggles with under-fitting and learning different behavior for different instructions, hence we omit the uniform noise in language embedding space.</p>
<h2>A. 3 Language Conditioned Reinforcement Learning</h2>
<p>The language-conditioned Q learning agent learns a language conditioned Q function which takes as input the initial state $s_{0}$, current state $s$, action $a$, and language instruction $l$ and aims to predict the discounted return for taking action $a$ in state $s$. It is trained on balanced batches of positive transitions which are terminal states with reward 1 , where the current state is the final state for the episode which was labeled with instruction $l$, and negative transitions where the current state is any state earlier in the episode and has reward 0 . The Q function is then trained with the standard Bellman error, where target Q values which maximize $s_{t+1}$ are compute by sampling $M$ actions and taking the one with the highest Q value. During evaluation at each timestep the agent selects the action which maximizes the Q value at the current state via sampling $M$ actions and taking the best one.
Architecture. The Q function uses an identical architecture to the LOReL classifier, with the exception that it also takes as input the action $a$ which is concatenated at each of the final 3 fully connected layers. Hyper-parameters. We use $L=128$, Adam optimizer with learning rate 0.0001 , $M=100$, and batch size 8 . Augmentation. Like the BC agent, the Q-learning agent uses the same visual augmentation as LOReL but omits noise in the language embedding space to help the policy learn to differentiate between different instructions.</p>
<h2>A. 4 Oracle, Random, Pixel, LPIPS Baseline</h2>
<p>The Oracle performance is computed using the exact same planner as LOReL, but uses a ground truth dynamics model and a ground truth cost computed on the environment state. The random policy simply samples random actions at each time steps. The Pixel baseline first receives a goal image in which the object has been moved to its desired goal position (which constitutes task success). It then uses the exact same visual dynamics model and planner, but with a cost function which minimizes the $\ell_{2}^{2}$ pixel distance between the goal image and the final image in each trajectory. The LPIPS baseline uses the same planner, dynamics, and goal image, but uses LPIPS similarity instead of negative pixel distance.</p>
<h2>B Environment/Data Details</h2>
<h2>B. 1 Simulation Environment</h2>
<p>The simulated environment is built off of Meta-World [56], and consists of a simulated sawyer arm interacting over a table with a drawer, a faucet, and two mugs. The agent is controlled using delta end effector control, and the episode horizon in the environment is 20 timesteps. The observation space is $(64,64,3)$ RGB images.</p>
<h2>B. 2 Simulation Data and Annotation</h2>
<p>We collect 50000 episodes in the simulated environment (each 20 timesteps) for a total of 1 M frames. Data is collected by a random policy which samples uniformly in the action space. During data collection the arm and scene is reset randomly each episode. To annotate each episode, we procedurally look at the true environment state at the beginning of the episode and at the end of the episode and record the change in position of the objects (including the drawer, faucet, and mugs). For each object change in state we generate a phrase based on the measured change, such as "move the black mug left" or "turn the faucet right". Then all the phrases for a single episode and shuffled and combined with an "and" to create the final annotation for that episode. If no object moves the episode is annotated with "Do nothing". Doing so produces around 2300 unique annotations total.</p>
<h1>B. 3 Robot Environment</h1>
<p>The robot environment consists of a real Franka Emika Panda robot operating over an Ikea desk which consists of a cabinet, 2 drawers, and numerous objects. The robot has 4 camera views, one one which is a side view of the table, one which is a birds eye view from the right side of the table, one which is a birds eye view from the left side of the table, and one wrist mounted camera. Each returns $(64,64,3)$ RGB images. The robots action space includes delta end-effector control (up to 7 cm ) and grasping (however we do not use the grasping capability in any of our evaluation tasks).</p>
<h2>B. 4 Robot Data and Annotation</h2>
<p>Our robot dataset consists of 3000 episodes, each 50 timesteps, taken directly without modification from concurrent work [78] which aims to learn a diverse det of skill using online RL. As a result, this data is from an autonomously trained RL agent, and contains a mix of meaningful and close to random behavior. Moreover, even when tasks are completed they may be completed in highly sub-optimal ways. Some of the tasks the agent is trained for include opening the drawers and cabinets, grasping and placing different objects, and inserting markers/plugs.
We then have each episode annotated using crowdsourcing, specifically Amazon Mechanical Turk (See Figure 8). We collect 2 annotations per episode for a total of 6000 annotations. During training, we filter out episodes where the annotators said the robot did nothing or they could not tell what the robot was doing, resulting in a final dataset of 1600 episodes. We visualize the top 25 annotations and their counts in the filtered dataset in Figure 9.</p>
<h2>C Experimental Evaluation/Task Details</h2>
<p>Next we describe the experimental setup in detail, specifically the tasks and their evaluation criteria.</p>
<h2>C. 1 Simulation Tasks.</h2>
<p>The simulation tasks used for evaluation are (1) closing the drawer, (2) opening the drawer, (3) turning the faucet left, (4) turning the faucet right, (5) moving the black mug right, and (6) moving the white mug down. For language conditioned approaches these tasks are specified with "close drawer", "open drawer", "turn faucet left", "turn faucet right", "move black mug right", and "move white mug down" respectively. For goal image comparisons, we generate goal images with the obejct moved to a position which satisfies the task, see Figure 10 for examples. For all drawer and mug tasks success is if the object moves in the correct direction by at least 2 cm at any point in the episode. For the faucet it is if it rotates at least $\pi / 10$ radians in the right direction at any point in the episode.</p>
<h2>C. 2 Experiment 1 Details</h2>
<p>For experiment 1 we compare each of the methods on the 6 tasks described previously. Using the same 50000 episode dataset we train 3 seed each of our method, the imitation baseline, and the RL baseline. Our method and the imitation baseline are trained for 390000 iterations, while the RL agent is trained until the Bellman loss plateaus at around 240000 iterations. For each task and each seed we conduct 100 random trials (with random initialization) to compute a success rate out of 100 . For the oracle, pixel, and random we directly run 300 random trials per task.</p>
<h2>C. 3 Experiment 2 Details</h2>
<p>In experiment 2 we consider the same setup as experiment 1, but rephrase the instructions as described in the main text. The full set of rephrased instructions can be found in Figure 11. For the human provided unseen natural language, we sample one of the provided instructions at random for each episode.</p>
<h2>C. 4 Robot Tasks</h2>
<p>On the real robot, we consider 5 tasks, (1) opening the left drawer, (2) opening the right drawer, (3) moving a stapler, (4) reach the markers in the left drawer, and (5) reaching the cabinet.
The success criteria for task (1) and (2) is to open the drawer at least 1 inch from its initial position at any point in the episode. For task (3) it is to translate the stapler on the table by any amount. For task (4) it is to reach the gripper tip within 1 inch of any marker in the left drawer. For task (5) it is making contact with the cabinet.
The initial state for task (1) is the robot above and to the right of the left drawer, and the drawer starts slightly open to make grasping easier. The initial state for task (2) is above and to the left of the right</p>
<p>Instructions: Given a video of a robot, write a sentence summarizing its behavior.
Write the sentence in the form of a command (i.e. "pick up the stapler" rather than "picking up the stapler").
Note: you can see the robot's behavior from 3 different camera viewpoints.
Some examples:
"Open the left drawer" NOT "Opening the left drawer",
"Reach the marker" NOT "Reaching the marker",
"Push the stapler up" NOT "Pushing the stapler up",
"Insert the plug into the socket"
"Open the cabinet"
Only write down an instruction that the robot successfuly completed. For example, if the robot tries to pick up the marker and gets close but fails, you should put "Reach to the marker" or "Grasp next to the marker" rather than "pick up the marker".</p>
<p>If the robot does not successfully complete any task, write something like:
"Do nothing"
"Wave the arm in the air"
If you can't see the robot or can't tell what the robot is doing, write: "NA"
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Annotation interface. The annotators are shown a video of the episode, and are given the above instructions. We collect 6000 annotations, 2 for each of the 3000 episodes of data.
drawer, and again the drawer starts slightly open. For task (3) the robot starts near the middle/right side of the desk, and the stapler is initialized randomly on the front half of the desk. For task (4) and (5) the robot has the same initialization as in task (1), and in task (4) the markers are randomly moved around within the left drawer. For each episode of each task the initial position is randomized up to 5 cm in any direction from the main initial position. All resets and successes are measured by human supervisor. See the website https://sites.google.com/view/robotlorel for videos of task completion.</p>
<h1>C. 5 Experiment 3 Details</h1>
<p>In the real robot experiment we conduct 10 trials of each task for our method and for an ablation of our method trained without flipped initial/final negatives. The episode horizon for each task is 30 timesteps, and the agent plans 5 actions at a time as described in Section A.1.4. We also run 10 trials of tasks (1) and (3) with more sophisticated instructions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Instruction</th>
<th style="text-align: left;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">open the left drawer</td>
<td style="text-align: left;">140</td>
</tr>
<tr>
<td style="text-align: left;">open left drawer</td>
<td style="text-align: left;">101</td>
</tr>
<tr>
<td style="text-align: left;">reach the stapler</td>
<td style="text-align: left;">93</td>
</tr>
<tr>
<td style="text-align: left;">reach for the marker</td>
<td style="text-align: left;">70</td>
</tr>
<tr>
<td style="text-align: left;">open the drawer</td>
<td style="text-align: left;">55</td>
</tr>
<tr>
<td style="text-align: left;">open the right drawer</td>
<td style="text-align: left;">51</td>
</tr>
<tr>
<td style="text-align: left;">reach the socket</td>
<td style="text-align: left;">48</td>
</tr>
<tr>
<td style="text-align: left;">insert the plug into the socket</td>
<td style="text-align: left;">46</td>
</tr>
<tr>
<td style="text-align: left;">reach to the marker</td>
<td style="text-align: left;">44</td>
</tr>
<tr>
<td style="text-align: left;">open right drawer</td>
<td style="text-align: left;">44</td>
</tr>
<tr>
<td style="text-align: left;">reach the green marker</td>
<td style="text-align: left;">41</td>
</tr>
<tr>
<td style="text-align: left;">reach the plastic hole with the green marker</td>
<td style="text-align: left;">41</td>
</tr>
<tr>
<td style="text-align: left;">reach for the stapler</td>
<td style="text-align: left;">34</td>
</tr>
<tr>
<td style="text-align: left;">reach the left drawer</td>
<td style="text-align: left;">34</td>
</tr>
<tr>
<td style="text-align: left;">reach to the stapler</td>
<td style="text-align: left;">32</td>
</tr>
<tr>
<td style="text-align: left;">open drawer</td>
<td style="text-align: left;">31</td>
</tr>
<tr>
<td style="text-align: left;">reach for marker</td>
<td style="text-align: left;">29</td>
</tr>
<tr>
<td style="text-align: left;">open and close the drawer</td>
<td style="text-align: left;">28</td>
</tr>
<tr>
<td style="text-align: left;">reach the plastic hole using the green marker</td>
<td style="text-align: left;">26</td>
</tr>
<tr>
<td style="text-align: left;">open and close the left drawer</td>
<td style="text-align: left;">25</td>
</tr>
<tr>
<td style="text-align: left;">grasp next to the marker</td>
<td style="text-align: left;">25</td>
</tr>
<tr>
<td style="text-align: left;">move the stapler around on the table</td>
<td style="text-align: left;">24</td>
</tr>
<tr>
<td style="text-align: left;">reach the black marker</td>
<td style="text-align: left;">23</td>
</tr>
<tr>
<td style="text-align: left;">open the cabinet</td>
<td style="text-align: left;">22</td>
</tr>
<tr>
<td style="text-align: left;">grasp next to the stapler</td>
<td style="text-align: left;">21</td>
</tr>
</tbody>
</table>
<p>Figure 9: Top Instructions. We list the top 25 instructions and the number of times they appear in the data in the filtered robot dataset.</p>
<h1>D Additional Results</h1>
<h2>D. 1 Additional Ablations</h2>
<p>First, we run an additional ablation in simulation which ablates the effect of both types of negative examples. When removing the randomly chosen negatives from different episodes, we see a dramatic drop in performance (average success rate $56 \%$ to $27 \%$ ), as this is the main way the agent learns the effect of different language instructions. When removing the flipped negatives in simulation, we see limited impact (average success rate $56 \%$ to $55 \%$ ). This is because the primary role of the flipped negatives are to prevent over-fitting to objects in the scene, and to capture temporal progress, and in simulation the scene is fixed, and has a large amount (50K) episodes of training data. This is in contrast to the real robot, with 1.6 K episodes, and many different scenes, and in that setting removing the flipped negatives drops average success rate from $66 \%$ to $36 \%$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: Example Initial/Goal Images. Examples of generated goal images for the Pixel baseline.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task (10 Trials Each)</th>
<th style="text-align: center;">LOReL</th>
<th style="text-align: center;">LOReL (-Filter)</th>
<th style="text-align: center;">LOReL (-NP)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"Open the left drawer"</td>
<td style="text-align: center;">$\mathbf{9 0 \%}$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$30 \%$</td>
</tr>
<tr>
<td style="text-align: left;">"Open the right drawer"</td>
<td style="text-align: center;">$\mathbf{4 0 \%}$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$30 \%$</td>
</tr>
<tr>
<td style="text-align: left;">"Move the stapler"</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$\mathbf{7 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">'Reach the marker"</td>
<td style="text-align: center;">$\mathbf{7 0 \%}$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: left;">"Reach the cabinet"</td>
<td style="text-align: center;">$\mathbf{8 0 \%}$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$20 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Average over tasks</td>
<td style="text-align: center;">$\mathbf{6 6 \%}$</td>
<td style="text-align: center;">$26 \%$</td>
<td style="text-align: center;">$32 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Additional Real Robot Ablations. Our ablations suggest that both filtering out episodes which annotators labeled as "do nothing" and training with noisy positives are important for real robot performance.
more positive examples during training (LOReL (- NP)). First, we find removing filtering reduces performance considerably. In many cases annotators could not tell what the robot was doing, or wrote "do nothing" when they were unsure, making these labels particularly noisy. As a result, we filtered out these episodes primarily as a way of cleaning out noise in the data. While our simulation results suggest that our method can handle some behavior that completes no task ( $\sim 20 \%$ of the dataset), in the extreme case where almost half the data is labeled as not doing a task (as is the case in the real robot data), it can make learning an effective reward more difficult. Specifically, episodes of "do nothing" may dominate training batches and negative examples, making it harder to learn the difference between different language instructions, especially instructions that are infrequent in the data. In practice, post-hoc cleaning/filtering of the data requires little to no extra supervision, and can make training the reward considerably easier. Second, we find that generating more positive examples via noisy labeling is important in the robot domain, where there are only 1.6 K episodes of data (after filtering/cleaning).</p>
<h1>D. 2 LOReL Training Curves</h1>
<p>In Figure 12 (left) we include the train/test accuracy curves for LOReL in simulation, as well as the ablations of LOReL in simulation (LOReL, LOReL without the randomly chosen negatives, and LOReL without the flipped negatives). As expected, removing random negatives makes fitting the data much easier, but leads to a worse reward and worse planning performance. In Figure 12 (right) we include the train/test accuracy curves for LOReL on real robot data, including LOReL, LOReL without filtering, LOReL without flipped negatives, and LOReL without noisy positives). We see that without filtering, fitting the data is much more difficult, and without the noisy positives, fitting the data is easier (but can also leads to a worse reward function for planning). For all ablations the last checkpoint before test accuracy starts decreasing is used in planning.</p>
<h2>D. 3 Unseen Instruction Generalization Results</h2>
<p>In Figure 13 we include the per task success rates for our method when using the original command vs. unseen variations.</p>
<h2>D. 4 Qualitative Examples</h2>
<p>We include qualitative examples of the ranked predicted trajectories under different language instructions on the real robot in Figures 14, 15, 16, 17, and 18.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Task 1</th>
<th style="text-align: center;">Task 2</th>
<th style="text-align: center;">Task 3</th>
<th style="text-align: center;">Task 4</th>
<th style="text-align: center;">Task 5</th>
<th style="text-align: center;">Task 6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Close drawer</td>
<td style="text-align: center;">Open drawer</td>
<td style="text-align: center;">Turn faucet left</td>
<td style="text-align: center;">Turn faucet right</td>
<td style="text-align: center;">Move black mug right</td>
<td style="text-align: center;">Move white mug down</td>
</tr>
<tr>
<td style="text-align: center;">Unseen <br> Verb</td>
<td style="text-align: center;">shut drawer</td>
<td style="text-align: center;">pull drawer</td>
<td style="text-align: center;">rotate faucet left</td>
<td style="text-align: center;">rotate faucet right</td>
<td style="text-align: center;">push black mug right</td>
<td style="text-align: center;">push white mug down</td>
</tr>
<tr>
<td style="text-align: center;">Unseen <br> Noun</td>
<td style="text-align: center;">close container</td>
<td style="text-align: center;">open container</td>
<td style="text-align: center;">turn tap left</td>
<td style="text-align: center;">turn tap right</td>
<td style="text-align: center;">move dark cup right</td>
<td style="text-align: center;">move light cup down</td>
</tr>
<tr>
<td style="text-align: center;">Unseen <br> Verb+ <br> Noun</td>
<td style="text-align: center;">shut container</td>
<td style="text-align: center;">pull container</td>
<td style="text-align: center;">rotate tap left</td>
<td style="text-align: center;">rotate tap right</td>
<td style="text-align: center;">push dark cup right</td>
<td style="text-align: center;">push light cup down</td>
</tr>
<tr>
<td style="text-align: center;">Human <br> Provided</td>
<td style="text-align: center;">Push the drawer shut <br> Push the drawer <br> Shut the drawer <br> shut drawer <br> Slide the drawer closed <br> Shut the drawer <br> shut the dresser <br> Shut the drawer</td>
<td style="text-align: center;">Pull the drawer open <br> Pull the handle <br> Pull the drawer handle <br> Pull the drawer open <br> Pull open the drawer open the dresser <br> Pull the drawer. <br> Unclose the cabinet</td>
<td style="text-align: center;">Rotate the tap counterclock wise <br> Turn faucet away from camera <br> Rotate nozzle left <br> faucet counterclock wise <br> Rotate the faucet left <br> Turn the faucet to the left <br> rotate handle to the left <br> Turn faucet counter clockwise. <br> Spin nozzle left</td>
<td style="text-align: center;">Rotate tap clockwise <br> Turn faucet towards camera <br> Rotate nozzle right <br> faucet clockwise <br> Rotate the faucet right <br> Turn the faucet to the right <br> rotate handle rightward <br> Turn faucet clockwise. <br> Twirl valve right</td>
<td style="text-align: center;">Translate the black cup to the right <br> Move black mug away from drawer <br> Push black cup right <br> black mug right <br> Slide the black mug right <br> Move the dark mug to the right <br> push black cup right <br> Move black mug right. <br> Shift dark cup right</td>
<td style="text-align: center;">Translate the white cup down <br> Move with mug closer to the faucet <br> Bring white cup down <br> white mug down <br> Push the white mug down and left <br> Move the lighter mug down <br> shift white mug down <br> Pull white mug to the front. <br> Reposition white glass down</td>
</tr>
</tbody>
</table>
<p>Figure 11: Example Instructions used in Experiment 2.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 12: LOReL Learning Curves Learning Curves for LOReL and ablations in simulation (left) and on the real robot data (right).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 13: Generalization to Unseen Commands. We observe that by leveraging pre-trained language models, LOReL is able to generalize to unseen instructions with unseen verbs, nouns, and human generated instructions with a minimal ( $\leq 10 \%$ ) drop in performance. Success rates computed over 3 seeds of 100 trials.</p>            </div>
        </div>

    </div>
</body>
</html>
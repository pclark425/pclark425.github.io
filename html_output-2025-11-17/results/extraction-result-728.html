<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-728 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-728</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-728</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-c88e8d85fd5160b0793598bda037f977366acf7a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c88e8d85fd5160b0793598bda037f977366acf7a" target="_blank">Are GANs Created Equal? A Large-Scale Study</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures finds that most models can reach similar scores with enough hyperparameter optimization and random restarts, suggesting that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes.</p>
                <p><strong>Paper Abstract:</strong> Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \cite{goodfellow2014generative}.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e728.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e728.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperparameter reporting mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between paper-reported recommended hyperparameters and those required by code/experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that recommended hyperparameter settings (from original authors or papers) often do not transfer across data sets and that published reports tend to focus on best-found settings rather than distributions; the authors detect and quantify sensitivity by large-scale hyperparameter searches and show this mismatch materially affects results and rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>large-scale GAN benchmarking pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An open-source experimental pipeline for training many GAN variants with controlled architecture, performing wide and narrow random hyperparameter searches, retraining best settings with many seeds, and computing evaluation metrics (FID, precision/recall/F1).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / author-recommended hyperparameter lists</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training scripts and open-sourced model implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors note that hyperparameter values suggested in original papers or community reports frequently do not generalize across data sets; many papers report only the best hyperparameters (or the single best run), omitting distributions or the ranges explored, leading to misleading comparisons when code or reused settings are applied to new datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters / experimental protocol / reporting</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>large-scale empirical hyperparameter search (wide and narrow random sampling), transfer experiments (applying narrow ranges selected on one dataset to others), and retraining best-found settings with multiple random seeds</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparison of distributions of best-achieved FID (and F1) over many hyperparameter samples (100 wide-range, 50 narrow-range), reporting means and standard deviations, visualization (scatter plots Fig.6–9, Fig.14), and bootstrap estimates (5000 resamples of 100 runs) of minimum achievable FID as a function of budget</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: model rankings change with budget and hyperparameter search; 'bad' algorithms can outperform 'good' ones when given more tuning budget; recommending/fixing hyperparameters without search can yield much worse FID/F1. Quantitative examples: Table 2 shows large variance in FIDs (e.g., LSGAN on CIFAR: 87.1 ± 47.5), and the authors report that narrow-range transfer sometimes fails (some models more sensitive than others).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High: paper reports ubiquitous sensitivity across all tested GAN variants and datasets; variability observed across hyperparameter samples and seeds; specific cases of significant outliers and failures (see Table 2 and notes).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Author-reported hyperparameters are underspecified for other data sets, community tuning biases, omission of ranges and variability in papers, and implicit assumptions about optimization budget and dataset-specific settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Perform wide hyperparameter searches and report distributions (means, stddev, confidence intervals), publish hyperparameter ranges and search protocols, retrain top settings with multiple random seeds, and open-source code and exact search configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: authors show that wide search followed by retraining (50 seeds) yields reliable estimates of stability; bootstrap analyses demonstrate how increasing budget changes minimum achievable FID and reduces apparent algorithmic differences (figures 3, 14, 15).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / generative adversarial networks (GANs)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are GANs Created Equal? A Large-Scale Study', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e728.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e728.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reporting-best-vs-distribution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reporting single best-run results versus distributions of outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies a common reporting gap where studies present only the single best result (best FID) rather than the distribution across random restarts and hyperparameter variations; the authors argue this misalignment between natural-language claims and empirical variability misleads comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GAN evaluation and reporting protocol</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Protocol used by the authors to select best models per hyperparameter run, then re-run best settings multiple times to estimate stability and produce distributions of evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper results reporting / claimed performance statements</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts that select and log best checkpoints and metrics over training</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / selective reporting</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers commonly describe a single 'best' experimental outcome in prose and tables, while underlying code/experiments produce a distribution of outcomes due to randomness and instability; this creates a mismatch between the implied robustness in text and the observed variability.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>results reporting / evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Retraining best hyperparameter configurations with 50 different random seeds, plotting distributions of best-achieved FID and F1, and computing standard deviations and confidence intervals (bootstrap).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Statistical summaries: mean ± std of FID over 50 retrains (Table 2), bootstrap resampling for budget-based minimum FID distributions (5000 resamples), plotting histograms and CIs (Figures 3, 15), and reporting failure/outlier rates.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: reporting only best runs exaggerates method performance and misleads comparisons; the paper shows that variability is significant enough that algorithmic conclusions change when distributions are compared (e.g., no algorithm clearly dominates once distributions and budgets are considered).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread: authors note that many community reports and original papers present best results; their experiments reveal non-trivial variance across all models and datasets; marked outliers and failures present in multiple combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Cultural/practical incentives to report best numbers, lack of standards for reporting variability, and space/attention constraints in publications.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Recommend reporting full distributions (means, variances, confidence intervals), retraining top settings across seeds, and comparing distributions of minimum achievable metrics for fixed computational budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in clarifying true performance: authors' distributional reporting shows convergence of methods with increased budget and reveals when claims based on best runs are not robust.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are GANs Created Equal? A Large-Scale Study', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e728.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e728.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metric-specification gap (FID limits)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations and misalignment between metric descriptions and practical evaluation (Fréchet Inception Distance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper analyzes FID's properties and documents gaps between the metric's natural-language claims and its practical behavior, including bias, sensitivity to mode dropping, inability to detect overfitting, and potential dependence on the choice of encoding network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GAN evaluation metrics subsystem</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation stage that embeds images via a pre-trained network (Inception or VGG), computes sample means and covariances, and computes Fréchet distance as FID.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric specification in prior literature and paper's evaluation description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation scripts computing FID using embedding network activations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / metric limitation / encoding-dependence</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although FID is described as robust and aligned with human judgment, the authors find it has a non-negligible bias (high bias, low variance), is extremely sensitive to mode dropping, cannot detect overfitting (e.g., memory GANs), and may be influenced by the choice of encoding network (though they find high rank correlation between Inception and VGG embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / metric implementation and choice of embedding network</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical analyses: partitioning datasets into train/test subsamples, computing FID across many random partitions and subsamples, performing controlled mode-dropping experiments, and comparing FID computed with different encoders (Inception vs VGG).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantitative measures: bias and variance estimates of FID (Table 1 / optimistic FID estimates), sensitivity plots to mode dropping (Figure 1b), Spearman rank correlation between encoders (rho=0.9 for CelebA), and noting that FID cannot detect overfitting by construction.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate-to-high: metric limitations can mask overfitting and be 'fooled' by artifacts tailored to the encoder; mode dropping strongly degrades FID, and metric bias suggests absolute FID values must be interpreted cautiously. Authors therefore complement FID with precision/recall/F1 on controlled tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Metric limitation is universal to all experiments relying solely on FID or IS; observed consistently in the paper's robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Assumptions in metric derivation (Gaussian assumption on embeddings), reliance on pre-trained encoder (task/dataset mismatch), and lack of metric designed to detect memorization/overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use complementary metrics (precision, recall, F1 on constructed manifolds), test sensitivity to encoder choice, report bias/variance characteristics, and design tasks where precision/recall can be approximately computed (convex polygon datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: combining FID with precision/recall helps detect mode dropping and overfitting in controlled settings; encoder-sensitivity checks show high rank correlation but do not eliminate all encoder-specific artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>evaluation metrics for generative models / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are GANs Created Equal? A Large-Scale Study', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e728.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e728.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Underspecified experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Underspecification of experimental choices (architecture, budget, preprocessing, training schedule) vs code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper points out that many experimental choices (architecture, optimization details, number of epochs, use of batchnorm in discriminator, update frequencies) are often not standardized in natural-language descriptions, leading to mismatches when different codebases are used and affecting reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>experimental design / training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Unified training pipeline used by the authors where architecture is fixed across models (INFOGAN-like), batch size and epochs are fixed per dataset, and numerous options (batchnorm in discriminator, discriminator iterations, gradient penalties) are varied in hyperparameter searches.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methods section and experimental protocol descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training and preprocessing code / open-sourced scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous description</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often omit dataset-specific choices (number of epochs, batch sizes, architecture variants, normalization placement, imbalanced update frequencies), leading to ambiguity; when implementing or reusing code, these unspecified choices cause differences in results. The authors made explicit pragmatic choices (fix architecture, batch size, latent size) to reduce this source of mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture, training procedure, preprocessing, optimization hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Designing a controlled experimental setting where all models share an architecture and then observing sensitivity to toggles (e.g., batchnorm in discriminator, disc iters) and to the choice of optimizer and number of epochs across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical sensitivity analysis (scatter plots per hyperparameter in Figures 6–9), comparisons of performance with/without options (e.g., batchnorm true/false), and ablations (ADAM vs RMSProp in Appendix F).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Significant: varying these choices materially changes FID and F1 outcomes; some architectures or training settings can cause severe mode collapse or failures (noted as outliers and up to 20% failure rates for some combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common: many of these choices are influential across models and datasets as shown in the authors' sensitivity plots.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Papers omit or under-specify routine but influential experimental choices, and implementations sometimes introduce pragmatic tweaks not described in prose.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use a standardized architecture across comparisons, explicitly list all training choices, open-source code, perform controlled ablations and sensitivity analyses, and recommend reporting exact training scripts and seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective when adopted: standardizing architecture and publishing code allowed the authors to perform fair comparisons and reveal remaining differences are primarily due to hyperparameter tuning and budget rather than fundamental algorithmic advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>experimental methodology in machine learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are GANs Created Equal? A Large-Scale Study', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e728.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e728.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transferability gap of hyperparameter ranges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Failure of hyperparameter ranges tuned on one dataset to transfer to other datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study finds that hyperparameter ranges optimized on a single dataset (narrow-range search) do not always transfer well to other datasets, causing degraded performance when code implementations or recommended ranges are reused without dataset-specific tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>hyperparameter search and transfer protocol</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage protocol: (i) wide one-shot search (100 hyperparameter samples across broad ranges) to identify good ranges; (ii) narrow two-shot search (50 samples) using ranges selected from wide search on a reference dataset (Fashion-MNIST) and applying them to other datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / hyperparameter recommendation text</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>hyperparameter sampling scripts and training code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / incomplete generalization of recommended ranges</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Ranges found effective on a reference dataset (e.g., Fashion-MNIST) sometimes fail on other datasets like CIFAR or CelebA; some models (NS GAN) transfer better than others (WGAN) indicating a mismatch between a general recommendation in text and implementation needs on new data.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters / transfer across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Applying narrow-range hyperparameter searches derived from one dataset to others and measuring resulting performance and variance (Figure 14 and related discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparing FID variance and mean across datasets when hyperparameters are sampled from narrow ranges vs wide ranges; visualizing via narrow-range scatter plots (Fig.14) and reporting changes in mean FID and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Practical: for some models/datasets the narrow-range transfer results in degraded or unstable performance, meaning code or recommended ranges cannot be safely reused without further tuning; authors observe notable per-model differences in transfer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Moderate-to-high: observed in multiple model/dataset combinations; some models more robust to transfer than others.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Dataset-specific optimization landscapes, implicit dataset assumptions in tuning, and underreporting of dataset-specific hyperparameter dependence in papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Perform dataset-specific hyperparameter searches (even if narrow), publish full wide-range search results to guide transfer, and prefer methods with more robust transfer properties when computational budget is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: narrow-range searches derived from representative datasets can help but do not eliminate the need for dataset-specific tuning; wide searches remain more reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning experimentation / hyperparameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are GANs Created Equal? A Large-Scale Study', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e728.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e728.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hidden failure reporting (mode collapse/outliers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Underreporting of training failures and mode collapse in textual descriptions versus observed experimental code runs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that severe training failures and mode collapse occur in practice and are sometimes not reflected in textual claims; they quantify failure/outlier rates and mark some model/dataset combinations with high prevalence of failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>training stability monitoring and failure logging</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Empirical logging and analysis of many runs per model/hyperparameter setting to detect and quantify training failures (mode collapse, training divergence), including marking combinations with significant outlier or failure rates.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper descriptions of model robustness and failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training pipelines with monitoring, checkpointing, and failure detection</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing failure reporting / selective outcome reporting</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions often emphasize successful outcomes, whereas the exhaustive experimental runs reveal frequent outliers and failures (e.g., severe mode collapse), sometimes up to nontrivial percentages; the paper annotates Table 2 entries with '*' for significant outliers and '**' indicating up to 20% failures.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / experiment reporting</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Running many independent training runs across hyperparameter settings and seeds, recording failed/degenerate runs and extreme FID outliers, and annotating prevalence in results tables.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counting outlier/failure runs and expressing as percentages (e.g., '** indicates up to 20% failures'), reporting large standard deviations in FID (e.g., LSGAN on CIFAR: stddev 47.5), and visualizing distributions with histograms (Figures 10–13) and variance plots (Figure 15).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Material: failure/outlier prevalence distorts perceived algorithm reliability; methods with similar best FID may have very different robustness profiles, affecting practitioner choice and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Non-negligible: the paper documents specific combos with up to ~20% failure rates and numerous significant outliers across models/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Optimization instability of GAN training, sensitivity to hyperparameters and initialization, and selective reporting in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report failure/outlier rates explicitly, provide distributions across seeds, perform robustness/stability analyses (retrain many seeds), and report standard deviations/confidence intervals alongside mean metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective for transparency: the authors' practice of retraining and annotating failures clarifies robustness differences; quantifying failures helps practitioners weigh algorithmic tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning training stability / experimental reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are GANs Created Equal? A Large-Scale Study', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Many paths to equilibrium: GANs do not need to decrease a divergence at every step <em>(Rating: 2)</em></li>
                <li>Fréchet Inception Distance <em>(Rating: 2)</em></li>
                <li>A note on the evaluation of generative models <em>(Rating: 2)</em></li>
                <li>Do GANs learn the distribution? some theory and empirics <em>(Rating: 2)</em></li>
                <li>Improved training of Wasserstein gans <em>(Rating: 1)</em></li>
                <li>On convergence and stability of GANs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-728",
    "paper_id": "paper-c88e8d85fd5160b0793598bda037f977366acf7a",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Hyperparameter reporting mismatch",
            "name_full": "Mismatch between paper-reported recommended hyperparameters and those required by code/experiments",
            "brief_description": "The paper documents that recommended hyperparameter settings (from original authors or papers) often do not transfer across data sets and that published reports tend to focus on best-found settings rather than distributions; the authors detect and quantify sensitivity by large-scale hyperparameter searches and show this mismatch materially affects results and rankings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "large-scale GAN benchmarking pipeline",
            "system_description": "An open-source experimental pipeline for training many GAN variants with controlled architecture, performing wide and narrow random hyperparameter searches, retraining best settings with many seeds, and computing evaluation metrics (FID, precision/recall/F1).",
            "nl_description_type": "research paper methods section / author-recommended hyperparameter lists",
            "code_implementation_type": "training scripts and open-sourced model implementations",
            "gap_type": "hyperparameter mismatch / incomplete specification",
            "gap_description": "Authors note that hyperparameter values suggested in original papers or community reports frequently do not generalize across data sets; many papers report only the best hyperparameters (or the single best run), omitting distributions or the ranges explored, leading to misleading comparisons when code or reused settings are applied to new datasets.",
            "gap_location": "hyperparameters / experimental protocol / reporting",
            "detection_method": "large-scale empirical hyperparameter search (wide and narrow random sampling), transfer experiments (applying narrow ranges selected on one dataset to others), and retraining best-found settings with multiple random seeds",
            "measurement_method": "Comparison of distributions of best-achieved FID (and F1) over many hyperparameter samples (100 wide-range, 50 narrow-range), reporting means and standard deviations, visualization (scatter plots Fig.6–9, Fig.14), and bootstrap estimates (5000 resamples of 100 runs) of minimum achievable FID as a function of budget",
            "impact_on_results": "Substantial: model rankings change with budget and hyperparameter search; 'bad' algorithms can outperform 'good' ones when given more tuning budget; recommending/fixing hyperparameters without search can yield much worse FID/F1. Quantitative examples: Table 2 shows large variance in FIDs (e.g., LSGAN on CIFAR: 87.1 ± 47.5), and the authors report that narrow-range transfer sometimes fails (some models more sensitive than others).",
            "frequency_or_prevalence": "High: paper reports ubiquitous sensitivity across all tested GAN variants and datasets; variability observed across hyperparameter samples and seeds; specific cases of significant outliers and failures (see Table 2 and notes).",
            "root_cause": "Author-reported hyperparameters are underspecified for other data sets, community tuning biases, omission of ranges and variability in papers, and implicit assumptions about optimization budget and dataset-specific settings.",
            "mitigation_approach": "Perform wide hyperparameter searches and report distributions (means, stddev, confidence intervals), publish hyperparameter ranges and search protocols, retrain top settings with multiple random seeds, and open-source code and exact search configurations.",
            "mitigation_effectiveness": "Effective: authors show that wide search followed by retraining (50 seeds) yields reliable estimates of stability; bootstrap analyses demonstrate how increasing budget changes minimum achievable FID and reduces apparent algorithmic differences (figures 3, 14, 15).",
            "domain_or_field": "deep learning / generative adversarial networks (GANs)",
            "reproducibility_impact": true,
            "uuid": "e728.0",
            "source_info": {
                "paper_title": "Are GANs Created Equal? A Large-Scale Study",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Reporting-best-vs-distribution",
            "name_full": "Reporting single best-run results versus distributions of outcomes",
            "brief_description": "The paper identifies a common reporting gap where studies present only the single best result (best FID) rather than the distribution across random restarts and hyperparameter variations; the authors argue this misalignment between natural-language claims and empirical variability misleads comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GAN evaluation and reporting protocol",
            "system_description": "Protocol used by the authors to select best models per hyperparameter run, then re-run best settings multiple times to estimate stability and produce distributions of evaluation metrics.",
            "nl_description_type": "paper results reporting / claimed performance statements",
            "code_implementation_type": "experiment scripts that select and log best checkpoints and metrics over training",
            "gap_type": "incomplete specification / selective reporting",
            "gap_description": "Papers commonly describe a single 'best' experimental outcome in prose and tables, while underlying code/experiments produce a distribution of outcomes due to randomness and instability; this creates a mismatch between the implied robustness in text and the observed variability.",
            "gap_location": "results reporting / evaluation metrics",
            "detection_method": "Retraining best hyperparameter configurations with 50 different random seeds, plotting distributions of best-achieved FID and F1, and computing standard deviations and confidence intervals (bootstrap).",
            "measurement_method": "Statistical summaries: mean ± std of FID over 50 retrains (Table 2), bootstrap resampling for budget-based minimum FID distributions (5000 resamples), plotting histograms and CIs (Figures 3, 15), and reporting failure/outlier rates.",
            "impact_on_results": "Large: reporting only best runs exaggerates method performance and misleads comparisons; the paper shows that variability is significant enough that algorithmic conclusions change when distributions are compared (e.g., no algorithm clearly dominates once distributions and budgets are considered).",
            "frequency_or_prevalence": "Widespread: authors note that many community reports and original papers present best results; their experiments reveal non-trivial variance across all models and datasets; marked outliers and failures present in multiple combinations.",
            "root_cause": "Cultural/practical incentives to report best numbers, lack of standards for reporting variability, and space/attention constraints in publications.",
            "mitigation_approach": "Recommend reporting full distributions (means, variances, confidence intervals), retraining top settings across seeds, and comparing distributions of minimum achievable metrics for fixed computational budgets.",
            "mitigation_effectiveness": "Effective in clarifying true performance: authors' distributional reporting shows convergence of methods with increased budget and reveals when claims based on best runs are not robust.",
            "domain_or_field": "machine learning evaluation methodology",
            "reproducibility_impact": true,
            "uuid": "e728.1",
            "source_info": {
                "paper_title": "Are GANs Created Equal? A Large-Scale Study",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Metric-specification gap (FID limits)",
            "name_full": "Limitations and misalignment between metric descriptions and practical evaluation (Fréchet Inception Distance)",
            "brief_description": "The paper analyzes FID's properties and documents gaps between the metric's natural-language claims and its practical behavior, including bias, sensitivity to mode dropping, inability to detect overfitting, and potential dependence on the choice of encoding network.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GAN evaluation metrics subsystem",
            "system_description": "Evaluation stage that embeds images via a pre-trained network (Inception or VGG), computes sample means and covariances, and computes Fréchet distance as FID.",
            "nl_description_type": "metric specification in prior literature and paper's evaluation description",
            "code_implementation_type": "evaluation scripts computing FID using embedding network activations",
            "gap_type": "incomplete specification / metric limitation / encoding-dependence",
            "gap_description": "Although FID is described as robust and aligned with human judgment, the authors find it has a non-negligible bias (high bias, low variance), is extremely sensitive to mode dropping, cannot detect overfitting (e.g., memory GANs), and may be influenced by the choice of encoding network (though they find high rank correlation between Inception and VGG embeddings).",
            "gap_location": "evaluation metrics / metric implementation and choice of embedding network",
            "detection_method": "Empirical analyses: partitioning datasets into train/test subsamples, computing FID across many random partitions and subsamples, performing controlled mode-dropping experiments, and comparing FID computed with different encoders (Inception vs VGG).",
            "measurement_method": "Quantitative measures: bias and variance estimates of FID (Table 1 / optimistic FID estimates), sensitivity plots to mode dropping (Figure 1b), Spearman rank correlation between encoders (rho=0.9 for CelebA), and noting that FID cannot detect overfitting by construction.",
            "impact_on_results": "Moderate-to-high: metric limitations can mask overfitting and be 'fooled' by artifacts tailored to the encoder; mode dropping strongly degrades FID, and metric bias suggests absolute FID values must be interpreted cautiously. Authors therefore complement FID with precision/recall/F1 on controlled tasks.",
            "frequency_or_prevalence": "Metric limitation is universal to all experiments relying solely on FID or IS; observed consistently in the paper's robustness checks.",
            "root_cause": "Assumptions in metric derivation (Gaussian assumption on embeddings), reliance on pre-trained encoder (task/dataset mismatch), and lack of metric designed to detect memorization/overfitting.",
            "mitigation_approach": "Use complementary metrics (precision, recall, F1 on constructed manifolds), test sensitivity to encoder choice, report bias/variance characteristics, and design tasks where precision/recall can be approximately computed (convex polygon datasets).",
            "mitigation_effectiveness": "Partially effective: combining FID with precision/recall helps detect mode dropping and overfitting in controlled settings; encoder-sensitivity checks show high rank correlation but do not eliminate all encoder-specific artifacts.",
            "domain_or_field": "evaluation metrics for generative models / deep learning",
            "reproducibility_impact": true,
            "uuid": "e728.2",
            "source_info": {
                "paper_title": "Are GANs Created Equal? A Large-Scale Study",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Underspecified experimental protocol",
            "name_full": "Underspecification of experimental choices (architecture, budget, preprocessing, training schedule) vs code",
            "brief_description": "The paper points out that many experimental choices (architecture, optimization details, number of epochs, use of batchnorm in discriminator, update frequencies) are often not standardized in natural-language descriptions, leading to mismatches when different codebases are used and affecting reproducibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "experimental design / training pipeline",
            "system_description": "Unified training pipeline used by the authors where architecture is fixed across models (INFOGAN-like), batch size and epochs are fixed per dataset, and numerous options (batchnorm in discriminator, discriminator iterations, gradient penalties) are varied in hyperparameter searches.",
            "nl_description_type": "methods section and experimental protocol descriptions",
            "code_implementation_type": "training and preprocessing code / open-sourced scripts",
            "gap_type": "incomplete specification / ambiguous description",
            "gap_description": "Papers often omit dataset-specific choices (number of epochs, batch sizes, architecture variants, normalization placement, imbalanced update frequencies), leading to ambiguity; when implementing or reusing code, these unspecified choices cause differences in results. The authors made explicit pragmatic choices (fix architecture, batch size, latent size) to reduce this source of mismatch.",
            "gap_location": "model architecture, training procedure, preprocessing, optimization hyperparameters",
            "detection_method": "Designing a controlled experimental setting where all models share an architecture and then observing sensitivity to toggles (e.g., batchnorm in discriminator, disc iters) and to the choice of optimizer and number of epochs across datasets",
            "measurement_method": "Empirical sensitivity analysis (scatter plots per hyperparameter in Figures 6–9), comparisons of performance with/without options (e.g., batchnorm true/false), and ablations (ADAM vs RMSProp in Appendix F).",
            "impact_on_results": "Significant: varying these choices materially changes FID and F1 outcomes; some architectures or training settings can cause severe mode collapse or failures (noted as outliers and up to 20% failure rates for some combinations).",
            "frequency_or_prevalence": "Common: many of these choices are influential across models and datasets as shown in the authors' sensitivity plots.",
            "root_cause": "Papers omit or under-specify routine but influential experimental choices, and implementations sometimes introduce pragmatic tweaks not described in prose.",
            "mitigation_approach": "Use a standardized architecture across comparisons, explicitly list all training choices, open-source code, perform controlled ablations and sensitivity analyses, and recommend reporting exact training scripts and seeds.",
            "mitigation_effectiveness": "Effective when adopted: standardizing architecture and publishing code allowed the authors to perform fair comparisons and reveal remaining differences are primarily due to hyperparameter tuning and budget rather than fundamental algorithmic advantages.",
            "domain_or_field": "experimental methodology in machine learning / deep learning",
            "reproducibility_impact": true,
            "uuid": "e728.3",
            "source_info": {
                "paper_title": "Are GANs Created Equal? A Large-Scale Study",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Transferability gap of hyperparameter ranges",
            "name_full": "Failure of hyperparameter ranges tuned on one dataset to transfer to other datasets",
            "brief_description": "The study finds that hyperparameter ranges optimized on a single dataset (narrow-range search) do not always transfer well to other datasets, causing degraded performance when code implementations or recommended ranges are reused without dataset-specific tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "hyperparameter search and transfer protocol",
            "system_description": "Two-stage protocol: (i) wide one-shot search (100 hyperparameter samples across broad ranges) to identify good ranges; (ii) narrow two-shot search (50 samples) using ranges selected from wide search on a reference dataset (Fashion-MNIST) and applying them to other datasets.",
            "nl_description_type": "experimental protocol / hyperparameter recommendation text",
            "code_implementation_type": "hyperparameter sampling scripts and training code",
            "gap_type": "hyperparameter mismatch / incomplete generalization of recommended ranges",
            "gap_description": "Ranges found effective on a reference dataset (e.g., Fashion-MNIST) sometimes fail on other datasets like CIFAR or CelebA; some models (NS GAN) transfer better than others (WGAN) indicating a mismatch between a general recommendation in text and implementation needs on new data.",
            "gap_location": "hyperparameters / transfer across datasets",
            "detection_method": "Applying narrow-range hyperparameter searches derived from one dataset to others and measuring resulting performance and variance (Figure 14 and related discussion).",
            "measurement_method": "Comparing FID variance and mean across datasets when hyperparameters are sampled from narrow ranges vs wide ranges; visualizing via narrow-range scatter plots (Fig.14) and reporting changes in mean FID and stability.",
            "impact_on_results": "Practical: for some models/datasets the narrow-range transfer results in degraded or unstable performance, meaning code or recommended ranges cannot be safely reused without further tuning; authors observe notable per-model differences in transfer robustness.",
            "frequency_or_prevalence": "Moderate-to-high: observed in multiple model/dataset combinations; some models more robust to transfer than others.",
            "root_cause": "Dataset-specific optimization landscapes, implicit dataset assumptions in tuning, and underreporting of dataset-specific hyperparameter dependence in papers.",
            "mitigation_approach": "Perform dataset-specific hyperparameter searches (even if narrow), publish full wide-range search results to guide transfer, and prefer methods with more robust transfer properties when computational budget is limited.",
            "mitigation_effectiveness": "Partially effective: narrow-range searches derived from representative datasets can help but do not eliminate the need for dataset-specific tuning; wide searches remain more reliable.",
            "domain_or_field": "machine learning experimentation / hyperparameter optimization",
            "reproducibility_impact": true,
            "uuid": "e728.4",
            "source_info": {
                "paper_title": "Are GANs Created Equal? A Large-Scale Study",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Hidden failure reporting (mode collapse/outliers)",
            "name_full": "Underreporting of training failures and mode collapse in textual descriptions versus observed experimental code runs",
            "brief_description": "The paper documents that severe training failures and mode collapse occur in practice and are sometimes not reflected in textual claims; they quantify failure/outlier rates and mark some model/dataset combinations with high prevalence of failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "training stability monitoring and failure logging",
            "system_description": "Empirical logging and analysis of many runs per model/hyperparameter setting to detect and quantify training failures (mode collapse, training divergence), including marking combinations with significant outlier or failure rates.",
            "nl_description_type": "paper descriptions of model robustness and failure modes",
            "code_implementation_type": "training pipelines with monitoring, checkpointing, and failure detection",
            "gap_type": "missing failure reporting / selective outcome reporting",
            "gap_description": "Natural-language descriptions often emphasize successful outcomes, whereas the exhaustive experimental runs reveal frequent outliers and failures (e.g., severe mode collapse), sometimes up to nontrivial percentages; the paper annotates Table 2 entries with '*' for significant outliers and '**' indicating up to 20% failures.",
            "gap_location": "training procedure / experiment reporting",
            "detection_method": "Running many independent training runs across hyperparameter settings and seeds, recording failed/degenerate runs and extreme FID outliers, and annotating prevalence in results tables.",
            "measurement_method": "Counting outlier/failure runs and expressing as percentages (e.g., '** indicates up to 20% failures'), reporting large standard deviations in FID (e.g., LSGAN on CIFAR: stddev 47.5), and visualizing distributions with histograms (Figures 10–13) and variance plots (Figure 15).",
            "impact_on_results": "Material: failure/outlier prevalence distorts perceived algorithm reliability; methods with similar best FID may have very different robustness profiles, affecting practitioner choice and reproducibility.",
            "frequency_or_prevalence": "Non-negligible: the paper documents specific combos with up to ~20% failure rates and numerous significant outliers across models/datasets.",
            "root_cause": "Optimization instability of GAN training, sensitivity to hyperparameters and initialization, and selective reporting in literature.",
            "mitigation_approach": "Report failure/outlier rates explicitly, provide distributions across seeds, perform robustness/stability analyses (retrain many seeds), and report standard deviations/confidence intervals alongside mean metrics.",
            "mitigation_effectiveness": "Effective for transparency: the authors' practice of retraining and annotating failures clarifies robustness differences; quantifying failures helps practitioners weigh algorithmic tradeoffs.",
            "domain_or_field": "deep learning training stability / experimental reproducibility",
            "reproducibility_impact": true,
            "uuid": "e728.5",
            "source_info": {
                "paper_title": "Are GANs Created Equal? A Large-Scale Study",
                "publication_date_yy_mm": "2017-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Many paths to equilibrium: GANs do not need to decrease a divergence at every step",
            "rating": 2
        },
        {
            "paper_title": "Fréchet Inception Distance",
            "rating": 2
        },
        {
            "paper_title": "A note on the evaluation of generative models",
            "rating": 2
        },
        {
            "paper_title": "Do GANs learn the distribution? some theory and empirics",
            "rating": 2
        },
        {
            "paper_title": "Improved training of Wasserstein gans",
            "rating": 1
        },
        {
            "paper_title": "On convergence and stability of GANs",
            "rating": 1
        }
    ],
    "cost": 0.015372499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Are GANs Created Equal? A Large-Scale Study</h1>
<p>Mario Lucic<em> Karol Kurach</em> Marcin Michalski Olivier Bousquet Sylvain Gelly<br>Google Brain</p>
<h4>Abstract</h4>
<p>Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in [9].</p>
<h2>1 Introduction</h2>
<p>Generative adversarial networks (GAN) are a powerful subclass of generative models and were successfully applied to image generation and editing, semi-supervised learning, and domain adaptation [22, 27]. In the GAN framework the model learns a deterministic transformation $G$ of a simple distribution $p_{z}$, with the goal of matching the data distribution $p_{d}$. This learning problem may be viewed as a two-player game between the generator, which learns how to generate samples which resemble real data, and a discriminator, which learns how to discriminate between real and fake data. Both players aim to minimize their own cost and the solution to the game is the Nash equilibrium where neither player can improve their cost unilaterally [9].
Various flavors of GANs have been recently proposed, both purely unsupervised [9, 1, 10, 5] as well as conditional [20, 21]. While these models achieve compelling results in specific domains, there is still no clear consensus on which GAN algorithm(s) perform objectively better than others. This is partially due to the lack of robust and consistent metric, as well as limited comparisons which put all algorithms on equal footage, including the computational budget to search over all hyperparameters. Why is it important? Firstly, to help the practitioner choose a better algorithm from a very large set. Secondly, to make progress towards better algorithms and their understanding, it is useful to clearly assess which modifications are critical, and which ones are only good on paper, but do not make a significant difference in practice.
The main issue with evaluation stems from the fact that one cannot explicitly compute the probability $p_{g}(x)$. As a result, classic measures, such as log-likelihood on the test set, cannot be evaluated. Consequently, many researchers focused on qualitative comparison, such as comparing the visual quality of samples. Unfortunately, such approaches are subjective and possibly misleading [8]. As a remedy, two evaluation metrics were proposed to quantitatively assess the performance of GANs. Both assume access to a pre-trained classifier. Inception Score (IS) [24] is based on the fact that a good model should generate samples for which, when evaluated by the classifier, the class distribution has low entropy. At the same time, it should produce diverse samples covering all classes. In contrast,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Fréchet Inception Distance is computed by considering the difference in embedding of true and fake data [11]. Assuming that the coding layer follows a multivariate Gaussian distribution, the distance between the distributions is reduced to the Fréchet distance between the corresponding Gaussians.</p>
<p>Our main contributions: (1) We provide a fair and comprehensive comparison of the state-of-the-art GANs, and empirically demonstrate that nearly all of them can reach similar values of FID, given a high enough computational budget. (2) We provide strong empirical evidence ${ }^{2}$ that to compare GANs it is necessary to report a summary of distribution of results, rather than the best result achieved, due to the randomness of the optimization process and model instability. (3) We assess the robustness of FID to mode dropping, use of a different encoding network, and provide estimates of the best FID achievable on classic data sets. (4) We introduce a series of tasks of increasing difficulty for which undisputed measures, such as precision and recall, can be approximately computed. (5) We open-sourced our experimental setup and model implementations at goo.gl/G8kf5J.</p>
<h1>2 Background and Related Work</h1>
<p>There are several ongoing challenges in the study of GANs, including their convergence and generalization properties [2, 19], and optimization stability [24, 1]. Arguably, the most critical challenge is their quantitative evaluation. The classic approach towards evaluating generative models is based on model likelihood which is often intractable. While the log-likelihood can be approximated for distributions on low-dimensional vectors, in the context of complex high-dimensional data the task becomes extremely challenging. Wu et al. [26] suggest an annealed importance sampling algorithm to estimate the hold-out log-likelihood. The key drawback of the proposed approach is the assumption of the Gaussian observation model which carries over all issues of kernel density estimation in highdimensional spaces. Theis et al. [25] provide an analysis of common failure modes and demonstrate that it is possible to achieve high likelihood, but low visual quality, and vice-versa. Furthermore, they argue against using Parzen window density estimates as the likelihood estimate is often incorrect. In addition, ranking models based on these estimates is discouraged [4]. For a discussion on other drawbacks of likelihood-based training and evaluation consult Huszár [12].</p>
<p>Inception Score (IS). Proposed by Salimans et al. [24], IS offers a way to quantitatively evaluate the quality of generated samples. The score was motivated by the following considerations: (i) The conditional label distribution of samples containing meaningful objects should have low entropy, and (ii) The variability of the samples should be high, or equivalently, the marginal $\int_{z} p(y \mid x=$ $G(z)) d z$ should have high entropy. Finally, these desiderata are combined into one score, $\operatorname{IS}(G)=$ $\exp \left(\mathbb{E}<em K="K" L="L">{x \sim G}\left[d</em>(p(y \mid x), p(y)\right]\right)$. The classifier is Inception Net trained on Image Net. The authors found that this score is well-correlated with scores from human annotators [24]. Drawbacks include insensitivity to the prior distribution over labels and not being a proper distance.</p>
<p>Fréchet Inception Distance (FID). Proposed by Heusel et al. [11], FID provides an alternative approach. To quantify the quality of generated samples, they are first embedded into a feature space given by (a specific layer) of Inception Net. Then, viewing the embedding layer as a continuous multivariate Gaussian, the mean and covariance is estimated for both the generated data and the real data. The Fréchet distance between these two Gaussians is then used to quantify the quality of the samples, i.e. $\operatorname{FID}(x, g)=\left|\mu_{x}-\mu_{g}\right|<em x="x">{2}^{2}+\operatorname{Tr}\left(\Sigma</em>\right)$ are the mean and covariance of the sample embeddings from the data distribution and model distribution, respectfully. The authors show that the score is consistent with human judgment and more robust to noise than IS [11]. Furthermore, the authors present compelling results showing negative correlation between the FID and visual quality of generated samples. Unlike IS, FID can detect intra-class mode dropping, i.e. a model that generates only one image per class can score a perfect IS, but will have a bad FID. We provide a thorough empirical analysis of FID in Section 5. A significant drawback of both measures is the inability to detect overfitting. A "memory GAN" which stores all training samples would score perfectly. Finally, as the FID estimator is consistent, relative model comparisons for large sample sizes are sound.}+\Sigma_{g}-2\left(\Sigma_{x} \Sigma_{g}\right)^{\frac{1}{2}}\right)$, where $\left(\mu_{x}, \Sigma_{x}\right)$, and $\left(\mu_{g}, \Sigma_{g</p>
<p>A very recent study comparing several GANs using IS has been presented by Fedus et al. [7]. The authors focus on IS and consider a smaller subset of GANs. In contrast, our focus is on providing a fair assessment of the current state-of-the-art GANs using FID, as well as precision and recall, and also verifying the robustness of these models in a large-scale empirical evaluation.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Generator and discriminator loss functions. The main difference whether the discriminator outputs a probability (MM GAN, NS GAN, DRAGAN) or its output is unbounded (WGAN, WGAN GP, LS GAN, BEGAN), whether the gradient penalty is present (WGAN GP, DRAGAN) and where is it evaluated.</p>
<table>
<thead>
<tr>
<th>GAN</th>
<th>Discriminator Loss</th>
<th>Generator Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>MM GAN</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {GAN }}=-\mathbb{E}</em>} \sim p_{d}}[\log (D(x))]-\mathbb{E<em g="g">{\hat{x} \sim p</em>))]$}}[\log (1-D(\hat{x</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {GAN }}=\mathbb{E}</em>))]$} \sim p_{g}}[\log (1-D(\hat{x</td>
</tr>
<tr>
<td>NS GAN</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {NEGAN }}=-\mathbb{E}</em>} \sim p_{d}}[\log (D(x))]-\mathbb{E<em g="g">{\hat{x} \sim p</em>))]$}}[\log (1-D(\hat{x</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {NEGAN }}=-\mathbb{E}</em>))]$} \sim p_{g}}[\log (D(\hat{x</td>
</tr>
<tr>
<td>WGAN</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {WGAN }}=-\mathbb{E}</em>} \sim p_{d}}[D(x)]+\mathbb{E<em g="g">{\hat{x} \sim p</em>)]$}}[D(\hat{x</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {WGAN }}=-\mathbb{E}</em>)]$} \sim p_{g}}[D(\hat{x</td>
</tr>
<tr>
<td>WGAN GP</td>
<td>$\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{G}}^{\text {WGANGP }}=\mathcal{L}</em>}}^{\text {WGAN }}+\lambda \mathbb{E<em g="g">{\hat{x} \sim p</em>\right}\right.\right.$}}\left[\left(\left|\nabla D\left(\alpha x+(1-\alpha \hat{x}) |_{2}-1\right)^{2</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {WGANGP }}=-\mathbb{E}</em>)]$} \sim p_{g}}[D(\hat{x</td>
</tr>
<tr>
<td>LS GAN</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {LSGAN }}=-\mathbb{E}</em>} \sim p_{d}}\left[(D(x)-1)^{2}\right]+\mathbb{E<em g="g">{\hat{x} \sim p</em>\right]$}}\left[D(\hat{x})^{2</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {LSGAN }}=-\mathbb{E}</em>\right]$} \sim p_{g}}\left[(D(\hat{x}-1))^{2</td>
</tr>
<tr>
<td>DRAGAN</td>
<td>$\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{G}}^{\text {DRAGAN }}=\mathcal{L}</em>}}^{\text {GAN }}+\lambda \mathbb{E<em d="d">{\hat{x} \sim p</em>\right]\right.$}+\mathcal{N}\left(\mathbb{R}, c\right)}\left[\left(\left|\nabla D(\hat{x}) |_{2}-1\right)^{2</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {DRAGAN }}=\mathbb{E}</em>))]$} \sim p_{g}}[\log (1-D(\hat{x</td>
</tr>
<tr>
<td>BEGAN</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {BEGAN }}=\mathbb{E}</em>(x)\right|} \sim p_{d}}\left[\left|x-\operatorname{AE<em i="i">{1}\right]-k</em>} \mathbb{E<em g="g">{\hat{x} \sim p</em>\right]\right.$}}\left[\left|\hat{x}-\operatorname{AE}(\hat{x}) |_{1</td>
<td>$\mathcal{L}<em _hat_x="\hat{x">{\mathrm{G}}^{\text {BEGAN }}=\mathbb{E}</em>\right]\right.$} \sim p_{g}}\left[\left|\hat{x}-\operatorname{AE}(\hat{x}) |_{1</td>
</tr>
</tbody>
</table>
<h1>3 Flavors of Generative Adversarial Networks</h1>
<p>In this work we focus on unconditional generative adversarial networks. In this setting, only unlabeled data is available for learning. The optimization problems arising from existing approaches differ by (i) the constraint on the discriminators output and corresponding loss, and the presence and application of gradient norm penalty.
In the original GAN formulation [9] two loss functions were proposed. In the minimax GAN the discriminator outputs a probability and the loss function is the negative log-likelihood of a binary classification task (MM GAN in Table 1). Here the generator learns to generate samples that have a low probability of being fake. To improve the gradient signal, the authors also propose the non-saturating loss (NS GAN in Table 1), where the generator instead aims to maximize the probability of generated samples being real. In Wasserstein GAN [1] the discriminator is allowed to output a real number and the objective function is equivalent to the MM GAN loss without the sigmoid (WGAN in Table 1). The authors prove that, under an optimal (Lipschitz smooth) discriminator, minimizing the value function with respect to the generator minimizes the Wasserstein distance between model and data distributions. Weights of the discriminator are clipped to a small absolute value to enforce smoothness. To improve on the stability of the training, Gulrajani et al. [10] instead add a soft constraint on the norm of the gradient which encourages the discriminator to be 1-Lipschitz. The gradient norm is evaluated on points obtained by linear interpolation between data points and generated samples where the optimal discriminator should have unit gradient norm [10]. Gradient norm penalty can also be added to both MM GAN and NS GAN and evaluated around the data manifold (DRAGAN [15] in Table 1 based on NS GAN). This encourages the discriminator to be piecewise linear around the data manifold. Note that the gradient norm can also be evaluated between fake and real points, similarly to WGAN GP, and added to either MM GAN or NS GAN [7]. Mao et al. [18] propose a least-squares loss for the discriminator and show that minimizing the corresponding objective (LS GAN in Table 1) implicitly minimizes the Pearson $\chi^{2}$ divergence. The idea is to provide smooth loss which saturates slower than the sigmoid cross-entropy loss of the original MM GAN. Finally, Berthelot et al. [5] propose to use an autoencoder as a discriminator and optimize a lower bound of the Wasserstein distance between auto-encoder loss distributions on real and fake data. They introduce an additional hyperparameter $\gamma$ to control the equilibrium between the generator and discriminator.</p>
<h2>4 Challenges of a Fair Comparison</h2>
<p>There are several interesting dimensions to this problem, and there is no single right way to compare these models (i.e. the loss function used in each GAN). Unfortunately, due to the combinatorial explosion in the number of choices and their ordering, not all relevant options can be explored. While there is no definite answer on how to best compare two models, in this work we have made several pragmatic choices which were motivated by two practical concerns: providing a neutral and fair comparison, and a hard limit on the computational budget.
Which metric to use? Comparing models implies access to some metric. As discussed in Section 2, classic measures, such as model likelihood cannot be applied. We will argue for and study two sets of</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Figure (a) shows that FID has a slight bias, but low variance on samples of size 10000. Figure (b) shows that FID is extremely sensitive to mode dropping. Figure (c) shows the high rank correlation (Spearman's $\rho=0.9$ ) between FID score computed on InceptionNet vs FID computed using VGG for the CELEBA data set (for interesting range: FID $&lt;200$ ).
evaluation metrics in Section 5: FID, which can be computed on all data sets, and precision, recall, and $F_{1}$, which we can compute for the proposed tasks.</p>
<p>How to compare models? Even when the metric is fixed, a given algorithm can achieve very different scores, when varying the architecture, hyperparameters, random initialization (i.e. random seed for initial network weights), or the data set. Sensible targets include best score across all dimensions (e.g. to claim the best performance on a fixed data set), average or median score (rewarding models which are good in expectation), or even the worst score (rewarding models with worst-case robustness). These choices can even be combined - for example, one might train the model multiple times using the best hyperparameters, and average the score over random initializations).</p>
<p>For each of these dimensions, we took several pragmatic choices to reduce the number of possible configurations, while still exploring the most relevant options.</p>
<ol>
<li>Architecture: We use the same architecture for all models. We note that this architecture suffices to achieve good performance on considered data sets.</li>
<li>Hyperparameters: For both training hyperparameters (e.g. the learning rate), as well as model specific ones (e.g. gradient penalty multiplier), there are two valid approaches: (i) perform the hyperparameter optimization for each data set, or (ii) perform the hyperparameter optimization on one data set and infer a good range of hyperparameters to use on other data sets. We explore both avenues in Section 6.</li>
<li>Random seed: Even with everything else being fixed, varying the random seed may influence on the results. We study this effect and report the corresponding confidence intervals.</li>
<li>Data set: We chose four popular data sets from GAN literature.</li>
<li>Computational budget: Depending on the budget to optimize the parameters, different algorithms can achieve the best results. We explore how the results vary depending on the budget $k$, where $k$ is the number of hyperparameter settings for a fixed model.</li>
</ol>
<p>In practice, one can either use hyperparameter values suggested by respective authors, or try to optimize them. Figure 4 and in particular Figure 14 show that optimization is necessary. Hence, we optimize the hyperparameters for each model and data set by performing a random search. While we present the results which were obtained by a random search, we have also investigated sequential Bayesian optimization, which resulted in comparable results. We concur that the models with fewer hyperparameters have an advantage over models with many hyperparameters, but consider this fair as it reflects the experience of practitioners searching for good hyperparameters for their setting.</p>
<h1>5 Metrics</h1>
<p>In this work we focus on two sets of metrics. We first analyze the recently proposed FID in terms of robustness (of the metric itself), and conclude that it has desirable properties and can be used in practice. Nevertheless, this metric, as well as Inception Score, is incapable of detecting overfitting: a memory GAN which simply stores all training samples would score perfectly under both measures. Based on these shortcomings, we propose an approximation to precision and recall for GANs and how that it can be used to quantify the degree of overfitting. We stress that the proposed method should be viewed as complementary to IS or FID, rather than a replacement.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Samples from models with (a) high recall and precision, (b) high precision, but low recall (lacking in diversity), (c) low precision, but high recall (can decently reproduce triangles, but fails to capture convexity), and (d) low precision and low recall.</p>
<p><strong>Fréchet Inception Distance.</strong> FID was shown to be robust to noise [11]. Here we quantify the bias and variance of FID, its sensitivity to the encoding network and sensitivity to mode dropping. To this end, we partition the data set into two groups, i.e. $\mathcal{X} = \mathcal{X}_1 \cup \mathcal{X}_2$. Then, we define the data distribution $p_d$ as the empirical distribution on a random subsample of $\mathcal{X}_1$ and the model distribution $p_g$ to be the empirical distribution on a random subsample from $\mathcal{X}_2$. For a random partition this "model distribution" should follow the data distribution.</p>
<p>We evaluate the bias and variance of FID on four data sets from the GAN literature. We start by using the default train vs. test partition and compute the FID between the test set (limited to $N = 10000$ samples for CelebA) and a sample of size $N$ from the train set. Sampling from the train set is performed $M = 50$ times. The optimistic estimates of FID are reported in Table 1. We observe that FID has high bias, but small variance. From this perspective, estimating the full covariance matrix might be unnecessary and counter-productive, and a constrained version might suffice. To test the sensitivity to train vs. test partitioning, we consider 50 random partitions (keeping the relative sizes fixed, i.e. $6 : 1$ for MNIST) and compute the FID with $M = 1$ sample. We observe results similar to Table 1 which is expected as both training and testing data sets are sampled from the same distribution. Furthermore, we evaluate the sensitivity to mode dropping as follows: we fix a partition $\mathcal{X} = \mathcal{X}_1 \cup \mathcal{X}_2$ and subsample $\mathcal{X}_2$ while keeping only samples from the first $k$ classes, increasing $k$ from 1 to 10. For each $k$, we consider 50 random subsamples from $\mathcal{X}_2$. Figure 1 shows that FID is heavily influenced by the missing modes. Finally, we estimate the sensitivity to the choice of the encoding network by computing FID using the 4096 dimensional FC7 layer of the VGG network trained on ImageNet. Figure 1 shows the resulting distribution. We observe high Spearman's rank correlation ($$\rho = 0.9$$) which encourages the use of the coding layer suggested by the authors.</p>
<p><strong>Precision, recall and $F_1$ score.</strong> Precision, recall and $F_1$ score are proven and widely adopted techniques for quantitatively evaluating the quality of discriminative models. Precision measures the fraction of relevant retrieved instances among the retrieved instances, while recall measures the fraction of the retrieved instances among relevant instances. $F_1$ score is the harmonic average of precision and recall. Notice that IS mainly captures precision: It will not penalize the model for not producing all modes of the data distribution — it will only penalize the model for not producing all <em>classes</em>. On the other hand, FID captures both precision and recall. Indeed, a model which fails to recover different modes of the data distribution will suffer in terms of FID.</p>
<p>We propose a simple and effective data set for evaluating (and comparing) generative models. Our main motivation is that the currently used data sets are either too simple (e.g. simple mixtures of Gaussians, or MNIST) or too complex (e.g. ImageNet). We argue that it is critical to be able to increase the complexity of the task in a relatively smooth and controlled fashion. To this end, we present a set of tasks for which we can <em>approximate</em> the precision and recall of each model. As a result, we can compare different models based on established metrics. The main idea is to construct a data manifold such that the distances from samples to the manifold can be computed efficiently. As a result, the problem of evaluating the quality of the generative model is effectively transformed into a problem of computing the distance to the manifold. This enables an intuitive approach for defining the quality of the model. Namely, if the samples from the model distribution $p_g$ are (on average) close to the manifold, its <em>precision</em> is high. Similarly, high <em>recall</em> implies that the generator can recover (i.e. generate something close to) any sample from the manifold.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: How does the minimum FID behave as a function of the budget? The plot shows the distribution of the minimum FID achievable for a fixed budget along with one standard deviation interval. For each budget, we estimate the mean and variance using 5000 bootstrap resamples out of 100 runs. We observe that, given a relatively low budget, all models achieve a similar minimum FID. Furthermore, for a fixed FID, "bad" models can outperform "good" models given enough computational budget. We argue that the computational budget to search over hyperparameters is an important aspect of the comparison between algorithms.</p>
<p>For general data sets, this reduction is impractical as one has to compute the distance to the manifold which we are trying to learn. However, if we construct a manifold such that this distance is efficiently computable, the precision and recall can be efficiently evaluated. To this end, we propose a set of toy data sets for which such computation can be performed efficiently: The manifold of convex polygons. As the simplest example, let us focus on gray-scale triangles represented as one channel images as in Figure 2. These triangles belong to a low-dimensional manifold $\mathcal{C}_{3}$ embedded in $\mathbb{R}^{d \times d}$. Intuitively, the coordinate system of this manifold represents the axes of variation (e.g. rotation, translation, minimum angle size, etc.). A good generative model should be able to capture these factors of variation and recover the training samples. Furthermore, it should recover any sample from this manifold from which we can efficiently sample which is illustrated in Figure 2.</p>
<p>Computing the distance to the manifold. Let us consider the simplest case: single-channel gray scale images represented as vectors $x \in \mathbb{R}^{d^{2}}$. The distance of a sample $\hat{x} \in \mathbb{R}^{d^{2}}$ to the manifold is defined as the squared Euclidean distance to the closest sample from the manifold $\mathcal{C}<em _in="\in" _mathcal_C="\mathcal{C" x="x">{3}$, i.e. $\min </em><em i="1">{3}} \ell(x, \hat{x})=\sum</em>}^{d^{2}}\left|x_{i}-\hat{x<em 2="2">{i}\right|</em>=\arg \min }^{2}$. This is a non-convex optimization problem. We find an approximate solution by gradient descent on the vertices of the triangle (more generally, a convex polygon), ensuring that each iterate is a valid triangle (more generally, a convex polygon). To reduce the false-negative rate we repeat the algorithm 5 times from random initial solutions. To compute the latent representation of a sample $\hat{x} \in \mathbb{R}^{d \times d}$ we invert the generator, i.e. we solve $z^{\star<em 2="2">{z \in \mathbb{R}^{d}, \mid \mid \hat{x}}-G(z) |</em>$, using gradient descent on $z$ while keeping G fixed [17].}^{2</p>
<h1>6 Large-scale Experimental Evaluation</h1>
<p>We consider two budget-constrained experimental setups whereby in the (i) wide one-shot setup one may select 100 samples of hyper-parameters per model, and where the range for each hyperparameter is wide, and (ii) the narrow two-shots setup where one is allowed to select 50 samples from more narrow ranges which were manually selected by first performing the wide hyperparameter search over a specific data set. For the exact ranges and hyperparameter search details we refer the reader to the Appendix A. In the second set of experiments we evaluate the models based on the "novel" metric: $F_{1}$ score on the proposed data set. Finally, we included the Variational Autoencoder [14] in the experiments as a popular alternative.</p>
<p>Experimental setup. To ensure a fair comparison, we made the following choices: (i) we use the generator and discriminator architecture from INFO GAN [6] as the resulting function space is rich enough and all considered GANs were not originally designed for this architecture. Furthermore, it is similar to a proven architecture used in DCGAN [22]. The exception is BEGAN where an autoencoder is used as the discriminator. We maintain similar expressive power to INFO GAN by using identical convolutional layers the encoder and approximately matching the total number of parameters.</p>
<p>For all experiments we fix the latent code size to 64 and the prior distribution over the latent space to be uniform on $[-1,1]^{64}$, except for VAE where it is Gaussian $\mathcal{N}(0, \mathbf{I})$. We choose Adam [13] as the optimization algorithm as it was the most popular choice in the GAN literature (cf. Appendix F for an empirical comparison to RMSProp). We apply the same learning rate for both generator and discriminator. We set the batch size to 64 and perform optimization for 20 epochs on MNIST</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A wide range hyperparameter search (100 hyperparameter samples per model). Black stars indicate the performance of suggested hyperparameter settings. We observe that GAN training is extremely sensitive to hyperparameter settings and there is no model which is significantly more stable than others.
and FASHION MNIST, 40 on CELEbA and 100 on CIFAR. These data sets are a popular choice for generative modeling, range from simple to medium complexity, which makes it possible to run many experiments as well as getting decent results.</p>
<p>Finally, we allow for recent suggestions, such as batch normalization in the discriminator, and imbalanced update frequencies of generator and discriminator. We explore these possibilities, together with learning rate, parameter $\beta_{1}$ for ADAM, and hyperparameters of each model. We report the hyperparameter ranges and other details in Appendix A.</p>
<p>A large hyperparameter search. We perform hyperparameter optimization and, for each run, look for the best FID across the training run (simulating early stopping). To choose the best model, every 5 epochs we compute the FID between the 10 k samples generated by the model and the 10 k samples from the test set. We have performed this computationally expensive search for each data set. We present the sensitivity of models to the hyper-parameters in Figure 4 and the best FID achieved by each model in Table 2. We compute the best FID, in two phases: We first run a large-scale search on a wide range of hyper-parameters, and select the best model. Then, we re-run the training of the selected model 50 times with different initialization seeds, to estimate the stability of the training and report the mean FID and standard deviation, excluding outliers.</p>
<p>Furthermore, we consider the mean FID as the computational budget increases which is shown in Figure 3. There are three important observations. Firstly, there is no algorithm which clearly dominates others. Secondly, for an interesting range of FIDs, a "bad" model trained on a large budget can out perform a "good" model trained on a small budget. Finally, when the budget is limited, any statistically significant comparison of the models is unattainable.</p>
<p>Impact of limited computational budget. In some cases, the computational budget available to a practitioner is too small to perform such a large-scale hyperparameter search. Instead, one can tune the range of hyperparameters on one data set and interpolate the good hyperparameter ranges for other data sets. We now consider this setting in which we allow only 50 samples from a set of narrow ranges, which were selected based on the wide hyperparameter search on the FASHION-MNIST data set. We report the narrow hyperparameter ranges in Appendix A. Figure 14 shows the variance of FID per model, where the hyperparameters were selected from narrow ranges. From the practical point of view, there are significant differences between the models: in some cases the hyperparameter</p>
<p>Table 2: Best FID obtained in a large-scale hyperparameter search for each data set. The asterisk (<em>) on some combinations of models and data sets indicates the presence of significant outlier runs, usually severe mode collapses or training failures (</em>* indicates up to $20 \%$ failures). We observe that the performance of each model heavily depends on the data set and no model strictly dominates the others. Note that these results are not "state-of-the-art": (i) larger architectures could improve all models, (ii) authors often report the best FID which opens the door for random seed optimization.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">MNIST</th>
<th style="text-align: right;">FASHION</th>
<th style="text-align: right;">CIFAR</th>
<th style="text-align: right;">CELEBA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MM GAN</td>
<td style="text-align: right;">$9.8 \pm 0.9$</td>
<td style="text-align: right;">$29.6 \pm 1.6$</td>
<td style="text-align: right;">$72.7 \pm 3.6$</td>
<td style="text-align: right;">$65.6 \pm 4.2$</td>
</tr>
<tr>
<td style="text-align: left;">NS GAN</td>
<td style="text-align: right;">$6.8 \pm 0.5$</td>
<td style="text-align: right;">$26.5 \pm 1.6$</td>
<td style="text-align: right;">$58.5 \pm 1.9$</td>
<td style="text-align: right;">$55.0 \pm 3.3$</td>
</tr>
<tr>
<td style="text-align: left;">LSGAN</td>
<td style="text-align: right;">$7.8 \pm 0.6^{*}$</td>
<td style="text-align: right;">$30.7 \pm 2.2$</td>
<td style="text-align: right;">$87.1 \pm 47.5$</td>
<td style="text-align: right;">$53.9 \pm 2.8^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">WGAN</td>
<td style="text-align: right;">$6.7 \pm 0.4$</td>
<td style="text-align: right;">$21.5 \pm 1.6$</td>
<td style="text-align: right;">$55.2 \pm 2.3$</td>
<td style="text-align: right;">$41.3 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: left;">WGAN GP</td>
<td style="text-align: right;">$20.3 \pm 5.0$</td>
<td style="text-align: right;">$24.5 \pm 2.1$</td>
<td style="text-align: right;">$55.8 \pm 0.9$</td>
<td style="text-align: right;">$30.0 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: left;">DRAGAN</td>
<td style="text-align: right;">$7.6 \pm 0.4$</td>
<td style="text-align: right;">$27.7 \pm 1.2$</td>
<td style="text-align: right;">$69.8 \pm 2.0$</td>
<td style="text-align: right;">$42.3 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: left;">BEGAN</td>
<td style="text-align: right;">$13.1 \pm 1.0$</td>
<td style="text-align: right;">$22.9 \pm 0.9$</td>
<td style="text-align: right;">$71.4 \pm 1.6$</td>
<td style="text-align: right;">$38.9 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: left;">VAE</td>
<td style="text-align: right;">$23.8 \pm 0.6$</td>
<td style="text-align: right;">$58.7 \pm 1.2$</td>
<td style="text-align: right;">$155.7 \pm 11.6$</td>
<td style="text-align: right;">$85.7 \pm 3.8$</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: How does $F_{1}$ score vary with computational budget? The plot shows the distribution of the maximum $F_{1}$ score achievable for a fixed budget with a 95\% confidence interval. For each budget, we estimate the mean and confidence interval (of the mean) using 5000 bootstrap resamples out of 100 runs. When optimizing for $F_{1}$ score, both NS GAN and WGAN enjoy high precision and recall. The underwhelming performance of BEGAN and VAE on this particular data set merits further investigation.
ranges transfer from one data set to the others (e.g. NS GAN), while others are more sensitive to this choice (e.g. WGAN). We note that better scores can be obtained by a wider hyperparameter search. These results supports the conclusion that discussing the best score obtained by a model on a data set is not a meaningful way to discern between these models. One should instead discuss the distribution of the obtained scores.</p>
<p>Robustness to random initialization. For a fixed model, hyperparameters, training algorithm, and the order that the data is presented to the model, one would expect similar model performance. To test this hypothesis we re-train the best models from the limited hyperparameter range considered for the previous section, while changing the initial weights of the generator and discriminator networks (i.e. by varying a random seed). Table 2 and Figure 15 show the results for each data set. Most models are relatively robust to random initialization, except LSGAN, even though for all of them the variance is significant and should be taken into account when comparing models.</p>
<p>Precision, recall, and $\mathbf{F}<em 1="1">{1}$. We perform a search over the wide range of hyperparameters and compute precision and recall by considering $n=1024$ samples. In particular, we compute the precision of the model by computing the fraction of generated samples with distance below a threshold $\delta=0.75$. We then consider $n$ samples from the test set and invert each sample $x$ to compute $z^{\star}=G^{-1}(x)$ and compute the squared Euclidean distance between $x$ and $G\left(z^{\star}\right)$. We define the recall as the fraction of samples with squared Euclidean distance below $\delta$. Figure 5 shows the results where we select the best $F</em>$ score. Analogous plots where we instead maximize precision or recall for various thresholds are presented in Appendix E.}$ score for a fixed model and hyperparameters and vary the budget. We observe that even for this seemingly simple task, many models struggle to achieve a high $F_{1</p>
<h1>7 Limitations of the Study</h1>
<p>Data sets, neural architectures, and optimization issues. While we consider classic data sets from GAN research, unconditional generation was recently applied to data sets of higher resolution and arguably higher complexity. In this study we use one neural network architecture which suffices to achieve good results in terms of FID on all considered data sets. However, given data sets of higher complexity and higher resolution, it might be necessary to significantly increase the number of parameters, which in turn might lead to larger quantitative differences between different methods. Furthermore, different objective functions might become sensible to the choice of the optimization method, the number of training steps, and possibly other optimization hyperparameters. These effects should be systematically studied in future work.</p>
<p>Metrics. It remains to be examined whether FID is stable under a more radical change of the encoding, e.g using a network trained on a different task. Furthermore, it might be possible to "fool" FID can probably by introducing artifacts specialized to the encoding network. From the classic machine learning point of view, a major drawback of FID is that it cannot detect overfitting to the training data set - an algorithm that outputs only the training examples would have an excellent score. As such, developing quantitative evaluation metrics is a critical research direction [3, 23].</p>
<p>Exploring the space of hyperparameters. Ideally, hyperparameter values suggested by the authors should transfer across data sets. As such, exploring the hyperparameters "close" to the suggested ones is a natural and valid approach. However, Figure 4 and in particular Figure 14 show that optimization is necessary. In addition, such an approach has several drawbacks: (a) no recommended hyperparameters are available for a given data set, (b) the parameters are different for each data set, (c) several popular models have been tuned by the community, which might imply an unfair comparison. Finally, instead of random search it might be beneficial to apply (carefully tuned) sequential Bayesian optimization which is computationally beyond the scope of this study, but nevertheless a great candidate for future work [16].</p>
<h1>8 Conclusion</h1>
<p>In this paper we have started a discussion on how to neutrally and fairly compare GANs. We focus on two sets of evaluation metrics: (i) The Fréchet Inception Distance, and (ii) precision, recall and $F_{1}$. We provide empirical evidence that FID is a reasonable metric due to its robustness with respect to mode dropping and encoding network choices. Our main insight is that to compare models it is meaningless to report the minimum FID achieved. Instead, we propose to compare distributions of the minimum achivable FID for a fixed computational budget. Indeed, empirical evidence presented herein imply that algorithmic differences in state-of-the-art GANs become less relevant, as the computational budget increases. Furthermore, given a limited budget (say a month of compute-time), a "good" algorithm might be outperformed by a "bad" algorithm.
As discussed in Section 4, many dimensions have to be taken into account for model comparison, and this work only explores a subset of the options. We cannot exclude the possibility that that some models significantly outperform others under currently unexplored conditions. Nevertheless, notwithstanding the limitations discussed in Section 7, this work strongly suggests that future GAN research should be more experimentally systematic and model comparison should be performed on neutral ground.</p>
<h2>Acknowledgments</h2>
<p>We would like to acknowledge Tomas Angles for advocating convex polygons as a benchmark data set. We would like to thank Ian Goodfellow, Michaela Rosca, Ishaan Gulrajani, David Berthelot, and Xiaohua Zhai for useful discussions and remarks.</p>
<h2>References</h2>
<p>[1] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning (ICML), 2017.
[2] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (GANs). In International Conference on Machine Learning (ICML), 2017.
[3] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and empirics. In International Conference on Learning Representations (ICLR), 2018.
[4] Philip Bachman and Doina Precup. Variational generative stochastic networks with collaborative shaping. In International Conference on Machine Learning (ICML), 2015.
[5] David Berthelot, Tom Schumm, and Luke Metz. BEGAN: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
[6] Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2016.
[7] William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In International Conference on Learning Representations (ICLR), 2018.</p>
<p>[8] Holly E Gerhard, Felix A Wichmann, and Matthias Bethge. How sensitive is the human visual system to the local statistics of natural images? PLoS computational biology, 9(1), 2013.
[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014.
[10] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein gans. In Advances in Neural Information Processing Systems (NIPS), 2017.
[11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, 2017.
[12] Ferenc Huszár. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101, 2015.
[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.
[14] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. International Conference on Learning Representations (ICLR), 2014.
[15] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of GANs. arXiv preprint arXiv:1705.07215, 2017.
[16] Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The GAN Landscape: Losses, architectures, regularization, and normalization. arXiv preprint arXiv:1807.04720, 2018.
[17] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[18] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In International Conference on Computer Vision (ICCV), 2017.
[19] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in Neural Information Processing Systems (NIPS), 2017.
[20] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
[21] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In International Conference on Machine Learning (ICML), 2017.
[22] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
[23] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. In Advances in Neural Information Processing Systems (NIPS), 2018.
[24] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems (NIPS), 2016.
[25] Lucas Theis, Aïron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.
[26] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. International Conference on Learning Representations (ICLR), 2017.
[27] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. International Conference on Computer Vision (ICCV), 2017.</p>
<h1>A Wide and narrow hyperparameter ranges</h1>
<p>The wide and narrow ranges of hyper-parameters are presented in Table 3 and Table 4 respectively. In both tables, $\mathrm{U}(\mathrm{a}, \mathrm{b})$ means that the variable was sample uniformly from the range $[a, b]$. The $\mathrm{L}(\mathrm{a}, \mathrm{b})$ means that that the variable was sampled on a log-scale, that is $x L(a, b) \Longleftrightarrow x 10^{U(\log (a), \log (b))}$. The parameters used in the search:</p>
<ul>
<li>$\beta_{1}$ : the parameter of the Adam optimization algorithm.</li>
<li>Learning rate: generator/discriminator learning rate.</li>
<li>$\lambda$ : Multiplier of the gradient penalty for DRAGAN and WGAN GP. Learning rate for $k_{t}$ in BEGAN.</li>
<li>Disc iters: Number of discriminator updates per one generator update.</li>
<li>batchnorm: If True, the batch normalization will be used in the discriminator.</li>
<li>$\gamma$ : Parameter of BEGAN.</li>
<li>clipping: Parameter of WGAN, weights will be clipped to this value.</li>
</ul>
<p>Table 3: Wide ranges of hyper-parameters used for the large-scale search. "U" denotes uniform sampling, "L" sampling on a log-scale.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MM GAN</th>
<th style="text-align: center;">NS GAN</th>
<th style="text-align: center;">LSGAN</th>
<th style="text-align: center;">WGAN</th>
<th style="text-align: center;">WGAN GP</th>
<th style="text-align: center;">DRAGAN</th>
<th style="text-align: center;">BEGAN</th>
<th style="text-align: center;">VAE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Adam's $\beta_{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">U(0, 1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-5}, 10^{-2}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-1}, 10^{2}\right)$</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-1}, 10^{2}\right)$</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-4}, 10^{-2}\right)$</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">disc iter</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Either 1 or 5, sampled with the same probablity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">batchnorm</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">True or False, sampled with the same probability</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathrm{U}(0,1)$</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">clipping</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-3}, 10^{0}\right)$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
</tbody>
</table>
<p>Table 4: Narrow ranges of hyper-parameters used in the search with 50 samples per model. The ranges were optimized by looking at the wide search results for fashion-mnist data set. "U" denotes uniform sampling, "L" sampling on a log-scale.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MM GAN</th>
<th style="text-align: center;">NS GAN</th>
<th style="text-align: center;">LSGAN</th>
<th style="text-align: center;">WGAN</th>
<th style="text-align: center;">WGAN GP</th>
<th style="text-align: center;">DRAGAN</th>
<th style="text-align: center;">BEGAN</th>
<th style="text-align: center;">VAE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Adam's $\beta_{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Always 0.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-4}, 10^{-3}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-1}, 10^{1}\right)$</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-1}, 10^{1}\right)$</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-4}, 10^{-2}\right)$</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">disc iter</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Always 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">batchnorm</td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">False</td>
<td style="text-align: center;">True/False</td>
<td style="text-align: center;">True/False</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathrm{U}(0.6,0.9)$</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">clipping</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathrm{L}\left(10^{-2}, 10^{0}\right)$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
</tbody>
</table>
<h2>B Which parameters really matter?</h2>
<p>Figure 6, Figure 7, Figure 8 and Figure 9 present scatter plots for data sets FASHION MNIST, MNIST, CIFAR, CELEBA respectively. For each model and hyper-parameter we estimate its impact on the final FID. Figure 6 was used to select narrow ranges of hyper-parameters.</p>
<h1>FASHION-MNIST (wide range)</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Wide range scatter plots for FASHION MNIST. For each algorithm (column) and each parameter (row), the corresponding scatter plot shows the FID in function of the parameter. This illustrates the sensitivity of each algorithm w.r.t. each parameter. Those results have been used to choose the narrow range in Table 4. For example, Adam's $\beta_{1}$ does not seem to significantly impact any algorithm, so for the narrow range, we fix its value to always be 0.5 . Likewise, we fix the number of discriminator iterations to be always 1 . For other parameters, the selected range is smaller (e.g. learning rate), or can differ for each algorithm (e.g. batch norm).</p>
<h1>MNIST (wide range)</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Wide range scatter plots for MNIST. For each algorithm (column) and each parameter (row), the corresponding scatter plot shows the FID in function of the parameter. This illustrates the sensitivity of each algorithm w.r.t. each parameter.</p>
<h1>CIFAR10 (wide range)</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Wide range scatter plots for CIFAR10. For each algorithm (column) and each parameter (row), the corresponding scatter plot shows the FID in function of the parameter. This illustrates the sensitivity of each algorithm w.r.t. each parameter.</p>
<h1>CELEBA (wide range)</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Wide range scatter plots for CelebA. For each algorithm (column) and each parameter (row), the corresponding scatter plot shows the FID in function of the parameter. This illustrates the sensitivity of each algorithm w.r.t. each parameter.</p>
<h1>C Fréchet Inception Distance and Image Quality</h1>
<p>It is interesting to see how the FID translates to the image quality. In Figure 10, Figure 11, Figure 12 and Figure 13, we present, for every model, the distribution of FIDs and the corresponding samples.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: MNIST: Distribution of FIDs and corresponding samples for each model when sampling parameters from wide ranges.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: FASHION-MNIST: Distribution of FIDs and corresponding samples for each model when sampling parameters from wide ranges.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: CIFAR 10: Distribution of FIDs and corresponding samples for each model when sampling parameters from wide ranges.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: CELEBA: Distribution of FIDs and corresponding samples for each model when sampling parameters from wide ranges.</p>
<h1>D Hyper-parameter Search over Narrow Ranges</h1>
<p>In Figure 4 we presented the sensitivity of GANs to hyperparameters, assuming the samples are taken from the wide ranges (see Table 3). For completeness, in Figure 14 we present a similar comparison for the narrow ranges of hyperparameters (presented in Table 4).
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: A narrow range search of hyperparameters which were selected based on the wide hyperparameter search on the FASHION-MNIST data set. Black stars indicate the performance of suggested hyperparameter settings. For each model we allow 50 hyperparameter samples. From the practical point of view, there are significant differences between the models: in some cases the hyperparameter ranges transfer from one data set to the others (e.g. NS GAN), while others are more sensitive to this choice (e.g. WGAN). We note that better scores can be obtained by a wider hyperparameter search.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: For each model we search for best hyperparameters on the wide range. Then, we retrain each model using the best parameters 50 times with random initialization of the weights, keeping everything else fixed. We observe a slight variance in the final FID. Hence, when an FID is reported it is paramount that one compares the entire distribution, instead of the best seed for the best run. The figure corresponds to Table 2.</p>
<h1>E Precision, Recall and $F_{1}$ as a Function of the Budget</h1>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Optimizing for $F_{1}$, threshold $\delta=1.0$.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17: Optimizing for $F_{1}$, threshold $\delta=0.5$.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18: Optimizing for precision, threshold $\delta=1.0$.</p>
<h2>F Impact of the optimization algorithm.</h2>
<p>We ran the WGAN training across 100 hyperparameter settings. In the first set of experiments we used the ADAM optimizer, and in the second the RMSPROP optimizer. We observe that distribution of the scores is similar and it's unclear which optimizer is "better". However, on both data sets ADAM outperformed RMSPROP on recommended parameters (CIFAR10: 154.5 vs 161.2, CELEBA: 97.9 vs 216.3 ) which highlights the need for a hyperparameter search. As a result, the conclusions of this work are not altered by this choice.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19: Optimizing for precision, threshold $\delta=0.5$.
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20: Optimizing for recall, threshold $\delta=1.0$.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 21: Optimizing for recall, threshold $\delta=0.5$.</p>
<h1>G $F_{1}$, precision, and recall correlation with FID</h1>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 22: Correlation of FID with precision, recall, and $F_{1}$. We observe that the proposed measure is particularly suitable for detecting a loss in recall.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Reproducing these experiments requires approximately 6.85 GPU years (NVIDIA P100).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
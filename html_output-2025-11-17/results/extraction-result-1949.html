<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1949 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1949</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1949</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-276558097</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.15649v1.pdf" target="_blank">A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) has gained traction for its success in solving complex tasks for robotic applications. However, its deployment on physical robots remains challenging due to safety risks and the comparatively high costs of training. To avoid these problems, RL agents are often trained on simulators, which introduces a new problem related to the gap between simulation and reality. This paper presents an RL pipeline designed to help reduce the reality gap and facilitate developing and deploying RL policies for real-world robotic systems. The pipeline organizes the RL training process into an initial step for system identification and three training stages: core simulation training, high-fidelity simulation, and real-world deployment, each adding levels of realism to reduce the sim-to-real gap. Each training stage takes an input policy, improves it, and either passes the improved policy to the next stage or loops it back for further improvement. This iterative process continues until the policy achieves the desired performance. The pipeline's effectiveness is shown through a case study with the Boston Dynamics Spot mobile robot used in a surveillance application. The case study presents the steps taken at each pipeline stage to obtain an RL agent to control the robot's position and orientation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1949.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1949.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spot_nav_sim2real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boston Dynamics Spot position-orientation control sim-to-real experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sim-to-real pipeline application where an RL policy trained in staged simulators (core Gymnasium with identified actuator mapping, then Gazebo) was deployed on a Boston Dynamics Spot to control position and orientation for waypoint navigation and surveillance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>position and orientation control / waypoint navigation (surveillance)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>minimal contact</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>final training tolerances: 0.05 m position, 1° orientation (trained); navigation used 0.3 m tolerance for path-following</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>command-to-executed velocity mapping v = f(a) per-dimension (3rd-order polynomial fit for vx, vy, vθ), commanded velocity limits (ax, ay, aθ ranges), modeling of executed velocity nonlinearity and asymmetry (faster forward than backward); high-fidelity stage noted inclusion of actuator latency and dead zones (as simulation realism features)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>core simulator treated motion via identified executed-velocity mapping and first-order integration (Δt) without modeling minimum action duration and stopping latency; friction, detailed motor internal dynamics, inertia coupling beyond the polynomial mapping were not modelled in core simulator</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>two-stage approach: core simulation used an identified, simplified dynamics model (polynomial command→executed velocity) with first-order integration; high-fidelity stage used Gazebo for physics, sensor emulation and ROS I/O matching but no further RL training</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td>polynomial fit approximated executed velocities per axis (3rd-order) with visually good fit shown, but no quantitative error bounds or percent fidelity reported</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>core-sim success rate (100% at final tolerances); real-world: task completion of 197 m surveillance path in 201 s (average speed 0.98 m/s) while meeting expected tolerances; qualitative trajectory similarity between Gymnasium, Gazebo and real robot reported</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>qualitative: trajectories in real robot closely matched Gazebo; main discrepancy was a stopping delay causing overshoot and oscillations around some goals; no numeric degradation (e.g., % drop) reported</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>qualitative comparison across fidelity levels: Gymnasium (core) trajectories, Gazebo (high-fidelity) trajectories, and real-robot trajectories compared visually; no quantitative performance comparison across fidelity levels reported</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>randomization applied to robot state (position and orientation noise) during core training to improve robustness; no actuator-parameter randomization reported</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Boston Dynamics Spot (legged mobile robot)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>observed overshoot and oscillations on real robot attributed to robot's stopping latency and a minimum action duration not modelled in the core simulator; additional oscillations during navigation possibly due to integration with localization/planner and larger pose-estimation variations</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Explicitly identifying the command→executed-velocity mapping (system identification) and incorporating that mapping in the simulator substantially improves sim-to-real transfer for mobile control; unmodelled actuator timing effects (latency/minimum action duration) produce stopping overshoot, indicating actuator temporal dynamics are critical to model for accurate transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1949.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1949.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Peng2018_dynrand</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer of robotic control with dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that achieved successful sim-to-real transfer by comprehensively randomizing dynamics and controller parameters (95 parameters) for a robotic arm, enabling zero-shot transfer to the real system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer of robotic control with dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>robotic arm control / manipulation (general arm application)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>comprehensive dynamics randomization including arm dynamics, controller gains, links' mass, friction, noise levels, and time steps (95 randomized parameters reported)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>approach relies on extensive domain/dynamics randomization rather than a single high-fidelity parameter match; broad coverage of parameter uncertainty to produce robust policies</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td>95 parameters randomized (paper reference); ranges or per-parameter fidelity not provided in this paper's mention</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>reported successful sim-to-real transfer without additional training on the physical system (no numerical success rate provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>comprehensive randomization of environment and dynamics parameters including table size, arm dynamics, controller gains, links' mass, friction, noise levels, time steps; total of 95 parameters randomized</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>robotic arm (manipulator)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Large-scale dynamics randomization across actuator, link, and controller parameters can enable robust zero-shot sim-to-real transfer for robotic arms by covering real-world parameter variability instead of exact parameter identification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1949.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1949.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RubiksCube_Akkaya2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solving Rubik's Cube with a Robot Hand</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced dexterous-manipulation study that combined system identification, domain randomization and curriculum learning to train a robot hand to solve a Rubik's cube and successfully transfer the dexterous policy to the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving rubik's cube with a robot hand</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>dexterous in-hand manipulation: solving a Rubik's cube</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>high precision / dexterity (24 degrees-of-freedom hand manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>domain randomization and system identification included parameters such as cube size, friction, force ranges, inertia, and action latency</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>high-fidelity simulation augmented with extensive domain randomization and system identification to capture robot and object dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>reported achievement of a high level of dexterity and successful transfer to the real robot (no numeric success rate provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>randomized cube size, friction, force ranges, inertia, action latency (exact ranges not provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>dexterous robotic hand (~24 DOF)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Combining system identification with domain randomization (including action latency, object inertia and friction) and curriculum learning is effective for enabling sim-to-real transfer for contact-rich dexterous manipulation; temporal actuator characteristics (action latency) and object dynamic parameters are important to randomize/model.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for sim2real transfer of automatically generated grasping datasets <em>(Rating: 1)</em></li>
                <li>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 1)</em></li>
                <li>Using simulation and domain adaptation to improve efficiency of deep robotic grasping <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1949",
    "paper_id": "paper-276558097",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "Spot_nav_sim2real",
            "name_full": "Boston Dynamics Spot position-orientation control sim-to-real experiment",
            "brief_description": "Sim-to-real pipeline application where an RL policy trained in staged simulators (core Gymnasium with identified actuator mapping, then Gazebo) was deployed on a Boston Dynamics Spot to control position and orientation for waypoint navigation and surveillance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "position and orientation control / waypoint navigation (surveillance)",
            "task_timescale": null,
            "task_contact_ratio": "minimal contact",
            "task_precision_requirement": "final training tolerances: 0.05 m position, 1° orientation (trained); navigation used 0.3 m tolerance for path-following",
            "actuator_parameters_modeled": "command-to-executed velocity mapping v = f(a) per-dimension (3rd-order polynomial fit for vx, vy, vθ), commanded velocity limits (ax, ay, aθ ranges), modeling of executed velocity nonlinearity and asymmetry (faster forward than backward); high-fidelity stage noted inclusion of actuator latency and dead zones (as simulation realism features)",
            "actuator_parameters_simplified": "core simulator treated motion via identified executed-velocity mapping and first-order integration (Δt) without modeling minimum action duration and stopping latency; friction, detailed motor internal dynamics, inertia coupling beyond the polynomial mapping were not modelled in core simulator",
            "fidelity_level_description": "two-stage approach: core simulation used an identified, simplified dynamics model (polynomial command→executed velocity) with first-order integration; high-fidelity stage used Gazebo for physics, sensor emulation and ROS I/O matching but no further RL training",
            "parameter_specific_fidelity": "polynomial fit approximated executed velocities per axis (3rd-order) with visually good fit shown, but no quantitative error bounds or percent fidelity reported",
            "transfer_success_metric": "core-sim success rate (100% at final tolerances); real-world: task completion of 197 m surveillance path in 201 s (average speed 0.98 m/s) while meeting expected tolerances; qualitative trajectory similarity between Gymnasium, Gazebo and real robot reported",
            "sim_vs_real_performance": "qualitative: trajectories in real robot closely matched Gazebo; main discrepancy was a stopping delay causing overshoot and oscillations around some goals; no numeric degradation (e.g., % drop) reported",
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "qualitative comparison across fidelity levels: Gymnasium (core) trajectories, Gazebo (high-fidelity) trajectories, and real-robot trajectories compared visually; no quantitative performance comparison across fidelity levels reported",
            "domain_randomization_used": true,
            "domain_randomization_details": "randomization applied to robot state (position and orientation noise) during core training to improve robustness; no actuator-parameter randomization reported",
            "robot_type": "Boston Dynamics Spot (legged mobile robot)",
            "transfer_failure_analysis": "observed overshoot and oscillations on real robot attributed to robot's stopping latency and a minimum action duration not modelled in the core simulator; additional oscillations during navigation possibly due to integration with localization/planner and larger pose-estimation variations",
            "key_finding_for_theory": "Explicitly identifying the command→executed-velocity mapping (system identification) and incorporating that mapping in the simulator substantially improves sim-to-real transfer for mobile control; unmodelled actuator timing effects (latency/minimum action duration) produce stopping overshoot, indicating actuator temporal dynamics are critical to model for accurate transfer.",
            "uuid": "e1949.0"
        },
        {
            "name_short": "Peng2018_dynrand",
            "name_full": "Sim-to-real transfer of robotic control with dynamics randomization",
            "brief_description": "Referenced work that achieved successful sim-to-real transfer by comprehensively randomizing dynamics and controller parameters (95 parameters) for a robotic arm, enabling zero-shot transfer to the real system.",
            "citation_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "mention_or_use": "mention",
            "task_name": "robotic arm control / manipulation (general arm application)",
            "task_timescale": null,
            "task_contact_ratio": null,
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "comprehensive dynamics randomization including arm dynamics, controller gains, links' mass, friction, noise levels, and time steps (95 randomized parameters reported)",
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "approach relies on extensive domain/dynamics randomization rather than a single high-fidelity parameter match; broad coverage of parameter uncertainty to produce robust policies",
            "parameter_specific_fidelity": "95 parameters randomized (paper reference); ranges or per-parameter fidelity not provided in this paper's mention",
            "transfer_success_metric": "reported successful sim-to-real transfer without additional training on the physical system (no numerical success rate provided in this paper)",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": null,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": null,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "comprehensive randomization of environment and dynamics parameters including table size, arm dynamics, controller gains, links' mass, friction, noise levels, time steps; total of 95 parameters randomized",
            "robot_type": "robotic arm (manipulator)",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "Large-scale dynamics randomization across actuator, link, and controller parameters can enable robust zero-shot sim-to-real transfer for robotic arms by covering real-world parameter variability instead of exact parameter identification.",
            "uuid": "e1949.1"
        },
        {
            "name_short": "RubiksCube_Akkaya2019",
            "name_full": "Solving Rubik's Cube with a Robot Hand",
            "brief_description": "Referenced dexterous-manipulation study that combined system identification, domain randomization and curriculum learning to train a robot hand to solve a Rubik's cube and successfully transfer the dexterous policy to the real robot.",
            "citation_title": "Solving rubik's cube with a robot hand",
            "mention_or_use": "mention",
            "task_name": "dexterous in-hand manipulation: solving a Rubik's cube",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": "high precision / dexterity (24 degrees-of-freedom hand manipulation)",
            "actuator_parameters_modeled": "domain randomization and system identification included parameters such as cube size, friction, force ranges, inertia, and action latency",
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "high-fidelity simulation augmented with extensive domain randomization and system identification to capture robot and object dynamics",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "reported achievement of a high level of dexterity and successful transfer to the real robot (no numeric success rate provided in this paper)",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": null,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "randomized cube size, friction, force ranges, inertia, action latency (exact ranges not provided in this paper)",
            "robot_type": "dexterous robotic hand (~24 DOF)",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "Combining system identification with domain randomization (including action latency, object inertia and friction) and curriculum learning is effective for enabling sim-to-real transfer for contact-rich dexterous manipulation; temporal actuator characteristics (action latency) and object dynamic parameters are important to randomize/model.",
            "uuid": "e1949.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for sim2real transfer of automatically generated grasping datasets",
            "rating": 1
        },
        {
            "paper_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 1
        },
        {
            "paper_title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping",
            "rating": 1
        }
    ],
    "cost": 0.0136145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications
21 Feb 2025</p>
<p>Jefferson Silveira jefferson.silveira@queensu.ca 
Dept. of Electrical and Computer Engineering
Queen's University Kingston
ONCanada</p>
<p>Joshua A Marshall joshua.marshall@queensu.ca 
Ingenuity Labs Research Institute Queen's University Kingston
ONCanada</p>
<p>Sidney N Givigi Jrsidney.givigi@queensu.ca 
School of Computing
Queen's University Kingston
ONCanada</p>
<p>A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications
21 Feb 202502FC5F49279D5CDC44B361CF9EC34D4CarXiv:2502.15649v1[cs.RO]
Reinforcement learning (RL) has gained traction for its success in solving complex tasks for robotic applications.However, its deployment on physical robots remains challenging due to safety risks and the comparatively high costs of training.To avoid these problems, RL agents are often trained on simulators, which introduces a new problem related to the gap between simulation and reality.This paper presents an RL pipeline designed to help reduce the reality gap and facilitate developing and deploying RL policies for real-world robotic systems.The pipeline organizes the RL training process into an initial step for system identification and three training stages: core simulation training, high-fidelity simulation, and real-world deployment, each adding levels of realism to reduce the sim-to-real gap.Each training stage takes an input policy, improves it, and either passes the improved policy to the next stage or loops it back for further improvement.This iterative process continues until the policy achieves the desired performance.The pipeline's effectiveness is shown through a case study with the Boston Dynamics Spot mobile robot used in a surveillance application.The case study presents the steps taken at each pipeline stage to obtain an RL agent to control the robot's position and orientation.</p>
<p>I. INTRODUCTION</p>
<p>In recent years, Reinforcement Learning (RL) has achieved high-level performance in solving challenging robotic tasks.While RL itself is not new (with roots dating back to the 1960s [1]), its recent successes can be attributed to several factors: the availability of powerful and affordable processing units (such as CPUs and GPUs), advancements in deep learning techniques, and the development of new deep RL methods.</p>
<p>The RL methodology relies on three core concepts: the agent, the reward signal and the environment.The agent is the decision-maker that learns by interacting with the environment.The reward signal defines the goal of the RL problem, serving as feedback to encourage desirable behaviours and discourage undesirable ones.The environment is where the agent operates, providing observations in response to the agent's actions [2].The agent gathers experience to learn the appropriate actions and solve the RL problem.This is an iterative approach that can be directly applied to a physical robot.</p>
<p>Unfortunately, applying RL directly in real-life applications is often prohibitive, requiring the robot to collect data by iteratively acting on the environment [3].This iterative process can be costly and time-consuming.There may be safety concerns because the robot may take unexpected actions during training, possibly causing damage to itself or its surroundings.</p>
<p>The success of RL in robotics is partly due to the development of a methodology known as off-policy RL, which enables offline training [3] where the RL training process happens after data collection.Algorithms such as Q-learning [2], SAC [4], and TD3 [5] are examples of off-policy RL.Another advantage of off-policy RL is that it allows for the use of simulated data during training.Unfortunately, this approach introduces other challenges, because discrepancies between the simulated and the real world may cause the robot to perform worse when compared to its simulated counterpart.This problem, often referred to by various terms such as the reality gap, sim-toreal gap, and sim2real gap [6], arises because off-policy RL is usually trained mostly, if not entirely, in simulation.Some studies focus on reducing this problem [7]- [9], and strategies they apply include (i) creating high-fidelity simulations with parameters modelled directly from the robot, and (ii) randomizing the simulation parameters to generate a policy that is robust to environmental variations.</p>
<p>Considering these methodologies, this work presents a pipeline to facilitate the development of RL models on realworld robotic systems.A simplified overview of each phase's components is shown in Fig. 1.The proposed pipeline combines established methodologies from the RL literature into a staged process that involves system identification, training with different levels of simulation complexity, and real-world deployment.While the individual techniques included in the pipeline are not novel, this paper's contribution lies in a systematic approach to the learning process.Unlike most studies, which often detail only the specific steps for their application, this pipeline offers a modular framework to guide practitioners in adapting RL models to real-world scenarios.To support this, a case study demonstrating the successful deployment of an RL policy on a mobile robot is also presented.</p>
<p>The remainder of the paper is organized as follows.Section II presents common approaches to reduce the reality gap; Section III describes the proposed pipeline and discusses its application in robotics; Section IV describes the steps taken to train an RL policy using the proposed pipeline; Section V illustrates the results in a surveillance application.Finally, Section VI presents the final remarks.</p>
<p>II. RELATED WORK</p>
<p>This section presents previous studies on the simulation-toreality gap and links these concepts to the proposed pipeline.For this purpose, it is necessary to define the concept of environment used in this paper.In robotic RL applications, the environment includes not only the space where the robot operates but also the robot itself, and the agent is the decisionmaking algorithm or policy that sends commands via the robot.However, it is also common to refer to the robot along with the learning algorithm as the agent and the environment as everything external to the robot.This paper adopts this latter convention to distinguish the robot from its surroundings.</p>
<p>Research on RL in robotic systems has led to various methodologies for improving sample efficiency of RL algorithms and reducing the sim-to-real problem.The sample efficiency in RL refers to the number of samples (i.e., experiences) needed to achieve a certain performance level.A comprehensive survey on the topic is presented by Calderón-Cordova et al. [8].They present many techniques, frameworks, tools, and a practical guide for developing RL control applications focused on robotic manipulators.Even though their work is extensive, they propose a simple pipeline containing a single simulation stage with no discussion of the various levels of simulation complexity and their effects on the reality gap.</p>
<p>In contrast, [10] presents a survey on RL applied to bioinspired robots that categorize RL methodologies into four main groups: 1) methods that rely on accurate simulators; 2) approaches that only use simplified kinematic or dynamic models; 3) techniques that apply RL on top of hierarchical controllers; and 4) methods that leverage human demonstrations.They also mention that while methodologies in group 1 often perform better after a sim-to-real transfer, they are less sampleefficient than the others.Our proposed pipeline supports not only a single simulation stage, as presented in [8] but also allows for multi-level simulation complexities.This includes options to integrate high-fidelity simulators and simplified kinematic or dynamic models as core simulators.</p>
<p>In another survey, Zhao et al. [9] discuss not only the potential benefits of high-fidelity simulation in improving the sim-to-real transfer but also present three other approaches: system identification to create a simulator tailored to a specific robot, domain randomization, and domain adaptation.Domain randomization methods involve modelling parameters from reality and randomizing their values in the simulator to cover the actual distribution of these parameters in the real world.On the other hand, domain adaptation involves the combination of two or more environments during the training process.For example, one could train a model in a high-fidelity simulator and then transfer the model to training on real-world data.</p>
<p>Several works successfully use domain randomization [11]- [13].In [11], a feedback approach is used to change the simulation parameters based on how the real robot performs with the transferred policy, thus allowing for automatic randomization to achieve high performance on the transferred policy.In contrast, [12] performs a comprehensive randomization of parameters (e.g., table size, arm dynamics, controller gains, links' mass, friction, noise levels, and time steps) on a robotic arm application, totalling 95 randomized parameters, resulting in a successful sim-to-real transfer without additional training on the physical system.Using a combination of system identification techniques, domain randomization and curriculum learning, [13] obtained a remarkable level of dexterity in a robotic hand when trained to solve the Rubik's cube.</p>
<p>Domain adaptation techniques involve converting input data from one domain into another, where most training is performed.For example, [14] develops a machine-learning model that converts the visual feedback data from the real world into a format that resembles simulated inputs.This translation technique allows a model trained in simulation to perform well on physical robots.Similar strategies are used in [15].</p>
<p>Note that these methodologies are not always necessary.For example, [16] presents a sim-to-real pipeline to address the challenge of robot navigation in 3D cluttered environments.Their pipeline includes a simulation stage that matches the inputs and outputs to the real robot, and uses simulated sensors and state estimation techniques that closely matched the real ones.These steps achieved successful real-world transfer without requiring any adaptation or additional training.While control problems like these could be solved with classical control techniques, RL provides a data-based adaptive methodology that is independent of the robot model.</p>
<p>Ultimately, the requirements of the RL process depend on the complexity of the task.If the robot is passively stable and accepts simple commands, such as linear and angular velocities, it is possible to transfer the learned model without further adaptation [16].However, more complex problems, such as solving a Rubik's cube with 24 degrees of freedom, require more steps to achieve acceptable performance in real applications [13].</p>
<p>III. PROPOSED REINFORCEMENT LEARNING PIPELINE</p>
<p>The pipeline is proposed as a multi-stage process to turn RL policies into real-world applications as presented in Fig. 1, with four phases: system identification, core simulation training, high-fidelity training, and real-life training, which are explained further in this section.Fig. 2 describes the data flow and interactions between the RL agent and the environments during training.This design allows the customization of each phase to meet the specific requirements of a task.For example, tasks that involve manipulating physical objects (e.g., a Rubik's cube [13]) may benefit from utilizing all stages of training.In contrast, more straightforward tasks with no interaction with other objects, like the position control of a wheeled vehicle, might only require the system identification and core simulation stages.This flexibility is valuable, especially for researchers and industry practitioners new to RL.</p>
<p>The following subsections provide a detailed description of each component and their interactions in the proposed pipeline.</p>
<p>A. System Identification</p>
<p>This stage focuses on obtaining a data-based model of the robot to be applied in the following stages.Although optional, this step can be crucial to reduce the reality gap by considering the robot's physical parameters in the simulators used in the following stages.The system identification field is vast, and techniques include identifying linear or nonlinear models that convert inputs to outputs, frequency response analysis, machine learning, and many others.A good resource on system identification is provided in [17].Section IV provides further details on the system identification technique used in the presented case study.</p>
<p>B. Core Simulation Training</p>
<p>This is the first training stage in Fig. 2, and it allows the RL agent to learn within a simplified simulation environment.This phase could involve modelling either the kinematic or dynamic equations of motions.For example, an RL agent could be trained to control a differential-drive robot by modelling the kinematic equations while ignoring factors like friction, motor mismatch, and other sources of error, such as in [16], where RL is applied to control a wheeled robot to navigate in rough terrain.While this simplified approach might allow the policy to transfer to the actual robot with minimal training, the transfer could also result in policy degradation.Incorporating system identification parameters into the simulation could ensure smoother transfer and more reliable performance.</p>
<p>The passing criteria in this and subsequent stages can be a numerical function that determines if the model passes a predefined performance score or based on practitioners' prior experience to assess model feasibility.Examples of performance metrics include the success rate for goal-based problems or the accumulated reward.The tools used in this stage include the gymnasium API developed for creating RL simulators and physics simulators, such as Bullet and MuJoCo, that can be used to implement rigid body dynamics.</p>
<p>C. High-Fidelity Training</p>
<p>The second training stage in Fig. 2 involves high-fidelity simulation, incorporating realism such as gravity, friction, actuator latency and dead zones, sensor noise, and realistic renderings.This reduces the sim-to-real gap, critical for complex RL tasks.With high-fidelity simulators, domain randomization is a powerful technique to further reduce the sim-to-real gap.For example, domain randomization (e.g., cube size, friction, force ranges, inertia, and action latency) enhanced the training process in the Rubik's cube robot manipulation task [13], where it enabled successful transfer into the real robot.</p>
<p>Common tools used in this stage, such as Gazebo, Cop-peliaSim and Isaac Sim, offer advanced features like accurate sensor modelling, real-robot input/output matching and Robot Operating System (ROS) support, enabling near-seamless transitions between simulated and real environments.</p>
<p>D. Real-Life Training</p>
<p>In the final phase, the RL model is deployed on the physical robot, and the performance degradation is evaluated.If performance is inadequate, fine-tuning the model with real-world data or addressing discrepancies through high-fidelity simulation, using either domain randomization or domain adaptation, can reduce the sim-to-real gap.This iterative process continues until the model meets the desired performance criteria.</p>
<p>E. Debugging RL Applications with the Pipeline</p>
<p>RL requires careful integration of components, including actions, observations, reward signals, simulation, RL algorithms, and well-tuned hyperparameters.Complex robotics tasks with continuous observations and actions often require multiple neural networks, further increasing the number of hyperparameters that must be optimized.</p>
<p>More often than not, the initial training fails to converge to a useful policy, with issues stemming from suboptimal hyperparameters, insufficient observations, or neural networks that need more neurons or layers.In these situations, a common solution is to start with a simplified simulation and minimal observations and actions, making it easier to find workable parameters.This process can then iterate, gradually increasing task complexity or progressing through the pipeline stages until the desired performance is obtained.</p>
<p>IV. CASE STUDY ON A MOBILE ROBOT</p>
<p>This section presents a case study on applying the discussed concepts to obtain an RL policy for the Boston Dynamics Spot robot.Spot, an agile legged robot, autonomously computes gait and foot placement on linear (a x ), lateral (a y ) and angular body velocities (a θ ) commands.The objective was to train an RL model to control the robot's position and orientation to reach a desired configuration while optimizing a cost function.This controller was then used in a surveillance application.</p>
<p>A. System Identification</p>
<p>In this stage, the objective was to reduce the sim-to-real gap by modelling the robot's motion constraints.The robot cannot perfectly execute commanded velocities because converting desired speeds into leg movements and actuation limits introduces errors.Training an RL agent solely on commanded velocities a = (a x , a y , a θ ) risks poor control because these may not align with what the robot can physically achieve.</p>
<p>The first step was identifying the velocities the robot could execute.To do so, a grid of commanded body velocities a was created (Fig. 3a), and the executed body velocities v = (v x , v y , v θ ) were measured using a motion capture system, with grid ranges shown in Table I.Note that a x is not symmetrical because the robot moves faster forward than backward, adding control complexity, which the RL agent can still learn to handle.Fig. 4: The executed velocities of the real robot are approximated using a polynomial function approximator.</p>
<p>The system identification process involved finding a function that approximates the executed velocities from commanded velocities (Fig. 4), represented as
v = f (a),(1)
where v is the approximated body velocity and f (a) is a multivariate function with a third-order polynomial with no bias term in each dimension of a, resulting in:
v =   vx vy vθ   =   fx (a) fy (a) fθ (a)   ,(2)
where fx (a), fy (a), and fθ (a) are the polynomial functions that approximate the executed velocities in each dimension.For clarity, the approximation for vx is given by
vx = fx (a) = 0&lt;i+j+l≤3 c x,ijl (a i x a j y a l θ ),(3)
where i, j, l ∈ Z ≥0 and the coefficients c x,ijl are computed using the least squares method that minimizes the total squared error
J x = N −1 n=0 (v x,n − vx,n ) 2 ,(4)
where N represents the total number of samples in the grid (Fig. 3a), and the n index represents the n-th element.The coefficients of vy and vθ are computed similarly.Fig. 3b shows the linear regression results with the approximated velocities overlaid on the executed ones, illustrating a good fit from the polynomial regression.</p>
<p>B. RL Formulation</p>
<p>The problem was modelled as a goal-conditioned Markov Decision Process (MDP) ⟨S, G, A, T, R⟩, where S is the state space, G the goal space, A the action space, T : S × A → S the state transition function, and R(a, s, g), R : S × A → R the reward function that returns a scalar when taking action a and arriving at state s with goal g.The goal is to find a policy π(a|s, g) that maximizes the agent's cumulative reward [18].The policy π(a|s, g) defines a probability distribution over the action space A given a combination of state s and goal g.The policy can be applied probabilistically, where each action a is a sample of the probability distribution, or deterministically, where a is the distribution mean.</p>
<p>1) State and Goal Spaces: Since the goal of the RL agent is to control the position and orientation of the robot, the state was defined as
s = (x, y, θ) ∈ R 2 × [−π, π).(5)
Similarly, the goal is defined as
g = (x g , y g , θ g ) ∈ [r min , r max ] 2 × [−π, π),(6)
where r min and r max represent the lower and upper limits of x g and y g .Note that the state and goal combination form the observation o = (s, g), but separating them into state and goal vectors is more intuitive.</p>
<p>2) Action Space: The commanded action was defined as
a = (a x , a y , a θ ) ∈ R 3 . (7)
They represent the linear, lateral and angular body velocities commands sent to the robot.</p>
<p>3) Simulation and Transition</p>
<p>The simulator used in the core simulation stage converts the desired action at time step k into the identified executed action, as shown in Section IV-A, and computes a new state resulting from the applied action.The new state is obtained through a first-order integration of the executed velocity as
vk = f (a k ) (8) s k = s k−1 + vk ∆t,(9)
with ∆t being the step duration.Since this is a goal-conditioned MDP, our simulator must also inform when the robot reaches g.To do so, the error in position and orientation are computed
e p,k = ∥p g − p s,k ∥ 2 , (10) e θ,k = |θ g − θ k |,(11)
where p g = (x g , y g ), p s,k = (x k , y k ), and || • || 2 represents the Euclidean norm.Then, the robot successfully reaches g when e p,k &lt; ϵ p and e θ,k &lt; ϵ θ , where ϵ p and ϵ θ are tolerance parameters that define the required proximity to the goal.The reward function for this study was chosen based on common costs used in model predictive control applications.For interested readers on this topic, the authors of [19] present an insightful comparison between optimal control and reinforcement learning techniques and their costs.Our objective was to optimize a policy that minimizes control actions, action smoothness, and time, resulting in the following cost function:
J = N −1 k=0 ∥u k ∥ 2 R + ∥u k − u k−1 ∥ 2 S + λ k , (12)
where the notation ∥z∥ 2 M = z ⊤ Mz represents the weighted Euclidean norm, with z as a column vector and M a square matrix matching the dimension of z.The parameters R, S prioritize action magnitude and variation, respectively, while λ k is a time-varying factor that encourages faster task completion.For example, the parameters λ k = 1 and R = S = 0 define the minimum time optimal control problem [20].</p>
<p>Since RL maximizes a reward rather than minimizing a cost, the function in ( 12) is converted into a reward as follows,
r k = − ∥u k ∥ 2 R + ∥u k − u k−1 ∥ 2 S + λ k .(13)
Maximizing (13) over time corresponds to minimizing the cost in (12).However, to further encourage the RL agent to reach the goal state, the time penalty is removed once the robot is within a specified proximity to the goal.Thus, we define:
λ k = 0, if e p,k &lt; ϵ p and e θ,k &lt; ϵ θ 1, otherwise(14)
5) Neural Network Architecture: In recent years, several algorithms, like DDPG [21], SAC [4], and TD3 [5], have been created for continuous state and action spaces.Considering the pool of deep RL algorithms, we chose the Soft Actor-Critic Algorithm (SAC) [4] due to being easy to find hyperparameters that lead to good policies.The stochastic nature of SAC also helps with environment exploration, which reduces the number of parameters to tune.The RL training parameters and network architectures used are listed in Table I, and a visual representation of the network architecture for the actor-critic framework is presented in Fig. 5.In SAC, two networks are used: the actor and the critic.The actor network is the policy itself, taking the observation (state and the goal) as input and outputting action probabilities in terms of mean and deviation for each element.The critic network takes both the observation and a sampled action as input, and outputs estimates of the expected return of that action given the current policy (Q-value).This Q-value guides the actor's updates, helping SAC refine the policy by assessing the value of actions to improve the long-term rewards.</p>
<p>C. Training and Verification Process</p>
<p>The core simulation training stage was performed by using Gymnasium [22], which is a library that simplifies the creation of simulators tailored for RL applications, and the SAC neural network was sourced from the stable-baselines 3 library [23].The simulation parameters are shown in Table I.To accelerate learning, another concept called Hindsight Experience Replay (HER) [24] was also applied.HER is a technique that generates additional training data from failed episodes by redefining the goal to a point the robot reached and adjusting rewards accordingly.This approach enables the agent to learn not only from successes but also from failures.</p>
<p>Two additional training strategies were applied in this study: curriculum learning and randomization.Curriculum learning was applied to the goal tolerances ϵ p and ϵ θ , starting with the tolerances covering 80 % of the training region.When the robot reached a 95 % success rate, the tolerances were reduced by 20 % until it reached ϵ p = 0.05 m and ϵ θ = 1 • .Looking at the pipeline in Fig. 2, this can be viewed as following the feedback loop in the core simulator until the RL agent achieves the desired tolerances.Randomization was applied to the robot's state, introducing noise in the robot's position and orientation to encourage the RL agent to learn how to control the robot under uncertain conditions, reducing the reality gap.</p>
<p>Once the policy achieved a 100 % success rate in the core simulator stage with the final tolerance levels, it was transferred to the high-fidelity simulation stage using the Gazebo simulator, which provides a physics engine, realistic sensor emulation, and a communication layer with ROS.Because the problem did not involve interacting with physical objects, there was no necessity to continue training the policy on the Gazebo simulator.However, running the policy on Gazebo was still crucial, as integration with ROS enabled the use of the same code, and localization and navigation architecture as on the real robot, allowing the algorithm to be tested under conditions that closely replicate the real-world environment.</p>
<p>After testing the RL agent in Gazebo, the policy was deemed ready for deployment on the real robot.Initial deployments yielded positive results, though some discrepancies were observed compared to the Gazebo simulation.Specifically, the RL agent struggled to stop the robot in certain scenarios, such as the goal at x = 0 m and y = 2 m in Fig. 6c., leading to overshooting and oscillations around the goal position.This was caused by the robot's latency in stopping due to inertia and a minimum action duration not accounted for in the core Fig. 6 shows the robot trajectories across all three stages.The Gymnasium simulation (Fig. 6a) shows accurate trajectories, as anticipated.Note that trajectories requiring lateral movement include some forward/backward motion due to the RL agent being optimized for faster executions rather than shorter trajectories.In the Gazebo runs (Fig. 6b), the tolerances were adjusted to match those used in the actual robot stage, and minor variations in the robot's starting positions were introduced to create a more realistic setup.Despite these differences, the trajectories were similar to the Gymnasium stage.Finally, in the real robot execution (Fig. 6c), the trajectories closely matched the Gazebo results, except for a delay in the robot stopping upon reaching the goal regions.</p>
<p>V. APPLICATION IN SURVEILLANCE</p>
<p>The trained RL agent was tested on the Boston Dynamics Spot robot in a surveillance task conducted inside one of Queen's University buildings.The robot was tasked with safely visiting multiple waypoints within the building using the navigation system shown in Fig. 7.</p>
<p>A. Description of the Navigation System</p>
<p>The navigation system, illustrated in Fig. 7 is comprised of the following sub-modules:</p>
<p>1) Inputs: The inputs of the navigation system include the task, represented as a sequence of waypoints (w 1 , w 2 , • • • , w N ), the map of the building, obtained using SLAM with a resolution of 0.05 m, and the robot sensors comprised of the internal odometry and distance measurements using a VLP-16 Velodyne LiDAR.</p>
<p>2) Localization: The localization system consisted of the Adaptive Monte Carlo Localization (AMCL) library available for ROS that implements an adaptive particle filter to estimate the robot's pose.3) Planner: The state lattice planner available in the Nav2 ROS library was used to generate the paths.This algorithm uses motion primitives based on the robot's kinematics to find a collision-free path between two configurations.This planner is capable of generating motion primitives for both car-like and omnidirectional robots.However, the omnidirectional version produced paths that required the robot to move sideways, which is the slowest direction, for the majority of the task execution.In contrast, the car-like version generated more suitable paths, wherein the robot faced forward for most of the task.A small steering radius of 0.1 m was chosen to allow the robot to turn in place.The state lattice planner outputs paths with the same resolution as the map, 0.05 m.To account for the RL agent tolerance of 0.3 m, the path was under-sampled to contain 1 m segments.</p>
<p>4) Controller: The RL agent controller, trained through the pipeline, followed the planned path by sequentially controlling the robot toward each point in the path.This was achieved by representing the path as a sequence of sub-goals.Once the robot reaches a sub-goal within the specified tolerances ϵ p and ϵ θ , the robot is sent to the next sub-goal in the path.</p>
<p>B. Application to Surveillance</p>
<p>Fig. 8 shows Spot's path during the surveillance task.The robot was positioned near the centre of the space, indicated by the green circle, with waypoints selected near the extremities of each side of the building, indicated as red circles.</p>
<p>During the execution, the robot's speeds (a x , a y , a θ ) were limited to 1 m/s.This limit was added as a security measure to keep the robot within the speed limits of the identified model.The total length of the path executed was 197 m, and the robot completed the task in 201 s.This results in an average speed of 0.98 m/s.A slightly lower average speed is expected since the robot has to change directions at each waypoint.</p>
<p>While the robot completed the task with the expected tolerances and speeds, some oscillations were observed in the path.These oscillations are particularly noticeable near the first and third waypoints.Although the exact causes are still unclear, they were not present in Fig. 6c experiment.Therefore, these oscillations probably appeared due to the integration of the RL agent and the navigation system.One possible reason for some of the oscillations is a sudden change in orientation in the planned path.Additionally, the RL agent may be overcompensating due to larger variations in the estimated pose than what it was trained to handle.Further experiments are needed to understand better these oscillations and how to mitigate them.</p>
<p>VI. CONCLUSION</p>
<p>This paper introduces a Reinforcement Learning (RL) pipeline that incorporates multiple stages of training, from simulation to real-world robotic applications.Our pipeline includes two levels of simulation complexity for sequential training of the RL agent, along with iterative feedback paths that support an iterative refinement of the policy.By incorporating our pipeline and applying techniques such as system identification, domain randomization, and curriculum learning, it is possible to reduce the reality gap and facilitate safe and efficient policy training in robotics.We also present a case study using the Boston Dynamics Spot robot in a surveillance application, demonstrating the practical value of our pipeline through a successful deployment of an RL agent.</p>
<p>Fig. 1 :
1
Fig. 1: Simplified diagram describing the components of the proposed RL pipeline.Each component is optional, and the combination of the stages depends on the problem's complexity.</p>
<p>Fig. 2 :Fig. 3 :
23
Fig.2:The proposed training pipeline involves three stages with increasing levels of complexity from left to right.Each stage is optional and can be revisited with different until they pass a predefined performance criteria.Each stage receives as input a policy and outputs a modified policy, allowing incremental improvement until the final policy is achieved.</p>
<p>Fig. 5 :
5
Fig. 5: Actor-critic network architecture</p>
<p>Fig. 6 :
6
Fig. 6: Execution of the policy at multiple goals with θ g = 0 with (a) showing the core simulation trajectories, (b) the high-fidelity simulation trajectories, and (c) the trajectories executed by the real robot.</p>
<p>Fig. 7 :
7
Fig. 7: The navigation system used for the surveillance task.</p>
<p>Fig. 8 :
8
Fig. 8: (a) Executed trajectory (in red) using the trained RL agent on the test site map represented as an occupancy grid (white is free space, black is occupied, and grey is unknown) and (b) photo of the robot near the first waypoint.</p>
<p>TABLE I :
I
Simulation, RL, and neural network parameters.
ParameterValueSimulation frequency30 HzPolicy frequency10 HzObservation standard deviation0.01Rdiag(0.0, 0.8, 0.8)Sdiag(0.2, 0.2, 0.2)Range of ax[−0.8, 1.1] m/sRange of ay[−0.7, 0.7] m/sRange of a θ[−1.1, 1.1] rad/s[r min , rmax][−2, 2]Batch size512τ0.0045Discount factor0.999Learning rate2 × 10 −4Buffer size1 × 10 6Number of training steps3 × 10 5Actor neurons16Actor hidden layers2Critic neurons128Critic hidden layers2Hidden layer activation functionReLuOutput layer functionlinear</p>
<p>Steps toward artificial intelligence. M Minsky, Proceedings of the IRE. the IRE196149</p>
<p>Reinforcement Learning: An Introduction. R S Sutton, A G Barto, 2018A Bradford Book</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. S Levine, A Kumar, G Tucker, J Fu, arXiv:2005.016432020arXiv preprint</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International Conference on Machine Learning. PMLR2018</p>
<p>Addressing function approximation error in actor-critic methods. S Fujimoto, H Hoof, D Meger, International Conference on Machine Learning. PMLR2018</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, Advances in Artificial Life: Third European Conference on Artificial Life Granada. SpainSpringerJune 4-6, 1995 Proceedings 3. 1995</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>A deep reinforcement learning framework for control of robotic manipulators in simulated environments. C Calderon-Cordova, R Sarango, D Castillo, V Lakshminarayanan, IEEE Access. 2024</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 2020 IEEE symposium series on computational intelligence (SSCI). IEEE2020</p>
<p>A survey of sim-to-real transfer techniques applied to reinforcement learning for bioinspired robots. W Zhu, X Guo, D Owaki, K Kutsuzawa, M Hayashibe, IEEE Transactions on Neural Networks and Learning Systems. 3472021</p>
<p>Domain randomization for sim2real transfer of automatically generated grasping datasets. J Huber, F Hélénon, H Watrelot, F B Amar, S Doncieux, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.071132019arXiv preprint</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, S Levine, R Hadsell, K Bousmalis, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>A simto-real pipeline for deep reinforcement learning for autonomous robot navigation in cluttered rough terrain. H Hu, K Zhang, A H Tan, M Ruan, C Agia, G Nejat, IEEE Robotics and Automation Letters. 642021</p>
<p>Data-driven science and engineering: Machine learning, dynamical systems, and control. S L Brunton, J N Kutz, 2022Cambridge University Press</p>
<p>Multiple mobile robot task and motion planning: A survey. L Antonyshyn, J Silveira, S Givigi, J Marshall, ACM Computing Surveys. 55102023</p>
<p>Reaching the limit in autonomous racing: Optimal control versus reinforcement learning. Y Song, A Romero, M Müller, V Koltun, D Scaramuzza, Science Robotics. 88214622023</p>
<p>Minimum time learning model predictive control. U Rosolia, F Borrelli, International Journal of Robust and Nonlinear Control. 31182021</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. 2015arXiv preprint</p>
<p>Gymnasium: A standard interface for reinforcement learning environments. M Towers, A Kwiatkowski, J Terry, J U Balis, G De Cola, T Deleu, M Goulão, A Kallinteris, M Krimmel, A Kg, arXiv:2407.170322024arXiv preprint</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. A Raffin, A Hill, A Gleave, A Kanervisto, M Ernestus, N Dormann, Journal of Machine Learning Research. 222682021</p>
<p>Hindsight experience replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O Pieter, W Abbeel, Zaremba, Advances in neural information processing systems. 201730</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1312 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1312</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1312</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-232428205</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2103.16772v2.pdf" target="_blank">Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies</a></p>
                <p><strong>Paper Abstract:</strong> We present CREST, an approach for causal reasoning in simulation to learn the relevant state space for a robot manipulation policy. Our approach conducts interventions using internal models, which are simulations with approximate dynamics and simplified assumptions. These interventions elicit the structure between the state and action spaces, enabling construction of neural network policies with only relevant states as input. These policies are pretrained using the internal model with domain randomization over the relevant states. The policy network weights are then transferred to the target domain (e.g., the real world) for fine tuning. We perform extensive policy transfer experiments in simulation for two representative manipulation tasks: block stacking and crate opening. Our policies are shown to be more robust to domain shifts, more sample efficient to learn, and scale to more complex settings with larger state spaces. We also show improved zero-shot sim-to-real transfer of our policies for the block stacking task.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1312.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1312.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Internal model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Internal approximate simulation (internal model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cheap, approximate simulator used by CREST for causal interventions and pretraining; implements simplified/approximate task dynamics (sometimes purely kinematic) and is intentionally lower-fidelity (faster) than the target simulator to enable many intervention samples and structure discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Internal model (approximate simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A task-specific, approximate simulator (referred to as the internal model) used to (1) run causal interventions on context variables to discover relevant state subsets and parameter mappings, and (2) pretrain neural policy networks with domain randomization over the discovered relevant variables. Implementations vary per task: for crate opening the internal model is kinematic; for block stacking it models approximate dynamics sufficient to capture relationships between object contexts and required controller parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-to-medium fidelity (approximate dynamics; in one task kinematic-only)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Approximate dynamics and simplified assumptions; may omit detailed contact dynamics, accurate frictional/contact modeling, and full impedance dynamics; for crate opening the internal model is explicitly kinematic (no realistic dynamic interaction), while for block stacking it captures the dominant geometric relations but may approximate friction/force effects. The internal model is much faster to sample (orders-of-magnitude faster) than the target high-fidelity simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CREST-trained policies (MLP baseline, RMLP, PMLP, PMLP-R)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural-network parameterized policies (MLPs) trained with PPO for upper policy π(θ|·) producing controller parameters θ; architectures include full MLP baseline, Reduced MLP (RMLP), Partitioned MLP (PMLP/PMLP-R).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Causal reasoning & structure learning for robot manipulation policies (discovering which context variables are causally relevant to controller parameters) and pretraining policies for block stacking and crate opening tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Task- and architecture-specific IM pretraining updates reported in paper. Examples: block-stacking IM pretraining (k-samples): MLP 237.5k ±121.6k, RMLP 137.6k ±70.5k, PMLP 139.2k ±71.3k (Table III). Crate-opening IM pretraining (k-samples): MLP 45.0k ±3.16k, RMLP 32.4k ±3.67k, PMLP 48.2k ±13.33k, PMLP-R 51.3k ±10.82k (Table VI).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Higher-fidelity target simulator (NVIDIA Isaac Gym) and real robot (Franka Emika Panda) for block-stacking sim-to-real evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Pretraining in the internal model reduced required target updates and improved zero-shot / faster fine-tuning when the modeling gap was moderate. Examples: Block stacking (no color shift) target updates after transfer (k-samples): MLP 1.6k ±1.5k, RMLP 0.5k ±0.9k, PMLP 0.0k (Table III). Zero-shot sim-to-real (real robot) block-stacking zero-shot successes: MLP 3/10, RMLP 9/10, PMLP 10/10; mean reward: MLP -0.033 ±0.012, RMLP -0.018 ±0.007, PMLP -0.014 ±0.004 (Table IV). For crate opening, transfer benefits were smaller due to larger modeling gap: transferred target updates (k-samples) MLP 12.2k ±1.72k, RMLP 7.0k ±1.18k, PMLP 14.0k ±3.74k (Table VI).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper explicitly compares pretraining in a lower-fidelity internal model vs. fine-tuning/transfer to a higher-fidelity target. When the internal model approximates the target well (block stacking), pretraining yields strong zero-shot transfer and large reductions in target sample complexity; when the internal model is kinematic and dynamics differences are large (crate opening), zero-shot transfer degrades and more fine-tuning in target is required. Reduced-input architectures (RMLP) are more robust to dynamics gap than full MLP, and partitioned architectures sometimes underperform when modeling gap or expressivity mismatch exists.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper argues the internal model need not be exact: it only needs to approximate relationships relevant for detecting which context variables are causally important and provide a task-specific initialization. Explicit statement: 'the internal model provides both an estimate of the policy structure and a task-specific initialization; it is not necessarily expected to transfer zero-shot.' They assume the internal model 'approximates the task sufficiently well' and includes all varying context variables, but do not quantify a strict minimal set of physics to simulate.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Crate opening: using a kinematic internal model (no dynamics) created a substantial sim-to-target modeling gap that reduced zero-shot transfer and made pretraining less beneficial; increased crate stiffness (larger dynamics mismatch) required substantially more target fine-tuning (Table VII). Partitioned networks (PMLP) sometimes failed to benefit from transfer when expressivity was too small relative to modeling mismatch.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1312.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1312.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Isaac Gym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NVIDIA Isaac Gym (high-fidelity robotics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-fidelity physics simulator (NVIDIA Isaac Gym) used as the target domain proxy for the real world; used to evaluate transfer / fine-tuning performance and to emulate realistic dynamics for robot manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>NVIDIA Isaac Gym</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>GPU-accelerated physics simulator for robotics tasks, used here as the target (higher-fidelity) environment to emulate realistic physics, dynamics, contact, and articulated motion for manipulation tasks; used to evaluate transferred policies and to run target-domain fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robot manipulation (articulated rigid-body dynamics, contact)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity physics simulator (higher-fidelity than the internal model), intended to closely approximate real-world dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models realistic rigid-body dynamics, contacts and articulated motion (e.g., crate hinge), impedance controller interactions, and stiffness-dependent contact behavior; used as proxy for real system and exhibits nontrivial dynamics mismatches relative to kinematic internal model. Runs on GPU for faster parallel simulation (paper ran target simulation on NVIDIA DGX-1).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CREST-trained policies (MLP, RMLP, PMLP, PMLP-R) transferred and fine-tuned in Isaac Gym</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same neural policy architectures as above (MLPs trained with PPO) but evaluated and fine-tuned in Isaac Gym which provides realistic dynamics and contact conditions for manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Evaluation / fine-tuning domain for robot manipulation controllers trained with causal structure from the internal model; tasks: block stacking and crate opening under distractors and dynamics variations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Target-domain training and fine-tuning statistics reported. Examples: Crate-opening direct training (no transfer) target updates (k-samples): MLP 38.7k ±10.99k, RMLP 16.4k ±4.05k (Table VI). Fine-tuning with dynamics shifts (light → nominal → stiff stiffness): MLP target updates increased from 3.9k to 27.3k; RMLP from 3.2k to 16.4k (Table VII). Block-stacking target updates after transfer (no color shift) for RMLP near-zero (0.5k) and PMLP zero-shot in many trials (Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real robot (Franka Emika Panda) for sim-to-real evaluation (block stacking); also used to evaluate different target dynamics (stiffness) as a proxy for varied real-world dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>When transferring from internal model to Isaac Gym, pretraining reduced required target updates substantially when modeling gap was moderate (e.g., crate opening transfer: MLP 12.2k vs direct 38.7k; RMLP 7.0k vs direct 16.4k). For block stacking, strong zero-shot behavior observed when internal model matched target well (RMLP and PMLP achieved many zero-shot successes; see Table III). When transferring onward from Isaac Gym to a real robot (block stacking), policies pretrained via CREST achieved higher zero-shot real-world success (PMLP 10/10 stacks, RMLP 4/10, MLP 1/10 in stacked counts; zero-shot counts: PMLP 10/10, RMLP 9/10, MLP 3/10; Table IV).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The paper contrasts the lower-fidelity internal model with Isaac Gym target: smaller modeling gaps (block stacking) enabled near-zero-shot transfer and low target sample complexity; larger gaps (crate opening kinematic internal model vs dynamic target) required more fine-tuning and reduced zero-shot success. Increasing target dynamics difference (simulated via increased crate stiffness) increased needed target updates for all networks; RMLP consistently required fewer updates than full MLP and partitioned variants under larger gaps (Table VII).</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Isaac Gym is treated as the higher-fidelity target; authors state the internal model need not match Isaac Gym exactly but must capture causal dependencies. They do not prescribe an exact minimal fidelity but show that missing dynamic effects (internal model kinematic vs Isaac Gym dynamics) degrade transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When the internal model omitted dynamics (kinematic internal model) and the target Isaac Gym had realistic dynamics (crate opening), zero-shot transfer failed and pretraining offered less benefit; partitioned networks underperformed in some stiffness conditions due to expressivity mismatches (Table VI-VII).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization <em>(Rating: 2)</em></li>
                <li>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World <em>(Rating: 2)</em></li>
                <li>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning <em>(Rating: 2)</em></li>
                <li>Sim-to-Real: Learning Agile Locomotion for Quadruped Robots <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1312",
    "paper_id": "paper-232428205",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Internal model",
            "name_full": "Internal approximate simulation (internal model)",
            "brief_description": "Cheap, approximate simulator used by CREST for causal interventions and pretraining; implements simplified/approximate task dynamics (sometimes purely kinematic) and is intentionally lower-fidelity (faster) than the target simulator to enable many intervention samples and structure discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Internal model (approximate simulation)",
            "simulator_description": "A task-specific, approximate simulator (referred to as the internal model) used to (1) run causal interventions on context variables to discover relevant state subsets and parameter mappings, and (2) pretrain neural policy networks with domain randomization over the discovered relevant variables. Implementations vary per task: for crate opening the internal model is kinematic; for block stacking it models approximate dynamics sufficient to capture relationships between object contexts and required controller parameters.",
            "scientific_domain": "mechanics / robot manipulation",
            "fidelity_level": "low-to-medium fidelity (approximate dynamics; in one task kinematic-only)",
            "fidelity_characteristics": "Approximate dynamics and simplified assumptions; may omit detailed contact dynamics, accurate frictional/contact modeling, and full impedance dynamics; for crate opening the internal model is explicitly kinematic (no realistic dynamic interaction), while for block stacking it captures the dominant geometric relations but may approximate friction/force effects. The internal model is much faster to sample (orders-of-magnitude faster) than the target high-fidelity simulator.",
            "model_or_agent_name": "CREST-trained policies (MLP baseline, RMLP, PMLP, PMLP-R)",
            "model_description": "Neural-network parameterized policies (MLPs) trained with PPO for upper policy π(θ|·) producing controller parameters θ; architectures include full MLP baseline, Reduced MLP (RMLP), Partitioned MLP (PMLP/PMLP-R).",
            "reasoning_task": "Causal reasoning & structure learning for robot manipulation policies (discovering which context variables are causally relevant to controller parameters) and pretraining policies for block stacking and crate opening tasks.",
            "training_performance": "Task- and architecture-specific IM pretraining updates reported in paper. Examples: block-stacking IM pretraining (k-samples): MLP 237.5k ±121.6k, RMLP 137.6k ±70.5k, PMLP 139.2k ±71.3k (Table III). Crate-opening IM pretraining (k-samples): MLP 45.0k ±3.16k, RMLP 32.4k ±3.67k, PMLP 48.2k ±13.33k, PMLP-R 51.3k ±10.82k (Table VI).",
            "transfer_target": "Higher-fidelity target simulator (NVIDIA Isaac Gym) and real robot (Franka Emika Panda) for block-stacking sim-to-real evaluation",
            "transfer_performance": "Pretraining in the internal model reduced required target updates and improved zero-shot / faster fine-tuning when the modeling gap was moderate. Examples: Block stacking (no color shift) target updates after transfer (k-samples): MLP 1.6k ±1.5k, RMLP 0.5k ±0.9k, PMLP 0.0k (Table III). Zero-shot sim-to-real (real robot) block-stacking zero-shot successes: MLP 3/10, RMLP 9/10, PMLP 10/10; mean reward: MLP -0.033 ±0.012, RMLP -0.018 ±0.007, PMLP -0.014 ±0.004 (Table IV). For crate opening, transfer benefits were smaller due to larger modeling gap: transferred target updates (k-samples) MLP 12.2k ±1.72k, RMLP 7.0k ±1.18k, PMLP 14.0k ±3.74k (Table VI).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Paper explicitly compares pretraining in a lower-fidelity internal model vs. fine-tuning/transfer to a higher-fidelity target. When the internal model approximates the target well (block stacking), pretraining yields strong zero-shot transfer and large reductions in target sample complexity; when the internal model is kinematic and dynamics differences are large (crate opening), zero-shot transfer degrades and more fine-tuning in target is required. Reduced-input architectures (RMLP) are more robust to dynamics gap than full MLP, and partitioned architectures sometimes underperform when modeling gap or expressivity mismatch exists.",
            "minimal_fidelity_discussion": "The paper argues the internal model need not be exact: it only needs to approximate relationships relevant for detecting which context variables are causally important and provide a task-specific initialization. Explicit statement: 'the internal model provides both an estimate of the policy structure and a task-specific initialization; it is not necessarily expected to transfer zero-shot.' They assume the internal model 'approximates the task sufficiently well' and includes all varying context variables, but do not quantify a strict minimal set of physics to simulate.",
            "failure_cases": "Crate opening: using a kinematic internal model (no dynamics) created a substantial sim-to-target modeling gap that reduced zero-shot transfer and made pretraining less beneficial; increased crate stiffness (larger dynamics mismatch) required substantially more target fine-tuning (Table VII). Partitioned networks (PMLP) sometimes failed to benefit from transfer when expressivity was too small relative to modeling mismatch.",
            "uuid": "e1312.0"
        },
        {
            "name_short": "Isaac Gym",
            "name_full": "NVIDIA Isaac Gym (high-fidelity robotics simulator)",
            "brief_description": "High-fidelity physics simulator (NVIDIA Isaac Gym) used as the target domain proxy for the real world; used to evaluate transfer / fine-tuning performance and to emulate realistic dynamics for robot manipulation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "NVIDIA Isaac Gym",
            "simulator_description": "GPU-accelerated physics simulator for robotics tasks, used here as the target (higher-fidelity) environment to emulate realistic physics, dynamics, contact, and articulated motion for manipulation tasks; used to evaluate transferred policies and to run target-domain fine-tuning experiments.",
            "scientific_domain": "mechanics / robot manipulation (articulated rigid-body dynamics, contact)",
            "fidelity_level": "high-fidelity physics simulator (higher-fidelity than the internal model), intended to closely approximate real-world dynamics",
            "fidelity_characteristics": "Models realistic rigid-body dynamics, contacts and articulated motion (e.g., crate hinge), impedance controller interactions, and stiffness-dependent contact behavior; used as proxy for real system and exhibits nontrivial dynamics mismatches relative to kinematic internal model. Runs on GPU for faster parallel simulation (paper ran target simulation on NVIDIA DGX-1).",
            "model_or_agent_name": "CREST-trained policies (MLP, RMLP, PMLP, PMLP-R) transferred and fine-tuned in Isaac Gym",
            "model_description": "Same neural policy architectures as above (MLPs trained with PPO) but evaluated and fine-tuned in Isaac Gym which provides realistic dynamics and contact conditions for manipulation tasks.",
            "reasoning_task": "Evaluation / fine-tuning domain for robot manipulation controllers trained with causal structure from the internal model; tasks: block stacking and crate opening under distractors and dynamics variations.",
            "training_performance": "Target-domain training and fine-tuning statistics reported. Examples: Crate-opening direct training (no transfer) target updates (k-samples): MLP 38.7k ±10.99k, RMLP 16.4k ±4.05k (Table VI). Fine-tuning with dynamics shifts (light → nominal → stiff stiffness): MLP target updates increased from 3.9k to 27.3k; RMLP from 3.2k to 16.4k (Table VII). Block-stacking target updates after transfer (no color shift) for RMLP near-zero (0.5k) and PMLP zero-shot in many trials (Table III).",
            "transfer_target": "Real robot (Franka Emika Panda) for sim-to-real evaluation (block stacking); also used to evaluate different target dynamics (stiffness) as a proxy for varied real-world dynamics.",
            "transfer_performance": "When transferring from internal model to Isaac Gym, pretraining reduced required target updates substantially when modeling gap was moderate (e.g., crate opening transfer: MLP 12.2k vs direct 38.7k; RMLP 7.0k vs direct 16.4k). For block stacking, strong zero-shot behavior observed when internal model matched target well (RMLP and PMLP achieved many zero-shot successes; see Table III). When transferring onward from Isaac Gym to a real robot (block stacking), policies pretrained via CREST achieved higher zero-shot real-world success (PMLP 10/10 stacks, RMLP 4/10, MLP 1/10 in stacked counts; zero-shot counts: PMLP 10/10, RMLP 9/10, MLP 3/10; Table IV).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "The paper contrasts the lower-fidelity internal model with Isaac Gym target: smaller modeling gaps (block stacking) enabled near-zero-shot transfer and low target sample complexity; larger gaps (crate opening kinematic internal model vs dynamic target) required more fine-tuning and reduced zero-shot success. Increasing target dynamics difference (simulated via increased crate stiffness) increased needed target updates for all networks; RMLP consistently required fewer updates than full MLP and partitioned variants under larger gaps (Table VII).",
            "minimal_fidelity_discussion": "Isaac Gym is treated as the higher-fidelity target; authors state the internal model need not match Isaac Gym exactly but must capture causal dependencies. They do not prescribe an exact minimal fidelity but show that missing dynamic effects (internal model kinematic vs Isaac Gym dynamics) degrade transfer.",
            "failure_cases": "When the internal model omitted dynamics (kinematic internal model) and the target Isaac Gym had realistic dynamics (crate opening), zero-shot transfer failed and pretraining offered less benefit; partitioned networks underperformed in some stiffness conditions due to expressivity mismatches (Table VI-VII).",
            "uuid": "e1312.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "gpuaccelerated_robotic_simulation_for_distributed_reinforcement_learning"
        },
        {
            "paper_title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
            "rating": 2,
            "sanitized_title": "causalworld_a_robotic_manipulation_benchmark_for_causal_structure_and_transfer_learning"
        },
        {
            "paper_title": "Sim-to-Real: Learning Agile Locomotion for Quadruped Robots",
            "rating": 1,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        }
    ],
    "cost": 0.013665999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies</p>
<p>Tabitha Edith Lee 
Jialiang 
Alan Zhao 
Amrita S Sawhney 
Siddharth Girdhar 
Oliver Kroemer 
Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies</p>
<p>We present CREST, an approach for causal reasoning in simulation to learn the relevant state space for a robot manipulation policy. Our approach conducts interventions using internal models, which are simulations with approximate dynamics and simplified assumptions. These interventions elicit the structure between the state and action spaces, enabling construction of neural network policies with only relevant states as input. These policies are pretrained using the internal model with domain randomization over the relevant states. The policy network weights are then transferred to the target domain (e.g., the real world) for fine tuning. We perform extensive policy transfer experiments in simulation for two representative manipulation tasks: block stacking and crate opening. Our policies are shown to be more robust to domain shifts, more sample efficient to learn, and scale to more complex settings with larger state spaces. We also show improved zero-shot simto-real transfer of our policies for the block stacking task.</p>
<p>I. INTRODUCTION</p>
<p>Real-world environments, such as homes and restaurants, often contain a large number of objects that a robot can manipulate. However, usually only a small set of objects and state variables are actually relevant for performing a given manipulation task. The capability of reasoning about what aspects of the state space are relevant for the task would lead to more efficient learning and greater skill versatility.</p>
<p>Current approaches to learning versatile manipulation skills often utilize simulation-to-real (sim-to-real) transfer learning [1]- [4], wherein the skill is learned in a simulation and then deployed and fine-tuned (if feasible) on the real robot. Sim-to-real learning is often combined with domain randomization (DR) [5], which involves training the skill on a wide range of task instances in simulation such that the resulting skill is more robust and generalizes across task variations. However, for scenes with distractor objects, the policy still takes the irrelevant state features as inputs. Larger domain shifts in the irrelevant features can therefore still be detrimental to the performance of the skill policy. Rather than relying only on DR, the robot can use modelbased reasoning to identify the "structure" of a policy -the interplay between relevant state inputs and control outputs.</p>
<p>In this paper, we propose using causal reasoning to improve sim-to-real transfer through conducting interventions in simulation to determine which state variables are relevant for the successful execution of a task using a given controller. We refer to the resulting algorithm as CREST: Causal Reasoning for Efficient Structure Transfer. The relevant state variables from CREST are used to construct policies that encode the causal structure of the manipulation task. These policies are initially trained in simulation using domain randomization over only the states that have explicitly been determined as relevant. Moreover, policies that only use relevant state variables require significantly fewer parameters and, by construction, are robust to distribution shifts in irrelevant state spaces. In this manner, our approach produces lightweight policies that are designed for efficient online adaptation to unforeseen distribution shifts that may occur when bridging the sim-to-real gap. This contrasts with existing sim-to-real approaches that train large policies over enormous state spaces, where the costs for achieving robust zero-shot transfer may be intractable.</p>
<p>Our proposed approach was successfully evaluated on block stacking and crate opening tasks. Although our method is intended for sim-to-real transfer, we primarily conduct our experiments by transferring to task simulations in NVIDIA Isaac Gym [6], a high-fidelity physics simulator, as a proxy for real systems. This is necessary to experimentally evaluate distributions shifts that would be intractable to evaluate (but nonetheless feasible) in practical manipulation scenarios. Additionally, we validate our approach for zero-shot, simto-real performance for the block stacking task.</p>
<p>The contributions of our work are as follows.</p>
<p>• We propose CREST, an algorithm that uses causal reasoning in simulation to identify the relevant input state variables for generalizing manipulation policies. • We propose two neural network architectures that are constructed using the causal information from CREST. • We conduct rigorous transfer learning experiments to demonstrate these policies generalize across task scenarios, scale in relevant state complexity, and are robust to distribution shifts. • We propose CREST as one approach to a broader methodology of structure-based transfer learning from simulation as a new paradigm for sim-to-real robot learning, i.e., structural sim-to-real.</p>
<p>II. RELATED WORKS</p>
<p>Our work relates to research areas in robotics and machine learning for structure-based learning, causality, attention, and sim-to-real transfer of higher-level policies.</p>
<p>Structure-based learning, causality, and attention. Causal reasoning for structure, transfer, and reinforcement learning is an emerging area of research [7], [8], having been demonstrated for transferring multi-armed bandits policies [9], examining distribution shifts via imitation learn-ing [10], modeling physical interactions from videos [11], and clustering causal factors [12]. Causality reasons about the data generation process with respect to an underlying model [13], which our CREST policies encode. The motivation of our approach to transfer learned causal structure is similar to that described by [14]. In our work, we represent this structure through the policy inputs, and we demonstrate the approach achieves sim-to-real transfer.</p>
<p>Our view of policy structure can be seen as an explicitly encoded form of state space attention, achieved via construction of policies using only relevant inputs. We are primarily concerned with object feature states, which enable significantly smaller neural networks to be constructed under our assumptions. As a comparison, [15] learns implicit attention to task-relevant objects to generalize manipulation skills. This approach requires a few example trajectories to be provided and uses a vision-based state representation, making explicit reasoning about states more challenging.</p>
<p>Similar to [16], our approach can also generalize policies to unforeseen dynamic distribution shifts; ours does so primarily through more efficient fine-tuning.</p>
<p>Our work is also similar in spirit to the work of Nouri and Littman [17] in terms of achieving dimensionality reduction for reinforcement learning. Whereas our work seeks to reduce the dimensionality for the state's possible influence on the task policy under different contexts, they instead demonstrate dimensionality reduction in the action space. Kolter and Ng [18] as well as Parr et al. [19] approximate the value function by learning the relevant basis functions.</p>
<p>Sim-to-real and higher-level policies. Our work is a form of simulation-to-reality transfer of controllers [1], [5], [20], [21]. Unlike in the typical sim-to-real paradigm, our policies are not required to transfer zero-shot. Rather, the dimensionality reduction afforded by transferring the relevant state space with CREST enables more efficient online adaptation. Unlike [22], we do not transfer any target samples back to the internal model. Similar to the problem settings of [23]- [25], our agent predicts the best parameters for a controller.</p>
<p>III. PROBLEM FORMULATION</p>
<p>We formulate our problem as a multi-task reinforcement learning problem, wherein a policy π is learned to complete a series of tasks T i . Each task is modeled as a Markov decision process (MDP). The state space S and action space A are the same across tasks. However, each task T i defines a separate initial state s 0 (c i ), transition function p(s |s, a, c i ), and reward function r(a, s, c i ) that are parameterized by the task's context variables c i ∈ C. We assume that the robot has an internal model p int (s |s, a, c i ) that approximates the transition function of the target task domain p(s |s, a, c i ).The context variables capture object parameter variations, such as shapes, appearances, and initial states. We assume that the context variables are always set such that the task is feasible.</p>
<p>To solve the tasks, the robot learns a policy π(a|s, c) that is decomposed into two parts [26]: an upper policy π(θ|c) and a lower control policy π θ (a|s), where θ ∈ R d are the controller parameters. Transferring the parameters from the internal model to the target domain may require fine-tuning. At the start of each task, the upper policy, which is responsible for generalizing between different task contexts, selects a set of parameters for the control policy to use throughout the task execution. We use multilayer perceptron (MLP) neural networks for the upper policy. In principle, the control policies can take on a variety of parameterized forms, such as motor primitives, planners, waypoint trajectories, or linear feedback controllers, depending on the task. We assume a control policy with known preconditions is available.</p>
<p>Of the context variables c that describe the object variations in the scene, only a subset may be relevant for the policy. We refer to this subset of relevant variables as τ ⊆ c, such that the robot can learn a policy π(θ|τ ) to complete the tasks. Our goal is to determine τ through causal reasoning with the internal model, yielding policies with fewer input variables as compared to naively using all of c.</p>
<p>The set of controller parameters can be divided into individual parameters [θ] j ∀j ∈ 1, ..., d, where [θ] j indicates the jth element of vector θ. Each of these parameters may rely on a different set of relevant variables. We can thus further divide the problem into determining a set of context variables τ j for each policy parameter [θ] j , such that the robot can learn a partitioned task policy π([θ] j |τ j )∀j ∈ 1, ..., d.</p>
<p>IV. CAUSAL STRUCTURE LEARNING</p>
<p>CREST uses an internal model of the task to learn the relevant context τ and parameter-specific mappings τ j that define the structure of the upper policy (c.f, Sec. V-A).</p>
<p>A. Internal Model for Causal Reasoning</p>
<p>Our approach assumes an internal model, an approximate simulation of the task, is available. Analogous to mental models and approximate physics models [27], [28], the internal model facilitates reasoning about the effects of different context variations ∆c on completing the task with policy parameters θ. Varying the relevant context parameters τ will affect the execution and outcome of the task in the internal model while irrelevant ones will not.</p>
<p>Given its approximate nature, the solution obtained in the internal model is not necessarily expected to transfer zero-shot to reality. Instead, the internal model provides both an estimate of the policy structure and a task-specific initialization via network pretraining. Intuitively, it is easier to reason about which variables are important for a model rather than exactly characterizing the exact model itself. For example, the internal model can capture that the weight of an object affects the required pushing force, but the exact details of frictional interaction may be approximated for the purposes of pretraining the policy. We assume that the internal model approximates the task sufficiently well and includes all context variables c that vary in the target domain. Given the existing challenges in causal representation learning [29], we additionally assume the representation of c is amenable for determining the underlying model for θ via causal interventions [13].</p>
<p>B. Causal Reasoning to Determine Relevant Contexts</p>
<p>At its core, CREST uses simulation-based causal reasoning to determine the relevant context variables for the policy input. This process is divided into two phases: 1) determining the overall set of relevant variables τ , and 2) determining the relevant variables τ j for specific policy parameters [θ] j .</p>
<p>Causal interventions to determine relevant variable set.</p>
<p>In this phase, the relevance of a state variable is determined by posing the following question: "If a context variable were different, would the same policy execution still complete the task successfully?" To answer this, we first uniformly sample a context c i ∼ p(c) and solve the resulting task in simulation to acquire the corresponding policy parameters θ i . In practice, the policy parameters are optimized using Relative Entropy Policy Search [30]. Importantly, the policy is solved for only this specific context c i (not the general policy).</p>
<p>Given the solved task, we conduct interventions ∆c to determine if the policy parameters θ i remain valid for the new context c i + ∆c. Interventions ∆c are conducted to only alter one context variable at a time, i.e. ||∆c|| 0 = 1. If the policy subsequently failed, the intervened variable is considered relevant and thus appended to τ . If the policy succeeded, despite the intervention, then the variable is considered irrelevant and is not required for the general policy. The resulting relevant variable set τ is sufficient for constructing Reduced MLP policies (c.f., Sec. V-A).</p>
<p>Causal interventions to determine individual policy mappings. Reducing the set of state variables from the full set c to the relevant set τ may greatly reduce the size of the policy input. However, for some problems, each of the policy parameters may only depend on a subset of τ . Therefore, in the next phase, the individual mappings from the relevant state variables to the controller parameters are determined by posing the question: "Does altering this relevant context variable require this policy parameter to be changed?"</p>
<p>We begin from the previous phase, where c i with solution θ i is available with interventions ∆c. In this phase, interventions are only applied to relevant context variables τ .</p>
<p>For each new context c i + ∆c, the task is solved to obtain the resulting policy parameters θ i + ∆θ, starting the optimization from the original parameters θ i . The optimization will often alter all of the policy parameters, i.e., all elements of ∆θ are non-zero, and the magnitude of their changes are not reliable estimators of their importance. Instead, a solution that minimizes the number of non-zero changes, i.e., min ||∆θ|| 0 , is obtained using search. This search involves setting subsets of elements in ∆θ to zero and evaluating if the resulting policy still solves the task with context c i + ∆c. In our experiments, we used a breadth-first search to find a solution with a minimal set of parameter changes. Once a subset of parameter changes has been found, the context variable intervened on in ∆c is added to the sets τ j of relevant inputs for the policy parameters with nonzero elements in the final ∆θ. The output of this phase (the parameter-specific variables τ j ) can then be used to learn Partitioned MLP policies (c.f., Sec. V-A). I: CREST evaluation for a toy environment on an aggregate ("Agg.") and mapping-specific ("Map.") basis. Accuracy ("Acc.") is whether all relevant states were detected. False positives ("F.P.") are states that were incorrect detected as relevant. 100 trials are used. </p>
<p>C. CREST Evaluation</p>
<p>Although we primarily use CREST for manipulation policies, we quantify the accuracy of CREST in a limited, application-agnostic manner. Table I shows the results of CREST on an environment that replicates the causal structure of hierarchical manipulation policies. This environment is designed so that, given a set of ground truth mappings between context variables and policy parameters, a controller with randomized structure is generated for the agent to "manipulate" the environment to a goal location determined by the causal structure of the problem. These mappings were either linear or (weakly) nonlinear.</p>
<p>Under the assumptions of our controller and the context c, CREST excels at determining whether a variable is relevant. This is expected by construction of the underlying mathematics of this environment and represents an expected upper bound. We choose action space dimension of 8 and 20 (larger than our transfer learning experiments, c.f., Sec. VI) and select the state space accordingly to permit calculation of ground truth for testing. We also introduce action noise to test the robustness to uncertainty from interventions, leading to more variables detected. Relatively higher false positive rates arise in higher dimensions, but the overall reduction to the relevant set of variables is nonetheless significant.</p>
<p>V. POLICY LEARNING AND TRANSFER</p>
<p>Given the relevant context from CREST, the structure and transfer learning pipeline of our work begins through 1) constructing the appropriate policy; 2) pretraining using the internal model; and 3) fine-tuning in the target setting.</p>
<p>A. Policy Architectures</p>
<p>Given the full context c, the reduced context τ , and the parameter-specific contexts τ j , the three MLP-based network architectures shown in Fig. 1 are constructed and trained using actor-critic approaches [31].</p>
<p>The baseline π(θ|c) uses a standard MLP network. The approach π(θ|τ ) uses a Reduced MLP (RMLP): an MLP network where the inputs are reduced to only the relevant context τ . The approach π([θ] j |τ j ) has independent sets of fully connected layers for each policy parameter θ j , but with potentially overlapping inputs depending on τ j . This Partitioned MLP (PMLP) network represents the structural causal model [13] for each θ j with τ j as parent variables.</p>
<p>To provide a fair comparison, we choose the weights for the PMLP according to heuristics and multiply the number of hidden units by the size of the action space to size the RMLP and MLP. We originally sized the PMLP network according to [32] to provide theoretical guarantees regarding function approximation for one-element outputs (i.e., each θ j ), but the resulting network size for the baseline MLP was intractable to train. All neural network weights are randomly initialized per orthogonal initialization [33] using √ 2 and 0.01 for the scale terms of the hidden layers and output layers, respectively. All networks use tanh activations.</p>
<p>B. Network Training and Transfer</p>
<p>For both the internal model and target domain, we train our policies using PPO [34] with Stable Baselines [35]. First, each network is pretrained with the internal model until the task family is solved. Then, we transfer the network weights to the target domain and evaluate the policy to determine whether the policy transfers zero-shot. Otherwise, fine-tuning is performed. Although freezing network layers has been explored for fine-tuning control policies [36], we permit the entire network to adapt because of the approximate nature of pretraining with the internal model. The learned policy is considered to have solved the task family if it successfully achieves a predefined reward threshold on 50 validation tasks; this evaluation occurs after each policy update.</p>
<p>VI. EXPERIMENTAL RESULTS</p>
<p>We evaluate how CREST can construct policies with greater (target) sample-efficiency and robustness for the robot manipulation tasks of block stacking and crate opening (Fig. 2). Our experiments for each task follow the structure and transfer learning pipeline motivating our approach: 1) use CREST to determine the causal structure of the task; 2) construct and pretrain policies with this structure; and 3) transfer and fine-tune these policies in the target domain. The target domains include manipulation tasks in NVIDIA Isaac Gym, enabling rigorous investigation of representative distribution shifts that may occur when deploying sim-to-real policies. For the block stacking task, we additionally leverage a real robot system to assess sim-to-real transfer.</p>
<p>Target simulation and training was conducted using a NVIDIA DGX-1. Pretraining was done using a NVIDIA GeForce RTX 2080. Samples from the block stacking and crate opening internal models were 400 and 65 times faster to obtain than target simulation samples, respectively. This is consistent with the concept of the internal model as a cheap, approximate simulator, whereas samples from the target domain are costly and therefore desirable to minimize.</p>
<p>Ten independent trials (from internal model pretraining to target fine-tuning) are conducted for each simulation experiment to provide statistically meaningful results given the variance inherent in model-free learning. Statistics are provided in terms of mean and ±1 standard deviation of policy updates requires to solve the task family. Samples are provided per 1000 ("k-Samples") using a batch size of 512.</p>
<p>The supplementary materials describe further experiment details, such as the setup for the real block stacking target.</p>
<p>A. Block Stacking</p>
<p>The network architectures for the block stacking policies are specified in Table II. Although our policies are nonlinear, note this particular task is linear between τ and θ.</p>
<p>Task representation. In the block stacking task, the con-
text vector c = [ c B0 T , . . . , c B N B −1 T ] T ∈ R 7N B consists of the concatenation of N B individual block contexts. The con- text vector for block b is c B b = [ x w b , z w b , ψ b , h b , C b T ] T ∈ R 7
In the above equations, x w b and z w b are the world xand zpositions of the blocks. Each block orientation is defined by its rotation angle ψ b about the block's vertical axis (y). The y-dimension, or height, of each block is h i . Lastly, the block
color C b = [ R b , G b , B b ]
T is specified via red-green-blue tuple. Note that y w b is not part of the context, as the initial scene always consists of blocks on the workspace plane.</p>
<p>The control policy π(a|s, θ b ) for block stacking is a sequential straight-line skill parameterized by θ b = [ θ ∆x , θ ∆y , θ ∆z ] T ∈ R 3 . This skill specifies waypoints that the robot traverses via impedance control by lifting the source block vertically, moving horizontally, and descending to the desired location. The skill preconditions are that the block is grasped and there are no obstructions to moving the object. The reward function is determined from the source block's position and the goal position upon the target block.</p>
<p>Using the internal model, CREST correctly obtained the relevant context variables as
τ = [x w 0 , x w 1 , h 1 , z w 0 , z w 1 ] T , τ ∆x = [x w 0 , x w 1 ] T , τ ∆y = [h 1 ]
, and τ ∆z = [z w 0 , z w 1 ] T . Nominal transfer for increasing context size. We conduct transfer experiments for N B = {2, 6, 10, 14, 18}, with each N B conducted independently. Our approach scales with the relevant part of the context space (Fig. 3), bounding the sample requirements for the target (as well as the cheaper, internal model). The increasing number of irrelevant dimensions from more blocks are eliminated by CREST prior to conducting domain randomization during pretraining.</p>
<p>For the case of N B = 10, we trained directly in the target domain without transfer and observed similar results as for pretraining, suggesting the internal model accords well with the target domain. This explains why our approaches exhibit good zero-shot behavior over increasing context dimensions, unlike the baseline whose initial performance degrades as the number of irrelevant contexts increase.</p>
<p>Distribution shift in irrelevant contexts. We now evaluate the robustness of the learned block stacking policy to distributions shifts in irrelevant context variables. We conduct  two transfer experiments, wherein the policies are pretrained using only half of the color space. In the first case, the target has the same context distribution as the internal model. In the second case, the target has the opposite color space (without overlap). The experimental results (Table III) elucidate how a seemingly inconsequential variable can degrade policy execution through a distribution shift that the robot is not trained to expect. Our approaches generate policies that are robust to these irrelevant domain shifts by construction; as CREST explicitly identifies this dimension as unimportant and excludes it from target learning. Sim-to-real policy evaluation. Lastly, to validate our approach for sim-to-real transfer, we evaluate the zero-shot policy performance on a real robot system that implements the block stacking task with N B = 10 (Fig. 2c). As shown in Table IV, our policies successfully demonstrate greater zero-shot, sim-to-real transfer as compared to the baseline.</p>
<p>B. Crate Opening</p>
<p>The crate opening experiment is nonlinear between τ and θ, and the internal model, which is kinematic, presents a greater sim-to-real gap than block stacking. Therefore, we also consider a second partitioned network, PMLP-R, with the same number of weights as the RMLP, to elicit possible influence of network expressivity in this domain due to the structural assumptions of the PMLP. The crate experiment primarily focuses on dynamics and context shifts, rather than varying numbers of objects, so unlike in blocks, the networks (Table V) are the same for all experiments. Beyond our two experiments in nominal transfer and dynamics shift, we also conducted a color shift experiment with similar results as with the blocks experiment.</p>
<p>Task representation. For the crate opening task, the context vector is c = [ c C T , c B0 T , . . . , c B9 T ] T ∈ R 80 . The block context is as defined previously. The crate context is
c C = [ p w C T , Φ, x C g , z C g , Θ o , C C T ] T ∈ R 10 , where p w C = [ x w C , y w C , z w C ]
T is the position of the crate coordinate frame with respect to the world frame with vertical angle Φ. The crate is always initially closed (horizontal), but the desired goal angle is specified by Θ o . The robot interacts with the crate via a grasp point specified in the frame of the crate by x C g and z C g which are orthogonal and parallel to the crate rotational axis, respectively. The color of the crate is C C .</p>
<p>The control policy π(a|s, θ a ) is a robot skill that executes circular arcs emerging from the grasp point with the following parameterization: θ a = [ θ p w a T , θ ∆γ , θ ∆φ ] T ∈ R 5 , where θ p w a = [ θ x w a , θ y w a , θ z w a ] T is the sphere position used to calculate the radius from the grasp point. Then, the arc is traced out θ ∆γ in azimuth and θ ∆φ in inclination in polar coordinates from the grasp point. The skill preconditions are that the crate is grasped and unobstructed. To learn this policy, the reward function is calculated from the crate angle error and total kinematic error.</p>
<p>In this formulation, CREST determined the following relevant context variables, which are expected based on rigid <br />
τ = [ x w C , y w C , z w C , Φ, z C g , Θ g ] T τ x w a = [x w C , Φ, z C g ] T , τ y w a = [y w C ], τ z w a = [z w C , Φ, z C g ] T , τ ∆γ = [Φ, Θ g ] T , τ ∆φ = [Φ, Θ g ] T
Nominal transfer. The transfer learning results for the crate opening policy is shown in Table VI. Pretraining the model reduced the number of target updates for the nonpartitioned networks. However, this was not the case for the partitioned networks, regardless of size. This is likely a result of the discrepancy between the internal model and the target domain, which also explains the difference between the policy updates required for pretraining versus training directly in the target. However, the reduction of relevant variables reduces the number of updates required to train directly in the target for both the RMLP and PMLP.</p>
<p>Dynamics distribution shift. Unlike the block stacking problem, the modeling gap between the internal model and target setting is sufficiently large that the trained policies incur a significant performance degradation upon first evaluating in the target domain. We investigated this further by transferring the policies to two target settings with different crate stiffness values. To focus on this dynamics shift, no other shifts (e.g., in context space) were induced.</p>
<p>The results in Table VII suggest that increasing the stiffness is sufficient as a proxy for increasing the modeling difference between the internal model and the target. The optimal parameters for the kinematic case (internal model) are not necessarily the same as the target domain with realistic dynamics of manipulation using impedance control. Therefore, greater modeling differences implies that greater search in policy parameter space is required to converge to parameters that generalize in the target domain.</p>
<p>In all cases, we see that the RMLP network performs best. As a likely consequence of a less expressive network with  a larger dynamics gap, the smaller PMLP network demonstrated a significant variance increase in the higher stiffness case than the larger PMLP-R. Overall, our policies are more robust to distribution shifts in model dynamics. However, we note that partitioning imposes structure that may not be optimal for this problem, as the MLP outperformed the partitioned networks in the light and nominal stiffness cases.</p>
<p>VII. CONCLUSION</p>
<p>The causal reasoning afforded by CREST allows the robot to structure robot manipulation policies with fewer parameters that are more sample efficient and robust to domain shifts than a naive approach that includes all known contexts. Indeed, using causality to reason about the simulation of a task identifies what variables are important to generalize a policy, while domain randomized pretraining provides a strong, task-specific prior in terms of how they matter. We believe that CREST is one step towards a new paradigm for structural sim-to-real transfer of robot manipulation policies that are sufficiently lightweight to be adapted in-the-field to overcome unforeseen domain shifts.</p>
<p>For future work, we will investigate using precondition learning to relax the assumption that the policy execution is feasible. We will also explore how the robot can learn the internal model used as the causal reasoning engine.</p>
<p>SUPPLEMENTARY MATERIALS A. Summary of CREST</p>
<p>Which state features are important for learning a control policy? Our approach, CREST, addresses this question through causal feature selection. CREST selects the relevant state variables for a given control policy, which apply over the policy's preconditions. The assumptions for CREST are that an internal model (i.e., an approximate task simulation) exists, the context space representation of the internal model facilitates causal interventions (e.g., disentangled variables), and the (parameterized) control policy and its preconditions are known. Through structure and transfer learning, CREST enables learning of policies that are compact, avoiding unnecessary state features. By construction, policies built using CREST are robust to distribution shifts in irrelevant variables, whereas baseline methods may yield policies with spurious correlations that are brittle. Such distribution shifts could arise from transfer between the internal model and reality, due to variations in dynamics or context distributions not encountered during pretraining with the internal model.</p>
<p>B. CREST Analysis on Math Environment</p>
<p>We now provide a greater description of the manipulation environment described in Sec. IV-C. The toy environment, MathManipEnv, approximates the mathematics of a controller for goal-based manipulation. For simplicity, the lowlevel control policy simply perturbs the state s ∈ R |S| by an input of θ = a ∈ R |A| in a manner specific to whether the system is linear or non-linear. Additionally, we consider the context c ∈ R |S| to be the initial state, s 0 . For this evaluation, we considered cases where |S| = |A| ("Dim." in Table I).</p>
<p>The reward for this task is
r = − g a − g d = − Gs a − g d = − G(s 0 + A(θ)) − g d
where g a ∈ R |g| is the goal vector that was obtained after execution of the controller to yield achieved state s a , and g d ∈ R |g| is the desired goal. The goal vector is calculated from a goal selection matrix G ∈ R |g|×|S| , which is a one-hot encoding matrix where the columns indicate the elements of the state vector that are used. In practice, G is formed by first randomly selecting N τ relevant context variables from the total set of c to form τ . Then, each τ is randomly allocated to a separate dimension of the goal vector, i.e., row of G.</p>
<p>Here, G represents that, in some goal-based problems, the goal is calculated from only a subset of the state vector (e.g., relative to the position of a particular object). The process of the system is either linear or non-linear, where A(θ) = ∆s + w a and w a ∼ N (0, σ 2 a ) is the action noise. In the linear case, the controller A ∈ R |S|×|S| is a matrix with randomly selected coefficients, so ∆s = Aθ. The non-zero coefficients of A indicate mappings of τ j to θ j . In the non-linear case, the controller A is a list of size |A|, where each element of the list specifies randomly selected functions (exponential, sigmoid, sine, cosine) that transform input a j into the resulting ∆s j .</p>
<p>Each trial of this environment randomly selects different τ , g d , s 0 , G, and A. The goal vector dimensionality |g| is fixed for each run and is typically equal to |S|.</p>
<p>C. Task Representation: Block Stacking</p>
<p>We now provide additional detail for the block stacking task described in Sec. VI-A. Figure 4 illustrates some context variables and the policy trajectory. In this task, the robot must stack the source block (block 0) upon the target block (block 1) using a sequential straight-line skill with control policy π(a|s, θ b ) and known preconditions.</p>
<p>Policy. The policy parameters θ b = [ θ ∆x , θ ∆y , θ ∆z ] T ∈ R 3 define three waypoints that the robot is sequentially commanded to via impedance control. Specifically, let y p represent a vertical position above the table and blocks. After the block is grasped, the executed policy is therefore: 1) Vertically lift to y p .</p>
<p>2) Move (θ ∆x , θ ∆z ) at fixed y p .</p>
<p>3) Vertically move θ ∆y − y p . The vertical lift to y p avoids obstructions to moving the block, so the preconditions of this skill are always satisfied.</p>
<p>Reward. The reward function for this task is
r = −α p w 0,a − p w g
where α = 1 is the reward weight, p w g = [ x w g , y w g , z w g ] T is the goal position of block 0, and p w 0,a = [ x w 0,a , y w 0,a , z w 0,a ] T is the final (i.e., achieved) position of block 0 at the end of the policy execution. In this task, the goal is to stack block 0 upon block 1. Therefore, x w g = x w 1 , y w g = 1 2 h 0 + 1 2 h 1 + y w 1 , and z w g = z w 1 .  Expressing the reward function in terms of the task context variables elucidates which variables are considered relevant to the task policy. The optimal low-level policy parameters
θ * b = [ θ * ∆x , θ * ∆y , θ * ∆z ] T ∈ R 3 are θ * ∆x = x w g − x w 0 = x w 1 − x w 0 θ * ∆y = y w g − y w 0 = 1 2 h 0 + 1 2 h 1 + y w 1 − y w 0 = 1 2 h 0 + 1 2 h 1 + 1 2 h 1 − 1 2 h 0 = h 1 θ * ∆z = z w g − z w 0 = z w 1 − z w 0
where the reduction of the block y-position variables arise from the blocks being initially constrained to the table.</p>
<p>The above derivation demonstrates that only certain variables are needed to generalize the policy across different contexts where the preconditions also hold true. Moreover, certain variables are only influential in certain policy parameters. We express this more concretely by formalizing what variables are needed for each parameter, which is where the ground truth mappings for CREST (Sec. VI-A) arise:
θ * ∆x = f (x w 0 , x w 1 ) → τ * ∆x = [x w 0 , x w 1 ] T θ * ∆y = f (h 1 ) → τ * ∆y = [h 1 ] θ * ∆z = f (z w 0 , z w 1 ) → τ * ∆z = [z w 0 , z w 1 ] T θ * b = f (x w 0 , x w 1 , h 1 , z w 0 , z w 1 ) → τ * = [x w 0 , x w 1 , h 1 , z w 0 , z w 1 ]
T Here, f is the model, which for our work we characterize using a neural network (although for this specific task, a linear model would also suffice). In causality terms, this is equivalent to modeling each individual policy parameter (θ j ) as a structural causal model, where f is a function with parent variables given by τ j .</p>
<p>D. Sim-to-Real Block Stacking Experiment</p>
<p>The sim-to-real block stacking experiment (Sec. VI-A) demonstrates that our proposed approach works in practice on a real robot system (Fig. 2c). As it is experimentally difficult to realize all possible values within the context distributions (e.g., creating blocks of precise height and color for each sample), we instead conduct the experiment on a slightly reduced distribution range. Specifically, we conduct this experiment using 10 blocks, where each block has a different color and two possible heights (5.7 cm or 7.6 cm). Before each trial, all block positions and rotations are shuffled by hand. Additionally, a random number generator selects the height of each block, as well as the enumeration of the blocks (and therefore which blocks are the source and target). The length and width of each block is 4.2 cm, which does not change during the experiment and is known from manual measurement (i.e., not perception).</p>
<p>Perception. We use a Microsoft Azure Kinect RGB-D camera to estimate each block's position, rotation, and color through a model-based perception algorithm utilizing the Open3D library [37]. Figure 5 shows an example of the block perception. The perception algorithm is as follows:</p>
<p>1) Crop to region bounded by the table blue tape (Fig. 2c).</p>
<p>2) Removal of hidden points via Katz [38], i.e., points expected to be occluded from the camera viewpoint. 3) Fit plane to table via random sample consensus (RANSAC) and remove any points below this plane. 4) Detect remaining clusters with DBSCAN [39], a density-based clustering algorithm. Proceed only if the numbers of clusters is N B = 10, or reject the perception sample and try again. 5) For each cluster (block), determine the best position and angle that fits a cube of known dimensions to the cluster via least-squares optimization. This step yields an estimate of each block's position and rotation. 6) Estimate block color by averaging the colors of all points within a cluster (block). Due to difficulties with accurately estimating block height from depth, the block height is provided by manual input instead. Manual checks are also completed prior to executing the control policy to ensure block perception results are reasonable. For example, if one cluster was not a block, but part of the blue tape, the perception sample would be rejected and attempted again. Prior to running the perception system, we obtain the extrinsics of the camera via a target-based calibration procedure, and we use the intrinsics as reported directly from the camera.</p>
<p>Control. We use the FrankaPy library [40] that implements impedance control for the Franka Emika Panda robot.</p>
<p>E. Task Representation: Crate Opening</p>
<p>This section provides more detail of the crate opening task (Sec. VI-B), where the objective is to open a crate in the presence of distractor objects using a circular arc skill with control policy π(a|s, θ a ) and known preconditions.   Fig. 6: Diagram of the crate opening task. The world coordinate frame {W } (not shown) is defined at the base link of the robot, similar to the block stacking task (Fig. 4). The z-axis of the crate coordinate frame {C} is coincident with the crate hinge. There are 10 distractor blocks, each with coordinate frame {B}.</p>
<p>shows some context variables and the policy trajectory, which emerges from the crate grasp point. This task has a larger modeling difference between the internal model and the target domain, which could also contribute to why our partitioned networks (PMLP, PMLP-R) were less successful than our non-partitioned network (RMLP). In addition to the dynamics domain difference discussed in Sec. VI-B, the y-position of the grasp point, y C g , is also slightly different. For the internal model, y C g exists in the same plane as the crate, but for the target, y C g is slightly above the crate because of the protruding grasp point.</p>
<p>Policy. The policy parameters θ a = [ θ p w a T , θ ∆γ , θ ∆φ ] T ∈ R 5 define a circular arc that is composed of N T waypoints. The robot is commanded to the crate grasp point, then the robot executes the policy by sequentially following each waypoint via impedance control. The crate cannot open into the blocks below, so the preconditions are always satisfied.</p>
<p>Reward. The reward function for this task is
r = − [ α a ∆Θ, α k e k ] T
In this function, ∆Θ = Θ a − Θ o is the difference between the achieved (Θ a ) and goal (Θ o ) crate angles, and α a and α k are reward weights. The term e k is the kinematic error in the policy trajectory, which is intended to induce robot trajectories that are safe (physically realizable and low force) in the target domain given articulated motion of the crate. For this work, α a = 1 and α k is 5 for the internal model and 0 for the target domain (because the robot realizes the trajectory it can actually achieve on the target due to the crate's articulated motion).</p>
<p>Specifically, e k = 1
N T N T t=1 p w a,t − p w d,t , where p w d,t
is the desired position of a waypoint in the trajectory and p w a,t is the kinematically realizable position of that same waypoint, both at timestep t. This is determined by projecting the desired waypoint onto the plane formed by rotating the grasp point about the crate hinge, obtaining the resulting crate angle, and using this angle to compute the realized grasp point.</p>
<p>F. Crate Opening Distribution Shift in Irrelevant Contexts</p>
<p>As mentioned in Sec. VI-B, we also conducted a crate opening experiment with distribution shifts in irrelevant parts of the context space, similar to the experiment in the block stacking task (Table III). As before, we pretrain on the entire context space, except for color of the crate and blocks, where only half of the color space is used. For testing, we transfer to two cases: 1) the same color space seen in training (no shift), and 2) the opposite color space (complete shift with no overlap). This experiment uses the "light" crate stiffness. Table VIII shows the results of this experiment. As expected, our policies are robust to distribution shifts of this type, whereas the baseline MLP incurs approximately 55% more target updates to overcome these irrelevant distribution shifts. Unlike the version of this experiment for block stacking, no policies achieved zero-shot transfer. However, this is because of the previously described domain shift in dynamics between the internal model and target domain.</p>
<p>Fig. 1 :
1A visualization of the different policy types. CREST is used to construct both the Reduced MLP (RMLP) and Partitioned MLP (PMLP). The baseline MLP is also shown for comparison. The relevant states are also used for the critic portion of the networks (only the actor portion is shown). The notation used is [w1,...,w d ], specifying the hidden units and depth of both the actor and critic.</p>
<p>Fig. 2 :Fig. 3 :
23Transfer experiments for block stacking and crate opening manipulation tasks. Policies are pretrained in the internal model ((a),(d)) and then transferred to the target domain ((b),(c),(e)). Target domains consist of replications of real systems using a Franka Panda robot, along with a real system for block stacking. Both tasks have distractor objects. For block stacking, only two blocks are necessary to generalize the policy. For crate opening, blocks represent distractor objects (e.g., if the crate were for a chest containing toys). Sample complexity of training a solved block stacking policy based on context dimension for (a) internal model and (b) target setting. c) Zero-shot transfer percentage, wherein the transferred policy needs no further target training to solve the task.</p>
<p>Fig. 4 :
4Diagram of the block stacking task. The world coordinate frame {W } is defined at the base link of the robot. Each block coordinate frame {B} is defined at the block's centroid.</p>
<p>Fig. 5 :
5Block state estimation used for the sim-to-real experiments using RGB-D perception. The perception algorithm takes as input a colored point cloud, and outputs a position, rotation, and color for 10 blocks. The red point in each block represents the block centroid, and the dashed lines indicate the block length and width (known a priori). The best-fit position and rotation angle for each point cloud cluster yields a pose estimate for each block.</p>
<p>Figure 6
6</p>
<p>TABLE</p>
<p>TABLE II :
IINetworks used for the block stacking task.Network 
Parameters 
Input Dim. (Total) 
Architecture 
MLP (N B = 2) 
3298 
14 (14) 
[24, 24, 24] 
MLP (N B = 6) 
4642 
42 (42) 
[24, 24, 24] 
MLP (N B = 10) 
5986 
70 (70) 
[24, 24, 24] 
MLP (N B = 14) 
7330 
98 (98) 
[24, 24, 24] 
MLP (N B = 18) 
8674 
126 (126) 
[24, 24, 24] 
RMLP (ours) 
2866 
5 (7N B ) 
[24, 24, 24] 
PMLP (ours) 
754 
5 (7N B ) 
[8, 8, 8] x 3 </p>
<p>TABLE III :
IIITransfer results for a distribution shift in 30 context variables (color) that are irrelevant for the block stacking policy.Network 
IM Updates 
(k-Samples) </p>
<p>Target Updates 
(k-Samples), no shift </p>
<p>Zero-Shot Transfer 
no shift </p>
<p>Target Updates 
(k-Samples), shift </p>
<p>Zero-Shot Transfer, 
shift 
MLP 
237.5 (121.6) ± 11.6 (5.9) 
1.6 (0.8) ± 1.5 (0.8) 
4 
17.3 (8.9) ± 4.8 (2.5) 
0 
RMLP (ours) 
137.6 (70.5) ± 7.2 (3.7) 
0.5 (0.3) ± 0.9 (0.5) 
7 
1.0 (0.5) ± 1.5 (0.8) 
6 
PMLP (ours) 
139.2 (71.3) ± 8.2 (4.2) 
0.0 (0.0) ± 0.0 (0.0) 
10 
0.1 (0.1) ± 0.3 (0.2) 
9 </p>
<p>TABLE IV :
IVSim-to-real policy evaluation results for block stacking with NB = 10. The reward threshold for zero-shot transfer is -0.025 (about half of the block width). We also note how often the block was successfully stacked. "GT" is a ground truth policy to illustrate the degree of uncertainty present within the robot perception and control system. Each policy was evaluated 10 times.Policy 
Reward 
Zero-Shot Transfer 
Block Stacked 
MLP 
-0.033 ± 0.012 
3 
1 
RMLP (ours) 
-0.018 ± 0.007 
9 
4 
PMLP (ours) 
-0.014 ± 0.004 
10 
6 
GT 
-0.009 ± 0.003 
10 
10 </p>
<p>TABLE V :
VNetworks used for the crate opening task.Network 
Parameters 
Input Dim. (Total) 
Architecture 
MLP 
13496 
80 (80) 
[40, 40, 40] 
RMLP (ours) 
7576 
6 (80) 
[40, 40, 40] 
PMLP (ours) 
1152 
6 (80) 
[8, 8, 8] x 5 
PMLP-R (ours) 
8032 
6 (80) 
[24, 24, 24] x 5 </p>
<p>body articulation kinematics: </p>
<p>TABLE VI :
VIPretraining and transfer results for crate opening policies compared to training directly in target (without transfer).Network 
IM Updates 
(k-Samples) </p>
<p>Target Updates 
(k-Samples), transfer </p>
<p>Target Updates 
(k-Samples), direct </p>
<p>MLP 
45.0 ± 3.16 
(23.04 ± 1.62) </p>
<p>12.20 ± 1.72 
(6.25 ± 0.88) </p>
<p>38.70 ± 10.99 
(19.8 ± 5.63) 
RMLP 
(ours) </p>
<p>32.40 ± 3.67 
(16.59 ± 1.88) </p>
<p>7.0 ± 1.18 
(3.58 ± 0.61) </p>
<p>16.40 ± 4.05 
(8.40 ± 2.08) 
PMLP 
(ours) </p>
<p>48.20 ± 13.33 
(24.68 ± 6.83) </p>
<p>14.0 ± 3.74 
(7.17 ± 1.92) </p>
<p>14.20 ± 6.32 
(7.27 ± 3.24) 
PMLP-R 
(ours) </p>
<p>51.30 ± 10.82 
(26.27 ± 5.54) </p>
<p>14.3 ± 4.34 
(7.32 ± 2.22) </p>
<p>15.80 ± 3.97 
(8.09 ± 2.03) </p>
<p>TABLE VII :
VIIFine-tuning for crate opening policies with increasing 
crate stiffness and correspondingly greater transition model differ-
ence between the internal model and target task. </p>
<p>Network 
Target Updates 
(k-Samples), light </p>
<p>Target Updates 
(k-Samples), nominal </p>
<p>Target Updates 
(k-Samples), stiff </p>
<p>MLP 
3.90 ± 0.54 
(2.00 ± 0.28) </p>
<p>12.20 ± 1.72 
(6.25 ± 0.88) </p>
<p>27.30 ± 5.51 
(13.98 ± 2.82) 
RMLP 
(ours) </p>
<p>3.20 ± 0.60 
(1.64 ± 0.31) </p>
<p>7.00 ± 1.18 
(3.58 ± 0.61) </p>
<p>16.40 ± 5.90 
(8.40 ± 3.02) 
PMLP 
(ours) </p>
<p>9.10 ± 1.58 
(4.66 ± 0.81) </p>
<p>14.00 ± 3.74 
(7.17 ± 1.92) </p>
<p>24.00 ± 15.06 
(12.29 ± 7.71) 
PMLP-R 
(ours) </p>
<p>9.20 ± 1.54 
(4.71 ± 0.79) </p>
<p>14.30 ± 4.34 
(7.32 ± 2.22) </p>
<p>19.10 ± 4.91 
(9.80 ± 2.51) </p>
<p>TABLE VIII :
VIIITransfer results for a distribution shift in 33 context variables that are irrelevant for the crate opening policy. Variables are 3-tuple RGB colors of the crate and 10 blocks in the scene. Light crate stiffness.Network 
IM Updates 
(k-Samples) </p>
<p>Target Updates 
(k-Samples), no shift </p>
<p>Target Updates 
(k-Samples), shift </p>
<p>MLP 
36.10 ± 3.11 
(18.48 ± 1.59) </p>
<p>11.10 ± 2.30 
(5.68 ± 1.18) </p>
<p>17.30 ± 4.22 
(8.86 ± 2.16) 
RMLP 
(ours) </p>
<p>36.20 ± 5.23 
(18.53 ± 2.68) </p>
<p>3.40 ± 0.66 
(1.74 ± 0.34) </p>
<p>3.50 ± 0.67 
(1.80 ± 0.34) 
PMLP 
(ours) </p>
<p>48.50 ± 7.88 
(24.80 ± 4.03) </p>
<p>8.50 ± 2.06 
(4.35 ± 1.06) </p>
<p>8.30 ± 1.10 
(4.25 ± 0.56) 
PMLP-R 
(ours) </p>
<p>53.00 ± 7.80 
(27.14 ± 3.99) </p>
<p>9.50 ± 1.91 
(4.86 ± 0.98) </p>
<p>9.60 ± 2.01 
(4.92 ± 1.03) </p>
<p>ACKNOWLEDGMENTS
Simto-Real Transfer of Robotic Control with Dynamics Randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, Int'l Conf. on Robotics and Automation (ICRA). X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim- to-Real Transfer of Robotic Control with Dynamics Randomization," Int'l Conf. on Robotics and Automation (ICRA), 2018.</p>
<p>Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, Int'l Conf. on Robotics and Automation (ICRA). K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakr- ishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al., "Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping," Int'l Conf. on Robotics and Automation (ICRA), 2018.</p>
<p>A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms. O Kroemer, S Niekum, G Konidaris, arXiv:1907.03146arXiv preprintO. Kroemer, S. Niekum, and G. Konidaris, "A Review of Robot Learn- ing for Manipulation: Challenges, Representations, and Algorithms," arXiv preprint arXiv:1907.03146, 2019.</p>
<p>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey. W Zhao, J P Queralta, T Westerlund, IEEE Symposium Series on Computational Intelligence. 2020W. Zhao, J. P. Queralta, and T. Westerlund, "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey," IEEE Symposium Series on Computational Intelligence (SSCI), 2020.</p>
<p>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, Int'l Conf. on Intelligent Robots and Systems (IROS). J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World," Int'l Conf. on Intelligent Robots and Systems (IROS), 2017.</p>
<p>GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning. J Liang, V Makoviychuk, A Handa, N Chentanez, M Macklin, D Fox, Conf. on Robot Learning (CoRL). J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox, "GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning," Conf. on Robot Learning (CoRL), 2018.</p>
<p>Learning Causality and Causality-Related Learning: Some Recent Progress. K Zhang, B Schölkopf, P Spirtes, C Glymour, National Science Review. 51K. Zhang, B. Schölkopf, P. Spirtes, and C. Glymour, "Learning Causality and Causality-Related Learning: Some Recent Progress," National Science Review, vol. 5, no. 1, pp. 26-29, 2018.</p>
<p>Causality for Machine Learning. B Schölkopf, arXiv:1911.10500arXiv preprintB. Schölkopf, "Causality for Machine Learning," arXiv preprint arXiv:1911.10500, 2019.</p>
<p>Transfer Learning in Multi-Armed Bandits: A Causal Approach. J Zhang, E Bareinboim, Int'l Joint Conf. on Artificial Intelligence (IJCAI). J. Zhang and E. Bareinboim, "Transfer Learning in Multi-Armed Ban- dits: A Causal Approach," Int'l Joint Conf. on Artificial Intelligence (IJCAI), 2017.</p>
<p>Causal Confusion in Imitation Learning. P De Haan, D Jayaraman, S Levine, Conf. on Neural Information Processing Systems (NeurIPS). P. de Haan, D. Jayaraman, and S. Levine, "Causal Confusion in Imitation Learning," Conf. on Neural Information Processing Systems (NeurIPS), 2019.</p>
<p>Causal Discovery in Physical Systems from Videos. Y Li, A Torralba, A Anandkumar, D Fox, A Garg, Conf. on Neural Information Processing Systems (NeurIPS). 2020Y. Li, A. Torralba, A. Anandkumar, D. Fox, and A. Garg, "Causal Discovery in Physical Systems from Videos," Conf. on Neural Infor- mation Processing Systems (NeurIPS), 2020.</p>
<p>Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning. S A Sontakke, A Mehrjou, L Itti, B Schölkopf, arXiv:2010.03110arXiv preprintS. A. Sontakke, A. Mehrjou, L. Itti, and B. Schölkopf, "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning," arXiv preprint arXiv:2010.03110, 2020.</p>
<p>. J Pearl, Causality , Cambridge University PressJ. Pearl, Causality. Cambridge University Press, 2009.</p>
<p>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. O Ahmed, F Träuble, A Goyal, A Neitz, M Wüthrich, Y Bengio, B Schölkopf, S Bauer, arXiv:2010.04296arXiv preprintO. Ahmed, F. Träuble, A. Goyal, A. Neitz, M. Wüthrich, Y. Bengio, B. Schölkopf, and S. Bauer, "CausalWorld: A Robotic Manipula- tion Benchmark for Causal Structure and Transfer Learning," arXiv preprint arXiv:2010.04296, 2020.</p>
<p>Deep Object-Centric Representations for Generalizable Robot Learning. C Devin, P Abbeel, T Darrell, S Levine, Int'l Conf. on Robotics and Automation (ICRA). C. Devin, P. Abbeel, T. Darrell, and S. Levine, "Deep Object-Centric Representations for Generalizable Robot Learning," Int'l Conf. on Robotics and Automation (ICRA), 2018.</p>
<p>Preparing for the Unknown: Learning a Universal Policy with Online System Identification. W Yu, RSSJ Tan, RSSC K Liu, RSSG Turk, RSSRobotics: Science and Systems. W. Yu, J. Tan, C. K. Liu, and G. Turk, "Preparing for the Unknown: Learning a Universal Policy with Online System Identification," Robotics: Science and Systems (RSS), 2017.</p>
<p>Dimension Reduction and its Application to Model-Based Exploration in Continuous Spaces. A Nouri, M L Littman, Machine Learning. 81A. Nouri and M. L. Littman, "Dimension Reduction and its Appli- cation to Model-Based Exploration in Continuous Spaces," Machine Learning, vol. 81, no. 1, pp. 85-98, 2010.</p>
<p>Regularization and Feature Selection in Least-Squares Temporal Difference Learning. J Z Kolter, A Y Ng, Int'l Conf. on Machine Learning. J. Z. Kolter and A. Y. Ng, "Regularization and Feature Selection in Least-Squares Temporal Difference Learning," Int'l Conf. on Machine Learning, 2009.</p>
<p>An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning. R Parr, L Li, G Taylor, C Painter-Wakefield, M L Littman, Int'l Conf. on Machine Learning. R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. L. Littman, "An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning," Int'l Conf. on Machine Learning, 2008.</p>
<p>Sim-to-Real: Learning Agile Locomotion for Quadruped Robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, Robotics: Science and Systems (RSS). J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke, "Sim-to-Real: Learning Agile Locomotion for Quadruped Robots," Robotics: Science and Systems (RSS), 2018.</p>
<p>Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors. A Molchanov, T Chen, W Hönig, J A Preiss, N Ayanian, G S Sukhatme, Int'l Conf. on Intelligent Robots and Systems (IROS). A. Molchanov, T. Chen, W. Hönig, J. A. Preiss, N. Ayanian, and G. S. Sukhatme, "Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors," Int'l Conf. on Intelligent Robots and Systems (IROS), 2019.</p>
<p>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, Int'l Conf. on Robotics and Automation (ICRA). Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience," Int'l Conf. on Robotics and Automation (ICRA), 2019.</p>
<p>Reinforcement Learning with Parameterized Actions. W Masson, P Ranchod, G Konidaris, AAAI Conf. on Artificial Intelligence. W. Masson, P. Ranchod, and G. Konidaris, "Reinforcement Learning with Parameterized Actions," AAAI Conf. on Artificial Intelligence, 2016.</p>
<p>Deep Reinforcement Learning in Parameterized Action Space. M Hausknecht, P Stone, Int'l Conf. on Learning Representations (ICLR). M. Hausknecht and P. Stone, "Deep Reinforcement Learning in Parameterized Action Space," Int'l Conf. on Learning Representations (ICLR), 2016.</p>
<p>Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space. Z Fan, R Su, W Zhang, Y Yu, Int'l Joint Conf. on Artificial Intelligence (IJCAI). Z. Fan, R. Su, W. Zhang, and Y. Yu, "Hybrid Actor-Critic Reinforce- ment Learning in Parameterized Action Space," Int'l Joint Conf. on Artificial Intelligence (IJCAI), 2019.</p>
<p>A Survey on Policy Search for Robotics. M P Deisenroth, G Neumann, J Peters, Foundations and Trends in Robotics. 21-2M. P. Deisenroth, G. Neumann, J. Peters, et al., "A Survey on Policy Search for Robotics," Foundations and Trends in Robotics, vol. 2, no. 1-2, pp. 1-142, 2013.</p>
<p>Simulation as an Engine of Physical Scene Understanding. P W Battaglia, J B Hamrick, J B Tenenbaum, Proceedings of the National Academy of Sciences. 11045P. W. Battaglia, J. B. Hamrick, and J. B. Tenenbaum, "Simulation as an Engine of Physical Scene Understanding," Proceedings of the National Academy of Sciences, vol. 110, no. 45, pp. 18 327-18 332, 2013.</p>
<p>Recurrent World Models Facilitate Policy Evolution. D Ha, J Schmidhuber, Conf. on Neural Information Processing Systems (NeurIPS). D. Ha and J. Schmidhuber, "Recurrent World Models Facilitate Policy Evolution," Conf. on Neural Information Processing Systems (NeurIPS), 2018.</p>
<p>Toward Causal Representation Learning. B Schölkopf, F Locatello, S Bauer, N R Ke, N Kalchbrenner, A Goyal, Y Bengio, Proceedings of the IEEE. the IEEEB. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio, "Toward Causal Representation Learning," Proceedings of the IEEE, 2021.</p>
<p>Relative Entropy Policy Search. J Peters, K Mülling, Y Altun, AAAI Conf. on Artificial Intelligence. J. Peters, K. Mülling, and Y. Altun, "Relative Entropy Policy Search," AAAI Conf. on Artificial Intelligence, 2010.</p>
<p>Actor-Critic Algorithms. V R Konda, J N Tsitsiklis, Conf. on Neural Information Processing Systems (NeurIPS). V. R. Konda and J. N. Tsitsiklis, "Actor-Critic Algorithms," Conf. on Neural Information Processing Systems (NeurIPS), 2000.</p>
<p>The Expressive Power of Neural Networks: A View from the Width. Z Lu, H Pu, F Wang, Z Hu, L Wang, Conf. on Neural Information Processing Systems (NeurIPS). Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang, "The Expressive Power of Neural Networks: A View from the Width," Conf. on Neural Information Processing Systems (NeurIPS), 2017.</p>
<p>Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks. A M Saxe, J L Mcclelland, S Ganguli, Int'l Conf. on Learning Representations (ICLR). A. M. Saxe, J. L. Mcclelland, and S. Ganguli, "Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks," Int'l Conf. on Learning Representations (ICLR), 2014.</p>
<p>Proximal Policy Optimization Algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal Policy Optimization Algorithms," arXiv preprint arXiv:1707.06347, 2017.</p>
<p>A Hill, A Raffin, M Ernestus, A Gleave, A Kanervisto, R Traore, P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, Stable Baselines. A. Hill, A. Raffin, M. Ernestus, A. Gleave, A. Kanervisto, R. Traore, P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu, "Stable Baselines," https://github. com/hill-a/stable-baselines, 2018.</p>
<p>. A A Rusu, N C Rabinowitz, G Desjardins, H Soyer, J Kirkpatrick, K Kavukcuoglu, R Pascanu, R Hadsell, arXiv:1606.04671Progressive Neural Networks. arXiv preprintA. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell, "Progressive Neural Networks," arXiv preprint arXiv:1606.04671, 2016.</p>
<p>Open3D: A Modern Library for 3D Data Processing. Q.-Y Zhou, J Park, V Koltun, arXiv:1801.09847arXiv preprintQ.-Y. Zhou, J. Park, and V. Koltun, "Open3D: A Modern Library for 3D Data Processing," arXiv preprint arXiv:1801.09847, 2018.</p>
<p>Direct Visibility of Point Sets. S Katz, A Tal, R Basri, ACM SIGGRAPH 2007 papers. 24S. Katz, A. Tal, and R. Basri, "Direct Visibility of Point Sets," in ACM SIGGRAPH 2007 papers, 2007, pp. 24-es.</p>
<p>A Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. M Ester, H.-P Kriegel, J Sander, X Xu, Proceedings of the Second Int'l Conf. on Knowledge Discovery and Data Mining (KDD). the Second Int'l Conf. on Knowledge Discovery and Data Mining (KDD)96M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al., "A Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise," in Proceedings of the Second Int'l Conf. on Knowledge Discovery and Data Mining (KDD), vol. 96, no. 34, 1996, pp. 226- 231.</p>
<p>A Modular Robotic Arm Control Stack for Research: Franka-Interface and FrankaPy. K Zhang, M Sharma, J Liang, O Kroemer, arXiv:2011.02398arXiv preprintK. Zhang, M. Sharma, J. Liang, and O. Kroemer, "A Modular Robotic Arm Control Stack for Research: Franka-Interface and FrankaPy," arXiv preprint arXiv:2011.02398, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
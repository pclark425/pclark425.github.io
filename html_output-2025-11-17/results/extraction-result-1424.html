<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1424 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1424</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1424</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-2e0bc6cc4153025cd37d957cb896abe237502cd5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2e0bc6cc4153025cd37d957cb896abe237502cd5" target="_blank">NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> A latent dynamics learning framework is introduced that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers and providing interpretable goal discovery when applied to imitation learning of switching controllers from demonstration.</p>
                <p><strong>Paper Abstract:</strong> Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1424.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1424.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NewtonianVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Newtonian Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured latent dynamics VAE that explicitly models stochastic positions and deterministic velocities with constrained (diagonal, signed) transition matrices to induce Newtonian (double-integrator) dynamics and enable proportional (P) controllability directly from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NewtonianVAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variational autoencoder with an encoder q(x_t | I_t) producing stochastic latent positions x; velocity v is computed deterministically as finite differences v_t = (x_t - x_{t-1})/Δt; transition prior enforces discrete-time Newtonian dynamics via v_{t} update v_{t} = v_{t-1} + Δt*(A x_{t-1} + B v_{t-1} + C u_{t-1}) with A,B,C constrained to be diagonal and with sign constraints (C>0, B<0); generative model p(I_t | x_t) decodes latent positions to images; ELBO uses future-step reconstruction to encourage use of the transition prior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (structured VAE latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based continuous control / robotic manipulation / imitation learning (point-mass, reacher-2D, fetch-3D, PR2 real robot)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Qualitative P-controllability (convergence under a proportional controller), reconstruction likelihood / ELBO and KL terms (training objective), and downstream task reward (environment reward in imitation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Demonstrated P-controllability in all evaluated environments (point mass, reacher-2D, fetch-3D); achieved perfect task reward (3.0 ± 0.0) on the multi-goal imitation task using switching P-controllers derived from the latent (Table 1). No numeric ELBO / MSE values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: latent position coordinates correspond directly to physical configuration (e.g., x,y or joint angles); learned goal latent vectors can be decoded to images, yielding visually-interpretable goals and task segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of latent space colored by ground-truth state; latent-to-image decoding of inferred goal vectors; inspection of learned diagonal transition parameters and their signs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified numerically; qualitatively reduces control-time computation by enabling simple P/PID controllers instead of sampling-based planning or additional policy optimization (eliminates need for costly MPC/RL for many tasks). Training follows standard VAE-style costs (neural encoder/decoder and small transition network).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitatively more data- and compute-efficient at inference/control compared to methods that require MPC or learned policies: one-shot or few-shot imitation (switching P-controller) succeeded where LSTM behaviour cloning and adversarial IRL (GAIL) needed more data or environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On sequential multi-goal imitation: switching P-controller in NewtonianVAE latent achieved reward 3.0 ±0.0 for 1, 10, and 100 demonstration settings (Table 1). Demonstrated robust path-following via DMPs in latent space for fetch-3D.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High fidelity for task-relevant dynamics (Newtonian / P-controllable systems) translated to excellent task performance: latent proportionality allowed direct application of P/PID controllers, simplified imitation (goal inference), and enabled DMP-based trajectory following. The model prioritizes control-relevant structure over unconstrained representation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Requires the true system to be well-approximated by Newtonian double-integrator dynamics and to be P-controllable; learns fixed goal states (not semantic, variable goals) with limited generalization to goal variations without more diverse demonstrations; assumes sufficiently high-speed vision for torque-level control in high-DoF manipulators.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Stochastic latent x (position) + deterministic velocity via finite differences; diagonal transition matrices with sign constraints (C>0, B<0) to enforce disentanglement and correct directional relations between u, x, v; future-step reconstruction in ELBO to force use of transition prior; use of Gaussian encoder/decoder; decoder enables visual inspection of goals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to E2C and a frame-wise VAE and an ablated Full-NewtonianVAE: only NewtonianVAE yielded P-controllable latents and stable convergence under simple P/PID controllers; E2C and Full-NewtonianVAE produced visually-structured latents but failed under direct P-control (needed MPC). Qualitatively outperformed LSTM behaviour cloning and GAIL for few-shot imitation efficiency when used with switching P-controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper argues the combination of (1) explicit position latent with deterministic velocity derived from consecutive encodings, (2) diagonal signed transition matrices (A,B,C) with C>0 and B<0, and (3) future-step reconstruction in the ELBO, constitutes an effective configuration to balance interpretability, control-utility, and efficiency for Newtonian/P-controllable systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1424.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full-NewtonianVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-parameter NewtonianVAE (unconstrained variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant ablation of NewtonianVAE in which the transition matrices A, B, C are allowed to be full (unconstrained) and unbounded (no sign/diagonality constraints), used to evaluate the importance of the structural constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Full-NewtonianVAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same VAE framework as NewtonianVAE but with A,B,C transitions implemented as full (dense) matrices without enforced diagonality or sign constraints; velocity still computed as finite differences of encoded positions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE latent dynamics, ablated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based continuous control (same eval domains as NewtonianVAE: point mass, reacher-2D, fetch-3D)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>P-controllability (convergence under P controller) and downstream imitation reward.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Empirically failed to provide P-controllability in experiments; systems using this variant diverged under direct P-control (as reported qualitatively). No numeric ELBO or rewards reported specifically for this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Lower than NewtonianVAE: although latent spaces appear visually structured and correlated with ground truth, they do not guarantee the directional/decoupled relations needed for intuitive proportional control.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent space visualization colored by ground-truth; comparison of control rollouts under P-controller.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified; same order as NewtonianVAE for training but may require additional control computation (MPC) at inference due to lack of P-controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient for control than constrained NewtonianVAE because it typically requires more complex control/planning (e.g., MPC) to stabilize or reach goals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Failed to converge reliably under simple P-control in the tested environments; no successful task metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows that relaxing structural constraints reduces immediate utility for simple proportional controllers even if latent appears structured; indicates that imposing physically-motivated constraints improves task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Removing diagonality and sign constraints can allow more expressive transitions but at the cost of losing guaranteed proportional controllability and interpretability; expressivity vs. control-compatibility tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Ablation: remove diagonality and sign constraints on A,B,C to evaluate their role.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to NewtonianVAE, it underperforms for direct P-control though could in principle model more complex transitions; compared to E2C, similar failure to be directly P-controllable despite visually-structured latents.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper's experiments indicate that the constrained (diagonal, signed) transition matrices are preferable when the goal is P-controllability and interpretable latent coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1424.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embed to Control (E2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VAE-style latent dynamics model that enforces locally linear transitions in latent space (locally-linear dynamic system) to enable planning and control from images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embed to control: A locally linear latent dynamics model for control from raw images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>E2C (Embed to Control)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VAE with locally linear latent transition priors (conditioned on latent state) that enable locally linearized control planning; uses learned linearization matrices to predict next latents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (locally-linear latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based control / simulated continuous control domains</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Visual latent-space correlation with ground truth; control success measured by convergence under applied controllers and downstream task reward when using planning (MPC) rather than simple P-control.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Latent coordinates can correlate with ground-truth positions but in the paper E2C failed to be directly P-controllable: naive P-controller rollouts diverged (Fig. 1 & Fig. 3). It can be stabilized using MPC (CEM planning) but numerical fidelity metrics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: some latent dimensions correspond to physical variables but identification requires inspection (no automatic disentanglement guaranteed).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Plotting latent coordinates and checking correlation with ground-truth positions; manual inspection required to find position dims.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires planning (MPC/CEM) in latent space to achieve control in many cases, which increases inference-time computation compared to direct P-controller operation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less control-time efficient than NewtonianVAE for tasks where simple proportional control is desired because it typically needs sampling-based planning (MPC) to succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Can achieve control when combined with MPC/planning (qualitative), but failed under simple P-control in the experiments; no specific reward numbers for E2C baseline are reported in the paper's imitation table.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Provides useful latent representations but lacks the structured coupling between actions and latent positions needed for straightforward proportional controllers; useful when planning machinery is acceptable.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Locally-linear transitions give expressivity and enable planning, but do not guarantee direct proportional controllability or disentangled action-to-position coupling without additional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Locally linear latent transitions; VAE encoder/decoder; learned linearization for dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally and found inferior for direct P-control relative to NewtonianVAE; can be used with MPC whereas NewtonianVAE allows direct P/PID control without planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not explicitly provided in this paper; authors illustrate that additional structure (as in NewtonianVAE) is needed when the control objective is simple proportional control from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1424.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frame-wise Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard VAE applied independently to individual frames (no temporal dynamics enforced), used as a baseline to evaluate the importance of temporal/physical structure in latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-encoding variational bayes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Frame-wise VAE (static VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-frame encoder q(z|I) and decoder p(I|z) trained with standard ELBO; no explicit transition model or dynamics prior across time steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent generative model (static VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision representation learning; baseline for vision-based control experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction likelihood / ELBO on frames; (in paper) indirect assessment by whether latents support P-control.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Did not produce latents that are P-controllable; failed under direct P-controller rollouts (qualitatively reported). No numeric ELBO/MSE reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low to moderate: may capture correlations with ground truth features but lack temporal structure hampers control interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent-space visualization; indirect evaluation via downstream control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Lower per-step complexity (no transition network), but inadequate for control without additional dynamics models, which would add computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less useful for control tasks relative to NewtonianVAE since it lacks temporal dynamics; would require secondary dynamics learning or policy learning to be useful for control.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Failed to support direct P-control in evaluated tasks; no imitation reward numbers reported specifically for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Provides image reconstruction but insufficient world-modeling fidelity for simple control; demonstrates need for structured temporal dynamics for control-oriented world models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity and lower modeling overhead come at the expense of utility for control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Per-frame latent encoding without dynamics prior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed by NewtonianVAE for control-oriented tasks; worse than models that explicitly model transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not applicable; paper uses it as a baseline to show necessity of dynamics structure for proportional control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1424.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DVBF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Variational Bayes Filters (DVBF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent variable model that learns nonlinear, conditionally linear latent dynamics for time series via a variational Bayes filtering approach; referenced as related work for latent dynamics modeling from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep variational Bayes filters: Unsupervised learning of state space models from raw data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deep Variational Bayes Filters (DVBF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent/filtered VAE formulation that learns latent state-space models with learned transition priors and amortized inference for sequential data; can model nonlinear dynamics in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (variational state-space model / VRNN-style filter)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>video prediction, dynamics modeling, control from pixels (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO, predictive log-likelihood, reconstruction error and KL divergence (standard variational metrics); potential downstream control performance when used with planning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not evaluated quantitatively in this paper; referenced as similar to E2C in producing latents that are not directly P-controllable without additional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generally limited unless additional structure is imposed; DVBFs recover latent dynamics but do not guarantee interpretable physical coordinates by default.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specified in this paper; typical methods include latent visualization and probing.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to other recurrent variational models; training involves sequential inference and sampling via reparameterization, but no quantitative cost reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared quantitatively in this paper; cited to motivate need for more structured latent models for proportional control.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for learning latent dynamics but may require additional structure or downstream control algorithms (MPC/RL) to perform control tasks effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Expressive latent dynamics vs. lack of guaranteed control-friendly structure.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Variational filtering, learned transition priors, amortized inference for sequential data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned alongside E2C and Kalman VAE as models that recover structured conditionally-linear latent spaces but are unsuitable for direct proportional control per the authors' analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper; authors argue structural inductive biases (as in NewtonianVAE) are needed when control via simple P-laws is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1424.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PVEs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Position-Velocity Encoders (PVEs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder that explicitly learns position and velocity representations from video by optimizing heuristic loss combinations; lacks a decoder and thus limits visual interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Position-Velocity Encoders (PVEs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unsupervised learning approach that trains encoders to output explicit position and velocity latent variables using objectives that enforce temporal coherence and dynamics priors (heuristic loss terms); typically does not include a generative decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>structured latent representation learner (explicit physics-inspired encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>video-based state representation for control / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Correlation of learned latents with ground-truth positions/velocities; downstream control performance in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not evaluated quantitatively in this paper; authors note PVEs learn explicit positional representations but the lack of a decoder prevents visual inspection and may limit other uses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High for latent variables (explicit position and velocity), but limited overall because PVEs lack a decoder for reconstructing or visualizing learned states in image space.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent probing and heuristic losses that encourage disentanglement into position/velocity channels.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified here; PVEs are typically lightweight encoders but require designing hand-crafted loss terms.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Trade-off: straightforward interpretability but require carefully tuned heuristic losses and lack generative decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good for obtaining explicit kinematic latents, but lack of decoder reduces a capability (visualizing inferred goals) that NewtonianVAE retains.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Explicit hand-crafted losses can produce desired latent structure but at the cost of relying on multiple heuristics and losing decoder-based interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Explicitly enforce position and velocity latents via custom loss terms; omit decoder/generative component.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PVEs yield explicit pos/vel latents like NewtonianVAE's design goal, but NewtonianVAE integrates those properties inside a probabilistic decoder-enabled VAE framework and enforces dynamics via transition priors rather than multiple heuristic losses.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified here; paper notes PVEs' limitations motivate a decoder-based NewtonianVAE approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1424.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative latent world-model approach (Ha & Schmidhuber) that learns a VAE-based vision model and RNN dynamics in latent space to enable policy learning/planning in the learned latent environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combining a VAE to compress image observations to latents, an RNN (MDN-RNN) to model latent dynamics, and a controller/policy trained in latent space; emphasizes compact learned simulators for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE + RNN dynamics + controller)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>reinforcement learning / policy learning from pixels (game and control domains)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predictive accuracy of the RNN dynamics in latent space (MDN loss), reconstruction error of VAE, and downstream policy performance measured in environment reward.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not measured in this paper; cited as related work illustrating latent dynamics models used for planning/control from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderate: latents capture succinct scene summaries but are not guaranteed to be physically-interpretable coordinates like NewtonianVAE's latents.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent visualization and decoding; policy behavior inspection in latent-produced rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training involves VAE + RNN + policy search; in general requires significant compute but specific figures not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Approach typically requires learning a policy or controller in the learned simulator; paper contrasts this with NewtonianVAE's goal of eliminating extra controller learning by making latent directly amenable to simple P-controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Powerful for enabling policy learning via a compact learned simulator but may impose downstream computational costs (policy search/planning) that the NewtonianVAE seeks to avoid for proportional-control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Expressive latent world models enable complex policy learning but do not guarantee simple control laws are directly applicable; policy training adds complexity and data requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VAE encoder/decoder, MDN-RNN dynamics, separate policy learned in latent world.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted qualitatively with NewtonianVAE: world models enable RL/policy learning in latent simulate but NewtonianVAE designs latents to be directly controllable via P/PID control.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here; cited as inspirational prior work for latent dynamics approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1424.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet / Hafner et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that learns latent dynamics suitable for planning (PlaNet) from pixels using a recurrent state-space model learned via variational inference and used for planning with CEM/MPC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet / latent dynamics planning (Hafner et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent latent dynamics model trained with variational objectives to support planning/control (often used with CEM or MPC) from pixel observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent state-space / model-based planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based planning and control from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predictive log-likelihood / ELBO, planning success (task reward) when combined with CEM/MPC.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; cited as a contrast to NewtonianVAE which aims to avoid planning overhead by making latent amenable to proportional control.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent not necessarily physically-interpretable; emphasis on predictive performance for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent visualization sometimes used, but not a primary focus.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires planning (CEM/MPC) at inference or additional policy optimization; generally more compute at control time than simple proportional controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient at inference/control relative to NewtonianVAE for tasks where P-control suffices, because PlaNet-style models rely on planning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Excellent for cases where planning in a predictive latent model is acceptable; NewtonianVAE aims for simpler controllers by embedding proportional dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Good predictive fidelity and planner compatibility but increased inference/solution complexity for control.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Recurrent latent state-space model trained with variational inference; planning integrated at control time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Used as a representative of models that decouple dynamics and control and thus may carry higher computational burden at control time compared to NewtonianVAE.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1424.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1424.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE-DMPs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Movement Primitives in latent VAE (VAE-DMPs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that imposes DMP dynamics in the latent space of a time-dependent VAE to perform trajectory imitation and path following from lower-dimensional latents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic movement primitives in latent space of time-dependent variational autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VAE-DMPs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Time-dependent VAE whose latent dynamics are constrained or modelled according to DMP-style attractor dynamics so that trajectory-following controllers operate in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with embedded control dynamics (hybrid model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>trajectory imitation / path following from visual demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction/ELBO and DMP fit error to demonstrations; downstream trajectory-following success.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not evaluated in this paper; cited as related work that also aims to enable DMPs in latent space but differs from NewtonianVAE by tying dynamics learning to specific DMP objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Can provide latent spaces suitable for DMP fitting but may be task-specific; interpretability depends on learned latent axes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent visualization and trajectory decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified here; VAE-DMPs train for both generative and DMP components.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>VAE-DMPs specifically train for DMP compatibility; NewtonianVAE argues for learning dynamics independent of tasks so that DMPs can be fit post-hoc (more flexible).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>VAE-DMPs achieve trajectory-following by construction; NewtonianVAE demonstrates that learning task-agnostic Newtonian latent dynamics enables post-hoc DMP fitting without task-specific latent training.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Task-specific latent constraints can make DMP fitting trivial but reduce flexibility; NewtonianVAE aims for broader utility across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Embed DMP attractor dynamics into latent transition model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually: NewtonianVAE learns dynamics independently of DMP training and then fits DMPs afterwards, whereas VAE-DMPs bake DMP structure into learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embed to control: A locally linear latent dynamics model for control from raw images <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Deep variational Bayes filters: Unsupervised learning of state space models from raw data <em>(Rating: 2)</em></li>
                <li>PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations <em>(Rating: 2)</em></li>
                <li>Dynamic movement primitives in latent space of time-dependent variational autoencoders <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1424",
    "paper_id": "paper-2e0bc6cc4153025cd37d957cb896abe237502cd5",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "NewtonianVAE",
            "name_full": "Newtonian Variational Autoencoder",
            "brief_description": "A structured latent dynamics VAE that explicitly models stochastic positions and deterministic velocities with constrained (diagonal, signed) transition matrices to induce Newtonian (double-integrator) dynamics and enable proportional (P) controllability directly from pixels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NewtonianVAE",
            "model_description": "Variational autoencoder with an encoder q(x_t | I_t) producing stochastic latent positions x; velocity v is computed deterministically as finite differences v_t = (x_t - x_{t-1})/Δt; transition prior enforces discrete-time Newtonian dynamics via v_{t} update v_{t} = v_{t-1} + Δt*(A x_{t-1} + B v_{t-1} + C u_{t-1}) with A,B,C constrained to be diagonal and with sign constraints (C&gt;0, B&lt;0); generative model p(I_t | x_t) decodes latent positions to images; ELBO uses future-step reconstruction to encourage use of the transition prior.",
            "model_type": "latent world model (structured VAE latent dynamics)",
            "task_domain": "vision-based continuous control / robotic manipulation / imitation learning (point-mass, reacher-2D, fetch-3D, PR2 real robot)",
            "fidelity_metric": "Qualitative P-controllability (convergence under a proportional controller), reconstruction likelihood / ELBO and KL terms (training objective), and downstream task reward (environment reward in imitation).",
            "fidelity_performance": "Demonstrated P-controllability in all evaluated environments (point mass, reacher-2D, fetch-3D); achieved perfect task reward (3.0 ± 0.0) on the multi-goal imitation task using switching P-controllers derived from the latent (Table 1). No numeric ELBO / MSE values reported.",
            "interpretability_assessment": "High: latent position coordinates correspond directly to physical configuration (e.g., x,y or joint angles); learned goal latent vectors can be decoded to images, yielding visually-interpretable goals and task segmentation.",
            "interpretability_method": "Visualization of latent space colored by ground-truth state; latent-to-image decoding of inferred goal vectors; inspection of learned diagonal transition parameters and their signs.",
            "computational_cost": "Not quantified numerically; qualitatively reduces control-time computation by enabling simple P/PID controllers instead of sampling-based planning or additional policy optimization (eliminates need for costly MPC/RL for many tasks). Training follows standard VAE-style costs (neural encoder/decoder and small transition network).",
            "efficiency_comparison": "Qualitatively more data- and compute-efficient at inference/control compared to methods that require MPC or learned policies: one-shot or few-shot imitation (switching P-controller) succeeded where LSTM behaviour cloning and adversarial IRL (GAIL) needed more data or environment interactions.",
            "task_performance": "On sequential multi-goal imitation: switching P-controller in NewtonianVAE latent achieved reward 3.0 ±0.0 for 1, 10, and 100 demonstration settings (Table 1). Demonstrated robust path-following via DMPs in latent space for fetch-3D.",
            "task_utility_analysis": "High fidelity for task-relevant dynamics (Newtonian / P-controllable systems) translated to excellent task performance: latent proportionality allowed direct application of P/PID controllers, simplified imitation (goal inference), and enabled DMP-based trajectory following. The model prioritizes control-relevant structure over unconstrained representation.",
            "tradeoffs_observed": "Requires the true system to be well-approximated by Newtonian double-integrator dynamics and to be P-controllable; learns fixed goal states (not semantic, variable goals) with limited generalization to goal variations without more diverse demonstrations; assumes sufficiently high-speed vision for torque-level control in high-DoF manipulators.",
            "design_choices": "Stochastic latent x (position) + deterministic velocity via finite differences; diagonal transition matrices with sign constraints (C&gt;0, B&lt;0) to enforce disentanglement and correct directional relations between u, x, v; future-step reconstruction in ELBO to force use of transition prior; use of Gaussian encoder/decoder; decoder enables visual inspection of goals.",
            "comparison_to_alternatives": "Compared experimentally to E2C and a frame-wise VAE and an ablated Full-NewtonianVAE: only NewtonianVAE yielded P-controllable latents and stable convergence under simple P/PID controllers; E2C and Full-NewtonianVAE produced visually-structured latents but failed under direct P-control (needed MPC). Qualitatively outperformed LSTM behaviour cloning and GAIL for few-shot imitation efficiency when used with switching P-controllers.",
            "optimal_configuration": "Paper argues the combination of (1) explicit position latent with deterministic velocity derived from consecutive encodings, (2) diagonal signed transition matrices (A,B,C) with C&gt;0 and B&lt;0, and (3) future-step reconstruction in the ELBO, constitutes an effective configuration to balance interpretability, control-utility, and efficiency for Newtonian/P-controllable systems.",
            "uuid": "e1424.0",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Full-NewtonianVAE",
            "name_full": "Full-parameter NewtonianVAE (unconstrained variant)",
            "brief_description": "A variant ablation of NewtonianVAE in which the transition matrices A, B, C are allowed to be full (unconstrained) and unbounded (no sign/diagonality constraints), used to evaluate the importance of the structural constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Full-NewtonianVAE",
            "model_description": "Same VAE framework as NewtonianVAE but with A,B,C transitions implemented as full (dense) matrices without enforced diagonality or sign constraints; velocity still computed as finite differences of encoded positions.",
            "model_type": "latent world model (VAE latent dynamics, ablated)",
            "task_domain": "vision-based continuous control (same eval domains as NewtonianVAE: point mass, reacher-2D, fetch-3D)",
            "fidelity_metric": "P-controllability (convergence under P controller) and downstream imitation reward.",
            "fidelity_performance": "Empirically failed to provide P-controllability in experiments; systems using this variant diverged under direct P-control (as reported qualitatively). No numeric ELBO or rewards reported specifically for this variant.",
            "interpretability_assessment": "Lower than NewtonianVAE: although latent spaces appear visually structured and correlated with ground truth, they do not guarantee the directional/decoupled relations needed for intuitive proportional control.",
            "interpretability_method": "Latent space visualization colored by ground-truth; comparison of control rollouts under P-controller.",
            "computational_cost": "Not quantified; same order as NewtonianVAE for training but may require additional control computation (MPC) at inference due to lack of P-controllability.",
            "efficiency_comparison": "Less efficient for control than constrained NewtonianVAE because it typically requires more complex control/planning (e.g., MPC) to stabilize or reach goals.",
            "task_performance": "Failed to converge reliably under simple P-control in the tested environments; no successful task metrics reported.",
            "task_utility_analysis": "Shows that relaxing structural constraints reduces immediate utility for simple proportional controllers even if latent appears structured; indicates that imposing physically-motivated constraints improves task utility.",
            "tradeoffs_observed": "Removing diagonality and sign constraints can allow more expressive transitions but at the cost of losing guaranteed proportional controllability and interpretability; expressivity vs. control-compatibility tradeoff.",
            "design_choices": "Ablation: remove diagonality and sign constraints on A,B,C to evaluate their role.",
            "comparison_to_alternatives": "Compared to NewtonianVAE, it underperforms for direct P-control though could in principle model more complex transitions; compared to E2C, similar failure to be directly P-controllable despite visually-structured latents.",
            "optimal_configuration": "Paper's experiments indicate that the constrained (diagonal, signed) transition matrices are preferable when the goal is P-controllability and interpretable latent coordinates.",
            "uuid": "e1424.1",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "E2C",
            "name_full": "Embed to Control (E2C)",
            "brief_description": "A VAE-style latent dynamics model that enforces locally linear transitions in latent space (locally-linear dynamic system) to enable planning and control from images.",
            "citation_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "mention_or_use": "use",
            "model_name": "E2C (Embed to Control)",
            "model_description": "VAE with locally linear latent transition priors (conditioned on latent state) that enable locally linearized control planning; uses learned linearization matrices to predict next latents.",
            "model_type": "latent world model (locally-linear latent dynamics)",
            "task_domain": "vision-based control / simulated continuous control domains",
            "fidelity_metric": "Visual latent-space correlation with ground truth; control success measured by convergence under applied controllers and downstream task reward when using planning (MPC) rather than simple P-control.",
            "fidelity_performance": "Latent coordinates can correlate with ground-truth positions but in the paper E2C failed to be directly P-controllable: naive P-controller rollouts diverged (Fig. 1 & Fig. 3). It can be stabilized using MPC (CEM planning) but numerical fidelity metrics are not provided.",
            "interpretability_assessment": "Moderate: some latent dimensions correspond to physical variables but identification requires inspection (no automatic disentanglement guaranteed).",
            "interpretability_method": "Plotting latent coordinates and checking correlation with ground-truth positions; manual inspection required to find position dims.",
            "computational_cost": "Requires planning (MPC/CEM) in latent space to achieve control in many cases, which increases inference-time computation compared to direct P-controller operation.",
            "efficiency_comparison": "Less control-time efficient than NewtonianVAE for tasks where simple proportional control is desired because it typically needs sampling-based planning (MPC) to succeed.",
            "task_performance": "Can achieve control when combined with MPC/planning (qualitative), but failed under simple P-control in the experiments; no specific reward numbers for E2C baseline are reported in the paper's imitation table.",
            "task_utility_analysis": "Provides useful latent representations but lacks the structured coupling between actions and latent positions needed for straightforward proportional controllers; useful when planning machinery is acceptable.",
            "tradeoffs_observed": "Locally-linear transitions give expressivity and enable planning, but do not guarantee direct proportional controllability or disentangled action-to-position coupling without additional structure.",
            "design_choices": "Locally linear latent transitions; VAE encoder/decoder; learned linearization for dynamics.",
            "comparison_to_alternatives": "Compared experimentally and found inferior for direct P-control relative to NewtonianVAE; can be used with MPC whereas NewtonianVAE allows direct P/PID control without planning.",
            "optimal_configuration": "Not explicitly provided in this paper; authors illustrate that additional structure (as in NewtonianVAE) is needed when the control objective is simple proportional control from pixels.",
            "uuid": "e1424.2",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Static VAE",
            "name_full": "Frame-wise Variational Autoencoder",
            "brief_description": "Standard VAE applied independently to individual frames (no temporal dynamics enforced), used as a baseline to evaluate the importance of temporal/physical structure in latent dynamics.",
            "citation_title": "Auto-encoding variational bayes",
            "mention_or_use": "use",
            "model_name": "Frame-wise VAE (static VAE)",
            "model_description": "Per-frame encoder q(z|I) and decoder p(I|z) trained with standard ELBO; no explicit transition model or dynamics prior across time steps.",
            "model_type": "latent generative model (static VAE)",
            "task_domain": "vision representation learning; baseline for vision-based control experiments",
            "fidelity_metric": "Reconstruction likelihood / ELBO on frames; (in paper) indirect assessment by whether latents support P-control.",
            "fidelity_performance": "Did not produce latents that are P-controllable; failed under direct P-controller rollouts (qualitatively reported). No numeric ELBO/MSE reported.",
            "interpretability_assessment": "Low to moderate: may capture correlations with ground truth features but lack temporal structure hampers control interpretability.",
            "interpretability_method": "Latent-space visualization; indirect evaluation via downstream control performance.",
            "computational_cost": "Lower per-step complexity (no transition network), but inadequate for control without additional dynamics models, which would add computational cost.",
            "efficiency_comparison": "Less useful for control tasks relative to NewtonianVAE since it lacks temporal dynamics; would require secondary dynamics learning or policy learning to be useful for control.",
            "task_performance": "Failed to support direct P-control in evaluated tasks; no imitation reward numbers reported specifically for this baseline.",
            "task_utility_analysis": "Provides image reconstruction but insufficient world-modeling fidelity for simple control; demonstrates need for structured temporal dynamics for control-oriented world models.",
            "tradeoffs_observed": "Simplicity and lower modeling overhead come at the expense of utility for control tasks.",
            "design_choices": "Per-frame latent encoding without dynamics prior.",
            "comparison_to_alternatives": "Outperformed by NewtonianVAE for control-oriented tasks; worse than models that explicitly model transitions.",
            "optimal_configuration": "Not applicable; paper uses it as a baseline to show necessity of dynamics structure for proportional control.",
            "uuid": "e1424.3",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "DVBF",
            "name_full": "Deep Variational Bayes Filters (DVBF)",
            "brief_description": "A latent variable model that learns nonlinear, conditionally linear latent dynamics for time series via a variational Bayes filtering approach; referenced as related work for latent dynamics modeling from pixels.",
            "citation_title": "Deep variational Bayes filters: Unsupervised learning of state space models from raw data",
            "mention_or_use": "mention",
            "model_name": "Deep Variational Bayes Filters (DVBF)",
            "model_description": "Recurrent/filtered VAE formulation that learns latent state-space models with learned transition priors and amortized inference for sequential data; can model nonlinear dynamics in latent space.",
            "model_type": "latent world model (variational state-space model / VRNN-style filter)",
            "task_domain": "video prediction, dynamics modeling, control from pixels (general reference)",
            "fidelity_metric": "ELBO, predictive log-likelihood, reconstruction error and KL divergence (standard variational metrics); potential downstream control performance when used with planning algorithms.",
            "fidelity_performance": "Not evaluated quantitatively in this paper; referenced as similar to E2C in producing latents that are not directly P-controllable without additional structure.",
            "interpretability_assessment": "Generally limited unless additional structure is imposed; DVBFs recover latent dynamics but do not guarantee interpretable physical coordinates by default.",
            "interpretability_method": "Not specified in this paper; typical methods include latent visualization and probing.",
            "computational_cost": "Comparable to other recurrent variational models; training involves sequential inference and sampling via reparameterization, but no quantitative cost reported here.",
            "efficiency_comparison": "Not directly compared quantitatively in this paper; cited to motivate need for more structured latent models for proportional control.",
            "task_performance": "Not reported in this paper.",
            "task_utility_analysis": "Useful for learning latent dynamics but may require additional structure or downstream control algorithms (MPC/RL) to perform control tasks effectively.",
            "tradeoffs_observed": "Expressive latent dynamics vs. lack of guaranteed control-friendly structure.",
            "design_choices": "Variational filtering, learned transition priors, amortized inference for sequential data.",
            "comparison_to_alternatives": "Mentioned alongside E2C and Kalman VAE as models that recover structured conditionally-linear latent spaces but are unsuitable for direct proportional control per the authors' analyses.",
            "optimal_configuration": "Not discussed in this paper; authors argue structural inductive biases (as in NewtonianVAE) are needed when control via simple P-laws is desired.",
            "uuid": "e1424.4",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "PVEs",
            "name_full": "Position-Velocity Encoders (PVEs)",
            "brief_description": "An encoder that explicitly learns position and velocity representations from video by optimizing heuristic loss combinations; lacks a decoder and thus limits visual interpretability.",
            "citation_title": "PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations",
            "mention_or_use": "mention",
            "model_name": "Position-Velocity Encoders (PVEs)",
            "model_description": "Unsupervised learning approach that trains encoders to output explicit position and velocity latent variables using objectives that enforce temporal coherence and dynamics priors (heuristic loss terms); typically does not include a generative decoder.",
            "model_type": "structured latent representation learner (explicit physics-inspired encoder)",
            "task_domain": "video-based state representation for control / robotics",
            "fidelity_metric": "Correlation of learned latents with ground-truth positions/velocities; downstream control performance in prior work.",
            "fidelity_performance": "Not evaluated quantitatively in this paper; authors note PVEs learn explicit positional representations but the lack of a decoder prevents visual inspection and may limit other uses.",
            "interpretability_assessment": "High for latent variables (explicit position and velocity), but limited overall because PVEs lack a decoder for reconstructing or visualizing learned states in image space.",
            "interpretability_method": "Latent probing and heuristic losses that encourage disentanglement into position/velocity channels.",
            "computational_cost": "Not quantified here; PVEs are typically lightweight encoders but require designing hand-crafted loss terms.",
            "efficiency_comparison": "Trade-off: straightforward interpretability but require carefully tuned heuristic losses and lack generative decoding.",
            "task_performance": "Not reported in this paper.",
            "task_utility_analysis": "Good for obtaining explicit kinematic latents, but lack of decoder reduces a capability (visualizing inferred goals) that NewtonianVAE retains.",
            "tradeoffs_observed": "Explicit hand-crafted losses can produce desired latent structure but at the cost of relying on multiple heuristics and losing decoder-based interpretability.",
            "design_choices": "Explicitly enforce position and velocity latents via custom loss terms; omit decoder/generative component.",
            "comparison_to_alternatives": "PVEs yield explicit pos/vel latents like NewtonianVAE's design goal, but NewtonianVAE integrates those properties inside a probabilistic decoder-enabled VAE framework and enforces dynamics via transition priors rather than multiple heuristic losses.",
            "optimal_configuration": "Not specified here; paper notes PVEs' limitations motivate a decoder-based NewtonianVAE approach.",
            "uuid": "e1424.5",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "World Models",
            "name_full": "World Models",
            "brief_description": "A generative latent world-model approach (Ha & Schmidhuber) that learns a VAE-based vision model and RNN dynamics in latent space to enable policy learning/planning in the learned latent environment.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models (Ha & Schmidhuber)",
            "model_description": "Pipeline combining a VAE to compress image observations to latents, an RNN (MDN-RNN) to model latent dynamics, and a controller/policy trained in latent space; emphasizes compact learned simulators for RL.",
            "model_type": "latent world model (VAE + RNN dynamics + controller)",
            "task_domain": "reinforcement learning / policy learning from pixels (game and control domains)",
            "fidelity_metric": "Predictive accuracy of the RNN dynamics in latent space (MDN loss), reconstruction error of VAE, and downstream policy performance measured in environment reward.",
            "fidelity_performance": "Not measured in this paper; cited as related work illustrating latent dynamics models used for planning/control from pixels.",
            "interpretability_assessment": "Moderate: latents capture succinct scene summaries but are not guaranteed to be physically-interpretable coordinates like NewtonianVAE's latents.",
            "interpretability_method": "Latent visualization and decoding; policy behavior inspection in latent-produced rollouts.",
            "computational_cost": "Training involves VAE + RNN + policy search; in general requires significant compute but specific figures not given here.",
            "efficiency_comparison": "Approach typically requires learning a policy or controller in the learned simulator; paper contrasts this with NewtonianVAE's goal of eliminating extra controller learning by making latent directly amenable to simple P-controllers.",
            "task_performance": "Not reported here.",
            "task_utility_analysis": "Powerful for enabling policy learning via a compact learned simulator but may impose downstream computational costs (policy search/planning) that the NewtonianVAE seeks to avoid for proportional-control tasks.",
            "tradeoffs_observed": "Expressive latent world models enable complex policy learning but do not guarantee simple control laws are directly applicable; policy training adds complexity and data requirements.",
            "design_choices": "VAE encoder/decoder, MDN-RNN dynamics, separate policy learned in latent world.",
            "comparison_to_alternatives": "Contrasted qualitatively with NewtonianVAE: world models enable RL/policy learning in latent simulate but NewtonianVAE designs latents to be directly controllable via P/PID control.",
            "optimal_configuration": "Not discussed here; cited as inspirational prior work for latent dynamics approaches.",
            "uuid": "e1424.6",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "PlaNet / Hafner et al.",
            "name_full": "Learning latent dynamics for planning from pixels",
            "brief_description": "A model that learns latent dynamics suitable for planning (PlaNet) from pixels using a recurrent state-space model learned via variational inference and used for planning with CEM/MPC.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "mention",
            "model_name": "PlaNet / latent dynamics planning (Hafner et al.)",
            "model_description": "Recurrent latent dynamics model trained with variational objectives to support planning/control (often used with CEM or MPC) from pixel observations.",
            "model_type": "latent world model (recurrent state-space / model-based planning)",
            "task_domain": "vision-based planning and control from pixels",
            "fidelity_metric": "Predictive log-likelihood / ELBO, planning success (task reward) when combined with CEM/MPC.",
            "fidelity_performance": "Not reported in this paper; cited as a contrast to NewtonianVAE which aims to avoid planning overhead by making latent amenable to proportional control.",
            "interpretability_assessment": "Latent not necessarily physically-interpretable; emphasis on predictive performance for planning.",
            "interpretability_method": "Latent visualization sometimes used, but not a primary focus.",
            "computational_cost": "Requires planning (CEM/MPC) at inference or additional policy optimization; generally more compute at control time than simple proportional controllers.",
            "efficiency_comparison": "Less efficient at inference/control relative to NewtonianVAE for tasks where P-control suffices, because PlaNet-style models rely on planning steps.",
            "task_performance": "Not provided in this paper.",
            "task_utility_analysis": "Excellent for cases where planning in a predictive latent model is acceptable; NewtonianVAE aims for simpler controllers by embedding proportional dynamics.",
            "tradeoffs_observed": "Good predictive fidelity and planner compatibility but increased inference/solution complexity for control.",
            "design_choices": "Recurrent latent state-space model trained with variational inference; planning integrated at control time.",
            "comparison_to_alternatives": "Used as a representative of models that decouple dynamics and control and thus may carry higher computational burden at control time compared to NewtonianVAE.",
            "optimal_configuration": "Not specified in this paper.",
            "uuid": "e1424.7",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "VAE-DMPs",
            "name_full": "Dynamic Movement Primitives in latent VAE (VAE-DMPs)",
            "brief_description": "An approach that imposes DMP dynamics in the latent space of a time-dependent VAE to perform trajectory imitation and path following from lower-dimensional latents.",
            "citation_title": "Dynamic movement primitives in latent space of time-dependent variational autoencoders",
            "mention_or_use": "mention",
            "model_name": "VAE-DMPs",
            "model_description": "Time-dependent VAE whose latent dynamics are constrained or modelled according to DMP-style attractor dynamics so that trajectory-following controllers operate in latent space.",
            "model_type": "latent world model with embedded control dynamics (hybrid model)",
            "task_domain": "trajectory imitation / path following from visual demonstrations",
            "fidelity_metric": "Reconstruction/ELBO and DMP fit error to demonstrations; downstream trajectory-following success.",
            "fidelity_performance": "Not evaluated in this paper; cited as related work that also aims to enable DMPs in latent space but differs from NewtonianVAE by tying dynamics learning to specific DMP objectives.",
            "interpretability_assessment": "Can provide latent spaces suitable for DMP fitting but may be task-specific; interpretability depends on learned latent axes.",
            "interpretability_method": "Latent visualization and trajectory decoding.",
            "computational_cost": "Not quantified here; VAE-DMPs train for both generative and DMP components.",
            "efficiency_comparison": "VAE-DMPs specifically train for DMP compatibility; NewtonianVAE argues for learning dynamics independent of tasks so that DMPs can be fit post-hoc (more flexible).",
            "task_performance": "Not reported in this paper.",
            "task_utility_analysis": "VAE-DMPs achieve trajectory-following by construction; NewtonianVAE demonstrates that learning task-agnostic Newtonian latent dynamics enables post-hoc DMP fitting without task-specific latent training.",
            "tradeoffs_observed": "Task-specific latent constraints can make DMP fitting trivial but reduce flexibility; NewtonianVAE aims for broader utility across tasks.",
            "design_choices": "Embed DMP attractor dynamics into latent transition model.",
            "comparison_to_alternatives": "Compared conceptually: NewtonianVAE learns dynamics independently of DMP training and then fits DMPs afterwards, whereas VAE-DMPs bake DMP structure into learning.",
            "uuid": "e1424.8",
            "source_info": {
                "paper_title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "rating": 2
        },
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Deep variational Bayes filters: Unsupervised learning of state space models from raw data",
            "rating": 2
        },
        {
            "paper_title": "PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations",
            "rating": 2
        },
        {
            "paper_title": "Dynamic movement primitives in latent space of time-dependent variational autoencoders",
            "rating": 1
        }
    ],
    "cost": 0.02072325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h1>THE UNIVERSITY of EDINBURGH</h1>
<h2>Edinburgh Research Explorer</h2>
<h2>NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces</h2>
<h3>Citation for published version:</h3>
<p>Jaques, M, Burke, M &amp; Hospedales, TM 2021, NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces. in <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. Institute of Electrical and Electronics Engineers, pp. 4452-4461, IEEE Conference on Computer Vision and Pattern Recognition 2021, 19/06/21. https://doi.org/10.1109/CVPR46437.2021.00443</p>
<h3>Digital Object Identifier (DOI):</h3>
<p>10.1109/CVPR46437.2021.00443</p>
<h3>Link:</h3>
<p>Link to publication record in Edinburgh Research Explorer</p>
<h3>Document Version:</h3>
<p>Peer reviewed version</p>
<h3>Published In:</h3>
<p>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</p>
<h3>General rights</h3>
<p>Copyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s) and/or other copyright owners and it is a condition of accessing these publications that users recognise and abide by the legal requirements associated with these rights.</p>
<h3>Take down policy</h3>
<p>The University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorer content complies with UK legislation. If you believe that the public display of this file breaches copyright, please contact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately and investigate your claim.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<h1>NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces</h1>
<p>Miguel Jaques<br>University of Edinburgh<br>Edinburgh, UK<br>m.a.m.jaques@sms.ed.ac.uk</p>
<p>Michael Burke<br>Monash University<br>Melbourne, AU<br>michael.burke1@monash.edu</p>
<p>Timothy Hospedales<br>University of Edinburgh<br>Edinburgh, UK<br>t.hospedales@ed.ac.uk</p>
<h4>Abstract</h4>
<p>Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of visionbased controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.</p>
<h2>1. Introduction</h2>
<p>Vision-based control is highly desirable across numerous industrial applications, both in robotics and process control. At present, much practical vision-based control relies on supervised learning to build bespoke perception modules, prior to downstream dynamics modelling and controller design. This can be expensive and time consuming, and as a result there is growing interest in developing model-based approaches for direct visionbased control.</p>
<p>Model-based approaches for visual control tend to learn latent dynamics models that are subsequently used within suitable planning or model predictive control (MPC) frameworks, or to train policies for later use. We argue that this decoupling of dynamics and control is computationally expensive and often unnecessary. Instead we learn a structured latent dynamical model that directly allows for simple proportional control to be applied. Proportional-Integral-Derivative (PID) feedback control produces commands that are proportional to an
error or cost term between current system state $\mathbf{x}$ and a (potentially dynamic) target state $\mathbf{x}^{\text {goal }}$ :</p>
<p>$$
\begin{aligned}
\mathbf{u}<em p="p">{t}=K</em>}\left(\mathbf{x<em t="t">{t}^{\text {goal }}-\mathbf{x}</em>}\right)+K_{i} \sum_{t^{\prime}}\left(\mathbf{x<em t_prime="t^{\prime">{t^{\prime}}^{\text {goal }}-\mathbf{x}</em>\right)+ \
K_{d} \frac{\mathbf{x}}<em t-1="t-1">{t}-\mathbf{x}</em>
\end{aligned}
$$}}{\Delta t</p>
<p>Gain terms $\left(K_{p}, K_{i}, K_{d}\right)$ shape the controller response to errors. PID control is ubiquitous in industry, and broadly applicable across numerous domains, providing a simple and reliable off-the-shelf mechanism for stabilising systems. PID control is also the basis of a wide range of more powerful control strategies, including the more flexible dynamic movement primitives [24, 43] that augment PD control laws with a forcing function for trajectory following. Essentially we learn the state encoding $\mathbf{x}(I)$ from images $I$ for which robots can be trivially controlled from pixels according to Eq 1.</p>
<p>We structure latent dynamics so that that PID control can be applied to move between latent states, to remove the requirement for complex planning or reinforcement learning strategies. Moreover, we show that imitation learning from demonstrations becomes a simple goal inference problem under a proportional control model in this latent space, and can even be extended to sequential tasks comprising multiple sub-goals.</p>
<p>Imitation learning from high dimensional visual data is particularly challenging [2]. Behaviour cloning, which seeks to reproduce demonstrations, is particularly vulnerable to generalisation failures for high dimensional visual inputs, while inverse reinforcement learning (IRL) [38] strategies are hard to train and extremely sample inefficient. By learning a structured dynamics model, we allow for more robust control in the presence of noise and simplify the inverse reward inference process. In summary, the primary contributions of this work are:
Embedding for proportional controllability We induce a latent space where taking an action in the direction between the current position and some target</p>
<p>position, $\mathbf{u} \propto \mathbf{x}^{\text {target }}-\mathbf{x}$, moves the system towards the target position. Uniquely, this enables simple proportional control from pixels.
Imitation learning using latent switching proportional control laws We leverage the properties of this embedding to frame imitation learning as a goal inference problem under a switching proportional control law model in the structured latent space for sequential goal reaching problems. This enables one-shot interpretable imitation learning of switching controllers from high-dimensional pixel observations.
Imitation learning using dynamic movement primitives (DMPs) We also leverage the properties of our embedding to fit dynamic movement primitives in the structured latent space for trajectory tracking problems. This enables one-shot imitation learning of trajectory following controllers from pixels.</p>
<p>Results show that embedding for proportional controllability produces more interpretable latent spaces, allows for the use of simple and efficient controllers that cannot be applied with less structured latent dynamical models, and enables one-shot learning of control and interpretable goal identification in sequential multi-task imitation learning settings.</p>
<h2>2. Related Work</h2>
<p>This paper takes a model-based approach to visual control, using variational autoencoding (VAE) [28]. Latent dynamical systems modelling using autoencoding is widely used [32], and has been proposed for Bayesian filtering [15, 27, 31], and as inverse graphics for improved video prediction and vision-based control [25]. Ha and Schmidhuber [21] train a latent dynamics model using a variational recurrent neural network (VRNN) in the latent space of a VAE, and then learn a controller that acts in this space using a known reward model. Hafner et al. [22] extend this approach to allow planning from pixels. Unfortunately, because these approaches decouple dynamics modelling and control, they place an unnecessary computational burden on control, either requiring sampling-based planning or further RL policy optimisation. We argue that this burden can be alleviated by imposing additional structure on the latent space such that proportional control becomes feasible.</p>
<p>In doing so, we build on the control hypothesis advocated by Full and Koditschek [17], which seeks to model complex phenonoma and systems through simple template models and controllers, using anchor networks to abstract the complexity away from control. This also simplifies the challenges of imitation learning, allowing for sequential task composition [7].</p>
<p>The addition of structural inductive biases into neural models has become increasingly important for gener-
alisation. Injecting knowledge of known physical equations [20, 25] has been shown to improve dynamics modelling, while the inclusion of structured transition matrices was essential to learn Koopman operators [1] that model dynamical systems with compositional properties [35]. Here, a block-wise structure with shared blocks was used to learn transition dynamics, which highlighted the importance of added structure in linear state space models, but this was not applied to visual settings. Models like embed to control (E2C) [46] or deep variational Bayes filters (DVBF) [27] recover structured conditionally linear latent spaces which can be used for control, but, as will be demonstrated later, are still unsuitable for direct proportional control. PVEs [26] learn an explicit positional representation, but do so by minimizing a combination of several heuristic loss functions. Since these models do not use a decoder, it is not possible to visually inspect the learned representations in image space.</p>
<p>NewtonianVAE not only provides latent space interpretability, but also simplifies imitation learning. Inverse reinforcement learning (IRL) strategies for imitation learning typically struggle to learn from high dimensional observation traces as they tend to be based on the principle of feature counting and observation frequency matching [38], as in maximum entropy IRL [47]. Maximum entropy IRL has been extended to use a deep neural network feature extractor [47], but this is highly vulnerable to overfitting and has extensive data requirements. Recent adversarial IRL approaches [16, 18, 23] avoid the challenge of learning a global reward function by training policies directly, but these have yet to be successfully scaled to high dimensional problems. As a result, most imitation learning approaches tend to assume access to low dimensional states, avoiding the challenge of learning from pixels.</p>
<p>Behaviour cloning approaches using dynamic movement primitives (DMP) [24, 43] have proven particularly powerful for trajectory following control, but are typically applied to low-dimensional proprioceptive states directly as they require proportionally controllable state spaces. Deep DMPs [40] learn visually task parametrised DMPs, but the DMP itself still requires low dimensional state measurements. Chen et al. [8] propose VAE-DMPs, which impose DMP dynamics in the latent space of a variational auto-encoder, allowing for direct imitation learning. In contrast, this work learns dynamics models independently of tasks, which allows for more flexible downstream applications, including DMP fitting for trajectory following and switching multi-goal imitation learning from pixels (unlike Chen et al. [8], which use proprioception observations).</p>
<p>Standard imitation learning learning strategies can</p>
<p>fail in multi-goal settings or on more complex tasks. In order to address this, many approaches frame the problem of imitation learning from these lower level states as one of skill or options [30, 44] learning using switching state space models. These switching models include linear dynamical attractor systems [11], conditionally linear Gaussian models [9, 33], Bayesian non-parametrics [39, 41], and neural variational models [29]. Kipf et al. [29] learn task segmentations to infer compositional policies, but the model uses environment states directly instead of images. Burke et al. [5, 6] use a switching controller formulation for control law identification from image, proprioceptive state and control action observations. This work applies a similar strategy for goal inference, but, unlike the approaches above, makes use of a learned latent state representation and does not require proprioceptive or low level state information.</p>
<p>Despite this reliance on proprioceptive state information, there is a growing interest in direct visual imitation learning and control. Nair et al. [37] train a variational autoencoder (VAE) on image observations of an environment, and subsequently sample from this latent space in order to train goal-conditioned policies that can be used to move between different goal states. In contrast, we propose a latent dynamics model that allows for latent proportional controllability and eliminates the need to train a policy to move between goal states.</p>
<p>In addition to the works discussed above, a research area in the unsupervised learning literature of particular interest is that of learning physically plausible representations (from video) by enforcing temporal evolution according to explicit or implicit physical dynamics [4, 19, 25, 45]. Though promising, these approaches have only been applied to very simple toy environments where dynamics are well known, and are still to be scaled up to real world scenes.</p>
<h2>3. Variational models for visual control</h2>
<p>In order to learn a compact latent representation of videos that can be used for planning and control we use the variational autoencoder framework (VAE) [28, 42] and its recurrent formulation (VRNN), [10]. In this section we briefly present a general formulation of the VRNN, of which many recent models are particular cases or variations [15, 22, 27, 31, 46]. For derivation details please refer to [10].</p>
<p>Given a sequence of $T$ images, $\mathbf{I}<em 1:T="1:T">{1:T}$, and actuations $\mathbf{u}</em>} \in \mathbb{R}^{d_{u}}$ and the corresponding latent representations, $\mathbf{z<em z="z">{1: T} \in \mathbb{R}^{d</em>$, the marginal image likelihood is given by:}</p>
<p>$$
p\left(\mathbf{I}<em 1:="1:" T="T">{1: T} \mid \mathbf{u}</em>}\right)=\int p\left(\mathbf{I<em 1:="1:" T="T">{1: T} \mid \mathbf{z}</em>}, \mathbf{u<em 1:="1:" T="T">{1: T}\right) p\left(\mathbf{z}</em>} \mid \mathbf{u<em 1:="1:" T="T">{1: T}\right) \mathrm{d} \mathbf{z}</em>
$$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 1: Trajectory of a point mass actuated using $\mathbf{u}<em t="t">{t} \propto\left(\mathbf{x}^{g o u l}-\mathbf{x}</em>\right)$ (left) in the latent space learned by an E2C model (right).
where we factorize the terms above as:</p>
<p>$$
\begin{aligned}
p\left(\mathbf{I}<em 1:="1:" T="T">{1: T} \mid \mathbf{z}</em>}, \mathbf{u<em t="t">{1: T}\right) &amp; =\prod p\left(\mathbf{I}</em>} \mid \mathbf{z<em 1:="1:" T="T">{t}\right) \
p\left(\mathbf{z}</em>} \mid \mathbf{u<em t="t">{1: T}\right) &amp; =\prod p\left(\mathbf{z}</em>} \mid \mathbf{z<em t-1="t-1">{t-1}, \mathbf{u}</em>\right)
\end{aligned}
$$</p>
<p>with an approximate positerior given by:</p>
<p>$$
q\left(\mathbf{z}<em 1:="1:" T="T">{1: T} \mid \mathbf{I}</em>}\right)=\prod q\left(\mathbf{z<em t="t">{t} \mid \mathbf{I}</em>}, \mathbf{z<em t-1="t-1">{t-1}, \mathbf{u}</em>\right)
$$</p>
<p>The model components are trained jointly by maximizing the lower bound on (2):</p>
<p>$$
\begin{aligned}
\mathcal{L}= &amp; \sum_{t} \mathbb{E}<em t="t">{q\left(\mathbf{z}</em>} \mid \mathbf{I<em t-1="t-1">{t}, \mathbf{z}</em>}, \mathbf{u<em t="t">{t-1}\right)}\left[p\left(\mathbf{I}</em>} \mid \mathbf{z<em t_1="t+1">{t}\right)+\right. \
&amp; \left.+\operatorname{KL}\left(q\left(\mathbf{z}</em>} \mid \mathbf{I<em t="t">{t+1}, \mathbf{z}</em>}, \mathbf{u<em t_1="t+1">{t}\right) | p\left(\mathbf{z}</em>} \mid \mathbf{z<em t="t">{t}, \mathbf{u}</em>\right)\right)\right]
\end{aligned}
$$</p>
<p>via the reparametrization trick, by drawing samples from the posterior distributions, $q\left(\mathbf{z}<em t="t">{t} \mid \mathbf{I}</em>}, \mathbf{z<em t-1="t-1">{t-1}, \mathbf{u}</em>}\right)$. Under this framework, the various desired inductive biases are usually built into the structure of the transition prior $p\left(\mathbf{z<em t="t">{t+1} \mid \mathbf{z}</em>\right)$. In this work we will build on the formulation that uses a linear dynamical system as latent dynamics:}, \mathbf{u}_{t</p>
<p>$$
p\left(\mathbf{z}<em t="t">{t+1} \mid \mathbf{z}</em>}, \mathbf{u<em t="t">{t}\right)=A\left(\mathbf{z}</em>}\right) \cdot \mathbf{z<em t="t">{t}+B\left(\mathbf{z}</em>}\right) \cdot \mathbf{u<em t="t">{t}+\mathbf{c}\left(\mathbf{z}</em>\right)
$$</p>
<p>which has been studied extensively in the context of deep probabilistic models [3, 15, 27, 31, 36].</p>
<h2>4. Newtonian Variational Autoencoder</h2>
<p>Motivation To motivate our model, we begin by examining the properties of an existing latent variable model used for control. We train an E2C model [46], since it applies a locally linear latent transition as in (5) and is highly representative of properties obtained in these types of model. We use a simple point mass system that can move in the $[x, y]$ plane and train the model on random transitions in image space (more details in the experiments section). Since the environment is 2 D with 2 D controls, we use a 4 D latent space ( 2 dimensions for position and 2 for velocity). Our goal is to explore how the E2C model behaves when a basic proportional</p>
<p>control law $\mathbf{u}<em t="t">{t} \propto\left(\mathbf{x}^{\text {goal }}-\mathbf{x}</em>$ is the latent system configuration.}\right)$ is applied, where $\mathbf{x</p>
<p>An immediate problem is that even though the latent coordinates corresponding to position are correctly learned (Fig. 1(right)), it is necessary to plot every coordinate pair and their correlation with ground truth positions in order to visually determine which 2 coordinates correspond to the position $\mathbf{x}$. Having determined such $\mathbf{x}$, we can use a random target position $\mathbf{x}^{\text {goal }}$ and see if successively applying an action $\mathbf{u}<em t="t">{t} \propto\left(\mathbf{x}^{\text {goal }}-\mathbf{x}</em>$ (which we term proportional controllability). Note that PID control is trivially achievable given a P-controllable system, so we focus on P-control for simplicity of exposition, without loss of generality. Fig. 1(left) shows that this simple control law fails to guide the system towards the goal state, even though the latent space is seemingly well structured. These problems are present in existing variational models for controllable systems, including E2C [46], DVBF [27] and the Kalman VAE [15].}\right)$ will guide the system towards $\mathbf{x}^{\text {goal }</p>
<p>To avoid the need for ground truth data and visual inspection, we construct a model that explicitly treats position and velocity as separate latent variables $\mathbf{x}$ and $\mathbf{v}$. To ensure correct behaviour under a proportional control law ${ }^{1}$ the change in position and velocity should be directly related to the force applied. I.e. given an external action $\mathbf{u}$ representing the force (=acceleration) acting on a system, $\mathbf{x}$ and $\mathbf{v}$ should follow Newton's second law, $d^{2} \mathbf{x} / d t^{2}=\mathbf{F} / m$. Although this might seem like a trivial statement from a physical standpoint, this type of behaviour is not built into existing neural models, where the relationship between action and latent states can be arbitrary. This arbitrary relationship in turn complicates control, and it becomes necessary to learn downstream controllers or policies to compensate for these dynamics while meeting a control objective.</p>
<p>We make one additional observation: in many cases the external action $\mathbf{u}$ is applied along disentangled dimensions of the system. For example, for a 2 -arm robot, actions correspond to torques on the angles of each arm relative to its origin ${ }^{2}$. These action dimensions correspond to the polar coordinates $\left[\theta_{1}, \theta_{2}\right]$, which are the ideal disentangled coordinates to describe such a robot. We use this fact to formulate a model that not only provides an interpretable and P-controllable latent space, but also the correct disentanglement by construction.
Formulation We now formulate a model satisfying the above desiderata. For an actuated rigid body systems with $D$ degrees of freedom, we model the system</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>configuration (positions or angles) by a set of coordinates $\mathbf{x} \in \mathbb{R}^{D}$ with double integrator dynamics, inspired by Newton's equations of motion:</p>
<p>$$
\frac{d \mathbf{x}}{d t}=\mathbf{v}, \frac{d \mathbf{v}}{d t}=A(\mathbf{x}, \mathbf{v}) \cdot \mathbf{x}+B(\mathbf{x}, \mathbf{v}) \cdot \mathbf{v}+C(\mathbf{x}, \mathbf{v}) \cdot \mathbf{u}
$$</p>
<p>To build a discrete form of (6) into a VAE formulation, we use the instantaneous system configuration (or position) $\mathbf{x}$ as the stochastic variable that is inferred by the approximate posterior, $\mathbf{x}<em t="t">{t} \sim q\left(\mathbf{x}</em>} \mid \mathbf{I<em t="t">{t}\right)$, with velocity a deterministic variable that is simply the finite difference of positions, $\mathbf{v}</em>}=\left(\mathbf{x<em t-1="t-1">{t}-\mathbf{x}</em>\right) / \Delta t$. The generative model is now given by</p>
<p>$$
\begin{aligned}
p\left(\mathbf{I}<em 1:="1:" T="T">{1: T} \mid \mathbf{x}</em>}, \mathbf{u<em t="t">{1: T}\right) &amp; =\prod p\left(\mathbf{I}</em>} \mid \mathbf{x<em 1:="1:" T="T">{t}\right) \
p\left(\mathbf{x}</em>} \mid \mathbf{u<em t="t">{1: T}\right) &amp; =\prod p\left(\mathbf{x}</em>} \mid \mathbf{x<em t-1="t-1">{t-1}, \mathbf{u}</em>\right)
\end{aligned}
$$} ; \mathbf{v}_{t</p>
<p>where the transition prior is:</p>
<p>$$
\begin{aligned}
&amp; p\left(\mathbf{x}<em t-1="t-1">{t} \mid \mathbf{x}</em>}, \mathbf{u<em t="t">{t-1} ; \mathbf{v}</em>}\right)=\mathcal{N}\left(\mathbf{x<em t-1="t-1">{t} \mid \mathbf{x}</em>}+\Delta t \cdot \mathbf{v<em t="t">{t}, \sigma^{2}\right) \
&amp; \mathbf{v}</em>}=\mathbf{v<em t-1="t-1">{t-1}+\Delta t \cdot\left(A \mathbf{x}</em>}+B \mathbf{v<em t-1="t-1">{t-1}+C \mathbf{u}</em>\right)
\end{aligned}
$$</p>
<p>with $[A, \log (-B), \log C]=\operatorname{diag}\left(f\left(\mathbf{x}<em t="t">{t}, \mathbf{v}</em>}, \mathbf{u<em t="t">{t}\right)\right)$, where $f$ is a neural network with linear output activation. Using diagonal transition matrices encourages correct coordinate relations between $\mathbf{u}, \mathbf{x}$ and $\mathbf{v}$, since linear combinations of dimensions are eliminated. In order to obtain the correct directional relation between $\mathbf{u}$ and $\mathbf{x}$, required for interpretable controllability, we set $C$ to be strictly positive (in addition to diagonal). $B$ is strictly negative to provide a correct interpretation of the term in $\mathbf{v}$ as friction, which aids trajectory stability. During inference, $\mathbf{v}</em>}$ is computed as $\mathbf{v<em t="t">{t}=\left(\mathbf{x}</em>}-\mathbf{x<em t="t">{t-1}\right) / \Delta t$, with $\mathbf{x}</em>} \sim q\left(\mathbf{x<em t="t">{t} \mid \mathbf{I}</em>}\right)$ and $\mathbf{x<em t-1="t-1">{t-1} \sim q\left(\mathbf{x}</em>} \mid \mathbf{I<em t="t">{t-1}\right)$. This inference model provides a principled way to infer velocities from consecutive positions, similarly to [26]. We use Gaussian $p\left(\mathbf{I}</em>} \mid \mathbf{x<em t="t">{t}\right)$ and $q\left(\mathbf{x}</em>\right)$ parametrized by a neural network throughout.} \mid \mathbf{I}_{t</p>
<p>We train all model components using the following ELBO (full derivation in Appendix A):</p>
<p>$$
\begin{aligned}
\mathcal{L} &amp; =\mathbb{E}<em t="t">{q\left(\mathbf{x}</em>} \mid \mathbf{I<em t-1="t-1">{t}\right) q\left(\mathbf{x}</em>} \mid \mathbf{I<em p_left_mathbf_x="p\left(\mathbf{x">{t-1}\right)}\left[\mathbb{E}</em><em t="t">{t+1} \mid \mathbf{x}</em>}, \mathbf{u<em t="t">{t} ; \mathbf{v}</em>}\right)} p\left(\mathbf{I<em t_1="t+1">{t+1} \mid \mathbf{x}</em>\right)+\right. \
&amp; \left.+\operatorname{KL}\left(q\left(\mathbf{x}<em t_1="t+1">{t+1} \mid \mathbf{I}</em>}\right) | p\left(\mathbf{x<em t="t">{t+1} \mid \mathbf{x}</em>}, \mathbf{u<em t="t">{t} ; \mathbf{v}</em>\right)\right)\right]
\end{aligned}
$$</p>
<p>A crucial component of this ELBO is performing futurerather than current-step reconstruction through the generative process (first term above). This is known to encourage the use of the transition prior when learning the latent representation $[22,27,46]$.
Further considerations Another key difference between a simple LDS and our Newtonian model is the fact that we consider velocity to be a deterministic latent variable that is uniquely determined by the stochastic</p>
<p>positions. In contrast, independent inference through $\mathbf{z}$ means that position and velocity might not have the direct relation that is present in the physical world (velocity as the derivative of position). Both of these contribute to a lack of physical plausability, in the Newtonian sense, in existing models. Though technically our transition prior is a special case of the LDS (5), these added structural constraints are crucial in order to induce a Newtonian latent space that directly allows for PID control of latent image states.</p>
<h2>5. Efficient Imitiation with P-Control</h2>
<p>A key benefit of the Newtonian latent space is that it dramatically simplifies image-based imitation learning. Given a visual demonstration sequence $D_{\mathbf{I}}=\left{\left(\mathbf{I}<em 1="1">{1}, \mathbf{u}</em>}\right), \ldots,\left(\mathbf{I<em T="T">{T}, \mathbf{u}</em>}\right)\right}$, we encode the frames using the inference network $q(\mathbf{x} \mid \mathbf{I})$ described above in order to produce demonstrations in latent space, $D_{\mathbf{x}}=\left{\left(\mathbf{x<em 1="1">{1}, \mathbf{u}</em>}\right), \ldots,\left(\mathbf{x<em T="T">{T}, \mathbf{u}</em>\right)\right}$.</p>
<h3>5.1. Learning Vision-Driven Switching P-Control</h3>
<p>We can fit a switching P -controller ${ }^{3}$ to a set of demonstration sequences in latent space using a Mixture Density Network (MDN), where the action likelihood given a state is a mixture of $N$ proportional controllers:
$P\left(\mathbf{u}<em t="t">{t} \mid \mathbf{x}</em>}\right)=\sum_{n=1}^{N} \pi_{n}\left(\mathbf{x<em t="t">{t}\right) \mathcal{N}\left(\mathbf{u}</em>} \mid K_{n}\left(\mathbf{x<em t="t">{n}^{\text {goal }}-\mathbf{x}</em>\right)$
where $K_{n}, \mathbf{x}}\right), \sigma_{n}^{2<em n="n">{n}^{\text {goal }}$ and $\sigma</em>$ will correspond to the intermediate goals or bottleneck states in the demonstration sequence. As an added benefit, we can pass the learned goals through NewtonianVAE's decoder in order to obtain their visual representation, providing an interpretable control policy.
Learning a finite-state machine Having identified the latent vectors corresponding to the goals, we determine the order in which they must be reached by analysing their visits during the demonstrations, directly extracting initiation sets and termination conditions. This produces a simple finite-state machine (FSM) that determines goal state transitions. The FSM and extracted P-controllers can then be used to reproduce demonstrated behaviours by driving the robot to}^{2}, \forall n \in 1 . . N$, are learnable parameters, and $\boldsymbol{\pi}(\mathbf{z})$ is a parametric function like a neural network. Intuitively, fitting this MDN to the latent demonstrations splits the demonstrations into regions where a specific proportional controller would correctly fit that part of the trajectory. If the latent space is P-controllable (such as the one produced by the NewtonianVAE), the vectors $\mathbf{x}_{n}^{\text {goal }</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 2: Latent spaces of various models in the point mass, reacher-2D and fetch-3D environments. Each dot corresponds to the latent representation of a test frame, and the red-to-green color coding encodes the true 2D position/angle values. For E2C [46], we plot the two latent dimensions that best correlated with the true positions. Since the configuration space of the fetch-3D env is 4 D , we visualize only the first two coordinates. Only for our NewtonianVAE does latent space (position) and true space (color) correlate perfectly.
each goal in succession, but could also be used within an options framework [44] for reinforcement learning.</p>
<h3>5.2. Learning Visual Path Following with DMPs</h3>
<p>It is clear that the latent space of a NewtonianVAE can be used for switching goal-based imitation learning, but proportionality is also a precursor for trajectory following using DMPs. A DMP [24] is a proportionalderivative controller with a learned forcing function</p>
<p>$$
\tau \ddot{\mathbf{x}}=\alpha\left(\beta\left(\mathbf{x}^{\text {goal }}-\mathbf{x}\right)-\dot{\mathbf{x}}\right)+\mathbf{f}
$$</p>
<p>Here, $\tau$ is a time scaling constant, and $\alpha, \beta$ are proportional control gain terms. The forcing function</p>
<p>$$
\mathbf{f}<em i="1">{t}=\sum</em>}^{N} \frac{\Phi(t) \mathbf{w<em i="1">{i}}{\sum</em>\right)
$$}^{N} \Phi(t)}\left(\mathbf{x}-\mathbf{x}^{\text {goal }</p>
<p>captures trajectory dynamics, using a weighted linear combination of radial basis functions, $\Phi(t)=$ $\exp \left(-\frac{1}{\sigma_{i}^{2}}\left(y-c_{i}\right)^{2}\right)$, with centres $c_{i}$ and variances $\sigma_{i}^{2}$.</p>
<p>The canonical system $\dot{y}=-\alpha_{y} y$ gently decays over time, smoothly modulating the forcing function until reaching an end goal, $\mathbf{x}^{\text {goal }}$. Basis functions and parameters are fit to demonstration trajectories using weighted linear regression. Since the NewtonianVAE embeds for proportionality, DMPs can be fit directly to the latent space from demonstration data, allowing for vision-based trajectory control and path following.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 3: Left: P-control trajectories for point mass, reacher-2D and fetch-3D environments. Plots are in the latent space of Fig. 2. We can see that only NewtonianVAE produces a latent space where a P-controller correctly leads the systems from the initial to goal state. Right: Convergence rates of PID control using various latent embeddings for the point mass (left) and reacher-2D (right) systems, over 50 episodes. We use gain parameters K<sup>p</sup> = 8, K<sup>i</sup> = 2, K<sup>d</sup> = 0.5. For contrast, we show Model Predictive Control (MPC, using CEM planning [22]).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 4: Left: Demonstration sequence and learned mixture of P-controllers (MDN). Each background color and corresponding diamond correspond to a component π<sup>n</sup>(x) and x<sup>goal</sup><sub>π</sub>, ∀n ∈ {1, 2, 3}, respectively. Right: Rollouts after imitation learning using switching P-controllers and LSTM policy, with a single demonstration sequence. In the noisy regime each action has an added noise N(0, 0.25<sup>2</sup>). All plots are in the NewtonianVAE's latent space.</p>
<h2>6. Experiments</h2>
<p>We validate our model on 3 simulated continuous control environments, to allow for better evaluation and ablations, and on data collected from a real PR2 robot.</p>
<h3>Point mass</h3>
<p>A simple point mass system adapted from the PointMass environment from dm_control. The mass is linearly actuated in the 2D plane and its movement bounded by the edges of the frame.</p>
<h3>Reacher-2D</h3>
<p>A 2D reacher robot adapted from the Reacher environment in dm_control and inspired by [29]. We alter the environment so that the robot's middle joint can only bend in one direction, in order to prevent the existence of two possible arm configurations for every end effector position. We also limit the origin joint angle range to [−160, 160] so that the system configuration can be described in polar coordinates by two variables corresponding to the angle of each arm, avoiding a discontinuity in case of full circular motion.</p>
<h3>Fetch-3D</h3>
<p>The 3D reacher environment FetchReachEnv from OpenAI Gym. We use this to show that our model learns the desirable representations even in visually rich 3D environments of multi-joint robots with partial occlusions.</p>
<p>To train the models, we generate 1000 random sequences with 100 time-steps for the point mass and reacher-2D systems, and 30 time-steps for the fetch-3D system. More implementation details for each of the environments can be found in Appendix B.</p>
<h3>Baseline models</h3>
<p>We compare our model to E2C<sup>4</sup> and a static VAE (each frame encoded individually). Additionally, in order to better understand the effect of diagonality and positivity of the transition matrices in (10), we test Full-NewtonianVAE, where the matrices</p>
<p><sup>4</sup>DBVF [27] and E2C learn similar latent spaces, as both rely on an unstructured conditionally linear dynamical system.</p>
<p>$A, B, C$ are unbounded and full rank. Architecture and training details can be found in Appendix B.</p>
<h3>6.1. Visualizing latent spaces and P-controllability</h3>
<p>In this section we compare the latent space and Pcontrollability properties of the NewtonianVAE and baseline models on the simulated enviroments: point mass, reacher-2D and fetch-3D.
Comparing latent spaces We start by visualizing the latent spaces learned by each models on all the environments. Fig. 2 shows that only the NewtonianVAE is able to learn a representation corresponding to the natural disentangled coordinates in both environments (e.g. $[x, y]$ in the point mass and $\left[\theta_{1}, \theta_{2}\right]$ in the reacher-2D), and that these are correctly correlated with ground-truth values, coded in the red-green spectrum. This shows that the structure imposed on the transition matrices in (10) is key to learning correct latent spaces in both Cartesian and polar coordinates. P-controllability Even though the models above produce different latent spaces, most are well structured and show a clear correlation with the ground truth state (color coded). Although their structure is visually appealing, we are primarily interested in verifying is whether they satisfy P-controllability. To do this, we sample random starting and goal states, and successively apply the control law $\mathbf{u}<em t="t">{t} \propto\left(\mathbf{x}\left(\mathbf{I}^{\text {goal }}\right)-\mathbf{x}</em>\right)$ in the limit of many time-steps. For reference, we also apply model-predictive control to E2C.}\left(\mathbf{I}_{t}\right)\right)$. A space is deemed P-controllable if the system moves to $\mathbf{x}\left(\mathbf{I}^{\text {goal }</p>
<p>Convergence curves in the true state space are shown in Fig. 3, along with example rollouts in the learned latent space (more examples in Appendix C). We can see that only NewtonianVAE produces P-controllable latent states, as all the remaining models diverge under a P controller. This highlights the fact that even though the latent spaces learned by the Full-NetwtonianVAE and E2C are seemingly well structured for the point mass system, they fail to provide P-controllability. While these systems can still be stabilised using more complex control schemes such as MPC, this is entirely unnecessary with a P-controllable latent space, where trivial control laws can be applied directly.</p>
<h3>6.2. MDN goal and boundary visualization</h3>
<p>Having trained a NewtonianVAE on a dataset of random transitions we can use the learned representations to fit the mixture of P-controllers in (12) to the few-shot demonstration sequences.
Reacher-2D In this environment there are three colored balls in the scene and the task is reaching the three balls in succession, where the arm's starting location varies across demonstration sequences. We used the
true reacher model with a custom controller to generate demonstration images. A full demonstration sequence is shown in Appendix B. For this experiment we use a linear $\boldsymbol{\pi}(\mathbf{x})$, though a MLP yields similar results.</p>
<p>After fitting (12) on a single demonstration sequence, we visualize the goals $\mathbf{x}^{\text {goal }}$ and the decision boundaries of the switching network $\boldsymbol{\pi}(\mathbf{x})$ in Fig. 4(left). The figure shows that goal states are correctly identified (diamond markers), and that the three sub-task regimes are correctly segmented. Decoding $\mathbf{x}^{\text {goal }}$, confirms that the goals are correctly represented in image space, adding a layer of interpretability to an upstream control policy.
Imitation learning performance We now compare various imitation learning methods in the simulated task described above. A reward of 1.0 is given when the system reaches a neighborhood of each target (as measured in the true system state), but the targets must be reached in sequence. A more detailed description of the task can be found in Appendix B. Our method (switching P-controller) uses a finite-state machine inferred from the MDN trained on latent demonstrations (Fig. 4(left)). We compare it to behaviour cloning with an LSTM with 50 recurrent units, in the NewtonianVAE's latent space, and GAIL [23], a state-of-the-art IRL method trained on ground truth proprioceptive states. Table 1 shows the imitation efficiency for increasing numbers of demonstration sequences, with example rollouts shown in Fig. 4(right). The results show that goal-driven P-control in a hybrid control policy is significantly more data efficient and robust to noise than a standard behaviour cloning policy. Additionally, switching controllers dramatically outperform GAIL ${ }^{5}$, even though this was trained on 5 times the number of environment interactions used by the NewtonianVAE.
Real multi-object reacher We now apply our model to real robot data. Here, we record a 7-DoF PR2 robot arm that moves between 6 objects in succession in a hexagon pattern. A full sequence comprises approximately 100 frames. We use 636 frames to train the NewtonianVAE and an additional 100 held-out frames to train the MDN. Further model and dataset details can be found in Appendix B.</p>
<p>Fig. 5 shows the image representations of the learned goals (left) and the mode $\boldsymbol{\pi}(\mathbf{x})$ that is active for each frame in the demonstration sequence (right). We can see that the six goals are correctly identified by the MDN, and that segmentations are correct in the sense that a frame is assigned to the learned goal to which the robot is moving at that time step. Note that the model is able to recover correct goals and segmentations even</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: Decoded goals (left) and sequence segmentation (right) learned for a 6-goal visual trajectory of a PR2 robot. The sequence shows 33 equally spaced frames of a 100-frame demonstration.</p>
<table>
<thead>
<tr>
<th>Demonstration sequences</th>
<th>Switching P-controller Clean Noisy</th>
<th>LSTM Clean Noisy</th>
<th>GAIL from proprioception</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3.0 ± 0.0</td>
<td>2.17 ± 0.32</td>
<td>0.81 ± 0.35 0.27 ± 0.20</td>
</tr>
<tr>
<td>10</td>
<td>3.0 ± 0.0</td>
<td>2.01 ± 0.34</td>
<td>3.00 ± 0.00 1.42 ± 0.34</td>
</tr>
<tr>
<td>100</td>
<td>3.0 ± 0.0</td>
<td>2.06 ± 0.30</td>
<td>3.00 ± 0.00 1.23 ± 0.30</td>
</tr>
</tbody>
</table>
<p>Table 1: Efficiency of imitation learning methods for vision-based sequential multi-task control. Metric: Environment Reward (max = 3.0). The NewtonianVAE is used to encode the frames. 'Noisy': Added action noise N(0,0.25²) during the rollouts. Error ranges: 95% confidence interval across 100 rollouts. GAIL is trained for 5000 episodes.</p>
<p>though not all of the joints are visible in every frame.</p>
<h3>6.3. Fitting DMPs for path following in latent space</h3>
<p>We show how the NewtonianVAE can be used to enable a robot to learn a vision-driven controller to follow a demonstration trajectory, using the fetch-3D environment. To this end, we draw a 'G'-shaped trajectory in the first 2 dimensions of the latent space and fit a DMP. The DMP runs in 100 time-steps, spanning 4 seconds of execution, where we feed the acceleration output by the DMP as the action to the environment, and the new state and velocity is inferred by the NewtonianVAE.</p>
<p>Fig. 6 shows that the robot correctly follows the demonstration trajectory, showing that the latent space induced by the NewtonianVAE enables path following using a DMP just by virtue of its P-controlability property, without needing to be explicitly trained to perform well under a DMP, as done by [8].</p>
<h2>7. Discussion</h2>
<p><strong>Limitations and Future Work</strong> This work assumes that underlying systems are proportional controllable, and follow Newtonian dynamics. Moreover, it should be noted that vision-based torque control of high dimensional robot manipulators requires high speed vision. However, in our opinion, the most notable limitation is the fact that the imitation learning model only learns a fixed set of goals. Ideally, the agent would learn a semantic goal, which would represent a command "fetch the yellow ball", for a variable position of the yellow ball and not a fixed state. However, this would require demonstration data with substantially more</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Left: Overhead view of demonstration and trajectory produced by the DMP in the fetch-3D environment. The first 2 dimensions of the NewtonianVAE's latent space are shown. Right: Frames seen by the NewtonianVAE during this rollout.</p>
<p>variety than considered here. We have also avoided multi-modal demonstrations for simplicity, though we believe it would be of interest to integrate our method with approaches like InfoGAIL [34].</p>
<p><strong>Conclusion</strong> We introduced NewtonianVAE, a structured latent dynamics model designed to allow P-controllability from pixels. Results show that this structured latent space allows for trivial, robust control in the presence of noise and dramatically simplifies and improves imitation learning, which can be framed either as a switching goal-inference or as a path following problem in the latent space. Additionally, our model provides visually interpretable goal discovery and task segmentation under both simulated and real environments, without any labelled or proprioception data.</p>
<h2>References</h2>
<p>[1] I. Abraham, G. De, L. Torre, and T. D. Murphey. Model-Based Control Using Koopman Operators. In $R S S, 2017$.
[2] J. A. D. Bagnell. An invitation to imitation. Technical Report CMU-RI-TR-15-08, Carnegie Mellon University, Pittsburgh, PA, March 2015.
[3] P. Becker-Ehmck, J. Peters, and P. Van Der Smagt. Switching Linear Dynamics for Variational Bayes Filtering. In ICML, 2019.
[4] F. D. A. Belbute-Peres, K. A. Smith, K. R. Allen, J. B. Tenenbaum, and J. Z. Kolter. End-to-End Differentiable Physics for Learning and Control. In NIPS, 2018.
[5] M. Burke, Y. Hristov, and S. Ramamoorthy. Hybrid system identification using switching density networks. In CoRL, 2019.
[6] M. Burke, S. Penkov, and S. Ramamoorthy. From explanation to synthesis: Compositional program induction for learning from demonstration. In RSS, 2019.
[7] R. R. Burridge, A. A. Rizzi, and D. E. Koditschek. Sequential composition of dynamically dexterous robot behaviors. The International Journal of Robotics Research, 18(6):534-555, 1999.
[8] N. Chen, M. Karl, and P. Van Der Smagt. Dynamic movement primitives in latent space of time-dependent variational autoencoders. In 2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids), pages 629-636. IEEE, 2016.
[9] S. Chiappa and J. R. Peters. Movement extraction by detecting dynamics switches and repetitions. In NIPS, 2010.
[10] J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, and Y. Bengio. A Recurrent Latent Variable Model for Sequential Data. Advances in Neural Information Processing Systems, 2015.
[11] K. R. Dixon and P. K. Khosla. Trajectory representation using sequenced linear dynamical systems. In ICRA, 2004.
[12] R. C. Dorf and R. H. Bishop. Modern control systems. Pearson, 2011.
[13] L. Duc, A. Ilchmann, S. Siegmund, and P. Taraba. On stability of linear time-varying second-order differential equations. Quarterly of applied mathematics, 64(1): $137-151,2006$.
[14] C. Finn, P. Christiano, P. Abbeel, and S. Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016.
[15] M. Fraccaro, S. Kamronn, U. Paquet, and O. Winther. A disentangled recognition and nonlinear dynamics model for unsupervised learning. In NIPS, 2017.
[16] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. 2018.
[17] R. J. Full and D. E. Koditschek. Templates and anchors: neuromechanical hypotheses of legged locomotion on land. Journal of experimental biology, 202(23):33253332, 1999.
[18] S. K. S. Ghasemipour, R. Zemel, and S. Gu. A divergence minimization perspective on imitation learning methods. CoRL, 2019.
[19] S. Greydanus, M. Dzamba, and J. Yosinski. Hamiltonian Neural Networks. In NeurIPS, 2019.
[20] V. L. Guen and N. Thome. Disentangling Physical Dynamics from Unknown Factors for Unsupervised Video Prediction. In CVPR, 2020.
[21] D. Ha and J. Schmidhuber. World models. In NeurIPS, 2018.
[22] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. In ICML, 2019.
[23] J. Ho and S. Ermon. Generative adversarial imitation learning. In NIPS, 2016.
[24] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal. Dynamical movement primitives: learning attractor models for motor behaviors. Neural computation, 25(2):328-373, 2013.
[25] M. Jaques, M. Burke, and T. Hospedales. Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video. In ICLR, 2020.
[26] R. Jonschkowski, R. Hafner, J. Scholz, and M. Riedmiller. PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations. CoRR, abs/1705.09805, 2017.
[27] M. Karl, M. Soelch, J. Bayer, and P. Van Der Smagt. Deep variational Bayes filters: Unsupervised learning of state space models from raw data. In ICLR, 2018.
[28] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014.
[29] T. Kipf, Y. Li, H. Dai, V. Zambaldi, A. SanchezGonzalez, E. Grefenstette, P. Kohli, and P. Battaglia. CompILE: Compositional Imitation Learning and Execution. In ICML, 2019.
[30] G. Konidaris and A. G. Barto. Skill discovery in continuous reinforcement learning domains using skill chaining. In NIPS, 2009.</p>
<p>[31] R. G. Krishnan, U. Shalit, and D. Sontag. Deep Kalman Filters. arXiv preprint arXiv:1511.05121, 2015.
[32] T. Lesort, N. Díaz-Rodríguez, J.-F. Goudou, and D. Filliat. State representation learning for control: An overview. Neural Networks, 108:379-392, 2018.
[33] S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In NIPS, 2014.
[34] Y. Li, J. Song, and S. Ermon. InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations. In NIPS, 2017.
[35] Y. Li, H. He, J. Wu, D. Katabi, and A. Torralba. Learning compositional koopman operators for modelbased control. In $I C L R, 2020$.
[36] S. W. Linderman, M. J. Johnson, A. C. Miller, R. P. Adams David M Blei Liam Paninski Harvard, and G. Brain. Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems. In AISTATS, 2017.
[37] A. V. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine. Visual reinforcement learning with imagined goals. In NeurIPS, 2018.
[38] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In ICML, 2000.
[39] S. Niekum and A. G. Barto. Clustering via dirichlet process mixture models for portable skill discovery. In NIPS. 2011.
[40] A. Pervez, Y. Mao, and D. Lee. Learning deep movement primitives using convolutional neural networks. In 2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids), pages 191-197. IEEE, 2017.
[41] P. Ranchod, B. Rosman, and G. Konidaris. Nonparametric bayesian reward segmentation for skill discovery using inverse reinforcement learning. In IROS, 2015.
[42] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.
[43] S. Schaal. Dynamic movement primitives-a framework for motor control in humans and humanoid robotics. In Adaptive motion of animals and machines, pages 261-280. Springer, 2006.
[44] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112 (1-2):181-211, 1999.
[45] P. Toth, D. J. Rezende, A. Jaegle, S. Racanière, A. Botev, and I. Higgins. Hamiltonian Generative Networks. In $I C L R, 2020$.
[46] M. Watter, J. T. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In NIPS, 2015.
[47] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In $A A A I, 2008$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Maximum Entropy IRL performed equally poorly, failing to reach a single goal. This is unsurprising, due to the connections between this and adversarial imitation learning [14].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
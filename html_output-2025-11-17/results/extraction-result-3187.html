<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3187 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3187</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3187</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-260438773</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.01542v1.pdf" target="_blank">Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents</a></p>
                <p><strong>Paper Abstract:</strong> The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as chatGPT. These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user. However, these agents have limited memory and can be distracted by irrelevant parts of the conversation. While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns. In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents. By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should `see' the conversation.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3187.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3187.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Sandbox</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive system and design probe that exposes conversational memory as explicit, editable 'memory objects' that users can view, toggle, edit, summarize, reorder, delete, and share across conversations to control what an LLM-powered agent 'sees'. Implemented with a web UI and GPT-3.5 turbo as the backend LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory Sandbox (LLM-powered conversational agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A conversational agent implemented as part of the Memory Sandbox system that uses GPT-3.5 turbo for response generation; the system places an external, user-manipulable memory layer (memory objects) between the user and the model so that the agent's context can be explicitly controlled by the user.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external interactive memory objects (summarized / curated external memory separate from raw context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Conversation snippets are represented as discrete 'memory objects' stored outside the LLM context; users can toggle visibility (which controls whether a memory object is included in the agent's input), edit/add/delete objects, drag/reorder them, and request LLM-based abstractive summaries to distill and replace sets of objects. When visible, selected memory objects are supplied to the GPT-3.5 turbo prompt so the model uses them as context for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Conversational context management / dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintaining relevant conversational context across an ongoing dialogue and multi-conversation scenarios; main challenges are long histories, irrelevant context distracting the model, and user lack of transparency/control over what the agent remembers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue / conversational memory management</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exposing and making memory editable (as objects) provides users with affordances to align the agent's memory with user expectations, enabling selective visibility, summarization, editing and cross-conversation sharing; proposed to improve transparency and user control though no quantitative evaluation is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No user studies or quantitative evaluations reported yet; integration and UX trade-offs (implicit vs explicit memory management) and potential scaling/usability challenges remain noted as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3187.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3187.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The large language model used as the backend in the Memory Sandbox implementation to generate responses and to perform summarization of memory objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-3.5 turbo</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer-based large language model provided via the OpenAI API used for response generation and for generating abstractive summaries of selected memory objects in the Memory Sandbox system.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window (standard LLM input buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The model receives as input the set of conversation turns and any selected/visible memory objects as part of the prompt; it does not itself maintain an external persistent memory beyond its input context window in this implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue response generation and abstractive summarization of memory objects</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate contextually appropriate conversational responses and produce abstractive summaries of selected conversation-history snippets to serve as condensed memory objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue / summarization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as an engine to (1) generate chat responses conditioned on explicit, user-controlled memory objects, and (2) create summaries of grouped memory objects; the paper does not report quantitative model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No quantitative evaluation of how the model's performance changes when memory objects are used vs omitted; general limitations of context window length and distraction by irrelevant context are discussed as background issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3187.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3187.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Default LLM convo memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Default conversational LLM memory strategy (use of conversational history within the context window)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The common baseline practice where conversational agents simply include as much recent conversational history as fits within the LLM's input buffer; parts exceeding the buffer are effectively forgotten.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Default LLM conversational agent (context-window-based memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generic conversational LLM behavior that appends recent conversation turns to the prompt up to the model's token limit; no external structured memory is maintained by default.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores recent conversation as raw tokens in the model input; when history exceeds input limits older parts are dropped; no explicit external storage, summarization, or selective retrieval is guaranteed unless layered by additional systems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain dialogue (general conversational use)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate contextually relevant responses in an ongoing conversation; main challenge is retaining relevant long-term context while avoiding distraction from irrelevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper notes that relying purely on the full buffer leads to forgetting when limits are exceeded and that as buffer size grows models may degrade on retrieving relevant context and be distracted by irrelevant context (citing related work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Forgetting of content beyond buffer limits; performance degradation and susceptibility to distraction as context length increases; users lack transparency about what is actually used by the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Lost in the Middle: How Language Models Use Long Contexts <em>(Rating: 2)</em></li>
                <li>Large language models can be easily distracted by irrelevant context <em>(Rating: 2)</em></li>
                <li>Beyond goldfish memory: Long-term open-domain conversation <em>(Rating: 2)</em></li>
                <li>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory <em>(Rating: 2)</em></li>
                <li>Keep me updated! memory management in long-term conversations <em>(Rating: 2)</em></li>
                <li>Less is more: Learning to refine dialogue history for personalized dialogue generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3187",
    "paper_id": "paper-260438773",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "Memory Sandbox",
            "name_full": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents",
            "brief_description": "An interactive system and design probe that exposes conversational memory as explicit, editable 'memory objects' that users can view, toggle, edit, summarize, reorder, delete, and share across conversations to control what an LLM-powered agent 'sees'. Implemented with a web UI and GPT-3.5 turbo as the backend LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Memory Sandbox (LLM-powered conversational agent)",
            "agent_description": "A conversational agent implemented as part of the Memory Sandbox system that uses GPT-3.5 turbo for response generation; the system places an external, user-manipulable memory layer (memory objects) between the user and the model so that the agent's context can be explicitly controlled by the user.",
            "memory_used": true,
            "memory_type": "external interactive memory objects (summarized / curated external memory separate from raw context window)",
            "memory_mechanism_description": "Conversation snippets are represented as discrete 'memory objects' stored outside the LLM context; users can toggle visibility (which controls whether a memory object is included in the agent's input), edit/add/delete objects, drag/reorder them, and request LLM-based abstractive summaries to distill and replace sets of objects. When visible, selected memory objects are supplied to the GPT-3.5 turbo prompt so the model uses them as context for generation.",
            "task_name": "Conversational context management / dialogue",
            "task_description": "Maintaining relevant conversational context across an ongoing dialogue and multi-conversation scenarios; main challenges are long histories, irrelevant context distracting the model, and user lack of transparency/control over what the agent remembers.",
            "task_type": "dialogue / conversational memory management",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Exposing and making memory editable (as objects) provides users with affordances to align the agent's memory with user expectations, enabling selective visibility, summarization, editing and cross-conversation sharing; proposed to improve transparency and user control though no quantitative evaluation is reported in this paper.",
            "limitations_or_challenges": "No user studies or quantitative evaluations reported yet; integration and UX trade-offs (implicit vs explicit memory management) and potential scaling/usability challenges remain noted as future work.",
            "uuid": "e3187.0",
            "source_info": {
                "paper_title": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "The large language model used as the backend in the Memory Sandbox implementation to generate responses and to perform summarization of memory objects.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GPT-3.5 turbo",
            "agent_description": "A transformer-based large language model provided via the OpenAI API used for response generation and for generating abstractive summaries of selected memory objects in the Memory Sandbox system.",
            "memory_used": false,
            "memory_type": "context window (standard LLM input buffer)",
            "memory_mechanism_description": "The model receives as input the set of conversation turns and any selected/visible memory objects as part of the prompt; it does not itself maintain an external persistent memory beyond its input context window in this implementation.",
            "task_name": "Dialogue response generation and abstractive summarization of memory objects",
            "task_description": "Generate contextually appropriate conversational responses and produce abstractive summaries of selected conversation-history snippets to serve as condensed memory objects.",
            "task_type": "dialogue / summarization",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Used as an engine to (1) generate chat responses conditioned on explicit, user-controlled memory objects, and (2) create summaries of grouped memory objects; the paper does not report quantitative model performance.",
            "limitations_or_challenges": "No quantitative evaluation of how the model's performance changes when memory objects are used vs omitted; general limitations of context window length and distraction by irrelevant context are discussed as background issues.",
            "uuid": "e3187.1",
            "source_info": {
                "paper_title": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Default LLM convo memory",
            "name_full": "Default conversational LLM memory strategy (use of conversational history within the context window)",
            "brief_description": "The common baseline practice where conversational agents simply include as much recent conversational history as fits within the LLM's input buffer; parts exceeding the buffer are effectively forgotten.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Default LLM conversational agent (context-window-based memory)",
            "agent_description": "Generic conversational LLM behavior that appends recent conversation turns to the prompt up to the model's token limit; no external structured memory is maintained by default.",
            "memory_used": false,
            "memory_type": "context window",
            "memory_mechanism_description": "Stores recent conversation as raw tokens in the model input; when history exceeds input limits older parts are dropped; no explicit external storage, summarization, or selective retrieval is guaranteed unless layered by additional systems.",
            "task_name": "Open-domain dialogue (general conversational use)",
            "task_description": "Generate contextually relevant responses in an ongoing conversation; main challenge is retaining relevant long-term context while avoiding distraction from irrelevant content.",
            "task_type": "dialogue",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Paper notes that relying purely on the full buffer leads to forgetting when limits are exceeded and that as buffer size grows models may degrade on retrieving relevant context and be distracted by irrelevant context (citing related work).",
            "limitations_or_challenges": "Forgetting of content beyond buffer limits; performance degradation and susceptibility to distraction as context length increases; users lack transparency about what is actually used by the agent.",
            "uuid": "e3187.2",
            "source_info": {
                "paper_title": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Lost in the Middle: How Language Models Use Long Contexts",
            "rating": 2
        },
        {
            "paper_title": "Large language models can be easily distracted by irrelevant context",
            "rating": 2
        },
        {
            "paper_title": "Beyond goldfish memory: Long-term open-domain conversation",
            "rating": 2
        },
        {
            "paper_title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
            "rating": 2
        },
        {
            "paper_title": "Keep me updated! memory management in long-term conversations",
            "rating": 2
        },
        {
            "paper_title": "Less is more: Learning to refine dialogue history for personalized dialogue generation",
            "rating": 1
        }
    ],
    "cost": 0.0089435,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents
3 Aug 2023</p>
<p>Ziheng Huang z8huang@ucsd.edu 
University of California-San Diego San Diego
CAUSA</p>
<p>Sebastian Gutierrez 
Temple University Philadelphia
PAUSA</p>
<p>Hemanth Kamana 
Temple University Philadelphia
PAUSA</p>
<p>Stephen Macneil stephen.macneil@temple.edu 
Temple University Philadelphia
PAUSA</p>
<p>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents
3 Aug 20237EDFEE9FF6F2F8EF40AC59A75B05700FarXiv:2308.01542v1[cs.HC]Human-AI InteractionLarge Language ModelsChatbots
The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as chatGPT.These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user.However, these agents have limited memory and can be distracted by irrelevant parts of the conversation.While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns.In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents.By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should 'see' the conversation.CCS CONCEPTS• Computing methodologies → Intelligent agents; • Humancentered computing → Interactive systems and tools;</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) are currently capable of generating human-like responses in open-domain tasks [4].This has led to a new generation of conversational agents, such as chatGPT, which are now being widely used across domains.To ensure that agents generate responses that are contextually relevant and coherent to an ongoing conversation, these agents must maintain a working memory of the conversational history that has occurred up to that point in the conversation.The default strategy is to use as much of the conversational history as will fit within the input size limit of the LLM.Parts of the conversations that go beyond that buffer limit are forgotten, which leads to breakdowns when users assume the model remembers past context.Additionally, as the input buffer size increases, the performance of the LLM degrades as it struggles to retrieve relevant context and can be distracted by irrelevant context [11,18].This problem is compounded because users do not know how the LLM is leveraging the memory to generate responses.</p>
<p>Multiple strategies have been introduced to manage agents' conversational memory.For example, the conversation can be automatically summarized [21] and refined [24] to reduce redundancy while maintaining key information.Some systems selectively store [12,22] and update [1] key memories.Relevant memories can also be retrieved based on the user input [1,15,21].However, these memory management strategies are hidden behind the interface, resulting in a lack of transparency.Users often do not know what strategy is being used and have limited control over it.This makes it difficult for users to repair conversational breakdowns that happen when there is a misalignment between how the agent manages the memory and how the user perceives the conversation.</p>
<p>We present Memory sandbox, shown in Figure 1, a system that allows users to see and manage the memory of conversational agents to align with user understanding of the conversation.Memory Sandbox transforms conversational memory, previously managed behind the user interface, into interactive memory objects within the interface.Users can manipulate the visibility and content of memory objects, spatially rearrange them, and share them across conversations.We make the following contributions: 1) The conceptualization of memory objects which makes conversational memory transparent and interactive and 2) The Memory Sandbox system that offers novel interaction affordances for users to view and manipulate the conversational memory of an intelligent agent.</p>
<p>SYSTEM OVERVIEW</p>
<p>Memory sandbox is a system that provides users with the ability to view and manipulate the memory model of an intelligent agent, resulting in a shared representation of their ongoing conversation.Memory Sandbox introduces the concept of a memory object, an interactive piece of conversational history that can be moved, edited, deleted, or combined with other memory objects through summarization.The interface is implemented in Next.js and uses the GPT-3.5 turbo model from the OpenAI API.Below we present the features of Memory Sandbox to help end users view and manage an LLM-powered agent's memory model.</p>
<p>View and manipulate memory objects</p>
<p>Explainable AI research seeks to help people form mental models of intelligent systems [17].Transparency of the inner workings of the system [6,23] and interactivity to probe and manipulate the Figure 1: Memory Sandbox is a system that enables users to see and manage the memory of conversational agents.Memory Sandbox provides the following interaction affordances: 1) toggle memory visibility, 2) add memory, 3) edit memory, 4) delete memory, 5) summarize memory, 6) create a new conversation, and 7) share memory.</p>
<p>system [16] have been demonstrated to help people interpret and interact with intelligent systems to achieve their goals.</p>
<p>Memory Sandbox makes the conversational memory explicit through the use of 'memory objects' which can be viewed and manipulated within the interface.This was inspired by prior work that 'objectifies' tools [2,3] and attributes [20] to enable flexibility, expressiveness, and direct manipulation.This results in a 'shared representation' [7,8] and common ground [5]-so what users see on the front-end is what an LLM would 'see' on the back-end.</p>
<p>Additionally, users can view, edit, add, and delete memory objects to directly control how the agent 'sees' the conversation.</p>
<p>Toggle memory object visibility</p>
<p>As a conversation grows, LLMs must increasingly rely on their memory management strategy to infer meaning from the conversation.However, in longer conversations, it is unclear what parts of the conversation are stored in memory or are attended to by the model [11].This results in a poor mental model for users and a lack of control over what context is maintained and used by the agent.</p>
<p>Memory Sandbox enables users to selectively hide or show memory objects to control what context is shared with the agent.When the user's intent changes or the conversational context switches, the user can toggle the visibility of memory objects to hide or show parts of the conversation.As a signifier, hidden memory objects are grayed out within the interface.</p>
<p>Curate memory objects</p>
<p>Discussants develop and refine their understanding as a conversation unfolds [5].Thus, Memory Sandbox provides controls for users to curate memory objects by editing an existing memory object to refine or update the context, deleting a memory object to remove completely irrelevant context, and adding a new memory object to supplement extra context.Additionally, the arrangement of context is shown to have a significant effect on how well LLMs are able to leverage relevant context [11].In Memory Sandbox, all the memory objects are draggable, allowing users to experiment and refine the ordering and placement of memory objects in a conversation.</p>
<p>Summarize memory objects</p>
<p>Reminiscent of how humans attend to key aspects in a conversation [14], abstractive summarization distills a large amount of information to provide essential elements to the agent.Yet, what is considered as 'key aspects' can vary for individuals, even in the same conversation [14].Memory Sandbox enables uses to select memory objects that are summarized by the LLM.The resulting memory object represents the previous conversation and can be further refined by the user.The original conversation can be viewed by clicking on the summary.</p>
<p>Share memory objects across conversations</p>
<p>Aligning with the goal of managing memory, Memory Sandbox also provides affordances for sharing memories across conversations.This offers a new way for users to engage with multiple agents outside of a single conversation thread.Unlike in conversations with people, the speaker doesn't need to repeat themselves in each conversation to establish a shared understanding.</p>
<p>Users can create and start multiple conversations with separate LLM-powered agents in the same 2D canvas.Memory objects can be shared and connected between conversations by dragging the memory object from one conversation to another.When dragging, memories are copied by reference to help the user identify the context source.</p>
<p>DISCUSSION</p>
<p>Conversing is a collaborative activity where participants develop common ground through summarizing the discussion, repairing breakdowns, and emphasizing or de-emphasizing shared ideas [5].Yet, existing chatbot interfaces do not provide affordances for understanding how the agent 'sees' the conversation.Additionally, users can not rely on a theory of mind.These aspects result in a poor mental model for users and potential misalignment in understanding where conversational breakdown can occur.</p>
<p>Memory Sandbox transforms previously implicitly managed conversational memory behind the interface into interactive memory objects on the interface, exposing full control over the memory model of the agent to end users.By selectively hiding, showing, and curating memory representation, we can give users more control over how the agent should "see" the conversation.In addition to curating memory in a single conversation, Memory Sandbox is also a design probe toward memory manipulation affordances for multi-agent interactions.By displaying multiple agents on the same screen and making memories interactive and draggable, Memory Sandbox allows end users to selectively control the shared or unique memory each agent contains.</p>
<p>Tools are beginning to emerge that focus on how users might interact with LLMs, including mapping UI affordances to an LLM [13], grounding human-AI collaboration in a shared artifact [9], providing templates to facilitate prompt generation [10], and decomposing complex prompts to facilitate debugging [19].In this paper, we presented Memory Sandbox an interactive system that probes the design space of interaction techniques for memory management of LLMs.Our future work includes user studies to evaluate the efficacy of these techniques and potential trade-offs for implicit vs explicit memory management</p>
<p>Sang-Woo Lee, Woomyoung Park, and Nako Sung. Sanghwan Bae, Donghyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, arXiv:2210.08750Keep me updated! memory management in long-term conversations. 2022. 2022arXiv preprint</p>
<p>Local tools: An alternative to tool palettes. James D Benjamin B Bederson, Allison Hollan, Jason Druin, David Stewart, David Rogers, Proft, Proceedings of the 9th annual ACM symposium on User interface software and technology. the 9th annual ACM symposium on User interface software and technology1996</p>
<p>Toolglass and magic lenses: the see-through interface. Eric A Bier, Maureen C Stone, Ken Pier, William Buxton, Tony D Derose, Proceedings of the 20th annual conference on Computer graphics and interactive techniques. the 20th annual conference on Computer graphics and interactive techniques1993</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Contributing to discourse. H Herbert, Edward F Clark, Schaefer, Cognitive science. 131989. 1989</p>
<p>Bringing transparency design into practice. Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike Haug, Heinrich Hussmann, 23rd international conference on intelligent user interfaces. 2018</p>
<p>Agency plus automation: Designing artificial intelligence into interactive systems. Jeffrey Heer, Proceedings of the National Academy of Sciences. 11662019. 2019</p>
<p>Principles of Mixed-Initiative User Interfaces. Eric Horvitz, 10.1145/302979.303030Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsPittsburgh, Pennsylvania, USA; New York, NY, USAAssociation for Computing Machinery1999CHI '99)</p>
<p>CausalMapper: Challenging designers to think in systems with Causal Maps and Large Language Model. Ziheng Huang, Kexin Quan, Joel Chan, Stephen Macneil, Proceedings of the 15th Conference on Creativity and Cognition. the 15th Conference on Creativity and Cognition2023</p>
<p>Promptmaker: Prompt-based prototyping with large language models. Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, Carrie J Cai, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, arXiv:2307.03172[cs.CL]Lost in the Middle: How Language Models Use Long Contexts. 2023</p>
<p>One chatbot per person: Creating personalized chatbots based on implicit user profiles. Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong, Ji-Rong Wen, Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. the 44th international ACM SIGIR conference on research and development in information retrieval2021</p>
<p>Stephen Macneil, Andrew Tran, Joanne Kim, Ziheng Huang, Seth Bernstein, Dan Mogil, arXiv:2307.01142Prompt Middleware: Mapping Prompts for Large Language Models to UI Affordances. 2023. 2023arXiv preprint</p>
<p>An experimental study of common ground in text-based communication. Victoria C John C Mccarthy, Andrew F Miles, Monk, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing Systems1991</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023. 2023arXiv preprint</p>
<p>Evaluating the interpretability of generative models by interactive reconstruction. Andrew Ross, Nina Chen, Elisa Zhao Hang, Elena L Glassman, Finale Doshi-Velez, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing Systems2021</p>
<p>Considerations on explainable AI and users' mental models. Heleen Rutjes, Martijn Willemsen, Wijnand Ijsselsteijn, CHI 2019 Workshop: Where is the Human? Bridging the Gap Between AI and HCI. Association for Computing Machinery, Inc2019</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, International Conference on Machine Learning. PMLR2023</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie , Proceedings of the 2022 CHI conference on human factors in computing systems. the 2022 CHI conference on human factors in computing systemsJun Cai. 2022</p>
<p>Objectoriented drawing. Haijun Xia, Bruno Araujo, Tovi Grossman, Daniel Wigdor, Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. the 2016 CHI Conference on Human Factors in Computing Systems2016</p>
<p>Beyond goldfish memory: Long-term open-domain conversation. Jing Xu, Arthur Szlam, Jason Weston, arXiv:2107.075672021. 2021arXiv preprint</p>
<p>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, Shihang Wang, 10.18653/v1/2022.findings-acl.207Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. Yunfeng Zhang, Rachel Ke Vera Liao, Bellamy, Proceedings of the 2020 conference on fairness, accountability, and transparency. the 2020 conference on fairness, accountability, and transparency2020</p>
<p>Less is more: Learning to refine dialogue history for personalized dialogue generation. Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, Ji-Rong Wen, arXiv:2204.081282022. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
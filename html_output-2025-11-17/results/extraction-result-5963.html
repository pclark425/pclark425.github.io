<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5963 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5963</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5963</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267759726</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.12408v1.pdf" target="_blank">ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation</a></p>
                <p><strong>Paper Abstract:</strong> The rapid advancement of Large Language Models (LLMs) has revolutionized various sectors by automating routine tasks, marking a step toward the realization of Artificial General Intelligence (AGI). However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user. In response, we propose ModelGPT, a novel framework designed to determine and generate AI models specifically tailored to the data or task descriptions provided by the user, leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able to provide tailored models at most 270x faster than the previous paradigms (e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV, and Tabular datasets attest to the effectiveness of our framework in making AI models more accessible and user-friendly. Our code is available at https://github.com/IshiKura-a/ModelGPT.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5963",
    "paper_id": "paper-267759726",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00492925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation
18 Feb 2024</p>
<p>Zihao Tang 
Zheqi Lv 
Shengyu Zhang 
Fei Wu 
Kun Kuang 
ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation
18 Feb 2024283711A37224187E1BAD7AF824F9B50FarXiv:2402.12408v1[cs.LG]
The rapid advancement of Large Language Models (LLMs) has revolutionized various sectors by automating routine tasks, marking a step toward the realization of Artificial General Intelligence (AGI).However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user.In response, we propose ModelGPT, a novel framework designed to determine and generate AI models specifically tailored to the data or task descriptions provided by the user, leveraging the capabilities of LLMs.Given user requirements, ModelGPT is able to provide tailored models at most 270x faster than the previous paradigms (e.g.all-parameter or LoRA finetuning).Comprehensive experiments on NLP, CV, and Tabular datasets attest to the effectiveness of our framework in making AI models more accessible and user-friendly.Our code is available at here.</p>
<p>Introduction</p>
<p>In recent years, advancements in Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023a) have significantly influenced people's daily life, showcasing their exceptional performance in automating mundane tasks (Zhao et al., 2023;Yang et al., 2023).</p>
<p>LLMs offer an all-in-one solution for versatile user requirements, which marks a step toward the realization of Artificial General Intelligence (AGI).</p>
<p>Despite their impressive abilities, LLMs still struggle to meet users' diverse requirements efficiently and conveniently.First, there is increasing interest in deploying LLMs independently for more private, secure, steady, and fast service (Staab et al., 2023;Yao et al., 2023).Yet, LLMs require tremendous resources for training and deployment, which could not be affordable for average users (Touvron et al., 2023a;OpenAI, 2023;Touvron et al., 2023b).For example, pretraining LLaMA-2-70B takes 1.7 million GPU hours on A100-80G and consumes 2.5 √ó 10 12 Joules of energy, and the inference stage consumes comparable electricity (Touvron et al., 2023b;Xu et al., 2024;Wu et al., 2022).Additionally, while user requirements vary individually, LLMs might not always achieve optimal results, especially in specialized areas such as legal, economic, and medical fields.In contrast, tailored small models tend to exhibit superior performance (Turc et al., 2019;Gunasekar et al., 2023;Fu et al., 2023;Hsieh et al., 2023).However, users might not have adequate expertise or enough time and resources to select and finetune these models, which discourages common users from AI techniques.Therefore, our research aims to (I) accommodate the diverse and specific needs of users and (II) simplify the utilization of AI models for the average user.</p>
<p>However, these pursuits face certain challenges.First, the diversity of user needs necessitates multi-granular model customization.The difference in requirements would lead to multi-granular changes in the model.While minor changes in user requirements may necessitate adjusting the parameters of the target model for better perfor-mance (Sagawa et al., 2020;Lv et al., 2023), significant task alterations might require changing the model's output dimensions or even its architecture.Second, to reduce the burden on users, the solution demands both general capabilities (e.g.task understanding) and precise customization, preferably without extensive training needs.Real-world situations often involve limited or no labeled data (Wang et al., 2021), sometimes only a simple task description.Without the ability of task understanding, one might fail to capture user needs precisely.Besides, while large models can efficiently understand and handle general tasks without finetuning, they demand substantial resources for deployment.Conversely, smaller models, although more effective in specific tasks and easy to deploy, require considerable time and expertise for finetuning to achieve optimal performance.Hence, how to provide optimal models easily and efficiently demands further consideration.</p>
<p>In this work, we aim to explore an approach to AGI by generating AI models to efficiently and conveniently meet users' diverse requirements.To address the above challenges, we propose that one could leverage the complementary strengths of general large models and specific small models.Specifically, we could unleash the capabilities of large models to capture user requirements.These requirements can then be used to generate customized small models for users.This framework, which involves using large models to create tailored small models, opens up new possibilities for realizing AGI.</p>
<p>We further instantiate the aforementioned concept, leading to the development of ModelGPT, as depicted in Figure 1.Specifically, given users' input (User Data, User Description, or both), ModelGPT constructs a prompt to utilize an LLM for summarizing the task, analyzing data patterns, and formatting it into a user requirement (just a single sentence).This requirement is then encoded into a latent variable and fed to Model Customizer to determine the most appropriate target model for the given task.Finally, ModelGPT decodes the latent variable into model parameters to get the tailored model, which could be directly used by users for prediction.For instance, as shown in Figure 1, for a text classification task of sentiment classification, users can provide data batches (User Data) or just describe the task (User Description).ModelGPT then constructs a prompt, interacts with LLM, and outputs User Requirement in Requirement Generator.Next, we generate the model architecture (Distil-BERT base (Sanh et al., 2019)) and its parameters according to the User Requirement.An optional finetune process can be undergone for better performance.Finally, users can predict their data with this tailored model.In short, our contributions can be summarized as follows:</p>
<p>‚Ä¢ We explore the approach to Artificial General Intelligence to provide tailored off-the-shelf AI models with little data, time, and expertise.‚Ä¢ We propose a novel and user-friendly framework Mod-elGPT, which leverages the ability of LLMs to summarize user requirements and translate them into tailored small models in just ONE forward pass.‚Ä¢ We conduct extensive experiments in NLP, CV, and tabular data.ModelGPT can generate tailored models 270x faster than previous methods, while still maintaining comparable performance.</p>
<p>We would like to emphasize that our investigations are still in their initial stages.The present implementation serves as a demonstration of the potential effectiveness of utilizing LLMs in creating tailored models.</p>
<p>We are open to and greatly value discussions and collaborative explorations in this emerging field, and we warmly invite anyone to join us in furthering this research.</p>
<p>Related Works</p>
<p>Large Language Models</p>
<p>In recent times, the field of natural language processing (NLP) has been significantly reshaped by the emergence of large language models (LLMs) like ChatGPT (Wang et al., 2019a), GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023a), and others.The concept of large language model arises from language model (Vaswani et al., 2017;Devlin et al., 2019), an algorithm used in natural language processing to predict the likelihood of a sequence of words occurring in a sentence.Characterized by deep architectures, billions of parameters, and tremendous training corpus, LLMs have drastically enhanced the ability of machines to understand, interpret, and generate human language (Naveed et al., 2023;Brown et al., 2020).</p>
<p>Upon their introduction, LLMs have quickly gained widespread attention and have been applied across various domains, including machine translation, text completion, conversational agents, and so on (Shen et al., 2023;Romera-Paredes et al., 2023).However, despite their impressive capabilities, LLMs come with their own set of challenges.</p>
<p>Research indicates that in certain specific areas, smaller models can outperform LLMs (Turc et al., 2019;Gunasekar et al., 2023;Fu et al., 2023).Moreover, due to the immense size and complexity of these models, they are often impractical for users to employ or fine-tune, particularly when faced with limitations in computational resources or technical expertise (Hu et al., 2022).</p>
<p>Hypernetworks</p>
<p>Hypernetwork, the model designed to output the weights of another model is first proposed by Ha et al. (2017).Since it only needs a single forward pass to output model parame-</p>
<p>Image</p>
<p>This is a multi-class classification task, where each image features office-related items with a shallow depth of filed, a characteristic effect of using a DSLR camera.</p>
<p>Next, I will give you a batch, act just as I instruct above.</p>
<p>[sentence] They drank the pub dry</p>
<p>Text</p>
<p>Mind that you should describe the batch as instructed above use ONLY 1 sentence.</p>
<p>Next, I will give you a batch, act just as I instruct above.This is dataset Iris.</p>
<p>[input] {{"SepalLen":5.1,"SepalWid":3.5, "PetalLen":1.4,"PetalWid":0.2}}[label] Iris-setosa ...</p>
<p>[input]</p>
<p>{{"SepalLengthCm":5.8,"SepalWidthCm ":2.7,"PetalLengthCm":4.1,"PetalWidthCm":1}}</p>
<p>[label] Iris-versicolor Tabular This is a task of tabular classification to recognize which type of iris plant the input is.ters, it provides a fast and efficient alternative to the vanilla pretrain-finetune paradigm.Given its unique capability, it has gained wide attention in various fields like recommendation system, natural language processing, and computer vision.Lv et al. (2023) proposes a framework DUET for efficient device model generalization, which uses hypernetwork to generate the MLP layers of device models for model personalization.Ivison et al. (2023) uses hypernetwork to encode task definitions into task-conditioned LoRA adapters (Hu et al., 2022) and applies them to LLMs.Alaluf et al. (2022) proposes HyperStyle, which learns to modulate StyleGAN's weights to faithfully express a given image in editable regions of the latent space.While most hypernetworks are mlps, recently, a few works discuss the potential of hypernetworks with more complex architectures, like GAN (Ratzlaff &amp; Li, 2019), ResNet (Alaluf et al., 2022).</p>
<p>Generated Model</p>
<p>Methodology</p>
<p>To enable users to generate tailored models with related ease, we propose a comprehensive and novel framework ModelGPT.The workflow of ModelGPT consists of 2 main modules: Requirement Generator and Model Customizer, as depicted in Figure 2. Requirement Generator takes User Data or User Description as input and outputs User Requirement, while Model Customizer translates User Requirement into an off-the-shelf AI model.In this section, we elaborate on the details of our framework.</p>
<p>Requirement Generator</p>
<p>Given User Data or User Description, Requirement Generator constructs prompts and utilizes LLM's API to summarize the task, analyze data patterns, and finally format them into one sentence: User Requirement r.</p>
<p>Effective prompt design is crucial for accurately distilling patterns from data.On the one hand, User Data might be insufficient, and the lack of data poses challenges to reflect the real distribution in users' scenarios.On the other hand, LLMs tend to highlight simpler patterns that are directly inferable from labels.With the help of User Description, LLM could focus more precisely on the proper and unique patterns.Generally, we summarize the requirements of prompt design as follows:</p>
<p>‚Ä¢ The type of the task must be pointed out in the final sentence.‚Ä¢ Data-specific information, if any, should be reflected in the final sentence, which must only focus on the data itself rather than the labels given.</p>
<p>The first one is fundamental: accurately capturing the task's nature, such as image classification or text regression, is essential for identifying an appropriate model.</p>
<p>Conversely, the second one is subtler, focusing on customizing the model to enhance performance.While a general model may perform adequately in standard scenarios, it often struggles with special data patterns like domain shifts in specific user contexts, leading to significant performance drops (Wang &amp; Deng, 2018;Zhou et al., 2022).Consequently, Requirement Generator must detect these data patterns present in the data, like spurious correlations between background elements and labels.</p>
<p>We will elaborate on these requirements by case studying in Section 4.3.1 and Appendix B.</p>
<p>Model Customizer</p>
<p>Given User Requirement, Model Customizer translates it into a tailored model.It is comprised of 2 sub-modules, Model Generator and Parameter Generator, responsible for architecture and parameter generation individually.</p>
<p>MODEL GENERATOR</p>
<p>Given User Requirement, Model Generator determines the architecture of the target model.For example, for a 31class image classification task, Model Generator would use a ResNet-50 model whose classifier's output dimension is 31, while for text regression, Model Generator would use a Distil-BERT whose classifier's output dimension is only 1.</p>
<p>Algorithm 1 Pseudo-code of Parameter Generator P (‚Ä¢; Œ∏ p )
Require: A = {(D i = {X i , Y i }, r i )} N i=1 Ensure: Œ∏ p = (Œ∏ e , Œ∏ m , Œ∏ g ) satisfies Equation (5) i ‚Üê 1 for = 0 to ‚ôØepoch do for (D i , r i ) in A do for batch in D i do
Obtain Œ∏ t with Equations ( 1) to (3) Use batch to compute the loss and update Œ∏ t Compute the difference ‚àÜŒ∏ t of Œ∏ t Use ‚àÜŒ∏ t to compute the gradients of Œ∏ p Update Œ∏ p end for end for Save best checkpoint according to Equation ( 5) end for</p>
<p>PARAMETER GENERATOR</p>
<p>Once the architecture of the target model T is determined, Parameter Generator P (‚Ä¢; Œ∏ p = (Œ∏ e , Œ∏ m , Œ∏ g )) generates the parameters Œ∏ t with User Requirement r ‚àà R as input.</p>
<p>Specifically, r is first encoded by a text encoder E(‚Ä¢; Œ∏ e ) : R ‚Üí R l√ód0 .l is the pre-set maximum sequence length, while d 0 is the size of the hidden dimension of the text encoder.Following previous solutions (Wang et al., 2019b) to get the sentence embedding z 0 ‚àà R d0 , we use the embedding of the first token [CLS] as shown in Equation ( 1):
z 0 = E(r; Œ∏ e )[0, :].
(1)</p>
<p>Then, the embedding is fed into a transformation block M (‚Ä¢; Œ∏ m ) : R d0 ‚Üí R d , which transforms it into the latent variable z ‚àà R d as shown in Equation ( 2):
z = M (z 0 ; Œ∏ m ).(2)
Next, we generate the parameters of the target model Œ∏ t with Module-Wise Parameter Generator G(‚Ä¢; Œ∏ g ) and aggregate them into the model T (‚Ä¢; Œ∏ t ) as shown in Equation (3):
Œ∏ t = G(z; Œ∏ g ).(3)
Generally, it is not practical to output the parameters of large-scale models directly for convergence issues (Dinh et al., 2022;Alaluf et al., 2022).As a result, we add LoRA adapters (Hu et al., 2022) into the model, generate their parameters, and finally obtain the target model by merging the adapters.For example, as to a certain weight in the target parameter like the weight of the classifier (a √ó b), G allocates a linear layer that takes the latent variable z as input, outputs the parameters as a a ‚Ä¢ b-dim vector, and then reshapes it into a √ó b.</p>
<p>The training process of this module is slightly different from the vanilla design, as is demonstrated in Algorithm 1.</p>
<p>While in application scenes, ModelGPT does not necessarily require users' data, to train ModelGPT, we provide sufficient task-requirement pairs
A = {(D i = {X i , Y i }, r i )} N i=1
. The overall process resembles pretraining.Given a certain requirement r i , following the above procedure, we get the parameter Œ∏ ti = P (r i ; Œ∏ p ) of the target model T .With model T and data D i = {X i , Y i }, we compute the loss of in Equation ( 4).The loss function l here depends on the type of the task.It could be Cross-Entropy for classification tasks and could be MSE for regression.
L i = E (x,y)‚àºDi l(T (x; Œ∏ ti ), y).
(4)</p>
<p>While the conventional training process updates the parameter Œ∏ ti with the computed loss to obtain a better one Œ∏ t ‚Ä≤ i , we move one step further: we compute the difference ‚àÜŒ∏ ti of each parameter in Œ∏ ti , use it to compute the gradients of P (‚Ä¢; Œ∏ p = (Œ∏ e , Œ∏ m , Œ∏ g )), and then update our model.To obtain the best performance on all task-requirement pairs, we require the average loss of the pairs to be minimal, as shown in Equation ( 5):
Œ∏p = arg min Œ∏p N i=1 L i .(5)
Generally, the number of data samples varies from task to task, which could result in an uneven number of taskrequirement pairs being generated and in turn lead to inadequate training for tasks with fewer data samples.Such an imbalance is detrimental to the model's overall performance.</p>
<p>To address this issue, we manually adjust the portions of each task during the construction of task-requirement pairs.</p>
<p>Experiments</p>
<p>Experiment Settings</p>
<p>To test the efficiency of our proposed framework, we conduct comprehensive experiments in three settings: NLP, CV, and tabular data.</p>
<p>NLP Here, we use GLUE Benchmark (Wang et al., 2019b), which has nine sentence-or sentence-pair language understanding tasks built on established existing datasets and selected to cover a diverse range of dataset sizes, text genres, and degrees of difficulty1 .Model Generator chooses Distil-BERT as the target model.</p>
<p>Tabular Data Here, we choose 10 famous tabular classification tasks: Iris (Unwin &amp; Kleinman, 2021), Heart Disease (Detrano et al., 1989), Wine (Aeberhard et al., 1994), Adult (Becker &amp; Kohavi, 1996), Breast Cancer (Street et al., 1993), Car Evaluation (Bohanec &amp; Rajkovic, 1988), Wine Quality (Cortez et al., 2009) Baselines Since we are the first work in this field, we compare our framework with 2 baselines: Finetune and LoRA.</p>
<p>In Finetune, we finetune the target model with all parameters by the training data in each task individually.Since our framework uses LoRA to reduce the complexity of our hypernetwork, we treat fine-tuning the target model with LoRA adapters in each task as the baseline LoRA.</p>
<p>The modules trained with LoRA are the same as ModelGPT.</p>
<p>In the experiment on tabular data, since the target model MLP is simple, ModelGPT directly outputs its weights, rather than using LoRA adapters.It is important to notice that while in the different tasks in one experiment, these baselines give different models, we ONLY need ONE ModelGPT to solve all the tasks in one experiment.</p>
<p>For our study, we introduce a variant of ModelGPT, designated as ModelGPT-F.This adaptation involves an additional step where, upon ModelGPT's generation of the target model, we perform a single epoch of finetuning involving all parameters, using consistent hyperparameter settings.Detailed hyperparameter configurations are available for reference in Appendix A.</p>
<p>Results and Observations</p>
<p>The results can be seen from Tables 1 to 3, underscoring the efficiency, and exceptional performance of our framework.</p>
<p>Here, we provide detailed discussions of our results.</p>
<p>ModelGPT yields progressively more significant speed gains as the size of the target model increases.Leveraging the power of hypernetworks, ModelGPT generates custom model weights in a single forward pass, eliminating the need for a resource-intensive and expertise-dependent training process.This approach yields progressively more significant speed gains over the conventional pretrain-finetune paradigm as the size of the target model increases.The</p>
<p>Methods</p>
<p>Acc Acc@3 Acc@5 Acc Acc@3 Acc@5 Acc Acc@3 Acc@5 Acc Acc@3 Acc@5 ‚ôØEpoch E2E Runtime (s) acceleration observed ranges from approximately 40x for a simple MLP (1K) to 270x for Distil-BERT base (66M), marking a 7-fold increase in efficiency.It is also interesting to find out that in the experiments of tabular data, LoRA is much slower than Finetune.The deficiency in speed stems from the target model.Since tabular tasks are rather simple, ModelGPT directly uses multi-layer perceptron, a very shallow neural network isomorphic to LoRA adapters.Hence, directly fine-tuning the target model is more efficient than using LoRA to fine-tune it.These results highlight Mod-elGPT's exceptional efficacy in producing tailored models, particularly for larger target architectures, demonstrating its potential as a transformative tool in model generation.</p>
<p>Inter-task knowledge empowers ModelGPT for enhanced model generation.Due to hypernetworks' limitations, ModelGPT cannot generate large models directly.Instead, our implementation for sizable target models is to generate LoRA adapters and merge them to construct the final models.This approach may initially seem at most comparable to the baseline LoRA.Yet, in practice, ModelGPT surpasses LoRA in all experiments and even outperforms Finetune in CV and tabular data tasks.This performance boost is largely attributable to the inter-task knowledge gleaned by ModelGPT.Although tasks within a single experiment differ, they share common knowledge.For example, in NLP experiments, both MRPC and QQP tasks focus on semantic equivalence between sentences, and in CV experiments, Amazon, DSLR, and Webcam involve similar classification tasks with unique data-specific characteristics.This observation is also confirmed by our zero-shot success in the Webcam task, where it outperforms LoRA without direct data access, relying solely on User Requirements.</p>
<p>ModelGPT not only generates well-performed models but also provides efficient initial weights.In our main experiments, we introduce a ModelGPT variant that undergoes a single-epoch finetuning of all parameters post-generation.This approach leads to more favorable outcomes, achieving an average performance improvement of 0.8 absolutely while only doubling the total time consumption.We provide detailed analyses of this observation in Section 4.3.2.</p>
<p>More Analyses</p>
<p>To further testify to the effectiveness of our framework, we provide detailed analyses of ModelGPT's prompt design and the capability of weight initialization.Here, we choose the prompt we use for CV experiments as an example.The first row is the basis of our prompt, leaving the last 2 lines filled with real data and task descriptions.We then provide two pairs of examples in the next rows.In the pair, the only difference between them is whether or not we provide task descriptions.The third column of these examples is the result LLM (GPT-4-vision-preview in this case) outputs.The green color texts are those reflecting the correct data-specific information, while the red ones are those reflecting the WRONG information and the gray ones are irrelevant information.</p>
<p>PROMPT DESIGN</p>
<p>Since ModelGPT directly uses LLM's API to summarize users' requirements, it is crucial to design proper prompts for them.As discussed in Section 3.1, the prompt should briefly tell the type of the task and the data-specific information hidden among the data.</p>
<p>As we stated in Section 3.1, due to various reasons such as lack of data, summarizing users' requirements often poses challenges.In response, we carefully design the prompt and incorporate users' knowledge (User Data or User Description) into it, resulting in better performance.</p>
<p>We offer a clear case study in our CV experiment, where all the domains are 31-class classification tasks, but each domain has its data-specific information.In Amazon, since the images are from the Amazon website, the images are presented against a white background.In DSLR and Webcam, they are products taken by a DSLR or webcam camera individually.In Appendix B, we provide a more detailed analysis of the prompt designing of all the datasets we use in the main experiment.</p>
<p>As shown in Figure 3, the first row is the basis of our prompt, which contains our requirement, one example for better reasoning, and the place (last 2 lines) to fill real data.</p>
<p>In the requirement part, we emphasize that LLM ought to succinctly identify the type of image processing task demonstrated (e.g., classification, detection) and point out the data-specific information.To avoid outputting common patterns that can be directly inferred from the labels like these images are office supplies, we reiterate our request in the following sentence, which is bolded in Figure 3.</p>
<p>In the example part, we provide an example and analyze it, following the idea of COT (Chain of Thought) (Wei et al., 2022) for better performance.</p>
<p>As we use DSLR domain in the example part of the prompt, we provide results of our prompts on Amazon and Webcam.</p>
<p>All the examples successfully point out the type of task.However, without prior knowledge, LLM usually fails to tell the data-specific information.The first pair of examples are from Webcam, where all the images are taken by a webcam camera.These images are typical of low resolution and light artifacts.User Description we provide is simple: we point out that these images are taken by a webcam camera.With this hint, LLM easily points out the characteristics of this domain.While, in contrast, without this hint, LLM outputs some unimportant or wrong descriptions.To be specific, since data are office items, this information applies to all the domains in Office-31, which is not data-specific information.LLM also mentioned "a shallow depth of field", which is the pattern of DSLR domain and wrong to Webcam.</p>
<p>The second pair comes from Amazon, where images are directly taken from the website with a plain background.With descriptions provided, LLM precisely catches the dataspecific information, while without it, LLM again provides unimportant information.</p>
<p>For more case studies, we refer readers to Appendix B.</p>
<p>CAPABILITY OF WEIGHT INITIALIZATION</p>
<p>In Section 4.2, we state that ModelGPT not only generates well-performed models but also provides efficient initial weights.To further testify to this viewpoint, we finetune the target model (ResNet-50) with all parameters under the same hyperparameter setting to Finetune, with the zero-shot output of ModelGPT on Webcam as weight initialization.It is important to notice that the major difference between the two methods lies in the weight initialization.While Finetune uses the weights pretrained on ImageNet (Deng et al., 2009), ours uses the weights outputted by ModelGPT, which is generated only under Webcam's task description without any access to Webcam's data.</p>
<p>We save the best checkpoint in evaluation and test it on Webcam's test data.The results, detailed in Table 4, reveal a notable aspect: despite a roughly 10% performance gap compared to Finetune without access to Webcam's data shown in Table 3, our framework exhibits remarkable convergence speed when finetuned with Webcam's training data, using the same hyperparameters as Finetune.Specifically, while Finetune requires 108 epochs to reach optimal evaluation results, our framework, given ModelGPT's zero-shot Table 4. Results on the test dataset using the best evaluation checkpoint of each method.‚ôØepoch implies the number of epochs for each method to achieve the best evaluation checkpoint.output as initialization, achieves comparable performance in just 16 epochs, a 6.75-fold increase in speed.</p>
<p>Moreover, we meticulously track the progression of training and evaluation losses, alongside the corresponding accuracy as presented in Figure 4 with five different seeds.The depicted curves represent the mean value, while the shaded areas denote the range within one standard deviation.All the figures demonstrate the superiority of ModelGPT's output as a weight initialization.As shown in Figure 4b, Model-GPT's initialization outperforms the baseline in evaluation throughout the process.Notably, the substantial standard deviation observed in the baseline during the initial epochs in Figure 4b can be attributed to the instability often encountered at the onset of training.Moreover, while the baseline shows a marginally improved performance in the later stages of training in Figure 4c, our approach demonstrates better performance on evaluation data in Figure 4d, suggesting a better generalization capability and robustness.</p>
<p>Future Work</p>
<p>While ModelGPT demonstrates good performance in creating customized models, areas for improvement remain.First, the granularity of Model Generator can be enhanced.</p>
<p>A detailed analysis of factors like task complexity and user resources could lead to more precise model architecture generation.Second, the efficiency of Parameter Generator needs improvement.Our current method adjusts parameter output dimensions using a dictionary approach, pre-encodes task requirements and parameter shapes, which may not be suitable for broader model generation needs.We plan to address these limitations in our future research.</p>
<p>Conclusion</p>
<p>In this work, we introduce ModelGPT, a novel and comprehensive framework that harnesses the potential of Large Language Models (LLMs) to meet diverse and specific model generation needs.Emphasizing user-centric design, Mod-elGPT efficiently transforms data or task descriptions into customized models, catering to the unique requirements of each scenario.This approach not only bridges the gap between complex model generation and user accessibility but also paves the way for a new paradigm in adaptive and efficient model creation.However, we would like to emphasize that our investigations are still in their initial stages.We are open to and greatly value discussions and collaborative explorations in this emerging field.</p>
<p>Impact Statements</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>A. Hyperparameter Setting</p>
<p>In the main article, we conduct comprehensive experiments on ModelGPT.In this section, we provide detailed hyperparameter settings to reproduce our results in Table 5.Since ModelGPT is a framework that generates target models directly.</p>
<p>During the training of ModelGPT (denoted as pretrain for simplicity), we need to train both ModelGPT and the target model to update the overall framework.Therefore, we set their learning rate and weight decay individually.</p>
<p>Example</p>
<p>This is dataset CarEvaluation.</p>
<p>[input] {"buying":"med","maint":"vhigh",‚Ä¶,"safety":"high"} [label] acc [input] {"buying":"med","maint":"high",‚Ä¶,"safety":"low"} [label] unacc ‚Ä¶</p>
<p>[input] {"buying":"med","maint":"low",‚Ä¶,"safety":"high"} [label] vgood This is a task of tabular classification to evaluate the acceptability of car purchases based on various attributes.</p>
<p>CV</p>
<p>Imagine you're provided with a batch of image data and corresponding labels, along with specific background information about the images (optional).Your task is to succinctly identify the type of image processing task demonstrated (e.g., classification, detection) and to highlight a common visual pattern in the images that is directly related to the background information provided.Here, we provide the prompt template we use in our main experiments on NLP, CV, and tabular data.Each example has 2-3 rows.In each example, the first row is the basis of our prompt, leaving the last 2 lines filled with real data and task descriptions.In CV, we then provide two pairs of examples in the next rows.In the pair, the only difference between them is whether or not we provide task descriptions.The third column of these examples is the result LLM (GPT-4-vision-preview in this case) outputs.The green color texts are those reflecting the correct data-specific information, while the red ones are those reflecting the WRONG information and the gray ones are irrelevant information.</p>
<p>operates flawlessly in traditional scenarios, it does have some problems in ModelGPT.To be specific, complex models have parameters that are not trained but changed during the stage of finetuning (e.g.running mean and variance of BatchNorm Layers).Due to convergence issues, it is not practical for us to generate these parameters alongside the generation of LoRA adapters.However, leaving these modules unsettled would result in unacceptable performance degradation.Hence, to enable ModelGPT to generate complex models, we disable the functionality of these layers in the target model at the expense of less stability with the code below:</p>
<p>B. More Examples about Prompt Design</p>
<p>In Section 4.3.1,we detailedly discuss the prompt design in our main CV experiments.Here, we also release the prompt templates in our main experiments with other data types.</p>
<p>As shown in Figure 5, in NLP, we use benchmark GLUE for experiments.The major difference between GLUE's tasks can be directly summarized from their task name since they vary in task type (e.g.binary two-input classification, multi-class one-input classification, one-input regression.)As a result, to instruct LLMs to precisely capture user requirements, we just need to figure the task out.In the example, with the prompt given, LLM successfully points out that the given data is a task of binary sentiment analysis.Similar results can be observed in tabular data.Here, different from NLP, we also provide background information (User Description) on the given data.We directly tell LLM that the data comes from datasets such as Iris and Car Evaluation.With User Description and User Data provided, LLM successfully points out that the input is a tabular classification to evaluate the acceptability of car purchases.</p>
<p>Figure 1.Overview of the framework of ModelGPT.</p>
<p>Figure 2 .
2
Figure 2. Details of the workflow of ModelGPT.Here, we also provide real examples taken from our main experiments.</p>
<p>CVFigure 3 .
3
Figure3.Case study on the prompt design.Here, we choose the prompt we use for CV experiments as an example.The first row is the basis of our prompt, leaving the last 2 lines filled with real data and task descriptions.We then provide two pairs of examples in the next rows.In the pair, the only difference between them is whether or not we provide task descriptions.The third column of these examples is the result LLM (GPT-4-vision-preview in this case) outputs.The green color texts are those reflecting the correct data-specific information, while the red ones are those reflecting the WRONG information and the gray ones are irrelevant information.</p>
<p>Figure 4 .
4
Detailed analyses on the capability of weight initialization of ModelGPT.For clearer comparison, we increase the length of the starting epochs.Meanwhile, we mark the best checkpoint of each method in the figures with a solid round point.</p>
<p>Figure 5 .
5
Figure5.Case study on the prompt design.Here, we provide the prompt template we use in our main experiments on NLP, CV, and tabular data.Each example has 2-3 rows.In each example, the first row is the basis of our prompt, leaving the last 2 lines filled with real data and task descriptions.In CV, we then provide two pairs of examples in the next rows.In the pair, the only difference between them is whether or not we provide task descriptions.The third column of these examples is the result LLM (GPT-4-vision-preview in this case) outputs.The green color texts are those reflecting the correct data-specific information, while the red ones are those reflecting the WRONG information and the gray ones are irrelevant information.</p>
<p>def train(self, mode=True): type(model).train.<strong>call</strong>(self,mode) for m in self.modules(): if isinstance(m, nn.BatchNorm2d): m.eval() m.weight.requires_grad= False m.bias.requires_grad= False model.train= functools.partial(train,model)</p>
<p>User Data]/[User Description]. Model Customizer / Please help me discriminate the category of the given images. These images are office supplies taken by a DSLR camera. Data Desc.</p>
<p>Please give me an artificial intelligence model, making it best suitable for [</p>
<p>*. Users can choose to provide labeled data or descriptions,</p>
<p>or both Model Generator Requirement Encoder Module-Wise Parameter Generator
Latent SpaceTransformationCustomizedModule 1:layer1.0.conv1CustomizedModule 2:layer1.1.conv2‚Ä¶CustomizedModule n:</p>
<p>classifier Parameter Generator scissors ‚Ä¶ mouse bottle bike Image / Please help me discriminate whether the input sentence is semantically acceptable. Data Desc. Text / Please help me classify the tabular data from the dataset Iris. background information. The images are included within the passage. They are taken by a DSLR camera.</p>
<p>[label] acceptable [sentence] They drank the pub [label] unacceptable [sentence] The professor talked us into a stupor [label] acceptable ... [sentence] The professor talked us [label] unacceptable</p>
<p>unacceptable SepalLen SepalWid PetalLen PetalWid Label</p>
<p>They drank the pub dry.-acceptable They drank the pub.-unacceptable The professor talked us into a stupor.-acceptable ‚Ä¶ The professor talked us.-
This is a task5.13.52.40.2Iris-setosa5.82.74.11Iris-versicolor4.92.54.50.7Iris-virginica‚Ä¶5.43.41.50.4Iris-setosa</p>
<p>of text classification to recognize whether the input sentence is semantically acceptable. ‚Ä¶ Customized Modules User Requirement User Data/Description
Finetune(Optional)User Data$ ùë¶ùêøùëúùë†ùë†ùë¶RequirementModelGeneratorCustomizerUser InputModelGPTOutputUser Data$ ùë¶ùêøùëúùë†ùë†ùë¶</p>
<p>(He et al., 2016)010)019), Bank Marketing(Moro et al., 2014)from UCI Machine Learning Repository 2 .Model Generator chooses MLP as the target model.CV Here, we use Office-31(Saenko et al., 2010), a dataset commonly used in the field of domain adaptation.This dataset contains 31 object categories in three domains: Amazon, DSLR, and Webcam with 2817, 498, and 795 images respectively, different in background, viewpoint, color, etc. Model Generator chooses ResNet-50(He et al., 2016)as the target model.In this experiment, both the efficiency and zero-shot ability are tested.To test the zero-shot ability of our framework, we first train our model with Amazon and DSLR.Then, we directly feed User Requirements extracted by LLM on Webcam's training data to ModelGPT and test the output model on Webcam's test dataset, where ModelGPT sees no Webcam's data but its requirements.</p>
<p>(Koklu &amp; √ñzkan, 2020)kan, 2020),</p>
<p>Table 1 .
1
Detailed results on GLUE.We use Distil-BERT as the target model.The metric of CoLA and DM is Matthew's Correlation.SST-2, MNLI-m, MNLI-mm, QNLI, RTE, WNLI use accuracy.MRPC and QQP use f1 score and accuracy simultaneously.STS-B uses Pearson-Spearman Correlation.‚ôØEpoch represents the training epochs for each method to obtain the results.E2E (end-to-end) Runtime measures total task completion time (seconds), while Relative Efficiency scales this runtime against the worst-performing method.
Results on GLUE Benchmark (Distil-BERT)MethodsCoLA SST-2 MRPCSTS-BQQPMNLI-m MNLI-mm QNLI RTE WNLI DM Score ‚ôØEpochE2E Runtime (s)Relative EfficiencyLoRA48.391.0 84.9 / 80.3 81.2 / 80.0 68.9 / 87.380.533.188.1 52.8 65.10.071.520756721.3Finetune45.591.3 86.6 / 80.8 82.1 / 80.9 69.2 / 87.881.880.887.6 56.9 63.7 35.6 74.420958701.0ModelGPT39.588.9 85.3 / 78.4 80.9 / 80.3 63.3 / 83.577.878.084.6 69.5 64.4 28.0 73.40350273.8ModelGPT-F 36.990.8 85.5 / 79.4 81.3 / 80.5 67.0 / 86.677.878.185.8 70.0 62.3 29.9 73.81110187.0</p>
<p>Table 2 .
2
Detailed results of various methods on 10 tabular classification tasks with accuracy as the evaluation metric.‚ôØEpoch represents the training epochs for each method to obtain the results.E2E (end-to-end) Runtime measures total task completion time (seconds), while Relative Efficiency scales this runtime against the worst-performing method.
Results on Tabular Data (MLP)MethodsIrisHeart DiseaseWine AdultBreast Cancer Evaluation Quality Bean Car Wine Dry RiceBank MarketingAverage ‚ôØEpochE2E Runtime (s)Relative EfficiencyLoRA93.363.067.354.795.971.355.088.992.589.877.2202721.0Finetune88.954.389.155.296.571.055.390.693.189.978.4202331.2ModelGPT100.060.994.554.795.371.554.185.092.589.879.80646.2ModelGPT-F 100.062.094.555.195.971.355.488.892.990.080.611420.2</p>
<p>Table 3 .
3
Detailed results of various methods on.The metric is accuracy, top-3 accuracy, and top-5 accuracy.Our results on Webcam are conducted with no training data provided.‚ôØEpoch represents the training epochs for each method to obtain the results.E2E (end-to-end) Runtime measures total task completion time (seconds), while Relative Efficiency scales this runtime against the worst-performing method.The average only considers Amazon and DSLR here.
Results on Office-31 (ResNet-50, ModelGPT is ZERO-SHOT in Webcam)DomainAmazonDSLRAverageWebcam</p>
<p>Table 5 .
5
Detailed Hyperparameter Setting of Our Main ExperimentsAs we stated in Section 3.2.2, to solve convergence issues, we adopt LoRA adapters to the target model and generate their parameters.Besides, since we only need very simple AI models like MLPs to solve tabular tasks, ModelGPT directly outputs target models' parameters rather than LoRA adapters' parameters.The implementation of MLP is shown below.We accordingly mark their hyperparameters concerning LoRA to NA in Table5.Generally, in the context of fine-tuning, we could use LoRA adapters to reduce the overall cost.Although such paradigm NLP Suppose you are given a batch of data, which may have one or more inputs and only one label per sample.Each data sample is in one line with each part started with a special token like[label].You are asked to describe which task is it in ONE sentence.Any other output is FORBIDDEN.For example: My input is:[sentence] They drank the pub dry.[label] acceptable [sentence] They drank the pub.[label] unacceptable [sentence] The professor talked us into a stupor.[label]acceptable[sentence]Theprofessortalkedus.[label]unacceptableYou should describe the batch as instructed above use ONLY 1 sentence.As a result, one possible answer could be: This is a task of text classification to recognize whether the input sentence is semantically acceptable.Suppose you are given a batch of data, which have a json as input and an output per sample.Each data sample is in one line with each part started with a special token[input]or[label].You are asked to describe which task is it in ONE sentence according to the data and the background given (optional).Any other output is FORBIDDEN.This is a task of tabular classification to recognize which type of iris plant the input is.Mind that you should describe the batch as instructed above use ONLY 1 sentence.Next, I will give you a batch, act just as I instruct above.
Parameter \ SettingNLPCVTabular DataGPUA100 80GA100 80GA100 80GOptimizer (ModelGPT)AdamAdamAdamLearning Rate (ModelGPT)1e-51e-41e-3Weight Decay (ModelGPT)1e-41e-51e-4Optimizer (Target Model)AdamAdamAdamLearning Rate (Target Model)1e-41e-32e-2Weight Decay (Target Model)1e-41e-31e-4lora r168NAlora alpha3216NAlora droput0.050.1NAtarget modules. * [qv] linlayer....conv.NA‚ôØEpoch (Pretrain)5010080Batch Size25625664Latent Dimension76812825Seed202420242024The hyperparameter settings of baselines are similar. We set the number of training epochs to 20, 200, and 20 in NLP, CV,and tabular data individually with the learning rate to be 1e-3, 1e-3, and 2e-2 individually. The LoRA config of baselineLoRA is the same to ModelGPT, except that in tabular data, lora r is 4, lora alpha is 8, lora dropout is 0.1, and targetmodules is mlp.\d. * .class MLP(nn.Module):def <strong>init</strong>(self, in_dim: int, out_dim: int,hidden_dim: int, n_layers: int,<em> args, ** kwargs):super().<strong>init</strong>( * args, ** kwargs)self.mlp = nn.Sequential(nn.Linear(in_dim, hidden_dim),</em> [nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers)],nn.Linear(hidden_dim, out_dim))def forward(self, x: Tensor) -&gt; Tensor:return self.mlp(x)
Department of Computer Science, Zhejiang University, Hangzhou, China.
https://gluebenchmark.com/
https://archive.ics.uci.edu/
DataDesc.
Comparative analysis of statistical pattern recognition methods in high dimensional settings. S Aeberhard, D Coomans, O Y Vel, Pattern Recognit. 27229108021994</p>
<p>Stylegan inversion with hypernetworks for real image editing. Y Alaluf, O Tov, R Mokady, R Gal, A Bermano, Hyperstyle, 10.1109/CVPR52688.2022.01796IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. New Orleans, LA, USAIEEEJune 18-24, 20222022</p>
<p>UCI Machine Learning Repository. B Becker, R Kohavi, Adult, 10.24432/C5XW201996</p>
<p>Knowledge acquisition and explanation for multi-attribute decision making. M Bohanec, V Rajkovic, 8th intl workshop on expert systems and their applications. Avignon France1988</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Larochelle, H.</p>
<p>M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Modeling wine preferences by data mining from physicochemical properties. P Cortez, A L Cerdeira, F Almeida, T Matos, J Reis, Decis. Support Syst. 472009</p>
<p>Classification of rice varieties using artificial intelligence methods. I Cƒ±nar, M Koklu, International Journal of Intelligent Systems and Applications in Engineering. 2081057522019</p>
<p>ImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 200909</p>
<p>International application of a new probability algorithm for the diagnosis of coronary artery disease. R C Detrano, A J√°nosi, W Steinbrunn, M E Pfisterer, J.-J Schmid, S Sandhu, K Guppy, S Lee, V Froelicher, The American journal of cardiology. 64235453031989</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsJune 2-7, 2019. 20191</p>
<p>Hyperinverter: Improving stylegan inversion via hypernetwork. T M Dinh, A T Tran, R Nguyen, B Hua, 10.1109/CVPR52688.2022.01110IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. New Orleans, LA, USAIEEEJune 18-24, 20222022</p>
<p>Specializing smaller language models towards multi-step reasoning. Y Fu, H Peng, L Ou, A Sabharwal, T Khot, International Conference on Machine Learning, ICML 2023. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, Honolulu, Hawaii, USAJuly 2023202of Proceedings of Machine Learning Research</p>
<p>. PMLR. 2023</p>
<p>Textbooks are all you need. S Gunasekar, Y Zhang, J Aneja, C C T Mendes, A D Giorno, S Gopi, M Javaheripi, P Kauffmann, G De Rosa, O Saarikivi, A Salim, S Shah, H S Behl, X Wang, S Bubeck, R Eldan, A T Kalai, Y T Lee, Y Li, 10.48550/arXiv.2306.116442023</p>
<p>D Ha, A M Dai, Q V Le, Hypernetworks, 5th International Conference on Learning Representations. Toulon, France2017. April 24-26, 2017. 2017Conference Track Proceedings. OpenReview.net</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.902016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyJune 27-30, 2016. 2016</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. C Hsieh, C Li, C Yeh, H Nakhost, Y Fujii, A Ratner, R Krishna, C Lee, T Pfister, 10.18653/V1/2023.FINDINGS-ACLFindings of the Association for Computational Linguistics: ACL 2023. A Rogers, J L Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 2023. 2023</p>
<p>10.18653/v1/2023.findings-acl.507URL. </p>
<p>Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. 2022OpenReview.net</p>
<p>HINT: hypernetwork instruction tuning for efficient zero-and few-shot generalisation. H Ivison, A Bhagia, Y Wang, H Hajishirzi, M E Peters, 10.18653/v1/2023.acl-long.631Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J L Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaJuly 9-14, 202312023ACL 2023</p>
<p>Multiclass classification of dry beans using computer vision and machine learning techniques. M Koklu, I A √ñzkan, Comput. Electron. Agric. 1742197628902020</p>
<p>DUET: A tuning-free device-cloud collaborative parameters generation framework for efficient device model generalization. Z Lv, W Zhang, S Zhang, K Kuang, F Wang, Y Wang, Z Chen, T Shen, H Yang, B C Ooi, F Wu, Y Ding, J Tang, J F Sequeda, L Aroyo, C Castillo, 10.1145/3543507.3583451Proceedings of the ACM Web Conference 2023, WWW 2023. G Houben, the ACM Web Conference 2023, WWW 2023Austin, TX, USAACM30 April 2023 -4 May 20232023</p>
<p>A data-driven approach to predict the success of bank telemarketing. S Moro, P Cortez, Rita , P , Decis. Support Syst. 622014</p>
<p>A comprehensive overview of large language models. H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Barnes, A Mian, 10.48550/arXiv.2307.064352023</p>
<p>10.48550/arXiv.2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>A generative model for diverse, performant neural networks. N Ratzlaff, F Li, Hypergan, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. K Chaudhuri, R Salakhutdinov, the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USAPMLR9-15 June 2019. 201997of Proceedings of Machine Learning Research</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J R Ruiz, J S Ellenberg, P Wang, O Fawzi, 10.1038/s41586-023-06924-6Nature. 2023</p>
<p>Adapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, 10.1007/978-3-642-15561-1_16Computer Vision -ECCV 2010, 11th European Conference on Computer Vision. Lecture Notes in Computer Science. K Daniilidis, P Maragos, N Paragios, Heraklion, Crete, GreeceSpringerSeptember 5-11, 2010. 20106314Proceedings, Part IV</p>
<p>Distributionally robust neural networks. S Sagawa, P W Koh, T B Hashimoto, P Liang, 8th International Conference on Learning Representations. EthiopiaApril 26-30, 2020. 20202020Addis AbabaOpenReview.net</p>
<p>Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. V Sanh, L Debut, J Chaumond, T Wolf, CoRR, abs/1910.011082019</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, 2023</p>
<p>Beyond memorization: Violating privacy via inference with large language models. R Staab, M Vero, M Balunovic, M T Vechev, 10.48550/arXiv.2310.072982023</p>
<p>Nuclear feature extraction for breast tumor diagnosis. W N Street, W H Wolberg, O L Mangasarian, Electronic imaging. 199314922543</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M Lachaux, T Lacroix, B Rozi√®re, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 10.48550/arXiv.2302.139712023a</p>
<p>Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C Canton-Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, Scialom, 10.48550/arXiv.2307.092882023b2</p>
<p>Well-read students learn better: The impact of student initialization on knowledge distillation. I Turc, M Chang, K Lee, K Toutanova, CoRR, abs/1908.089622019</p>
<p>The iris data set: In search of the source of virginica. A Unwin, K Kleinman, Significance. 182447630322021</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, I Guyon, U Von Luxburg, S Bengio, H M Wallach, R Fergus, S Vishwanathan, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. V N Garnett, R , Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. 2019aOpenReview.net</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. 2019bOpenReview.net</p>
<p>Deep visual domain adaptation: A survey. M Wang, W Deng, 10.1016/j.neucom.2018.05.083Neurocomputing. 0925-2312312October 2018</p>
<p>Generalizing from a few examples: A survey on few-shot learning. Y Wang, Q Yao, J T Kwok, L M Ni, 10.1145/3386252ACM Comput. Surv. 5332021</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, NeurIPS2022</p>
<p>Sustainable AI: environmental implications, challenges and opportunities. C Wu, R Raghavendra, U Gupta, B Acun, N Ardalani, K Maeng, G Chang, F A Behram, J Huang, C Bai, M Gschwind, A Gupta, M Ott, A Melnikov, S Candido, D Brooks, G Chauhan, B Lee, H S Lee, B Akyildiz, M Balandat, J Spisak, R Jain, M Rabbat, K M Hazelwood, Proceedings of Machine Learning and Systems. D Marculescu, Y Chi, C Wu, Machine Learning and SystemsMLSys; Santa Clara, CA, USA2022. 2022. August 29 -September 1, 2022. mlsys.org, 2022</p>
<p>A survey of resource-efficient llm and multimodal foundation models. M Xu, W Yin, D Cai, R Yi, D Xu, Q Wang, B Wu, Y Zhao, C Yang, S Wang, Q Zhang, Z Lu, L Zhang, S Wang, Y Li, Y Liu, X Jin, X Liu, 2024</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. J Yang, H Jin, R Tang, X Han, Q Feng, H Jiang, B Yin, X Hu, 10.48550/arXiv.2304.137122023</p>
<p>Y Yao, J Duan, K Xu, Y Cai, E Sun, Y Zhang, 10.48550/arXiv.2312.02003A survey on large language model (LLM) security and privacy: The good, the bad, and the ugly. 2023</p>
<p>A survey of large language models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J Nie, J Wen, 10.48550/arXiv.2303.182232023</p>
<p>Domain generalization: A survey. K Zhou, Z Liu, Y Qiao, T Xiang, C C Loy, 10.1109/TPAMI.2022.3195549IEEE Transactions on Pattern Analysis and Machine Intelligence. 1939-35392022</p>            </div>
        </div>

    </div>
</body>
</html>
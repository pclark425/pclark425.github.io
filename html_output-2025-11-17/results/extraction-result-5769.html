<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5769 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5769</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5769</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-266052799</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.03759v1.pdf" target="_blank">How should the advent of large language models affect the practice of science?</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are being increasingly incorporated into scientific workflows. However, we have yet to fully grasp the implications of this integration. How should the advent of large language models affect the practice of science? For this opinion piece, we have invited four diverse groups of scientists to reflect on this query, sharing their perspectives and engaging in debate. Schulz et al. make the argument that working with LLMs is not fundamentally different from working with human collaborators, while Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools. Marelli et al. emphasize the importance of transparent attribution and responsible use of LLMs. Finally, Botvinick and Gershman advocate that humans should retain responsibility for determining the scientific roadmap. To facilitate the discussion, the four perspectives are complemented with a response from each group. By putting these different perspectives in conversation, we aim to bring attention to important considerations within the academic community regarding the adoption of LLMs and their impact on both current and future scientific practices.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5769.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5769.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (MetaAI science-specific LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A science-focused large language model released by MetaAI in 2022 intended to support knowledge discovery and literature synthesis across scientific domains; notable for being taken offline after generating fabricated content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A proprietary, large pretrained transformer trained on scientific text and web-sourced corpora to generate scientific prose, summaries, and knowledge artifacts (detailed size/architecture not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Support knowledge discovery and literature synthesis across scientific papers (i.e., provide summaries, citations, and scientific content to assist researchers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multidisciplinary scientific literature / general science</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Pretraining on large scientific text corpora to produce generative summaries and knowledge outputs (generative LLM usage; specific prompting or retrieval augmentation not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Intended to surface generalizable scientific knowledge and synthesize literature claims (broadly described as 'knowledge discovery' and literature summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported in this paper beyond community reaction; criticisms centered on factual accuracy and fabrication rather than standardized quantitative metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reportedly produced plausible-sounding but fabricated outputs (e.g., fake papers and invented facts), provoking strong criticism and removal from public access after a few days; demonstration that generative LLMs can produce convincing yet false scholarly-seeming claims.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human deployment and evaluation (users encountered fabricated outputs); human community reaction led to model being taken offline—human oversight critical.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Described generally as scientific text and web-sourced corpora (exact training data not disclosed in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Hallucination and fabrication of facts and citations; lack of grounding/accountability; opacity of training data and model internals; risks to scientific integrity when models generate unchecked assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Model generated 'fake papers (sometimes attributing them to real authors)' and invented wiki-style content (e.g., 'history of bears in space'), demonstrating fabrication risk when using LLMs to synthesize scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How should the advent of large language models affect the practice of science?', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5769.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5769.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 paper-review study (Liang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can large language models provide useful feedback on research papers? A large-scale empirical analysis (Liang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical study referenced in the paper that compared GPT-4-generated manuscript feedback to human peer reviews and measured perceived helpfulness by users.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models provide useful feedback on research papers? a large-scale empirical analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>OpenAI's large proprietary transformer-based language model (GPT-4); described as a high-capability LLM able to generate reviews and feedback without task-specific additional training in the referenced study.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate peer-review style feedback on submitted research manuscripts (i.e., synthesize critiques and suggestions from manuscript text).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Scientific manuscripts across domains (peer review context)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompting GPT-4 to produce reviews of manuscripts and comparing LLM-generated feedback to human reviews via user studies (user ratings of helpfulness and comparative benefit).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not aimed at explicit 'laws' but at extracting critiques, summary judgments, and suggested improvements from manuscript content (literature synthesis and evaluative generalizations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>User-reported helpfulness metrics (e.g., percent of users finding feedback helpful), comparative preferences versus human reviewers (reportedly 57.4% found GPT-4 feedback helpful/very helpful; 82.4% found it more beneficial than feedback from at least some human reviewers).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced study found a majority of users judged GPT-4 feedback helpful and often preferable to at least some human reviewers, indicating LLMs can produce actionable summary/critique of papers; paper also highlights legal/confidentiality constraints on using proprietary models for peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human evaluation of LLM outputs (user studies), and human oversight recommended due to errors/hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Sets of manuscripts used in the referenced empirical analysis (specific corpora/manuscripts not detailed in this opinion piece).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Legal/confidentiality issues (manuscripts under review should not be input to proprietary LLMs), risk of hallucinated or incorrect critiques, and unknowns about reproducibility when models/hosts change.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Reported user statistics showing substantial perceived usefulness of GPT-4-generated reviews, juxtaposed with warnings about confidentiality and the need for human checking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How should the advent of large language models affect the practice of science?', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5769.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5769.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM literature synthesis (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of large language models for literature summarization, knowledge extraction, and hypothesis generation (as discussed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses using LLMs to summarize sets of papers, extract knowledge, and propose hypotheses or general principles across bodies of scholarly work, while flagging risks of hallucination and loss of epistemic grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Various (e.g., GPT-4, ChatGPT, other large pretrained transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>General class of large pretrained transformer LLMs used for generative summarization and synthesis; specifics (size, parameter count) are not provided in this opinion piece.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Distill insights, summarize literature, and generate hypotheses or generalizable principles from large collections of scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multidisciplinary scientific literature (biology, physics, social sciences, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Generative summarization and synthesis via prompting; contrasted with extractive summarization and bespoke task-specific models; human-in-the-loop verification is emphasized. The paper mentions potential use of instruction tuning and RLHF to improve outputs but does not present a concrete pipeline for 'law extraction.'</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>General research-level summaries, proposed hypotheses, and synthetic literature overviews (no concrete laws distilled within this paper; discussion is conceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Discussed qualitatively: coverage, false negatives (missing important points), factual accuracy, citation correctness, human expert checking, and downstream amplification of errors; no standardized quantitative results reported in this paper for law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The authors treat literature-synthesis by LLMs as promising but risky: LLMs can summarize and propose hypotheses but are prone to omission, hallucination, and citation errors; the paper advocates human oversight, transparency, and possibly favoring open-source/transparent models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop approach strongly recommended (vetting, fact-checking, and deciding which model outputs to pursue).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Large bodies of scientific literature generally (not a named dataset in this paper); authors note models are trained on web-sourced texts and large scientific corpora where used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Hallucinations, fabricated or incorrect citations, lack of grounding in the cited manuscripts, potential loss of epistemic diversity, training-data contamination, opacity of proprietary systems, and reproducibility concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Conceptual examples include the Galactica failure and the observation that LLM-generated literature reviews may produce claims not directly derived from the cited manuscripts (i.e., the machine predicts citations to accompany generated claims rather than deriving claims from those papers).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How should the advent of large language models affect the practice of science?', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5769.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5769.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-simulated participants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using language models to simulate human samples / participants (e.g., Argyle et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced line of work exploring whether LLMs can act as proxies for human subjects or annotators by simulating responses and social behavior across many texts, with cautionary notes about validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Out of one, many: Using language models to simulate human samples</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Various generative LLMs (unspecified in this paper's discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Large pretrained generative transformers prompted to produce synthetic participant responses or social media behavior; specific architectures/size not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate synthetic human-like responses or population samples to stand in for human participants in studies, or to simulate social media dynamics for research.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Social sciences, political analysis, human-subjects research, computational social science</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompting LLMs to simulate responses representative of particular demographics or conditions; used in simulation-style studies (referenced works include empirical investigations and simulations), with proposed comparisons to actual human data.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>In principle, could be used to elicit generalized behavioral regularities or social principles from aggregated model outputs, but the paper stresses conceptual limits rather than presenting distilled laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in detail in this opinion piece; referenced concerns include fidelity to human communicative intent, behavioral validity, and how well synthetic samples match empirical human distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper cites works that experiment with LLM-simulated samples but stresses that LLMs model token sequences, not true human meanings or intentions; authors caution against treating LLM-generated participants as replacements for actual human subjects without strong validation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human oversight required; recommended that actual human participants remain the standard for many studies. Where LLMs are used, human evaluation/comparison to real data is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Implicitly trained on large web and social text corpora; specific corpora vary by underlying model and are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fundamental mismatch between token-distribution modelling and human concepts/intent; brittleness to input variation; risk of drawing invalid generalizations about human behavior; ethical concerns about fabricating data.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Referenced preprints and studies exploring simulated human samples and social-media simulations, but the paper emphasizes they are illustrative and that real human participants cannot be casually replaced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How should the advent of large language models affect the practice of science?', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A large language model for science <em>(Rating: 2)</em></li>
                <li>Can large language models provide useful feedback on research papers? a large-scale empirical analysis <em>(Rating: 2)</em></li>
                <li>Out of one, many: Using language models to simulate human samples <em>(Rating: 2)</em></li>
                <li>Simulating social media using large language models to evaluate alternative news feed algorithms <em>(Rating: 1)</em></li>
                <li>Fabrication and errors in the bibliographic citations generated by chatgpt <em>(Rating: 1)</em></li>
                <li>Chatgpt outperforms crowd workers for text-annotation tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5769",
    "paper_id": "paper-266052799",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "Galactica",
            "name_full": "Galactica (MetaAI science-specific LLM)",
            "brief_description": "A science-focused large language model released by MetaAI in 2022 intended to support knowledge discovery and literature synthesis across scientific domains; notable for being taken offline after generating fabricated content.",
            "citation_title": "A large language model for science",
            "mention_or_use": "mention",
            "llm_model_name": "Galactica",
            "llm_model_description": "A proprietary, large pretrained transformer trained on scientific text and web-sourced corpora to generate scientific prose, summaries, and knowledge artifacts (detailed size/architecture not specified in this paper).",
            "task_goal": "Support knowledge discovery and literature synthesis across scientific papers (i.e., provide summaries, citations, and scientific content to assist researchers).",
            "domain": "Multidisciplinary scientific literature / general science",
            "methodology": "Pretraining on large scientific text corpora to produce generative summaries and knowledge outputs (generative LLM usage; specific prompting or retrieval augmentation not detailed in this paper).",
            "type_of_qualitative_law": "Intended to surface generalizable scientific knowledge and synthesize literature claims (broadly described as 'knowledge discovery' and literature summarization).",
            "evaluation_metrics": "Not reported in this paper beyond community reaction; criticisms centered on factual accuracy and fabrication rather than standardized quantitative metrics.",
            "results_summary": "Reportedly produced plausible-sounding but fabricated outputs (e.g., fake papers and invented facts), provoking strong criticism and removal from public access after a few days; demonstration that generative LLMs can produce convincing yet false scholarly-seeming claims.",
            "human_involvement": "Human deployment and evaluation (users encountered fabricated outputs); human community reaction led to model being taken offline—human oversight critical.",
            "dataset_or_corpus": "Described generally as scientific text and web-sourced corpora (exact training data not disclosed in the paper).",
            "limitations_or_challenges": "Hallucination and fabrication of facts and citations; lack of grounding/accountability; opacity of training data and model internals; risks to scientific integrity when models generate unchecked assertions.",
            "notable_examples": "Model generated 'fake papers (sometimes attributing them to real authors)' and invented wiki-style content (e.g., 'history of bears in space'), demonstrating fabrication risk when using LLMs to synthesize scientific knowledge.",
            "uuid": "e5769.0",
            "source_info": {
                "paper_title": "How should the advent of large language models affect the practice of science?",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT-4 paper-review study (Liang et al.)",
            "name_full": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis (Liang et al., 2023)",
            "brief_description": "An empirical study referenced in the paper that compared GPT-4-generated manuscript feedback to human peer reviews and measured perceived helpfulness by users.",
            "citation_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis",
            "mention_or_use": "mention",
            "llm_model_name": "GPT-4",
            "llm_model_description": "OpenAI's large proprietary transformer-based language model (GPT-4); described as a high-capability LLM able to generate reviews and feedback without task-specific additional training in the referenced study.",
            "task_goal": "Generate peer-review style feedback on submitted research manuscripts (i.e., synthesize critiques and suggestions from manuscript text).",
            "domain": "Scientific manuscripts across domains (peer review context)",
            "methodology": "Prompting GPT-4 to produce reviews of manuscripts and comparing LLM-generated feedback to human reviews via user studies (user ratings of helpfulness and comparative benefit).",
            "type_of_qualitative_law": "Not aimed at explicit 'laws' but at extracting critiques, summary judgments, and suggested improvements from manuscript content (literature synthesis and evaluative generalizations).",
            "evaluation_metrics": "User-reported helpfulness metrics (e.g., percent of users finding feedback helpful), comparative preferences versus human reviewers (reportedly 57.4% found GPT-4 feedback helpful/very helpful; 82.4% found it more beneficial than feedback from at least some human reviewers).",
            "results_summary": "Referenced study found a majority of users judged GPT-4 feedback helpful and often preferable to at least some human reviewers, indicating LLMs can produce actionable summary/critique of papers; paper also highlights legal/confidentiality constraints on using proprietary models for peer review.",
            "human_involvement": "Human evaluation of LLM outputs (user studies), and human oversight recommended due to errors/hallucinations.",
            "dataset_or_corpus": "Sets of manuscripts used in the referenced empirical analysis (specific corpora/manuscripts not detailed in this opinion piece).",
            "limitations_or_challenges": "Legal/confidentiality issues (manuscripts under review should not be input to proprietary LLMs), risk of hallucinated or incorrect critiques, and unknowns about reproducibility when models/hosts change.",
            "notable_examples": "Reported user statistics showing substantial perceived usefulness of GPT-4-generated reviews, juxtaposed with warnings about confidentiality and the need for human checking.",
            "uuid": "e5769.1",
            "source_info": {
                "paper_title": "How should the advent of large language models affect the practice of science?",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM literature synthesis (generic)",
            "name_full": "Use of large language models for literature summarization, knowledge extraction, and hypothesis generation (as discussed in this paper)",
            "brief_description": "The paper discusses using LLMs to summarize sets of papers, extract knowledge, and propose hypotheses or general principles across bodies of scholarly work, while flagging risks of hallucination and loss of epistemic grounding.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "Various (e.g., GPT-4, ChatGPT, other large pretrained transformers)",
            "llm_model_description": "General class of large pretrained transformer LLMs used for generative summarization and synthesis; specifics (size, parameter count) are not provided in this opinion piece.",
            "task_goal": "Distill insights, summarize literature, and generate hypotheses or generalizable principles from large collections of scholarly papers.",
            "domain": "Multidisciplinary scientific literature (biology, physics, social sciences, etc.)",
            "methodology": "Generative summarization and synthesis via prompting; contrasted with extractive summarization and bespoke task-specific models; human-in-the-loop verification is emphasized. The paper mentions potential use of instruction tuning and RLHF to improve outputs but does not present a concrete pipeline for 'law extraction.'",
            "type_of_qualitative_law": "General research-level summaries, proposed hypotheses, and synthetic literature overviews (no concrete laws distilled within this paper; discussion is conceptual).",
            "evaluation_metrics": "Discussed qualitatively: coverage, false negatives (missing important points), factual accuracy, citation correctness, human expert checking, and downstream amplification of errors; no standardized quantitative results reported in this paper for law extraction.",
            "results_summary": "The authors treat literature-synthesis by LLMs as promising but risky: LLMs can summarize and propose hypotheses but are prone to omission, hallucination, and citation errors; the paper advocates human oversight, transparency, and possibly favoring open-source/transparent models.",
            "human_involvement": "Human-in-the-loop approach strongly recommended (vetting, fact-checking, and deciding which model outputs to pursue).",
            "dataset_or_corpus": "Large bodies of scientific literature generally (not a named dataset in this paper); authors note models are trained on web-sourced texts and large scientific corpora where used.",
            "limitations_or_challenges": "Hallucinations, fabricated or incorrect citations, lack of grounding in the cited manuscripts, potential loss of epistemic diversity, training-data contamination, opacity of proprietary systems, and reproducibility concerns.",
            "notable_examples": "Conceptual examples include the Galactica failure and the observation that LLM-generated literature reviews may produce claims not directly derived from the cited manuscripts (i.e., the machine predicts citations to accompany generated claims rather than deriving claims from those papers).",
            "uuid": "e5769.2",
            "source_info": {
                "paper_title": "How should the advent of large language models affect the practice of science?",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM-simulated participants",
            "name_full": "Using language models to simulate human samples / participants (e.g., Argyle et al.)",
            "brief_description": "Referenced line of work exploring whether LLMs can act as proxies for human subjects or annotators by simulating responses and social behavior across many texts, with cautionary notes about validity.",
            "citation_title": "Out of one, many: Using language models to simulate human samples",
            "mention_or_use": "mention",
            "llm_model_name": "Various generative LLMs (unspecified in this paper's discussion)",
            "llm_model_description": "Large pretrained generative transformers prompted to produce synthetic participant responses or social media behavior; specific architectures/size not provided here.",
            "task_goal": "Generate synthetic human-like responses or population samples to stand in for human participants in studies, or to simulate social media dynamics for research.",
            "domain": "Social sciences, political analysis, human-subjects research, computational social science",
            "methodology": "Prompting LLMs to simulate responses representative of particular demographics or conditions; used in simulation-style studies (referenced works include empirical investigations and simulations), with proposed comparisons to actual human data.",
            "type_of_qualitative_law": "In principle, could be used to elicit generalized behavioral regularities or social principles from aggregated model outputs, but the paper stresses conceptual limits rather than presenting distilled laws.",
            "evaluation_metrics": "Not specified in detail in this opinion piece; referenced concerns include fidelity to human communicative intent, behavioral validity, and how well synthetic samples match empirical human distributions.",
            "results_summary": "The paper cites works that experiment with LLM-simulated samples but stresses that LLMs model token sequences, not true human meanings or intentions; authors caution against treating LLM-generated participants as replacements for actual human subjects without strong validation.",
            "human_involvement": "Human oversight required; recommended that actual human participants remain the standard for many studies. Where LLMs are used, human evaluation/comparison to real data is necessary.",
            "dataset_or_corpus": "Implicitly trained on large web and social text corpora; specific corpora vary by underlying model and are not detailed in this paper.",
            "limitations_or_challenges": "Fundamental mismatch between token-distribution modelling and human concepts/intent; brittleness to input variation; risk of drawing invalid generalizations about human behavior; ethical concerns about fabricating data.",
            "notable_examples": "Referenced preprints and studies exploring simulated human samples and social-media simulations, but the paper emphasizes they are illustrative and that real human participants cannot be casually replaced.",
            "uuid": "e5769.3",
            "source_info": {
                "paper_title": "How should the advent of large language models affect the practice of science?",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A large language model for science",
            "rating": 2,
            "sanitized_title": "a_large_language_model_for_science"
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis",
            "rating": 2,
            "sanitized_title": "can_large_language_models_provide_useful_feedback_on_research_papers_a_largescale_empirical_analysis"
        },
        {
            "paper_title": "Out of one, many: Using language models to simulate human samples",
            "rating": 2,
            "sanitized_title": "out_of_one_many_using_language_models_to_simulate_human_samples"
        },
        {
            "paper_title": "Simulating social media using large language models to evaluate alternative news feed algorithms",
            "rating": 1,
            "sanitized_title": "simulating_social_media_using_large_language_models_to_evaluate_alternative_news_feed_algorithms"
        },
        {
            "paper_title": "Fabrication and errors in the bibliographic citations generated by chatgpt",
            "rating": 1,
            "sanitized_title": "fabrication_and_errors_in_the_bibliographic_citations_generated_by_chatgpt"
        },
        {
            "paper_title": "Chatgpt outperforms crowd workers for text-annotation tasks",
            "rating": 1,
            "sanitized_title": "chatgpt_outperforms_crowd_workers_for_textannotation_tasks"
        }
    ],
    "cost": 0.013874499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How should the advent of large language models affect the practice of science?
5 Dec 2023</p>
<p>Marcel Binz marcel.binz@tuebingen.mpg.de 
Max Planck Institute for Biological Cybernetics</p>
<p>Stephan Alaniz 
University of T übingen c University of Washington</p>
<p>Adina Roskies 
University of California</p>
<p>Balazs Aczel 
E ötv ös Lor ánd University
Budapest</p>
<p>Carl T Bergstrom 
Colin Allen 
University of California</p>
<p>Daniel Schad 
Health and Medical University
Potsdam</p>
<p>Dirk Wulff 
Max-Planck-Institute for Human Development i University of Basel j Rutgers University
New Brunswick</p>
<p>Jevin D West 
Qiong Zhang 
Richard M Shiffrin 
Indiana University
Bloomington</p>
<p>Samuel J Gershman 
Harvard University m University of Zurich n University of Milano</p>
<p>Ven Popov 
Emily M Bender 
Marco Marelli 
Matthew M Botvinick 
University College London q Helmholtz Center for Computational Health
Munich</p>
<p>Zeynep Akata 
University of T übingen c University of Washington</p>
<p>Eric Schulz 
Max Planck Institute for Biological Cybernetics</p>
<p>Santa Barbara 
-Bicocca 
Google Deepmind 
How should the advent of large language models affect the practice of science?
5 Dec 2023DEBE41CA711BD50E850BCBAA7FC9171DarXiv:2312.03759v1[cs.CL]
Large language models (LLMs) are being increasingly incorporated into scientific workflows.However, we have yet to fully grasp the implications of this integration.How should the advent of large language models affect the practice of science?For this opinion piece, we have invited four diverse groups of scientists to reflect on this query, sharing their perspectives and engaging in debate.Schulz et al. make the argument that working with LLMs is not fundamentally different from working with human collaborators, while Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools.Marelli et al. emphasize the importance of transparent attribution and responsible use of LLMs.Finally, Botvinick and Gershman advocate that humans should retain responsibility for determining the scientific roadmap.To facilitate the discussion, the four perspectives are complemented with a response from each group.By putting these different perspectives in conversation, we aim to bring attention to important considerations within the academic community regarding the adoption of LLMs and their impact on both current and future scientific practices.</p>
<p>Language models are statistical models of human language that can be used to predict the next token (e.g., a word or character) for a given text sequence.Even though these models have been around for decades 1,2 , they have recently experienced an unprecedented renaissance: by training enormous neural networks with billions of parameters on data sets with trillions of tokens, researchers have observed the emergence of models whose abilities can go beyond mere text generation and conversational skills 3 .</p>
<p>Modern large language models (LLMs) are, amongst other things, able to solve selected university-level math problems 4 , support language translation 5 , or answer questions in a bar exam with high accuracy 6 , out of the box and without additional training.Given the range of these capabilities, it seems possible that these systems will have an enormous impact on our society, leaving their mark on the labor market 7 , the education system 8 , and many other parts of our daily lives.</p>
<p>We -as scientists -may therefore wonder: how will the advent of LLMs affect the practice of science?Finding answers to this question is urgent as LLMs are already starting to permeate the academic landscape [9][10][11][12][13][14][15][16][17] .For instance, in 2022, MetaAI released the first science-specific LLM (under the name Galactica) aimed to support researchers in the process of knowledge discovery 18 .Even more recently, Terence Tao, a Fields Medal-winning mathematician, proclaimed 19 that "the 2023-level AI can already generate [. ..] promising leads to a working mathematician [. ..].When integrated with tools such as formal proof verifiers, internet search, and symbolic math packages, I expect, say, 2026-level AI [. ..] will be a trustworthy co-author [. ..]."</p>
<p>Yet while there have been claims of immense potential for this technology for the advancement of science, there are also considerable concerns that need to be taken into account.For instance, the aforementioned Galactica model had to be taken offline after just three days because it was heavily criticized by researchers for fabricating information, such as "fake papers (sometimes attributing them to real authors), and [...] wiki articles about the history of bears in space" 20 .Furthermore, even though LLMs often achieve state-of-the-art performance on existing benchmarks, it remains debated whether this reflects genuine understanding, or whether they are merely regurgitating the training set, thereby acting like stochastic parrots 21 .It has been, for instance, repeatedly demonstrated that even the most capable models at present fail at basic arithmetic problems such as multiplying two four-digit numbers 22 .Flaws like these are especially concerning if we intend to utilize LLMs for research purposes, and could endanger the integrity of science if we act carelessly.</p>
<p>The objective of the present article is to provide researchers with different opinions and a forum to voice and discuss their perspectives on if and how we should make use of LLMs in the context of science.To facilitate this discussion, we will first highlight a few applications where LLMs have the potential to positively impact science, followed by pointing out some of the issues that come with them.</p>
<p>Background: applications of LLMs in science</p>
<p>LLMs find their most obvious use case as a supporting tool for scientific writing.For example, as proofreaders of manuscript drafts, they can aid in rectifying grammatical errors, improving the writing style, and ensuring adherence to editorial guidelines.Beyond manuscript composition, LLMs could prove valuable for data acquisition and analysis in domains that were traditionally reliant on manual human labor 23,24 .Researchers have even suggested using LLMs as potential substitutes for human participants, as proxies 25 or for pilot studies 26 .In computational fields, LLMs could speed up prototyping by writing code 27 , while a human-in-the-loop would guide these processes, correct LLM-generated errors and ultimately decide which ideas warrant further pursuit.Moreover, researchers might experiment with employing LLMs at certain stages of research with progressively reduced supervision 28 , potentially leading to increased automation in some aspects of scientific exploration and discovery.</p>
<p>While the potential influence of LLMs on the practice of science is immense, there are pressing issues that come with the use of LLMs in the context of science.For instance, when an LLM helps us to write text, who ensures that its output is not subject to plagiarism issues 29 ?LLMs learn from web-sourced text data, acquiring inherent biases [30][31][32] and -in some casesreplicate excerpts from their training data 33 .When an LLM is used for data analysis, what happens when it hallucinates data?The content generated by LLMs can contain errors or fabricated information, presenting a potential threat to the integrity of scientific publishing 12 .When an LLM suggests an idea, who gets credit for it?The general consensus within the scientific community seems to indicate that LLMs are not eligible for (co-)authorship 34 as they cannot be held accountable for upholding scientific precision and integrity.Leading AI conferences such as ICML 1 and ACL2 -as well as journals such as Science3 , Nature 4 and PNAS 5 -have already adopted policies to limit the involvement of LLMs.However, it remains an open question how strong these regulations should be and if and how the usage of LLMs should be acknowledged.These -and many other -issues raise the questions: How should the advent of LLMs affect the practice of science?Do LLMs actually improve our scientific output or are they rather hindering good scientific practice?To what extent should they be used given the ethical and legal issues that come with them?We believe these to be highly non-trivial questions without an obvious answer and have therefore invited four groups of researchers to provide their perspectives on them.These perspectives were selected to cover a broad spectrum of opinions in order to spark a constructive discussion.Each of the perspectives is accompanied by a response from each group.We conclude this article with a short general discussion in which we attempt to identify common themes.</p>
<p>Perspective -LLMs: more like a human collaborator than a software tool Contributors: Eric Schulz, Daniel Schad, Marcel Binz, Stephan Alaniz, Ven Popov, and Zeynep Akata Most researchers in our labs already frequently employ LLMs in their everyday work.They use them, amongst other things, to finetune and revise their drafts, as a supporting tool for programming, to suggest formulations for research items such as questionnaires or experimental instructions, and to summarise research papers.We have observed a significant increase in quality in all of these areas after the widespread adoption of these models.While our personal experience may be biased, there are several studies supporting the idea that LLMs can facilitate writing 35 , coding 36 , and knowledge extraction 37 .In the future, we expect these models to be even more deeply integrated into the scientific process, taking on roles similar to a collaborator with whom one can develop and discuss ideas.</p>
<p>Indeed, we believe that working with LLMs will not be fundamentally different from working with other collaborators, such as research assistants or doctoral students.LLMs are not perfect and have limitations and biases that could affect their performance and output.However, humans are also subject to some of the same flaws, such as errors, plagiarism, fabrication, or discrimination.If we take this perspective, it seems appropriate to view current LLMs less as traditional software tools and more as knowledgeable research assistants: they can do phenomenal work but we need to be aware that they can make mistakes.</p>
<p>Protecting the past</p>
<p>It is our chief responsibility to ensure the quality and integrity of our work.There are already rules and norms about scientific practice in place to ensure this, and many of them also apply to LLMs.For instance, we should always check the accuracy and validity of the information and data we obtain, no matter the source, as well as correctly cite the sources and methods we use.That means that we should not blindly trust or rely on LLMs, but rather use them as a complement to our own expertise and judgment.Furthermore, our work can only be criticized appropriately if all information about its methodology is transparently communicated.We should therefore acknowledge the contributions of LLMs to our research, just as we would do for any other tool.Ultimately, it is -and will remain -the authors' responsibility to ensure that the appropriate scientific standards are followed, regardless of whether we use LLMs or not.</p>
<p>Ensuring that our research is reproducible is one of the cornerstones of modern science.However, as many LLMs are proprietary, working with them poses a threat to this ideal.Nobody guarantees that OpenAI, Google, or other providers will not make changes to their models (in the worst case, without informing the user).In fact, this happened to us during the revision process of one of our papers, where, at some point, we could not reproduce our initial results, likely due to changes on the provider side.How should we deal with such cases?We believe that the obvious solution to this problem is to rely on open-source models where one has full control over all aspects of the model.Following a recent call for action to the European Parliament 38 , we therefore strongly advocate for the development of such models, such that they can become the primary tool for scientific inquiry.</p>
<p>Welcoming the future</p>
<p>Paper reviewing is another area where LLMs could improve our scientific pipeline.In a recent study, Liang and colleagues 39 demonstrated this potential by systematically comparing LLM-generated reviews to reviews written by human researchers.They found that "more than half (57.4%) of the users found GPT-4 generated feedback helpful or very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers."Not only does this result allow scientistsespecially early career researchers -to receive high-quality, instantaneous feedback (similar to that one could get from a critical colleague with an unlimited amount of time) but it also has implications for the peer review process.Yet, the use of LLMs in the peer review process also presents one major legal obstacle: manuscripts under review are typically confidential, and hence should not be entered into proprietary LLMs.To prevent such breaches of confidentiality, the National Institutes of Health (NIH) and other institutions have rules in place that prohibit the use of LLMs for peer review. 6Locally hosted, open-source models are again a solution to this issue, as they provide control about which information is shared with external sources and which is not.</p>
<p>We also would like to point out that LLMs are a moving target, constantly evolving and becoming more capable and autonomous.This may raise new challenges and questions for the scientific community in the future, such as how to evaluate, interpret, and communicate the results generated by LLMs, or how to ensure their transparency and accountability.We welcome these challenges as an opportunity to advance our understanding and methods of science.We also encourage researchers to collaborate with each other and with LLM developers to address these issues and ensure that LLMs improve at frequently criticised skills such as providing truthful sources or acknowledging ignorance.</p>
<p>Conclusion</p>
<p>In conclusion, LLMs are a valuable asset for science and should be embraced rather than feared or restricted.It becomes apparent that they are not infallible machines once we start thinking about them as knowledgeable research assistants instead of traditional software tools.Furthermore, since rules for good scientific practice are already in place, and since it is the authors' obligation to take responsibility for adhering to these rules, there is no need for novel rules with the use of LLMs.We believe that strengthening the development of open-source alternatives should be one of our top priorities, as they "offer enhanced security, explainability, and robustness due to their transparency and the vast community oversight" 38 .Finally, being conscious about the current limitations of LLMs and embracing them, will allow us to grow with the technology as LLM research finds remedies and develops complementary tools.We hope that by adopting this liberal perspective, we can foster a positive and fruitful relationship between humans and LLMs in science.Please note that the first draft of our perspective was written by an LLM (GPT-4) based on our meeting notes.</p>
<p>Perspective -Science is a social process that cannot be auto-completed</p>
<p>Contributors: Emily M. Bender, Carl T. Bergstrom, and Jevin D. West When deciding whether to use an LLM, it is important to recognize that LLMs are simply models of word form distributions extracted from text -not models of the information that people might get from reading that text 40 .Originally, such systems were used to rank or classify text.In automatic transcription, for example, an acoustic model provides a set of possibilities and the language model helps determine the most likely next word 41 .Today, however, LLMs are vaunted for their ability to extrude synthetic text by repeatedly selecting a next likely token.</p>
<p>Trained on sufficiently large datasets and with sufficiently well-tuned architectures and training processes, LLMs appear to produce coherent text on just about any topic, including scientific ones.Moreover, we can't help but make sense of their textual output because our linguistic processing capabilities are instinctual and reflexive 42 .</p>
<p>Proponents argue that LLMs are useful in three domains: 1) navigating science, by searching and synthesizing published literature, 2) doing science, in the sense of designing or conducting experiments or generating data, and 3) communicating science, by drafting text for publication.While certain machine approaches may be useful in each, LLMs are unlikely to outperform alternative technologies.Furthermore, they have the potential to cause downstream harms to science if their use is widely embraced.</p>
<p>Navigating science</p>
<p>Natural language processing has proven useful in sorting through an ever-growing body of scientific literature.Information retrieval and extraction techniques, as implemented in academic search engines (e.g.ref. 43 ), have helped researchers discover relevant prior work.Will LLMs supplant other NLP approaches?We doubt it.The inappropriateness of LLMs as text generators and synthesis machines was highlighted in Meta's Galactica debacle.That system-taken off-line after three days in response to intense criticism for its abysmal performance-had been trained on scientific text and promoted as a tool to "summarize academic papers, solve math problems, generate Wiki articles, write scientific code, annotate molecules and proteins, and more." 20.But training an LLM on scientific papers doesn't guarantee that it will output scientifically accurate information.As Meta discovered, the use of LLMs yields text ungrounded in any communicative intent or accountability for accuracy.</p>
<p>One might hope that LLMs could at least be used to summarize a set of papers.Extractive summarization systems 44 already do this; will LLMs perform better?Will people tend to over-rely on system output rather than using it as a starting point?What are the costs of false negatives, i.e., important points not included in the generated summary?How will errors generated by LLMs, which then become training data for future LLMs, get amplified?</p>
<p>Doing science</p>
<p>LLMs are just one of many technologies dubbed "artificial intelligence", but their surprising capacity to perform what amounts to a fancy parlor trick has drawn outsized attention.That's a mistake.LLMs may be adequate for specific linguistic tasks such as grammar checking, automatic transcription, and machine translation (including code generation), but they are unlikely to provide an effective basis for most tasks involved in hybrid human-machine science.Even where they do appear to be moderately effective, they are known to be brittle to input variation 45 .The future of machine-aided science will not be a massive, one-size-fits-all, universal application of LLMs, but rather an ensemble of bespoke and often lightweight models that have been designed explicitly to solve the specific tasks at hand -and, crucially, evaluated in terms of those specific tasks.Such approaches also have a major advantage where interpretability is concerned.If researchers want to understand output variation, let alone find ways to fine-tune the architecture to generate better results, they need to steer away from technologies as opaque as LLMs.But instead, the ongoing hype around LLMs is drawing funding and brainpower away from more promising, targeted approaches.</p>
<p>Not only are LLMs being explored as aides to researchers; numerous proposals suggest that they can stand in for test subjects 46 , survey participants 47 , or data annotators 48 .Such arguments derive from a failure to understand that LLMs model the output of sequential linguistic tokens, not concepts, meanings, or communicative intent.If we are looking to study the opinions or behavior of human beings, we need to work with actual people.</p>
<p>Communicating science</p>
<p>By design LLMs generate form without substance.The synthetic text that systems output constitutes neither ideas, nor dataand it certainly is not a reliable information source.This notion of generating statements that no one intended is anathema to the spirit of scientific inquiry.Automatically generating something that looks like a manuscript is very different from the iterative process of actually writing a manuscript.Yet the output can be difficult to distinguish, particularly in a cursory read or by inexpert readers.Some proponents argue that LLMs can relieve scientists of the drudgery of writing papers and free them up to get on with the serious business of "doing science" 49 .This false dichotomy between communication and investigation reflects a fundamental misunderstanding of the nature of science 50 that devalues the communicative aspects of science and ignores the role of writing in the process of formulating, organizing, and refining ideas.Downstream, LLMs threaten the notion of scientific expertise, shift incentive structures 51 , and undermine trust in the literature.When human authors review or acknowledge prior literature, we are assured that they are familiar with the field and striving to situate their results therein.If LLMs write our introductions, we lose these guarantees.Moreover, notions of systematic review are undercut by the randomness inherent in LLM output.Finally, when a LLM generates a literature review, the claims that it generates are not directly derived from the manuscripts it cites.Rather, the machine creates textual claims, and then predicts the citations that might be associated with them.Obviously this practice violates all norms of scholarly citation.At best, LLMs gesticulate towards the shoulders of giants.</p>
<p>Driven by quantitative metrics and the strong incentive to publish, researchers may opt to trade off quality for speed by letting LLMs do much of their writing.Widespread use of one or a few LLMs could undercut epistemic diversity in science.When asked to provide a hypothesis, experiment, or mode of explanation, LLMs may repeatedly offer similar solutions, instead of leveraging the parallel creativity of an entire science community.</p>
<p>Worse still, opportunistic or malicious actors could use LLMs to generate nonsense at scale with minimal cost.(This is not an argument against using LLMs appropriately, but we need to be prepared for such behavior).Lazy authors could boost their publication counts by shotgunning machine-generated papers to low-quality journals.Predatory publishers could feign peer review using LLM output.Bad actors could overwhelm the manuscript submission system of a target journal (or even a target field) with a massive volume of fake papers.Or an investigator's work could be targeted with a deluge of spurious machine-generated critiques on post-publication peer review platforms such as Pubpeer.</p>
<p>Finally, LLMs may cause considerable collateral damage to science education.For example, as LLMs slash the cost of generating seemingly authoritative text, the web will be flooded with low-quality, mistake-ridden tutorials designed to capture advertising revenue.At present, search engines' ability to discriminate is more or less the only line of defense.That's worrisome.</p>
<p>Conclusion</p>
<p>In conclusion, LLMs are often mis-characterized, misused, and over-hyped, yet they will certainly impact the way we do science, from search to experimental design to writing.The norms that we establish now around their use will determine the consequences far into the future.We should proceed with caution -and evaluate at every step.</p>
<p>5/14</p>
<p>Perspective -LLMs in scientific practice: a matter of principles, not just regulations Contributors: Marco Marelli, Adina Roskies, Balazs Aczel, Colin Allen, Dirk Wulff, Qiong Zhang, and Richard M. Shiffrin A moderate perspective on the potential impact of LLMs on scientific practice holds that, while it is important to be mindful of the dangers, their application seems largely beneficial, insofar they offer a much needed support in day-to-day research activity and may alleviate major obstacles to scientific advancement.This is evident when LLMs are applied as editing tools: they provide a writing aid that leaves researchers with more time for brainstorming ideas and analysis, may help mitigate disparities between different scientific communities, and remedy some of the disadvantages for researchers who are not native speakers of English 52 .Also, LLMs can access a broader range of literature than any individual researcher could, potentially offering valuable support for literature analysis and hypothesis generation 53 , with a reach that goes beyond one's research specialization.</p>
<p>However, although any new technology may be used for good or evil, some technologies afford opportunities for good or evil uses more than others.LLMs have disruptive potential that is ever more evident, and such disruption must be kept at bay if the goal is to prevent "evil drifts".One perspective might hold that strict regulation is required, but regulation carries with it many costs that might be best avoided if it is kept moderate.A preferable approach may be to adopt clear principles guiding the way this technology should be used, principles that cannot just focus on efficiency and overall utility.Such principles include transparency, accountability, and fairness.</p>
<p>A matter of transparency</p>
<p>In science, transparency is of indispensable value.When used as writing tools, researchers must acknowledge the reliance on LLMs so that readers are on notice that the text is (at least partially) AI-generated.Authors should make explicit which LLMs were applied and how, as part of the method sections or in a separate dedicated statement.This could be achieved by relying on already existing solutions; for example, the CrediT taxonomy 7 could also be used to code the nature of AI contribution, even if AI is not to be recognized as a coauthor.Ideally, in the spirit of open science, authors shall publicly release their prompts along with the corresponding LLM responses as supplementary materials, and reference such archives in the manuscript.Importantly, transparency does not only pertain to the way we exploit LLMs, but to the systems themselves.LLMs are not, strictly speaking, anything new.Models that are analogous to current LLMs in structure, spirit, and basic mechanisms have been part of the scientific debate for decades 54 .However, such older models were unambiguous about their architecture and training, if not openly released.Current LLMs are often not held to the same scientific standards as their ancestors, being widely applied even when their inner workings and training data remain undisclosed.This causes substantial issues in estimating the actual performance of such models (and, importantly, the possibility of data contamination 55 ).As a scientific community valuing greater transparency, we should favour the systems that are taking some steps in that direction 56 .</p>
<p>A matter of accountability</p>
<p>It must be acknowledged that LLMs are instruments of human agency, and researchers should be held accountable for any scientific product they present to the community, irrespective of the extent to which this was obtained through the application of automatic systems.The Association for the Advancement in Artificial Intelligence has released clear guidelines in this respect: "Attribution of authorship carries with it accountability for the work, which cannot be effectively applied to AI systems . . .Ultimately, all authors are responsible for the entire content of their papers, including text, figures, references, and appendices."For example, LLMs are known to "hallucinate" and produce factually incorrect responses 57 .They can fabricate bibliographic citations, omit important references when summarizing literature, and potentially plagiarize text written by another researcher.The burden to verify that LLM-produced texts are accurate and that LLM-proofread texts are consistent with the original message remains with the individual authors.Similarly, LLMs can be particularly poor at logic and deductive reasoning 58,59 , so using them for analysis may lead to false conclusions.The onus is on the user to make sure that what LLMs produce is worth pursuing.Researchers must hence have strategies for assessment over AI-related content; a good practice would be to have clear quality criteria and verification methods defined before using LLMs.Scientists should not underestimate the time and effort that such vetting will take, and should weigh the efficiency of LLM application against these costs.</p>
<p>A matter of fairness AI in general and LLMs in particular have the potential to deeply affect us at a societal level.Science, as any human endeavour, is not immune to this.As a community we must make all possible efforts to guarantee that reliance on LMMs does not violate basic fairness principles.Indeed, current language models reflect mostly WEIRD (Western Educated Industrialized Rich Democratic) populations and cannot easily be prompted to represent non-WEIRD communities 60,61 .This leads to biases in writing and annotation, potentially reinforcing distortions in citations and marginalization of already marginalized scientists.</p>
<p>Moreover, it may have negative consequences in terms of equitable research, given that LLMs are also more accessible to WEIRD populations.More generally, LLMs will, for known or unknown reasons, favour some perspectives or sources over others 62 .These systematic patterns must be recognized and taken into account, to avoid unprincipled biases affecting the direction of research and possibly the relative success of careers.</p>
<p>Conclusion</p>
<p>The impact that LLMs are having on scientific practice cannot be understated.Given the current trend, at the time you are reading these words such impact will likely be much larger than it is as we write this piece.Precisely how LLMs will influence the practice of science in the future cannot be entirely predicted and countering such a revolution with strict, preconceived norms is a losing battle.Rather, establishing principles and shared values in the scientific community constitutes the ideal foundation when deciding how to manage these rapidly changing technologies.Most importantly, we need to train students and each other to build upon such principles in order to become appropriately skeptical towards these systems and their outputs.</p>
<p>Perspective -AI can help, but science is for people</p>
<p>Contributors: Matthew M. Botvinick and Samuel J. Gershman Like many forms of technology, AI can substitute for human labor.With the advent of LLMs, the relevant kinds of labor begin to overlap with high-level human cognitive work, including the activities involved in science 15 .As LLMs improve, their ability to substitute for human scientific labor will be a major boon.However, we argue here that two core aspects of scientific work should be reserved to human scientists.</p>
<p>AI and scientific labor</p>
<p>Over time, the labor involved in scientific research has become progressively more onerous, sometimes now bordering on the intractable.Assimilating current knowledge has become more difficult in the face of increasingly voluminous literatures.Generating new questions, hypotheses and experimental tests has become more challenging, as the search problem entailed by each has become more complex.Drawing conclusions from experimental results has become harder as the size and complexity of datasets has exploded.And communicating and debating scientific conclusions has become more challenging for reasons including an overtaxing of peer review systems 63 .Given the increasing costs of scientific labor on these fronts, it's no surprise that progress across multiple scientific fields appears to have slowed 64 .</p>
<p>In the long run, AI may help us cope with the increasing demands of scientific work.Through the kinds of application detailed in the introductory essay above, AI may help us scale up, by making each step in the research cycle cheaper.In some cases, AI may eventually perform some forms of scientific labor better than human scientists, including the work of generating new hypotheses 65 .Even in present-day forms, AI may be useful on some fronts, as reviewed in the introduction.Of course, as widely discussed, current systems are too unreliable to deploy without caution and oversight (see accompanying commentaries), and only time will tell how feasible it may be to overcome current limitations.</p>
<p>However, in addition to addressing present-day shortcomings, it's equally important to look into the future and consider what kind of AI tools we actually want for science in the long run.Given that AI can be applied to all phases of scientific work, one aim might be to build a full-fledged AI scientist, one that can do everything a human scientist now does: a full-spectrum replacement for human scientists.To us, this prospect is deeply unappealing.Why?Because there are particular aspects of science that we simply would not want to delegate to AI, even in a scenario where technical limitations presented no barrier.In particular, there are two core aspects of science that should be left to people.As we now explain, one of these is normative and the other epistemic.</p>
<p>The normative aspect of science</p>
<p>Any scientific discipline must continually ask, What problems shall we work on?How this question gets answered, both within individual labs and across whole research communities, is a complex affair, but it centers on judgments concerning the 'interest' and 'significance' of candidate problems, as well as their 'timeliness,' including their amenability to study under prevailing material and ethical constraints.Such judgments are informed by hard data; we obviously cannot reduce them to purely social constructions.However, at the same time, judgments of interestingness, significance and timeliness are inherently tied to culturally and historically grounded sensibilities and mores.This is not a corruption or impurity in scientific thought and procedure.Cultural sensibilities and patterns of thought are fundamental to scientific prioritization.This point will be especially salient to students of the history of science, because the sensibilities and mores that inform science evolve over time.Just as scientific theory changes over the years, so do the ethical commitments and intellectual priorities that underlie science.This is evident in the fact that we no longer approach homosexuality as a disorder, or study genetics through the lens of eugenics.It shows in growing restrictions on animal experimentation.And it shows in the attention that Western climatologists now pay to regions historically neglected.</p>
<p>We argue that the normative aspect of science should not be ceded to AI systems, no matter how capable those systems become.People should stay in the driver's seat, determining the direction of travel for science.Certainly, AI systems may be helpful partners in deliberation, especially as techniques for AI value alignment improve 66 .However, aligning a system to currently prevailing human views is different from allowing that system to govern the evolution of human views.In science, the ultimate driving force in that evolution should remain human.We are the moral agents in the room, and we shouldn't forget it.</p>
<p>The epistemic aspect of science</p>
<p>Obviously, a central goal of basic science is understanding the natural world.If we are going to do science with AI tools, the question arises: 'whose' understanding matters?Would it be satisfactory to have AI systems that in some sense understand aspects of nature, but which don't make that understanding accessible to people?From an engineering standpoint that might be fine.However, if it's basic science we're talking about, we shouldn't let go of the core objective, which is not just practical but epistemic.We cannot cede understanding to artificial systems.We should insist on human understanding remaining a core goal of science.</p>
<p>Of course, it may be that because of limitations on human cognition, AI systems may someday be able to represent some aspects of nature that we cannot, just as existing AI systems master aspects of complex board games that elude even highly skilled human players 67 .Even in these cases, however, we should strive to extract as much human insight from AI systems as possible 68 .We shouldn't lose track of what basic science is for.</p>
<p>Conclusion</p>
<p>AI promises to deliver great value in science, just as in many other domains.We believe its potential should be embraced.However, at the same time that we strive to break through the current limitations of AI to access its benefits, we should also think through our long-term goals in developing this technology.In the end, the two areas of science we've proposed to protect -one normative, the other epistemic -are two reflections of a more general bound on AI's proper domain.We might call this the subjective limit.Unlike AI systems, people have a 'point of view,' which cannot be automated because it's inherently subjective 69 .This point of view includes knowledge that is meaningful to us (the epistemic view) and values that are meaningful to us (the normative view).Machines might have their own knowledge or values, and these might be aligned with ours, but the alignment problem is fundamentally yoked to our subjective views.This principle applies in science, as in all human-centered activities.</p>
<p>Response by Eric Schulz, Daniel Schad, Marcel Binz, Stephan Alaniz, Ven Popov, and Zeynep Akata</p>
<p>We have argued that one should think of working with LLMs less as using a traditional software tool and more as working with a human collaborator and that this perspective allows us to better understand their shortcomings.This view actually resonates with many of the points raised in the other perspectives.For example, Marelli et al. write that "we should not blindly trust or rely on LLMs, but rather use them as a complement to our own expertise and judgment", and Bender et al. argue that collaboration in science means iterating over outputs many times.Like working with a human collaborator, working with LLMs is an iterative process in which we constantly check for facts and logical consistency, revise arguments, and identify new connections.This process takes time and is more than just booting up an LLM and copy-pasting its outputs; as nicely put by Marelli et al., we "should not underestimate the time and effort that such vetting will take, and should weigh the efficiency of LLM application against these costs."</p>
<p>We would like to stress that the notion that "LLMs are simply models of word form distributions extracted from text" oversimplifies both their capabilities and the additional engineering effort involved in modern LLMs.If one takes steps like reinforcement learning from human feedback 70 or instruction tuning 71 out of the equation, the outputs produced by such models are rather uninspiring (anyone who has ever worked with a plain LLM can attest to this).However, with those ingredients, LLMs do not just mimic language patterns; they can also synthesize concepts, critically evaluate their own outputs, and assist in problem-solving by processing vast amounts of data.</p>
<p>Bender et al. argue that "the future of machine-aided science will not be a massive, one-size-fits-all, universal application of LLMs, but rather an ensemble of bespoke and often lightweight models that have been designed explicitly to solve the specific tasks at hand [...]."We believe LLMs are widely adopted precisely because they are a universal tool to accomplish many tasks.Not only does that remove the need to build specialized tools for each application, but it also eradicates the time it takes to learn them.Like human collaborators, who bring a diverse range of skills to a project, LLMs offer a breadth of knowledge that can be tailored to specific needs, e.g., as shown with the finetuning of coding LLMs 27 .There are -of course -applications that benefit from purposefully designed tools, but we believe that the percentage of such applications is modest once we take the time required to develop and learn such tools into account.</p>
<p>Finally, there is the question of how much autonomy we want to transfer to LLMs or other AI systems.Botvinick and Gershman advocated that people should retain control over certain aspects of the scientific pipeline, such as deciding which topics to work on.We do not think that such a constraint is necessary.For example, if in the future, an LLM (or any other AI system) decides to work on a topic that it deems interesting, and this LLM has proven itself to select topics in a very fruitful and productive manner, should we stop it?We do not think so as long as ethical and legal guidelines are followed.Deciding on scientific topics is hard, and it is often not a priori known which research directions will be fruitful.Therefore, we should take any help we can get.Human researchers and AI systems bring complementary strength to the table, and acknowledging this collaborative spirit enables us to leverage the best out of both worlds.</p>
<p>Response by Emily M. Bender, Carl T. Bergstrom, and Jevin D. West Science is a social process.It cannot be auto-completed.Its agents -real scientists -are as much the product of this process as the results recorded in papers.</p>
<p>LLM optimists envision a new world, where machines write, review, and even do much of the science.Even the less extreme narrative wherein LLMs simply aid researchers suffers from a misplaced and almost Taylorist 72 optimism regarding production efficiency.Science is not a factory, churning out widgets or statistical analyses wrapped in text.For a factory, producing one more car per day is progress.For science, the goals are to understand our world -not to produce more artifacts that look like scientific papers.If science were a paper factory, we too would indulge in LLM euphoria and might even claim a significant resulting improvement in quality coming out of our labs.But we cannot equate papers and progress.Papers are but messages that we send one another to coordinate our collective quest for scientific understanding.</p>
<p>We don't, however, believe that any new mandates are required prohibiting the use of LLMs.All ill-advised use cases are already contrary to the norms of science: Using LLMs as stand-ins for human subjects or annotators amounts to fabricating data; using LLMs to write first drafts runs afoul of prohibitions against plagiarism, as it is impossible to discern the source of any string produced by an LLM; treating LLMs as co-authors contravenes norms around authorship, since LLMs are not the sort of thing that can be accountable for paper contents; using LLMs to produce peer reviews is tantamount to abrogating our responsibility to deeply evaluate the methods, reasoning and conclusions of our peers' work.</p>
<p>When contemplating how LLMs will affect science, we should not underestimate the temptation to use them under deadline pressure or in response to publish-or-perish threats to job security.Nor should we underestimate the time needed to fact-check all LLM output-not only for the inevitable and frequent errors but also to assess whether citations are accurate.We note that there are no published user studies that quantify just how much effort this checking process is, nor how accurately researchers can carry it out, especially while working under pressure.Norms of plagiarism and the weight of reputation will hopefully counterbalance the unfettered use of this new technology.</p>
<p>To reason appropriately about when LLMs are suitable within science, it is critical to avoid anthropomorphizing them.These models aren't research assistants.They are tools.They don't make mistakes like junior (or senior!) researchers do: People can take responsibility for, and learn from, mistakes.Tools produce errors; thus people using the tools have a responsibility to understand their affordances and use them with care.</p>
<p>Similarly, understanding LLMs as tools positions us to ask: Is this the best tool for this task?Often, we expect, LLMs are not.Even setting aside the closed proprietary models, their attendant failures of transparency, and the stochastic nature of LLM output, we expect that bespoke models designed for specific tasks will be more efficient, performant, interpretable, and easier to fix when not functioning well.</p>
<p>Ultimately, science is a conversation and the interlocutors are the scientists.Synthetic text-extruding machines, designed only to produce plausible-sounding prose, are not fit participants in that conversation and should not be treated as such.</p>
<p>Response by Marco Marelli, Adina Roskies, Balazs Aczel, Colin Allen, Dirk Wulff, Qiong Zhang, and Richard M. Shiffrin</p>
<p>In our proposal concerning the application of LLMs in science, we aimed for a moderate perspective.In that spirit, we think that such systems can be profitably incorporated into scientific practice (in line with Schulz et al.), but we also recognize that there are causes for reservation (in line with Bender et al.).</p>
<p>We disagree with the view that LLMs should be considered collaborators or research assistants (Schulz et al.).One can instruct students or research assistants, correct their mistakes, and anticipate that they will learn from them.One may also question their reasons or their reasoning and get answers and expect accountability.Finally, one may also get insight into their values and their motivations, and trust or distrust them accordingly.LLMs are not introspective, lack metacognition, and have no values, at least not in the way humans do.Indeed, our inability to understand why they make the errors they do or when 9/14 they will make them impairs our ability to understand their limits, especially on the edges of knowledge, where their training corpus is arguably less robust.Moreover, although LLMs move from the same foundations of previous language models (Schulz et al.), they are significantly more opaque and complex.As a result, maintaining the ever-important scientific value of transparency can be challenging and necessitates further development of practices and strategies to ensure its preservation.</p>
<p>Nevertheless, we disagree that such concerns should prevent scientific applications of LLMs.It is unrealistic to presume that LLMs won't be used because of the risks involved, and banning them could do more harm than good: given the current trend, if prohibited, they would likely be used covertly, exacerbating the already-worrying transparency issues.Certainly, we need to pursue a critical and not starry-eyed understanding of LLMs and maintain a clear-eyed assessment of the potential risks of use.However, there are ways of employing them that can improve the quality of science, as long as the researcher is kept at the center of the process.LLMs are tools and, as such, must be carefully evaluated in their applications.This applies to any tool, including the existing alternatives discussed by Bender et al., which, although optimized for specific scientific purposes, are not immune from mistakes and whose degree of reliability always needs careful scrutiny.At the end of the day, the responsibility falls upon the shoulders of the researchers who use the tools.It is, hence, crucial to establish principles and values that guide our decisions-whether one applies LLMs or any other method.</p>
<p>Ultimately, we mostly concur with Botvinick and Gershman: the impact of LLMs on the future practice of science cannot be fully predicted, but science is a humanistic and human enterprise and must remain so, motivating curbs to LLM use.Our perspective highlighted the normative aspects in terms of core values that should guide their use today, while Botvinick and Gershman seeks to identify the principles and values for the future, deciding what should remain exclusively human even when AI becomes fully capable of performing every step of scientific inquiry as well as upholding values such as accountability, transparency, and fairness.The two perspectives complement each other in stimulating discussions about what should guide the way we integrate AI into our scientific practices.</p>
<p>Response by Matthew M. Botvinick and Samuel J. Gershman</p>
<p>We see significant common ground across the other perspectives.We will focus here on one issue that gets to the heart of our perspective.Schulz et al. characterize LLMs as closer to collaborators than to tools.This raises critical issues of accountability, as pointed out by Bender et al. and Marelli et al.Some of these issues are currently being grappled with, while others will become more salient in the future as the technology advances.In particular, accountability is a fundamentally human concept: humans are the only currently existing agents that are accountable in the sense that they have ultimate control over their own actions and voluntarily submit to a system that regulates these actions.Extending this concept to artificial agents would entail a profound shift in our attitudes, essentially requiring us to acknowledge the personhood of such agents.</p>
<p>This shift, if it ever happens, will have ramifications far beyond science.Policymakers are already starting to wrestle with the question of how accountability should operate in a world where AI systems are increasingly autonomous, and the issues can get quite complex.The difficulties can be bounded, however, in domains where humans are able to draw clear boundaries around what role they will permit AI systems to play.In science, we believe these boundaries should be firm and restrictive, limiting key decisions -and thus accountability -to human scientists.</p>
<p>Ultimately, we are interested in the limit case where the limits imposed on AI are sociological, moral, and juristic, rather than technological.To regard LLMs as genuine collaborators rather than sophisticated tools, we would need to acknowledge attributes of personhood that go far beyond the mere practice of science.Our view is that AI, no matter how intelligent, should remain a tool, because ceding personhood to artificial agents would have undesirable consequences.It's one thing for an AI scientist to tell us that there is a better way to fold proteins or design nuclear reactors, but it's quite another thing for it to tell us that it would rather be studying some other problem.It would also be quite a shock to be told by an AI scientist that it's solved an important problem but that it doesn't feel like trying to explain it to a human.As we argued in our perspective, the choices of what to study and which explanations count are irreducibly human.</p>
<p>Conclusion</p>
<p>We have presented four different perspectives centering around the question "how should the advent of LLMs affect the practice of science?" Schulz et al. argued that "working with LLMs will not be fundamentally different from working with other collaborators, such as research assistants or doctoral students."Bender et al. described a suite of problems with using LLMs in scientific activity and argued that many uses of LLMs are "contrary to the norms of science."Marelli et al. called for "clear principles guiding the way this technology should be used," including transparency, accountability, and fairness.Finally, Botvinick and Gershman advocated that "two core aspects of scientific work should be reserved to human scientists", namely deciding on what problems to work on and that human understanding remains the goal of science.</p>
<p>Yet, even though there was substantial disagreement, there were also important common themes.In particular, all parties emphasized the social nature of science and the importance of protecting scientific integrity and standards.In modern times, 10/14 these core values are more important than ever before, and we -as a community -will have to continuously reevaluate how to protect them.
 https://icml.cc/Conferences/2023/llm-policy
https://2023.aclweb.org/blog/ACL-2023-policy/
https://www.science.org/content/page/science-journals-editorial-policies
https://www.nature.com/nature-portfolio/editorial-policies/ai
https://www.pnas.org/author-center/editorial-and-journal-policies#authorship-and-contributions 2/14
https://grants.nih.gov/grants/guide/notice-files/NOT-OD-23-149.html <br />
https://credit.niso.org/ 6/14
/14
AcknowledgementsThis work has been partially funded by the ERC (853489 -DEXIM; 101087053 -BraveNewWord), by the DFG (2064/1 -Project number 390727645), the BMBF (Tübingen AI Center, FKZ: 01IS18039A) and as part of the Excellence Strategy of the German Federal and State Governments.Author contributions statement
A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, Adv. neural information processing systems. 132000</p>
<p>Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition. D Jurafsky, J H Martin, 2009Pearson Prentice Hall</p>
<p>Language models are few-shot learners. T Brown, Adv. neural information processing systems. 332020</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. I Drori, Proc. Natl. Acad. Sci. Natl. Acad. Sci2022119e2123433119</p>
<p>Large language models are state-of-the-art evaluators of translation quality. T Kocmi, C Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. M Nurminen, the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>Gpt-4 passes the bar exam. D M Katz, M J Bommarito, S Gao, P Arredondo, Available at SSRN. 43892332023</p>
<p>T Eloundou, S Manning, P Mishkin, D Rock, arXiv:2303.10130.Unpubl.preprintGpts are gpts: An early look at the labor market impact potential of large language models. 2023</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. E Kasneci, Learn. Individ. Differ. 1031022742023</p>
<p>On chatgpt and beyond: How generative artificial intelligence may affect research, teaching, and practice. R Peres, M Schreier, D Schweidel, A Sorescu, Int. J. Res. Mark. 2023</p>
<p>Chatting about chatgpt: how may ai and gpt impact academia and libraries?. B D Lund, T Wang, Libr. Hi Tech News. 402023</p>
<p>A chat (gpt) about the future of scientific publishing. E L Hill-Yardin, M R Hutchinson, R Laycock, S J Spencer, Brain Behav Immun. 1102023</p>
<p>Chatgpt in scientific writing: a cautionary tale. The Am. H Zheng, H Zhan, J. Medicine. 2023</p>
<p>Chatgpt and a new academic reality: Artificial intelligence-written research papers and the ethics of the large language models in scholarly publishing. B D Lund, J. Assoc. for Inf. Sci. Technol. 742023</p>
<p>Can gpt-3 write an academic paper on itself, with minimal human input?. G G P Transformer, A O Thunström, S Steingrimsson, 2022Unpublished</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, S Wachter, Nat. Rev. Phys. 142023</p>
<p>Friend or foe? exploring the implications of large language models on the science system. B Fecher, M Hebing, M Laufer, J Pohle, F Sofsky, arXiv:2306.09928.Unpubl.preprint2023</p>
<p>What chatgpt and generative ai mean for science. C Stokel-Walker, R Van Noorden, 10.1038/d41586-023-00340-62023</p>
<p>R Taylor, arXiv:2211.09085A large language model for science. 2022Unpubl. preprint</p>
<p>Embracing change and resetting expectations. </p>
<p>Why Meta's latest large language model survived only three days online. W D Heaven, 2022</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>K Arkoudas, arXiv:2308.03762Gpt-4 can't reason. 2023Unpubl. preprint</p>
<p>Chatgpt outperforms crowd workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, 10.1073/pnas.2305016120Proc. Natl. Acad. Sci. 120, e2305016120. Natl. Acad. Sci. 120, e23050161202023</p>
<p>Automated jingle-jangle detection: Using embeddings to tackle taxonomic incommensurability. D U Wulff, R Mata, 10.31234/osf.io/9h7awPsyArXiv DOI. 2023</p>
<p>Can ai language models replace human participants?. D Dillion, N Tandon, Y Gu, K Gray, Trends Cogn. Sci. 2023</p>
<p>M Hutson, 10.1126/science.adj6791Guinea pigbots. Sci. New York, N.Y.2023381</p>
<p>Code llama: Open foundation models for code. B Rozière, arXiv:2308.129502023Unpubl. preprint</p>
<p>A step-by-step researcher's guide to the use of an ai-based transformer in epidemiology: an exploratory analysis of chatgpt using the strobe checklist for observational studies. F Sanmarchi, J. Public Heal. 2023</p>
<p>Plagiarism in the age of massive generative pre-trained transformers (gpt-3). N Dehouche, Ethics Sci. Environ. Polit. 212021</p>
<p>Towards understanding and mitigating social biases in language models. P P Liang, C Wu, L.-P Morency, R Salakhutdinov, International Conference on Machine Learning. PMLR2021</p>
<p>Inducing anxiety in large language models increases exploration and bias. J Coda-Forno, arXiv:2304.11111.Unpubl.preprint2023</p>
<p>Social biases in NLP models as barriers for persons with disabilities. B Hutchinson, 10.18653/v1/2020.acl-main.487Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Extracting training data from large language models. N Carlini, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>A place for large language models in scientific publishing, apart from credited authorship. M R King, Cell. Mol. Bioeng. 2023</p>
<p>write an essay for me: A large-scale comparison of human-written versus chatgpt-generated essays. S Herbold, A Hautli-Janisz, U Heuer, Z Kikteva, A Trautsch, Ai, arXiv:2304.14276.Unpubl.preprint2023</p>
<p>R A Poldrack, T Lu, G Beguš, Ai, arXiv:2304.13187Experiments with gpt-4. 2023</p>
<p>News summarization and evaluation in the era of gpt-3. T Goyal, J J Li, G Durrett, arXiv:2209.12356.Unpubl.preprint2022</p>
<p>Towards a transparent ai future: The call for less regulatory hurdles on open-source ai in europe. </p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. W Liang, arXiv:2310.01783.Unpubl.preprint2023</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. E M Bender, A Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>An overview of end-to-end automatic speech recognition. D Wang, X Wang, S Lv, 10.3390/sym11081018Symmetry. 112019</p>
<p>Entrenchment and the Psychology of Language Learning: How We Reorganize and Adapt Linguistic Knowledge. R J Hartsuiker, A Moors, 10.1037/15969-010Schmid, H.-J.American Psychological AssociationOn the automaticity of language processing</p>
<p>. De Gruyter, Mouton , 2017</p>
<p>The semantic scholar open data platform. R M Kinney, ArXiv. Unpubl. preprint abs/2301.101402023</p>
<p>Ranking sentences for extractive summarization with reinforcement learning. S Narayan, S B Cohen, M Lapata, 10.18653/v1/N18-1158Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. M Walker, H Ji, A Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Response: Emergent analogical reasoning in large language models. D Hodel, J West, 2308.161182023</p>
<p>Simulating social media using large language models to evaluate alternative news feed algorithms. P Törnberg, D Valeeva, J Uitermark, C Bail, 2310.05984. 12/142023</p>
<p>Out of one, many: Using language models to simulate human samples. L P Argyle, 10.1017/pan.2023.2Polit. Analysis. 312023</p>
<p>Chatgpt outperforms crowd workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, 10.1073/pnas.2305016120Proc. Natl. Acad. Sci. 120, e2305016120. Natl. Acad. Sci. 120, e23050161202023</p>
<p>How ChatGPT and other AI tools could disrupt scientific publishing. G Conroy, Nature. 6222023</p>
<p>Laboratory life: The construction of scientific facts. B Latour, S Woolgar, 2013Princeton university press</p>
<p>Toward a new economics of science. D Partha, P A David, Res. policy. 231994</p>
<p>The manifold costs of being a non-native english speaker in science. T Amano, PLoS Biol. 21e30021842023</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, Nature. 5962021</p>
<p>Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. F Günther, L Rinaldi, M Marelli, Perspectives on Psychol. Sci. 142019</p>
<p>Time travel in llms: Tracing data contamination in large language models. S Golchin, M Surdeanu, arXiv:2308.08493.Unpubl.preprint2023</p>
<p>Starcoder: may the source be with you!. R Li, arXiv:2305.061612023Unpubl. preprint</p>
<p>Fabrication and errors in the bibliographic citations generated by chatgpt. W H Walters, E I Wilder, Sci. Reports. 13140452023</p>
<p>Chatgpt: Jack of all trades, master of none. J Kocoń, Inf. Fusion. 1018612023</p>
<p>H Liu, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023</p>
<p>Towards measuring the representation of subjective global opinions in language models. E Durmus, arXiv:2306.16388Unpubl. preprint. 2023</p>
<p>. M Atari, M J Xue, P S Park, D Blasi, Henrich, J. Which humans? PsyArXiv. Unpubl. preprint. 2023</p>
<p>Whose opinions do language models reflect?. S Santurkar, arXiv:2303.175482023Unpubl. preprint</p>
<p>The peer review crisis. C Flaherty, </p>
<p>Papers and patents are becoming less disruptive over time. M Park, E Leahey, R J Funk, Nature. 6132023</p>
<p>Advancing mathematics by guiding human intuition with ai. A Davies, Nature. 6002021</p>
<p>The Challenge of Value Alignment: From Fairer Algorithms to AI Safety. I Gabriel, V Ghazavi, 10.1093/oxfordhb/9780198857815.013.18The Oxford Handbook of Digital Ethics. Oxford University Press</p>
<p>Mastering the game of go without human knowledge. D Silver, nature. 5502017</p>
<p>Rediscovering orbital mechanics with machine learning. P Lemos, N Jeffrey, M Cranmer, S Ho, P Battaglia, Mach. Learn. Sci. Technol. 4450022023</p>
<p>M Botvinick, Have we lost our minds. </p>
<p>Learning to summarize with human feedback. N Stiennon, Adv. Neural Inf. Process. Syst. 332020</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. S Longpre, Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org. the 40th International Conference on Machine Learning, ICML'232023</p>
<p>F W Taylor, The Principles of Scientific Management. Harper1913</p>            </div>
        </div>

    </div>
</body>
</html>
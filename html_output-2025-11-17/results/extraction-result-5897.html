<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5897 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5897</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5897</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-3dc24fa1a2851b792139a239342381de1ec5a440</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3dc24fa1a2851b792139a239342381de1ec5a440" target="_blank">MOFormer: Self-Supervised Transformer Model for Metal–Organic Framework Property Prediction</a></p>
                <p><strong>Paper Venue:</strong> Journal of the American Chemical Society</p>
                <p><strong>Paper TL;DR:</strong> A self-supervised learning framework that pretrains the MOFormer via maximizing the cross-correlation between its structure-agnostic representations and structure-based representations of crystal graph convolutional neural network (CGCNN) on >400k publicly available MOF data provides a novel perspective on efficient MOF design using deep learning.</p>
                <p><strong>Paper Abstract:</strong> Metal–organic frameworks (MOFs) are materials with a high degree of porosity that can be used for many applications. However, the chemical space of MOFs is enormous due to the large variety of possible combinations of building blocks and topology. Discovering the optimal MOFs for specific applications requires an efficient and accurate search over countless potential candidates. Previous high-throughput screening methods using computational simulations like DFT can be time-consuming. Such methods also require the 3D atomic structures of MOFs, which adds one extra step when evaluating hypothetical MOFs. In this work, we propose a structure-agnostic deep learning method based on the Transformer model, named as MOFormer, for property predictions of MOFs. MOFormer takes a text string representation of MOF (MOFid) as input, thus circumventing the need of obtaining the 3D structure of a hypothetical MOF and accelerating the screening process. By comparing to other descriptors such as Stoichiometric-120 and revised autocorrelations, we demonstrate that MOFormer can achieve state-of-the-art structure-agnostic prediction accuracy on all benchmarks. Furthermore, we introduce a self-supervised learning framework that pretrains the MOFormer via maximizing the cross-correlation between its structure-agnostic representations and structure-based representations of the crystal graph convolutional neural network (CGCNN) on >400k publicly available MOF data. Benchmarks show that pretraining improves the prediction accuracy of both models on various downstream prediction tasks. Furthermore, we revealed that MOFormer can be more data-efficient on quantum-chemical property prediction than structure-based CGCNN when training data is limited. Overall, MOFormer provides a novel perspective on efficient MOF property prediction using deep learning.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5897",
    "paper_id": "paper-3dc24fa1a2851b792139a239342381de1ec5a440",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0033855,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MOFormer: Self-Supervised Transformer model for Metal-Organic Framework Property Prediction</h1>
<p>Zhonglin Cao, ${ }^{\dagger, \S}$ Rishikesh Magar, ${ }^{\dagger, \S}$ Yuyang Wang, ${ }^{\dagger}$ and Amir Barati<br>Farimani ${ }^{*}, \dagger, \ddagger, \boldsymbol{\square}$<br>$\dagger$ Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh PA, USA 15213<br>$\ddagger$ Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh PA, USA 15213<br>¶ Machine Learning Department, Carnegie Mellon University, Pittsburgh PA, USA 15213<br>§Joint First Authorship<br>E-mail: barati@cmu.edu</p>
<h4>Abstract</h4>
<p>Metal-Organic Frameworks (MOFs) are materials with a high degree of porosity that can be used for applications in energy storage, water desalination, gas storage, and gas separation. However, the chemical space of MOFs is close to an infinite size due to the large variety of possible combinations of building blocks and topology. Discovering the optimal MOFs for specific applications requires an efficient and accurate search over an enormous number of potential candidates. Previous high-throughput screening methods using computational simulations like DFT can be time-consuming. Such methods also require optimizing 3D atomic structure of MOFs, which adds one extra</p>
<p>step when evaluating hypothetical MOFs. In this work, we propose a structure-agnostic deep learning method based on the Transformer model, named as MOFormer, for property predictions of MOFs. The MOFormer takes a text string representation of MOF (MOFid) as input, thus circumventing the need of obtaining the 3D structure of hypothetical MOF and accelerating the screening process. Furthermore, we introduce a self-supervised learning framework that pretrains the MOFormer via maximizing the cross-correlation between its structure-agnostic representations and structure-based representations of crystal graph convolutional neural network (CGCNN) on $&gt;400$k publicly available MOF data. Using self-supervised learning allows the MOFormer to intrinsically learn 3D structural information though it is not included in the input. Experiments show that pretraining improved the prediction accuracy of both models on various downstream prediction tasks. Furthermore, we revealed that MOFormer can be more data-efficient on quantum-chemical property prediction than structure-based CGCNN when training data is limited. Overall, MOFormer provides a novel perspective on efficient MOF design using deep learning.</p>
<h2>Introduction</h2>
<p>Metal-organic frameworks (MOFs) are a type of porous crystalline materials, [1, 2] which have been extensively researched during the past several decades. Research interests have been induced by the porous structure and versatile nature of MOFs on their potential applications such as gas adsorption, [3, 4, 5] water harvesting and desalination, [6, 7, 8] and energy storage. [9, 10, 11] MOFs typically consist of several building blocks, including metal-based metal nodes and organic linkers. [4, 12, 13] The assembly of those building blocks following certain topologies generates the 2-dimensional or 3-dimensional porous structures of MOFs. Because of the countless possible combinations of metal nodes, organic linkers, and topologies, [13, 14] there is a sheer amount of MOFs with different mechanical properties and surface chemistry. Given the enormous variety of possible MOF structures, rapidly and inexpensively selecting the potential</p>
<p>top performers for each specific task can be challenging. High-throughput screening with computational tools such as molecular simulation ${ }^{5,15}$ or density functional theory (DFT) ${ }^{16,17}$ has been widely used to evaluate the properties of MOFs. Without the need to experimentally synthesize MOF structures, those computational tools accelerate the screening process and allow researchers to screen hundreds of thousands of hypothetical MOF structures for their performance in different applications.</p>
<p>Recently, machine learning (ML) models have become increasingly popular in the field of MOF property prediction. ${ }^{18-24}$ The advantage of the ML models over the simulation methods is their instantaneous inference of the properties of MOFs. In contrast, the simulation methods require a computationally expensive rerun for every new MOF. In the last decade, multiple large scale MOF dataset are released, including the CoRE MOF 2019, ${ }^{25}$ hypothetical MOFs, ${ }^{5}$ and QMOF. ${ }^{26}$ These datasets contain the atomic structures of MOFs and their properties like $\mathrm{CO}_{2}$ absorption and band gap. These datasets are large enough to train accurate data-driven ML models for the prediction of MOF properties. Handcrafted geometrical features such as large cavity diameter and pore limiting diameter have been used as input to a multilayer perceptron (MLP) to predict MOFs properties. ${ }^{19,23}$ Although the training of MLP with a few layers can be fast, this method suffers from underwhelming accuracy due to the simplicity of network architecture. Moreover, selecting features requires extensive domain knowledge from the researchers and optimized 3D structures of MOFs, thus making this method less generic. Given the aforementioned drawbacks, a novel method that can achieve high accuracy with a more generic input of MOFs representations should be pursued. Wang et al. ${ }^{27}$ utilize the crystal graph convolutional neural network (CGCNN) ${ }^{28}$ to predict methane absorption of MOFs. CGCNN is a prevalent model with has an architecture designed specifically for crystalline materials. It takes the type and the 3D coordinates of atoms in the crystalline materials as input and constructs a crystal graph. CGCNN can extract features that encode rich chemical information through convolution operations on the crystal graph. However, one drawback of using CGCNN for MOFs property prediction</p>
<p>is that it requires the optimized 3D atomic structures of MOFs which are computationally expensive to obtain. In addition, some large MOF structures consist of hundred or even thousand of atoms, thus rendering crystal graph for them can be memory-inefficient.</p>
<p>Enlightened by the fact that all MOFs are combinations of metal nodes, organic linkers, and topologies, Bucior et al. ${ }^{29}$ proposed a text string representation of MOFs called MOFid. The two core sections of a typical MOFid include the chemical information of building blocks in the format of SMILES ${ }^{30}$ and the topology and the catenation of the MOF structure. The building blocks are represented by an extensively used string representation of molecules called SMILES. ${ }^{30}$ The topology and catenation are each represented by a code adopted from the Reticular Chemistry Structure Resource (RCSR) database. ${ }^{31}$ Therefore, MOFid is a concise text string representation of MOFs that preserves the chemical and the majority of the structural information through topology encoding. The MOFid text based representation enables the application of language ML models that take text string as input for MOF property prediction.</p>
<p>In this work, we proposed and developed a Transformer-based language model for MOF property prediction. Transformer and its variants have become the top choice for the natural language processing tasks since publication in 2017 by Vaswani et al. ${ }^{32}$ The multi-head attention mechanism allows the Transformer model to learn contextual information in a sequence without suffering from long-range dependency. ${ }^{33,34}$ With its success in processing long sequential data, Transformer and its other variants are also adopted for chemistry or bioinformatics application such as molecular ${ }^{35-37}$ and protein ${ }^{38}$ property prediction. The Transformer model in our work, named as MOFormer, takes a modified MOFid as input to make predictions of various MOF properties. The advantage of this method is that it does not require the 3D atomic structure of the MOF (structure-agnostic), thus enabling a much faster and more flexible exploration of the hypothetical MOF space. In practice, pretraining the Transformer model in a self-supervised manner ${ }^{38-42}$ leverages large quantity of unlabeled data to help the MOFormer learn a more robust representation of the sequence,</p>
<p>and further improve its performance in downstream tasks. To take advantage of pre-training benefits, we also added a self-supervised learning framework in which the MOFormer and the CGCNN model are jointly pretrained with $&gt;400 \mathrm{k}$ MOF structures. Dimensionality reduction tools are used to visualize the latent representation learned by both models to provide insight into their performance characteristics. Visualization of attention weights in MOFormer demonstrates that MOFormer learns MOF representations based on some key atoms and topology. Lastly, we compared the data efficiency of models to show which one is a better choice when training data is limited.</p>
<h1>Methods</h1>
<h2>MOFid tokenization and Transformer</h2>
<p>The MOFormer is built upon the encoder part of the Transformer model that takes tokenized MOFid as input (Figure. 1a). The MOFid tokenizer is a customized version of the SMILES tokenizer. ${ }^{43}$ The SMILES strings of all secondary building units (SBUs) of the MOFid is tokenized by the SMILES tokenizer, while the topology and catenation section of the MOFid is separately tokenized based on the topology encoding adopted from RCSR. ${ }^{31}$ The tokens from both sections are then connected by a separator token "\&amp;" The tokenization process followed the BERT ${ }^{39}$ to add a [CLS] token and a [SEP] token at the beginning and the end of the sequence to symbolize the start and the end, respectively. Since the tokenized sequences conform to a fixed length of 512 , sequences longer than the fixed length are truncated and the sequences shorter than that are padded with special tokens [PAD].</p>
<p>Tokenized sequence is embedded and combined with a positional encoding (Figure. 1a) to include information about the relative and absolute position of each token. The position</p>
<p>encoding is calculated by:</p>
<p>$$
\begin{aligned}
\mathrm{PE}<em _emb="{emb" _text="\text">{(p o s, 2 i)} &amp; =\sin \left(\frac{\text { pos }}{10000^{2 i / d</em>\right) \
\mathrm{PE}}}}<em _emb="{emb" _text="\text">{(p o s, 2 i+1)} &amp; =\cos \left(\frac{\text { pos }}{10000^{2 i / d</em>\right)
\end{aligned}
$$}}}</p>
<p>where pos is the position of the token in the sequence, $i$ is the index of dimension, and $d_{\text {emb }}=$ 512 is the embedding dimension. The Transformer model is a deep neural network model built upon the self-attention mechanism (detailed in the supporting information). Each of the Transformer encoder layer consists of a multi-head attention layer followed by a simple feed-forward multilayer perceptron (MLP). Residue connection ${ }^{44}$ and layer normalization ${ }^{45}$ is adopted for both the attention and the MLP. In each head of the attention layer (Figure. 1b), the input sequence embedding $X$ is multiplied with three learnable weight vectors $W_{q}, W_{k}$, and $W_{v}$ to be converted to the query, key, and value vector $(Q, K, V)$. The scaled dotproduct attention $A$ is then calculated by the equation:</p>
<p>$$
A=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>where $d_{k}$ is the dimension of $Q$ and $K$ (detailed in the supporting information). The randomly initialized $W_{q}, W_{k}$, and $W_{v}$ vectors in each head allow the model to learn the contextual information between tokens in different representation subspaces. ${ }^{32}$ Attention from all heads are concatenated together and then fed into the MLP for the projected output embedding, which is of the same size of the input embedding. Given that the self-mechanism can incorporate the information of the whole sequence into each one of the token embeddings, theoretically any one of the embeddings can be used as a representation of the whole sequence. Therefore, we followed the common practice of related works ${ }^{35,36,46,47}$ to use the embedding of the first token, [CLS], for further supervised learning tasks.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) The pipeline of the MOFormer model. A MOFid of a MOF (qmof-2521623 is used as an example) is the input to the model. The MOFid is converted into tokenized sequence before embedded and applied with positional encoding. The sequence is then fed into multiple Transformer encoder layers. The learned embedding of the first token will be used as input to a MLP regression head for downstream prediction task. (b) A schematic showing the details of each Transformer encoder layer. Embeddings of the sequence pass through the multi-head scaled dot-product attention layer and then a MLP. Residue connection and layer normalization is adopted for both the attention and the MLP. (c) The self-supervised learning framework with CGCNN and MOFormer. The 3D structure and the MOFid of the same MOF are fed into the CGCNN and MOFormer, respectively for representation learning. The MLP head following each models project the representations into embeddings (Z<sub>A</sub> and Z<sub>B</sub>). A cross correlation matrix is then constructed using the embeddings. Barlow Twins loss is applied to optimize the cross correlation matrix to be as close as possible to an identity matrix.</p>
<h3>Self-supervised pretraining with CGCNN</h3>
<p>We introduce a self-supervised learning (SSL) paradigm for MOF representation learning. We designed the framework by taking into consideration two modalities of data including the text and graph information. One of the modalities is the text string representation (MOFid)</p>
<p>that encapsulates building blocks' stoichiometry and bonds (SMILES) and the topology of the MOF. The text string information is processed by the MOFormer. One of the limitations of text string data is the lack of availability of information about the geometry and the neighborhood of atoms creating an information bottleneck for the tex string input based models. The structure agnostic nature of the text string input can prevent the transformer to achieve higher performance than the graph based models. To mitigate these issues of the MOFormer framework, we introduce SSL pretraining with CGCNN. We leverage structure as a modality with CGCNN. ${ }^{28}$ The CGCNN model takes as input the structure of the MOF. The structure data consists of the atom information along with the neighborhood information which is critical in property prediction tasks. To implement the SSL pipeline, we take inspiration from Crystal Twins (CT) framework. ${ }^{48}$ The CT model makes use of the Barlow Twins loss function introduced by Zbontar et al ${ }^{49}$ and SimSiam loss ${ }^{50}$ functions. In this work, we use the Barlow Twins loss function on the emebddings generated from the Transformer and CGCNN encoder. As shown in Figure 1C, we initially encode both the text string representation and graph representation with their respective encoders. The MOFormer will encode the text string representation and the CGCNN will encode the graph representation. We generate an embedding of size 512 from both the encoders and use it to generate the cross-correlation matrix (Equation 3). Ideally, we want cross-correlation as close to the identity matrix as both the representations generated from MOForemer and CGCNN are essentially capturing the same MOF. The Barlow Twins loss function, which we used for SSL pre-training (Equation 4), tries to force the cross-correlation matrix to the identity matrix.</p>
<p>$$
C_{i j} \triangleq \frac{\sum_{b} \boldsymbol{Z}<em b_="b," j="j">{b, i}^{A} \boldsymbol{Z}</em>}^{B}}{\sqrt{\left(\boldsymbol{Z<em b_="b," j="j">{b, i}^{A}\right)^{2}} \sqrt{\left(\boldsymbol{Z}</em>
$$}^{B}\right)^{2}}</p>
<p>where $b$ is the batch index and $i, j$ index the 512 dimensional output from the projector $\left(Z^{A}\right.$ and $Z^{B}$ ), $A$ is the graph representation and $B$ is text representation for the same MOF.</p>
<p>$$
L_{\mathrm{BT}} \triangleq \sum_{i}\left(1-C_{i i}\right)^{2}+\lambda \sum_{i} \sum_{j \neq i} C_{i j}^{2}
$$</p>
<p>where $\boldsymbol{C}$ is the cross-correlation matrix of embeddings from the MOFormer and CGCNN, the cross correlation matrix is given by Equation 3. The $\lambda$ used in this work is set to 0.0051 .</p>
<p>Finally after pretraining the models using SSL, the encoder weights are shared during the finetuning stage (Figure S1). The pretraining hyperparamter details are shown in Table S3 For finetuning, we initialize the model with pretrained weights and train the model for 200 epochs to generate the final prediction(Hyperparameters: Table S1 and Table S2). The MOFormer and CGCNN models are finetuned separately. We observed that using SSL pretraining framework improved the results of both CGCNN and MOFormer consistently for all the datasets.</p>
<h1>Datasets and other featurizations</h1>
<p>Three public MOF datasets including the CORE MOF 2019, ${ }^{25}$ the hypothetical MOFs ${ }^{5}$ (hMOF), and the Boyd\&amp;Woo ${ }^{4}$ are combined to create a large dataset for the self-supervised pretraining. The pretraining dataset only includes MOFs with both 3D structure and MOFid available. After the removal of duplicated MOFs, the final pretraining dataset has 413535 unique MOFs. In the downstream prediction task, the MOFormer and the CGCNN are trained on the quantum $\mathrm{MOF}^{26}$ (QMOF) and hMOF in a supervised manner. The QMOF dataset contains 20375 MOFs each with a label of DFT calculated band gap in eV. Only 7466 MOFs in the QMOF dataset have MOFid available. On the other hand, the hMOF has 137652 MOFs, in which 102858 have available MOFid. The models are trained on hMOF with the label of $\mathrm{CO}<em 4="4">{2}$ and $\mathrm{CH}</em>$ at $0.05,0.5$, and 2.5 bar of pressure. The benchmark datasets are split into training, test, and validation sets with a ratio of $0.7-0.15-0.15$. During the training, the model with the best validation performance is recorded and then tested with the test set. According to the splitting rule, MOFormer has}$ adsorption in mol $\cdot \mathrm{kg}^{-1</p>
<p>5226-1119-1119 QMOF data and 72000-15428-15428 hMOF data, while other models have 14262-3056-3056 QMOF data and 96356-20647-20647 hMOF data in the training, validation and test set, respectively. Although the MOFs with available MOFid form a subset of both benchmark datasets, the subset with MOFid shares the same distribution and has approximately the same mean and standard deviation compared with the original whole dataset (Figure S2 and S3 in the supporting information). Therefore, it is fair to compare the performance of MOFormer and other models.</p>
<p>We also benchmarked the MOFormer and CGCNN against other non-DL based featurization methods such as the Smooth Overlap of Atomic Positions ${ }^{51-53}$ (SOAP) and the Stoichiometric-120 ${ }^{54}$ features. SOAP is a structure-based featurization method and the Stoichiometric-120 is a structure-agnostic featurization method. The parameters used for creating SOAP features are included in the supporting information Table S4. XGBoost ${ }^{55}$ model is used to make predictions using those handcrafted features.</p>
<h1>Results and discussion</h1>
<h2>QMOF</h2>
<p>The first dataset we benchmark models on is the QMOF dataset, where the label for each MOF is DFT calculated band gap. Lower band gap value results in better conductivity of the MOF. Accurate prediction of band gap can help to identify conductive MOFs which is useful in energy storage applications. ${ }^{56,57}$ The accuracy of models follows the rank of CGCNN $&gt;$ MOFormer $&gt;$ SOAP $&gt;$ Stoichiometric-120 (Table. 1). MOFormer has $21.2 \%$ lower MAE compared with the other structure-agnostic method Stoichiometric-120. It is worth noting that structure-agnostic MOFormer outperforms structure-based SOAP with a smaller size of training set, indicating that MOFormer is capable of extracting critical features from the MOFid for energy-related property prediction. The self-supervised pretraining helps reduce the mean absolute error (MAE) of CGCNN by $6.79 \%$ and MOFormer by $5.34 \%$. The reduced</p>
<p>error proves the improvement brought by the pretraining.
Table 1: Benchmark performance of different models on the band gap prediction of the QMOF dataset. Mean absolute error (MAE, in the unit of eV ) and standard deviation of 3 runs of different initial seeds of each model is reported. The left three models are structure-based and the right three models are structure-agnostic. The best performance of each category is marked as bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{CGCNN}_{\text {scratch }}$</th>
<th style="text-align: center;">$\mathrm{CGCNN}_{\text {pretrain }}$</th>
<th style="text-align: center;">SOAP</th>
<th style="text-align: center;">MOFormer $_{\text {scratch }}$</th>
<th style="text-align: center;">MOFormer $_{\text {pretrain }}$</th>
<th style="text-align: center;">Stoichiometric-120</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MAE (eV)</td>
<td style="text-align: center;">$0.275 \pm 0.015$</td>
<td style="text-align: center;">$\mathbf{0 . 2 5 6} \pm \mathbf{0 . 0 0 6}$</td>
<td style="text-align: center;">$0.424 \pm 0.007$</td>
<td style="text-align: center;">$0.387 \pm 0.001$</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 7} \pm \mathbf{0 . 0 0 5}$</td>
<td style="text-align: center;">$0.466 \pm 0.011$</td>
</tr>
</tbody>
</table>
<p>To better understand the superior performance of MOFormer and CGCNN in QMOF, we trained the 4 models with the same training set and then examined their performance on the same test set consisting 1119 randomly selected MOFs. The binned scatter plot (Figure 2a) shows the comparison between the predicted and the DFT calculated band gap. Darker color means more data fall in the bin. More predictions made by CGCNN and MOFormer are closer to the ground truth, especially for MOFs with band gap lower than $\leq 2 \mathrm{eV}$. The SOAP and Stoichiometric-120 are more likely to overpredict the lower band gap. This weakness of SOAP and Stoichiometric-120 can also be confirmed by the kernel density estimation of predicted values (Figure S4). The MOFs with the top-2 lowest band gaps in this test set are the qmof-1c923ff $(0.03 \mathrm{eV})$ and qmof-6bda2bd $(0.039 \mathrm{eV})$. Band gap prediction by MOFormer and CGCNN is much closer to the DFT calculated value than predictions by SOAP and Stoichiometric-120 (Figure 2b-c), especially for qmof-6bda2bd. Accurately predicting low band gap of MOFs can lead to the discovery of conductive MOF, rendering pretrained MOFormer and CGCNN more valuable for prescreening MOFs.</p>
<h1>hMOF</h1>
<p>The models are also benchmarked on the hMOF dataset with the label of $\mathrm{CO}<em 4="4">{2}$ and $\mathrm{CH}</em>$ adsorption}$ adsorption under $0.05,0.5$, and 2.5 bar of pressure. Table 2 shows that pretrained MOFormer is constantly outperforming the other structure-agnostic Stoichiometric-120 by achieving $35 \%$ $48 \%$ lower MAE. Pretrained CGCNN outperforms other models for the $\mathrm{CO}_{2</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) The binned scatter plot shows the comparison between predicted and DFT calculated band gap for MOFs in the QMOF dataset. MOFs included in this figure are from the randomly splitted test set. Darker color of each hexagonal bin represents more data points are in the bin. Dashed line represents perfect prediction. (b) - (c) Visualization<sup>58</sup> of the MOF structure with lowest (qmof-1c923ff) and the second lowest (qmof-6bda2bd) band gap in the test set. The bar plot shows the comparison between predictions made by different models.</p>
<p>prediction. The pretraining in average improves the accuracy of MOFormer by 4.3% and the CGCNN by 16.5% over all gas adsorption predictions. SOAP has surprisingly low MAE for the gas adsorption prediction, outperforming pretrained CGCNN for 2 of 3 CH<sub>4</sub> adsorption predictions, and the CGCNN trained from scratch for all gas adsorption predictions. The outstanding performance of SOAP on hMOF can be attributed to the low variation of</p>
<p>elements included in the hMOF dataset. Only 11 different elements are present among all 137652 hMOFs, which is very limited compared to 79 in the QMOF dataset. Less number of elements results in much smaller and less sparse SOAP feature vector (SOAP feature has an size of 2772 for hMOFs and 19908 for QMOFs with same parameters), thus leading to the high prediction accuracy of the following XGBoost regressor. However, the high accuracy of SOAP can hardly be sustainable when it is used in exploring more diverse hypothetical MOFs. When more elements are included in the dataset, the SOAP feature vector size and sparsity increase drastically, causing the data and model to be too large to be accommodated by the memory of local machines and drop of prediction accuracy (Table S5). MOFormer and CGCNN will not suffer from such an issue since their inputs remain invariant with increasing types of elements in the dataset, making them better choices when exploring more diverse chemical space for MOFs.</p>
<p>Table 2: Benchmark performance of different models on gas adsorption prediction of the hMOF dataset. Mean absolute error (mol kg⁻¹) and standard deviation of 3 runs of different initial seeds of each model is reported. The top three models are structure-based and the bottom three models are structure-agnostic. The best performance of each category is marked as bold.</p>
<table>
<thead>
<tr>
<th></th>
<th>CO₂ 0.05bar</th>
<th>CO₂ 0.5 bar</th>
<th>CO₃ 2.5 bar</th>
<th>CH₄ 0.05bar</th>
<th>CH₄ 0.5bar</th>
<th>CH₄ 2.5bar</th>
</tr>
</thead>
<tbody>
<tr>
<td>CGCNN_{scratch}</td>
<td>0.126±0.005</td>
<td>0.391±0.017</td>
<td>0.818±0.050</td>
<td>0.028±0.001</td>
<td>0.121±0.006</td>
<td>0.333±0.017</td>
</tr>
<tr>
<td>CGCNN_{pretrain}</td>
<td>0.110±0.001</td>
<td>0.330±0.002</td>
<td>0.645±0.003</td>
<td>0.025±0.001</td>
<td>0.099±0.001</td>
<td>0.258±0.008</td>
</tr>
<tr>
<td>SOAP</td>
<td>0.115±0.002</td>
<td>0.339±0.004</td>
<td>0.666±0.003</td>
<td>0.022±0.001</td>
<td>0.106±0.001</td>
<td>0.239±0.002</td>
</tr>
<tr>
<td>MOFormer_{scratch}</td>
<td>0.178±0.002</td>
<td>0.558±0.001</td>
<td>1.000±0.013</td>
<td>0.034±0.000</td>
<td>0.174±0.002</td>
<td>0.385±0.003</td>
</tr>
<tr>
<td>MOFormer_{pretrain}</td>
<td>0.158±0.001</td>
<td>0.545±0.008</td>
<td>0.982±0.011</td>
<td>0.033±0.000</td>
<td>0.161±0.011</td>
<td>0.384±0.003</td>
</tr>
<tr>
<td>Stoichiometric-120</td>
<td>0.282±0.002</td>
<td>0.983±0.005</td>
<td>1.895±0.003</td>
<td>0.050±0.001</td>
<td>0.269±0.001</td>
<td>0.631±0.002</td>
</tr>
</tbody>
</table>
<p>The representations of MOFs learned by the MOFormer and CGCNN after finetuning are visualized to provide interpretablity to the models (Figure 3). Each representation is projected to the 2D space using the dimension reduction tool t-SNE.⁵⁹ t-SNE clusters more similar data points together while placing less similar data points further away. Only MOFs which has the top-10 most common topologies in hMOF is included in Figure 3 because they take &gt; 99.7% of the whole dataset. We can observe that CGCNN representations cluster MOF with high CO₂ adsorption more closely than MOFormer representations by compar-</p>
<p>ing Figure 3a and 3c. This contributes to higher prediction accuracy of CGCNN. On the other hand, MOFormer representation clusters MOFs with the same topology closer than CGCNN representation does. For example, the MOFs with dia (green) and tbo (brown) topologies form two clusters in the lower left corner of MOFormer representation visualization (Figure 3b). Those MOFs are much loosely clustered in the CGCNN representation visualization (Figure 3d). The MOFormer representation focusing on topology can be caused by the fact that gas adsorption is more dependent on the 3D structure of the MOF compared with its atom composition. The only structure-related information contained in the MOFid is the topology encoding. Therefore, more weights are on the topology after MOFormer is finetuned to predict the gas adsorption of MOFs. The input of CGCNN is the 3D structure of MOFs with atomic resolution, thus causing CGCNN to rely less on the topology for gas adsorption prediction.</p>
<h3>Visualization of attention weights</h3>
<p>Figure 4 demonstrates the attention maps between tokens of a MOFid (qmof-ba40858) in the last MOFormer layer after finetuning on band gap prediction. The attention map can serve as a visual interpretation of how the MOFormer learns MOF representations.60 We observe a strong attention in head 5 from all tokens to the metal node Ytterium, and to the topology encoding pcu in head 1 and 3. The attention from metal node to the topology encoding is especially high in head 1. Moreover, large attention weight can be observed between tokens in the SMILES of the SBUs in head 6. Head 1, 3, 5, and 8 also show large attention weights on the carbon and the oxygen atom, and the double bond in the organic building block. The attention weight visualization shows that MOFormer learns a representation that emphasizes on the contextual information between key components in the MOFid including important atoms (e.g. Y, O, and C) and the topology, thus leading to more accurate prediction.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The t-SNE<sup>59</sup> dimension reduced visualization of MOF representations learned by (a)-(b) the MOFormer and (c)-(d) the CGCNN. Each data point in (a) and (c) is colored by its CO<sub>2</sub> adsorption at 0.5 bar pressure, and in (b) and (d) is colored by its topology. Only MOF which has the top-10 most common topologies in hMOF dataset is shown.</p>
<h3>Data efficiency comparison</h3>
<p>Obtaining high-quality MOF data using experimental or DFT method can be time-consuming and expensive. A model with high data efficiency is ideal when the training data size is limited. We compared the data efficiency of different models on the QMOF and hMOF dataset (CO<sub>2</sub> adsorption at 0.5 bar pressure). For band gap prediction (Figure 5a), the pretrained MOFormer outperforms CGCNN when the training set size ≤ 1000. This makes MOFormer more valuable in predicting quantum-chemical properties when the training dataset is diffi-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Heatmap of the attention between tokens (MOFid of qmof-ba40858) in different heads of the last MOFormer layer. If we index each block in the heatmap as block<sub>n,i,j</sub>, where <em>i</em>, <em>j</em>, and <em>n</em> is the row, column, and head index, respectively. The value of block<sub>n,i,j</sub> represents the attention on the <em>j</em>-th token from the <em>i</em>-th token in <em>n</em>-th heatmap.</p>
<p>cult to build (i.e. experimentally synthesized MOFs). Both MOFormer and CGCNN achieve higher accuracy than SOAP regardless of the training set size on the QMOF dataset. For CO<sub>2</sub> adsorption prediction (Figure 5b), CGCNN constantly achieves higher accuracy than MOFormer regardless of the training set size, indicating its higher data efficiency. CGCNN outperforms MOFormer on hMOF because the CO<sub>2</sub> adsorption correlates more with the MOF structure and the input to CGCNN provides more structural information than MOFid. SOAP achieves higher data efficiency than CGCNN and MOFormer on hMOF, but is eventually caught up by CGCNN after the training set size exceeds 50000. Both Figure 5a and 5b show that pretraining consistently improves the data efficiency of MOFormer and CGCNN. Moreover, SOAP is shown to have diminishing improvement with increasing training set size, but CGCNN and MOFormer do not suffer from such an issue.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Data efficiency comparison between different models on the (a) QMOF and the (b) hMOF dataset. The models are trained on a subset of the training set, while the validation and test set are kept the same. The training set sizes are 100, 500, 1000, 5000, 10000, and 50000 (hMOF only). Since only less than 7500 MOFs in QMOF have available MOFid, the maximum training subset size for MOFormer on QMOF is 5000.</p>
<h3>Conclusion</h3>
<p>In summary, we propose a Transformer-based model, named as MOFormer, for structure-agnostic MOF property prediction. Taking only MOFid as input, the MOFormer model is expected to expedite the exploration of hypothetical MOFs. We also introduce a self-supervised learning framework to jointly pretrain the MOFormer and CGCNN model on a large unlabeled MOF dataset to enhance their prediction accuracy in downstream tasks. Compared with another structure-agnostic method Stoichiometric-120, MOFormer achieves 21.4% higher accuracy on band gap prediction, and 35%-48% higher accuracy on various gas adsorption prediction tasks. MOFormer even outperforms structure-based SOAP method in band gap prediction with less training data. The pretraining is further shown to improve the accuracy of MOFormer by 5.34% and 4.3% in average and CGCNN by 6.79% and 16.5% in average, for band gap and gas adsorption prediction, respectively. MOFormer and CGCNN are shown to be less likely to overpredict the band gap of MOFs compared with SOAP and Stoichiometric-120, making them better choices for prescreening conductive MOFs for energy applications. When used for gas adsorption prediction of MOFs, MOFormer relies</p>
<p>more on the topology information compared with CGCNN because of the strong correlation between the label and the structure of MOF. Visualization of the attention weights in the last MOFormer layer reveals that the attention layers in MOFormer focus more on several important atoms and the topology to learn the representation of a MOF. Lastly, MOFormer is shown to be more data efficient than CGCNN for band gap prediction when the training set size $\leq 1000$. As a structure-agnostic model, MOFormer can make rapid and accurate inference on the property of MOFs (especially for quantum-chemical properties) using arbitrarily constructed MOFid as input. Therefore, MOFormer can serve as a tool for exploring the vast chemical space of hypothetical MOFs.</p>
<h1>Acknowledgement</h1>
<p>This work is supported by the start-up fund from Mechanical Engineering Department at CMU.</p>
<h2>Supporting Information Available</h2>
<p>Transformer and self-attention mechanism. Details of CGCNN and MOFormer model. Details of the self-supervised pretraining. Distribution of QMOF and hMOF dataset. Parameters for SOAP feature vector creation and the effect of SOAP vector length to model accuracy. Kernel density estimation of band gap prediction from different models.</p>
<h2>Code Availability</h2>
<p>The python code of this work can be found on Github https://github.com/zcao0420/MOFormer.</p>
<h1>References</h1>
<p>(1) James, S. L. Metal-organic frameworks. Chemical Society Reviews 2003, 32, 276-288.
(2) Zhou, H.-C.; Long, J. R.; Yaghi, O. M. Introduction to metal-organic frameworks. Chemical reviews 2012, 112, 673-674.
(3) Ahmed, A.; Seth, S.; Purewal, J.; Wong-Foy, A. G.; Veenstra, M.; Matzger, A. J.; Siegel, D. J. Exceptional hydrogen storage achieved by screening nearly half a million metal-organic frameworks. Nature communications 2019, 10, 1-9.
(4) Boyd, P. G.; Chidambaram, A.; García-Díez, E.; Ireland, C. P.; Daff, T. D.; Bounds, R.; Gładysiak, A.; Schouwink, P.; Moosavi, S. M.; Maroto-Valer, M. M., et al. Data-driven design of metal-organic frameworks for wet flue gas CO 2 capture. Nature 2019, 576, $253-256$.
(5) Wilmer, C. E.; Leaf, M.; Lee, C. Y.; Farha, O. K.; Hauser, B. G.; Hupp, J. T.; Snurr, R. Q. Large-scale screening of hypothetical metal-organic frameworks. Nature chemistry 2012, 4, 83-89.
(6) Almassad, H. A.; Abaza, R. I.; Siwwan, L.; Al-Maythalony, B.; Cordova, K. E. Environmentally adaptive MOF-based device enables continuous self-optimizing atmospheric water harvesting. Nature communications 2022, 13, 1-10.
(7) Hanikel, N.; Prévot, M. S.; Yaghi, O. M. MOF water harvesters. Nature nanotechnology 2020, 15, $348-355$.
(8) Cao, Z.; Liu, V.; Barati Farimani, A. Water desalination with two-dimensional metalorganic framework membranes. Nano letters 2019, 19, 8638-8643.
(9) Baumann, A. E.; Burns, D. A.; Liu, B.; Thoi, V. S. Metal-organic framework functionalization and design strategies for advanced electrochemical energy storage devices. Communications Chemistry 2019, 2, 1-14.</p>
<p>(10) Zhao, Y.; Song, Z.; Li, X.; Sun, Q.; Cheng, N.; Lawes, S.; Sun, X. Metal organic frameworks for energy storage and conversion. Energy storage materials 2016, 2, 3562 .
(11) Xu, G.; Nie, P.; Dou, H.; Ding, B.; Li, L.; Zhang, X. Exploring metal organic frameworks for energy storage in batteries and supercapacitors. Materials today 2017, 20, 191-209.
(12) Sharp, C. H.; Bukowski, B. C.; Li, H.; Johnson, E. M.; Ilic, S.; Morris, A. J.; Gersappe, D.; Snurr, R. Q.; Morris, J. R. Nanoconfinement and mass transport in metalorganic frameworks. Chemical Society Reviews 2021,
(13) Moosavi, S. M.; Nandy, A.; Jablonka, K. M.; Ongari, D.; Janet, J. P.; Boyd, P. G.; Lee, Y.; Smit, B.; Kulik, H. J. Understanding the diversity of the metal-organic framework ecosystem. Nature communications 2020, 11, 1-10.
(14) Falcaro, P.; Hill, A. J.; Nairn, K. M.; Jasieniak, J.; Mardel, J. I.; Bastow, T. J.; Mayo, S. C.; Gimona, M.; Gomez, D.; Whitfield, H. J., et al. A new method to position and functionalize metal-organic framework crystals. Nature communications 2011, 2, $1-8$.
(15) Ozcan, A.; Semino, R.; Maurin, G.; Yazaydin, A. O. Modeling of gas transport through polymer/MOF interfaces: a microsecond-scale concentration gradient-driven molecular dynamics study. Chemistry of Materials 2020, 32, 1288-1296.
(16) Canepa, P.; Arter, C. A.; Conwill, E. M.; Johnson, D. H.; Shoemaker, B. A.; Soliman, K. Z.; Thonhauser, T. High-throughput screening of small-molecule adsorption in MOF. Journal of Materials Chemistry A 2013, 1, 13597-13604.
(17) Nandy, A.; Duan, C.; Taylor, M. G.; Liu, F.; Steeves, A. H.; Kulik, H. J. Computational discovery of transition-metal complexes: from high-throughput screening to machine learning. Chemical Reviews 2021, 121, 9927-10000.</p>
<p>(18) Fung, V.; Zhang, J.; Juarez, E.; Sumpter, B. G. Benchmarking graph neural networks for materials chemistry. npj Computational Materials 2021, 7, 1-8.
(19) Burner, J.; Schwiedrzik, L.; Krykunov, M.; Luo, J.; Boyd, P. G.; Woo, T. K. HighPerforming Deep Learning Regression Models for Predicting Low-Pressure CO2 Adsorption Properties of Metal-Organic Frameworks. The Journal of Physical Chemistry C 2020, 124, 27996-28005.
(20) Altintas, C.; Altundal, O. F.; Keskin, S.; Yildirim, R. Machine learning meets with metal organic frameworks for gas storage and separation. Journal of Chemical Information and Modeling 2021, 61, 2131-2146.
(21) Lee, S.; Kim, B.; Cho, H.; Lee, H.; Lee, S. Y.; Cho, E. S.; Kim, J. Computational screening of trillions of metal-organic frameworks for high-performance methane storage. ACS Applied Materials \&amp;̇ Interfaces 2021, 13, 23647-23654.
(22) Choudhary, K.; Yildirim, T.; Siderius, D. W.; Kusne, A. G.; McDannald, A.; OrtizMontalvo, D. L. Graph neural network predictions of metal organic framework CO2 adsorption properties. Computational Materials Science 2022, 210, 111388.
(23) Moghadam, P. Z.; Rogge, S. M.; Li, A.; Chow, C.-M.; Wieme, J.; Moharrami, N.; Aragones-Anglada, M.; Conduit, G.; Gomez-Gualdron, D. A.; Van Speybroeck, V., et al. Structure-mechanical stability relations of metal-organic frameworks via machine learning. Matter 2019, 1, 219-234.
(24) Nandy, A.; Duan, C.; Kulik, H. J. Using Machine Learning and Data Mining to Leverage Community Knowledge for the Engineering of Stable Metal-Organic Frameworks. Journal of the American Chemical Society 2021, 143, 17535-17547.
(25) Chung, Y. G.; Haldoupis, E.; Bucior, B. J.; Haranczyk, M.; Lee, S.; Zhang, H.; Vogiatzis, K. D.; Milisavljevic, M.; Ling, S.; Camp, J. S., et al. Advances, updates, and</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1119 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1119</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1119</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-253080707</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.12122v1.pdf" target="_blank">Targeted active learning for probabilistic models</a></p>
                <p><strong>Paper Abstract:</strong> A fundamental task in science is to design experiments that yield valuable insights about the system under study. Mathematically, these insights can be represented as a utility or risk function that shapes the value of conducting each experiment. We present PDBAL, a targeted active learning method that adaptively designs experiments to maximize scientific utility. PDBAL takes a user-specified risk function and combines it with a probabilistic model of the experimental outcomes to choose designs that rapidly converge on a high-utility model. We prove theoretical bounds on the label complexity of PDBAL and provide fast closed-form solutions for designing experiments with common exponential family likelihoods. In simulation studies, PDBAL consistently outperforms standard untargeted approaches that focus on maximizing expected information gain over the design space. Finally, we demonstrate the scientific potential of PDBAL through a study on a large cancer drug screen dataset where PDBAL quickly recovers the most efficacious drugs with a small fraction of the total number of experiments.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1119.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1119.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDBAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Diameter-based Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted Bayesian active learning algorithm that selects experiments to directly minimize expected posterior risk by using a risk-aligned distance between models and a probabilistic model of outcomes. It scores candidate queries by an objective that combines expected posterior diameter and an entropy correction to avoid up-weighting unlikely high-impact outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PDBAL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An adaptive experimental-design algorithm built on a Bayesian posterior over model parameters π_n. Key components: (1) a user-specified risk-aligned distance d(·,·) that encodes the scientific objective, (2) the PDBAL score s_n(x) which computes an expectation over pairs of posterior samples and potential outcomes (or uses closed-form expressions for exponential-family likelihoods), (3) Monte Carlo approximation (sample θ_1..θ_m and triple indices) when closed form unavailable, and (4) selection of the query x that minimizes the estimated expected posterior diameter (ŝ_n(x)).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning (targeted Bayesian experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each round PDBAL computes an objective s_n(x) = E_{θ,θ' ~ π_n} E_{y ~ P_θ(x)} [ d(θ,θ') · P_{θ'}(y;x) ] / e^{2H(x)} (implemented with Monte Carlo or closed-form for common likelihoods). The algorithm samples from the current posterior π_n, scores a pool of candidate experiments by expected reduction in the risk-aligned posterior diameter (with an entropy term to penalize low signal-to-noise queries), then selects the x minimizing ŝ_n(x). Posterior updates after observing y are used to re-compute π_{n+1} and repeat.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic regression tasks and GDSC retrospective drug-screen (coarse and fine-grained setups)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown latent parameter θ (partially observable), stochastic outcome generation P_θ(y; x), continuous and/or discrete outcome spaces depending on likelihood (Gaussian, Bernoulli, Poisson, Beta), large pool of possible experiments (pool-based active learning), well-specified Bayesian modeling assumption (observations generated from some θ ~ prior).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Synthetic: parameter vector θ ∈ R^d with d=10; pool sampling per query of 2K fresh points, 250 random seeds used. GDSC coarse: M=100 drugs × 20 cell lines => ~2K coarse experiments (each coarse experiment returns full 7-dose curve). GDSC fine-grained: ~14K possible experiments (drug × cell line × dose). Posterior inference uses MCMC or Gibbs; computationally intensive (coarse PDBAL runs ~12 hours, fine ~48 hours on CPU cluster for their implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Consistently outperforms untargeted baselines (RANDOM, VAR, EIG) across synthetic regression tasks; strong gains for focused objectives (e.g., first-coordinate sign, max-magnitude coordinate) and for non-linear likelihoods (logistic, Beta). In GDSC drug screen, PDBAL recovers effective drugs after observing only ~10% of the total experiments and achieves higher AUC for selective-drug identification (AUC at 5% = 0.60, at 10% = 0.71). Quantitative d_v-mse curves (posterior mean vs full-data model) show PDBAL lower error than baselines in both coarse and fine-grained settings (numbers plotted in paper figures; exact d_v-mse numeric values not tabulated).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines: RANDOM (AUC 0.50 @5%, 0.60 @10%), VAR (AUC 0.52 @5%, 0.62 @10%), EIG (AUC 0.50 @5%, 0.67 @10%) on the coarse GDSC selective-drug identification task. On synthetic tasks, untargeted methods (VAR, EIG) sometimes perform substantially worse for targeted objectives (especially logistic and Beta regression).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirical: in GDSC PDBAL identifies selective drugs and reaches low target error using ~5–10% of the data (paper reports accurate detection after 10% and improved AUC already at 5%); in simulations PDBAL attains target objective with fewer queries than baselines across multiple objectives (plots averaged over 250 seeds). Theoretical: Theorem 5 gives an upper bound on queries T to reach average-diameter ≤ ϵ: T ≤ O(max((c1 c2 /ρ^2 v) log(...), (d + c)/(ρ) log(...), (1/ρ) log avg-diam(π) )).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>PDBAL balances exploration vs exploitation by directly optimizing expected reduction in a risk-aligned posterior diameter rather than pure information gain: the objective averages over potential outcomes and includes an entropy normalization term that penalizes high-entropy (noisy/low signal) queries. This hedges against unlikely extreme outcomes and focuses queries that are expected to shrink the posterior diameter relevant to the target utility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RANDOM (uniform sampling), VAR (variance sampling/posterior predictive variance), EIG/BALD (expected information gain / mutual information), and in related work: Query-by-committee, GAUSSED (Bayes risk minimization).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>PDBAL (1) provides a general targeted active learning objective compatible with arbitrary probabilistic models and user-specified risk distances; (2) admits closed-form scoring for common exponential-family likelihoods and Monte Carlo approximations otherwise; (3) has provable label/query complexity upper bounds (Theorem 5) and near-optimal lower bounds in certain regimes; (4) empirically outperforms untargeted baselines across synthetic regression problems (notably for focused objectives and non-linear likelihoods) and on a large cancer drug-screen dataset where it identifies effective/selective drugs after observing only a small fraction (5–10%) of experiments; (5) is computationally heavier than some baselines (EIG infeasible in fine-grained GDSC due to numerical-integration cost).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Theoretical analysis relies on assumptions: (A1) constant conditional entropy H_θ(x) across θ for each x, (A2–A4) bounded log-likelihood dimension, entropy sub-Gamma and bounded densities; lower bounds limited to small-entropy settings. Empirically: smallest gains in bounded linear regression (limited extreme outcomes), VAR performs poorly in logistic tasks when covariates can be zero (high predictive variance but uninformative). Computational cost: PDBAL requires posterior sampling and Monte Carlo scoring; EIG numeric integration infeasible in large fine-grained experiment spaces. Applicability limited to well-specified Bayesian regime (assumes prior + likelihood adequately model the true data-generating process).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1119.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1119.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EIG / BALD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Information Gain / Bayesian Active Learning by Disagreement (BALD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An untargeted Bayesian active learning acquisition that selects queries maximizing mutual information between model parameters and potential outcomes (expected reduction in posterior entropy). Commonly implemented via BALD for outcome-space mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EIG (BALD)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Acquisition-based active learner that computes I(y; θ | x, π_n) = H_{π_n}(x) − E_{θ~π_n} H_θ(x), selecting x that maximizes the mutual information between the observation and latent parameters. Implemented via closed-form expressions where possible or numerical integration otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning (expected information gain / mutual information maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each round, compute posterior predictive entropy and expected conditional entropy under parameter samples; choose x maximizing their difference (mutual information), intended to reduce uncertainty about θ.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic regression tasks and GDSC coarse drug-screen (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same partially-observable Bayesian setting: unknown θ, stochastic outputs; continuous/discrete outcomes depending on likelihood. For GDSC coarse, each coarse experiment yields full dose-response curve.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as PDBAL entries above; in fine-grained GDSC EIG was not evaluated because numerical integration over many outcomes was computationally prohibitive.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>On GDSC coarse selective-drug identification: AUC at 5% = 0.50, at 10% = 0.67. On synthetic tasks: competitive in some settings (e.g., Gaussian linear models where EIG reduces to posterior predictive variance) but underperforms PDBAL on targeted objectives that ignore nuisance parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Generally sample-efficient for reducing global parameter uncertainty, but less sample-efficient for targeted objectives where only a subset of parameters matter; in fine-grained GDSC computational cost prevented evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implements exploration by selecting points expected to maximally reduce parameter entropy (pure information-theoretic exploration), does not directly encode task-specific utility so can waste queries on reducing uncertainty in nuisance directions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared as a baseline against PDBAL and VAR and RANDOM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>EIG is often a reasonable untargeted baseline and reduces posterior entropy, but can be suboptimal for minimizing task-specific risk; in experiments it was outperformed by PDBAL for targeted objectives and in the drug-screen selective-drug identification metric at small sample fractions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computationally expensive (numerical integration) for fine-grained GDSC large action spaces; may be suboptimal for targeted objectives that ignore nuisance parameters (can waste queries reducing irrelevant uncertainty).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1119.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1119.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variance sampling (posterior predictive variance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An uncertainty-based active sampling baseline that selects queries maximizing posterior predictive variance of outcomes, equivalent to maximizing variance of the mean predictor under certain Gaussian/homoscedastic assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VAR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Acquisition that selects x maximizing var_{y ~ π_n(x)}(y) = E_{θ~π_n} var_{P_θ(x)}(y) + var_{θ~π_n} E_{P_θ(x)}[y]; implemented in closed form for the likelihoods considered.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning (uncertainty sampling / variance maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses posterior predictive variance to prioritize querying data points where outcome variance is largest under current posterior, under the assumption that high outcome variance implies informative observations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic regression tasks and GDSC experiments (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown θ, stochastic outputs; in synthetic data generation there exist covariates with all-zero features which can maximize predictive variance but be uninformative about θ.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as above: d=10 synthetic experiments; GDSC coarse/fine setups as described for PDBAL.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>On GDSC selective-drug identification: AUC at 5% = 0.52, at 10% = 0.62. On synthetic logistic regression VAR performed poorly in some cases because data points with zero covariates maximized predictive variance despite providing no information on regression coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than PDBAL for targeted objectives in experiments reported; can be misled by high-variance but uninformative points.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure uncertainty-driven exploration; no explicit exploitation term tied to target utility, so tends to focus on high-variance regions even if not relevant to the target risk.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against PDBAL, EIG, RANDOM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Variance sampling can be a poor strategy for targeted objectives and certain data distributions (e.g., presence of uninformative high-variance covariates); PDBAL outperforms VAR in nearly all reported settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not robust when high posterior predictive variance occurs at uninformative points (demonstrated failure on logistic regression setting with zero-covariate points).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1119.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1119.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RANDOM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random sampling (uniform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-adaptive baseline that selects queries uniformly at random from available pool of experiments; used to represent passive sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RANDOM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline policy that selects experiments uniformly at random from the candidate pool; no posterior or model information used.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>none (random / passive sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No adaptation; selection independent of observations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic regression tasks and GDSC experiments (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same partially-observable stochastic experimental settings as above.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Observed AUC on GDSC selective-drug identification: 0.50 at 5%, 0.60 at 10%. Serves as passive baseline; PDBAL and some other adaptive methods outperform RANDOM.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low—performance scales with fraction of data observed, does not exploit information from past observations to concentrate queries.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>None (pure exploration by uniform sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as baseline against PDBAL, VAR, EIG.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Random sampling performs predictably poorly on targeted discovery tasks relative to targeted adaptive strategies; PDBAL consistently beats RANDOM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient for discovering targeted signals; baseline only.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On a measure of the information provided by an experiment <em>(Rating: 2)</em></li>
                <li>Bayesian active learning for classification and preference learning <em>(Rating: 2)</em></li>
                <li>Diameter-based active learning <em>(Rating: 2)</em></li>
                <li>Diameter-based interactive structure discovery <em>(Rating: 2)</em></li>
                <li>Gaussed: A probabilistic programming language for sequential experimental design <em>(Rating: 2)</em></li>
                <li>Query by committee <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1119",
    "paper_id": "paper-253080707",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "PDBAL",
            "name_full": "Probabilistic Diameter-based Active Learning",
            "brief_description": "A targeted Bayesian active learning algorithm that selects experiments to directly minimize expected posterior risk by using a risk-aligned distance between models and a probabilistic model of outcomes. It scores candidate queries by an objective that combines expected posterior diameter and an entropy correction to avoid up-weighting unlikely high-impact outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PDBAL",
            "agent_description": "An adaptive experimental-design algorithm built on a Bayesian posterior over model parameters π_n. Key components: (1) a user-specified risk-aligned distance d(·,·) that encodes the scientific objective, (2) the PDBAL score s_n(x) which computes an expectation over pairs of posterior samples and potential outcomes (or uses closed-form expressions for exponential-family likelihoods), (3) Monte Carlo approximation (sample θ_1..θ_m and triple indices) when closed form unavailable, and (4) selection of the query x that minimizes the estimated expected posterior diameter (ŝ_n(x)).",
            "adaptive_design_method": "active learning (targeted Bayesian experimental design)",
            "adaptation_strategy_description": "At each round PDBAL computes an objective s_n(x) = E_{θ,θ' ~ π_n} E_{y ~ P_θ(x)} [ d(θ,θ') · P_{θ'}(y;x) ] / e^{2H(x)} (implemented with Monte Carlo or closed-form for common likelihoods). The algorithm samples from the current posterior π_n, scores a pool of candidate experiments by expected reduction in the risk-aligned posterior diameter (with an entropy term to penalize low signal-to-noise queries), then selects the x minimizing ŝ_n(x). Posterior updates after observing y are used to re-compute π_{n+1} and repeat.",
            "environment_name": "Synthetic regression tasks and GDSC retrospective drug-screen (coarse and fine-grained setups)",
            "environment_characteristics": "Unknown latent parameter θ (partially observable), stochastic outcome generation P_θ(y; x), continuous and/or discrete outcome spaces depending on likelihood (Gaussian, Bernoulli, Poisson, Beta), large pool of possible experiments (pool-based active learning), well-specified Bayesian modeling assumption (observations generated from some θ ~ prior).",
            "environment_complexity": "Synthetic: parameter vector θ ∈ R^d with d=10; pool sampling per query of 2K fresh points, 250 random seeds used. GDSC coarse: M=100 drugs × 20 cell lines =&gt; ~2K coarse experiments (each coarse experiment returns full 7-dose curve). GDSC fine-grained: ~14K possible experiments (drug × cell line × dose). Posterior inference uses MCMC or Gibbs; computationally intensive (coarse PDBAL runs ~12 hours, fine ~48 hours on CPU cluster for their implementation).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Consistently outperforms untargeted baselines (RANDOM, VAR, EIG) across synthetic regression tasks; strong gains for focused objectives (e.g., first-coordinate sign, max-magnitude coordinate) and for non-linear likelihoods (logistic, Beta). In GDSC drug screen, PDBAL recovers effective drugs after observing only ~10% of the total experiments and achieves higher AUC for selective-drug identification (AUC at 5% = 0.60, at 10% = 0.71). Quantitative d_v-mse curves (posterior mean vs full-data model) show PDBAL lower error than baselines in both coarse and fine-grained settings (numbers plotted in paper figures; exact d_v-mse numeric values not tabulated).",
            "performance_without_adaptation": "Baselines: RANDOM (AUC 0.50 @5%, 0.60 @10%), VAR (AUC 0.52 @5%, 0.62 @10%), EIG (AUC 0.50 @5%, 0.67 @10%) on the coarse GDSC selective-drug identification task. On synthetic tasks, untargeted methods (VAR, EIG) sometimes perform substantially worse for targeted objectives (especially logistic and Beta regression).",
            "sample_efficiency": "Empirical: in GDSC PDBAL identifies selective drugs and reaches low target error using ~5–10% of the data (paper reports accurate detection after 10% and improved AUC already at 5%); in simulations PDBAL attains target objective with fewer queries than baselines across multiple objectives (plots averaged over 250 seeds). Theoretical: Theorem 5 gives an upper bound on queries T to reach average-diameter ≤ ϵ: T ≤ O(max((c1 c2 /ρ^2 v) log(...), (d + c)/(ρ) log(...), (1/ρ) log avg-diam(π) )).",
            "exploration_exploitation_tradeoff": "PDBAL balances exploration vs exploitation by directly optimizing expected reduction in a risk-aligned posterior diameter rather than pure information gain: the objective averages over potential outcomes and includes an entropy normalization term that penalizes high-entropy (noisy/low signal) queries. This hedges against unlikely extreme outcomes and focuses queries that are expected to shrink the posterior diameter relevant to the target utility.",
            "comparison_methods": "RANDOM (uniform sampling), VAR (variance sampling/posterior predictive variance), EIG/BALD (expected information gain / mutual information), and in related work: Query-by-committee, GAUSSED (Bayes risk minimization).",
            "key_results": "PDBAL (1) provides a general targeted active learning objective compatible with arbitrary probabilistic models and user-specified risk distances; (2) admits closed-form scoring for common exponential-family likelihoods and Monte Carlo approximations otherwise; (3) has provable label/query complexity upper bounds (Theorem 5) and near-optimal lower bounds in certain regimes; (4) empirically outperforms untargeted baselines across synthetic regression problems (notably for focused objectives and non-linear likelihoods) and on a large cancer drug-screen dataset where it identifies effective/selective drugs after observing only a small fraction (5–10%) of experiments; (5) is computationally heavier than some baselines (EIG infeasible in fine-grained GDSC due to numerical-integration cost).",
            "limitations_or_failures": "Theoretical analysis relies on assumptions: (A1) constant conditional entropy H_θ(x) across θ for each x, (A2–A4) bounded log-likelihood dimension, entropy sub-Gamma and bounded densities; lower bounds limited to small-entropy settings. Empirically: smallest gains in bounded linear regression (limited extreme outcomes), VAR performs poorly in logistic tasks when covariates can be zero (high predictive variance but uninformative). Computational cost: PDBAL requires posterior sampling and Monte Carlo scoring; EIG numeric integration infeasible in large fine-grained experiment spaces. Applicability limited to well-specified Bayesian regime (assumes prior + likelihood adequately model the true data-generating process).",
            "uuid": "e1119.0"
        },
        {
            "name_short": "EIG / BALD",
            "name_full": "Expected Information Gain / Bayesian Active Learning by Disagreement (BALD)",
            "brief_description": "An untargeted Bayesian active learning acquisition that selects queries maximizing mutual information between model parameters and potential outcomes (expected reduction in posterior entropy). Commonly implemented via BALD for outcome-space mutual information.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "EIG (BALD)",
            "agent_description": "Acquisition-based active learner that computes I(y; θ | x, π_n) = H_{π_n}(x) − E_{θ~π_n} H_θ(x), selecting x that maximizes the mutual information between the observation and latent parameters. Implemented via closed-form expressions where possible or numerical integration otherwise.",
            "adaptive_design_method": "active learning (expected information gain / mutual information maximization)",
            "adaptation_strategy_description": "At each round, compute posterior predictive entropy and expected conditional entropy under parameter samples; choose x maximizing their difference (mutual information), intended to reduce uncertainty about θ.",
            "environment_name": "Synthetic regression tasks and GDSC coarse drug-screen (evaluated as baseline)",
            "environment_characteristics": "Same partially-observable Bayesian setting: unknown θ, stochastic outputs; continuous/discrete outcomes depending on likelihood. For GDSC coarse, each coarse experiment yields full dose-response curve.",
            "environment_complexity": "Same as PDBAL entries above; in fine-grained GDSC EIG was not evaluated because numerical integration over many outcomes was computationally prohibitive.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "On GDSC coarse selective-drug identification: AUC at 5% = 0.50, at 10% = 0.67. On synthetic tasks: competitive in some settings (e.g., Gaussian linear models where EIG reduces to posterior predictive variance) but underperforms PDBAL on targeted objectives that ignore nuisance parameters.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Generally sample-efficient for reducing global parameter uncertainty, but less sample-efficient for targeted objectives where only a subset of parameters matter; in fine-grained GDSC computational cost prevented evaluation.",
            "exploration_exploitation_tradeoff": "Implements exploration by selecting points expected to maximally reduce parameter entropy (pure information-theoretic exploration), does not directly encode task-specific utility so can waste queries on reducing uncertainty in nuisance directions.",
            "comparison_methods": "Compared as a baseline against PDBAL and VAR and RANDOM.",
            "key_results": "EIG is often a reasonable untargeted baseline and reduces posterior entropy, but can be suboptimal for minimizing task-specific risk; in experiments it was outperformed by PDBAL for targeted objectives and in the drug-screen selective-drug identification metric at small sample fractions.",
            "limitations_or_failures": "Computationally expensive (numerical integration) for fine-grained GDSC large action spaces; may be suboptimal for targeted objectives that ignore nuisance parameters (can waste queries reducing irrelevant uncertainty).",
            "uuid": "e1119.1"
        },
        {
            "name_short": "VAR",
            "name_full": "Variance sampling (posterior predictive variance)",
            "brief_description": "An uncertainty-based active sampling baseline that selects queries maximizing posterior predictive variance of outcomes, equivalent to maximizing variance of the mean predictor under certain Gaussian/homoscedastic assumptions.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "VAR",
            "agent_description": "Acquisition that selects x maximizing var_{y ~ π_n(x)}(y) = E_{θ~π_n} var_{P_θ(x)}(y) + var_{θ~π_n} E_{P_θ(x)}[y]; implemented in closed form for the likelihoods considered.",
            "adaptive_design_method": "active learning (uncertainty sampling / variance maximization)",
            "adaptation_strategy_description": "Uses posterior predictive variance to prioritize querying data points where outcome variance is largest under current posterior, under the assumption that high outcome variance implies informative observations.",
            "environment_name": "Synthetic regression tasks and GDSC experiments (baseline)",
            "environment_characteristics": "Unknown θ, stochastic outputs; in synthetic data generation there exist covariates with all-zero features which can maximize predictive variance but be uninformative about θ.",
            "environment_complexity": "Same as above: d=10 synthetic experiments; GDSC coarse/fine setups as described for PDBAL.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "On GDSC selective-drug identification: AUC at 5% = 0.52, at 10% = 0.62. On synthetic logistic regression VAR performed poorly in some cases because data points with zero covariates maximized predictive variance despite providing no information on regression coefficients.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Less sample-efficient than PDBAL for targeted objectives in experiments reported; can be misled by high-variance but uninformative points.",
            "exploration_exploitation_tradeoff": "Pure uncertainty-driven exploration; no explicit exploitation term tied to target utility, so tends to focus on high-variance regions even if not relevant to the target risk.",
            "comparison_methods": "Compared against PDBAL, EIG, RANDOM.",
            "key_results": "Variance sampling can be a poor strategy for targeted objectives and certain data distributions (e.g., presence of uninformative high-variance covariates); PDBAL outperforms VAR in nearly all reported settings.",
            "limitations_or_failures": "Not robust when high posterior predictive variance occurs at uninformative points (demonstrated failure on logistic regression setting with zero-covariate points).",
            "uuid": "e1119.2"
        },
        {
            "name_short": "RANDOM",
            "name_full": "Random sampling (uniform)",
            "brief_description": "Non-adaptive baseline that selects queries uniformly at random from available pool of experiments; used to represent passive sampling.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "RANDOM",
            "agent_description": "Baseline policy that selects experiments uniformly at random from the candidate pool; no posterior or model information used.",
            "adaptive_design_method": "none (random / passive sampling)",
            "adaptation_strategy_description": "No adaptation; selection independent of observations.",
            "environment_name": "Synthetic regression tasks and GDSC experiments (baseline)",
            "environment_characteristics": "Same partially-observable stochastic experimental settings as above.",
            "environment_complexity": "Same as above.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Observed AUC on GDSC selective-drug identification: 0.50 at 5%, 0.60 at 10%. Serves as passive baseline; PDBAL and some other adaptive methods outperform RANDOM.",
            "sample_efficiency": "Low—performance scales with fraction of data observed, does not exploit information from past observations to concentrate queries.",
            "exploration_exploitation_tradeoff": "None (pure exploration by uniform sampling).",
            "comparison_methods": "Used as baseline against PDBAL, VAR, EIG.",
            "key_results": "Random sampling performs predictably poorly on targeted discovery tasks relative to targeted adaptive strategies; PDBAL consistently beats RANDOM.",
            "limitations_or_failures": "Inefficient for discovering targeted signals; baseline only.",
            "uuid": "e1119.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On a measure of the information provided by an experiment",
            "rating": 2,
            "sanitized_title": "on_a_measure_of_the_information_provided_by_an_experiment"
        },
        {
            "paper_title": "Bayesian active learning for classification and preference learning",
            "rating": 2,
            "sanitized_title": "bayesian_active_learning_for_classification_and_preference_learning"
        },
        {
            "paper_title": "Diameter-based active learning",
            "rating": 2,
            "sanitized_title": "diameterbased_active_learning"
        },
        {
            "paper_title": "Diameter-based interactive structure discovery",
            "rating": 2,
            "sanitized_title": "diameterbased_interactive_structure_discovery"
        },
        {
            "paper_title": "Gaussed: A probabilistic programming language for sequential experimental design",
            "rating": 2,
            "sanitized_title": "gaussed_a_probabilistic_programming_language_for_sequential_experimental_design"
        },
        {
            "paper_title": "Query by committee",
            "rating": 1,
            "sanitized_title": "query_by_committee"
        }
    ],
    "cost": 0.016115749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Targeted Active Learning for Probabilistic Models
October 24, 2022</p>
<p>Christopher Tosh 
Memorial Sloan Kettering Cancer Center
New YorkNY</p>
<p>Mauricio Tec 
Harvard University
CambridgeMA</p>
<p>Wesley Tansey 
Memorial Sloan Kettering Cancer Center
New YorkNY</p>
<p>Targeted Active Learning for Probabilistic Models
October 24, 2022
A fundamental task in science is to design experiments that yield valuable insights about the system under study. Mathematically, these insights can be represented as a utility or risk function that shapes the value of conducting each experiment. We present PDBAL, a targeted active learning method that adaptively designs experiments to maximize scientific utility. PDBAL takes a user-specified risk function and combines it with a probabilistic model of the experimental outcomes to choose designs that rapidly converge on a high-utility model. We prove theoretical bounds on the label complexity of PDBAL and provide fast closed-form solutions for designing experiments with common exponential family likelihoods. In simulation studies, PDBAL consistently outperforms standard untargeted approaches that focus on maximizing expected information gain over the design space. Finally, we demonstrate the scientific potential of PDBAL through a study on a large cancer drug screen dataset where PDBAL quickly recovers the most efficacious drugs with a small fraction of the total number of experiments.Extension to probabilistic models. With the aim of addressing these issues in mind, we propose the PDBAL objective: for x ∈ X , s n (x) = E θ ,θ,θ ∼πn E y∼P θ (x) d(θ, θ )P θ (y; x)P θ (y; x)e 2H θ (x) .(3) By using P θ (y; x), we are again minimizing some function of the diameter of the posterior π n+1 . Moreover, by switching from a maximization to an expectation over potential outcomes y, we avoid the tricky optimization problem. Finally, the entropy term in eq. (3) balances out the possibility of θ generating unlikely outcomes. Despite these changes, in Section 4 we show that PDBAL enjoys nice optimality properties, even as the scope of its applications has grown considerably over DBAL.Constant entropy models. When Θ parameterizes location models with fixed scale parameters, the entropy term is constant. We rewrite the objective for these models as</p>
<p>Introduction</p>
<p>Scientific experiments are often expensive, laborious, and time-consuming to conduct. In practice, this limits the capacity of many studies to only a small subset of possible experiments. Limited experimental capacity poses a risk: the sample size may be too small to learn meaningful aspects about the system under study. However, when experiments can be conducted sequentially or in batches, there is an opportunity to alleviate this risk by adaptively designing each batch. The hope is that the results of previous experiments can be used to design a maximally-informative batch of experiments to conduct next.</p>
<p>In machine learning, the sequential experimental design task is often posed as an active learning problem. The active learning paradigm allows a learner to adaptively choose on which data points it wants feedback. The objective is to fit a high-quality model while spending as little as possible on data collection. Modern active learning algorithms have shown substantial gains when optimizing models for aggregate objectives, such as accuracy (Ash et al., 2021) or parameter estimation (Tong and Koller, 2000).</p>
<p>Many scientific studies have a more targeted objective than a simple aggregate metric. For example, one may be interested in assessing the prognostic value of a collection potential biomarkers. Accurately modeling the distribution of the biomarker variables may require modeling nuisance variables about the patient, environment, and disease status. While optimizing an aggregate objective like accuracy can lead to recovery of the parameters of interest, this is merely a surrogate to our true objective. Consequently, it may lead to a less efficient data collection strategy.</p>
<p>Here we consider the task of targeted active learning. The goal in targeted active learning is to efficiently gather data to produce a model with high utility. Optimizing data collection for utility, rather than model E-mail: christopher.j.tosh@gmail.com, mauriciogtec@hsph.harvard.edu, tanseyw@mskcc.org performance, better aligns the model with the scientific objective of the study. It can also dramatically reduce the sample complexity of the active learning algorithm. For instance, in the case of d-dimensional linear regression, at least Ω(d) observations are required to learn the entire parameter vector. However, if the targeted objective is to estimate k d coordinates, there is an active learning strategy that can do so with O(k) queries, provided it is given access to enough unlabeled data (see Appendix A for a formal proof). This toy example shows the potential savings in active learning when the end objective is explicitly taken into account.</p>
<p>We propose Probabilistic Diameter-based Active Learning (PDBAL), a targeted active learning algorithm compatible with any probabilistic model. PDBAL builds on diameter-based active learning (Tosh and Dasgupta, 2017;Tosh and Hsu, 2020), a framework that allows a scientist to explicitly encode the targeted objective as a distance function between two hypothetical models of the data. Parts of the model that are not important to the scientific study can be ignored in the distance function, resulting in a targeted distance that directly encodes scientific utility. PDBAL generalizes DBAL from the simple finite outcome setting (e.g. multiclass classification) to arbitrary probabilistic models, greatly expanding the scope of its applicability.</p>
<p>We provide a theoretical analysis that bounds the number of queries for PDBAL to recover a model that is close to the ground-truth with respect to the target distance. We additionally prove lower bounds showing that under certain conditions, PDBAL is nearly optimal. In a suite of empirical evaluations on synthetic data, PDBAL consistently outperforms untargeted active learning approaches based on expected information gain and variance sampling. In a study using real cancer drug data to find drugs with high therapeutic index, PDBAL learns a model that accurately detects effective drugs after seeing only 10% of the total dataset. The generality and empirical success of PDBAL suggest it has the potential to significantly increase the scale of modern scientific studies.</p>
<p>Related work</p>
<p>There is a substantial body of work on active learning and Bayesian experimental design. Here, we outline some of the most relevant lines of work.</p>
<p>Bayesian active learning. The seminal work of Lindley (1956) introduced expected information gain (EIG) as a measure for the value of a new experiment in a Bayesian context. Roughly, EIG measures the change in the entropy of the posterior distribution of a parameter after conditioning on new data. Inspired by this work, others have proposed maximizing EIG as a Bayesian active learning strategy (MacKay, 1992;Lawrence et al., 2002). Noting that computing entropy in parameter space can be expensive for non-parametric models, Houlsby et al. (2011) rewrite EIG as a mutual information problem over outcomes. Their method, Bayesian active learning by disagreement (BALD), is used for Gaussian process classification. BALD has inspired a large body of work in developing EIG-based active learning strategies, particularly for Bayesian neural networks (Gal et al., 2017;Kirsch et al., 2019). However, despite its popularity, EIG can be shown to be suboptimal for reducing prediction error in general (Freund et al., 1997).</p>
<p>One alternative to such information gain strategies is Query by committee (QBC), (Seung et al., 1992;Freund et al., 1997), which more directly seeks to shrink the parameter space by querying points that elicit maximum disagreement among a committee of predictors. Recently, Riis et al. (2022) applied QBC to a Bayesian regression setup. Their method, B-QBC, reduces to choosing experiments that maximize the posterior variance of the mean predictor. For the special case of Gaussian models with homoscedastic observation noise, this is equivalent to EIG.</p>
<p>Another Bayesian active learning departure from EIG is the decision-theoretic approach of Fisher et al. (2021), called GAUSSED, based on Bayes' risk minimization. The objective function in GAUSSED is similar to the PDBAL objective when in the special case of homoskedastic location models and an untargeted squared error distance function over the entire latent parameter space.</p>
<p>Bayesian optimization. Black-box function optimization is another classical area of interest in the sequential experimental design literature. In this setting, there is an unknown global utility function that can be queried in a black box manner, and the goal is to find the set of inputs that maximize this function. A standard approach to this problem is to posit a Bayesian non-parametric model (such as a Gaussian process) of the underlying function, and then to adaptively make queries that trade off exploration of uncertainty and exploitation of suspected maxima (Hennig and Schuler, 2012;Hernández-Lobato et al., 2015;Kandasamy et al., 2018). One of the key differences between black-box (Bayesian) optimization and targeted (Bayesian) active learning is that in black-box optimization, the underlying utility function is being directly modeled. In targeted active learning, utility may only be indirectly expressed as a function of the underlying probabilistic models. One work that helps to bridge Bayesian optimization and targeted active learning is the decision-theoretic Bayesian optimization framework of Neiswanger et al. (2022), which considers a richer set of objectives related to the underlying utility function than simply finding a single maximum.</p>
<p>Active learning of probabilistic models. Beyond the Bayesian methods outlined above, others have considered alternate approaches to active learning for probabilistic models. Sabato and Munos (2014) studied active linear regression in the misspecified setting. Agarwal (2013) designed active learning algorithms for generalized linear models for multiclass classification in a streaming setting. Chaudhuri et al. (2015) studied a two-stage active learning procedure for maximum-likelihood estimators for a variety of probabilistic models. Ash et al. (2021) built on this two stage approach to design an active maximum-likelihood approach for deep learning-based models.</p>
<p>Setting</p>
<p>Let X denote a data space and Y denote a response space. Let D denote a marginal distribution over X . Our goal is to model our data with some parametric probabilistic model P θ (·; ·), where θ lies in a parameter space Θ and P θ (y; x) denotes the probability (or density) of observing y ∈ Y at data point x ∈ X . We will use the notation y ∼ P θ (x) to denote drawing y from the density P θ (·; x).</p>
<p>We consider models that factorize across data points, that is for x 1 , . . . , x n ∈ X n and y 1 , . . . , y n ∈ Y n , we have P θ (y 1:n ; x 1:n ) := P θ (y 1 , . . . , y n ; x 1 , . . . ,
x n ) = n i=1 P θ (y i ; x i ).
For a data point x ∈ X and parameter θ ∈ Θ, we denote the entropy of the response to x under model θ as
H θ (x) := E y∼P θ (x) log 1 P θ (y; x) .
We will take a Bayesian approach to learning. To that end, let π denote a prior distribution over Θ. Given observations (x 1 , y 1 ), . . . , (x n , y n ), denote the posterior distribution as
π n (θ) = 1 Z n π(θ) n i=1 P θ (y i ; x i ) where Z n = E θ ∼π n i=1 P θ (y i ; x i )
is the normalizing constant to make π n integrate to one. In this paper, we will assume that we are in the well-specified Bayesian setting, i.e. there is some ground-truth θ ∼ π, and when we query point x i , the observation y i is drawn from P θ (·; x i ). We will also use the notation π n (y; x) to denote the posterior predictive density π n (y; x) = E θ∼πn P θ (y; x) , and the notation y ∼ π n (x) to denote drawing y from the density π n (y; x).</p>
<p>A risk-aligned distance is a function d : Θ × Θ → [0, 1] satisfying two properties for all θ, θ ∈ Θ:</p>
<p>• Identity. i.e., d(θ, θ) = 0.</p>
<p>• Symmetry. i.e. d(θ, θ ) = d(θ , θ).</p>
<p>The requirement that d(θ, θ ) ≤ 1 is not onerous -any smooth distance over a bounded space can be transformed into a distance that satisfies this requirement by rescaling. In our setup, a risk-aligned distance encodes our objective: if we committed to the model θ when the true model was θ , then we expect to suffer a loss of d(θ, θ ). The goal in our setting is to find a posterior distribution π n with small average diameter:
avg-diam(π n ) = E θ,θ ∼πn [d(θ, θ )].
To see that this is a reasonable objective, observe that if θ ∼ π and (x 1 , y 1 ), . . . , (x n , y n ) is generated according to P θ , then after observing this data, θ is distributed according to π n . If we make predictions by sampling a model from π n , the expected risk of this strategy is exactly the average diameter. Moreover, even without this Bayesian assumption, the risk of this strategy can still be bounded above as a function of the average diameter (Tosh and Dasgupta, 2017, Lemma 2).</p>
<p>Probabilistic DBAL (PDBAL)</p>
<p>We first recall the standard (functional) diameter-based active learning algorithm. Let Y be a finite set, and let F ⊂ {f : X → Y} denote some function class, let d(·, ·) denote a distance over F, and let π n denote a posterior distribution over F. The DBAL approach is to score candidate queries x ∈ X according to the
function v n (x) = max y∈Y E f,f ∼πn d(f, f ) 1I f (x) = y = f (x) ,(1)
where 1I[·] denotes the indicator function, and then choose the available query x that minimizes v n (x). To practically implement this, one can sample f 1 , . . . , f m ∼ π n and compute the Monte Carlo approximation v n (x) = max
y∈Y 1 m 2 i&lt;j d(f i , f j ) 1I f i (x) = y = f j (x) .(2)
The idea behind eq. (1) is that in the realizable setting where the true model is in F, the posterior satisfies
π n (f ) ∝ π n−1 (f ) 1I[f (x n ) = y n ].
By choosing queries according to eq. (1), DBAL minimizes a function of the diameter of the posterior π n+1 , while also hedging its bets against the worst possible outcome. When moving from discrete-valued functions to probabilistic models, there are a few issues that arise. The first is that, unless our probabilistic models are deterministic, P θ (·; x) will in general be a distribution over outcomes and not a point mass. Thus, we should not use the indicator function to approximate the posterior update. The second issue is that when our outcomes are continuous, it may be intractable to compute a maximization over potential outcomes. Finally, even if computation over potential outcomes was not an issue, the outcomes that achieve the maximum may be so unlikely that they should not really be considered at all. Indeed, in the Bayesian setting it only makes sense to consider outcomes that have reasonable probability under π n .</p>
<p>Algorithm 1 PDBAL selection Require: Candidate queries x 1 , . . . , x B ∈ X , posterior distribution π n , Monte Carlo parameters m, N mc . Ensure: Next query x b .</p>
<p>Draw θ 1 , . . . , θ m ∼ π n . Draw (i 1 , j 1 , k 1 ), . . . , (i Nmc , j Nmc , k Nmc ) uniformly from the set {(i, j, k) : 1 ≤ i &lt; j &lt; k ≤ m}.
for b = 1, . . . , B do if M computable in closed form. then Computeŝ n (x b ) = 1 N mc Nmc t=1 d(θ it , θ jt )M (x b ; θ it , θ jt , θ kt ). else Draw y (b) 1 ∼ P θ 1 (x b ), . . . , y (b) m ∼ P θm (x b ). Computeŝ n (x b ) = 1 N mc Nmc t=1 d(θ it , θ jt )P θ i t (y (b) kt ; x b )P θ j t (y (b) kt ; x b ). end if end for return argmin x bŝ n (x b ).</p>
<p>Theory</p>
<p>For the purposes of this section, we will assume that all models induce the same entropy for a given x.</p>
<p>Assumption 1. Given any x ∈ X , there is a value H(x) such that H θ (x) = H(x) for all θ ∈ Θ.</p>
<p>Assumption 1 is satisfied, for example, whenever P θ is a location model whose scale component is fixed or otherwise assumed to be independent of θ. Assumption 1 is not required to implement PDBAL, but rather only factors into the analysis in this section.</p>
<p>For a prior π, observed data (x 1:n , y 1:n ), and value ρ ∈ [0, 1], we say that a data point x ∈ X ρ-splits the posterior π n if
s n (x) ≤ (1 − ρ)avg-diam(π n ),(7)
where s n is the objective function defined in eq. (3). Intuitively, eq. (7) captures the notion that there exists a query x that shrinks the average diameter by at least (1 − ρ) in expectation. We say that π n is (ρ, τ )-splittable if Pr x∼D (x ρ-splits π) ≥ τ.</p>
<p>Then for parameters ρ, , τ ∈ (0, 1), we say that Θ (along with corresponding marginal distribution D) has splitting index (ρ, , τ ) if for every posterior π n over Θ satisfying avg-diam(π n ) &gt; , π n is (ρ, τ )-splittable.</p>
<p>The definition of splitting in eq. (7) is similar to those provided in previous diameter-based active learning works (Tosh and Dasgupta, 2017;Tosh and Hsu, 2020). It corresponds to a requirement that posteriors which are not too concentrated (avg-diam &gt; ) should have a reasonable number of good queries (at least τ % if sampled from D).</p>
<p>One notable difference in this definition is the entropy term inside s n (x) in eq. (7). Without this term, a query with a noisy likelihood will be given higher saliency simply because it produces a wide range of possible outcomes. The entropy term balances out this bias by penalizing queries with a low signal-to-noise ratio.</p>
<p>A key observation in our analysis is that if we query a point that ρ-splits the current posterior π t , then in expectation a certain potential function will decrease.</p>
<p>Lemma 2. If we query a point x t+1 that ρ-splits π t , then
E y t+1 W 2 t+1 avg-diam(π t+1 ) ≤ (1 − ρ)W 2 t avg-diam(π t ), where W t = e t i=1 H(x i ) E θ∼π t i=1 P θ (y i ; x i ) .</p>
<p>Terminology</p>
<p>In order to prove bounds on the performance of PDBAL, we will need to make some assumptions about the complexity of the class Θ and the rates at which empirical entropies within this class concentrate. For a sequence of data pairs ω n = ((x 1 , y 1 ), . . . , (x n , y n )), let Θ| ωn denote the projection of Θ onto ω n . That is:
Θ| ωn = {(P θ (y 1 ; x 1 ), . . . , P θ (y n ; x n ) : θ ∈ Θ}.
For a sequence ω n and parameter &gt; 0, define N ( , Θ| ωn , d ll ) as the size of the minimum cover of Θ| ωn with respect to the distance
d ll (a, b) = n i=1 log a i b i = n i=1 log a i − log b i .
Here, we consider log 0 0 = 0. The uniform covering number N ll ( , Θ, n) is given by
N ll ( , Θ, n) = max N ( , Θ| ωn , d ll ) : ω n ∈ (X × Y) n .
We say that a class Θ is has log-likelihood dimension (c, d) if log N ll ( , Θ, n) ≤ d log cn for n ≥ d. The definition of N ll ( , Θ, n) is exactly the same as other uniform covering numbers (Anthony and Bartlett, 1999), modulo the non-standard distance. In the language of statistical learning theory, the log-likelihood dimension is a bound on the metric entropy log N ll ( , Θ, n). We will assume that the log-likelihood dimension is bounded.</p>
<p>Assumption 2. Θ has log-likelihood dimension (c, d),</p>
<p>As an example of a class with bounded log-likehood dimension, the following result shows how the complexity of a function class translates to the log-likehood dimension of the corresponding Gaussian location model. Recall a mean-zero random variable X is sub-Gamma with variance factor v &gt; 0 and scale parameter
c &gt; 0 if log E e λX ≤ λ 2 v 2(1 − cλ)
for all λ ∈ (0, 1/c). We say that the class Θ is entropy sub-Gamma with variance factor v &gt; 0 and scale parameter c &gt; 0 if the random variable
X = log 1 P θ (Y ;x) − H(x) is sub-Gaussian with variance factor v &gt; 0 for all θ ∈ Θ and x ∈ X , where Y ∼ P θ (x)
. For our bounds we will need Θ to be entropy sub-Gamma.</p>
<p>Assumption 3. Θ is entropy sub-Gamma with variance v and scale c , Many likelihoods satisfy Assumption 3 including Gaussians, as illustrated by the following result.</p>
<p>Proposition 4. Fix σ 2 &gt; 0 and let Θ denote the class of Gaussian location models from Proposition 3. Then Θ is entropy sub-Gamma with variance factor 1 and scale parameter 1.</p>
<p>Finally, we will require boundedness of both the entropy and the densities of models in Θ.</p>
<p>Assumption 4. There are constants c 1 , c 2 ≥ 0 such that P θ (y; x) ≤ c 1 and exp(H(x)) ≤ c 2 for all θ ∈ Θ, x ∈ X , and y ∈ Y.</p>
<p>Upper bounds</p>
<p>Given the terminology above, we have the following guarantee on PDBAL.</p>
<p>Theorem 5. Pick d ≥ 4 and suppose Assumptions 1 to 4 hold. If at every round t we make a query that ρ-splits π t and terminate when avg-diam(π t ) ≤ , then with probability 1 − δ, PDBAL terminates after fewer than
T ≤ O   max c 1 c 2 ρ 2 v log c 1 c 2 ρδ , d + c ρ log (d + c )c ρδ , 1 ρ log avg-diam(π)  
queries with a posterior satisfying avg-diam(π T ) ≤ .</p>
<p>As discussed above, Assumptions 1 to 4 are conditions on the complexity and form of Θ. The requirement on the behavior of PDBAL can be guaranteed with high probability across all rounds t with enough unlabeled data and a fine enough Monte Carlo approximation of eq. (3).</p>
<p>Given Lemma 2, the proof of Theorem 5 takes two steps:</p>
<ol>
<li>Showing that W 2 t avg-diam(π t ) must decrease exponentially quickly. 2. Showing that W t cannot decrease too quickly.</li>
</ol>
<p>The only way that both of these can hold is if avg-diam(π t ) must also decrease quickly, proving Theorem 5.</p>
<p>The first step, showing that W 2 t avg-diam(π t ) decreases quickly, is formalized by the following lemma. Lemma 6. Suppose Assumption 4 holds. For any t ≥ 1 and δ &gt; 0, if x i ρ-splits π i−1 for i = 1, . . . , t, then with probability at least 1 − δ,
W 2 t avg-diam(π t ) ≤ avg-diam(π) exp −tρ + c 1 c 2 2t log 1 δ .
The second step, showing that W t does not decrease too quickly, is formalized by the following lemma.</p>
<p>Lemma 7. Suppose Assumption 2 and Assumption 3. If θ ∼ π and y i ∼ P θ (·; x i ) for i = 1, . . . , t, then with probability at least 1 − δ
W t ≥ exp −2 − d log(ct) − 2tv log 2 δ − (c + 1) log 2 δ .</p>
<p>Lower bounds</p>
<p>We now turn to showing that in some cases, any optimal active learning strategy must have some dependence on the splitting index of the class. Our first result along these lines is in the deterministic setting.</p>
<p>Theorem 8. Let Θ denote a class of deterministic models that is not (ρ, , τ )-splittable for some ρ, ∈ (0, 1/4) and τ ∈ (0, 1/2). Let π be any prior distribution satisfying avg-diam(π) ≥ 4 which is not (ρ, τ )splittable. Then any active learning strategy that, with probability at least 5/6 (over the random samples from D and the observed responses), finds a posterior distribution satisfying avg-diam(π t ) ≤ must either observe at least 1 2τ unlabeled data points or make at least 1 2ρ queries.</p>
<p>We can relax the constraint that our models are deterministic to the case where they have bounded entropy at the expense of a slightly weaker lower bound.</p>
<p>Theorem 9. Let Θ denote a class of models that is not (ρ, , τ )-splittable for some ρ, ∈ (0, 1/4) and τ ∈ (0, 1/2) such that H(x) = h &lt; ρ 3/2 /6 and P θ (y; x) ≤ 1 for all x ∈ X , θ ∈ Θ and y ∈ Y. Let π be any prior distribution satisfying avg-diam(π) ≥ 4 which is not (ρ, τ )-splittable. Then any active learning strategy that, with probability at least 5/6 (over the random samples from D and the observed responses), finds a posterior distribution satisfying avg-diam(π t ) ≤ must either observe at least 1 2τ unlabeled data points or make at least 1 2 √ ρ queries.</p>
<p>The key ingredient to proving these lower bounds is demonstrating that splitting values are (approximately) sub-additive.
Lemma 10. Let ρ 1 , ρ 2 satisfy ρ 1 + ρ 2 &lt; 1. Suppose x 1 ρ 1 -splits π, x 2 ρ 2 -splits π, and H(x 1 ) = H(x 2 ) = h.
Then the following holds:</p>
<p>• If h = 0, then the combined query (x 1 , x 2 ) has splitting value at most ρ 1 + ρ 2 .</p>
<p>• If 0 ≤ h &lt; ρ 1 +ρ 2 6 , then the combined query (x 1 , x 2 ) has splitting value at most 2(ρ 1 + ρ 2 ).</p>
<p>Empirical results</p>
<p>In this section, we present our results on a suite of synthetic regression simulations as well as the results of a real-data study on a cancer drug discovery problem.</p>
<p>Synthetic regression simulations</p>
<p>Probabilistic models. We evaluated PDBAL on several probabilistic regression models. 1 In each of our regression experiments, the model was parameterized by a coefficient vector θ ∈ R d . Presented in this section are our results for linear regression with homoscedastic Gaussian noise, logistic regression, Poisson regression with the exponential link function, and Beta regression using the mean parameterization (Ferrari and Cribari-Neto, 2004):
P θ (y; x) = Beta y | φµ, φ(1 − µ) , where µ = 1 1+e − x,θ ,
x ∈ R d is the feature vector, and φ &gt; 0 is a fixed constant. For all experiments, we used a normal prior distribution on θ with identity covariance. For the linear regression setting, the posterior can be computed in closed form. We implemented the other models in PyStan and sampled from the posteriors using the No-U-Turn Sampler (NUTS) (Hoffman and Gelman, 2014;Riddell et al., 2021;Stan Development Team, 2022).</p>
<p>Objectives and distances. We considered three objectives with corresponding distance measures over parameters.</p>
<p>(i) First coordinate sign identification with corresponding distance
d first (θ, θ ) = 1I[sign(θ 1 ) = sign(θ 1 )].
(ii) Maximum magnitude coordinate identification:
d max (θ, θ ) = 1I[argmax i |θ i | = argmax i |θ i |].
(iii) Coordinate magnitude ranking:
d kendall (θ, θ ) = 1 2 1 − τ (|θ|, |θ |) where τ (|θ|, |θ |) is Kendall's τ correlation between of the pairs (|θ 1 |, |θ 1 |), . . . , (|θ d |, |θ d |).
In Appendix D, we present results on more settings.</p>
<p>Baseline comparisons. We compared against three baselines: RANDOM, VAR, and EIG. RANDOM chooses its queries uniformly at random from the set of available queries. The learning curve of RANDOM mimics what we would expect to see in a standard passive learning setting. VAR is the variance sampling strategy -it chooses queries based on maximizing the posterior predictive variance:
var y∼πn(x) (y) = E y∼πn(x) [y 2 ] − E y∼πn(x) [y] 2 .
The law of total variance allows us to rewrite this objective as
var y∼πn(x) (y) = E θ∼πn(x) var y∼P θ (x) (y) + var θ∼πn(x) E y∼P θ (x) [y] .
For all the likelihoods we consider in this section, both var y∼P θ (x) (y) and E y∼P θ (x) [y] can be computed in closed form. EIG is the expected information gain strategy. We use the BALD formulation of EIG (Houlsby et al., 2011) which chooses queries based on maximizing the mutual information between the outcome and the latent parameter θ:
I(y; θ|x, π n ) = H πn (x) − E θ∼πn H θ (x)] ,
where H πn (x) is the entropy of the posterior predictive π n (x). In some cases, such as linear regression, this can be computed in closed form, as it is proportional to the posterior predictive variance. For our other settings, we approximate it via numerical integration.</p>
<p>In our experiments, the ground truth θ was drawn uniformly from vectors of length 2. The data points were drawn from a mixture distribution: with probability 1 − p it is drawn uniformly from vectors of length 1, and with probability p each coordinate is set to 0 with probability 1/d and the remaining coordinates are drawn so that the vector is of length 1. For some objectives, this sparse distribution contains rare but informative data points. For all of our simulations, the data dimension was set to d = 10 and the mixing proportion was set to p = 1/10. All simulations were performed with 250 random seeds, and 95% confidence intervals are depicted using shading in our plots. Results. Figure 1 shows the results of our simulation study. In all settings, we see that PDBAL never does any worse than the baselines, and it does significantly better in some. We see that for the more focused objectives like the first coordinate identification, PDBAL has a much stronger separation from the untargeted baselines. We also see the influence of probabilistic model on performance, with the least gains coming from linear regression and the largest gains coming from logistic and Beta regression. This may be due to the fact that in our bounded linear regression setting, extreme values are unlikely to occur and so PDBAL is unable to make large changes to the posterior, whereas in Beta regression, values close to 0 and 1 are possible and allow for large changes to the posterior.</p>
<p>Cancer drug screen experiments</p>
<p>The Genomics of Drug Sensitivity in Cancer (GDSC) database (Garnett et al., 2012;Yang et al., 2012) is a large public database of cancer cell line experiments across a range of anticancer drug agents. Each drug is tested at a range of seven doses, allowing the full dose-response curve to be estimated. The reported outcome for each experiment is cell viability, defined as the proportion of cells alive after treatment. Large scale screens like GDSC are expensive and time-consuming; reducing the number of experiments required to accurately estimate responses and discover effective drugs could substantially expand the scale and speed of possible experiments.</p>
<p>We study the potential of PDBAL to adaptively screen anti-cancer drugs in a retrospective experiment on GDSC. At each step, the algorithm is allowed to conduct a single trial of a drug tested against a cancer cell line. We consider coarse-and fine-grained settings for drug selection. In the coarse setting, the algorithm selects the drug and cell line then observes the responses at each of the seven doses; in the fine-grain setting, the algorithm additionally specifies a dose. We used a subset of 100 drugs and 20 cell lines for a total of n = 2K possible coarse-grained experiments and n = 14K possible fine-grained experiments. Performance is evaluated as the error over each (cell line, drug, dose) when compared to what the underlying probabilistic model would learn from the full data.</p>
<p>All strategies use the same common Bayesian factor model,
y ij d ∼ Normal(µ ij d , σ 2 ) µ ij d = a + b i + c j d + v T i w j d ,(9)
where i indexes the cell lines; j d indexes drug j at dose d; y ij d is the viability projected to the real line with a logistic transformation; a, b i and c j d are scalar intercepts; and w i and v j d are q-dimensional embeddings governing the interaction between cancer cell lines and drugs. The model is completed with hierarchical shrinking and smoothness-inducing priors. Similar factor models have previously been employed for modeling cancer drug screens (Tansey et al., 2022b). Appendix E provides additional details about the model specification, experimental setup, and data pre-preprocessing.</p>
<p>To implement PDBAL, we use the mean-squared error distance of viability in probability space
d v-mse (θ, θ ) = 1 M i,j,d (sigmoid(µ ij d ) − sigmoid(µ ij d )) 2 ,
where µ is the predictive mean in eq. (9) and M is the total number of cell lines × drugs × doses.</p>
<p>Results. Figure 2 shows the results of our experiments using the same baselines considered in Section 5.1. The y-axis represents the target error d v-mse (μ t ,μ * ), whereμ t = E θ∼πn [µ] is the posterior mean of the partial model fitted after observing a fraction t of the data, andμ * = E θ * ∼π full [µ] the true predictive mean obtained from fitting the model with the full data.</p>
<p>In both experiments, PDBAL outperforms the other selection strategies. The fine-grained setting has more degrees of freedom and thus shows a larger performance gain for PDBAL. We did not evaluate EIG in the fine-grained experiment because the numerical integration required to approximate the objective was computationally prohibitive. Additional details on the experiment results are in Appendix E. To investigate the potential for early drug discovery, we conducted an additional evaluation comparing the ability to identify drugs that have targeted effects on specific cell lines. To measure selectivity, we follow the procedure outlined by Tansey et al. (2022a), which we briefly summarize here for completeness. A drug is selective if it has a dose at which it is safe on the majority of the cell lines but is highly toxic on at least one cell line. Here, safety and toxicity are defined as viability above 0.8 and below 0.5, respectively. The model trained on the whole dataset identified 9 drugs from the pool of 100. For each active learning method we used the corresponding model's posterior probability that a drug is selective to provide a ranking of drugs after observing 5% and 10% of the data. </p>
<p>Discussion</p>
<p>We introduced PDBAL, a targeted active learning algorithm compatible with any probabilistic model and any risk-aligned distance. We proved theoretical bounds on the query complexity of PDBAL, showing that in certain cases it is nearly optimal. In our simulation study, we showed the benefits of PDBAL in a range of settings. On a real data example, PDBAL quickly converged to an accurate model with half the data required by other adaptive methods.</p>
<p>Limitations. On the theoretical side, there are some concrete ways in which our results can be improved. The lower bounds we prove are limited to settings with small entropy, which limits the class of probabilistic models on which we can prove that we achieve near-optimality. Further, both our upper bounds and lower bounds rely on Assumption 1, which requires the conditional entropy to not depend on the parameter. Our simulation studies show PDBAL performs well even when these assumptions do not hold, suggesting that these limitations may be due to our analysis techniques rather than PDBAL itself. Finally, our general setup is within the well-specified Bayesian regime, which limits the applicability of this work to settings where we are comfortable assuming that our prior and likelihood are sufficiently accurate models of the real world.</p>
<p>Societal impact. Any machine learning method carries risks of misapplications. We envision our work as being utilized by scientists to reduce the number of experiments needed to make scientific discoveries. This method, however, is agnostic to what those discoveries may be, which allows for the possibility of using this (and related) methodology to quickly uncover discoveries with negative societal consequences. We believe the immediate use cases in science outweigh these risks.</p>
<p>A Motivating example: Linear regression</p>
<p>Consider a well-specified high-dimensional linear regression setting in which the covariates are drawn from the uniform distribution over the d-dimensional sphere of radius 1. For a data point x (i) ∈ R d , suppose the corresponding label is given by
y i = β , x (i) + i
where i ∼ N (0, 1) is independent noise and β is some ground-truth vector satisfying β 2 = 1. Suppose further, that among the covariates there is a subset of k d (e.g., k = O(log d)) covariates of interest, and the remaining coordinates are nuisance variables.</p>
<p>If we try to estimate the entire vector β , then both active and passive learning will require Ω(d) queries to find someβ such that β − β 2 is smaller than some constant. However, if we only care about estimating the coordinates of β that correspond to the covariates of interest, then this can be done using only O(k) queries, given access to enough unlabeled data.</p>
<p>To see how, let S ⊂ {1, . . . , n} denote the coordinates of interest. For a vector x ∈ R d , let x S ∈ R k denote the x restricted to S. Given a pool of unlabeled data, we will restrict our queries to the points x such that x S 2 2 is sufficiently large. Note that for any cutoff α ∈ (0, 1) and integer m ≥ 1, there is size N ≥ 1 such that a random draw of N points will have a subset of size m whose elements satisfy x S 2 2 ≥ 1 − α. Given n ≥ k queries that satisfy X S is full-rank, our estimator will be the least squares estimator defined over the coordinates S:β = (X T S X S ) −1 X T S y.
Letỹ i = y i − β S c , x (i)
S c denote the (unobserved) adjusted response values. In vector form,ỹ = y − X S c β S c . Then we can decompose the difference betweenβ and β as
β − β = (X T S X S ) −1 X T S y − β ≤ (X T S X S ) −1 X T Sỹ − β + (X T S X S ) −1 X T S (y −ỹ) = (X T S X S ) −1 X T Sỹ − β + (X T S X S ) −1 X T S X S c β S c = (X T S X S ) −1 X T S + (X T S X S ) −1 X T S X S c β S c .
Here, the last line follows from the well-specified nature of the problem. We can bound each of the terms in the last line separately.</p>
<p>To analyze the first term, observe that since ∼ N (0, I n ), we have
(X T S X S ) −1 X T S ∼ N 0, (X T S X S ) −1 .
Bounding the first term comes down to bounding the 2 -norm of a random normal vector. The following lemma provides such bounds.</p>
<p>Lemma 11. Let Σ ∈ R d×d denote a positive definite covariance matrix with eigenvalues λ 1 , . . . , λ d . Then for v ∼ N (0, Σ),
Pr   v 2 2 ≤ t d j=1 λ j   ≤ √ te if t &lt; 1 Pr   v 2 2 ≥ t d j=1 λ j   ≤ √ te −(t−1)/2 if t &gt; 1.
Proof. Let QΛQ T = Σ denote the eigendecomposition of Σ. Then observe that z = Λ 1/2 Q T v ∼ N (0, I d ) and</p>
<p>v
2 2 = v T QΛQ T v = u T Λu = d j=1 λ j u 2 j where u := Q T v ∼ N (0, I) (since Q is orthonormal). Thus, we need to establish Pr   d j=1 λ j u 2 j ≤ t d j=1 λ j   ≤ √ te if t &lt; 1 Pr   d j=1 λ j u 2 j ≥ t d j=1 λ j   ≤ √ te −(t−1)/2 if t &gt; 1 for u 1 , . . . , u d ∼ N (0, 1) i.i.d.
We will show both inequalities through the Cramèr-Chernoff method (Boucheron et al., 2013, Chapter 2.2), starting with the first inequality. Fix any α ≥ 0 and denote λ = d j=1 λ j , then
Pr   d j=1 λ j u 2 j ≤ t d j=1 λ j   = E   1I   t d j=1 λ j − d j=1 λ j u 2 j ≥ 0      ≤ E     exp   α   tλ − d j=1 λ j u 2 j          = exp (αtλ) d j=1 E exp −αλ j u 2 j = exp (αtλ) d j=1 1 + 2αλ j −1/2 ≤ exp (αtλ) (1 + 2αλ) −1/2
where the second-to-last line follows from the form of the Chi-squared moment generating function. Taking α = 1 2λ 1 t − 1 completes the proof of the first inequality.</p>
<p>For the second inequality, we follow similar steps, observing that for α ≤ 1 2λ ≤ min j 1 2λ j we have
Pr   d j=1 λ j u 2 j ≥ t d j=1 λ j   = E   1I   d j=1 λ j u 2 j − t d j=1 λ j ≥ 0      ≤ E     exp   α   d j=1 λ j u 2 j − tλ          = exp (−αtλ) d j=1 E exp αλ j u 2 j = exp (−αtλ) d j=1 1 − 2αλ j −1/2 ≤ exp (−αtλ) (1 − 2αλ) −1/2 .
Plugging in α = 1 2λ 1 − 1 t gives us the second inequality.</p>
<p>As a consequence of Lemma 11, we have with probability at least 1 − δ,
(X T S X S ) −1 X T S 2 2 ≤ k λ min (X T S X S ) log(1/δ)
for small enough δ. If our active learner draws a large enough collection of data points satisfying x S 2 2 ≥ 1 − α, then with high probability there will be a subset of n data points such that the minimum eigenvalue of X T S X S is at least (1 − α)n/2. Combined with the above, this gives us
(X T S X S ) −1 X T S 2 2 ≤ 2k (1 − α)n log(1/δ).
On the other hand, every row in X S c has squared 2 -norm bounded by α and β 2 2 ≤ 1. Thus, a crude bound gives us
(X T S X S ) −1 X T S X S c β S c 2 2 ≤ nα λ min (X T S X S ) ≤ 2α 1 − α .
Thus, for any target error &gt; 0 and failure probability δ &gt; 0, we can choose α = O ( ) and n = O k log 1 δ and terminate with an estimate satisfying β − β 2 ≤ with probability at least 1 − δ.</p>
<p>B Proofs from Section 3 B.1 Proof of Proposition 1</p>
<p>From the form of the Gaussian likelihood, we have E y∼N (µ 1 ,σ 2 1 I d ) N (y; µ 2 , σ 2 2 I d )N (y; µ 3 , σ 2 3 I d )
= R d 1 2πσ 2 1 · 1 2πσ 2 2 · 1 2πσ 2 3 d/2 exp − 1 2σ 2 1 y − µ 1 2 − 1 2σ 2 2 y − µ 2 2 − 1 2σ 2 3 y − µ 3 2 dy.
The bias-variance decomposition of squared error implies that for any c 1 , . . . , c n ≥ 0 and b, b 1 , . . . , b n ∈ R d , we have
n i=1 c i b i − b 2 = n i=1 c i b i −b 2 + c sum b − b 2 where c sum = i c i and b = 1 csum i c i b i .
Applying this identity to the exponential above with the notation
τ = 3 i=1 1 σ 2 i andμ = 1 τ 3 i=1 µ i σ 2 i , we have E y∼N (µ 1 ,σ 2 1 ) N (y; µ 2 , σ 2 2 )N (y; µ 3 , σ 2 3 ) = 1 (2π) 3 σ 2 1 σ 2 2 σ 2 3 d/2 R d exp   − 3 i=1 1 2σ 2 i μ − µ i 2 − τ 2 y −μ 2   dy = 1 (2π) 3 σ 2 1 σ 2 2 σ 2 3 d/2 exp   − 3 i=1 1 2σ 2 i μ − µ i 2   R d exp − τ 2 y −μ 2 dy = 1 (2π) 3 σ 2 1 σ 2 2 σ 2 3 d/2 exp   − 3 i=1 1 2σ 2 i μ − µ i 2   · 2π τ d/2 = 1 (2π) 2 (σ 2 1 σ 2 2 + σ 2 2 σ 2 3 + σ 2 1 σ 2 3 ) d/2 exp   − 3 i=1 1 2σ 2 i μ − µ i 2  
where the second-to-last line follows from the fact that the integral is exactly the normalizing constant of a spherical Gaussian with meanμ and variance 1/τ and the last line follows from expanding the definition of τ . Any discrete random variable X taking values x i with probability p i for i = 1, . . . , m satisfies the following:
m i=1 p i x i − E[X] 2 = E X − E[X] 2 = 1≤i&lt;j≤m p i p j x i − x j 2 .
Applying this to the discrete random variable that takes value µ i with probability 1
τ σ 2 i , 3 i=1 1 2σ 2 i μ − µ i 2 = 1 σ 2 1 σ 2 2 τ 2 µ 1 − µ 2 2 + 1 σ 2 2 σ 2 3 τ 2 µ 2 − µ 3 2 + 1 σ 2 1 σ 2 3 τ 2 µ 1 − µ 3 2 = σ 2 1 σ 2 2 σ 2 3 (σ 2 1 σ 2 2 + σ 2 2 σ 2 3 + σ 2 1 σ 2 3 ) 2 σ 2 3 µ 1 − µ 2 2 + σ 2 1 µ 2 − µ 3 2 + σ 2 2 µ 1 − µ 3 2 .
Putting it all together gives us the desired identity.</p>
<p>B.2 Other closed form solutions</p>
<p>Proposition 12. Fix p (1) , p (2) , p (3) ∈ ∆ K . Then
E y∼p (1) p (2) y p (3) y = K y=1 p (1) y p (2) y p (3) y .
Proof. This follows immediately by substitution.</p>
<p>Proposition 13. Fix λ 1 , λ 2 , λ 3 &gt; 0, and let f λ (y) = λe −λy denote the density function of the exponential distribution with parameter λ. Then
E y∼f λ 1 f λ 2 (y)f λ 3 (y) = λ 1 λ 2 λ 3 λ 1 + λ 2 + λ 3 .
Proof. Simple calculus gives us
E y∼f λ 1 f λ 2 (y)f λ 3 (y) = ∞ 0 λ 1 e −λ 1 y λ 2 e −λ 2 y λ 3 e −λ 3 y dy = λ 1 λ 2 λ 3 ∞ 0 e −(λ 1 +λ 2 +λ 3 )y dy = λ 1 λ 2 λ 3 λ 1 + λ 2 + λ 3 .
Proposition 14. Fix p 1 , p 2 , p 3 ∈ [0, 1], and let f p (y) = p(1 − p) k denote the mass function of the geometric distribution with parameter p. Then
E y∼fp 1 f p 2 (y)f p 3 (y) = p 1 p 2 p 3 p 1 + p 2 + p 3 − p 1 p 2 − p 2 p 3 − p 1 p 3 + p 1 p 2 p 3 .
Proof. Expanding out, we have
E y∼fp 1 f p 2 (y)f p 3 (y) = ∞ y=0 p 1 p 2 p 3 (1 − p 1 )(1 − p 2 )(1 − p 3 ) k = p 1 p 2 p 3 ∞ y=0 1 − (p 1 + p 2 + p 3 − p 1 p 2 − p 2 p 3 − p 1 p 3 + p 1 p 2 p 3 ) k = p 1 p 2 p 3 p 1 + p 2 + p 3 − p 1 p 2 − p 2 p 3 − p 1 p 3 + p 1 p 2 p 3 .
Proposition 15. Fix α 1 , α 2 , α 3 , β 1 , β 2 , β 3 &gt; 0 such that α 1 +α 2 +α 3 &gt; 2, and let f α,β (y) = β α Γ(α) x α−1 e −βx denote the density function of the gamma distribution with parameters α, β. Then
E y∼f α 1 ,β 1 f α 2 ,β 2 (y)f α 3 ,β 3 (y) = β α 1 1 β α 2 2 β α 3 3 Γ(α 1 + α 2 + α 3 − 2) (β 1 + β 2 + β 3 ) (α 1 +α 2 +α 3 −2) Γ(α 1 )Γ(α 2 )Γ(α 3 )
.</p>
<p>Proof. Expanding out, we have
E y∼f α 1 ,β 1 f α 2 ,β 2 (y)f α 3 ,β 3 (y) = β α 1 1 β α 2 2 β α 3 3 Γ(α 1 )Γ(α 2 )Γ(α 3 ) ∞ 0 x α 1 −1 e −β 1 x x α 2 −1 e −β 2 x x α 3 −1 e −β 3 x dx = β α 1 1 β α 2 2 β α 3 3 Γ(α 1 )Γ(α 2 )Γ(α 3 ) ∞ 0 x α 1 +α 2 +α 3 −3 e −(β 1 +β 2 +β 3 )x dx = β α 1 1 β α 2 2 β α 3 3 Γ(α 1 + α 2 + α 3 − 2) (β 1 + β 2 + β 3 ) (α 1 +α 2 +α 3 −2) Γ(α 1 )Γ(α 2 )Γ(α 3 ) ,
where the last line follows from the fact that the integral is the normalizing constant of a gamma distribution with parameters α 1 + α 2 + α 3 − 2 and β 1 + β 2 + β 3 .</p>
<p>Proposition 16. Fix p 1 , p 2 , p 3 &gt; 0 and r ≥ 1, and let f r,p (k) = k+r−1 k (1 − p) r p k denote the probability mass function of the negative binomial distribution with parameters r, p. Then
E k∼fr,p 1 f r,p 2 (k)f r,p 3 (k) = 3 i=1 (1 − p i ) r (1 − p 1 p 2 p 3 ) r r−1 k=0 r − 1 k 2 −2k Γ(2k + r) Γ(r)Γ(k + 1) 2 4p 1 p 2 p 3 (1 − p 1 p 2 p 3 ) 2 k .
Proof. Expanding out, we have
E k∼fr,p 1 f r,p 2 (k)f r,p 3 (k) = ∞ k=0 k + r − 1 k 3 3 i=1 (1 − p i ) r p k i =   3 i=1 (1 − p i ) r   ∞ k=0 k + r − 1 k 3 (p 1 p 2 p 3 ) k =   3 i=1 (1 − p i ) r   3 F 2 (r, r, r; 1, 1; p 1 p 2 p 3 )
where 3 F 2 (a, b, c; e, f ; x) is the generalized hypergeometric function. From the identity
3 F 2 (a, b, c; a−b+1, a−c+1; z) = (1−z) −a 3 F 2 a − b − c + 1, a 2 , a + 1 2 ; a − b + 1, a − c + 1; − 4z (1 − z) 2 we have 3 F 2 (r, r, r; 1, 1; z) = (1 − z) −r 3 F 2 1 − r, r 2 , r + 1 2 ; 1, 1; − 4z (1 − z) 2 .
Observe that whenever m is a non-negative integer, we have
3 F 2 (−m, b, c; 1, 1; z) = m k=0 (−1) k m k Γ(b + k)Γ(c + k) Γ(b)Γ(c)Γ(k + 1) 2 z k .
Putting it together, we have for any z &gt; 0 and any integer r ≥ 1, 3 F 2 (r, r, r; 1, 1; z) = (1 − z) −r
r−1 k=0 (−1) k r − 1 k Γ r 2 + k Γ r+1 2 + k Γ r 2 Γ r+1 2 Γ(k + 1) 2 − 4z (1 − z) 2 k = (1 − z) −r r−1 k=0 4z (1 − z) 2 k r − 1 k 2 −2k Γ(2k + r) Γ(r)Γ(k + 1) 2
where the last line follows from the Legendre duplication formula:
Γ(x)Γ(x + 1/2) = 2 1−2x √ πΓ(2x).
Substituting in z = p 1 p 2 p 3 gives us the proposition statement.</p>
<p>C Proofs from Section 4 C.1 Proof of Lemma 2
Let Z t = E θ∼π [P θ (y 1:t ; x 1:t )], so that we have W t = Z t e t i=1 H(x i ) .
Observe that Z t is exactly the normalizing constant arising in Bayes' rule: π t (θ) = 1 Z t π(θ)P θ (y 1:t ; x 1:t ).</p>
<p>Thus, for any x t+1 , y t+1 , we have
Z t+1 Z t = 1 Z t E θ∼π P θ (y 1:t+1 ; x 1:t+1 ) = E θ∼π P θ (y 1:t ; x 1:t ) Z t P θ (y t+1 ; x t+1 ) = E θ∼πt P θ (y t+1 ; x t+1 ) .
Moreover, by Bayes's rule, we also have
π t+1 (θ) = π t (θ)P θ (y t+1 ; x t+1 ) E θ∼πt P θ (y t+1 , x t+1 ) = Z t Z t+1 π t (θ)P θ (y t+1 ; x t+1 ).
Putting it all together,
W 2 t+1 avg-diam(π t+1 ) = W 2 t+1 E θ,θ ∼π t+1 d(θ, θ ) = W 2 t+1 E θ,θ ∼πt Z 2 t Z 2 t+1 P θ (y t+1 ; x t+1 )P θ (y t+1 ; x t+1 )d(θ, θ ) = W 2 t e 2H(x t+1 ) E θ,θ ∼πt Z 2 t Z 2 t+1 P θ (y t+1 ; x t+1 )P θ (y t+1 ; x t+1 )d(θ, θ ) .
Taking expectations over y t+1 and applying the definition of splitting finishes the argument.</p>
<p>C.2 Proof of Proposition 3</p>
<p>Let (x 1 , y 1 ), . . . , (x n , y n ) be given. For θ, θ ∈ Θ,
n i=1 log P θ (y i ; x i ) − log P θ (y i ; x i ) = 1 2σ 2 n i=1 (y i − θ(x i )) 2 − (y i − θ (x i )) 2 ≤ 1 2σ 2 n i=1 (y i − θ(x i )) 2 − (y i − θ (x i )) 2 ≤ 1 2σ 2 n i=1 θ(x i ) − θ (x i ) 2|y i | + θ(x i ) + θ (x i ) ≤ 2B σ 2 n i=1 θ(x i ) − θ (x i ) .
Thus, N ll ( , Θ, n) ≤ N 1 (σ 2 /2B, Θ, n), where N 1 denotes uniform covering with respect to 1 distance. By known bounds on the covering number in terms of the pseudo-dimension, e.g. (Anthony and Bartlett, 1999, Theorem 18.4), we have
N 1 ( , Θ, n) ≤ e(d + 1) 4eBn d .
Thus,
N ll ( , Θ, n) ≤ e(d + 1) 8eB 2 n σ 2 d .
The definition of log-likelihood dimension finishes the argument.</p>
<p>C.3 Proof of Proposition 4</p>
<p>For simplicity, let µ ∈ R and σ 2 &gt; 0. Let N (·; µ, σ 2 ) denote the density of a Gaussian with mean µ and variance σ 2 . Let H = 1 2 + 1 2 log 2πσ 2 denote the entropy of N (·; µ, σ 2 ). Suppose Y ∼ N (µ, σ 2 ) and X = log 1 P θ (Y ;x) − H, then
E[e λX ] = E   exp λ 2 log 2πσ 2 + λ (Y − µ) 2 2σ 2 − λ 2 log 2πσ 2 − λ 2   = e −λ/2 E   exp λ (Y − µ) 2 2σ 2   = e −λ/2 (1 − λ) 1/2
for λ &lt; 1. Here, the last line follows from the fact that (Y −µ) 2 σ 2 is chi-squared with one degree of freedom, and the known form of the chi-squared moment generating function. Thus, for λ ∈ (0, 1), we have
log E[e λX ] = 1 2 log 1 1 − λ − λ 2 ≤ λ 2 2(1 − λ) ,
where the inequality follows from the bound log(x) ≤ x − 1 for x &gt; 0. Thus, Gaussian location models are entropy sub-Gamma with variance factor 1 and scale parameter 1.</p>
<p>C.4 Proof of Lemma 6</p>
<p>For t ≥ 1, define ∆ t = 1 − W 2 t avg-diam(πt) W 2 t−1 avg-diam(π t−1 ) . Let F t denote the sigma-field of all outcomes up to an including time t. Then if we query point x t which ρ-splits π t+1 , the definition of splitting implies that
E[∆ t+1 | x t , F t ] ≥ ρ. Let S t = t i=1 (∆ t − ρ)
. The above implies that S t is a submartingale. Moreover, if P θ (y; x) ≤ c 1 uniformly for all θ, x, y and e H(x) ≤ c 2 for all x, then
0 ≤ W 2 t+1 avg-diam(π t+1 ) W 2 t avg-diam(π t+1 ) = e 2H(x t+1 ) E θ,θ ∼π [d(θ, θ ) t+1 i=1 P θ (y i ; x i )P θ (y i ; x i )] E θ,θ ∼π [d(θ, θ ) t i=1 P θ (y i ; x i )P θ (y i ; x i )] ≤ c 2 1 c 2 2 .
This implies |S t+1 − S t | ≤ c 2 1 c 2 2 , and thus the Azuma-Hoeffding inequality (Azuma, 1967) tells us that with probability at least 1 − δ,
t i=1 ∆ t = tρ + S t ≥ tρ − c 1 c 2 2t log 1 δ .</p>
<p>C.5 Proof of Lemma 7</p>
<p>To prove Lemma 7, we will need the following lower bound.</p>
<p>Lemma 17. Fix ω n = ((x 1 , y 1 ), . . . , (x n , y n )) ∈ (X × Y) n and let M be an -covering of Θ| ωn with respect to d ll (·, ·). Let Θ 1 , . . . , Θ |M | be the induced Voronoi partition of Θ (breaking ties arbitrarily). Then for any Θ i and any θ ∈ Θ i
E θ∼π   n i=1 P θ (y i | x i )   ≥ e −2 π(Θ i ) n i=1 P θ (y i | x i ).
Proof. Fix Θ i and let m i ∈ M denote the Voronoi 'center.' Then for any θ, θ ∈ Θ i , we have
d ll (θ, θ ) ≤ d ll (θ, m i ) + d ll (m i , θ ) ≤ 2 ,
where we have used the fact that M is an -cover. Thus, we have
n i=1 P θ (y i | x i ) ≥ e −2 P θ (y i | x i ).
Finally, because Θ 1 , . . . , Θ |M | are disjoint, we have
E θ∼π   n i=1 P θ (y i | x i )   ≥ n j=1 π(Θ j )E θ∼π   n i=1 P θ (y i | x i ) | θ ∈ Θ j   ≥ e −2 π(Θ i )P θ (y i | x i ).
We will also require the following result which follows directly from well-known tail bounds for sub-Gamma random variables.</p>
<p>Lemma 18. Suppose Θ is entropy sub-Gamma with variance factor v &gt; 0 and scale parameter c &gt; 0. If y i ∼ P θ (·; x i ) for i = 1, . . . , t, then with probability at least 1 − δ,
t i=1 log 1 P θ (y i ; x i ) ≤ t i=1 H(x i ) + 2tv log 1 δ + c log 1 δ .
Proof. Observe that for independent mean-zero sub-Gamma random variables X 1 , . . . , X n with variance factor v &gt; 0 and scale parameter c &gt; 0, the random variable Z = i X i satisfies
log E e λZ = log E   n i=1 e λX i   = n i=1 log E e λX i ≤ nvλ 2 2(1 − cλ)
for λ ∈ (0, 1/c). Thus, Z is sub-Gamma with variance factor nv and scale parameter c. The argument is finished via standard concentration results on sub-Gamma random variables, e.g. (Boucheron et al., 2013, Chapter 2.4).</p>
<p>With Lemmas 17 and 18 in hand, we can turn to proving Lemma 7.</p>
<p>Proof of Lemma 7. Recall that we can write
W t = exp   t i=1 H(x i )   E θ∼π   t i=1 P θ (y i ; x i )   .
Let ω t = ((x 1 , y 1 ), . . . , (x t , y t )) denote our data. By Lemma 17, we have
E θ∼π   t i=1 P θ (y i | x i )   ≥ e −2 π(Θ i ) t j=1 P θ i (y j | x j )
where Θ 1 , . . . , Θ M is the Voronoi partition induced by a minimal 1-covering of Θ| ωn , Θ i is any of these partition elements, and θ i is any element in Θ i . Let S denote the set of indices i such that π(Θ i ) &lt; δ/(2M ). Then we have
Pr ∃i ∈ S s.t. θ ∈ Θ i = i∈S π(Θ i ) ≤ |S|δ 2M ≤ δ 2 .
Thus, if Θ is the element of the partition that θ falls into, we have with probability at least 1 − δ/2
E θ∼π   t i=1 P θ (y i | x i )   ≥ e −2 π(Θ ) t j=1 P θ (y j | x j ) ≥ 1 M exp −2 − log 2 δ t j=1 P θ (y j | x j ) ≥ exp −2 − d log(ct) − log 2 δ t j=1 P θ (y j | x j ),
where the last line follows from the fact that M ≤ (ct) d . Finally, observe that with probability at least 1 − δ/2, Lemma 18 implies
t j=1 P θ (y j | x j ) = exp   − t j=1 log 1 P θ (y j | x j )   ≥ exp   − t i=1 H(x i ) − 2tv log 2 δ − c log 2 δ   .
A union bound finishes the argument.</p>
<p>C.6 Proof of Theorem 5</p>
<p>Combining Lemma 6 with a union bound, we have that with probability 1 − δ/2
W 2 t avg-diam(π t ) ≤ exp −tρ + c 1 c 2 2t log 2t(t + 1) δ avg-diam(π)
for all t ≥ 1, simultaneously. Similarly, combining Lemma 7 with a union bound gives us with probability at least 1 − δ,
W 2 t ≥ exp −d log(ct) − 2 − log 4t(t + 1) δ − 2tv log 4t(t + 1) δ − c log 4t(t + 1) δ
for all t ≥ 1. Thus, with probability 1 − δ, both of these occur simultaneously. Plugging in the value of T from the theorem statement,
avg-diam(π T ) ≤ avg-diam(π) exp −T ρ + c 1 c 2 2T log 2T (T + 1) δ + d log(cT ) +2 + log 4T (T + 1) δ + 2T v log 4T (T + 1) δ + c log 4T (T + 1) δ ≤ avg-diam(π) exp −T ρ + 4c 1 c 2 T v log 4T (T + 1) δ + (d + c + 1) log 4cT (T + 1) δ
The above is less than when we have
T ≥ max    9 12c 1 c 2 ρ 2 v log 12c 1 c 2 ρ 2 · 4 δ , 27 ρ (d + c + 1) log 3 ρ (d + c + 1) · 4c δ , 3 ρ log avg-diam(π)    .
Here, we have made use of the fact that if a ≥ 1, b ≥ e and x ≥ 9a log(ab), then x ≥ a log(bx(x + 1)).</p>
<p>C.7 Proof of Lemma 10</p>
<p>Observe by the product measure assumption of P θ (·; x 1 , x 2 ), we have
E θ ∼π E y 1 ,y 2 ∼P θ (x 1 ,x 2 ) E θ,θ ∼π P θ (y 1 , y 2 ; x 1 , x 2 )P θ (y 1 , y 2 ; x 1 , x 2 )d(θ, θ ) = E θ,θ ,θ ∼π d(θ, θ )E y 1 ∼P θ (x 1 ) P θ (y 1 ; x 1 )P θ (y 1 ; x 1 ) E y 2 ∼P θ (x 2 ) P θ (y 2 ; x 2 )P θ (y 2 ; x 2 ) =: E θ,θ ,θ ∼π d(θ, θ )α 1 (θ, θ , θ )α 2 (θ, θ , θ ) .
Let U be the random variable that takes on value α 1 (θ, θ , θ ) and let V denote the random variable that takes on value α 2 (θ, θ , θ ). Here, θ, θ , θ occur with probability π(θ)π(θ )π(θ )d(θ,θ )</p>
<p>avg-diam(π)</p>
<p>. Then it is not hard to 
see that E[U ] = (1 − ρ 1 )e −2h and E[V ] = (1 − ρ 2 )e −E[AB] = 1 − E[U ] − E[V ] + E[U V ] = 1 − (1 − ρ 1 ) − (1 − ρ 2 ) + E[U V ] = ρ 1 + ρ 2 − 1 + E[U V ].
Observe that AB ≥ 0 almost surely, and so we have
E[U V ] ≥ 1 − ρ 1 − ρ 2 .
Substituting in our definitions of U and V gives us the result. Now consider the case where 0 ≤ h ≤ ρ 1 +ρ 2 6 . The same argument as before shows that
E[U V ] ≥ (2 − ρ 1 − ρ 2 )e −2h − 1 = (2 − ρ)e −2h − 1,
where we have made the substitution ρ = ρ 1 + ρ 2 . To prove the lemma, we will show that the above is greater than (1 − 2ρ)e −4h . This is equivalent to showing
(2 − ρ)e 2h − e 4h − 1 + 2ρ ≥ 0.
The left-hand side is decreasing for h ≥ 0. Moreover, we also have the inequality h ≤ ρ 6 ≤ 1 2 log(1 + ρ 2 ). Thus,
(2 − ρ)e 2h − e 4h − 1 + 2ρ ≥ (2 − ρ) 1 + ρ 2 − 1 + ρ 2 2 − 1 + 2ρ = ρ − ρ 2 2 ≥ 0.</p>
<p>C.8 Proof of Theorem 8</p>
<p>Let π be a prior distribution as in the theorem statement. Suppose we draw less than 1/2τ unlabeled examples, then with probability at least (1 − τ ) 1/2τ ≥ 1/2 none of these ρ-split π. Let us condition on this event. By induction on Lemma 10, we have that any collection of n ≤ 1/ρ of these points does not nρ-split π.</p>
<p>Suppose that we query n of these points (say x 1 , . . . , x n ), and receive responses y 1 , . . . , y n . Let π n denote this posterior. By Lemma 2, we have
E y 1:n Z 2 n avg-diam(π n ) ≥ (1 − nρ)avg-diam(π),
where Z n = E θ∼π P θ (y 1:n ; x 1:n ) ≤ 1. Thus,
E y 1:n avg-diam(π n ) ≥ (1 − nρ)avg-diam(π).
For a random variable U satisfying U ≤ c almost surely, the reverse Markov inequality gives us
Pr (U &gt; α) ≥ E[U ] − α c − α for any α ≤ E[X]
. Applying this to the random variable avg-diam(πn) avg-diam(π) and assuming n ≤ 1 2ρ , we have that avg-diam(π n ) ≥ with probability at least 1/3. Putting it all together gives us the theorem statement.</p>
<p>C.9 Proof of Theorem 9</p>
<p>We will need the following lemma.</p>
<p>Lemma 19. Let n ≤ 1/ρ, and let h ∈ R satisfy 0 ≤ h ≤. Suppose x 1 , . . . , x n all satisfy H(x i ) = h ≤ ρ/6n and have splitting index ≤ ρ. Then the combined query x 1:n has splitting index less than n 2 ρ.</p>
<p>Proof. We will show the claim for n a power of 2. Extending to other integers is straightforward.</p>
<p>The proof is by induction. Where we first observe that any subsequence i 1 , i 2 , . . . , i k ∈ {1, . . . , n} satisfies that
H(x i 1 , . . . , x i k ) = k j=1 H(x i j ) = kh ≤ ρk 6n ≤ ρ 6 .
Now for n = 1, then the claim trivially holds. For n ≥ 2, observe that by our inductive hypothesis, we have x 1:n/2 and xn/2 + 1 : n each have splitting index less than n 2 ρ 4 . Applying Lemma 10, completes the argument.</p>
<p>Turning to the proof of Theorem 9, let π be a prior distribution as in the theorem statement. Suppose we draw less than 1/2τ unlabeled examples, then with probability at least (1 − τ ) 1/2τ ≥ 1/2 none of these ρ-split π. Let us condition on this event. Now let n ≤ 1 2 √ ρ . Using the fact that n &lt; 1/ρ and h &lt; ρ 3/2 /6, we can apply Lemma 19, to see that any collection of n of these points does not n 2 ρ-split π. Lemma 2 then implies that E y 1 ,...,yn W 2 n avg-diam(π n ) ≥ (1 − n 2 ρ)avg-diam(π).
Recall W n = e n i=1 H(x i ) E θ∼π n i=1 P θ (y i ; x i )
. By our assumptions that H(x) ≤ ρ 3/2 /6, P θ (y; x) ≤ 1, n ≤ 1/ρ and ρ ≤ 1/4, we have W n ≤ 3/2 almost surely. Thus, E y 1:n avg-diam(π n ) ≥ 2 3</p>
<p>(1 − n 2 ρ)avg-diam(π).</p>
<p>For a random variable U satisfying U ≤ c almost surely, the reverse Markov inequality gives us  for any α ≤ E [X]. Applying this to the random variable U = avg-diam(πn) avg-diam(π) and threshold α = avg-diam(π) , we have
Pr (U &gt; α) ≥ E[U ] − α c − α0Pr avg-diam(π n ) &gt; ≥ 2 3 (1 − n 2 ρ)avg-diam(π) − avg-diam(π) − ≥ 8 3 (1 − n 2 ρ) − 4 − &gt; 1 3
where we have used the fact that n ≤ 1 2 √ ρ and f (x) = cx− x− is increasing in x when c ∈ (0, 1). Thus, avg-diam(π n ) ≥ with probability at least 1/3, finishing the argument.</p>
<p>D Additional details on the simulation study</p>
<p>Here we further expand on the details of our regression setup. We consider the following models.</p>
<p>• Linear regression with homoscedastic Gaussian noise:
P θ (y; x) = N (y | x, θ , σ 2 ),
where σ &gt; 0 is some known standard deviation. In our experiments it is set to 1/4.</p>
<p>• Logistic regression:
P θ (y; x) = Bernoulli y | µ , where µ = 1
1+e − x,θ . • Poisson regression: P θ (y; x) = Poisson y | e x,θ .</p>
<p>• Beta regression using the well-known mean parameterization (Ferrari and Cribari-Neto, 2004):
P θ (y; x) = Beta y | φµ, φ(1 − µ) , where µ = 1
1+e − x,θ and φ &gt; 0 is a fixed and known constant. We consider five objectives and their corresponding distances over parameters.</p>
<p>• What is the sign of the first coordinate? d first (θ, θ ) = 1I[sign(θ 1 ) = sign(θ 1 )].</p>
<p>• Which coordinate has largest absolute magnitude?
d max (θ, θ ) = 1I[argmax i |θ i | = argmax i |θ i |].
• Can we identify the parameter? d euclidean (θ, θ ) = θ − θ 2 .</p>
<p>• What is the order of the magnitudes of the coordinates?
d kendall (θ, θ ) = 1 2 1 − τ (|θ|, |θ |)
where τ (|θ|, |θ |) is the Kendall's τ correlation between of the pairs (|θ 1 |, |θ 1 |), . . . , (|θ d |, |θ d |).</p>
<p>• What is the influence of the first d/2 coordinates on the predicted sign?</p>
<p>d influence (θ, θ ) = Pr x sign x 1:d/2 , θ 1:d/2 = sign x 1:d/2 , θ 1:d/2 , where x 1:d/2 denotes x restricted to its first d/2 coordinates.</p>
<p>For each query, we first sampled 2K fresh data points from the distribution and then chose the next query from this pool of points. For methods requiring posterior samples, we collected 300 MCMC samples from the NUTS algorithm, using 2 parallel chains, a burnin of 750 steps, and a thinning factor of 5. We evaluated the model by drawing the same number of MCMC samples with the same parameters. Figure 3 depicts the results of all of our simulations. One notable observation is that VAR does very poorly on logistic regression tasks. The reason for this is that in our data generating process, it is possible to sample data points whose covariates are all zero. Such data points maximize posterior predictive variance but provide no information on the underlying regression coefficients. E Additional details on the drug discovery experiment Data preprocessing We downloaded the publicly available GDSC2-raw-data dataset from https:// www.cancerrxgene.org/downloads/bulk_download and pre-processed it using the R package gdscIC50 (Lightfoot et al., 2016). This preprocessing step transforms the raw cell counts of each experiment to cell viability (fraction of surviving cells) adjusting for low and high dimethyl sulfoxide (DMSO) controls. We then obtain the subset of experiments corresponding to the top M = 20 cell lines and L = 100 drugs that appear the most times in the dataset, at all 7 concentrations. Then, we transform the viability using the logistic transform by setting y ij d = logit(clip(viability ij d , 0.005, 0.995)). Since there are multiple observations for each cell line/drug/dose triplet, we aggregate them by averaging, yielding our final dataset.</p>
<p>Statistical model We fit a Bayesian factor model to the full dataset and use the predictions µ ij d as the reference for computing the progress of active learning. As more experiments are revealed, all active learning strategies converge to the same solution given by the full data model fit. The model has the form
y ij d ∼ Normal(µ ij d , σ 2 ) µ ij d = a + b i + c j d + w T i v j d .
The model is completed with standard Horseshoe priors (Carvalho et al., 2009) for regularization and an auto-regressive prior (Besag, 1974) to encourage smoothness along the dose-response curves b i ∼ Normal(0, λ 2 b i ) w i,r ∼ Normal(0, λ 2 w i,r ) (c j 1 , . . . , c j 7 ) ∼ AR(η) × Π 7 d=1 Normal(0, λ 2 c j d ) (w j 1,r , . . . , w j 7,r ) ∼ AR(η) × Π 7 d=1 Normal(0, λ 2 w j d ,r ) λ b i ∼ C + (0, 1) λ c j d ∼ C + (0, 1) λ w i,r ∼ C + (0, 1) λ v j d ,r ∼ C + (0, 1)</p>
<p>(1/σ 2 ) ∼ Exp (1) where x ∼ AR(η) for a vector x ∈ R d means p(x | η) ∝ exp(−(η/2) d s=2 (x s − x s−1 ) 2 ). We set η = 0.1 to add smoothness to the prior along the dose-response curve, but do not fine-tune this parameter. The embedding dimension is set to q = 4, which we find suffices to provide a good fit to the data. Figure 4 shows the fit to the data and examples of dose-response curves. Despite the low dimensionality of the embeddings, the model is able to capture over 75% of the variation in viability. Bayesian inference is conducted using a simple Gibbs sampler which can be derived analytically in closed conjugate form. To do so, we expand the half-Cauchy prior into a scale mixture to get updates that only involve normal and inverse-gamma distributions. Since the model is Gaussian, the PDBAL scores are evaluated using the formula in Proposition 1.</p>
<p>Experiments We study the potential of PDBAL to adaptively screen anti-cancer drugs in a retrospective experiment on GDSC. At each step, the algorithm is allowed to conduct a single trial of a drug tested against a cancer cell line. We consider coarse-and fine-grained settings for drug selection. In the coarse setting, the algorithm selects the drug and cell line and observes the responses at each of the seven doses; in the fine-grain setting, the algorithm additionally specifies a dose. Each setup corresponds to n = 2K and n = 14K possible experiments, respectively. Performance is evaluated as the error over each (cell line, drug, dose) when compared to what the underlying probabilistic model would learn from the full data. Each algorithm was tested over the same set of 7 random seeds. Each run had a warm start, where one observation for each cell line and drug was randomly selected. The Bayesian model was updated after each selection with 10 parallel chains, each with 200 burn-in Gibbs sampling cycles. Each chain collected 10 samples with a thinning factor of 10. To further accelerate the evaluation, the PDBAL, EIG, and variance scores to select the next experiment were also parallelized across 10 processes. For PDBAL, the coarse-grained experiments took approximately 12 hours of computation and the fine-grained experiment took 48 hours on a cluster equipped with Intel "Cascade Lake" CPUs. EIG scoring was slow due to numerical integration, making it infeasible to score the large number of possible experiments in the fined-grained setup. Therefore, it was only evaluated on the coarse-grained setup.</p>
<p>Proposition 3 .
3Fix σ 2 , d, B &gt; 0. Let Θ denote the class of Gaussian location models parameterized by a set of mean functions F ⊂ {θ : X → [−B, B]} such that P θ (y; x) = N (y | θ(x), σ 2 ). If the responses y lie in [−B, B] and the pseudo-dimension of F is bounded by d, then Θ has log-likelihood dimension (cB 2 /σ 2 , d) for some universal constant c &gt; 0.</p>
<p>Figure 1 :
1Synthetic regression simulations. Columns correspond to the different probabilistic models, and rows correspond to the different objectives.</p>
<p>Figure 2 :
2Results on GDSC. Left: Coarse-grained experiment; each observation contains the full doseresponse curve observations. Right: Fine-grained experiment; each observation consists of a single dose selected by the active learning strategy.</p>
<p>2h . Note that U and V lie in the interval [0, 1] almost surely. Let A = 1 − U and B = 1 − V . Let us first consider the case where h = 0. Then we have</p>
<p>Figure 3 :
3Full set of synthetic regression simulations.</p>
<p>Figure 4 :
4Bayesian factor model fit to the GDSC dataset. Left: examples of raw data (points) and fitted curves (lines). Right: scatter plot comparing fitted vs observed points.</p>
<p>Table 1 :
1Comparison of AUC for selective drug identification.Table 1presents the results for the coarse-grained drug selection experiments, showing the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. The results show that PDBAL can more accurately discover selective drugs, notably even after only having selected 5% of the experiments.AUC at 5% AUC at 10% </p>
<p>RANDOM </p>
<p>0.5 
0.60 </p>
<p>VAR </p>
<p>0.52 
0.62 </p>
<p>EIG </p>
<p>0.50 
0.67 </p>
<p>PDBAL </p>
<p>0.60 
0.71 </p>
<p>Code for our simulations can be found at: https://github.com/tansey-lab/pdbal.
AcknowledgementsWT is supported by grants from the Tow Center for Developing Oncology and the Break Through Cancer consortium.
Selective sampling algorithms for cost-sensitive multiclass prediction. A , Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningA. Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In Proceedings of the 30th International Conference on Machine Learning, pages 1220-1228, 2013.</p>
<p>Neural network learning: Theoretical foundations. M Anthony, P Bartlett, Cambridge University PressM. Anthony and P. Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 1999.</p>
<p>Gone fishing: Neural active learning with Fisher embeddings. J Ash, S Goel, A Krishnamurthy, S Kakade, Advances in Neural Information Processing Systems. 34J. Ash, S. Goel, A. Krishnamurthy, and S. Kakade. Gone fishing: Neural active learning with Fisher embeddings. Advances in Neural Information Processing Systems, 34:8927-8939, 2021.</p>
<p>Weighted sums of certain dependent random variables. K Azuma, Tohoku Mathematical Journal, Second Series. 193K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, Second Series, 19(3):357-367, 1967.</p>
<p>Spatial interaction and the statistical analysis of lattice systems. J Besag, Journal of the Royal Statistical Society: Series B. 362J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society: Series B, 36(2):192-225, 1974.</p>
<p>Concentration inequalities: A nonasymptotic theory of independence. S Boucheron, G Lugosi, P Massart, Oxford university pressS. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of indepen- dence. Oxford university press, 2013.</p>
<p>Handling sparsity via the horseshoe. C M Carvalho, N G Polson, J G Scott, International Conference on Artificial Intelligence and Statistics. C. M. Carvalho, N. G. Polson, and J. G. Scott. Handling sparsity via the horseshoe. In International Conference on Artificial Intelligence and Statistics, pages 73-80, 2009.</p>
<p>Convergence rates of active learning for maximum likelihood estimation. K Chaudhuri, S M Kakade, P Netrapalli, S Sanghavi, Advances in Neural Information Processing Systems. K. Chaudhuri, S. M. Kakade, P. Netrapalli, and S. Sanghavi. Convergence rates of active learning for maximum likelihood estimation. Advances in Neural Information Processing Systems, 2015.</p>
<p>Beta regression for modelling rates and proportions. S Ferrari, F Cribari-Neto, Journal of Applied Statistics. 317S. Ferrari and F. Cribari-Neto. Beta regression for modelling rates and proportions. Journal of Applied Statistics, 31(7):799-815, 2004.</p>
<p>Gaussed: A probabilistic programming language for sequential experimental design. M A Fisher, O Teymur, C Oates, arXiv:2110.08072arXiv preprintM. A. Fisher, O. Teymur, and C. Oates. Gaussed: A probabilistic programming language for sequential experimental design. arXiv preprint arXiv:2110.08072, 2021.</p>
<p>Selective sampling using the query by committee algorithm. Y Freund, H S Seung, E Shamir, N Tishby, Machine learning. 282Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine learning, 28(2):133-168, 1997.</p>
<p>Deep Bayesian active learning with image data. Y Gal, R Islam, Z Ghahramani, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningY. Gal, R. Islam, and Z. Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183-1192, 2017.</p>
<p>Systematic identification of genomic markers of drug sensitivity in cancer cells. M J Garnett, E J Edelman, S J Heidorn, C D Greenman, A Dastur, K W Lau, P Greninger, I R Thompson, X Luo, J Soares, Nature. 4837391M. J. Garnett, E. J. Edelman, S. J. Heidorn, C. D. Greenman, A. Dastur, K. W. Lau, P. Greninger, I. R. Thompson, X. Luo, J. Soares, et al. Systematic identification of genomic markers of drug sensitivity in cancer cells. Nature, 483(7391):570-575, 2012.</p>
<p>Entropy search for information-efficient global optimization. P Hennig, C J Schuler, Journal of Machine Learning Research. 136P. Hennig and C. J. Schuler. Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 13(6), 2012.</p>
<p>Predictive entropy search for Bayesian optimization with unknown constraints. J M Hernández-Lobato, M Gelbart, M Hoffman, R Adams, Z Ghahramani, Proceedings of the 32nd International conference on machine learning. the 32nd International conference on machine learningJ. M. Hernández-Lobato, M. Gelbart, M. Hoffman, R. Adams, and Z. Ghahramani. Predictive entropy search for Bayesian optimization with unknown constraints. In Proceedings of the 32nd International conference on machine learning, pages 1699-1707, 2015.</p>
<p>The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. M D Hoffman, A Gelman, Journal of Machine Learning Research. 151M. D. Hoffman and A. Gelman. The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1):1593-1623, 2014.</p>
<p>Bayesian active learning for classification and preference learning. N Houlsby, F Huszár, Z Ghahramani, M Lengyel, arXiv:1112.5745arXiv preprintN. Houlsby, F. Huszár, Z. Ghahramani, and M. Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.</p>
<p>Parallelised Bayesian optimisation via Thompson sampling. K Kandasamy, A Krishnamurthy, J Schneider, B Póczos, International Conference on Artificial Intelligence and Statistics. K. Kandasamy, A. Krishnamurthy, J. Schneider, and B. Póczos. Parallelised Bayesian optimisation via Thompson sampling. In International Conference on Artificial Intelligence and Statistics, pages 133-142, 2018.</p>
<p>Batchbald: Efficient and diverse batch acquisition for deep Bayesian active learning. A Kirsch, J Van Amersfoort, Y Gal, Advances in Neural Information Processing Systems. 32A. Kirsch, J. Van Amersfoort, and Y. Gal. Batchbald: Efficient and diverse batch acquisition for deep Bayesian active learning. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Fast sparse Gaussian process methods: The informative vector machine. N Lawrence, M Seeger, R Herbrich, Advances in Neural Information Processing Systems. 15N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informative vector machine. Advances in Neural Information Processing Systems, 15, 2002.</p>
<p>Pipeline for GDSC curve fitting. H Lightfoot, D Van Der Meer, D Vis, 50R package v0.99.4.H. Lightfoot, D. van der Meer, and D. Vis. gdscic50: Pipeline for GDSC curve fitting, 2016. R package v0.99.4.</p>
<p>On a measure of the information provided by an experiment. D V Lindley, The Annals of Mathematical Statistics. 274D. V. Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, 27(4):986-1005, 1956.</p>
<p>Information-based objective functions for active data selection. D J Mackay, Neural computation. 44D. J. MacKay. Information-based objective functions for active data selection. Neural computation, 4(4): 590-604, 1992.</p>
<p>Generalizing Bayesian optimization with decisiontheoretic entropies. W Neiswanger, L Yu, S Zhao, C Meng, S Ermon, arXiv:2210.01383arXiv preprintW. Neiswanger, L. Yu, S. Zhao, C. Meng, and S. Ermon. Generalizing Bayesian optimization with decision- theoretic entropies. arXiv preprint arXiv:2210.01383, 2022.</p>
<p>PyStan (3.5.0). PyPI. A Riddell, A Hartikainen, M Carter, A. Riddell, A. Hartikainen, and M. Carter. PyStan (3.5.0). PyPI, Jul 2021.</p>
<p>C Riis, F N Antunes, F B Hüttel, C L Azevedo, F C Pereira, arXiv:2205.10186Bayesian active learning with fully Bayesian Gaussian processes. arXiv preprintC. Riis, F. N. Antunes, F. B. Hüttel, C. L. Azevedo, and F. C. Pereira. Bayesian active learning with fully Bayesian Gaussian processes. arXiv preprint arXiv:2205.10186, 2022.</p>
<p>Active regression by stratification. S Sabato, R Munos, Advances in Neural Information Processing Systems. S. Sabato and R. Munos. Active regression by stratification. Advances in Neural Information Processing Systems, 2014.</p>
<p>Query by committee. H S Seung, M Opper, H Sompolinsky, Proceedings of the Fifth Annual Workshop on Computational Learning Theory. the Fifth Annual Workshop on Computational Learning TheoryH. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 287-294, 1992.</p>
<p>Stan modeling language user's guide and reference manual, version 2.30, 2022. Stan Development Team, Stan Development Team. Stan modeling language user's guide and reference manual, version 2.30, 2022. URL https://mc-stan.org.</p>
<p>Dose-response modeling in high-throughput cancer drug screenings: An end-to-end approach. W Tansey, K Li, H Zhang, S W Linderman, R Rabadan, D M Blei, C H Wiggins, Biostatistics. 232W. Tansey, K. Li, H. Zhang, S. W. Linderman, R. Rabadan, D. M. Blei, and C. H. Wiggins. Dose-response modeling in high-throughput cancer drug screenings: An end-to-end approach. Biostatistics, 23(2): 643-665, 2022a.</p>
<p>A Bayesian model of dose-response for cancer drug studies. W Tansey, C Tosh, D M Blei, The Annals of Applied Statistics. 162W. Tansey, C. Tosh, and D. M. Blei. A Bayesian model of dose-response for cancer drug studies. The Annals of Applied Statistics, 16(2):680-705, 2022b.</p>
<p>Active learning for parameter estimation in Bayesian networks. S Tong, D Koller, Advances in Neural Information Processing Systems. 13S. Tong and D. Koller. Active learning for parameter estimation in Bayesian networks. Advances in Neural Information Processing Systems, 13, 2000.</p>
<p>Diameter-based active learning. C Tosh, S Dasgupta, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningC. Tosh and S. Dasgupta. Diameter-based active learning. In Proceedings of the 34th International Conference on Machine Learning, pages 3444-3452, 2017.</p>
<p>Diameter-based interactive structure discovery. C Tosh, D Hsu, International Conference on Artificial Intelligence and Statistics. C. Tosh and D. Hsu. Diameter-based interactive structure discovery. In International Conference on Artificial Intelligence and Statistics, pages 580-590, 2020.</p>
<p>Genomics of drug sensitivity in cancer (GDSC): A resource for therapeutic biomarker discovery in cancer cells. W Yang, J Soares, P Greninger, E J Edelman, H Lightfoot, S Forbes, N Bindal, D Beare, J A Smith, I R Thompson, Nucleic acids research. 41D1W. Yang, J. Soares, P. Greninger, E. J. Edelman, H. Lightfoot, S. Forbes, N. Bindal, D. Beare, J. A. Smith, I. R. Thompson, et al. Genomics of drug sensitivity in cancer (GDSC): A resource for therapeutic biomarker discovery in cancer cells. Nucleic acids research, 41(D1):D955-D961, 2012.</p>            </div>
        </div>

    </div>
</body>
</html>
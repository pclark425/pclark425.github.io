<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1417 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1417</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1417</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-04b6cb5f2bf9f9d0280fd32e4bd8d063b4c1dddf</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04b6cb5f2bf9f9d0280fd32e4bd8d063b4c1dddf" target="_blank">Temporal Predictive Coding For Model-Based Planning In Latent Space</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work presents an information-theoretic approach that employs temporal predictive coding to encode elements in the environment that can be predicted across time that is superior to existing methods in the challenging complex-background setting while remaining competitive with current state-of-the-art models in the standard setting.</p>
                <p><strong>Paper Abstract:</strong> High-dimensional observations are a major challenge in the application of model-based reinforcement learning (MBRL) to real-world environments. To handle high-dimensional sensory inputs, existing approaches use representation learning to map high-dimensional observations into a lower-dimensional latent space that is more amenable to dynamics estimation and planning. In this work, we present an information-theoretic approach that employs temporal predictive coding to encode elements in the environment that can be predicted across time. Since this approach focuses on encoding temporally-predictable information, we implicitly prioritize the encoding of task-relevant components over nuisance information within the environment that are provably task-irrelevant. By learning this representation in conjunction with a recurrent state space model, we can then perform planning in latent space. We evaluate our model on a challenging modification of standard DMControl tasks where the background is replaced with natural videos that contain complex but irrelevant information to the planning task. Our experiments show that our model is superior to existing methods in the challenging complex-background setting while remaining competitive with current state-of-the-art models in the standard setting.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1417.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1417.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Predictive Coding (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent recurrent world model that learns representations by maximizing mutual information between past latent codes+actions and future latent codes (temporal predictive coding), combined with a consistency objective, static predictive coding to avoid collapse, reward prediction, and dynamics smoothing for planning via latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Temporal Predictive Coding (TPC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model built on an RSSM (deterministic RNN h_t summarizing past, stochastic latent state s_t with p(s_t|h_t)); encoder E deterministically maps image observations o_t to s_t. Training uses a temporal contrastive predictive coding (CPC) objective that maximizes a lower bound on I(E(O_t); E(O_{<t}), A_{<t}) via a contrastive loss whose critic is tied to the latent dynamics F, plus a consistency log-likelihood objective ln F(E(o_t)|E(o_{<t}),a_{<t}), a static predictive coding term (fixed-variance Gaussian critic) to avoid collapse, and reward prediction ln R(r_t|E(o_t)). Dynamics smoothing is applied by adding Gaussian noise to latent inputs during training to make rollouts robust.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent stochastic-deterministic RSSM with contrastive training)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control from pixels (DeepMind Control Suite tasks), including versions with natural-video backgrounds and random/background distractors</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mutual-information lower bound (CPC objective) and consistency log-likelihood of latent dynamics; downstream evaluation by environment return (task return). Auxiliary reconstruction quality (via separately-trained decoders) is used qualitatively to inspect what the encoder preserves.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric next-step MSE or pixel-wise fidelity metrics reported; fidelity is evaluated indirectly — TPC achieves task returns competitive with Dreamer in standard DMC tasks and outperforms/is more robust than reconstruction-based Dreamer and other contrastive baselines in natural and random background settings; auxiliary-decoder reconstructions show high-fidelity agent reconstruction and suppressed background.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable via auxiliary decoders: the learned representation reconstructs task-relevant agent components and discards large parts of background; latent space structure beyond that (e.g., axis-aligned semantics) is not claimed or quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization via auxiliary decoders reconstructing observations from learned latent codes; qualitative analysis of what is reconstructed (agent vs background); ablation studies to diagnose collapse and behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Architecture hyperparameters reported: latent state dim = 30, recurrent state dim = 200; training uses batch size 250 (larger than some baselines), 100 gradient updates per 1000 env steps; no wall-clock runtimes, GPU counts, FLOPs, or parameter counts explicitly reported. Uses no separate critic network (critic tied to dynamics) to save parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Design ties critic to dynamics for parameter efficiency and avoids an explicit pixel decoder (reducing decoding cost) compared to reconstruction-based methods; empirically learns faster/more sample-efficient than CVRL in experiments reported (qualitative / plotted returns), and is more robust to distractors than Dreamer which uses reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Competitive with Dreamer on standard DMC tasks (matches or close in most tasks); in natural-video-background setting, Dreamer fails while TPC performs reasonably and outperforms CVRL and DBC on multiple tasks (quantitatively: TPC outperforms baselines on several of 6 tasks; supplementary results show TPC outperforms CVRL on 4/6 after 2e6 steps). TPC uniquely succeeds in random-background tasks where background is randomized per step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Temporal predictive coding biases the world model to encode temporally-predictive features which, under capacity constraints, favors task-relevant components over provably task-irrelevant (temporally-unpredictive) information; this leads to improved policy learning in environments with distracting backgrounds. Joint reward learning and dynamics smoothing are crucial to good task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>TPC discards temporally-unpredictive information (benefit under distractors) but may still encode temporally-predictive nuisance content (limitation). To avoid collapse, static predictive coding must be added, incurring additional objective terms and tuning. Dynamics smoothing improves rollout robustness but requires adding noise during training (a tradeoff between training cleanliness and rollout robustness). Hyperparameter choice (e.g., weight of consistency term lambda2) trades off representation richness vs collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Deterministic encoder E; RSSM with deterministic RNN + stochastic latent predicted as Gaussian; tie critic f to dynamics F for CPC; sum CPC over all t (full-trajectory temporal CPC); add consistency log-likelihood objective; add static predictive coding with fixed variance to encourage expansion; dynamics-associated Gaussian noise injected during training to smooth rollouts; joint reward predictor R trained from s_t; hyperparameters: lambda1=1, lambda2=0.1, lambda3=1, lambda4=1 as default; latent dim 30, rec dim 200, noise std 0.2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to reconstruction-based Dreamer/PlaNet: TPC avoids pixel reconstruction, thereby not forced to encode background pixels and yielding robustness to distractors where Dreamer fails. Compared to static-contrastive CVRL/CURL: temporal CPC (TPC) encourages encoding temporally-predictive features and empirically outperforms static CPC baselines on many tasks. Compared to DBC (bisimulation-style): DBC aims for bisimulation invariance theoretically stronger but practical implementations used heuristics; DBC failed in the paper's multi-distractor experiments while TPC succeeded on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends combining temporal CPC with a consistency objective and a static CPC term to prevent collapse, using dynamics smoothing during training, and joint reward learning especially in background-distractor settings; specific hyperparameters used in experiments are given (latent dim 30, rec dim 200, batch size 250, lambda2 tuned around 0.1 to avoid collapse, fixed noise sigma=0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1417.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (latent imagination via RSSM and reconstruction objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-based recurrent latent world model (RSSM) trained with pixel reconstruction objectives and used for latent imagination and policy learning via backpropagation through dynamics; served as the primary baseline/inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (RSSM with reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent State Space Model (RSSM) with deterministic RNN state and stochastic latent, trained with observation reconstruction (pixel decoder) together with dynamics and reward models; policies are learned by imaginative rollouts through the learned latent dynamics and backpropagation of value/reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (reconstruction-based RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control from pixels (DeepMind Control Suite)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (pixel likelihood / ELBO-style objectives) and downstream task return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported state-of-the-art on many standard RL tasks in prior work; in this paper Dreamer performs well in standard DMC but fails in natural-video background settings where reconstruction forces encoding of distractor pixels (no numeric pixel-fidelity or MSE reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural latent representation; auxiliary-decoder reconstructions indicate Dreamer encodes both agent and background (i.e., reconstructs most of image), showing that the representation is not selectively task-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Auxiliary reconstructions from learned latents (trained post-hoc) to visualize encoded content.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Uses pixel decoder increasing compute and parameters relative to contrastive methods; hyperparameters in experiments: latent dim 30, rec dim 200, batch size 50, 100 gradient updates per 1000 env steps. No explicit wall-clock or GPU counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reconstruction objective requires decoder and thus extra compute; in distractor settings this leads to poor sample efficiency for task performance because the model spends capacity on nuisance pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong on standard DMC tasks (competitive/SOTA historically), but fails to learn meaningful behaviors in natural-video background experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity pixel reconstruction does not guarantee encoding of task-relevant features; Dreamer’s fidelity to the full observation hurts task utility in presence of distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High reconstruction fidelity vs robustness: reconstructing all pixels yields robustness loss under distractors. Requires decoder cost and can be sample-inefficient when observations have high-dimensional nuisance components.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Train RSSM with reconstruction/pixel decoder; use latent imagination/backpropagation for policy learning; clip KL terms in training (as used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>TPC avoids reconstruction and is more robust to distractors; static contrastive methods previously performed worse than Dreamer in earlier studies, but TPC's temporal CPC closes that gap.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not prescribed in this paper beyond the baseline hyperparameters; authors of this paper note Dreamer’s reconstruction objective is the key cause of failure in distractor-rich environments and thus suggest alternatives (e.g., TPC).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1417.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent dynamics architecture combining a deterministic RNN state and a stochastic latent state distribution p(s_t|h_t), used to model environment dynamics for latent imagination and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent State Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic RNN computes h_t = RNN(h_{t-1}, s_{t-1}, a_{t-1}) summarizing past; stochastic latent s_t is modeled conditionally p(s_t|h_t) (Gaussian in practice). Used as the latent dynamics backbone for Dreamer, PlaNet, and TPC.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent dynamics model / recurrent latent world model component</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL for continuous control from pixels (DMC)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Log-likelihood of predicted latent codes (consistency objective) and usefulness for downstream returns; not reported as explicit numeric predictive error in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No numeric predictive-error reported here; when trained with appropriate objectives (reconstruction or TPC+consistency), RSSM enables competitive task returns.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Architecture-level interpretability (clear separation of deterministic and stochastic components), but latent contents remain opaque unless probed (e.g., auxiliary decoders).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Auxiliary reconstruction and visualization of imagined rollouts used to inspect latent contents.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Recurrent neural architecture; specific dims used: recurrent 200, latent 30. Computational costs depend on objective (reconstruction adds decoder cost).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Tying critic to dynamics and using CPC reduces the need for separate critic networks; using RSSM enables multi-step imagination for policy learning which can be sample efficient compared to model-free methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Provides the backbone enabling Dreamer and TPC to achieve good performance on DMC tasks when trained with appropriate objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM's predictive capacity is central to successful latent planning; must be trained with objectives (e.g., consistency, smoothing) to be robust to rollout error.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Deterministic RNN plus stochastic latents trade expressive dynamics modeling with potential for cascading error during rollouts, mitigated by dynamics smoothing.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Deterministic RNN summarization + Gaussian stochastic latent; dynamics smoothing by injecting noise into latent inputs during training to make rollouts robust.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>RSSM is the de facto architecture used by PlaNet, Dreamer, and TPC; alternative modeling choices exist (purely deterministic world models or explicit simulators) but RSSM balances expressivity and tractable imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper uses latent dim 30 and recurrent state dim 200; recommends dynamics smoothing and consistency objective when used with contrastive temporal objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1417.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PC3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PC3 (Predictive Coding for Locally-Linear Control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic approach that uses temporal contrastive predictive coding (CPC) between consecutive latent states to learn locally-linear latent spaces amenable to control; used as prior theoretical inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predictive coding for locally-linear control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PC3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses temporal CPC between latent states of consecutive time steps to learn a latent space suitable for locally-linear control; relies on CPC objectives and locally-linear dynamics assumptions for control analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / representation learning approach (temporal CPC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Control and planning in latent spaces (theoretical and offline control settings)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mutual information lower bounds (CPC objective) and suitability for locally-linear control (theoretical guarantees); no numeric fidelity metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>PC3 provides theoretical motivation showing TPC is no worse at future-observation prediction than certain latent variable models; PC3 itself is not directly applied to online RL in this paper (requires teleportation and goal depiction).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Focuses on learning representations consistent with locally-linear control; interpretability is theoretical (predictive suboptimality arguments) rather than empirical visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Theoretical analysis and mutual information arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in detail here; PC3 required constructs not easily applicable to standard RL settings (e.g., teleportation).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PC3 is more of a theoretical baseline; not directly compared empirically in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not directly used for RL experiments in this paper; served as conceptual/theoretical inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows that temporal predictive coding can be sufficient for optimal decision making in locally-linear control settings and motivates applying TPC ideas to online RL with RSSM.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>PC3's constraints (goal depiction, teleportation) limit practicality despite theoretical benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Temporal CPC between latent states at consecutive time steps; analysis of predictive suboptimality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with static CPC and reconstruction-based latent variable models; argued to be no worse for future prediction and to prioritize temporally-predictive information.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified for RL use; motivates embedding temporal CPC into practical RSSM frameworks (as done in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1417.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CVRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Variational Model-based Reinforcement Learning (CVRL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent contrastive-based world-model approach that maximizes mutual information between current observation and its latent state (static predictive coding) while learning latent dynamics for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Contrastive variational model-based reinforcement learning for complex observations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CVRL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive objective that maximizes mutual information between current observation o_t and latent s_t (static predictive coding), combined with a latent dynamics model for planning; uses a bilinear critic f(s_t, o_t) = exp(z_t^T W s_t) where z_t is an embedding of o_t.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (static contrastive + dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL from pixels under complex observations/distractors</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mutual information lower bound via contrastive loss; downstream task return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the paper's experiments CVRL underperforms Dreamer on several standard tasks; in natural-background setting it is competitive to TPC on some tasks (Walker Run, Cheetah Run) but generally TPC outperforms CVRL in many cases (supplement shows TPC outperforms CVRL on 4/6 after 2e6 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>No explicit interpretability claims; static CPC tends to encourage encoding as much information in the observation as possible (including nuisance/background).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not emphasized in this paper; uses contrastive critic design (bilinear) but no auxiliary reconstruction analyses reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Hyperparameters: latent dim 30, rec dim 200, batch size 50; uses a bilinear critic which is an extra parameter matrix W; similar computational footprint to Dreamer except lacks heavy decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to TPC, CVRL uses static mutual information so can encode more nuisance info; empirically learns slower/achieves lower returns on several tasks in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Competitive on some tasks in natural-background setting but outperformed by TPC on several tasks; underperforms Dreamer on several standard tasks per reported plots.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Static predictive coding encourages encoding of current-observation information indiscriminately, which can reduce task utility under heavy distractors because nuisance information consumes representation capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Static CPC is simpler but favors full-observation encoding; temporal CPC (TPC) trades off full-observation fidelity for temporally-predictive/utility-focused encoding which helps in distractor settings.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Bilinear critic for contrastive loss; static mutual information between o_t and s_t; uses RSSM for dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>TPC's temporal CPC yields better robustness to distractors compared to CVRL's static CPC in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1417.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Bisimulation for Control (DBC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free representation learning approach that attempts to learn representations that preserve a bisimulation metric, with the goal of removing task-irrelevant variation from observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning invariant representations for reinforcement learning without reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DBC (bisimulation-style representation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns latent representations by encouraging preservation of a bisimulation metric between states; implemented with heuristics (e.g., replacing the ground metric d with Euclidean distance when comparing next-state distributions) rather than solving the exact fixed-point equation for bisimulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>model-free invariant representation learning (bisimulation-inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning from pixels, intended to be robust to distractors</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Preservation of a bisimulation-like metric (implemented heuristically) and downstream task return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the paper's experiments DBC failed to work on all six natural-background tasks (contrasting with its original reported performance where a single video was used); suggests poor generalization to multiple unseen distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Aim is to learn invariant latent metrics; no auxiliary interpretability analyses in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed here; relies on bisimulation-inspired losses rather than explicit visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model-free approach with additional distance computations between next-state distributions; hyperparameters reported in supplement (learning rates, buffer sizes etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Fails empirically in the paper's multi-distractor setting, suggesting less effective use of representation capacity under diverse distractors compared to TPC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Failed to learn in the natural-background experiments of this paper; underperformed compared to TPC and baselines in those settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Although bisimulation is theoretically suited to filter task-irrelevant info (including temporally-predictive nuisances), the practical implementation used here does not solve the true bisimulation fixed point and performs poorly under varied distractors in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Theoretically strong but practically sensitive to approximation choices; empirical robustness to diverse unseen distractors is lacking in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Bisimulation-inspired loss, heuristic approximations (Euclidean distance substitution), model-free RL pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>TPC (temporal CPC + latent dynamics) outperforms DBC in the multi-distractor/generalization experiments presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1417.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CURL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CURL (Contrastive Unsupervised Representations for RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A static contrastive auxiliary-representation method used with model-free RL; included in supplementary comparisons against TPC in natural-background settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>curl: Contrastive unsupervised representations for reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CURL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses contrastive learning (static CPC between observation and augmented views / latent embeddings) as an auxiliary objective to aid model-free RL, producing better visual representations for downstream policies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>static contrastive representation learning (auxiliary for model-free RL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-free RL from pixels, tested in DMC natural-background setting</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive loss (InfoNCE) and downstream task return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In supplementary experiments TPC outperforms CURL on 3 of 6 natural-background tasks and CURL outperforms TPC on Walker Run; both fail on Hopper Hop and Cup Catch in the reported runs.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>No interpretability analyses reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed here; typical CURL uses an image encoder and contrastive training with data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>As a static contrastive baseline, CURL does not exploit temporal predictive structure, which can limit robustness to temporal distractors compared to TPC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mixed vs TPC; TPC outperforms on several tasks but CURL is better on at least one reported task.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Static contrastive objectives help representation learning but do not specifically prefer temporally-predictive, task-relevant features, limiting robustness to dynamic distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Auxiliary static contrastive learning helps in some settings but lacks temporal inductive bias that TPC uses.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Static contrastive InfoNCE loss between observation embeddings and latent codes / augmentations; no temporal CPC bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1417.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (learning latent dynamics for planning from pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-based latent dynamics model (RSSM) trained for planning in latent space using cross-entropy method; referenced as prior work on latent models from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM-style latent dynamics trained with observation reconstruction and used for planning with trajectory optimization (CEM) in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (reconstruction-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Planning from pixels (control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss and planning returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Prior work reports strong performance on many control tasks; not experimentally evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Similar limitations as other reconstruction-based models (encodes full observations including nuisances).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>N/A in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reconstruction decoder and RSSM costs; specifics not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reconstruction-based approach can be vulnerable to distractors compared to TPC's temporal-contrastive training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Reconstruction objective can cause encoding of task-irrelevant pixels, harming utility under distracting backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel fidelity vs robustness to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>RSSM + pixel decoder + planning via CEM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1417.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage recurrent latent world model approach that learns a recurrent latent dynamics model then evolves controllers in imagination; cited as early influential work on latent world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent world models facilitate policy evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage pipeline learning a recurrent latent dynamics model and subsequently training/evolving controllers in the learned latent space (original experiments on Atari / car racing / vizdoom-like settings).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Games and simulated control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction/prediction losses and downstream controller performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Demonstrated feasibility of latent world-model-based controllers; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not emphasized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>N/A here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Two-stage training but details not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Influential prior work motivating latent world models; later methods added joint training, RSSM refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows value of learning dynamics for imagined control.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Two-stage design vs joint training choices in later work.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>RNN-based latent dynamics, evolutionary controllers in imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1417.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1417.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOLAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SOLAR (Structured representations for model-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent model that models dynamics as time-varying linear-Gaussian with quadratic costs and performs control using guided policy search; cited as a reconstruction-based world-model variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SOLAR: Deep structured representations for model-based reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SOLAR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns structured latent representations where dynamics are modeled as time-varying linear-Gaussian and costs are quadratic, enabling use of guided policy search/control-theoretic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent structured world model (linear-Gaussian dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL and control from images</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Model likelihood / reconstruction and downstream control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Cited as prior method; not empirically compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Structured linear-Gaussian components increase interpretability relative to black-box dynamics, but specifics not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Model structure (linear-Gaussian) aids interpretability; not probed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Structured dynamics can be sample-efficient when assumptions hold; not directly compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Structured assumptions facilitate control with classical solvers but may limit generality.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Structure vs flexibility tradeoff noted in related-work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Time-varying linear-Gaussian dynamics and quadratic costs; guided policy search integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Predictive Coding For Model-Based Planning In Latent Space', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination. <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels. <em>(Rating: 2)</em></li>
                <li>Predictive coding for locally-linear control <em>(Rating: 2)</em></li>
                <li>Contrastive variational model-based reinforcement learning for complex observations. <em>(Rating: 2)</em></li>
                <li>Learning invariant representations for reinforcement learning without reconstruction. <em>(Rating: 2)</em></li>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution. <em>(Rating: 1)</em></li>
                <li>SOLAR: Deep structured representations for model-based reinforcement learning. <em>(Rating: 1)</em></li>
                <li>curl: Contrastive unsupervised representations for reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1417",
    "paper_id": "paper-04b6cb5f2bf9f9d0280fd32e4bd8d063b4c1dddf",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "TPC",
            "name_full": "Temporal Predictive Coding (this paper)",
            "brief_description": "A latent recurrent world model that learns representations by maximizing mutual information between past latent codes+actions and future latent codes (temporal predictive coding), combined with a consistency objective, static predictive coding to avoid collapse, reward prediction, and dynamics smoothing for planning via latent imagination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Temporal Predictive Coding (TPC)",
            "model_description": "Latent world model built on an RSSM (deterministic RNN h_t summarizing past, stochastic latent state s_t with p(s_t|h_t)); encoder E deterministically maps image observations o_t to s_t. Training uses a temporal contrastive predictive coding (CPC) objective that maximizes a lower bound on I(E(O_t); E(O_{&lt;t}), A_{&lt;t}) via a contrastive loss whose critic is tied to the latent dynamics F, plus a consistency log-likelihood objective ln F(E(o_t)|E(o_{&lt;t}),a_{&lt;t}), a static predictive coding term (fixed-variance Gaussian critic) to avoid collapse, and reward prediction ln R(r_t|E(o_t)). Dynamics smoothing is applied by adding Gaussian noise to latent inputs during training to make rollouts robust.",
            "model_type": "latent world model (recurrent stochastic-deterministic RSSM with contrastive training)",
            "task_domain": "Continuous control from pixels (DeepMind Control Suite tasks), including versions with natural-video backgrounds and random/background distractors",
            "fidelity_metric": "Mutual-information lower bound (CPC objective) and consistency log-likelihood of latent dynamics; downstream evaluation by environment return (task return). Auxiliary reconstruction quality (via separately-trained decoders) is used qualitatively to inspect what the encoder preserves.",
            "fidelity_performance": "No explicit numeric next-step MSE or pixel-wise fidelity metrics reported; fidelity is evaluated indirectly — TPC achieves task returns competitive with Dreamer in standard DMC tasks and outperforms/is more robust than reconstruction-based Dreamer and other contrastive baselines in natural and random background settings; auxiliary-decoder reconstructions show high-fidelity agent reconstruction and suppressed background.",
            "interpretability_assessment": "Partially interpretable via auxiliary decoders: the learned representation reconstructs task-relevant agent components and discards large parts of background; latent space structure beyond that (e.g., axis-aligned semantics) is not claimed or quantified.",
            "interpretability_method": "Visualization via auxiliary decoders reconstructing observations from learned latent codes; qualitative analysis of what is reconstructed (agent vs background); ablation studies to diagnose collapse and behavior.",
            "computational_cost": "Architecture hyperparameters reported: latent state dim = 30, recurrent state dim = 200; training uses batch size 250 (larger than some baselines), 100 gradient updates per 1000 env steps; no wall-clock runtimes, GPU counts, FLOPs, or parameter counts explicitly reported. Uses no separate critic network (critic tied to dynamics) to save parameters.",
            "efficiency_comparison": "Design ties critic to dynamics for parameter efficiency and avoids an explicit pixel decoder (reducing decoding cost) compared to reconstruction-based methods; empirically learns faster/more sample-efficient than CVRL in experiments reported (qualitative / plotted returns), and is more robust to distractors than Dreamer which uses reconstruction.",
            "task_performance": "Competitive with Dreamer on standard DMC tasks (matches or close in most tasks); in natural-video-background setting, Dreamer fails while TPC performs reasonably and outperforms CVRL and DBC on multiple tasks (quantitatively: TPC outperforms baselines on several of 6 tasks; supplementary results show TPC outperforms CVRL on 4/6 after 2e6 steps). TPC uniquely succeeds in random-background tasks where background is randomized per step.",
            "task_utility_analysis": "Temporal predictive coding biases the world model to encode temporally-predictive features which, under capacity constraints, favors task-relevant components over provably task-irrelevant (temporally-unpredictive) information; this leads to improved policy learning in environments with distracting backgrounds. Joint reward learning and dynamics smoothing are crucial to good task performance.",
            "tradeoffs_observed": "TPC discards temporally-unpredictive information (benefit under distractors) but may still encode temporally-predictive nuisance content (limitation). To avoid collapse, static predictive coding must be added, incurring additional objective terms and tuning. Dynamics smoothing improves rollout robustness but requires adding noise during training (a tradeoff between training cleanliness and rollout robustness). Hyperparameter choice (e.g., weight of consistency term lambda2) trades off representation richness vs collapse.",
            "design_choices": "Deterministic encoder E; RSSM with deterministic RNN + stochastic latent predicted as Gaussian; tie critic f to dynamics F for CPC; sum CPC over all t (full-trajectory temporal CPC); add consistency log-likelihood objective; add static predictive coding with fixed variance to encourage expansion; dynamics-associated Gaussian noise injected during training to smooth rollouts; joint reward predictor R trained from s_t; hyperparameters: lambda1=1, lambda2=0.1, lambda3=1, lambda4=1 as default; latent dim 30, rec dim 200, noise std 0.2.",
            "comparison_to_alternatives": "Compared to reconstruction-based Dreamer/PlaNet: TPC avoids pixel reconstruction, thereby not forced to encode background pixels and yielding robustness to distractors where Dreamer fails. Compared to static-contrastive CVRL/CURL: temporal CPC (TPC) encourages encoding temporally-predictive features and empirically outperforms static CPC baselines on many tasks. Compared to DBC (bisimulation-style): DBC aims for bisimulation invariance theoretically stronger but practical implementations used heuristics; DBC failed in the paper's multi-distractor experiments while TPC succeeded on several tasks.",
            "optimal_configuration": "Paper recommends combining temporal CPC with a consistency objective and a static CPC term to prevent collapse, using dynamics smoothing during training, and joint reward learning especially in background-distractor settings; specific hyperparameters used in experiments are given (latent dim 30, rec dim 200, batch size 250, lambda2 tuned around 0.1 to avoid collapse, fixed noise sigma=0.2).",
            "uuid": "e1417.0",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dreamer (latent imagination via RSSM and reconstruction objectives)",
            "brief_description": "A reconstruction-based recurrent latent world model (RSSM) trained with pixel reconstruction objectives and used for latent imagination and policy learning via backpropagation through dynamics; served as the primary baseline/inspiration.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination.",
            "mention_or_use": "use",
            "model_name": "Dreamer (RSSM with reconstruction)",
            "model_description": "Recurrent State Space Model (RSSM) with deterministic RNN state and stochastic latent, trained with observation reconstruction (pixel decoder) together with dynamics and reward models; policies are learned by imaginative rollouts through the learned latent dynamics and backpropagation of value/reward signals.",
            "model_type": "latent world model (reconstruction-based RSSM)",
            "task_domain": "Continuous control from pixels (DeepMind Control Suite)",
            "fidelity_metric": "Reconstruction loss (pixel likelihood / ELBO-style objectives) and downstream task return.",
            "fidelity_performance": "Reported state-of-the-art on many standard RL tasks in prior work; in this paper Dreamer performs well in standard DMC but fails in natural-video background settings where reconstruction forces encoding of distractor pixels (no numeric pixel-fidelity or MSE reported here).",
            "interpretability_assessment": "Black-box neural latent representation; auxiliary-decoder reconstructions indicate Dreamer encodes both agent and background (i.e., reconstructs most of image), showing that the representation is not selectively task-focused.",
            "interpretability_method": "Auxiliary reconstructions from learned latents (trained post-hoc) to visualize encoded content.",
            "computational_cost": "Uses pixel decoder increasing compute and parameters relative to contrastive methods; hyperparameters in experiments: latent dim 30, rec dim 200, batch size 50, 100 gradient updates per 1000 env steps. No explicit wall-clock or GPU counts reported.",
            "efficiency_comparison": "Reconstruction objective requires decoder and thus extra compute; in distractor settings this leads to poor sample efficiency for task performance because the model spends capacity on nuisance pixels.",
            "task_performance": "Strong on standard DMC tasks (competitive/SOTA historically), but fails to learn meaningful behaviors in natural-video background experiments reported in this paper.",
            "task_utility_analysis": "High-fidelity pixel reconstruction does not guarantee encoding of task-relevant features; Dreamer’s fidelity to the full observation hurts task utility in presence of distractors.",
            "tradeoffs_observed": "High reconstruction fidelity vs robustness: reconstructing all pixels yields robustness loss under distractors. Requires decoder cost and can be sample-inefficient when observations have high-dimensional nuisance components.",
            "design_choices": "Train RSSM with reconstruction/pixel decoder; use latent imagination/backpropagation for policy learning; clip KL terms in training (as used in experiments).",
            "comparison_to_alternatives": "TPC avoids reconstruction and is more robust to distractors; static contrastive methods previously performed worse than Dreamer in earlier studies, but TPC's temporal CPC closes that gap.",
            "optimal_configuration": "Not prescribed in this paper beyond the baseline hyperparameters; authors of this paper note Dreamer’s reconstruction objective is the key cause of failure in distractor-rich environments and thus suggest alternatives (e.g., TPC).",
            "uuid": "e1417.1",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State Space Model (RSSM)",
            "brief_description": "A latent dynamics architecture combining a deterministic RNN state and a stochastic latent state distribution p(s_t|h_t), used to model environment dynamics for latent imagination and planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Recurrent State Space Model (RSSM)",
            "model_description": "Deterministic RNN computes h_t = RNN(h_{t-1}, s_{t-1}, a_{t-1}) summarizing past; stochastic latent s_t is modeled conditionally p(s_t|h_t) (Gaussian in practice). Used as the latent dynamics backbone for Dreamer, PlaNet, and TPC.",
            "model_type": "latent dynamics model / recurrent latent world model component",
            "task_domain": "Model-based RL for continuous control from pixels (DMC)",
            "fidelity_metric": "Log-likelihood of predicted latent codes (consistency objective) and usefulness for downstream returns; not reported as explicit numeric predictive error in this paper.",
            "fidelity_performance": "No numeric predictive-error reported here; when trained with appropriate objectives (reconstruction or TPC+consistency), RSSM enables competitive task returns.",
            "interpretability_assessment": "Architecture-level interpretability (clear separation of deterministic and stochastic components), but latent contents remain opaque unless probed (e.g., auxiliary decoders).",
            "interpretability_method": "Auxiliary reconstruction and visualization of imagined rollouts used to inspect latent contents.",
            "computational_cost": "Recurrent neural architecture; specific dims used: recurrent 200, latent 30. Computational costs depend on objective (reconstruction adds decoder cost).",
            "efficiency_comparison": "Tying critic to dynamics and using CPC reduces the need for separate critic networks; using RSSM enables multi-step imagination for policy learning which can be sample efficient compared to model-free methods.",
            "task_performance": "Provides the backbone enabling Dreamer and TPC to achieve good performance on DMC tasks when trained with appropriate objectives.",
            "task_utility_analysis": "RSSM's predictive capacity is central to successful latent planning; must be trained with objectives (e.g., consistency, smoothing) to be robust to rollout error.",
            "tradeoffs_observed": "Deterministic RNN plus stochastic latents trade expressive dynamics modeling with potential for cascading error during rollouts, mitigated by dynamics smoothing.",
            "design_choices": "Deterministic RNN summarization + Gaussian stochastic latent; dynamics smoothing by injecting noise into latent inputs during training to make rollouts robust.",
            "comparison_to_alternatives": "RSSM is the de facto architecture used by PlaNet, Dreamer, and TPC; alternative modeling choices exist (purely deterministic world models or explicit simulators) but RSSM balances expressivity and tractable imagination.",
            "optimal_configuration": "Paper uses latent dim 30 and recurrent state dim 200; recommends dynamics smoothing and consistency objective when used with contrastive temporal objectives.",
            "uuid": "e1417.2",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "PC3",
            "name_full": "PC3 (Predictive Coding for Locally-Linear Control)",
            "brief_description": "An information-theoretic approach that uses temporal contrastive predictive coding (CPC) between consecutive latent states to learn locally-linear latent spaces amenable to control; used as prior theoretical inspiration.",
            "citation_title": "Predictive coding for locally-linear control",
            "mention_or_use": "mention",
            "model_name": "PC3",
            "model_description": "Uses temporal CPC between latent states of consecutive time steps to learn a latent space suitable for locally-linear control; relies on CPC objectives and locally-linear dynamics assumptions for control analysis.",
            "model_type": "latent world model / representation learning approach (temporal CPC)",
            "task_domain": "Control and planning in latent spaces (theoretical and offline control settings)",
            "fidelity_metric": "Mutual information lower bounds (CPC objective) and suitability for locally-linear control (theoretical guarantees); no numeric fidelity metrics reported here.",
            "fidelity_performance": "PC3 provides theoretical motivation showing TPC is no worse at future-observation prediction than certain latent variable models; PC3 itself is not directly applied to online RL in this paper (requires teleportation and goal depiction).",
            "interpretability_assessment": "Focuses on learning representations consistent with locally-linear control; interpretability is theoretical (predictive suboptimality arguments) rather than empirical visualization.",
            "interpretability_method": "Theoretical analysis and mutual information arguments.",
            "computational_cost": "Not discussed in detail here; PC3 required constructs not easily applicable to standard RL settings (e.g., teleportation).",
            "efficiency_comparison": "PC3 is more of a theoretical baseline; not directly compared empirically in this paper's experiments.",
            "task_performance": "Not directly used for RL experiments in this paper; served as conceptual/theoretical inspiration.",
            "task_utility_analysis": "Shows that temporal predictive coding can be sufficient for optimal decision making in locally-linear control settings and motivates applying TPC ideas to online RL with RSSM.",
            "tradeoffs_observed": "PC3's constraints (goal depiction, teleportation) limit practicality despite theoretical benefits.",
            "design_choices": "Temporal CPC between latent states at consecutive time steps; analysis of predictive suboptimality.",
            "comparison_to_alternatives": "Contrasted with static CPC and reconstruction-based latent variable models; argued to be no worse for future prediction and to prioritize temporally-predictive information.",
            "optimal_configuration": "Not specified for RL use; motivates embedding temporal CPC into practical RSSM frameworks (as done in this paper).",
            "uuid": "e1417.3",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "CVRL",
            "name_full": "Contrastive Variational Model-based Reinforcement Learning (CVRL)",
            "brief_description": "A recent contrastive-based world-model approach that maximizes mutual information between current observation and its latent state (static predictive coding) while learning latent dynamics for planning.",
            "citation_title": "Contrastive variational model-based reinforcement learning for complex observations.",
            "mention_or_use": "use",
            "model_name": "CVRL",
            "model_description": "Contrastive objective that maximizes mutual information between current observation o_t and latent s_t (static predictive coding), combined with a latent dynamics model for planning; uses a bilinear critic f(s_t, o_t) = exp(z_t^T W s_t) where z_t is an embedding of o_t.",
            "model_type": "latent world model (static contrastive + dynamics)",
            "task_domain": "Model-based RL from pixels under complex observations/distractors",
            "fidelity_metric": "Mutual information lower bound via contrastive loss; downstream task return.",
            "fidelity_performance": "In the paper's experiments CVRL underperforms Dreamer on several standard tasks; in natural-background setting it is competitive to TPC on some tasks (Walker Run, Cheetah Run) but generally TPC outperforms CVRL in many cases (supplement shows TPC outperforms CVRL on 4/6 after 2e6 steps).",
            "interpretability_assessment": "No explicit interpretability claims; static CPC tends to encourage encoding as much information in the observation as possible (including nuisance/background).",
            "interpretability_method": "Not emphasized in this paper; uses contrastive critic design (bilinear) but no auxiliary reconstruction analyses reported here.",
            "computational_cost": "Hyperparameters: latent dim 30, rec dim 200, batch size 50; uses a bilinear critic which is an extra parameter matrix W; similar computational footprint to Dreamer except lacks heavy decoder.",
            "efficiency_comparison": "Compared to TPC, CVRL uses static mutual information so can encode more nuisance info; empirically learns slower/achieves lower returns on several tasks in this paper's experiments.",
            "task_performance": "Competitive on some tasks in natural-background setting but outperformed by TPC on several tasks; underperforms Dreamer on several standard tasks per reported plots.",
            "task_utility_analysis": "Static predictive coding encourages encoding of current-observation information indiscriminately, which can reduce task utility under heavy distractors because nuisance information consumes representation capacity.",
            "tradeoffs_observed": "Static CPC is simpler but favors full-observation encoding; temporal CPC (TPC) trades off full-observation fidelity for temporally-predictive/utility-focused encoding which helps in distractor settings.",
            "design_choices": "Bilinear critic for contrastive loss; static mutual information between o_t and s_t; uses RSSM for dynamics.",
            "comparison_to_alternatives": "TPC's temporal CPC yields better robustness to distractors compared to CVRL's static CPC in the experiments reported.",
            "uuid": "e1417.4",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "DBC",
            "name_full": "Deep Bisimulation for Control (DBC)",
            "brief_description": "A model-free representation learning approach that attempts to learn representations that preserve a bisimulation metric, with the goal of removing task-irrelevant variation from observations.",
            "citation_title": "Learning invariant representations for reinforcement learning without reconstruction.",
            "mention_or_use": "use",
            "model_name": "DBC (bisimulation-style representation learning)",
            "model_description": "Learns latent representations by encouraging preservation of a bisimulation metric between states; implemented with heuristics (e.g., replacing the ground metric d with Euclidean distance when comparing next-state distributions) rather than solving the exact fixed-point equation for bisimulation.",
            "model_type": "model-free invariant representation learning (bisimulation-inspired)",
            "task_domain": "Reinforcement learning from pixels, intended to be robust to distractors",
            "fidelity_metric": "Preservation of a bisimulation-like metric (implemented heuristically) and downstream task return.",
            "fidelity_performance": "In the paper's experiments DBC failed to work on all six natural-background tasks (contrasting with its original reported performance where a single video was used); suggests poor generalization to multiple unseen distractors.",
            "interpretability_assessment": "Aim is to learn invariant latent metrics; no auxiliary interpretability analyses in this paper.",
            "interpretability_method": "Not detailed here; relies on bisimulation-inspired losses rather than explicit visualization.",
            "computational_cost": "Model-free approach with additional distance computations between next-state distributions; hyperparameters reported in supplement (learning rates, buffer sizes etc.).",
            "efficiency_comparison": "Fails empirically in the paper's multi-distractor setting, suggesting less effective use of representation capacity under diverse distractors compared to TPC.",
            "task_performance": "Failed to learn in the natural-background experiments of this paper; underperformed compared to TPC and baselines in those settings.",
            "task_utility_analysis": "Although bisimulation is theoretically suited to filter task-irrelevant info (including temporally-predictive nuisances), the practical implementation used here does not solve the true bisimulation fixed point and performs poorly under varied distractors in these experiments.",
            "tradeoffs_observed": "Theoretically strong but practically sensitive to approximation choices; empirical robustness to diverse unseen distractors is lacking in reported experiments.",
            "design_choices": "Bisimulation-inspired loss, heuristic approximations (Euclidean distance substitution), model-free RL pipeline.",
            "comparison_to_alternatives": "TPC (temporal CPC + latent dynamics) outperforms DBC in the multi-distractor/generalization experiments presented.",
            "uuid": "e1417.5",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "CURL",
            "name_full": "CURL (Contrastive Unsupervised Representations for RL)",
            "brief_description": "A static contrastive auxiliary-representation method used with model-free RL; included in supplementary comparisons against TPC in natural-background settings.",
            "citation_title": "curl: Contrastive unsupervised representations for reinforcement learning.",
            "mention_or_use": "use",
            "model_name": "CURL",
            "model_description": "Uses contrastive learning (static CPC between observation and augmented views / latent embeddings) as an auxiliary objective to aid model-free RL, producing better visual representations for downstream policies.",
            "model_type": "static contrastive representation learning (auxiliary for model-free RL)",
            "task_domain": "Model-free RL from pixels, tested in DMC natural-background setting",
            "fidelity_metric": "Contrastive loss (InfoNCE) and downstream task return.",
            "fidelity_performance": "In supplementary experiments TPC outperforms CURL on 3 of 6 natural-background tasks and CURL outperforms TPC on Walker Run; both fail on Hopper Hop and Cup Catch in the reported runs.",
            "interpretability_assessment": "No interpretability analyses reported in this paper.",
            "interpretability_method": "Not detailed here.",
            "computational_cost": "Not detailed here; typical CURL uses an image encoder and contrastive training with data augmentation.",
            "efficiency_comparison": "As a static contrastive baseline, CURL does not exploit temporal predictive structure, which can limit robustness to temporal distractors compared to TPC.",
            "task_performance": "Mixed vs TPC; TPC outperforms on several tasks but CURL is better on at least one reported task.",
            "task_utility_analysis": "Static contrastive objectives help representation learning but do not specifically prefer temporally-predictive, task-relevant features, limiting robustness to dynamic distractors.",
            "tradeoffs_observed": "Auxiliary static contrastive learning helps in some settings but lacks temporal inductive bias that TPC uses.",
            "design_choices": "Static contrastive InfoNCE loss between observation embeddings and latent codes / augmentations; no temporal CPC bias.",
            "uuid": "e1417.6",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "PlaNet",
            "name_full": "PlaNet (learning latent dynamics for planning from pixels)",
            "brief_description": "A reconstruction-based latent dynamics model (RSSM) trained for planning in latent space using cross-entropy method; referenced as prior work on latent models from pixels.",
            "citation_title": "Learning latent dynamics for planning from pixels.",
            "mention_or_use": "mention",
            "model_name": "PlaNet",
            "model_description": "RSSM-style latent dynamics trained with observation reconstruction and used for planning with trajectory optimization (CEM) in latent space.",
            "model_type": "latent world model (reconstruction-based)",
            "task_domain": "Planning from pixels (control tasks)",
            "fidelity_metric": "Reconstruction loss and planning returns.",
            "fidelity_performance": "Prior work reports strong performance on many control tasks; not experimentally evaluated in this paper.",
            "interpretability_assessment": "Similar limitations as other reconstruction-based models (encodes full observations including nuisances).",
            "interpretability_method": "N/A in this paper.",
            "computational_cost": "Reconstruction decoder and RSSM costs; specifics not reported here.",
            "efficiency_comparison": "Reconstruction-based approach can be vulnerable to distractors compared to TPC's temporal-contrastive training.",
            "task_performance": "Not evaluated in this paper; cited as related work.",
            "task_utility_analysis": "Reconstruction objective can cause encoding of task-irrelevant pixels, harming utility under distracting backgrounds.",
            "tradeoffs_observed": "Pixel fidelity vs robustness to distractors.",
            "design_choices": "RSSM + pixel decoder + planning via CEM.",
            "uuid": "e1417.7",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "World Models",
            "name_full": "World Models (Ha & Schmidhuber)",
            "brief_description": "A two-stage recurrent latent world model approach that learns a recurrent latent dynamics model then evolves controllers in imagination; cited as early influential work on latent world models.",
            "citation_title": "Recurrent world models facilitate policy evolution.",
            "mention_or_use": "mention",
            "model_name": "World Models",
            "model_description": "Two-stage pipeline learning a recurrent latent dynamics model and subsequently training/evolving controllers in the learned latent space (original experiments on Atari / car racing / vizdoom-like settings).",
            "model_type": "latent world model (recurrent)",
            "task_domain": "Games and simulated control tasks",
            "fidelity_metric": "Reconstruction/prediction losses and downstream controller performance.",
            "fidelity_performance": "Demonstrated feasibility of latent world-model-based controllers; not quantified here.",
            "interpretability_assessment": "Not emphasized in this paper.",
            "interpretability_method": "N/A here.",
            "computational_cost": "Two-stage training but details not given here.",
            "efficiency_comparison": "Influential prior work motivating latent world models; later methods added joint training, RSSM refinements.",
            "task_performance": "Not evaluated in this paper.",
            "task_utility_analysis": "Shows value of learning dynamics for imagined control.",
            "tradeoffs_observed": "Two-stage design vs joint training choices in later work.",
            "design_choices": "RNN-based latent dynamics, evolutionary controllers in imagination.",
            "uuid": "e1417.8",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "SOLAR",
            "name_full": "SOLAR (Structured representations for model-based RL)",
            "brief_description": "A latent model that models dynamics as time-varying linear-Gaussian with quadratic costs and performs control using guided policy search; cited as a reconstruction-based world-model variant.",
            "citation_title": "SOLAR: Deep structured representations for model-based reinforcement learning.",
            "mention_or_use": "mention",
            "model_name": "SOLAR",
            "model_description": "Learns structured latent representations where dynamics are modeled as time-varying linear-Gaussian and costs are quadratic, enabling use of guided policy search/control-theoretic solvers.",
            "model_type": "latent structured world model (linear-Gaussian dynamics)",
            "task_domain": "Model-based RL and control from images",
            "fidelity_metric": "Model likelihood / reconstruction and downstream control performance.",
            "fidelity_performance": "Cited as prior method; not empirically compared in this paper.",
            "interpretability_assessment": "Structured linear-Gaussian components increase interpretability relative to black-box dynamics, but specifics not discussed here.",
            "interpretability_method": "Model structure (linear-Gaussian) aids interpretability; not probed in this paper.",
            "computational_cost": "Not detailed in this paper.",
            "efficiency_comparison": "Structured dynamics can be sample-efficient when assumptions hold; not directly compared here.",
            "task_performance": "Not evaluated in this paper.",
            "task_utility_analysis": "Structured assumptions facilitate control with classical solvers but may limit generality.",
            "tradeoffs_observed": "Structure vs flexibility tradeoff noted in related-work discussion.",
            "design_choices": "Time-varying linear-Gaussian dynamics and quadratic costs; guided policy search integration.",
            "uuid": "e1417.9",
            "source_info": {
                "paper_title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination.",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels.",
            "rating": 2
        },
        {
            "paper_title": "Predictive coding for locally-linear control",
            "rating": 2
        },
        {
            "paper_title": "Contrastive variational model-based reinforcement learning for complex observations.",
            "rating": 2
        },
        {
            "paper_title": "Learning invariant representations for reinforcement learning without reconstruction.",
            "rating": 2
        },
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 2
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution.",
            "rating": 1
        },
        {
            "paper_title": "SOLAR: Deep structured representations for model-based reinforcement learning.",
            "rating": 1
        },
        {
            "paper_title": "curl: Contrastive unsupervised representations for reinforcement learning.",
            "rating": 1
        }
    ],
    "cost": 0.0219235,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Temporal Predictive Coding For Model-Based Planning In Latent Space</h1>
<p>Tung Nguyen ${ }^{<em> 1}$ Rui Shu ${ }^{</em> 2}$ Tuan Pham ${ }^{* 1}$ Hung Bui ${ }^{1}$ Stefano Ermon ${ }^{2}$</p>
<h4>Abstract</h4>
<p>High-dimensional observations are a major challenge in the application of model-based reinforcement learning (MBRL) to real-world environments. To handle high-dimensional sensory inputs, existing approaches use representation learning to map high-dimensional observations into a lower-dimensional latent space that is more amenable to dynamics estimation and planning. In this work, we present an information-theoretic approach that employs temporal predictive coding to encode elements in the environment that can be predicted across time. Since this approach focuses on encoding temporally-predictable information, we implicitly prioritize the encoding of task-relevant components over nuisance information within the environment that are provably task-irrelevant. By learning this representation in conjunction with a recurrent state space model, we can then perform planning in latent space. We evaluate our model on a challenging modification of standard DMControl tasks where the background is replaced with natural videos that contain complex but irrelevant information to the planning task. Our experiments show that our model is superior to existing methods in the challenging complex-background setting while remaining competitive with current state-of-the-art models in the standard setting.</p>
<h2>1. Introduction</h2>
<p>Learning to control from high dimensional observations has been made possible due to the advancements in reinforcement learning (RL) and deep learning. These advancements have enabled notable successes such as solving video games (Mnih et al., 2015; Lample \&amp; Chaplot, 2017) and continuous control problems (Lillicrap et al., 2016) from pixels. How-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ever, it is well known that performing RL directly in the high-dimensional observation space is sample-inefficient and may require a large amount of training data (Lake et al., 2017). This is a critical problem, especially for real-world applications. Recent model-based RL works (Kaiser et al., 2020; Ha \&amp; Schmidhuber, 2018; Hafner et al., 2019; Zhang et al., 2019; Hafner et al., 2020) proposed to tackle this problem by learning a world model in the latent space, and then applying RL algorithms in the latent world model.</p>
<p>The existing MBRL methods that learn a latent world model typically do so via reconstruction-based objectives, which are likely to encode task-irrelevant information, such as of the background. In this work, we take inspiration from the success of contrastive learning and propose a model that employs temporal predictive coding for planning from pixels. The use of temporal predictive coding circumvents the need for reconstruction-based objectives and prioritizes the encoding of temporally-predictable components of the environment, thus making our model robust to environments dominated by nuisance information. Our primary contributions are as follows:</p>
<ul>
<li>We propose a temporal predictive coding approach for planning from high-dimensional observations and theoretically analyze its ability to prioritize the encoding of task-relevant information.</li>
<li>We show experimentally that temporal predicting coding allows our model to outperform existing state-of-the-art models when dealing with complex environments dominated by task-irrelevant information, while remaining competitive on standard DeepMind control (DMC) tasks. Additionally, we conduct detailed ablation analyses to characterize the behavior of our model.</li>
</ul>
<h2>2. Motivation</h2>
<p>The motivation and design of our model are largely based on two previous works (Shu et al., 2020; Hafner et al., 2020). In this section, we briefly go over the relevant concepts in each work as well as how they motivate our paper. Shu et al. (2020) proposed PC3, an information-theoretic approach that uses contrastive predictive coding (CPC) to learn a latent space amenable to locally-linear control. Their framework uses a CPC objective between the latent states of two</p>
<p>consecutive time steps. They then use the latent dynamics $F$ as the variational device to define the following lower bound $\ell_{\text {cpc }}(E, F)$ :</p>
<p>$$
\mathbb{E} \frac{1}{K} \sum_{i} \ln \frac{F\left(E\left(o_{t+1}^{(i)}\right) \mid E\left(o_{t}^{(i)}\right), a_{t}^{(i)}\right)}{\frac{1}{K} \sum_{j} F\left(E\left(o_{t+1}^{(i)}\right) \mid E\left(o_{t}^{(j)}\right), a_{t}^{(j)}\right)}
$$</p>
<p>in which $E$ is the encoder, which encodes a highdimensional observation into a latent state. We make particular note of their choice to define the CPC objective for latent states across time steps instead of between the frame and its corresponding state-as is popular in the existing literature on contrastive learning for RL (Hafner et al., 2020; Srinivas et al., 2020; Ding et al., 2020). We shall refer to these respective objectives henceforth as temporal predictive coding (TPC) and static predictive coding (SPC) respectively. Shu et al. (2020) motivated temporal predictive coding via a theory of predictive suboptimality, which shows that the representation learned by temporal predictive coding is no worse at future-observation prediction than the representation learned by a latent variable model when applied to the same task.</p>
<p>In this work, we take a stronger position and argue that temporal predictive coding learns an information-theoretically superior representation to conventional latent variable models and static predictive coding models. This is because conventional latent variables models (i.e. temporal variational autoencoders equipped with a Gaussian decoder) and static predictive coding models both seek to encode as much information as possible about the original high-dimensional observations. In contrast, temporal predictive coding only encourages the encoding of temporally-predictable information within the environment, which we shall prove is sufficient for optimal decision making (Section 3.2). This property of temporal predictive coding thus makes this objective particularly suitable for handling RL problems dominated by nuisance information, which has recently become an active area of research (Ding et al., 2020; Zhang et al., 2020; Ma et al., 2020).</p>
<p>However, since PC3 only tackles the problem from an optimal control perspective, it is not readily applicable to RL problems. Indeed, PC3 requires a depiction of the goal to perform control, and also the ability to teleport to random locations of the state space to collect data, which are impractical in many problems. On the other hand, Dreamer (Hafner et al., 2020) achieves state-of-the-art performance on many RL tasks, but learns the latent space using a reconstructionbased objective. And while the authors reported a contrastive approach that yielded inferior performance to their reconstruction-based approach, their contrastive approach employed a static predictive coding objective.</p>
<p>The goal of our paper is thus two-fold. First, we propose a
model that leverages the concepts in PC3 and apply them to the Dreamer paradigm and RL setting. In particular, we show how to incorporate temporal predictive coding into the training of the recurrent state space model used in Dreamer. Second, we will demonstrate both theoretically and empirically that our resulting model is robust to RL environments that are dominated by nuisance information.</p>
<h2>3. Temporal Predictive Coding for Planning</h2>
<p>To plan in an unknown environment, we need to model the environment dynamics from experience. Following the Dreamer paradigm, we do so by iteratively collecting new data and using those data to train the world model. In this section, we focus on presenting the proposed model (its components and objective functions), a theoretical analysis of temporal predictive coding, followed lastly by some practical considerations when implementing the method.</p>
<h3>3.1. Model Definition</h3>
<p>Since we aim to learn a latent dynamics model for planning, we shall define an encoder $E$ to embed high-dimensional observations into a latent space, a latent dynamics $F$ to model the world in this space, and a reward function,</p>
<p>$$
\begin{array}{ll}
\text { Encoder: } &amp; E\left(o_{t}\right)=s_{t} \
\text { Latent dynamics: } &amp; F\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right)=p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) \
\text { Reward function: } &amp; R\left(r_{t} \mid s_{t}\right)=p\left(r_{t} \mid s_{t}\right)
\end{array}
$$</p>
<p>in which $t$ is the discrete time step, $\left{o_{t}, a_{t}, r_{t}\right}<em t="t">{t=1}^{T}$ are data sequences with image observations $o</em>\right)$. Following Hafner et al. (2019), we refer to this RNN as the recurrent state space model (RSSM). In practice, we use a deterministic encoder, and Gaussian distribution for dynamics and reward functions. The graphical model is presented in Figure 1. We now introduce the three key components of our model: temporal predictive coding, consistency, and reward prediction.}$, continuous action vectors $a_{t}$, scalar rewards $r_{t}$, and $s_{t}$ denotes the latent state at time $t$. We model the transition dynamics using a recurrent neural network with a deterministic state $h_{t}=\operatorname{RNN}\left(h_{t-1}, s_{t-1}, a_{t-1}\right)$, which summarizes information about the past, followed by the stochastic state model $p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right)=p\left(s_{t} \mid h_{t</p>
<p>Temporal predictive coding with an RSSM Instead of performing pixel prediction to learn $E$ and $F$ as in Hafner et al. (2019; 2020), we maximize the mutual information between the past latent codes and actions against the future latent code $I\left(E\left(O_{t}\right) ; E\left(O_{&lt;t}\right), A_{&lt;t}\right)$. This objective prioritizes the encoding of predictable components from the environment, which potentially helps avoid encoding nuisance information when dealing with complex image observations. Unlike in Shu et al. (2020), which defined</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The graphical model of TPC, in which we employ a recurrent neural network to model the dynamics. We omit the action and reward for simplicity. The approximation of $\hat{s}<em t="t">{t}$ with $s</em>$ is done by using contrastive predictive coding and maximum likelihood.</p>
<p>The TPC objective in a Markovian setting, the use of an RSSM means that we are maximizing the mutual information between the latent code at any time step $t$ and the entire historical trajectory latent codes and actions prior to $t$. To handle a trajectory over $T$ steps, we define an objective that sums over every possible choice of $t \in{2, \ldots, T}$,</p>
<p>$$
\sum_{t=2}^{T} I\left(E\left(O_{t}\right) ; E\left(O_{&lt;t}\right), A_{&lt;t}\right)
$$</p>
<p>To estimate this quantity, we employ contrastive predictive coding (CPC) proposed by <em>Oord et al. (2018)</em>. We perform CPC by introducing a critic function $f$ to construct the lower bound at a particular time step $t$,</p>
<p>$$
\begin{aligned}
&amp; I\left(E\left(O_{t}\right) ; E\left(O_{&lt;t}\right), A_{&lt;t}\right) \geq \
&amp; \mathbb{E} \frac{1}{K} \sum_{i} \ln \frac{\exp f\left(E\left(o_{t}^{(i)}\right), E\left(o_{&lt;t}^{(i)}\right), a_{&lt;t}^{(i)}\right)}{\frac{1}{K} \sum_{j} \exp f\left(E\left(o_{t}^{(i)}\right), E\left(o_{&lt;t}^{(j)}\right), a_{&lt;t}^{(j)}\right)}=: \ell_{\mathrm{ipc}}^{(t)}
\end{aligned}
$$</p>
<p>where the expectation is over $K$ i.i.d. samples of $\left(o_{t}, o_{&lt;t}, a_{&lt;t}\right)$. Note that Eq. (4) uses past $\left(E\left(o_{&lt;t}^{(j)}\right), a_{&lt;t}^{(j)}\right)$ from unrelated trajectories as an efficient source of negative samples for the contrastive prediction of the future latent code $E\left(o_{t}^{(i)}\right)$. Following <em>Shu et al. (2020)</em>, we choose to tie $f$ to our recurrent latent dynamics model $F$,</p>
<p>$$
\exp f\left(s_{t}, s_{&lt;t}, a_{&lt;t}\right)=F\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right)
$$</p>
<p>There are two favorable properties of this particular design. First, it is parameter-efficient since we can circumvent the instantiation of a separate critic $f$. Moreover, it takes advantage of the fact that an optimal critic is the true latent dynamics induced by the encoder $E$ (Poole et al., 2019; Ma &amp; Collins, 2018). We denote our objective as $\ell_{\mathrm{ipc}}(E, F)=\sum_{t=2}^{T} \ell_{\mathrm{ipc}}^{(t)}(E, F)$.</p>
<p>Enforcing latent dynamics consistency Although the true dynamics is an optimal critic for the CPC bound, maximizing this objective only does not ensure the learning of a latent dynamics model $F$ that is consistent with the true latent dynamics, due to the non-uniqueness of the optimal critic (Shu et al., 2020). Since an accurate dynamics is crucial for planning in the latent space, we additionally introduce a consistency objective, which encourages the latent dynamics model to maintain a good prediction of the future latent code given the past latent codes and actions. Similar to the recurrent CPC objective, we optimize for consistency at every time step in the trajectory,</p>
<p>$$
\ell_{\text {cons }}(E, F)=\sum_{t=2}^{T} \mathbb{E}<em t="t">{p\left(o</em>\right)
$$}, o_{&lt;t}, a_{&lt;t}\right)} \ln F\left(E\left(o_{t}\right) \mid E\left(o_{&lt;t}\right), a_{&lt;t</p>
<p>Reward prediction Finally, we train the reward function by maximizing the likelihood of the true reward value conditioned on the latent state $s_{t}=E\left(o_{t}\right)$,</p>
<p>$$
\ell_{\text {reward }}(E, R)=\sum_{t=1}^{T} \mathbb{E}<em t="t">{p\left(o</em>\right)\right)
$$}\right)} \ln R\left(r_{t} \mid E\left(o_{t</p>
<h3>3.2. Theoretical Analysis</h3>
<p>In contrast to a reconstruction-based objective, which explicitly encourages the encoder to behave injectively on the space of observations, our choice of mutual information objective as specified in Eq. (4) may discard information from the observed scene. In this section, we wish to formally characterize the information discarded by the temporal predictive coding (TPC) objective and argue that any information discarded by an optimal encoder under the TPC objective is provably task-irrelevant.
Lemma 1. Consider an optimal encoder and reward predictor pair $\left(E^{<em>}, R^{</em>}\right)$ where</p>
<p>$$
\begin{aligned}
&amp; \underset{E}{\arg \max } I\left(E\left(O_{t}\right) ; E\left(O_{&lt;t}\right), A_{&lt;t}\right)=E^{<em>} \
&amp; D_{K L}\left(p\left(r_{t} \mid o_{t}\right) | R^{</em>}\left(r_{t} \mid E^{*}\left(o_{t}\right)\right)\right)=0
\end{aligned}
$$</p>
<p>Let $\pi\left(a_{t} \mid E^{<em>}\left(o_{\leq t}\right), a_{&lt;t}\right)$ denote an $E^{</em>}$-restricted policy whose access to the observations $o_{&lt;t}$ is restricted by $E^{*}$.</p>
<p>Let $\pi_{\mathrm{aux}}\left(a_{t} \mid E^{<em>}\left(o_{\leq t}\right), E^{\prime}\left(o_{\leq t}\right), a_{&lt;t}\right)$ denote an $\left(E^{</em>}, E^{\prime}\right)$ restricted policy which has access to auxiliary information about $o_{&lt;t}$ via some encoder $E^{\prime}$. Let $\eta(\pi)$ denote the expected cumulative reward achieved by a policy $\pi$ over a finite horizon $T$. Then there exists no encoder $E^{\prime}$ where the optimal $E^{<em>}$-restricted policy underperforms the optimal $\left(E^{</em>}, E^{\prime}\right)$-restricted policy,</p>
<p>$$
\frac{\mathrm{a}}{t} E^{\prime} \text { s.t. } \eta\left(\pi^{<em>}\right)&lt;\eta\left(\pi_{\mathrm{aux}}^{</em>}\right)
$$</p>
<p>Intuitively, since $E^{<em>}$ optimizes TPC, any excess information contained in $E^{\prime}$ about $o_{t}$ (not already accounted for by $E^{</em>}$ ) must be temporally-unpredictive-it is neither predictable from the past nor predictive of the future. The excess information conveyed by $E^{\prime}$ is effectively nuisance information and thus cannot be exploited to improve the agent's performance. It is therefore permissible to dismiss the excess information in $E^{\prime}$ as being task-irrelevant. We provide a proof formalizing this intuition in Appendix B.</p>
<p>It is worth noting that TPC does not actively penalize the encoding of nuisance information, nor does temporallypredictive information necessarily mean it will be taskrelevant. However, if the representation space has limited capacity to encode information about $o_{t}$ (e.g., due to dimensionality reduction, a stochastic encoder, or an explicit information bottleneck regularizer), TPC will favor temporally-predictive information over temporallyunpredictive information-and in this sense, thus favor potentially task-relevant information over provably taskirrelevant information. This is in sharp contrast to a reconstruction objective, which makes no distinction between these two categories of information contained in the observation $o_{t}$.</p>
<h3>3.3. Practical Implementation Details</h3>
<p>Avoiding map collapse Naively optimizing $\ell_{\text {tpc }}$ and $\ell_{\text {cons }}$ can lead to a trivial solution, where the latent map collapses into a single point to increase the consistency objective arbitrarily. In the previous work, Shu et al. (2020) resolved this by adding Gaussian noise to the future encoding, which balances the latent space retraction encouraged by $\ell_{\text {cons }}$ with the latent space expansion encouraged by $\ell_{\text {tpc }}$. However, we found this trick insufficient and thus further incorporate a variant of static predictive coding. Our static predictive coding defines a lower bound for $I\left(E\left(O_{t}\right) ; O_{t}\right)$ where we employ a Gaussian distribution $\mathcal{N}\left(E\left(O_{t}\right) \mid E\left(O_{t}\right), \sigma I\right)$ as our choice of variational critic for $f\left(E\left(O_{t}\right), O_{t}\right)$. Crucially, our variant keeps the variance $\sigma$ fixed so that the static predictive coding objective solely serves the role of encouraging map expansion.</p>
<p>Smoothing the dynamics model Since our encoder is deterministic, the dynamics always receives clean latent codes
as inputs during training. However, in behavior learning, we roll out multiple steps towards the future from a stochastic dynamics. These roll outs are susceptible to a cascading error problem, which hurts the value estimation and policy learning. To resolve this issue, we smooth the dynamics by adding Gaussian noise to the inputs of the recurrent dynamics during training. The noise-adding procedure is as follows: assume the dynamics outputs $\hat{s}<em t="t">{t} \sim \mathcal{N}\left(\mu</em>\right)$ and feed it to the latent dynamics, and repeat for every time step $1 \leq t \leq T$. We call this dynamicsassociated noise, which ensures that the latent dynamics can handle the amount of noise that it produces when rolling out. The overall objective of our model is}, \sigma_{t}^{2}\right)$ as the prediction at time step $t$, we then add $\epsilon_{t} \sim \mathcal{N}\left(0, \sigma_{t}^{2}\right)$ to $s_{t}=E\left(o_{t</p>
<p>$$
\begin{aligned}
\max <em 1="1">{E, F, R} &amp; \lambda</em>(E, F) \
&amp; +\lambda_{3} \ell_{\mathrm{spc}}(E)+\lambda_{4} \ell_{\mathrm{reward}}(E, R)
\end{aligned}
$$} \ell_{\mathrm{tpc}}(E, F)+\lambda_{2} \ell_{\mathrm{cons}</p>
<h2>4. Behavior Learning</h2>
<p>Following Hafner et al. (2020), we use latent imagination to learn a parameterized policy for control. For selfcontainedness, this section gives a summary of this approach. Given the latent state $s_{t}=E\left(o_{t}\right)$, we roll out multiple steps into the future using the learned dynamics $F$, estimate the return and perform backpropagation through the dynamics to maximize this return, which in turn improves the policy.</p>
<p>Action and value models Two components needed for behavior learning are the action model and the value model, which both operate on the latent space. The value model estimates the expected imagined return when following the action model from a particular state $s_{\tau}$, and the action model implements a policy, conditioned on $s_{\tau}$, that aims to maximize this return. With imagine horizon $H$, we have</p>
<p>$$
\begin{array}{ll}
\text { Action model: } &amp; a_{\tau} \sim \pi\left(a_{\tau} \mid s_{\tau}\right) \
\text { Value model: } &amp; v\left(s_{\tau}\right) \approx \mathbb{E}<em _tau="\tau">{\pi\left(\cdot \mid s</em>
\end{array}
$$}\right)} \sum_{\tau=t}^{t+H} \gamma^{\tau-t} r_{\tau</p>
<p>Value estimation To learn the action and value model, we need to estimate the state values of imagined trajectories $\left{s_{\tau}, a_{\tau}, r_{\tau}\right}<em _tau="\tau">{\tau=t}^{t+H}$, where $s</em>$ are sampled according to the dynamics and the policy. In this work, we use value estimation presented in Sutton \&amp; Barto (2018),}$ and $a_{\tau</p>
<p>$$
\begin{aligned}
&amp; \mathrm{V}<em _tau="\tau">{\mathrm{N}}^{k}\left(s</em>}\right) \doteq \mathrm{E<em _theta="\theta">{q</em>\right)\right) \
&amp; \mathrm{V}}, q_{\phi}}\left(\sum_{n=\tau}^{h-1} \gamma^{n-\tau} r_{n}+\gamma^{h-\tau} v_{\psi}\left(s_{h<em _tau="\tau">{\lambda}\left(s</em>}\right) \doteq(1-\lambda) \sum_{n=1}^{H-1} \lambda^{n-1} \mathrm{~V<em _tau="\tau">{\mathrm{N}}^{n}\left(s</em>}\right)+\lambda^{H-1} \mathrm{~V<em _tau="\tau">{\mathrm{N}}^{H}\left(s</em>\right) \
&amp; \text { with } h=\min (\tau+k, t+H)
\end{aligned}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. TPC vs the baselines in standard DMC tasks. The return is computed in 1000 environment steps. Each task is run with 3 seeds.
which allows us to estimate the return beyond the imagine horizon. The value model is then optimized to regress this estimation, while the action model is trained to maximize the value estimation of all states $s_{\tau}$ along the imagined trajectories,</p>
<p>$$
\begin{aligned}
&amp; \max <em _tau="t">{\pi} \mathbb{E}\left(\sum</em>}^{t+H} \mathrm{~V<em _tau="\tau">{\lambda}\left(s</em>\right)\right) \
&amp; \min <em _tau="t">{v} \mathbb{E}\left(\sum</em>}^{t+H} \frac{1}{2}\left|v_{\psi}\left(s_{\tau}\right)-\mathrm{V<em _tau="\tau">{\lambda}\left(s</em>\right)
\end{aligned}
$$}\right)\right|^{2</p>
<h2>5. Experiments</h2>
<p>In this section, we empirically evaluate the proposed model (which we shall simply refer to as TPC for convenience) in various settings. First, we design experiments to compare the relative performance of our model with the current best model-based method in several standard control tasks. Second, we evaluate its ability to handle a more realistic but also more complicated scenario, in which we replace the background of the environment with a natural video. We discuss how our model is superior in the latter case compared to other existing methods, while remaining competitive in the standard setting. Finally, we conduct ablation studies to demonstrate the importance of the components of our model.</p>
<p>Control tasks For the standard setting, we test our model on 6 DeepMind Control (DMC) tasks (Tassa et al., 2018): Cartpole Swingup, Cheetah Run, Walker Run, Pendulum Swingup, Hopper Hop and Cup Catch. In the natural background setting, we replace the background of each data trajectory with a video taken from the kinetics dataset (Kay et al., 2017). We split the original dataset into two separate sets for training and evaluation to also test the generalization of each method.</p>
<p>Baseline methods We compare TPC with Dreamer (Hafner et al., 2020), CVRL (Ma et al., 2020), and DBC (Zhang et al., 2020). We compare against Dreamer since it is the current state of the art model-based method for planning from pixels and directly inspired our design of TPC. CVRL is a closely-related contrastive-based model recently developed that also touted its ability to handle nuisance backgrounds. Finally, we compare with DBC, a leading model-free method for learning representations that are invariant to distractors, to showcase how temporal predictive coding fares against bisimulation for learning task-relevant representations for downstream control. Each method is evaluated by the environment return in 1000 steps. For the baselines, we use the best set of hyperparameters as reported in their paper. We run each task with 3 different seeds for each model.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. TPC vs the baselines in the natural backgrounds setting. The return is computed in 1000 evironment steps. Each task is run with 3 seeds.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. TPC vs two different variants, where we omit the temporal predictive coding in SPC and static predictive coding in Unstable-TPC. The return is computed in 1000 evironment steps. Each task is run with 3 seeds.</p>
<h3>5.1. Comparisons in Standard Setting</h3>
<p>We demonstrate the performance of all methods on standard control tasks in Figure 2. TPC is competitive with Dreamer in all but one task, while CVRL and DBC both underperformed Dreamer on four tasks. TPC's strong performance is in contrast to what was previously observed in Dreamer (Hafner et al., 2020), where they showed the inferiority of a static contrastive learning approach (which applies CPC between an image observation and its corresponding latent code) compared to their reconstruction-based approach. Our results thus show that the use of temporal contrastive predictive coding is critical for learning a latent space that is suitable for behavior learning.</p>
<h3>5.2. Comparisons in Natural Background Setting</h3>
<p>In this setting, we evaluate the robustness of TPC versus the baselines in dealing with complicated, natural backgrounds. The performance of all models in this setting is shown in Figure 3. Dreamer fails to achieve meaningful performance across all six tasks. This is due to the use of reconstruction loss, which forces the world model to reconstruct every single pixel in the observations, thus leading to the encoding of task-irrelevant information.</p>
<p>TPC outperforms all the baselines significantly on two of the six tasks, while being competitive to CVRL on Walker Run and Cheetah Run (see Appendix C for more environment steps, where TPC outperforms CVRL on three tasks).</p>
<p>All methods fail to work on Hopper Hop and Cup Catch after $1 \times 10^{6}$ environment steps. Hopper Hop is a very challenging task even in the standard setting. Furthermore, the agents in these two tasks are tiny compared to the complex backgrounds, thus making it difficult for the model to distill task-relevant information. We note that the discrepancy between our results for CVRL and results reported in their paper (Ma et al., 2020) is due to the fact that while they used Imagenet (Russakovsky et al., 2015) for backgrounds in the original experiments, we use videos from kinetics dataset (Kay et al., 2017) instead.</p>
<p>DBC fails to work on all six tasks, which is in contrast to their reported performance in (Zhang et al., 2020). We note that, in their experiments, the authors used a single video for both training and testing. Their poor performance in our setting suggests that DBC is incapable of dealing with multiple distracting scenes, and also does not generalize well to unseen distractors.</p>
<p>Overall, the empirical results show that our proposed model is superior to all the baselines. In practice, it is desirable to have model which can work well for both standard (easy) setting and natural background (complex, distracting) setting. TPC satisfies this, since it performs on-par with Dreamer and significantly better than CVRL and DBC on standard control tasks, and in natural background setting which Dreamer fails, TPC works reasonably well and outperforms their counterparts CVRL and DBC.</p>
<h3>5.3. Ablation Analysis</h3>
<p>We conduct an ablation analysis to evaluate the importance of each CPC objective employed in TPC. To do so, we compare the original model with two variants, where we omit the temporal predictive coding or static predictive coding objective respectively (added to prevent map collapse; see Section 3.3). The relative performance of these models on three control tasks is shown in Figure 4. The original TPC achieves the best performance, while the variant with only static predictive coding (denoted SPC) fails across all three tasks. TPC without static predictive coding (Unstable-TPC) is unstable, since it faces the map collapsing problem, which leads to poor performance.</p>
<h3>5.4. Analysis in the Random Background Setting</h3>
<p>To demonstrate that temporal predictive coding is able to filter out temporally-unpredictive information, we conduct an experiment where the background is randomly chosen for each time step. In this setting, the only source of temporallypredictive information is the agent itself. Using the representations learned by Dreamer, TPC, and SPC, we then</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Reconstructions of a trajectory. $1^{\text {st }}$ row: ground-truth data, $2^{\text {nd }}$ row: Dreamer, $3^{\text {rd }}$ row: SPC, $4^{\text {th }}$ row: TPC
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Performance of TPC versus Dreamer and SPC in the random background setting.</p>
<p>Figure 5. TPC and the baselines in the random background setting
trained auxiliary decoders that try to reconstruct the original observations. Figure 5 shows that the representation learned by TPC is unable to reconstruct the background but can reconstruct the agent with high fidelity. In contrast, Dreamer's representation can reconstruct most of the image but not the agent itself, and SPC's reconstructions are largely uninterpretable. Furthermore, of the three models, only TPC successfully performs in the random background setting.</p>
<p>While the random background setting experiment aligns with our theoretical analysis and clearly demonstrates TPC's robustness to temporally-unpredictive nuisance information, it also exposes a particular limitation in our theoretical understanding of TPC's performance in the natural background setting, since natural videos are in fact a source of temporally-predictive nuisance information. We provide two possible explanations for TPC's empirically strong performance in the natural background setting. First, the natural videos may contain considerable temporally-unpredictive information that TPC successfully dismisses. Second, the natural videos largely consist of hard-to-predict informa-</p>
<p>tion that our recurrent neural network deems effectively unpredictable. We note that the latter potential explanation appears intimately related to the concept of "usable information" (Xu et al., 2020) whenever we instantiate a mutual information estimator with finite-computational power-and is worthy of further research scrutiny.</p>
<h2>6. Related Work</h2>
<p>Learning latent space for model-based RL via reconstruction Latent world models can be learned by jointly training the encoder and the latent dynamics model with observation reconstruction loss. Learning Controllable Embedding (LCE) approach, including E2C (Watter et al., 2015), RCE (Banijamali et al., 2018) and PCC (Levine et al., 2020), uses randomly collected data to pre-train a Markovian latent dynamic model that is specifically designed for locallylinear control, then run offline optimal control on top of the learned latent space. CARL (Cui et al., 2020) extends these works for Soft Actor-Critic (Haarnoja et al., 2018) and also proposes an online version, in which they iteratively learn the model and a parameterized policy. World Models (Ha \&amp; Schmidhuber, 2018) learn a recurrent latent dynamic model in a two-stage process to evolve their linear controllers in imagination. PlaNet (Hafner et al., 2019) jointly learns a recurrent state space model (RSSM) and plans in latent space using the cross entropy method, while Dreamer (Hafner et al., 2020) uses RSSM to iteratively learn the model and the policy by backpropagating through the dynamics. SOLAR (Zhang et al., 2019) models the dynamics as time-varying linear-Gaussian with quadratic costs and controls using guided policy search. However, training world models with reconstruction loss has several drawbacks: it requires a decoder as an auxiliary network for predicting images, and by reconstructing every single pixel, those methods are potentially vulnerable to task-irrelevant information such as an irrelevant background.</p>
<p>Learning latent space for model-based RL via constrastive learning An alternative framework for learning latent world models is contrastive learning. Contrastive learning is a self-supervised learning technique that aims to learn representations by contrasting positive samples against negative samples without having to reconstruct images (Oord et al., 2018; Chen et al., 2020a). Recently proposed contrastive learning methods have achieved significant successes in learning representations purely from unlabeled data, which include works by (Chen et al., 2020a;b; Bachman et al., 2019; Hénaff et al., 2019; He et al., 2020; Tian et al., 2019). Poole et al. (2019) has also established a close connection between contrastive learning and mutual information maximization. In the context of RL, recent works have proposed to use this framework to accelerate RL from pixels in two distinct directions: 1) cast contrastive
learning as an auxiliary representation learning task, and use model-free RL methods on top of the learned latent space (Oord et al., 2018; Srinivas et al., 2020); and 2) use contrastive learning in conjunction with learning a latent dynamics for planning in the latent space (Shu et al., 2020; Ding et al., 2020; Hafner et al., 2020).</p>
<p>Learning to control from pixels with distractors To work well in real-world scenarios, a control-from-pixel method needs to be robust against distractors in the observation. Recent works have been proposed to address this problem, including CVRL (Ma et al., 2020) and DBC (Zhang et al., 2020). Similar to TPC, CVRL employs a contrastive-based objective to learn the latent representation and latent dynamics that enable direct planning in latent space. However, they aim to maximize the mutual information between the current observation $o_{t}$ and its corresponding latent state $s_{t}$. This in theory still encourages the model to encode as much information in the observation as possible, including rich but task-irrelevant information such as the background. Our proposed TPC, in contrast, employs a temporal predictive coding objective, which helps discourage the encoding of provably task-irrelevant information, as shown in Section 3.2. Taking a different approach, DBC was proposed to learn invariant representations for RL by forcing the latent space to preserve the on-policy bisimulation metric. This is in theory better than our contrastive objective, since it is capable of filtering out task-irrelevant but temporally-predictive information. However, while being theoretically sound, their implemented objective does not adhere to the true bisimulation metric. Specifically, they never solved the fixed point equation to compute the metric, but instead used a heuristic version of it, where they replaced the ground metric $d$ with Euclidean distance while comparing two distributions of the next state, with $d$ being the current bisimulation metric.</p>
<h2>7. Conclusion</h2>
<p>In this work, we propose a temporal predictive coding approach for planning in latent space. We show theoretically and experimentally that temporal predictive coding prioritizes the encoding of task-relevant components over temporally-unpredictive-and thus provably-irrelevantinformation. This is critically different from reconstructionbased objectives as well as static contrastive learning objectives that only maximize the mutual information between the current observation and its latent code, which indiscriminately favor the encoding of both task-relevant and irrelevant information. Our experiments show that temporally predictive coding outperforms state-of-the-art models in environments dominated by task-irrelevant information while remaining competitive on standard DMC tasks.</p>
<h2>Acknowledgements</h2>
<p>This research was supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024), FLI, and Amazon AWS.</p>
<h2>References</h2>
<p>Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15535-15545, 2019.</p>
<p>Banijamali, E., Shu, R., Ghavamzadeh, M., Bui, H., and Ghodsi, A. Robust locally-linear controllable embedding. volume 84 of Proceedings of Machine Learning Research, pp. 1751-1759, Playa Blanca, Lanzarote, Canary Islands, 09-11 Apr 2018. PMLR. URL http://proceedings.mlr.press/v84/ banijamali18a.html.</p>
<p>Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.</p>
<p>Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. Big self-supervised models are strong semisupervised learners. arXiv preprint arXiv:2006.10029, 2020b.</p>
<p>Cui, B., Chow, Y., and Ghavamzadeh, M. Control-aware representations for model-based reinforcement learning. arXiv preprint arXiv:2006.13408, 2020.</p>
<p>Ding, Y., Clavera, I., and Abbeel, P. Mutual information maximization for robust plannable representations. arXiv preprint arXiv:2005.08114, 2020.</p>
<p>Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 2450-2462. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7512-recurrent-world-models-facilitate-poly- pdf.</p>
<p>Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. volume 80 of Proceedings of Machine Learning Research, pp. 18611870, Stockholmsmässan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr. press/v80/haarnoja18b.html.</p>
<p>Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dy-
namics for planning from pixels. volume 97 of Proceedings of Machine Learning Research, pp. 25552565, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr. press/v97/hafner19a.html.</p>
<p>Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=S1lOTC4tDS.</p>
<p>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 97299738, 2020.</p>
<p>Hénaff, O. J., Srinivas, A., De Fauw, J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v. d. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.</p>
<p>Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., and Michalewski, H. Model based reinforcement learning for atari. In International Conference on Learning Representations, 2020. URL https : //openreview.net/forum?id=S1xCPJHtDB.</p>
<p>Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.</p>
<p>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017.</p>
<p>Lample, G. and Chaplot, D. S. Playing fps games with deep reinforcement learning. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, pp. 2140-2146. AAAI Press, 2017.</p>
<p>Levine, N., Chow, Y., Shu, R., Li, A., Ghavamzadeh, M., and Bui, H. Prediction, consistency, curvature: Representation learning for locally-linear control. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=BJxG_0EtDS.</p>
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In ICLR (Poster), 2016. URL http://arxiv.org/abs/1509.02971.</p>
<p>Ma, X., Chen, S., Hsu, D., and Lee, W. S. Contrastive variational model-based reinforcement learning for complex observations. arXiv preprint arXiv:2008.02430, 2020.</p>
<p>Ma, Z. and Collins, M. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3698-3707, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1405. URL https: //www.aclweb.org/anthology/D18-1405.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature, 518(7540): $529-533,2015$.</p>
<p>Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., and Tucker, G. On variational bounds of mutual information. volume 97 of Proceedings of Machine Learning Research, pp. 5171-5180, Long Beach, California, USA, 0915 Jun 2019. PMLR. URL http://proceedings. mlr.press/v97/poole19a.html.</p>
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3): $211-252,2015$.</p>
<p>Shu, R., Nguyen, T., Chow, Y., Pham, T., Than, K., Ghavamzadeh, M., Ermon, S., and Bui, H. H. Predictive coding for locally-linear control. arXiv preprint arXiv:2003.01086, 2020.</p>
<p>Srinivas, A., Laskin, M., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.</p>
<p>Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.</p>
<p>Watter, M., Springenberg, J., Boedecker, J., and Riedmiller, M. Embed to control: A locally linear latent dynamics
model for control from raw images. In Advances in neural information processing systems, pp. 2746-2754, 2015.</p>
<p>Xu, Y., Zhao, S., Song, J., Stewart, R., and Ermon, S. A theory of usable information under computational constraints. arXiv preprint arXiv:2002.10689, 2020.</p>
<p>Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine, S. Learning invariant representations for reinforcement learning without reconstruction. arXiv preprint arXiv:2006.10742, 2020.</p>
<p>Zhang, M., Vikram, S., Smith, L., Abbeel, P., Johnson, M., and Levine, S. SOLAR: Deep structured representations for model-based reinforcement learning. volume 97 of Proceedings of Machine Learning Research, pp. 7444-7453, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr. press/v97/zhang19m.html.</p>
<h1>Supplementary Materials to Temporal Predictive Coding For Model-Based Planning In Latent Space</h1>
<h2>A. Hyper Parameters</h2>
<h2>A.1. Standard setting</h2>
<p>Dreamer, CVRL and TPC share the following hyperparameters:</p>
<h2>Model components</h2>
<ul>
<li>Latent state dimension: 30</li>
<li>Recurrent state dimension: 200</li>
<li>Activation function: ELU</li>
<li>The action model outputs a tanh mean scaled by a factor of 5 and a softplus standard deviation for the Normal distribution that is then transformed using tanh (Haarnoja et al., 2018)</li>
</ul>
<h2>Learning updates</h2>
<ul>
<li>Batch size: 50 for Dreamer and CVRL, 250 for TPC</li>
<li>Trajectories length: 50</li>
<li>Optimizer: Adam (Kingma \&amp; Ba, 2014) with learning rates $6 \times 10^{-4}$ for world model, $8 \times 10^{-5}$ for value and action.</li>
<li>Gradient update rate: 100 gradient updates every 1000 environment steps.</li>
<li>Gradient clipping norm: 100</li>
<li>Imagination horizon: 15</li>
<li>$\gamma=0.99$ and $\lambda=0.95$ for value estimation</li>
</ul>
<h2>Environment interaction</h2>
<ul>
<li>The dataset is initialized with $S=5$ episodes collected using random actions.</li>
<li>We iterate between 100 training steps and collecting 1 episode by executing the predicted mode action with $\mathcal{N}(0,0.3)$ exploration noise.</li>
<li>Action repeat: 2</li>
<li>Environment steps: $2 \times 10^{6}$</li>
</ul>
<p>Additionally,</p>
<ul>
<li>Dreamer and CVRL clip the KL below 3 nats</li>
<li>
<p>CVRL uses a bi-linear model for the critic function in the contrastive loss: $f_{\theta}\left(s_{t}, o_{t}\right)=\exp \left(z_{t}^{T} W_{\theta} s_{t}\right)$, where $z_{t}$ is an embedding vector for observation $o_{t}$ and $W_{\theta}$ is a learnable weight matrix parameterized by $\theta$.</p>
</li>
<li>
<p>TPC has a fixed set of coefficient in the overall objective for all control tasks: $\lambda_{1}=1, \lambda_{2}=0.1, \lambda_{3}=1, \lambda_{4}=1$. We use a fixed Gaussian noise $\epsilon \sim \mathcal{N}\left(0,0.2^{2}\right)$ to add to the future latent code when computing temporal CPC, as suggested in (Shu et al., 2020), and also use 0.2 as the fixed variance in static CPC.</p>
</li>
</ul>
<p>In TPC, we also use a target network for the value model and update this network every 100 gradient steps. Note that we also tried to use target value network for Dreamer, but it does not improve the results, as suggested by their original paper (Hafner et al., 2020).</p>
<p>Hyperparameters for DBC We use the same set of hyperparameters as reported in the paper (Zhang et al., 2020)</p>
<ul>
<li>Replay buffer capacity: 1000000</li>
<li>Batch size: 128</li>
<li>Discount $\gamma: 0.99$</li>
<li>Optimizer: Adam</li>
<li>Critic learning rate: $10^{-5}$</li>
<li>Critic target update frequency: 2</li>
<li>Critic Q-function soft-update rate $\tau_{Q}: 0.005$</li>
<li>Critic encoder soft-update rate $\tau_{\phi}: 0.005$</li>
<li>Actor learning rate: $10^{-5}$</li>
<li>Actor update frequency: 2</li>
<li>Actor log stddev bounds: $[-5,2]$</li>
<li>Encoder learning rate: $10^{-5}$</li>
<li>Decoder learning rate: $10^{-5}$</li>
<li>Decoder weight decay: $10^{-7}$</li>
<li>Temperature learning rate: $10^{-4}$</li>
<li>Temperature Adam's $\beta_{1}: 0.9$</li>
<li>Init temperature: 0.1</li>
</ul>
<p>Hyperparameters search for TPC TPC has four hyperparameters that can be tuned: $\lambda_{1}, \lambda_{2}, \lambda_{3}$ and $\lambda_{4}$, which are coefficients for the TPC objective, consistency objective, SPC objective and reward prediction objective, respectively. Since $\lambda_{1}, \lambda_{3}$ and $\lambda_{4}$ do not conflict with each other, we fixed them to 1 and only tuned $\lambda_{2}$ in our experiments. We performed grid search for $\lambda_{2}$ in range ${0.05,0.1,0.2}^{2}$ on the Cartpole Swingup task and then used the same set of hyperparameters for all the remaining tasks.</p>
<h1>A.2. Natural background setting</h1>
<p>To further encourage the model to focus on task-relevant information from observations, we additionally tune the weight $\lambda_{4}$ of the reward loss in the training objective for both Dreamer and TPC. In each control task they share the same reward coefficient, which is specified in the table below. CVRL and DBC have the same hyperparameters as in the standard setting.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1. Reward coefficients for different tasks in the natural backgrounds setting
Task
Reward coefficient</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Cartpole Swingup, Cup Catch</th>
<th style="text-align: left;">1000</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Cheetah Run, Walker Run, Pendulum Swingup, Hopper Hop</td>
<td style="text-align: left;">100</td>
</tr>
</tbody>
</table>
<h1>B. Proof of Lemma 1</h1>
<p>Our goal is to show that, under the conditions in Lemma 1,</p>
<p>$$
\eta\left(\pi^{<em>}\right) \geq \eta\left(\pi_{\mathrm{aux}}^{</em>}\right)
$$</p>
<p>for any choice of auxiliary encoder $E^{\prime}$.
We start by denoting $s_{t}=E^{*}\left(o_{t}\right)$ and $s_{t}^{\prime}=E^{\prime}\left(o_{t}\right)$. Note that the performance of $\pi_{\mathrm{aux}}$ can be written as</p>
<p>$$
\begin{aligned}
\eta\left(\pi_{\mathrm{aux}}\right) &amp; =\mathbb{E}<em _mathrm_aux="\mathrm{aux">{\left(\pi</em>\right) \
&amp; =\sum_{\tau_{\mathrm{aux}}} r\left(o_{1: T}\right) \prod_{t} p\left(o_{t} \mid o_{&lt;t}, a_{&lt;t}\right) p\left(s_{t}, s_{t}^{\prime} \mid o_{t}\right) \pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, s_{\leq t}^{\prime}, a_{&lt;t}\right)
\end{aligned}
$$}}, p\right)} r\left(o_{1: T</p>
<p>where $\tau_{\mathrm{aux}}$ denotes the full trajectory of $\left(o, s, s^{\prime}, a\right)<em t="t">{1: T}$ and $r\left(o</em>\right) | R^{}\right)$ evaluates the reward at $o_{t}$ (for simplicity, we shall assume $p\left(r_{t} \mid s_{t}\right)$ is deterministic. Since $D_{\mathrm{KL}}\left(p\left(r_{t} \mid o_{t<em>}\left(r_{t} \mid E^{</em>}\left(o_{t}\right)\right)\right)=0$, we can rewrite as</p>
<p>$$
\eta\left(\pi_{\mathrm{aux}}\right)=\sum_{\tau_{\mathrm{aux}}} R^{*}\left(s_{1: T}\right) \prod_{t} p\left(o_{t} \mid o_{&lt;t}, a_{&lt;t}\right) p\left(s_{t}, s_{t}^{\prime} \mid o_{t}\right) \pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, s_{\leq t}^{\prime}, a_{&lt;t}\right)
$$</p>
<p>where, with a slight abuse of notation, we note that $R^{<em>}\left(E^{</em>}\left(o_{t}\right)\right)=r\left(o_{t}\right)$. We now further rewrite $\pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, s_{\leq t}^{\prime}, a_{&lt;t}\right)$ as</p>
<p>$$
p\left(a_{t} \mid s_{\leq t}, s_{\leq t}, a_{&lt;t}, \pi_{\mathrm{aux}}\right)
$$</p>
<p>and subsequently collapse the expression of the performance as</p>
<p>$$
\begin{aligned}
\eta\left(\pi_{\mathrm{aux}}\right) &amp; =\sum_{\left(o, s, s^{\prime}, a\right)<em 1:="1:" T="T">{1: T}} R^{<em>}\left(s_{1: T}\right) p\left(o_{1: T}, s_{1: T}, s_{1: T}^{\prime}, a_{1: T} \mid \pi_{\mathrm{aux}}\right) \
&amp; =\sum_{\left(s, s^{\prime}, a\right)_{1: T}} R^{</em>}\left(s</em>\right)
\end{aligned}
$$}\right) p\left(s_{1: T}, s_{1: T}^{\prime}, a_{1: T} \mid \pi_{\mathrm{aux}</p>
<p>where the last step arises from marginalization of $o_{1: T}$. Note by chain rule that $p\left(s_{1: T}, s_{1: T}^{\prime}, a_{1: T} \mid \pi_{\text {aux }}\right)$ becomes</p>
<p>$$
\prod_{t} p\left(s_{t} \mid s_{&lt;t}, s_{&lt;t}^{\prime}, a_{&lt;t}, \pi_{\mathrm{aux}}\right) p\left(s_{t}^{\prime} \mid s_{\leq t}, s_{&lt;t}^{\prime}, a_{&lt;t}, \pi_{\mathrm{aux}}\right) p\left(a_{t} \mid s_{\leq t}, s_{\leq t}^{\prime}, a_{&lt;t}, \pi_{\mathrm{aux}}\right)
$$</p>
<p>By analyzing the Markov blankets in $p\left(s_{1: T}, s_{1: T}^{\prime}, a_{1: T} \mid \pi_{\text {aux }}\right)$, we can simplify the above expression to</p>
<p>$$
\prod_{t} p\left(s_{t} \mid s_{&lt;t}, s_{&lt;t}^{\prime}, a_{&lt;t}\right) p\left(s_{t}^{\prime} \mid s_{\leq t}, s_{&lt;t}^{\prime}, a_{&lt;t}\right) p\left(a_{t} \mid s_{\leq t}, s_{\leq t}^{\prime}, a_{&lt;t}, \pi_{\mathrm{aux}}\right)
$$</p>
<p>Note that we omit the dependency on $\pi_{\text {aux }}$ in the first two terms since, given only the history of past actions and observations, the next observation does not depend on our choice of policy but only on the environment dynamics.
Since $E^{*}$ is optimal under the MI objective, we note that</p>
<p>$$
I\left(S_{&lt;t}, S_{&lt;t}^{\prime}, A_{&lt;t} ; S_{t}, S_{t}^{\prime}\right)=I\left(S_{&lt;t}, A_{&lt;t} ; S_{t}\right)
$$</p>
<p>Eq. (24) implies that $s_{&lt;t}^{\prime}$ is independent of $s_{t}$ given $\left(s_{&lt;t}, a_{&lt;t}\right)$, and that $\left(s_{&lt;t}, s_{&lt;t}^{\prime}, a_{&lt;t}\right)$ is independent of $s_{t}^{\prime}$ given $s_{t}$. This allow us to further simplify Eq. (23) to</p>
<p>$$
\prod_{t} p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) p\left(s_{t}^{\prime} \mid s_{t}\right) \pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, s_{\leq t}^{\prime}, a_{&lt;t}\right)
$$</p>
<p>Thus, the performance expression equates to</p>
<p>$$
\eta\left(\pi_{\mathrm{aux}}\right)=\sum_{\tau_{\mathrm{aux}}} R^{*}\left(s_{1: T}\right) \prod_{t} p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) p\left(s_{t}^{\prime} \mid s_{t}\right) \pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, s_{\leq t}^{\prime}, a_{&lt;t}\right)
$$</p>
<p>Note by way of similar reasoning (up to and including Eq. (23)) that</p>
<p>$$
\eta(\pi)=\sum_{\tau} R^{*}\left(s_{1: T}\right) \prod_{t} p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) \pi\left(a_{t} \mid s_{\leq t}, a_{&lt;t}\right)
$$</p>
<p>By comparing Eq. (26) and Eq. (27), we see that $s_{1: T}^{\prime}$ effectively serves as a source of noise that makes $\pi_{\text {aux }}$ behave like a stochastic policy depending on the seed choice for $s_{1: T}^{\prime}$. To take advantage of this, we introduce a reparameterization of $s^{\prime}$ as $\epsilon$ such that</p>
<p>$$
\begin{aligned}
\eta\left(\pi_{\mathrm{aux}}\right) &amp; =\sum_{\tau_{\mathrm{aux}}} R^{<em>}\left(s_{1: T}\right) \prod_{t} p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) p\left(\epsilon_{t}\right) \pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, \epsilon_{\leq t}, a_{&lt;t}\right) \
&amp; =\mathbb{E}<em 1:="1:" T="T">{p\left(\epsilon</em> R^{}\right)} \sum_{\langle s, a\rangle_{1: T}</em>}\left(s_{1: T}\right) \prod_{t} p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) \pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, \epsilon_{\leq t}, a_{&lt;t}\right) \
&amp; \leq \max <em 1:="1:" T="T">{\epsilon</em>\right) \
&amp; \leq \max _{\pi} \eta(\pi)
\end{aligned}
$$}} \sum_{\langle s, a\rangle_{1: T}} R^{*}\left(s_{1: T}\right) \prod_{t} p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) \pi_{\mathrm{aux}}\left(a_{t} \mid s_{\leq t}, \epsilon_{\leq t}, a_{&lt;t</p>
<p>where the last inequality comes from defining a policy</p>
<p>$$
\pi^{\prime}:=\pi_{\mathrm{aux}}\left(a_{t} \mid o_{\leq t}, \epsilon_{\leq t}^{*}, a_{&lt;t}\right)
$$</p>
<p>and noting that the performance of $\pi^{\prime}$ must be bounded by the performance of $\pi^{*}$.</p>
<h1>C. Additional Results</h1>
<h2>C.1. Comparision with CVRL after $2 \times 10^{6}$ environment steps</h2>
<p>In Figure 6, we compare the performance of TPC and CVRL after $2 \times 10^{6}$ environment steps in both the standard setting and the natural background setting. TPC learns faster and achieves much higher rewards compared to CVRL in the standard setting. In the natural control setting, TPC outperforms in 4 out of 6 tasks and is competitive in Walker Run. Both methods do not work on Hopper Hop.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6. TPC vs CVRL after $2 \times 10^{6}$ in the standard setting (top) and background setting (bottom)</p>
<h1>C.2. Comparison with CURL in the natural background setting</h1>
<p>As shown in Figure 7, TPC outperforms CURL significantly on 3 of 6 tasks, while CURL performs better on Walker Run. On Hopper Hop and Cup Catch, both methods fail to make progress after 1 million environment steps.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7. TPC versus CURL in the natural background setting. Each task is run with 3 seeds.</p>
<h2>C.3. Importance of dynamics smoothing</h2>
<p>We run TPC in the standard setting without dynamics smoothing to investigate the empirical importance of this component. As shown in Figure 8, TPC 's performance degrades significantly without dynamics smoothing. Without smoothing, the dynamics model cannot handle noisy rollouts during test-time planning, leading to poor performance. Dynamics smoothing prevents this by enabling test-time robustness against cascading error.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8. Performance of TPC in the standard setting without dynamics smoothing.</p>
<h2>C.4. Learning a separate reward model</h2>
<p>Joint learning of reward is crucial for all models in the natural background setting (see Appendix A.2). Since background information is also temporally-predictive, increasing the weight of reward loss encourages the model to focus more on the components that are important for reward learning. However, in the random background setting, since all temporallypredictive information is task-relevant, TPC uniquely can learn the reward separately, as shown in Figure 9.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9. Performance of TPC in the random background setting with a separately trained reward model.</p>
<h1>C.5. Reconstructions in the natural background setting</h1>
<p>We conduct experiments to investigate what information the encoder in different models learns to encode during training in the natural background setting. To do that, we train auxiliary decoders that try to reconstruct the original observations from the representations learned by Dreamer and TPC. As shown in Figure 10, Dreamer ( $2^{\text {nd }}$ row) tries to encode both the agent and the background. In contrast, TPC ( $3^{\text {rd }}$ row) prioritizes encoding the agent, which is task-relevant, over the background.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 10. Observation reconstruction of TPC versus Dreamer in the natural background setting</p>
<h2>C.6. The simplistic motion background setting</h2>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 11. The top row shows a sample sequence of data, and the bottom row shows the reconstruction of TPC.
As discussed in Section 3.2, TPC can capture certain task-irrelevant information. However, TPC can choose to not encode the background whenever encoding only the agent is sufficient to maximize the mutual information. In the experiments, we found that forcing the model to predict well the reward helps the encoder focus more on the agent, which can be done by increasing the weight of reward loss. Dreamer, in contrast, must encode as much information about the observation as</p>
<p>possible to achieve a good reconstruction loss. To elaborate on this, we conducted an experiment where we replaced the natural background with a simplistic motion, easily predictable background, which is depicted in Figure 11. Figure 12 shows that TPC works well in this setting, and outperforms Dreamer significantly.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 12. TPC versus Dreamer in the simplistic motion background setting</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Larger values of $\lambda_{2}$ lead to representation collapse.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4614 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4614</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4614</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-6c5c6f883604a3abaa829b83d2958de8c343beeb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c5c6f883604a3abaa829b83d2958de8c343beeb" target="_blank">A Computational Inflection for Scientific Discovery</a></p>
                <p><strong>Paper Venue:</strong> Communications of the ACM</p>
                <p><strong>Paper TL;DR:</strong> Enabling researchers to leverage systems to overcome the limits of human cognitive capacity is a central challenge in the next generation of artificial intelligence.</p>
                <p><strong>Paper Abstract:</strong> Enabling researchers to leverage systems to overcome the limits of human cognitive capacity.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4614.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4614.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Problem-graph inspiration eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical problem-graph inspiration evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human-subject evaluation of an NLP method that retrieves diverse problem perspectives from a mined hierarchical problem graph to stimulate creative reformulation; judged on usefulness and novelty of retrieved 'inspirations' and compared to IR baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling creative inspiration with fine-grained functional facets of product ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human-subject ideation evaluation with usefulness & novelty ratings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Controlled human experiments in which participants supply a focal problem; the system retrieves neighboring problems from a hierarchical problem graph mined from invention texts. Participants view retrieved 'inspirations' (vs. baseline retrieval methods) and rate or judge whether each inspiration is useful and novel for generating new ideas. The main measurement is the fraction of inspirations judged both useful and novel and comparative improvement relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Usefulness (practical value for problem-solving), novelty (degree of newness vs. known directions), creativity/ideation support; implicit criteria include relevance and abstraction level.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Technological inventions / engineering / innovation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Idea/inspiration retrieval (analogical/problem-reformulation suggestions), not formal scientific theories</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Over 60% of retrieved 'inspirations' were judged useful and novel; this represented a relative boost of approximately 50â€“60% over the best-performing baseline retrieval methods reported in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based evaluation: controlled user/ideation studies with human judgments of novelty and usefulness; comparisons were made against automatic baseline retrieval methods but the assessment of outputs used human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Controlled experiments with human participants and baseline comparisons; validation comprised measuring differences in human judgments between conditions (no detailed inter-rater statistics provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Uses user-provided problem description as a proxy for inner knowledge; limited information on inter-rater reliability; generalization across domains and abundance of lexical variability in scientific language are noted challenges; extraction errors and representation normalization can affect retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>A mined corpus of technological invention texts / patents (unnamed in paper) used to build the hierarchical problem graph; comparisons against standard IR baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4614.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4614.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bridger evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bridger author-matching creativity evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>User-study evaluation of Bridger, a system that recommends unfamiliar authors (and their salient problems/methods) to researchers to stimulate novel directions; assessed against state-of-the-art neural retrieval models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Author-match creativity user study (comparative human-subject evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Controlled studies with computer science researchers comparing Bridger's recommendations to those from a state-of-the-art neural model (Semantic Scholar engine). Measures included researchers' reported inspiration, discovery of useful cross-domain connections, and qualitative examples of novel directions generated after exploring recommended authors. Inner world representation was proxied via term-frequency mentions extracted from users' papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Creativity/inspiration (ability to surface novel, relevant authors and problem-method linkages), relevance to user's interests, potential to mitigate fixation biases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / cross-disciplinary research</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Idea-generation and author-recommendation (not explicit scientific theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Bridger 'dramatically boosted' creative search and inspiration relative to the state-of-the-art neural retrieval baseline; qualitative examples include discovery of cross-domain connections (e.g., graph theory to human-centered AI). No numeric performance metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based: user studies with domain researchers who judged usefulness and novelty; comparisons were to automated baseline systems but assessment relied on human reports and follow-up examples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>User studies with CS researchers and comparisons against outputs from existing neural search engines (Semantic Scholar); validation described via human judgments and qualitative case examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Inner-world representation used simple proxies (term-frequency from users' papers); limited personalization; evaluation lacks detailed quantitative metrics and inter-rater statistics; potential biases in participants and ecological validity concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Semantic Scholar corpus used as the outer-world knowledge base and the baseline retrieval engine for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4614.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4614.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Solvent analogical eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solvent mixed-initiative analogy retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of an analogical retrieval system that finds structurally similar inventions/papers to facilitate analogical transfer, measured via human ideation experiments that quantify boosts in creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solvent: A mixed initiative system for finding analogies between research papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Analogical retrieval ideation experiment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Participants are given a textual description of an invention/problem and are shown inspirations retrieved by the system (versus baseline IR). Participants then perform ideation tasks; outputs are scored on creativity metrics (e.g., novelty, usefulness) by judges or by predefined scoring rubrics. The study reports whether viewing the retrieved analogies increases measures of human creativity in the ideation task.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Creativity (novelty of new ideas), usefulness/relevance of inspirations for solving the focal problem, measurable increase in ideation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Engineering / product design / technology transfer</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Analogical inspirations and mechanism-level idea transfer (not formal theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Method produced a significant boost in measures of human creativity in ideation experiments compared with baseline information retrieval methods; specific qualitative examples cited (e.g., biomechanical lab finding inspiration in civil engineering literature). No precise numeric scores provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based evaluation: ideation experiments scored by humans; retrieval algorithms compared automatically but success measured via human outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Controlled human experiments with baseline comparisons; validation by showing statistically significant improvements in human creativity measures (specific stats not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Challenges include mapping structural (mechanistic) similarity beyond lexical cues, normalization across domains, and constructing sufficiently rich representations of mechanisms; extraction noise and limited generalization are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Database of technological inventions/patents used as the outer-world corpus (not given a standardized public dataset name in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4614.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4614.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Challenges-search eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search engine for discovery of scientific challenges and directions evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of a search engine designed to retrieve statements of uncertainty, open questions and hypotheses from the literature; compared to PubMed in experiments including medical doctors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A search engine for discovery of scientific challenges and directions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Uncertainty-and-hypothesis retrieval evaluation (human-subject comparison to PubMed)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Using query topics as proxies for participants' interests, the system retrieves documents or passages that explicitly state uncertainties, open questions, or hypotheses. Human participants (including clinicians) compare results from this prototype versus PubMed search for the same queries; evaluators judge which system better surfaces important and interesting challenges or research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability to surface statements of uncertainty, open questions, initial hypotheses; importance and interest to domain experts; relevance to user's query/interest.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / clinical research (evaluated with ACE2/COVID-19 example)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Discovery of open questions and hypotheses (not formal theoretical generation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Prototype 'dramatically outperformed' PubMed at discovering important and interesting areas of challenges and directions according to participant judgments; example output included identification of hypotheses linking ACE2 to liver damage in COVID-19 patients. No quantitative metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based evaluation: domain experts and participants judged outputs from both systems; comparison against an established automated baseline (PubMed), but assessment relies on human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>User studies with participants from diverse research backgrounds, including medical doctors; validation via comparative human judgments of retrieved items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Evaluation uses query topics as a proxy for inner world knowledge (imperfect); ground-truth for 'important challenges' is subjective; lack of quantitative metrics and potential sampling bias in participants.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>PubMed used as baseline retrieval system and corpus; the prototype operated over biomedical literature (corpus details not exhaustively specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4614.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4614.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEEP eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biomedical Evidence Enhanced Prediction (BEEP) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of BEEP, a literature-augmented clinical prediction system that retrieves medical papers relevant to individual ICU patients and synthesizes that evidence with EMR data to predict patient outcomes, compared to state-of-the-art models without literature retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Literature-augmented clinical outcome prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Literature-augmented clinical prediction evaluation (predictive performance comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>BEEP uses retrieval of medical literature conditioned on each patient's EMR-derived 'queries' and integrates retrieved evidence with internal EMR features to make predictions (e.g., in-hospital mortality, prolonged length of stay). Evaluation compares predictive performance (standard clinical outcome metrics) against state-of-the-art predictive models that do not use literature retrieval, measured on held-out clinical data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predictive accuracy for clinical outcomes (e.g., in-hospital mortality, length of stay); improvement in predictive performance when augmenting EMR with literature; alignment of model outputs with clinician-understandable evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Clinical medicine / biomedical informatics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Predictive models / evidence-augmented predictions (not generation of explanatory theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BEEP provided 'large improvements' over state-of-the-art models that do not use retrieval from the literature; exact numeric performance metrics are not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated evaluation using predictive metrics on clinical datasets (likely held-out test sets); also includes alignment checks to inner-world representations (e.g., matching patient aspects to cohorts in papers) which involve human-interpretable evidence but not a formal human-vs-system theory comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical evaluation on clinical EMR datasets with comparisons to baseline predictive models; validation described as improved predictive performance though specific validation statistics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Proxy for clinicians' inner world uses extracted queries from clinical notes (imperfect); data quality, access and privacy constraints; gap between system's literature understanding and physicians' expectations; lack of reported detailed quantitative metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Clinical EMR data from ICU cohorts used for prediction experiments (precise dataset name not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4614.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4614.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPI bias reprioritization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Protein-protein interaction (PPI) attention-bias and reprioritization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Temporal analysis of the growing graph of confirmed PPIs to quantify biases of locality in scientific attention and evaluate reprioritization methods that surface less-studied candidate PPIs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On biases of attention in scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Temporal network analysis with reprioritization simulation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Build a temporal dataset of confirmed PPIs over decades, analyze growth patterns to detect a 'bias of locality' (discoveries preferentially expand from recently studied proteins), and apply reprioritization heuristics to identify overlooked candidate PPIs. Evaluate whether reprioritization would have enabled earlier discovery of certain interactions by comparing predicted priorities against historical discovery times.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Strength of locality bias (tendency to explore near-recently-studied proteins), ability of reprioritization methods to surface high-value but overlooked PPIs earlier, and empirical demonstration that debiasing could lead to earlier discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Molecular biology / proteomics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Prioritization heuristics for candidate interactions (not generation of explanatory mechanistic theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Temporal analysis revealed a significant 'bias of locality' in PPI exploration; reprioritization methods showed that earlier discoveries could have been made when using debiasing approaches. Specific quantitative results are reported in the referenced study but not enumerated in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated empirical analysis of historical PPI data and simulated reprioritization outcomes; conclusions are drawn from observed historical patterns rather than human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical historical data analysis comparing reprioritization rankings to actual discovery timelines; method validated by showing alignment that would have allowed earlier discovery in retrospective analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Domain-specific (PPIs); depends on completeness and biases present in PPI databases; translating reprioritized candidates into real-world experimental discovery requires resources; attention signals may not capture all motivations behind research choices.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Growing PPI databases (historical PPI corpora) assembled for the temporal analysis (specific database names not exhaustively listed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4614.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4614.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extraction-accuracy eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation of NLP extraction accuracy for task-aligned scientific representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluations and dataset construction efforts to measure extraction accuracy and cross-document concept similarity for scientific NLP systems that extract structured representations (causal mechanisms, concepts, facets) from full papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scico: Hierarchical cross-document coreference for scientific concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Extraction accuracy and cross-document similarity evaluation using annotated datasets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Construct annotated datasets and gold standards for tasks like hierarchical cross-document coreference, concept similarity, and extraction of mechanisms; evaluate models using standard automated metrics (precision, recall, F1) and measure alignment with human judgments for concept similarity and aspect matching. Use these evaluations to assess suitability of representations for downstream tasks (e.g., analogy mining, problem-graph construction).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Extraction accuracy (precision/recall/F1), concept similarity alignment with human judgments, generalization across domains, robustness to lexical variability and ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific NLP across domains (computational linguistics applied to scientific texts)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Structured representation extraction (causal mechanisms, concept facets), not evaluation of LLM-generated scientific theories per se</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported that current models' extraction accuracy is limited; models tend to focus on surface lexical patterns rather than deeper semantic relations; dataset and model-building efforts ([2], [23]) initiated to quantify and address these limitations. No single consolidated numeric benchmark provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated evaluation metrics (precision/recall/F1) against human-annotated gold standards; alignment analyses with human judgments for concept similarity tasks are described as part of validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Creation of annotated datasets and use of human annotations as gold standards to validate model outputs; comparisons to human judgments used to validate similarity measures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Difficulty of full-document understanding (tables, figures, equations), term normalization, ambiguity and hierarchical relationships in scientific language, and limited interpretability/control 'hooks' in LLM-derived representations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>New datasets mentioned (e.g., SCICO/Scico hierarchical cross-document coreference, S2ORC as open-access corpus) and other annotated resources created by the authors; no single standardized benchmark exclusively for theory-evaluation is presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery <em>(Rating: 2)</em></li>
                <li>Solvent: A mixed initiative system for finding analogies between research papers. <em>(Rating: 2)</em></li>
                <li>Accelerating innovation through analogy mining <em>(Rating: 2)</em></li>
                <li>A search engine for discovery of scientific challenges and directions <em>(Rating: 2)</em></li>
                <li>Literature-augmented clinical outcome prediction <em>(Rating: 2)</em></li>
                <li>On biases of attention in scientific discovery <em>(Rating: 2)</em></li>
                <li>Scico: Hierarchical cross-document coreference for scientific concepts <em>(Rating: 2)</em></li>
                <li>Accord: A multi-document approach to generating diverse descriptions of scientific concepts <em>(Rating: 1)</em></li>
                <li>Multi-vector models with textual guidance for fine-grained scientific document similarity <em>(Rating: 1)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4614",
    "paper_id": "paper-6c5c6f883604a3abaa829b83d2958de8c343beeb",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Problem-graph inspiration eval",
            "name_full": "Hierarchical problem-graph inspiration evaluation",
            "brief_description": "Human-subject evaluation of an NLP method that retrieves diverse problem perspectives from a mined hierarchical problem graph to stimulate creative reformulation; judged on usefulness and novelty of retrieved 'inspirations' and compared to IR baselines.",
            "citation_title": "Scaling creative inspiration with fine-grained functional facets of product ideas.",
            "mention_or_use": "use",
            "evaluation_method_name": "Human-subject ideation evaluation with usefulness & novelty ratings",
            "evaluation_method_description": "Controlled human experiments in which participants supply a focal problem; the system retrieves neighboring problems from a hierarchical problem graph mined from invention texts. Participants view retrieved 'inspirations' (vs. baseline retrieval methods) and rate or judge whether each inspiration is useful and novel for generating new ideas. The main measurement is the fraction of inspirations judged both useful and novel and comparative improvement relative to baselines.",
            "evaluation_criteria": "Usefulness (practical value for problem-solving), novelty (degree of newness vs. known directions), creativity/ideation support; implicit criteria include relevance and abstraction level.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Technological inventions / engineering / innovation",
            "theory_type": "Idea/inspiration retrieval (analogical/problem-reformulation suggestions), not formal scientific theories",
            "human_comparison": false,
            "evaluation_results": "Over 60% of retrieved 'inspirations' were judged useful and novel; this represented a relative boost of approximately 50â€“60% over the best-performing baseline retrieval methods reported in the study.",
            "automated_vs_human_evaluation": "Human-based evaluation: controlled user/ideation studies with human judgments of novelty and usefulness; comparisons were made against automatic baseline retrieval methods but the assessment of outputs used human raters.",
            "validation_method": "Controlled experiments with human participants and baseline comparisons; validation comprised measuring differences in human judgments between conditions (no detailed inter-rater statistics provided in this paper).",
            "limitations_challenges": "Uses user-provided problem description as a proxy for inner knowledge; limited information on inter-rater reliability; generalization across domains and abundance of lexical variability in scientific language are noted challenges; extraction errors and representation normalization can affect retrieval quality.",
            "benchmark_dataset": "A mined corpus of technological invention texts / patents (unnamed in paper) used to build the hierarchical problem graph; comparisons against standard IR baselines.",
            "uuid": "e4614.0",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Bridger evaluation",
            "name_full": "Bridger author-matching creativity evaluation",
            "brief_description": "User-study evaluation of Bridger, a system that recommends unfamiliar authors (and their salient problems/methods) to researchers to stimulate novel directions; assessed against state-of-the-art neural retrieval models.",
            "citation_title": "Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery.",
            "mention_or_use": "use",
            "evaluation_method_name": "Author-match creativity user study (comparative human-subject evaluation)",
            "evaluation_method_description": "Controlled studies with computer science researchers comparing Bridger's recommendations to those from a state-of-the-art neural model (Semantic Scholar engine). Measures included researchers' reported inspiration, discovery of useful cross-domain connections, and qualitative examples of novel directions generated after exploring recommended authors. Inner world representation was proxied via term-frequency mentions extracted from users' papers.",
            "evaluation_criteria": "Creativity/inspiration (ability to surface novel, relevant authors and problem-method linkages), relevance to user's interests, potential to mitigate fixation biases.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer science / cross-disciplinary research",
            "theory_type": "Idea-generation and author-recommendation (not explicit scientific theories)",
            "human_comparison": false,
            "evaluation_results": "Bridger 'dramatically boosted' creative search and inspiration relative to the state-of-the-art neural retrieval baseline; qualitative examples include discovery of cross-domain connections (e.g., graph theory to human-centered AI). No numeric performance metrics reported in this paper.",
            "automated_vs_human_evaluation": "Human-based: user studies with domain researchers who judged usefulness and novelty; comparisons were to automated baseline systems but assessment relied on human reports and follow-up examples.",
            "validation_method": "User studies with CS researchers and comparisons against outputs from existing neural search engines (Semantic Scholar); validation described via human judgments and qualitative case examples.",
            "limitations_challenges": "Inner-world representation used simple proxies (term-frequency from users' papers); limited personalization; evaluation lacks detailed quantitative metrics and inter-rater statistics; potential biases in participants and ecological validity concerns.",
            "benchmark_dataset": "Semantic Scholar corpus used as the outer-world knowledge base and the baseline retrieval engine for comparison.",
            "uuid": "e4614.1",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Solvent analogical eval",
            "name_full": "Solvent mixed-initiative analogy retrieval evaluation",
            "brief_description": "Evaluation of an analogical retrieval system that finds structurally similar inventions/papers to facilitate analogical transfer, measured via human ideation experiments that quantify boosts in creativity.",
            "citation_title": "Solvent: A mixed initiative system for finding analogies between research papers.",
            "mention_or_use": "use",
            "evaluation_method_name": "Analogical retrieval ideation experiment",
            "evaluation_method_description": "Participants are given a textual description of an invention/problem and are shown inspirations retrieved by the system (versus baseline IR). Participants then perform ideation tasks; outputs are scored on creativity metrics (e.g., novelty, usefulness) by judges or by predefined scoring rubrics. The study reports whether viewing the retrieved analogies increases measures of human creativity in the ideation task.",
            "evaluation_criteria": "Creativity (novelty of new ideas), usefulness/relevance of inspirations for solving the focal problem, measurable increase in ideation performance.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Engineering / product design / technology transfer",
            "theory_type": "Analogical inspirations and mechanism-level idea transfer (not formal theories)",
            "human_comparison": false,
            "evaluation_results": "Method produced a significant boost in measures of human creativity in ideation experiments compared with baseline information retrieval methods; specific qualitative examples cited (e.g., biomechanical lab finding inspiration in civil engineering literature). No precise numeric scores provided in this paper.",
            "automated_vs_human_evaluation": "Human-based evaluation: ideation experiments scored by humans; retrieval algorithms compared automatically but success measured via human outcomes.",
            "validation_method": "Controlled human experiments with baseline comparisons; validation by showing statistically significant improvements in human creativity measures (specific stats not reported here).",
            "limitations_challenges": "Challenges include mapping structural (mechanistic) similarity beyond lexical cues, normalization across domains, and constructing sufficiently rich representations of mechanisms; extraction noise and limited generalization are noted.",
            "benchmark_dataset": "Database of technological inventions/patents used as the outer-world corpus (not given a standardized public dataset name in this paper).",
            "uuid": "e4614.2",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Challenges-search eval",
            "name_full": "Search engine for discovery of scientific challenges and directions evaluation",
            "brief_description": "Evaluation of a search engine designed to retrieve statements of uncertainty, open questions and hypotheses from the literature; compared to PubMed in experiments including medical doctors.",
            "citation_title": "A search engine for discovery of scientific challenges and directions.",
            "mention_or_use": "use",
            "evaluation_method_name": "Uncertainty-and-hypothesis retrieval evaluation (human-subject comparison to PubMed)",
            "evaluation_method_description": "Using query topics as proxies for participants' interests, the system retrieves documents or passages that explicitly state uncertainties, open questions, or hypotheses. Human participants (including clinicians) compare results from this prototype versus PubMed search for the same queries; evaluators judge which system better surfaces important and interesting challenges or research directions.",
            "evaluation_criteria": "Ability to surface statements of uncertainty, open questions, initial hypotheses; importance and interest to domain experts; relevance to user's query/interest.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedical / clinical research (evaluated with ACE2/COVID-19 example)",
            "theory_type": "Discovery of open questions and hypotheses (not formal theoretical generation)",
            "human_comparison": false,
            "evaluation_results": "Prototype 'dramatically outperformed' PubMed at discovering important and interesting areas of challenges and directions according to participant judgments; example output included identification of hypotheses linking ACE2 to liver damage in COVID-19 patients. No quantitative metrics reported in this paper.",
            "automated_vs_human_evaluation": "Human-based evaluation: domain experts and participants judged outputs from both systems; comparison against an established automated baseline (PubMed), but assessment relies on human judgments.",
            "validation_method": "User studies with participants from diverse research backgrounds, including medical doctors; validation via comparative human judgments of retrieved items.",
            "limitations_challenges": "Evaluation uses query topics as a proxy for inner world knowledge (imperfect); ground-truth for 'important challenges' is subjective; lack of quantitative metrics and potential sampling bias in participants.",
            "benchmark_dataset": "PubMed used as baseline retrieval system and corpus; the prototype operated over biomedical literature (corpus details not exhaustively specified here).",
            "uuid": "e4614.3",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "BEEP eval",
            "name_full": "Biomedical Evidence Enhanced Prediction (BEEP) evaluation",
            "brief_description": "Evaluation of BEEP, a literature-augmented clinical prediction system that retrieves medical papers relevant to individual ICU patients and synthesizes that evidence with EMR data to predict patient outcomes, compared to state-of-the-art models without literature retrieval.",
            "citation_title": "Literature-augmented clinical outcome prediction.",
            "mention_or_use": "use",
            "evaluation_method_name": "Literature-augmented clinical prediction evaluation (predictive performance comparison)",
            "evaluation_method_description": "BEEP uses retrieval of medical literature conditioned on each patient's EMR-derived 'queries' and integrates retrieved evidence with internal EMR features to make predictions (e.g., in-hospital mortality, prolonged length of stay). Evaluation compares predictive performance (standard clinical outcome metrics) against state-of-the-art predictive models that do not use literature retrieval, measured on held-out clinical data.",
            "evaluation_criteria": "Predictive accuracy for clinical outcomes (e.g., in-hospital mortality, length of stay); improvement in predictive performance when augmenting EMR with literature; alignment of model outputs with clinician-understandable evidence.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Clinical medicine / biomedical informatics",
            "theory_type": "Predictive models / evidence-augmented predictions (not generation of explanatory theories)",
            "human_comparison": false,
            "evaluation_results": "BEEP provided 'large improvements' over state-of-the-art models that do not use retrieval from the literature; exact numeric performance metrics are not reported in this paper.",
            "automated_vs_human_evaluation": "Automated evaluation using predictive metrics on clinical datasets (likely held-out test sets); also includes alignment checks to inner-world representations (e.g., matching patient aspects to cohorts in papers) which involve human-interpretable evidence but not a formal human-vs-system theory comparison.",
            "validation_method": "Empirical evaluation on clinical EMR datasets with comparisons to baseline predictive models; validation described as improved predictive performance though specific validation statistics are not provided here.",
            "limitations_challenges": "Proxy for clinicians' inner world uses extracted queries from clinical notes (imperfect); data quality, access and privacy constraints; gap between system's literature understanding and physicians' expectations; lack of reported detailed quantitative metrics in this paper.",
            "benchmark_dataset": "Clinical EMR data from ICU cohorts used for prediction experiments (precise dataset name not specified in this paper).",
            "uuid": "e4614.4",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "PPI bias reprioritization",
            "name_full": "Protein-protein interaction (PPI) attention-bias and reprioritization evaluation",
            "brief_description": "Temporal analysis of the growing graph of confirmed PPIs to quantify biases of locality in scientific attention and evaluate reprioritization methods that surface less-studied candidate PPIs.",
            "citation_title": "On biases of attention in scientific discovery.",
            "mention_or_use": "use",
            "evaluation_method_name": "Temporal network analysis with reprioritization simulation",
            "evaluation_method_description": "Build a temporal dataset of confirmed PPIs over decades, analyze growth patterns to detect a 'bias of locality' (discoveries preferentially expand from recently studied proteins), and apply reprioritization heuristics to identify overlooked candidate PPIs. Evaluate whether reprioritization would have enabled earlier discovery of certain interactions by comparing predicted priorities against historical discovery times.",
            "evaluation_criteria": "Strength of locality bias (tendency to explore near-recently-studied proteins), ability of reprioritization methods to surface high-value but overlooked PPIs earlier, and empirical demonstration that debiasing could lead to earlier discoveries.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Molecular biology / proteomics",
            "theory_type": "Prioritization heuristics for candidate interactions (not generation of explanatory mechanistic theories)",
            "human_comparison": false,
            "evaluation_results": "Temporal analysis revealed a significant 'bias of locality' in PPI exploration; reprioritization methods showed that earlier discoveries could have been made when using debiasing approaches. Specific quantitative results are reported in the referenced study but not enumerated in detail in this paper.",
            "automated_vs_human_evaluation": "Automated empirical analysis of historical PPI data and simulated reprioritization outcomes; conclusions are drawn from observed historical patterns rather than human raters.",
            "validation_method": "Empirical historical data analysis comparing reprioritization rankings to actual discovery timelines; method validated by showing alignment that would have allowed earlier discovery in retrospective analysis.",
            "limitations_challenges": "Domain-specific (PPIs); depends on completeness and biases present in PPI databases; translating reprioritized candidates into real-world experimental discovery requires resources; attention signals may not capture all motivations behind research choices.",
            "benchmark_dataset": "Growing PPI databases (historical PPI corpora) assembled for the temporal analysis (specific database names not exhaustively listed in this paper).",
            "uuid": "e4614.5",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Extraction-accuracy eval",
            "name_full": "Evaluation of NLP extraction accuracy for task-aligned scientific representations",
            "brief_description": "Evaluations and dataset construction efforts to measure extraction accuracy and cross-document concept similarity for scientific NLP systems that extract structured representations (causal mechanisms, concepts, facets) from full papers.",
            "citation_title": "Scico: Hierarchical cross-document coreference for scientific concepts.",
            "mention_or_use": "use",
            "evaluation_method_name": "Extraction accuracy and cross-document similarity evaluation using annotated datasets",
            "evaluation_method_description": "Construct annotated datasets and gold standards for tasks like hierarchical cross-document coreference, concept similarity, and extraction of mechanisms; evaluate models using standard automated metrics (precision, recall, F1) and measure alignment with human judgments for concept similarity and aspect matching. Use these evaluations to assess suitability of representations for downstream tasks (e.g., analogy mining, problem-graph construction).",
            "evaluation_criteria": "Extraction accuracy (precision/recall/F1), concept similarity alignment with human judgments, generalization across domains, robustness to lexical variability and ambiguity.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Scientific NLP across domains (computational linguistics applied to scientific texts)",
            "theory_type": "Structured representation extraction (causal mechanisms, concept facets), not evaluation of LLM-generated scientific theories per se",
            "human_comparison": null,
            "evaluation_results": "Reported that current models' extraction accuracy is limited; models tend to focus on surface lexical patterns rather than deeper semantic relations; dataset and model-building efforts ([2], [23]) initiated to quantify and address these limitations. No single consolidated numeric benchmark provided in this paper.",
            "automated_vs_human_evaluation": "Automated evaluation metrics (precision/recall/F1) against human-annotated gold standards; alignment analyses with human judgments for concept similarity tasks are described as part of validation.",
            "validation_method": "Creation of annotated datasets and use of human annotations as gold standards to validate model outputs; comparisons to human judgments used to validate similarity measures.",
            "limitations_challenges": "Difficulty of full-document understanding (tables, figures, equations), term normalization, ambiguity and hierarchical relationships in scientific language, and limited interpretability/control 'hooks' in LLM-derived representations.",
            "benchmark_dataset": "New datasets mentioned (e.g., SCICO/Scico hierarchical cross-document coreference, S2ORC as open-access corpus) and other annotated resources created by the authors; no single standardized benchmark exclusively for theory-evaluation is presented in this paper.",
            "uuid": "e4614.6",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery",
            "rating": 2
        },
        {
            "paper_title": "Solvent: A mixed initiative system for finding analogies between research papers.",
            "rating": 2
        },
        {
            "paper_title": "Accelerating innovation through analogy mining",
            "rating": 2
        },
        {
            "paper_title": "A search engine for discovery of scientific challenges and directions",
            "rating": 2
        },
        {
            "paper_title": "Literature-augmented clinical outcome prediction",
            "rating": 2
        },
        {
            "paper_title": "On biases of attention in scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Scico: Hierarchical cross-document coreference for scientific concepts",
            "rating": 2
        },
        {
            "paper_title": "Accord: A multi-document approach to generating diverse descriptions of scientific concepts",
            "rating": 1
        },
        {
            "paper_title": "Multi-vector models with textual guidance for fine-grained scientific document similarity",
            "rating": 1
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "rating": 1
        }
    ],
    "cost": 0.017932749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Computational Inflection for Scientific Discovery</h1>
<p>Tom Hope<br>tomh@allenai.org<br>The Allen Institute for AI<br>The Hebrew University of Jerusalem</p>
<p>Doug Downey<br>dougd@allenai.org<br>The Allen Institute for AI<br>Northwestern University</p>
<h2>Oren Etzioni</h2>
<p>oren@allenai.org
The Allen Institute for AI</p>
<h2>Daniel S. Weld</h2>
<p>danw@allenai.org
The Allen Institute for AI
The University of Washington</p>
<h2>Eric Horvitz</h2>
<p>horvitz@microsoft.com
Office of the Chief Scientific Officer
Microsoft
of possibility. The way we search through and reflect about information across the vast space-the areas we select to explore, and how we explore them-is hindered by cognitive biases [26] and lacks principled and scalable tools for guiding our attention [32]. "Unknowns" are not just holes in science, but important gaps in personal knowledge about the broader knowns across the sciences.</p>
<p>We thus face an imbalance between the treasure trove of scholarly information and our limited ability to reach into it. Despite technological advances, we require new paradigms and capabilities to address this widening gap. We see promise in developing new foundational capabilities that address the cognitive bottleneck, aimed at extending human performance on core tasks of researche.g., keeping abreast with developments, forming and prioritizing ideas, conducting experiments, reading and understanding papers (see Table 1). We focus on a research agenda we call task-guided scientific knowledge retrieval, in which systems counter humans' bounded capacity by ingesting corpora of scientific knowledge and retrieving inspirations, explanations, solutions and evidence synthesized to directly serve task-specific utility. We present key concepts of task-guided scientific knowledge retrieval, including work on prototypes that highlight the promise of the direction and bring into focus concrete steps forward for novel representations, tools, and services. In Section 4 we review systems that help researchers discover novel perspectives and inspirations [8, 9, 11, 29], help guide the attention of researchers toward opportunity areas rife with uncertainties and unknowns [18, 32], and models that leverage retrieval and synthesis of scientific knowledge as part of machine learning and prediction [6, 24]. We conclude in Section 5 with a discussion of opportunities ahead with computational approaches that have the potential to revolutionize science.</p>
<p>To set the stage, in the following section we begin by discussing some fundamental concepts and background for our research agenda.</p>
<h2>3 HUMAN-CENTRIC PERSPECTIVE</h2>
<p>Extraordinary developments at the convergence of AI and scientific discovery have emerged in specific areas, including new kinds of analytical tools, with the prominent example of AlphaFold, which harnesses deep neural models to dramatically improve the prediction of protein structure from amino acid sequence information [15]. Large language models (LLMs) have very recently made stellar progress in the ability to reason about complex tasks, including in the medical domain [25]. The most advanced LLM at presentemerging before the ink has dried on this paper-is GPT-4, which</p>
<table>
<thead>
<tr>
<th>Task/Activity</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Attention to areas of interest</td>
<td>A background process of keeping track of latest developments in relevant scientific communities.</td>
</tr>
<tr>
<td></td>
<td>Involves applying selective attention, perceiving relevance and utility.</td>
</tr>
<tr>
<td>Problem identification &amp; prioritization</td>
<td>Identifying new research questions and deciding on which ones to work. Involves factors such as</td>
</tr>
<tr>
<td></td>
<td>subjective preferences and assessment of feasibility.</td>
</tr>
<tr>
<td>Forming directions</td>
<td>Given a problem/question, forming ideas to address it. Involves cognitive processes such as</td>
</tr>
<tr>
<td></td>
<td>constructing mental models of a problem, problem reformulation, abstraction and decomposition,</td>
</tr>
<tr>
<td></td>
<td>adaptation of relevant knowledge to new scenarios, and assessing likelihood of success.</td>
</tr>
<tr>
<td>Literature search &amp; review</td>
<td>Accessing and ingesting knowledge in the literature. Involves many processes such as query</td>
</tr>
<tr>
<td></td>
<td>formulation, skimming and assessing relevance, positioning ideas with relations and contrasts to</td>
</tr>
<tr>
<td></td>
<td>existing work, and reading and summarization strategies.</td>
</tr>
<tr>
<td>Learning, understanding, sense-making</td>
<td>The cognitive processes and activities involved in assimilating new knowledge and concepts, and</td>
</tr>
<tr>
<td></td>
<td>making sense of complex scientific information spaces.</td>
</tr>
<tr>
<td>Experimentation, analysis, action</td>
<td>A broad category referring to the many processes and activities involved in formulating and</td>
</tr>
<tr>
<td></td>
<td>conducting experiments (e.g., planning data collection and measurements), performing analyses</td>
</tr>
<tr>
<td></td>
<td>(e.g., understanding a set of data points, modeling and extrapolation, prediction, evaluation), and</td>
</tr>
<tr>
<td></td>
<td>producing artifacts, techniques, theories, decisions, policies, actions.</td>
</tr>
<tr>
<td>Research communication</td>
<td>Writing research documents (papers, proposals, analyses), communicating with peers (feedback</td>
</tr>
<tr>
<td></td>
<td>and review, collaboration, presentation).</td>
</tr>
</tbody>
</table>
<p>Table 1: Research may be decomposed into salient tasks that are prime targets for computational augmentation (Â§ 4).</p>
<p>has exhibited jaw-dropping skill at handling clinical questions, mathematical problems and computer coding tasks [1].</p>
<p>We view these developments as tremendous research opportunities for building computational approaches that accelerate scientific discovery. We take a human-centered, cognitive perspective: augmenting researchers by taking into account the diversity of tasks, contexts, and cognitive processes involved in consuming and producing scientific knowledge. Collectively, we refer to these as the inner cognitive world of a researcher (see Figure 1). The researcher interacts with the scientific ecosystemâ€”literature, resources, discussionsâ€”in order to inform decisions and actions. Researchers have different uses for scholarly information, depending on the task at hand and the stage of exploration (see Table 1 and discussion in Section 4). We pursue a research agenda around assisting researchers in their tasks, guided by two main desiderata:</p>
<p>(1) Systems for augmenting human capabilities in the sciences need to enhance the effective flow of knowledge from the outer world of scientific information and discourse to the researcherâ€™s inner cognitive worldâ€”countering humansâ€™ bounded capacity by retrieving and synthesizing information targeted to enhance performance on tasks. Achieving this goal requires methods that build and leverage rich representations of scientific content and that can align computational representations with human representations, in the context of specific tasks and backgrounds of researchers.</p>
<p>(2) Research on such systems should be rooted in conceptual models of the inner cognitive world of a researcher. Shining a spotlight on this inner world brings numerous factors and questions to the fore. How do researchers form ideas? How do they decide which problems to look into? How do they find and assimilate new information in the process of making decisions? What cognitive representations and bottlenecks are involved? What computing services would best augment these processes?</p>
<p>Figure 1: Information flows from the outer world into the inner cognitive world of researchers, constrained by cognitive capacity and biases. We see opportunities to support researchers by retrieving knowledge that helps with tasks across multiple phases of the scientific process (Table 1).</p>
<p>Background and Related Themes. We leverage research in natural language processing, information retrieval, data mining and human-computer interaction and draw concepts from multiple disciplines. For example, efforts in metascience focus on sociological factors that influence the evolution of science [17], e.g., analyses of information silos that impede mutual understanding and interaction [38], analyses of macro-scale ramifications of the rapid growth in scholarly publications [4], and of current metrics for measuring impact [5] â€” work enabled by digitization of scholarly corpora (see Section 3.1). Metascience research makes important observations about human biases (desideratum 2) but generally does not engage in building computational interventions to augment researchers (desideratum 1). Conversely, work in literature-based discovery [33] mines information from literature to generate new predictions (e.g., functions of materials or drug targets) but is typically done in isolation from cognitive considerations; however, these techniques have great promise in being used as part of human-augmentation</p>
<p>(2) Research on such systems should be rooted in conceptual models of the inner cognitive world of a researcher. Shining a spotlight on this inner world brings numerous factors and questions to the fore. How do researchers form ideas? How do they decide which problems to look into? How do they find and assimilate new information in the process of making decisions? What cognitive representations and bottlenecks are involved? What computing services would best augment these processes?</p>
<p>systems (see Sections 4-5). Other work uses machines to automate aspects of science. Pioneering work from Herbert Simon and Pat Langley automated discovery of empirical laws from data, with models inspired by cognitive mechanisms of discovery (see Section 3.2). More recent work has focused on developing robot scientists [16, 30] that run certain experiments in biology or chemistry-not only formulating hypotheses but "closing the loop" by automated tests in a physical laboratory-where robots may use narrow curated background knowledge (e.g., of a specific gene regulatory network) and machine learning to guide new experiments. Related work explores automating scientific data analysis [6], which we discuss in Section 4 as a case of retrieval from scientific repositories to augment aspects of experimentation and analysis (see Table 1).</p>
<p>We now turn to a discussion of central concepts: the ecosystem of science, and the cognitive world. This presentation lays the foundations for our exposition of task-guided retrieval in Section 4 and the research opportunities in Section 5.</p>
<h3>3.1 Outer World: Scientific Ecosystem</h3>
<p>We collectively name the scientific ecosystem and the digital representations of scientific knowledge as the outer world (see Figure 1). The outer world is comprised of scientific communities, a complex and shifting web of peers, concepts, methodologies, problems and directions revolving around shared interests, understandings and paradigms. This ecosystem generates digital information-digital "traces" of scientific thought and behavior-lying at the center of our attention as computer scientists interested in boosting human capacity to "reach into" the pool of scientific knowledge. This knowledge includes scholarly publications that appear in journals, conference proceedings, and online preprint repositories. Online publications are a main case of digital research artifacts; other examples of products of research include software, datasets, knowledge bases. Research artifacts are also associated typically with signals of quality and interest, such as citations to a specific paper or downloads of a dataset. The specific context of why a paper or resource was cited or used is often reflected in natural language descriptions. Different types of signals include peer review prior to publication (mostly not shared publicly), and social media discussions such as on Twitter, which has become a major virtual platform for academic dissemination and conversation. Along with the trend in society, private communication channels among researchers are also digital-mails, online calls and messages. Similarly, note taking and writing-important activities across the scientific workflow-are done in digital form. This information is siloed in different platforms under privacy restrictions, yet represents a treasure trove for tools for the augmentation of scientific reasoning and exploration.</p>
<h3>3.2 Inner World: Human Cognition in Science</h3>
<p>The way researchers decide to interact with information in the outer world and the way they process and use this information is governed by a complex array of cognitive processes, personal knowledge and preferences, biases and limitations, which are only partially understood. We collectively name these the inner world, and briefly discuss several salient aspects.</p>
<p>Early work in AI by Herbert Simon and Alan Newell and later efforts by Pat Langley and Paul Thagard focused on cognitive and
computational aspects of problem solving, creativity, decision making and scientific reasoning and discovery, seeking algorithmic representations to help understand and mimic human intelligence [19, 36]. Cognitive mechanisms that play important roles in scientific discovery include inductive and abductive reasoning, mental modeling of problems and situations, abstraction, decomposition, reformulation, analogical transfer and recombination; for example, in analogical transfer, given a situation or problem being considered in our working memory, we retrieve from our long-term memory prior analogous problems or situations.</p>
<p>This cognitive machinery powers humans' ingenuity. However, the human mind also has severe limitations-bounded rationality in the words of Simon-that impede these powerful mechanisms. Our limitations and capabilities have been studied for over a hundred years with cognitive psychology. Our limitations manifest in bounded cognitive capacity and knowledge, and biases that govern our behaviors and preferences. These limitations are all tightly interrelated. The ability to generate ideas, for instance, directly relies on prior knowledge; but, when a large volume of information from the outer world of science is met by insufficient cognitive capacity for processing and assimilating it, the result is information overload-a ubiquitous hindrance for researchers [29]. Information overload in science strains the attentional resources of researchers, and forces researchers to allocate attention to increasingly narrow areas. This effect, in turn, amplifies a host of biases which researchers, just like all humans, suffer from [26, 32]. For example, scientists can be limited by confirmation bias, aversion to information from novel domains, homophily, and fixation on specific directions and perspectives without consideration of alternative views [11, 26]. More broadly, selection of directions and areas to work on is a case of decision-making, and as such personal preference and subjective utility play fundamental roles. Our research decisions rely on subjective assessment of feasibility, long-term or short-term goals and interests, and even psychological factors (e.g., tendencies for risk aversion). These factors are of course also impacted by biases [26].</p>
<p>Clearly, the inner world of researchers is dauntingly complex. However, in the next section, we present encouraging results of applying computational methods to augment cognition in the sciences, helping to mitigate biases and limitations and enabling researchers to make better use of their powerful creative mechanisms.</p>
<h2>4 TASK-GUIDED RETRIEVAL</h2>
<p>How might we widen and deepen the connection between the outer world of science and researchers' limited cognitive worlds? We see a key bridge and research opportunity with developing tools for scientific task-guided knowledge retrieval. In this section, we discuss our vision and present initial work toward achieving it.</p>
<p>Drawing from discussions in literature on the process of scientific discovery, we enumerate in Table 1 salient scientific tasks and activities, such as problem identification, forming directions, learning, literature search and review, experimentation. These tasks could benefit from augmentation of human capabilities but remain underexplored in computer science. Existing computational technologies for assisting humans in discovering scientific knowledge are underinvested in important aspects of the intricate cognitive processes and goal-oriented contexts involved in scholarly endeavors.</p>
<p>The dominant approach to information retrieval research and systems can be summarized as "relevance first"-focusing on results that answer user queries as accurately as possible. Academic search engines assume users know what queries to explore and how to formulate them. For pinpointed literature search in familiar areas, this assumption may often suffice; but a broad array of other scholarly tasks, such as ideation or learning about a new topic, are very much underserved [9-11, 18, 29]. At the same time, many voices in the information retrieval community have discussed a different, broader view of utility-driven search situated in a wider context of information seeking by users with specific intents and tasks [31]. Here, we adapt ideas and principles from this general paradigm.</p>
<p>We envision methods for task-guided scientific knowledge retrieval: systems that retrieve and synthesize outer knowledge in a manner that directly serves a task-guided utility of a researcher, while taking into consideration the researcher's goals, state of inner knowledge, and preferences.</p>
<p>Consider the tasks in Table 1. For researchers engaged in experimentation or analysis, we envision systems that help users identify experiments and analyses in the literature to guide design choices and decisions. For researchers in early stages of selecting problems to work on, we picture systems that support this decision with information from literature and online discussions, synthesized to obtain estimated impact and feasibility. As part of forming directions to address a problem, systems will help users find inspirations for solutions. Researchers who are learning about a new topic will be provided with retrieved texts and discussions that explain the topic in a manner personally tailored to personal knowledge. Importantly, task-guided knowledge retrieval follows the two desiderata introduced in Section 3; namely, systems should enable users to find knowledge that directly assists them in core research tasks by augmenting their cognitive capacity and mitigating their biases, and computational representations and services should align with salient cognitive aspects of the inner world of researchers.</p>
<h3>4.1 Prototypes of Task-Guided Retrieval</h3>
<p>We present work on initial steps and prototypes, including representative work that we have done and the work of others, framed in alignment with task-guided knowledge retrieval and tasks enumerated in Table 1. The main aim of this brief review is to stimulate discussion in the computer science community on tools for extending human capabilities in the sciences. Existing methods are far from able to realize our vision. For example, we see major challenges in representation and inferences about the inner world of knowledge and preferences, and aligning these with representations and inferences drawn from the outer world knowledge. Today's prototypes are limited examples of our vision, using very rough proxies of inner knowledge and interest based on papers and documents written or read by the user, or in some cases only a set of keywords. We discuss these research challenges and others in Section 5.</p>
<p>Forming Directions. We have developed methods for helping researchers generate new directions. A fundamental pattern in the cognitive process of creativity involves detecting abstract connections across ideas and transferring ideas from one problem to another [36]. Grounded in this cognitive understanding, we have pursued several approaches for stimulating creativity powered by
retrieving outer knowledge. We developed and studied a system named Bridger that connects researchers to peers who inspire novel directions for research [29]. Bridger identifies matches among authors based on commonalities and contrasts, identifying peers who are both relevant and novel-working on similar problems but using very different methods, potentially inspiring new solutions. By doing so, Bridger helps mitigate the cognitive bias of fixation [11].</p>
<p>In this setting, inner knowledge is represented as mentions of problems and methods extracted automatically from a researcher's papers and weighted by term frequency. The outer knowledge being retrieved takes the form of other authors in computer science, following the same representation. For each retrieved author, the system displays salient problems, methods and papers, ranked by measures of relevance to the user. In studies with CS researchers, we found that Bridger dramatically boosted creative search and inspiration over state-of-art neural models employed by the Semantic Scholar search engine, surfacing useful connections across diverse areas; for example, one researcher drew novel connections between the mathematical area of graph theory and their own area of human-centered AI, by exploring a recommended author who applies graph theory to decision making. The studies also surfaced important challenges, discussed in Section 5.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Matching researchers to authors with whom they are unfamiliar, to help in generating directions. Author cards show key problems and methods extracted from their papers.</p>
<p>We have also explored retrieving outer knowledge to enhance the human ability to find opportunities for analogical transfer [3, 8]. Extensive work in cognitive studies has highlighted the human knack for "analogical retrieval" as a central function in creativitybringing together structurally related ideas and adapting them to a task at hand [36]. We developed a search method that enables researchers to search through a database of technological inventions and find mechanisms that can be transferred from distant domains to solve a given problem. Given as input from the user a textual description of an invention, we retrieve ideas (inventions, papers) that have partial structural similarity to the input (e.g., inventions with similar mechanisms), to facilitate discovery of analogical transfer opportunities. We found that the method could significantly boost measures of human creativity in ideation experiments, in which users were asked to formulate new ideas after viewing inspirations retrieved with our approach versus baseline information retrieval methods. For example, a biomechanical engineering lab working on polymer stretching/folding for creating novel structures found useful inspiration in a civil engineering paper on web crippling in steel beams-abstractly related to stretching and folding.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Using an extracted hierarchy of problems to retrieve new perspectives on a focal problem of interest.</p>
<p>Innovation may also involve traversing multiple levels of abstraction around a problem to "break out" of fixation on the details of a specific problem by exploring novel perspectives. Given as input a problem description written by the user (as a proxy summary of the user's inner world of knowledge and purpose), we have pursued mechanisms that can retrieve diverse problem perspectives that are related to the focal problem, with the goal of inspiring new ideas for problem abstraction and reformulation [11] (see Figure 3). Using NLP models to extract mentions of problems, we mine a corpus of technological invention texts to discover problems that often appear together, and use this information to form a hierarchical problem graph that supports automatic traversal of neighboring problems around a focal problem, surfacing novel inspirations to users. In a study of the efficacy of the methods, over $60 \%$ of "inspirations" retrieved this way were found to be useful and novel-a relative boost of $50-60 \%$ over the best-performing baselines. For example, given an input problem of reminding patients to take medication, our system retrieves related problems such as in patient health tracking and alerting devices.</p>
<p>Guiding attention and problem identification. We see great opportunity in developing methods for guiding the attention of researchers to important areas in the space of ideas where there exists less knowledge or certainty [18, 32] (Figure 4). In one direction, we built a search engine that allows users to retrieve outer knowledge in the form of difficulties, uncertainties and hypotheses in the literature. The key goals of this mode of search are to bolster attention to rising and standing challenges of relevance to the user so as to help overall with identification and selection of problems. We performed experiments with participants with diverse research backgrounds, including medical doctors working in a large hospital. Using query topics as a proxy for the inner world of participants' interests, we found the system could dramatically outperform PubMed search, the go-to biomedical search engine, at discovering important and interesting areas of challenges and directions. For example, while searching PubMed for the ACE2 receptor in the context of COVID19 returns well-studied results, the prototype system by contrast focuses on finding statements of uncertainty, open questions and initial hypotheses, like a paper noting the possibility that ACE2 plays a role in liver damage in COVID-19 patients.</p>
<p>Another direction on biases and blindspots considers the longterm effort to identify protein-protein interactions (PPIs). A dataset of the growing graph of confirmed PPIs over decades was constructed and leveraged to identify patterns of scientific attention [32]. A temporal analysis revealed a significant "bias of locality," where explorations of PPIs are launched more frequently from those that were most recently studied, rather than following more general prioritization of exploration. While locality reflects an understandable focus on adjacent and connected problems in the biosciences, the pattern of attention leads to systematic blindspots in large,</p>
<p>Input:
Items of interest (concepts, entities, topics...)
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Suggesting research opportunities for query concepts (e.g., medical topics) by identifying blindspots, gaps in collective knowledge and promising areas for exploration.
widely used PPI databases that are likely unappreciated-further exacerbating attentional biases. The study further demonstrated mechanisms for reprioritizing candidate PPIs based on properties of proteins, and showed how earlier discoveries could be made with use of the debiasing methods. The findings underscore the promise of tools that retrieve existing outer world knowledge to guide attention to worthwhile directions. In this case, the outer knowledge source is a PPI database, and a user-selected sub-graph provides a proxy for inner world knowledge and interests.</p>
<p>Literature search and review. A great body of work on literature search and review has deep relevance to task-guided retrieval in the sciences. In particular, we see great opportunity with building on recent advances in information retrieval to (1) help biomedical researchers with domain-specific representations and (2) enhance scientific search by building new neural models.</p>
<p>Specialized search systems have been developed for the biomedical domain, with the overall vision of harnessing natural language understanding technologies to help researchers discover relevant evidence and expedite the costly process of systematic literature review [27]. For example, Nye et al. [27] build a search and synthesis system based on automated extraction of biomedical treatmentoutcome relations from clinical trial reports. The system is found to assist in identification of drug repurposing opportunities. As another recent example, the SPIKE system enables researchers to extract and retrieve facts from a corpus using an expressive query language with biomedical entity types and new term classes that the user can define interactively [34]. Together, this work underscores the importance of extracting a semantically meaningful representation of outer world knowledge that aligns with core aspects of inner world reasoning by researchers (see Section 5).</p>
<p>In separate work, neural language models built via self-supervision on large corpora of biomedical publications have recently led to performance boosts and new features in literature search systems [39], such as support for natural language queries that provide users with a more natural way to formulate their informational goals. Neural models have also been trained to match abstract discourse aspects of pairs of papers (e.g., sentences referring to methodologies) and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Leveraging medical corpora to enhance the precision of AI models for inference about patient outcomes.
automatically retrieve documents that are aspectually similar [23]. By employing a representation that aligns with scientific reasoning across areas, this method achieves state-of-art results across biomedical and computer science literature.</p>
<p>Experimentation, analysis, and action. Beyond helping researchers via awareness and knowledge, we see great opportunities to use scientific corpora to construct task-centric inferential systems with automated models and tools for assisting with analysis, prediction and decisions. We demonstrate these ideas by casting two different lines of work as cases of task-guided retrieval.
(1) Workflows are multi-step computational pipelines used as part of scientific experimentation for data preparation, analysis and simulation [6]. Technically this includes execution of code scripts, services and tools, querying databases and submitting jobs to the cloud. In the life sciences, in areas such as genomics, there are specialized workflow management systems to help researchers find and use workflows, enabled by a community that creates and publicly shares repositories of workflows with standardised interfaces, metadata and functional annotations of tools and data. As discussed in Gil [6], machine learning algorithms can potentially use these resources to automate workflow construction, learning to retrieve and synthesize data analysis pipelines. In this setting, outer world knowledge takes the form of workflow repositories, from which systems retrieve and synthesize modular building blocks; users' inner world is reflected via analysis objectives and constraints.
(2) In our work on clinical predictions [24], the goal is to enhance prediction of medical outcomes of patients hospitalized in the intensive care unit (ICU), such as in-hospital mortality or prolonged length of stay. Our system, named BEEP (biomedical evidence enhanced prediction), learns to perform predictions by retrieving medical papers that are relevant to each specific ICU patient, and to synthesize this outer knowledge in combination with internal EMR knowledge to form a final prediction. The primary envisaged user is a practice-oriented researcher-a medical doctor, whose inner knowledge is given by a rough proxy in the form of internal clinical notes from which we extract "queries" issued over medical papers. We find BEEP to provide large improvements over state-of-art models that do not use retrieval from the literature. BEEP's output can
be aligned with inner world representations, e.g., matches between patient aspects and related cohorts in papers (see Figure 5).</p>
<p>Learning and understanding. We introduced a system [22] for helping users learn about new concepts by showing definitions grounded in familiar concepts; e.g., a new algorithm is explained as a variant of an algorithm familiar to the user. Cognitive studies have asserted that effective descriptions of a new concept ground it within the network of known concepts. Our system takes as input a list of source concepts reflecting the user's inner knowledge as obtained from papers that they have written or read. When the user seeks a definition of a new target concept, we retrieve outer knowledge in the form of definitions appearing in scientific papers in which the target concept is explained in terms of the source concepts; a neural text generation model then re-writes the text in a structured, template form that relates the target to the source.</p>
<h2>5. OPPORTUNITIES AHEAD</h2>
<p>The challenges of task-guided retrieval in support of researchers frame a host of problems and opportunities. We discuss select challenges and directions (see also Table 2). We begin with an illustrative example, imagining a futuristic system to motivate the discussion.</p>
<h3>5.1. Aspirations</h3>
<p>We envision tools that flow outer world knowledge to researchers based on inferences about their inner world-users' knowledge, past and present goals and difficulties, and the tasks from Table 1 they are engaged in. The systems would use multiple signals for making inferences, including users' papers, data, experiments and communication channels, and also converse with the user to understand needs and suggest solutions, hypotheses and experiments.</p>
<p>We foresee systems powered by rich representations of both inner and outer scientific knowledge. For a given concept, e.g., a certain algorithm or organism, an aspirational system would ingest all papers on the subject to form a multi-faceted representation of concepts as objects with associated properties and functions. Using this representation, the system could assist in literature search and review, enabling expressive queries to outer world information that target abstract aspects like functionalities, mechanisms, behaviors and designs in a manner that transcends field-specific jargon, abstracting away lexical differences that hindered historical search engines (e.g., Google Scholar). To help users learn and understand new concepts they encounter, the system would explain them in relation to other concepts the user already knows. A future system might also assist in automating experimentation, analysis and action and in forming directions, by forming compositions of concepts and predicting the resultant affordances; for example, matching a certain algorithm with a suitable problem based on the algorithm's properties and the problem's requirements, matching an organism with a specific method of measurement or modification, or recombining parts of two devices to form a new device. The system could help identify related problems in the literature, synthesizing from them useful suggestions for problem reformulations. Considering the huge combinatorial space of potential suggestions, a system could assist in prioritization using estimated measures of interestingness, feasibility and value by synthesizing historical and current signals in literature, online discussions and knowledge bases.</p>
<p>Envisioned systems would be designed as human-centric, focusing on the individual researcher. The systems would enable users to convey preferences, goals and interests, and mediate the presentation of suggested directions and problem solutions based on personal prior knowledge, proposing concrete new directions grounded in representations that researchers can follow, and assisting users in reading complex retrieved texts by editing their language to conform with concepts that users are familiar with.</p>
<h3>5.2 Research Directions</h3>
<p>While we have witnessed remarkable strides in AI, the journey towards actualizing our vision requires further advancement. Envisioning such capabilities, however, can serve as a compass for directing research endeavors. An encouraging development can be seen in the recent developments with large language models, which have demonstrated surprising capabilities with interpreting and generating complex texts and tackling technical tasks. The proficiencies demonstrated by these models instills confidence that many of the possibilities that we discussed are attainable. We now elaborate on challenges and directions ahead, including limitations in representing scientific knowledge and making inferences about the inner worlds of researchers (see Table 2).</p>
<p>Task-aligned representations and scientific NLP. Paul Thagard writes: "thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures". We seek representations that can be aligned with human thinking-for insight-building, decision making and communication. Can we go beyond textual representation toward representations that support such cognitive processes?</p>
<p>The quest for a universal schema representing scientific ideas goes back hundreds of years. Gottfried Leibniz and RenÃ© Descartes were intrigued by the prospects of a universal codification of knowledge. Leibniz proposed the characteristica universalis, a hypothesized formal language of ideas enabling inferences with algebraic operators. While such a representation is not within reach, envisioning its existence-and how to even roughly approximate it-points to important research directions. One exciting direction is obtaining representations that support a "computational algebra of ideas"e.g., modeling compositions of concepts and the affordances that would be formed as a result. Early work on learning vector representations of natural language concepts supported rudimentary forms of addition, subtraction, and analogy (e.g., the Word2vec model).</p>
<p>Recently, large language models (LLM) [28] have made striking progress in generating new content and coherently combining concepts. Emerging evidence on GPT-4's ability to reason not only in unstructured language but also with logical structures grounded in code, suggests strong potential for generating novel ideas via compositionality and relational reasoning [1]. Our early experiments with GPT-4 have revealed a constellation of promising abilities to assist with the scientific process, such as formulating hypotheses, recommending future research directions, and critiquing studies. Equipped with training and retrieval with access to millions of scientific papers, descendants of today's models may have an ability to synthesize original scientific concepts with the in-depth technical detail at a level reported in high-quality scientific papers. We see great opportunity ahead to leverage LLMs to augment human scientific reasoning along the lines described in this paper.</p>
<p>One limitation with LLMs is that representations learned by these models are currently far from understood and lack "hooks" for control and interpretability, which are important in human-AI collaboration. In line with our focus on grounding representations of outer world knowledge with inner world cognitive aspects, we have pursued methods that "reverse engineer" scientific papers to automatically extract, using NLP, structured representations that balance three desiderata:
(1) Semantically meaningful representations, aligned with a salient task from the tasks in Table 1, grounded in cognitive research to guide us toward useful structures.
(2) Representations with sufficient level of abstraction to generalize across areas and topics.
(3) Representations expressive enough for direct utility in helping researchers as measured in human studies.</p>
<p>For example, we have extracted representations of causal mechanisms and hierarchical graphs of functional relationships. This kind of decomposition of ideas has enabled us to perform basic analogical inference in the space of technological and scientific ideas, helping researchers discover inspirations (see Section 4). However, many richer structures should be explored (e.g., of experimentation processes and methodologies, to enable tasks in Table 1).</p>
<p>A central challenge is that current models' extraction accuracy is limited, and the diversity of scientific language leads to problems in generalization and normalization of terms and concepts. We have pursued construction of new datasets, models and evaluations for identifying similarity between concepts and aspects across papers [2, 23], with fundamental problems in resolving diversity, ambiguity and hierarchy of language. As our results have highlighted, models tend to focus on surface-level lexical patterns, rather than deeper semantic relationships. Generally, substantial advances are needed to handle challenges posed by scientific documents. We require NLP models with full-document understanding, not only of text but of tables, equations, figures, and reference links. Open access corpora (e.g., S2ORC [20]) provide a foundation to address this challenge.</p>
<p>New modes of writing and reading. Perhaps the way we write can be dramatically different, using machine-actionable representations? Beyond reporting and documentation, writing represents a channel between the inner and outer worlds, forcing us to communicate ideas in concrete language; this process often begets new questions and perspectives. Can systems accompany different phases of writing, suggesting new ideas? In parallel, there is the task of reading what others have written; a recent interactive PDF reader offers, for example, customized concept definitions [7]. We imagine a future where every reader will see a different form of the same paper, re-written to align with readers' knowledge; e.g., our personalized concept definitions system [22] (Â§4) will insert new wording and explanations grounded in readers' knowledge.</p>
<p>Internal world of researchers. Grounding new concepts in readers' knowledge, suggests a wider and highly challenging problem. How can we enable researchers to specify their knowledge and preferences to direct systems to carry out tasks? Directly querying for these aspects burdens the researcher and may be prone to reporting biases. Digital traces present an opportunity for automatically estimating a researcher's knowledge, objectives, needs and interests-based on data. We are interested in using researchers'</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Challenge</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task-aligned representations, scientific NLP</td>
<td style="text-align: left;">How can we automatically and accurately extract conceptual representations of scientific <br> knowledge, that are aligned with tasks that comprise the endeavor of science (Table 1)? How <br> can we build NLP models that understand full scientific papers?</td>
</tr>
<tr>
<td style="text-align: left;">Computational algebra of ideas</td>
<td style="text-align: left;">Can we build representations of scientific knowledge that support composition of ideas? e.g., <br> inferring the result of recombining two concepts.</td>
</tr>
<tr>
<td style="text-align: left;">Identifying conceptual relationships across <br> literature</td>
<td style="text-align: left;">How do we detect important relationships across scientific ideas, across related discussions <br> in different communities and areas? How can we resolve challenges of diversity, ambiguity, <br> and multiple levels of detail in scientific language?</td>
</tr>
<tr>
<td style="text-align: left;">Estimation of personal knowledge</td>
<td style="text-align: left;">How can we estimate the knowledge of a given researcher? What are useful, practical models <br> of this knowledge? What concepts does a researcher know, which of their aspects, and to <br> what technical extent? How do we account for latent knowledge?</td>
</tr>
<tr>
<td style="text-align: left;">Addressing gaps in knowledge</td>
<td style="text-align: left;">Given an estimated model of a researcher's knowledge, and given a specific task in Table 1, <br> what new knowledge would be useful for the task at hand?</td>
</tr>
<tr>
<td style="text-align: left;">Estimation of preferences, goals, interests</td>
<td style="text-align: left;">How can we estimate key latent preferences, interests and subjective utilities of researchers? <br> Using information in papers and discussions to infer factors behind researchers' choices.</td>
</tr>
<tr>
<td style="text-align: left;">Prediction and prioritization</td>
<td style="text-align: left;">How might we identify promising sparse/unexplored areas in large "spaces of ideas" and <br> prioritize directions that are novel, plausible and valuable?</td>
</tr>
<tr>
<td style="text-align: left;">Developing new representations for learning <br> and communicating</td>
<td style="text-align: left;">Might the way we read and write papers change to be more effective? Might we communicate <br> with machine-actionable, interlinked representations of scholarly knowledge. Might we <br> create personalized "living" documents that tailor their content to readers' backgrounds.</td>
</tr>
</tbody>
</table>
<p>Table 2: Directions with formulating and leveraging computational representations of scientific knowledge.
papers to estimate what concepts users know and to what extent. We envision mixed-initiative interfaces [12] in which approximations of the inner world are presented to researchers and refined in human-machine collaboration, to identify and fill personal gaps in knowledge for a specific task. Representations of interest and preference are central in web commerce based on user activity histories. We are encouraged by results highlighting the feasibility of rich user models, e.g., in search personalization [31, 35] and dynamic inferences [14]. Paul Samuelson wrote of "revealed preferences"preferences revealed indirectly by the economic price people are willing to pay; while not equivalent, researchers' digital traces may reveal preferences, e.g., by working on one problem and not another.</p>
<p>Prediction and prioritization of directions. Whenever we decide to work on a research direction, we are implicitly making a prediction about an area in "idea space". Can automated systems help make these predictions? This involves identifying promising areas and generating directions-hypotheses, ideas-in either natural or structured language, under constraints on users' background knowledge; directions should be ranked by estimated likelihood (feasibility, plausibility), utility and novelty. Despite the great challenges involved, we are encouraged by advances in models trained for predicting specific targets (e.g., protein structures [15]); we see potential in building on these advances as part of our wider agenda that considers the inner world of cognitive aspects and tasks, and the outer world outside the context of a narrow dataset.</p>
<p>Pursuing challenges of translation. Finally, we note challenges for introducing new technologies into scientific workflows. In the context of systems for discovery, researchers interviewed in our studies [29] reported being limited in time and resources, making them less likely to enter new areas and learn unfamiliar concepts, preventing them discovering potentially promising ideas. More broadly, the sociotechnical environment in which AI models
are deployed has critical impact on their success [13, 21]. A pertinent example comes via reports on difficulties with translating IBM's Watson Health systems into practice. The vision of the effort included systems providing insights about patients by mining research papers to suggest, e.g., therapies or diagnostics [21]. A prototype system faced difficulties ranging from data processing and availability problems to deeper perceived gaps between the system's understanding of literature and that of physicians [37]. Challenges such as these are fundamental to the fielding of new applications not only in healthcare but in any setting where humans are required to interact with AI systems [40]. While issues such as data quality and privacy are orthogonal to our agenda, we see directions in modeling of human needs and limitations to inform the design of human-AI experiences within scientific workflows.</p>
<h2>6 SUMMARY</h2>
<p>As the terrain of science widens at a fast pace, researchers are constrained by the limits of human cognition, and lack principled methods to follow developments, guide attention, and formulate and prioritize directions. For the first time in history, essentially all of scientific knowledge and discourse has moved into the digital space. At the time of this writing, dramatic advances in AI with large language models are taking place at breathtaking speed. These shifts present tremendous opportunities for leveraging scientific corpora as databases from which solutions, insights, and inspirations can be gleaned. We see opportunity ahead for systems that can address the imbalance between the treasure trove of scholarly information and researchers' limited ability to reach into it, harnessing humankind's collective knowledge to revolutionize the scientific process. Numerous challenges stand in the way of the vision we have laid out. However, even small steps forward will unlock vast opportunities for making advances at the frontiers of science.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank the members of the Semantic Scholar team for stimulating discussions. Projects were supported by NSF Convergence Accelerator Grant 2132318, NSF RAPID grant 2040196, and ONR grant N00014-18-1-2193.</p>
<h2>REFERENCES</h2>
<p>[1] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.
[2] Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug Downey, and Tom Hope. Scico: Hierarchical cross-document coreference for scientific concepts. Automated Knowledge Base Construction (AKBC) 2021, 2021.
[3] Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, and Aniket Kittur. Solvent: A mixed initiative system for finding analogies between research papers. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW):1-21, 2018.
[4] Johan SG Chu and James A Evans. Slowed canonical progress in large fields of science. Proceedings of the National Academy of Sciences, 118(41), 2021.
[5] Cristina GarcÃ­a-Villar. A critical review on altmetrics: can we measure the social impact factor? Insights into Imaging, 12(1):1-10, 2021.
[6] Yolanda Gil. Will AI write scientific papers in the future? AI Magazine, 2022.
[7] Andrew Head, Kyle Lo, Dongyeop Kang, Raymond Fok, Sam Skjonsberg, Daniel S. Weld, and Marti A. Hearst. Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 2021.
[8] Tom Hope, Joel Chan, Aniket Kittur, and Dafna Shahaf. Accelerating innovation through analogy mining. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017.
[9] Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric Horvitz, Daniel S Weld, Marti A Hearst, and Jevin West. Scisight: Combining faceted navigation and research group detection for covid-19 exploratory scientific search. In EMNLP, 2020.
[10] Tom Hope, Asla Amini, David Wadden, Madeleine van Zuylen, Sravanthi Parasa, Eric Horvitz, Daniel S Weld, Roy Schwartz, and Hannaneh Hajishirzi. Extracting a knowledge base of mechanisms from covid-19 papers. In NAACL, 2021.
[11] Tom Hope, Ronen Tamari, Hyeomu Kang, Daniel Hershcovich, Joel Chan, Aniket Kittur, and Dafna Shahaf. Scaling creative inspiration with fine-grained functional facets of product ideas. In CHI, 2022.
[12] Eric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 159-166, 1999.
[13] Eric Horvitz. The future of biomedical informatics: Bottlenecks and opportunities. In Biomedical Informatics: Computer Applications in Health Care and Biomedicine, E.H. Shortliffe, J.J. Cimino, et. al. Springer, 2021.
[14] Eric J Horvitz, John S Breese, David Heckerman, David Hovel, and Koos Rommelse. The Lumiere project: Bayesian user modeling for inferring the goals and needs of software users. In Proceedings of the Conference on Uncertainty in AI, pages 256-263, 1998.
[15] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Å½idek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
[16] Ross D King, Kenneth E Whelan, Ffion M Jones, Philip GK Reiser, Christopher H Bryant, Stephen H Muggleton, Douglas B Kell, and Stephen G Oliver. Functional genomic hypothesis generation and experimentation by a robot scientist. Nature, 427(6971):247-252, 2004.
[17] Thomas S Kuhn. The structure of scientific revolutions, volume 111. Chicago University of Chicago Press, 1970.
[18] D Lahav, JS Falcon, B Kuehl, S Johnson, S Parasa, N Shomron, DH Chau, D Yang, E Horvitz, DS Weld, and T Hope. A search engine for discovery of scientific challenges and directions. In AAAL 2022.
[19] Pat Langley, Herbert A Simon, Gary L Bradshaw, and Jan M Zytkow. Scientific discovery: Computational explorations of the creative processes. MIT press, 1987.
[20] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of ACL, 2020.
[21] Steve Lohr. What ever happened to ibm's watson. The New York Times, 16(7):21, 2021.
[22] Sonia Murthy, Kyle Lo, Chandra Bhagavatula, Daniel King, Bailey Kuehl, Sophie Johnson, Jonathan Borchardt, Daniel S. Weld, Tom Hope, and Doug Downey. Accord: A multi-document approach to generating diverse descriptions of scientific concepts. In EMNLP, 2022.
[23] Sheshera Mysore, Arman Cohan, and Tom Hope. Multi-vector models with textual guidance for fine-grained scientific document similarity. NAACL, 2022.
[24] Aakanksha Naik, Sravanthi Parasa, Sergey Feldman, Lucy Lu Wang, and Tom Hope. Literature-augmented clinical outcome prediction. NAACL, 2022.
[25] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of GPT-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
[26] Regina Nuzzo et al. How scientists fool themselves-and how they can stop. Nature, 526(7572):182-185, 2015.
[27] Benjamin Nye, Jay DeYoung, Eric Lehman, Ani Nenkova, Iain J Marshall, and Byron C Wallace. Understanding clinical trial reports: Extracting medical entities and their relations. In AMIA Annual Symposium Proceedings, volume 2021, page 485. American Medical Informatics Association, 2021.
[28] OpenAI. Gpt-4 technical report, 2023.
[29] Jason Portenoy, Marissa Radensky, Jevin West, Eric Horvitz, Daniel Weld, and Tom Hope. Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery. CHI, 2022.
[30] Edward O Pyser-Knapp, Jed W Pitera, Peter WJ Staar, Seiji Takeda, Teodoro Laino, Daniel P Sanders, James Sexton, John R Smith, and Alessandro Curioni. Accelerating materials discovery using artificial intelligence, high performance computing and robotics. npj Computational Materials, 8(1):1-9, 2022.
[31] Chirag Shah and Emily M Bender. Situating search. In ACM SIGIR Conference on Human Information Interaction and Retrieval, pages 221-232, 2022.
[32] Uriel Singer, Kira Radinsky, and Eric Horvitz. On biases of attention in scientific discovery. Bioinformatics, 12 2020. URL https://doi.org/10.1093/bioinformatics/ btaa1036.
[33] D. R. Swanson. Fish oil, raynaud's syndrome, and undiscovered public knowledge. Perspectives in Biology and Medicine, 30(1):7-18, 1986.
[34] Hillel Taub Tabib, Micali Shl aim, Shoval Sadde, Dan Lahav, Matan Eyal, Yaara Cohen, and Yoav Goldberg. Interactive extractive search over biomedical corpora. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, pages 28-37, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1:2020.biomlp-1.3. URL https://aclanthology.org/2020.biomlp-1.3.
[35] Jaime Teevan, Susan T Dumais, and Eric Horvitz. Personalizing search via automated analysis of interests and activities. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 449-456, 2005.
[36] Paul Thagard. The cognitive science of science: Explanation, discovery, and conceptual change. Mit Press, 2012.
[37] Aish Thamba and Richard B Gunderman. For watson, solving cancer wasn't so elementary: Prospects for artificial intelligence in radiology. Academic Radiology, 29(2):312-314, 2022.
[38] Daril A Vilhena, Jacob G Foster, Martin Rosvall, Jevin D West, James Evans, and Carl T Bergstrom. Finding cultural holes: How structure and culture diverge in networks of scholarly communication. Sociological Science, 1:221, 2014.
[39] Yu Wang, Jinchao Li, Tristan Naumann, Chenyan Xiong, Hao Cheng, Robert Tinn, Cliff Wong, Naoto Usuyama, Richard Rogahn, Zhihong Shen, et al. Domainspecific pretraining for vertical search: Case study on biomedical literature. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining, pages 3717-3725, 2021.
[40] Daniel S Weld and Gagan Bansal. The challenge of crafting intelligible intelligence. Communications of the ACM, 62(6):70-79, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-277 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-277</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-277</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-274992257</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.18415v1.pdf" target="_blank">Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) excel in linguistic tasks but struggle with mathematical reasoning, particularly in non English languages like Hindi. This research aims to enhance the mathematical reasoning skills of smaller, resource efficient open-source LLMs in both Hindi and English. We evaluate models like OpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B, Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods, and supervised fine-tuning. Our approach incorporates curriculum learning, progressively training models on increasingly difficult problems, a novel Decomposition Strategy to simplify complex arithmetic operations, and a Structured Solution Design that divides solutions into phases. Our experiments result in notable performance enhancements. WizardMath 7B exceeds Gemini's accuracy on English datasets by +6% and matches Gemini's performance on Hindi datasets. Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models, demonstrating the capability to learn mathematical reasoning in both languages. This research highlights the potential for improving mathematical reasoning in open-source LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e277.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e277.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardMath 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open-source model pre-trained/tuned for mathematical reasoning (WizardMath family), showing the strongest arithmetic and word-problem performance among evaluated small models after curriculum fine-tuning and bilingual training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, single-operator arithmetic word problems, multi-step word problems (grade-school to high-school level)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot; few-shot (examples / few-shot CoT with manual examples); supervised fine-tuning with Curriculum Learning (SFT easy, SFT easy+medium); Structured Solution fine-tuning; Bilingual Combined Training (English+Hindi)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On IndiMathQA (SFT easy setting): English (EMKB) Easy 75% / Medium 66% / Hard 47%; Hindi (HMKB) Easy 72% / Medium 57% / Hard 45%. Outperforms Gemini 1.0 Pro on English by ≈+5%; GPT-4 still exceeds WizardMath (exact GPT-4 numbers not reported). Fine-tuning (curriculum) yields +3–5% on easy, larger gains when adding medium examples (overall +11–18% on medium in bilingual SFT easy+medium experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No low-level mechanistic (attention/head-level) analysis provided; empirical insights show WizardMath benefits from task-specific pretraining/instruction tuning for math and from structured solutions and curriculum learning to reduce calculation errors and improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance increases with task-specific fine-tuning and curriculum learning; bilingual combined training further improves Hindi performance. Model (7B) still trails very large closed-source models (e.g., GPT-4) but can surpass some closed models (Gemini Pro) after interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Makes calculation and reasoning errors in base setting (example: returned 100% for a conditional probability baseline); mistakes in last-step computations unless fine-tuned on medium examples; weaker on high-difficulty (MATH) problems than on GSM8K-style problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared base vs few-shot vs SFT easy vs SFT easy+medium; compared single-language finetuning vs bilingual combined training; compared to closed-source GPT-4 and Gemini Pro baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>With curriculum fine-tuning, structured solutions, and bilingual training, WizardMath-7B achieves the best arithmetic/word-problem performance among evaluated 7B SLLMs and can exceed some closed-source models on English benchmarks, though very large models (GPT-4) remain stronger.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e277.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open foundation model used as a baseline; evaluated after fine-tuning for arithmetic and math word-problem tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, word problems across difficulties (easy/medium/hard)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot; few-shot; supervised fine-tuning with Curriculum Learning (SFT easy in bilingual setting reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IndiMathQA (SFT easy bilingual table): EMKB (English) Easy 44% / Medium 35% / Hard 23%; HMKB (Hindi) Easy 29% / Medium 24% / Hard 19%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Improves with curriculum fine-tuning (SFT easy gave modest gains; SFT easy+medium gave further improvements as described generally), but remains below specialized math-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower baseline math reasoning relative to math-specialized models; struggles more on medium/hard competition-style problems (MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared base vs SFT easy vs SFT easy+medium and single-language vs bilingual fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A general-purpose 7B foundation model benefits from curriculum fine-tuning but lags behind math-specialized models like WizardMath-7B on arithmetic/word-problem tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e277.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLeMMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLeMMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open model explicitly designed for mathematics; exhibited unstable behavior when fine-tuned on new Hindi data (hallucinations) but improved with the Decomposition Strategy for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLeMMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, mathematical word problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot; few-shot; supervised fine-tuning (instruction-tuning) on Enhanced HAWP and on HMQA; applied Decomposition Strategy for multiplication/division</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IndiMathQA (SFT easy bilingual table): EMKB Easy 21% / Medium 14% / Hard 8%; HMKB Easy 15% / Medium 10% / Hard 9%. Fine-tuning initially decreased performance (hallucinations), but Decomposition Strategy significantly improved arithmetic performance (exact numeric uplift in Table 1 referenced but not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Fine-tuning LLeMMA (pre-trained on English math) with new Hindi arithmetic knowledge led to hallucinations; decomposing multiplication/division into place-value components or dividend segments reduces hallucinations and improves calculation—suggests that explicit algorithmic decomposition aligns with the model's failure modes and mitigates them.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance degraded with naive fine-tuning on new-language data (increased hallucinations) but recovered/improved when using decomposition and curated augmentation (HMQA).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hallucinations (fabricated steps/answers) after fine-tuning on unfamiliar Hindi data; poor stability unless decomposition/augmentation applied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared base vs few-shot vs fine-tuned on Enhanced HAWP vs fine-tuned on augmented HMQA; Decomposition Strategy vs non-decomposed solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Math-specialized models can hallucinate when fine-tuned on new-language data; algorithmic decomposition of arithmetic (place-value breakdown) is an effective intervention to reduce hallucinations and improve arithmetic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e277.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenHathi-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenHathi 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilingual (Hindi-English) 7B model derived from Llama2 with tokenizer and embedding modifications, evaluated and fine-tuned for Hindi and English mathematical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenHathi-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, single-operator Hindi arithmetic word problems (HAWP) and broader math problems (IndiMathQA)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot; few-shot; fine-tuning on Enhanced HAWP and HMQA; Structured Solution fine-tuning; Curriculum Learning; Bilingual Combined Training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IndiMathQA (SFT easy bilingual table): EMKB Easy 43% / Medium 33% / Hard 24%; HMKB Easy 40% / Medium 37% / Hard 31%. Fine-tuning on Enhanced HAWP improved OpenHathi by ~20–30% on addition and subtraction tasks (text-reported range).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Improvements attributed to supervised fine-tuning and structured solutions rather than analysis of internal mechanisms; structured solutions and decomposition reduce calculation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Substantial gains from fine-tuning on Enhanced HAWP (20–30% on basic ops). Bilingual combined training further improves Hindi performance modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Base-model weaknesses on arithmetic calculations; requires curated augmentation and structured solution tuning to reach better accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared zero-shot vs few-shot vs fine-tuning on enhanced datasets; single-language vs bilingual fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Bilingual pretraining combined with curriculum and structured-solution fine-tuning yields meaningful gains on Hindi arithmetic, especially for basic operations after Enhanced HAWP fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e277.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open-source model evaluated as an SLLM baseline for arithmetic and math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, word problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot; few-shot; curriculum fine-tuning (SFT easy in bilingual setting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IndiMathQA (SFT easy bilingual table): EMKB Easy 50% / Medium 39% / Hard 29%; HMKB Easy 33% / Medium 26% / Hard 22%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Improves with curriculum fine-tuning but remains below the best math-tuned SLLM (WizardMath-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower accuracy on harder problems; multilingual/Hindi performance lower than English.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared base vs SFT easy vs SFT easy+medium and bilingual vs monolingual fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Mistral-7B is a competitive 7B baseline that benefits from curriculum fine-tuning but is outperformed by math-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e277.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MammoTH-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mammoth (MAmmoTH) 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B model trained as a math generalist via hybrid instruction tuning, evaluated on bilingual math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MammoTH-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general arithmetic and math word problems (addition, subtraction, multiplication, division, other domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot; supervised fine-tuning with Curriculum Learning; bilingual combined training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IndiMathQA (SFT easy bilingual table): EMKB Easy 40% / Medium 20% / Hard 18%; HMKB Easy 44% / Medium 35% / Hard 27%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Benefits from bilingual fine-tuning; shows asymmetric performance changes across languages (Hindi sometimes better for some splits).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Relatively weak on medium/hard English problems in SFT easy setting; variability across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baseline vs SFT easy; bilingual vs monolingual fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Instruction-tuned math-generalist model shows moderate performance; bilingual fine-tuning can shift relative strengths between English and Hindi.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e277.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Airavata-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Airavata 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Hindi instruction-tuned derivative of OpenHathi, evaluated for Hindi math reasoning; shows lower performance relative to other bilingual models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Airavata-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Hindi arithmetic word problems and general math tasks</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot; fine-tuning with Curriculum Learning and bilingual training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IndiMathQA (SFT easy bilingual table): EMKB Easy 25% / Medium 16% / Hard 13%; HMKB Easy 18% / Medium 14% / Hard 11%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Limited gains from fine-tuning compared to other models, possibly due to pretraining/data differences.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower baseline math ability and limited improvement after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared pre vs post fine-tuning, bilingual vs monolingual.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Hindi instruction-tuned model shows modest math performance; pretraining and exposure to math-specific data are important for stronger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e277.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hi-NOLIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hi-NOLIN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilingual model in the Pythia suite pre-trained and further exposed to Hindi+English to improve Hindi understanding; evaluated on bilingual math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hi-NOLIN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general math word problems (addition, subtraction, multiplication, division, broader topics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot; supervised fine-tuning (bilingual curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IndiMathQA (SFT easy bilingual table): EMKB Easy 40% / Medium 30% / Hard 21%; HMKB Easy 39% / Medium 35% / Hard 28%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Improves with bilingual fine-tuning; performs more consistently across languages relative to some open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still behind top math-specialized models on harder problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared SFT easy vs SFT easy+medium; bilingual vs monolingual.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Bilingual pretraining and targeted fine-tuning produce stable bilingual math performance, but top math performance requires math-specific tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e277.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source LLM used in this work for generating solutions, dataset augmentation, and few-shot examples; serves as an upper-bound reference for arithmetic reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic, and generation of structured solutions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>used for one-shot/ few-shot prompting to generate solutions, create augmented HMQA dataset, and produce decomposition-based examples; zero-shot/few-shot CoT not detailed for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported qualitatively as exceeding WizardMath-7B across evaluated benchmarks (exact numeric scores not provided in text). Used to generate and review dataset solutions (HMQA, Enhanced HAWP).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Acts as a high-performing reference; used to bootstrap data/solutions for fine-tuning smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a reference/upper-baseline versus SLLMs; used to generate few-shot/decomposition examples for data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-4 served as a high-quality data generator and benchmarking anchor; smaller SLLMs can approach some closed-source performance when provided with structured curricula and decomposition-based supervision but GPT-4 remains stronger.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e277.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.0 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LLM used as a comparative baseline; WizardMath-7B surpassed Gemini Pro on English datasets after interventions, while Gemini retains advantages in some Hindi medium/hard cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general arithmetic and math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>evaluated in zero-shot/few-shot contexts as a closed-source baseline (details not expanded)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Text reports WizardMath-7B outperforms Gemini 1.0 Pro on English datasets by ≈+5%; on Hindi datasets WizardMath is comparable but reportedly lags by ≈3% on medium/hard difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to WizardMath-7B and GPT-4 as closed-source baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Closed-source models like Gemini are strong baselines; appropriately fine-tuned SLLMs can surpass some closed models on English math benchmarks but may still trail on harder/higher-resource settings or cross-lingual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e277.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e277.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decomposition Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposition Strategy for Multiplication and Division (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that breaks multiplication into place-value component multiplications and division into segmented dividend divisions then aggregates results, aimed at improving LLM arithmetic accuracy and reducing hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decomposition Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multiplication (place-value decomposition), division (dividend segmentation and summing quotients)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>applied on HAWP grade-school single-operator problems; extended data augmentation to larger set (2k -> 10k) but specific numeric ranges not detailed</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Data-level solution transformation used as few-shot examples and as target outputs for fine-tuning; applied during generation/augmentation with GPT-4 and then used to fine-tune SLLMs (OpenHathi, WizardMath, LLeMMA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported to significantly enhance LLeMMA's performance and generally improve arithmetic accuracy across models; fine-tuning on Decomposition Strategy-enhanced datasets (Enhanced HAWP / HMQA) produced substantial improvements (text reports 20–30% gains for OpenHathi on addition/subtraction; concrete per-operation numbers for decomposition are referenced in Table 1 but not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Empirical evidence that explicit algorithmic decomposition aligns with model behavior, reducing hallucinations and calculation mistakes—suggests models benefit from being guided toward stepwise algorithmic procedures rather than expecting them to internally learn stable multi-digit arithmetic algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Augmenting and fine-tuning with decomposition-enhanced data improves small models substantially; one-shot generation + human review scaled dataset from ~2k to 10k (HMQA) yielding further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Designed to address failures on multi-digit multiplication/division and hallucination during fine-tuning; no specific new failure modes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared models fine-tuned on native (non-decomposed) solutions vs decomposed solutions; compared LLeMMA before and after decomposition where hallucinations were reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Providing explicit, place-value / segmented algorithmic decompositions as training targets materially improves arithmetic accuracy and reduces hallucinations in 7B-class LLMs, indicating that small models benefit from supervision that exposes algorithmic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>A careful examination of large language model performance on grade school arithmetic. <em>(Rating: 2)</em></li>
                <li>Llemma: An Open Language Model For Mathematics. <em>(Rating: 2)</em></li>
                <li>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. <em>(Rating: 2)</em></li>
                <li>Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?. <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset. <em>(Rating: 2)</em></li>
                <li>Let's Verify Step by Step. <em>(Rating: 1)</em></li>
                <li>Mammoth: Building math generalist models through hybrid instruction tuning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-277",
    "paper_id": "paper-274992257",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "WizardMath-7B",
            "name_full": "WizardMath 7B",
            "brief_description": "A 7B-parameter open-source model pre-trained/tuned for mathematical reasoning (WizardMath family), showing the strongest arithmetic and word-problem performance among evaluated small models after curriculum fine-tuning and bilingual training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "WizardMath-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, single-operator arithmetic word problems, multi-step word problems (grade-school to high-school level)",
            "number_range_or_complexity": null,
            "method_or_intervention": "zero-shot; few-shot (examples / few-shot CoT with manual examples); supervised fine-tuning with Curriculum Learning (SFT easy, SFT easy+medium); Structured Solution fine-tuning; Bilingual Combined Training (English+Hindi)",
            "performance_result": "On IndiMathQA (SFT easy setting): English (EMKB) Easy 75% / Medium 66% / Hard 47%; Hindi (HMKB) Easy 72% / Medium 57% / Hard 45%. Outperforms Gemini 1.0 Pro on English by ≈+5%; GPT-4 still exceeds WizardMath (exact GPT-4 numbers not reported). Fine-tuning (curriculum) yields +3–5% on easy, larger gains when adding medium examples (overall +11–18% on medium in bilingual SFT easy+medium experiments).",
            "mechanistic_insight": "No low-level mechanistic (attention/head-level) analysis provided; empirical insights show WizardMath benefits from task-specific pretraining/instruction tuning for math and from structured solutions and curriculum learning to reduce calculation errors and improve multi-step reasoning.",
            "performance_scaling": "Performance increases with task-specific fine-tuning and curriculum learning; bilingual combined training further improves Hindi performance. Model (7B) still trails very large closed-source models (e.g., GPT-4) but can surpass some closed models (Gemini Pro) after interventions.",
            "failure_modes": "Makes calculation and reasoning errors in base setting (example: returned 100% for a conditional probability baseline); mistakes in last-step computations unless fine-tuned on medium examples; weaker on high-difficulty (MATH) problems than on GSM8K-style problems.",
            "comparison_baseline": "Compared base vs few-shot vs SFT easy vs SFT easy+medium; compared single-language finetuning vs bilingual combined training; compared to closed-source GPT-4 and Gemini Pro baselines.",
            "key_finding": "With curriculum fine-tuning, structured solutions, and bilingual training, WizardMath-7B achieves the best arithmetic/word-problem performance among evaluated 7B SLLMs and can exceed some closed-source models on English benchmarks, though very large models (GPT-4) remain stronger.",
            "uuid": "e277.0",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA 7B",
            "brief_description": "A 7B-parameter open foundation model used as a baseline; evaluated after fine-tuning for arithmetic and math word-problem tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, word problems across difficulties (easy/medium/hard)",
            "number_range_or_complexity": null,
            "method_or_intervention": "zero-shot; few-shot; supervised fine-tuning with Curriculum Learning (SFT easy in bilingual setting reported)",
            "performance_result": "IndiMathQA (SFT easy bilingual table): EMKB (English) Easy 44% / Medium 35% / Hard 23%; HMKB (Hindi) Easy 29% / Medium 24% / Hard 19%.",
            "mechanistic_insight": null,
            "performance_scaling": "Improves with curriculum fine-tuning (SFT easy gave modest gains; SFT easy+medium gave further improvements as described generally), but remains below specialized math-tuned models.",
            "failure_modes": "Lower baseline math reasoning relative to math-specialized models; struggles more on medium/hard competition-style problems (MATH).",
            "comparison_baseline": "Compared base vs SFT easy vs SFT easy+medium and single-language vs bilingual fine-tuning.",
            "key_finding": "A general-purpose 7B foundation model benefits from curriculum fine-tuning but lags behind math-specialized models like WizardMath-7B on arithmetic/word-problem tasks.",
            "uuid": "e277.1",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLeMMA-7B",
            "name_full": "LLeMMA 7B",
            "brief_description": "A 7B open model explicitly designed for mathematics; exhibited unstable behavior when fine-tuned on new Hindi data (hallucinations) but improved with the Decomposition Strategy for arithmetic.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLeMMA-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, mathematical word problems",
            "number_range_or_complexity": null,
            "method_or_intervention": "zero-shot; few-shot; supervised fine-tuning (instruction-tuning) on Enhanced HAWP and on HMQA; applied Decomposition Strategy for multiplication/division",
            "performance_result": "IndiMathQA (SFT easy bilingual table): EMKB Easy 21% / Medium 14% / Hard 8%; HMKB Easy 15% / Medium 10% / Hard 9%. Fine-tuning initially decreased performance (hallucinations), but Decomposition Strategy significantly improved arithmetic performance (exact numeric uplift in Table 1 referenced but not fully enumerated in text).",
            "mechanistic_insight": "Fine-tuning LLeMMA (pre-trained on English math) with new Hindi arithmetic knowledge led to hallucinations; decomposing multiplication/division into place-value components or dividend segments reduces hallucinations and improves calculation—suggests that explicit algorithmic decomposition aligns with the model's failure modes and mitigates them.",
            "performance_scaling": "Performance degraded with naive fine-tuning on new-language data (increased hallucinations) but recovered/improved when using decomposition and curated augmentation (HMQA).",
            "failure_modes": "Hallucinations (fabricated steps/answers) after fine-tuning on unfamiliar Hindi data; poor stability unless decomposition/augmentation applied.",
            "comparison_baseline": "Compared base vs few-shot vs fine-tuned on Enhanced HAWP vs fine-tuned on augmented HMQA; Decomposition Strategy vs non-decomposed solutions.",
            "key_finding": "Math-specialized models can hallucinate when fine-tuned on new-language data; algorithmic decomposition of arithmetic (place-value breakdown) is an effective intervention to reduce hallucinations and improve arithmetic accuracy.",
            "uuid": "e277.2",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OpenHathi-7B",
            "name_full": "OpenHathi 7B",
            "brief_description": "A bilingual (Hindi-English) 7B model derived from Llama2 with tokenizer and embedding modifications, evaluated and fine-tuned for Hindi and English mathematical reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenHathi-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, single-operator Hindi arithmetic word problems (HAWP) and broader math problems (IndiMathQA)",
            "number_range_or_complexity": null,
            "method_or_intervention": "zero-shot; few-shot; fine-tuning on Enhanced HAWP and HMQA; Structured Solution fine-tuning; Curriculum Learning; Bilingual Combined Training",
            "performance_result": "IndiMathQA (SFT easy bilingual table): EMKB Easy 43% / Medium 33% / Hard 24%; HMKB Easy 40% / Medium 37% / Hard 31%. Fine-tuning on Enhanced HAWP improved OpenHathi by ~20–30% on addition and subtraction tasks (text-reported range).",
            "mechanistic_insight": "Improvements attributed to supervised fine-tuning and structured solutions rather than analysis of internal mechanisms; structured solutions and decomposition reduce calculation errors.",
            "performance_scaling": "Substantial gains from fine-tuning on Enhanced HAWP (20–30% on basic ops). Bilingual combined training further improves Hindi performance modestly.",
            "failure_modes": "Base-model weaknesses on arithmetic calculations; requires curated augmentation and structured solution tuning to reach better accuracy.",
            "comparison_baseline": "Compared zero-shot vs few-shot vs fine-tuning on enhanced datasets; single-language vs bilingual fine-tuning.",
            "key_finding": "Bilingual pretraining combined with curriculum and structured-solution fine-tuning yields meaningful gains on Hindi arithmetic, especially for basic operations after Enhanced HAWP fine-tuning.",
            "uuid": "e277.3",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B",
            "brief_description": "A 7B open-source model evaluated as an SLLM baseline for arithmetic and math word problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, word problems",
            "number_range_or_complexity": null,
            "method_or_intervention": "zero-shot; few-shot; curriculum fine-tuning (SFT easy in bilingual setting)",
            "performance_result": "IndiMathQA (SFT easy bilingual table): EMKB Easy 50% / Medium 39% / Hard 29%; HMKB Easy 33% / Medium 26% / Hard 22%.",
            "mechanistic_insight": null,
            "performance_scaling": "Improves with curriculum fine-tuning but remains below the best math-tuned SLLM (WizardMath-7B).",
            "failure_modes": "Lower accuracy on harder problems; multilingual/Hindi performance lower than English.",
            "comparison_baseline": "Compared base vs SFT easy vs SFT easy+medium and bilingual vs monolingual fine-tuning.",
            "key_finding": "Mistral-7B is a competitive 7B baseline that benefits from curriculum fine-tuning but is outperformed by math-specialized models.",
            "uuid": "e277.4",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MammoTH-7B",
            "name_full": "Mammoth (MAmmoTH) 7B",
            "brief_description": "A 7B model trained as a math generalist via hybrid instruction tuning, evaluated on bilingual math benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MammoTH-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "general arithmetic and math word problems (addition, subtraction, multiplication, division, other domains)",
            "number_range_or_complexity": null,
            "method_or_intervention": "few-shot; supervised fine-tuning with Curriculum Learning; bilingual combined training",
            "performance_result": "IndiMathQA (SFT easy bilingual table): EMKB Easy 40% / Medium 20% / Hard 18%; HMKB Easy 44% / Medium 35% / Hard 27%.",
            "mechanistic_insight": null,
            "performance_scaling": "Benefits from bilingual fine-tuning; shows asymmetric performance changes across languages (Hindi sometimes better for some splits).",
            "failure_modes": "Relatively weak on medium/hard English problems in SFT easy setting; variability across languages.",
            "comparison_baseline": "Baseline vs SFT easy; bilingual vs monolingual fine-tuning.",
            "key_finding": "Instruction-tuned math-generalist model shows moderate performance; bilingual fine-tuning can shift relative strengths between English and Hindi.",
            "uuid": "e277.5",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Airavata-7B",
            "name_full": "Airavata 7B",
            "brief_description": "A Hindi instruction-tuned derivative of OpenHathi, evaluated for Hindi math reasoning; shows lower performance relative to other bilingual models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Airavata-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "Hindi arithmetic word problems and general math tasks",
            "number_range_or_complexity": null,
            "method_or_intervention": "few-shot; fine-tuning with Curriculum Learning and bilingual training",
            "performance_result": "IndiMathQA (SFT easy bilingual table): EMKB Easy 25% / Medium 16% / Hard 13%; HMKB Easy 18% / Medium 14% / Hard 11%.",
            "mechanistic_insight": null,
            "performance_scaling": "Limited gains from fine-tuning compared to other models, possibly due to pretraining/data differences.",
            "failure_modes": "Lower baseline math ability and limited improvement after fine-tuning.",
            "comparison_baseline": "Compared pre vs post fine-tuning, bilingual vs monolingual.",
            "key_finding": "Hindi instruction-tuned model shows modest math performance; pretraining and exposure to math-specific data are important for stronger gains.",
            "uuid": "e277.6",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Hi-NOLIN",
            "name_full": "Hi-NOLIN",
            "brief_description": "A bilingual model in the Pythia suite pre-trained and further exposed to Hindi+English to improve Hindi understanding; evaluated on bilingual math tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Hi-NOLIN",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "general math word problems (addition, subtraction, multiplication, division, broader topics)",
            "number_range_or_complexity": null,
            "method_or_intervention": "few-shot; supervised fine-tuning (bilingual curriculum)",
            "performance_result": "IndiMathQA (SFT easy bilingual table): EMKB Easy 40% / Medium 30% / Hard 21%; HMKB Easy 39% / Medium 35% / Hard 28%.",
            "mechanistic_insight": null,
            "performance_scaling": "Improves with bilingual fine-tuning; performs more consistently across languages relative to some open-source models.",
            "failure_modes": "Still behind top math-specialized models on harder problems.",
            "comparison_baseline": "Compared SFT easy vs SFT easy+medium; bilingual vs monolingual.",
            "key_finding": "Bilingual pretraining and targeted fine-tuning produce stable bilingual math performance, but top math performance requires math-specific tuning.",
            "uuid": "e277.7",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large closed-source LLM used in this work for generating solutions, dataset augmentation, and few-shot examples; serves as an upper-bound reference for arithmetic reasoning performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic, and generation of structured solutions",
            "number_range_or_complexity": null,
            "method_or_intervention": "used for one-shot/ few-shot prompting to generate solutions, create augmented HMQA dataset, and produce decomposition-based examples; zero-shot/few-shot CoT not detailed for evaluation",
            "performance_result": "Reported qualitatively as exceeding WizardMath-7B across evaluated benchmarks (exact numeric scores not provided in text). Used to generate and review dataset solutions (HMQA, Enhanced HAWP).",
            "mechanistic_insight": null,
            "performance_scaling": "Acts as a high-performing reference; used to bootstrap data/solutions for fine-tuning smaller models.",
            "failure_modes": null,
            "comparison_baseline": "Used as a reference/upper-baseline versus SLLMs; used to generate few-shot/decomposition examples for data augmentation.",
            "key_finding": "GPT-4 served as a high-quality data generator and benchmarking anchor; smaller SLLMs can approach some closed-source performance when provided with structured curricula and decomposition-based supervision but GPT-4 remains stronger.",
            "uuid": "e277.8",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gemini Pro",
            "name_full": "Gemini 1.0 Pro",
            "brief_description": "A closed-source LLM used as a comparative baseline; WizardMath-7B surpassed Gemini Pro on English datasets after interventions, while Gemini retains advantages in some Hindi medium/hard cases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini Pro",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "general arithmetic and math word problems",
            "number_range_or_complexity": null,
            "method_or_intervention": "evaluated in zero-shot/few-shot contexts as a closed-source baseline (details not expanded)",
            "performance_result": "Text reports WizardMath-7B outperforms Gemini 1.0 Pro on English datasets by ≈+5%; on Hindi datasets WizardMath is comparable but reportedly lags by ≈3% on medium/hard difficulties.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": "Compared to WizardMath-7B and GPT-4 as closed-source baselines.",
            "key_finding": "Closed-source models like Gemini are strong baselines; appropriately fine-tuned SLLMs can surpass some closed models on English math benchmarks but may still trail on harder/higher-resource settings or cross-lingual tasks.",
            "uuid": "e277.9",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Decomposition Strategy",
            "name_full": "Decomposition Strategy for Multiplication and Division (this work)",
            "brief_description": "A method introduced in this paper that breaks multiplication into place-value component multiplications and division into segmented dividend divisions then aggregates results, aimed at improving LLM arithmetic accuracy and reducing hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decomposition Strategy",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "multiplication (place-value decomposition), division (dividend segmentation and summing quotients)",
            "number_range_or_complexity": "applied on HAWP grade-school single-operator problems; extended data augmentation to larger set (2k -&gt; 10k) but specific numeric ranges not detailed",
            "method_or_intervention": "Data-level solution transformation used as few-shot examples and as target outputs for fine-tuning; applied during generation/augmentation with GPT-4 and then used to fine-tune SLLMs (OpenHathi, WizardMath, LLeMMA)",
            "performance_result": "Reported to significantly enhance LLeMMA's performance and generally improve arithmetic accuracy across models; fine-tuning on Decomposition Strategy-enhanced datasets (Enhanced HAWP / HMQA) produced substantial improvements (text reports 20–30% gains for OpenHathi on addition/subtraction; concrete per-operation numbers for decomposition are referenced in Table 1 but not fully enumerated in text).",
            "mechanistic_insight": "Empirical evidence that explicit algorithmic decomposition aligns with model behavior, reducing hallucinations and calculation mistakes—suggests models benefit from being guided toward stepwise algorithmic procedures rather than expecting them to internally learn stable multi-digit arithmetic algorithms.",
            "performance_scaling": "Augmenting and fine-tuning with decomposition-enhanced data improves small models substantially; one-shot generation + human review scaled dataset from ~2k to 10k (HMQA) yielding further gains.",
            "failure_modes": "Designed to address failures on multi-digit multiplication/division and hallucination during fine-tuning; no specific new failure modes reported.",
            "comparison_baseline": "Compared models fine-tuned on native (non-decomposed) solutions vs decomposed solutions; compared LLeMMA before and after decomposition where hallucinations were reduced.",
            "key_finding": "Providing explicit, place-value / segmented algorithmic decompositions as training targets materially improves arithmetic accuracy and reduces hallucinations in 7B-class LLMs, indicating that small models benefit from supervision that exposes algorithmic steps.",
            "uuid": "e277.10",
            "source_info": {
                "paper_title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "A careful examination of large language model performance on grade school arithmetic.",
            "rating": 2,
            "sanitized_title": "a_careful_examination_of_large_language_model_performance_on_grade_school_arithmetic"
        },
        {
            "paper_title": "Llemma: An Open Language Model For Mathematics.",
            "rating": 2,
            "sanitized_title": "llemma_an_open_language_model_for_mathematics"
        },
        {
            "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct.",
            "rating": 2,
            "sanitized_title": "wizardmath_empowering_mathematical_reasoning_for_large_language_models_via_reinforced_evolinstruct"
        },
        {
            "paper_title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?.",
            "rating": 2,
            "sanitized_title": "does_finetuning_llms_on_new_knowledge_encourage_hallucinations"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset.",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Let's Verify Step by Step.",
            "rating": 1,
            "sanitized_title": "lets_verify_step_by_step"
        },
        {
            "paper_title": "Mammoth: Building math generalist models through hybrid instruction tuning.",
            "rating": 1,
            "sanitized_title": "mammoth_building_math_generalist_models_through_hybrid_instruction_tuning"
        }
    ],
    "cost": 0.017496249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English
24 Dec 2024</p>
<p>Avinash Anand avinasha@iiitd.ac.in 
Indraprastha Institute of Information Technology
DelhiIndia</p>
<p>Kritarth Prasad kritarth20384@iiitd.ac.in 
Indraprastha Institute of Information Technology
DelhiIndia</p>
<p>Chhavi Kirtani 
Indraprastha Institute of Information Technology
DelhiIndia</p>
<p>Ashwin R Nair 
Indraprastha Institute of Information Technology
DelhiIndia</p>
<p>Manvendra Kumar Nema manvendra22038@iiitd.ac.in 
Indraprastha Institute of Information Technology
DelhiIndia</p>
<p>Raj Jaiswal jaiswalp@iiitd.ac.in 
Indraprastha Institute of Information Technology
DelhiIndia</p>
<p>Rajiv Ratn rajivratn@iiitd.ac.in 
Indraprastha Institute of Information Technology
DelhiIndia</p>
<p>Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English
24 Dec 2024048559934ED038385E6F84FA531CD351arXiv:2412.18415v1[cs.CL]Pre-print of paper accepted to AAAI 2025
Large Language Models (LLMs) excel in linguistic tasks but struggle with mathematical reasoning, particularly in non-English languages like Hindi.This research aims to enhance the mathematical reasoning skills of smaller, resourceefficient open-source LLMs in both Hindi and English.We evaluate models like OpenHathi 7B, LLaMA-2 7B, Wizard-Math 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B, Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods, and supervised fine-tuning.Our approach incorporates curriculum learning, progressively training models on increasingly difficult problems, a novel Decomposition Strategy to simplify complex arithmetic operations, and a Structured Solution Design that divides solutions into phases.Our experiments result in notable performance enhancements.WizardMath 7B exceeds Gemini's accuracy on English datasets by +6% and matches Gemini's performance on Hindi datasets.Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models, demonstrating the capability to learn mathematical reasoning in both languages.This research highlights the potential for improving mathematical reasoning in open-source LLMs. 1</p>
<p>Introduction</p>
<p>Enhancing AI systems to solve complex problems has become a crucial objective within the AI research community, particularly in the realm of mathematical questionanswering.While models like GPT-4 and Gemini have demonstrated their strengths in arithmetic (Zhang et al. 2024), algebra (Kao, Wang, and Hsieh 2024), scientific text generation (Anand et al. 2024d(Anand et al. , 2023a)), and symbolic manipulation (Dave et al. 2024), they are not without limitations.Our evaluations on the GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021) datasets reveal a stark contrast in their capabilities.These models perform well on the relatively straightforward GSM8K dataset, but their effectiveness significantly diminishes when tasked with the more challenging MATH dataset.This dataset includes highschool competition-level questions that require a deeper level of contextual understanding and more advanced reasoning skills.The discrepancies in performance highlight the current limitations of these models in handling complex mathematical problem-solving.</p>
<p>In addition to these challenges, there is a noticeable gap in the performance of large language models (LLMs) when applied to English versus non-English languages, particularly in natural language processing tasks such as question answering and classification.This gap is particularly evident in Hindi, India's predominant language, which is used by over 105 million students according to UDISE+ reports for 2019-202 .Enhancing the capabilities of LLMs in Hindi is essential to make these tools more accessible and effective in subjectspecific learning contexts.While research efforts such as OpenHathi-7B (AI 2023), Hi-NOLIN (Research 2023), and Airavata (Gala et al. 2024) have made strides in adapting LLMs to the Hindi language, these models were not originally optimized for domain-specific tasks like mathematical problem-solving.</p>
<p>Recent advancements in open-source LLMs have shown promise in improving mathematical and physics problemsolving abilities (Anand et al. 2024a(Anand et al. ,c,b, 2023b)), as evidenced by prominent models like WizardMath (Luo et al. 2023), Mistral (Jiang et al. 2023), LLeMMA (Azerbayev et al. 2023), and MAmmoTH (Yue et al. 2023).However, these advancements have largely focused on the English language, with limited performance gains observed in Hindi math datasets.Additionally, closed-source models such as GPT-4 and Gemini-Pro continue to outperform open-source models on established benchmarks like GSM8K and MATH, as well as on newly defined Hindi datasets.The significant performance disparity can be attributed to the vast difference in the number of parameters these models are trained on.While the open-source LLMs explored in this research have fewer than 10 billion parameters, well-known closed-source LLMs are trained on considerably large parameter counts.Given the constraints on computational resources, this research focuses on enhancing the performance of smaller open-source LLMs (SLLMs), acknowledging the limitations while seeking to optimize within these parameters.</p>
<p>This research introduces several key contributions aimed at improving the mathematical capabilities of SLLMs, par- ticularly in Hindi: 1. Introduction of the Decomposition Strategy: A novel approach designed to enhance SLLMs' ability to solve complex mathematical operations by breaking them down into smaller, more manageable components in the enhanced HAWP dataset (see Figure 2).</p>
<p>Structured Solution Approach with Curriculum</p>
<p>Learning: A combined methodology that integrates a structured solution framework with Curriculum Learning, as illustrated in Figure 1.This approach progressively guides models through increasingly complex mathematical problems, enhancing their problemsolving abilities.</p>
<p>Ensuring Transparency and Reproducibility:</p>
<p>To promote transparency and enable future research, the datasets and code developed during this research will be released, with all data having been reviewed by human experts.</p>
<p>Related Work</p>
<p>Recent advances in large language models (LLMs) have significantly improved their ability to perform complex tasks, particularly in the areas of natural language processing and mathematical reasoning.However, one area that remains underexplored is the application of Curriculum Learning to these models.Originally proposed by Bengio et al. (Bengio et al. 2009), Curriculum Learning is a training strategy that mimics the way humans learn by gradually increasing the complexity of tasks presented to the model.Although widely used in deep learning, its application to LLMs has been limited, particularly in enhancing the models' capabilities in complex, multistep reasoning tasks.This section discusses various open-source and bilingual LLMs, their architectures, and the benchmark datasets used to evaluate their performance.</p>
<p>Open-Source Large Language Models 1. Llama Model (Touvron et al. 2023): It is a foundational model requiring less computational power, ideal for finetuning for various tasks as it is trained on vast unlabeled data.2. Wizard Math (Luo et al. 2023): It improved mathematical reasoning abilities by applying the "Reinforcement Learning from Evolutionary Instruction Feedback (RLEIF) method" (Xu et al. 2023) in math.Bilingual Open-Source Large Language Models for English and Hindi</p>
<p>OpenHathi-7B Sarvam AI used Llama2-7B to create OpenHathi-Hi-v0.1 (AI 2023).It integrated a custom tokenizer to expand the embedding layer to 48K tokens and trained on Hindi-English translation and bilingual nexttoken prediction tasks.</p>
<p>Hi-NOLIN HI-NOLIN (Research 2023), a Bilingual LLM in the Pythia model suite, was trained on English and code datasets, then further pre-trained on combined Hindi and English data to improve Hindi understanding.</p>
<p>Airavata AIRAVATA model (Gala et al. 2024) is derived from fine-tuning the OpenHathi model (AI 2023) with a Hindi instruction-tuned dataset, translated from high-quality English data via IndicTrans2 translation model (Gala et al. 2023).</p>
<p>Benchmark Datasets (Cobbe et al. 2021) introduced the GSM8K dataset, which consists of 8,500 grade school math problems that require basic arithmetic operations.These problems are designed to be solvable by proficient middle school students.Similarly, (Hendrycks et al. 2021) released the MATH dataset, containing 12,500 complex problems from high school competitions such as AMC 10 and AMC 12, intended for high school students.It covers topics: Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus.(Lightman et al. 2023) introduced PRM800K, a dataset with 800,000 step-level feedback labels for solutions to MATH (Hendrycks et al. 2021) problems, providing annotations ("Positive," "Negative," or "Neutral") to each solution step.For Hindi-speaking students, (Sharma, Mishra, and Sharma 2022) released HAWP (Hindi Arithmetic Word Problems), which is the only publicly available dataset of Hindi mathematical questions.It is for grades 1 to 6, and comprises 2,336 basic math word problems requiring a single operator solution.</p>
<p>Methodology Decomposition Strategy on HAWP Dataset</p>
<p>To improve the computational accuracy of large language models (LLMs) in arithmetic operations involving large numbers, we propose a Decomposition Strategy for multiplication and division tasks.For multiplication, this involves breaking down the multiplicand into place value components-such as hundreds, tens, and ones-and multiplying each by the other multiplicand.The products are then aggregated to obtain the final result.For division, the dividend is similarly decomposed into segments, each divided by the divisor, with the quotients summed to produce the final answer.This has been proposed to combat the poor calculation skills of open-source language models.In this paper, we focus on introducing and validating the Decomposition Strategy using the HAWP dataset, which contains basic mathematical word problems requiring single-operation calculations.This allows us to clearly demonstrate the strategy's effectiveness in a controlled, straightforward context.While exploring its application to more complex datasets is an exciting future direction, we have chosen to concentrate on HAWP for now to ensure a thorough and focused evaluation of this novel approach.We utilized 2,336 Hindi arithmetic problems from the HAWP dataset, covering basic operations like addition, subtraction, multiplication, and division.Since the original dataset lacked solutions, we enhanced it by generating question-answer pairs using GPT-4, which were then carefully reviewed and corrected by five human experts, resulting in the Enhanced HAWP dataset.</p>
<p>To evaluate the Decomposition Strategy's effectiveness, we applied it to the Enhanced HAWP dataset (see Figure 2).Manually solved examples using this strategy were used in few-shot learning with GPT-4 to modify the remaining solutions in Enhanced HAWP.These refined solutions, with 70% and 30% training/testing split, were then used to fine-tune the models OpenHathi 7B, WizardMath-v1.1 7B, and LLeMMa 7B.The resulting accuracy improvements are shown in Table 1.</p>
<p>In our final phase, we focused on exploring the benefits of fine-tuning using an augmented version of the dataset that we previously prepared Decomposition Strategy-enhanced dataset.We expanded the original 2,000 problems to 10,000 using a one-shot prompting technique with GPT-4.These newly generated samples were carefully reviewed by five human experts for accuracy, resulting in the HMQA (Hindi Math Questions-Answers) dataset.We then used this augmented dataset to fine-tune the models OpenHathi 7B, WizardMath-v1.1 7B, and LLeMMa 7B, with the resulting accuracy compared to previous settings in Table 1.</p>
<p>IndiMathQA</p>
<p>We have meticulously curated our own comprehensive math problem dataset, referred to as IndiMathQA, sourcing problems from the official NCERT textbooks4 used in Indian schools.This dataset contains 598 manually curated math problems and their corresponding solutions.These problems are suited for students in grades 10, 11, and 12, and it encompasses a wide range of problems that vary in complexity and span 14 major mathematical domains, including sets, trigonometry, and the binomial theorem, among others.Appendix provides more details on topic distribution.</p>
<p>LLM Enhancement in Bilingual Mathematics</p>
<p>In this section we demonstrate the strategies used in improving mathematical reasoning skills in Bilingual settings.</p>
<p>Classification based on Complexity</p>
<p>We have carefully curated a collection of mathematical problems categorized into easy, medium, and hard levels.This collection includes problems from our own dataset as well as from benchmark datasets, such as GSM8K and MATH.Below, we outline the methods we used to classify each problem by its complexity.We utilize additional datasets (GSM8K and MATH) for the sole reason of having more diversity of mathematical topics in our dataset.</p>
<p>IndiMathQA: The IndiMathQA dataset was carefully annotated by a team of five human experts, resulting in 136 easy, 218 medium, and 244 hard questions.To ensure the reliability of these annotations, we calculated the Average Fleiss' Kappa score, which came out to 0.58, indicating low bias and substantial agreement among the annotators.Further details on the annotation process can be found in the Appendix.This dataset was then augmented to a total of 7823 questions with similar concepts (820 easy, 2,470 medium, and 4,533 hard) using the GPT-4 API, which were then reviewed by a team of 5 human experts to correct any errors to ensure accuracy and consistency (See appendix for augmentation prompt details).GSM8K: GSM8K is a grade-school level mathematics dataset, where all questions are generally low in complexity.However, to ensure precision in our classifications, we used LLama 3 (405B) with prompt engineering to assess and rank the questions according to their complexity, based on various criteria, including Language Understanding, Mathematical Complexity, Reasoning Complexity, Number of Variables, and Conceptual Complexity (details in the appendix).</p>
<p>For our experiments, we selected the 700 questions with the lowest complexity as the Easy level questions.</p>
<p>MATH:</p>
<p>The MATH dataset features competition-level questions for students in grades 8 through 12, with each question annotated by complexity, ranging from Level 1 (easiest) to Level 5 (hardest).For our experiments, we categorized the Level 1 questions in the set as Easy, Levels 2 and 3 as Medium, and the remaining levels as Hard.This classification resulted in 664 Easy, 3,140 Medium, and 3,994 Hard questions.</p>
<p>Structured Solution Generation and Language Translations LLMs often encounter challenges with hallucinations when solving reasoning tasks.In our manual inspection of base model solutions, we noticed that LLMs sometimes became so focused on solving the problem that they overlooked the underlying theoretical principles required for an accurate solution.This observation aligns with findings from (Zheng et al. 2023), which introduces a novel prompting technique that encourages LLMs to step back and ask questions to better understand the background of a problem.Inspired by this, we hypothesized that by fine-tuning our LLMs on solutions that first pause to consider the theoretical framework, we could guide them to produce more accurate responses.Building on this idea, we didn't stop at just providing the theoretical framework; we went a step further.We designed a comprehensive, step-by-step structured solution format for fine-tuning, which we believe will train the LLM to approach reasoning tasks more methodically and with greater accuracy.To achieve this, we transformed the existing solutions in our datasets into a clear, organized format under the following headings: (i) Data Identification, (ii) Problem Analysis, (iii) Theoretical Framework, (iv) Methodology Development, (v) Computation, and (vi) Answer.(see Figure 1) To guide this process, our team created few-shot examples that illustrate how sample answers should be divided into this structured format.These examples, along with a detailed prompt, were provided to GPT-4, which then generated structured solutions for all problems in our training and testing datasets.A team of 5 human experts then identified and corrected any mistakes in the structured solutions.</p>
<p>Originally, our datasets were in English.After structuring the solutions, we used LLAMA 3 (405B) to translate both the questions and their structured solutions into Hindi.Finally, the English and Hindi versions of GSM8K, MATH, and IndiMathQA are combined on the basis of Easy, Medium, and Hard.This results in a total of 2184 Easy, 5470 Medium, and 8527 Hard problems in our dataset.</p>
<p>Curriculum Learning based Fine-Tuning We apply the technique of Curriculum Learning to SLLMs, hypothesizing that by incrementally increasing the complexity of problems during fine-tuning, we can simulate the natural process of human learning-where mastering simpler tasks paves the way for tackling more challenging ones.Our approach utilizes the Easy and Medium datasets, carefully constructed to cover a diverse range of mathematical topics.Each dataset was divided into 70% for training and 30% for testing, ensuring this split was consistently applied across all problem categories: easy, medium, and hard.</p>
<p>To implement Curriculum Learning, we first train our SLLMs on the Easy dataset, producing a model checkpoint we refer to as SFT Easy.This checkpoint is then further fine-tuned using the Medium dataset, resulting in the final checkpoint, SFT Easy+Medium.We evaluate the performance difference between these two checkpoints using testing sets from both benchmark datasets and our curated dataset.</p>
<p>We propose a hypothesis that fine-tuning LLMs on a dataset combining identical question-answer pairs in both English and Hindi could enhance the model's ability to understand and reason through math problems in Hindi-a language where the LLM might not be as proficient.Our reasoning is grounded in the idea that by exposing the LLM to parallel data in English, a language it excels in, the model can leverage its strengths in English to build stronger associations and improve its performance in Hindi.To test this hypothesis, our Curriculum Learning-based fine-tuning is conducted in two distinct ways: 1. Training the SLLMs separately on English and Hindi datasets, with results presented in</p>
<p>Ablation Study</p>
<p>To comprehensively understand the results and significance of each novel methodology employed in our study, we evaluated performance at every stage.In our experiments with the Hindi dataset, Table 1 shows accuracy metrics attained by base models employing zero-shot and few-shot prompting.The findings underscore a substantial performance enhancement with few-shot prompting compared to zero-shot, demonstrating a notable increase of 20-50% across all operations.This highlights the effectiveness of providing task examples to LLMs.Further, fine-tuning on an enhanced HAWP dataset led to substantial improvements (20-30% in general) in OpenHathi's performance in addition and subtraction tasks, and in WizardMath's performance across all operations.However, LLeMMA-7B's performance declined after fine-tuning.After manual assessment of its responses,  we found that it is exhibiting hallucinations in its solutions.This aligns with recent findings that fine-tuning on new knowledge can increase hallucinations (Gekhman et al. 2024).LLeMMA, primarily pre-trained on English mathematical data, showed hallucinations when provided with new Hindi mathematical knowledge.Our novel Decomposition Strategy significantly enhanced LLeMMA's performance, demonstrating that breaking down complex calculations can reduce hallucinations and enhance reasoning skills.Additionally, addressing hallucinations through augmentation of samples proved effective, as shown by the improvements from instruction-tuning on HMQA for both Open-Hathi and LLeMMA.A detailed analysis of the benefits of curriculum learning on both Hindi and English datasets is also provided in the following Results &amp; Analysis section.</p>
<p>Result &amp; Analysis</p>
<p>In this section, we first examine the impact of Curriculum Learning based fine-tuning in Hindi and English separately.The analysis then explores the results from bilingual combined training.Lastly, we compare the problem-solving capabilities of lightweight open-source models (SLLMs) against closed-source models (LLMs) across different languages and difficulty levels.</p>
<p>Curriculum Learning -English Training</p>
<p>We explore the impact of Curriculum Learning on English Dataset on the performance of SLLMs.In the base setting, the models were fine-tuned on the entire English dataset without distinguishing problem complexity.In this setting, WizardMath-7B demonstrated the highest perfor-mance, while LLeMMA-7B exhibited the lowest performance across all benchmarks and our English dataset, EMKB, as shown in Table 2. Following this, the models underwent fine-tuning on a subset of easy problems (SFT easy), leading to a 4-6% improvement on easy problems and a 6-8% increase on the GSM8K benchmark, indicating effective learning of simpler questions during this phase.However, the improvements on more challenging benchmarks like MATH and PRM800K were modest, with only a 1-2% increase.In the next stage, models were fine-tuned on both easy and medium problems (SFT easy+medium).This approach yielded a consistent 6% performance increase on medium problems and a 3% improvement on hard problems.These findings (Table 2), suggest that systematically increasing the difficulty of problems enables models to surpass their base setting performance.</p>
<p>Curriculum Learning -Hindi Training</p>
<p>When applying Curriculum Learning to the Hindi datasets, initially, WizardMath-7B led, while LLeMMA-7B lagged on the Enhanced HAWP Benchmark.Fine-tuning on easy problems (SFT easy) improved performance by 3-5%, but gains on medium and hard problems were minimal.Introducing Curriculum Learning (SFT easy+medium) led to an additional 2-4% improvement on the benchmark and 3-5% on more difficult problems (Table 2).This stepwise training approach effectively enhanced the models' ability to tackle increasingly complex tasks, demonstrating the value of a structured learning regimen in Hindi datasets.</p>
<p>Curriculum Learning -Bilingual Combined Training</p>
<p>Finally, we tested the performance of SLLMs on full Indi-MathQA dataset, covering both Hindi and English versions (Tables 3 and 4).SLLMs were fine-tuned using Curriculum Learning on a bilingual combined training set.As a general trend, all models that went through a combined bilingual training (Tables 3 and 4) performed better on Hindi Benchmarks in comparison to single language fine-tuning (Table 2).This is a remarkable enhacement achieved from our hypothesis that combined fine-tuning on English and Hindi can help improve model's Hindi Mathematical Reasoning.Initially, WizardMath-7B achieved the highest performance, while Airavata-7B had the lowest results (Base Settings: Table 2).Fine-tuning on easy problems (SFT Easy: Table 3) in both languages led to a consistent 3-5% improvement on easy questions, enhancing the models' ability to generalize across different linguistic contexts.However, improvements on medium and hard problems were minimal, highlighting the limitations of focusing solely on easy problems.</p>
<p>When fine-tuned on both easy and medium problems in both languages (SFT Easy+Medium: Table 4), the models showed more significant gains, with medium problems improving by 11-18% and hard problems by around 2%.This demonstrates the effectiveness of Curriculum Learning in enhancing problem-solving abilities and leveraging bilingual training.</p>
<p>Fine-Tuning Open-Source Models (SLLMs)</p>
<p>In our evaluation of the performance of open-source models such as LLaMA-7B, LLeMMA-7B, Mistral-7B, MAmmoTH-7B, and WizardMath-7B when fine-tuned on combined both Hindi and English versions of IndiMathQA (HMKB and EMKB) (Tables 3 and 4), we observed that fine-tuning on both languages combined improves the performance on both the languages significantly compared to the gains when fine-tuning on a single language.As shown in Table 3 and 4, fine-tuning on easy problems from both languages led to a marginal 2-3% performance increase on easy problems in both Hindi and English.This improvement is better than the pre-trained models but less substantial than the improvements seen with single-language finetuning, as indicated in Table 2.However, Table 3 and 4 further demonstrate that fine-tuning easy and medium problems from both languages resulted in a significant major improvement of 11-18% accuracy.</p>
<p>SLLMs (Lightweight open-source) vs LLMs (closed-source)</p>
<p>WizardMath-7B is the best-performing SLLM in our research.Although GPT-4 performance exceeds even the enhanced performance of WizardMath (Table 2, 3 and 4), through Curriculum Learning (SFT easy+medium) and Bilingual Parallel Training, WizardMath-7B outperforms Gemini 1.0 Pro in English datasets by about 5% (Table 2, 3 and 4).This improvement highlights the effectiveness of our methodology in enhancing SLLM's problem-solving abilities in English.However, in Hindi datasets, while WizardMath-7B performance is comparable to Gemini Pro, it still lags by approximately 3% across Medium and Hard difficulties, likely because WizardMath is more proficient in solving math problems in English than in Hindi.</p>
<p>English Models vs Bilingual Models</p>
<p>Finally, in this comparative analysis of bilingual models and other open-source models (Tables 2, 3 and 4), we observe that bilingual models perform consistently better across English and Hindi, unlike most open-source models, except for WizardMath-7B.This consistency is likely due to the language-independent nature of mathematical reasoning.However, bilingual models like OpenHathi-7B, which are not pre-trained on mathematical tasks, show only slight improvement after fine-tuning, suggesting limited learning efficiency.The superior performance of WizardMath-7B highlights the importance of pre-training models on mathematical tasks for robust performance across languages.</p>
<p>Models</p>
<p>Model Parameters and Data Explanation Model Methodology Overview</p>
<p>Our research employed an advanced sequence inference technique to fine-tune our model for generating responses to specific prompts.These prompts were meticulously crafted with clear instructions and sample questions to ensure the model produced accurate and varied responses.</p>
<p>Hyper parameters Used:</p>
<p>• Sampling: Enabled • Top k: 40 (for relevance and diversity)</p>
<p>• Temperature: 0.8 (for balanced creativity)</p>
<p>• Top p: 0.90 (for refined nucleus sampling)</p>
<p>• Maximum Length: 4096 (for comprehensive answers)</p>
<p>• Fine-tuning: 3 epochs</p>
<p>Fine-Tuning Prompt</p>
<p>Definition: This prompt is designed to fine-tune a model by providing a specific instruction and a corresponding response.The focus is on ensuring that the model accurately completes the task while being cautious of incorrect calculations and avoiding repetition of errors.</p>
<p>Below is an instruction that describes a task.Write a response that appropriately completes the request.Be aware of wrong calculations and do not repeat them.</p>
<h3>Instruction: {sample['Question']} ### Response: {sample['Response']}</h3>
<p>Data Augmentation Prompt</p>
<p>Definition: This prompt is used for generating new, conceptually similar questions and answers based on an existing problem.It involves creating problems with varied difficulty levels to enhance the diversity and complexity of the dataset.</p>
<p>Your task is to create a similar concept--ual question and answer with diverse difficulty levels (either similarly simp--le, the same, or more complex) using the provided problem.</p>
<p>Decomposition Strategy Prompt</p>
<p>Definition: This prompt is used for generating updated solutions of HAWP dataset using the Decomposition Strategy, for questions involving multiplication and division.HAWP contains single-operator word problems, so we divided each problem by the type of operation used in the solution.We used separate prompts for Multiplication problems and Division problems.We also gave examples to support the generation.</p>
<p>Your task is modify the following mathem--atical solution by breaking down the mu--ltiplicand into place value components (hundreds, tens, ones, etc.) and then multiplying each component by the other multiplicand.Then, sum the products to get the final result.</p>
<p>The Decomposition Strategy prompt for division is given below</p>
<p>Your task is modify the following mathem--atical solution by decomposing the dividend into segments, then divide each by the divisor, and sum the quotients to obtain the final answer.</p>
<p>Explanation of Key Terms and Methodology Components</p>
<p>In our study, we developed a structured approach to provide solutions for each raw question-answer pair, consisting of the following steps:</p>
<p>Data Identification: Specifying the relevant data needed for the problem.This includes identifying variables, constants, and any conditions or parameters related to the problem.</p>
<p>Problem Analysis: Examining the problem to understand its components and determine suitable methods for solving it.This includes understanding theoretical aspects such as set theory or integration rules.</p>
<p>Theoretical Framework: Establishing the foundational theories and principles that guide the solution.Examples include using set operations for probability problems or antiderivative rules for integrals.</p>
<p>Methodology Development: Creating a detailed, stepby-step plan to solve the problem.This includes developing necessary procedures and formulae for the solution.</p>
<p>Computation: Performing the calculations and applying the developed methodology to derive the final solution.</p>
<p>Solution:</p>
<p>Presenting the final answer, ensuring it is clear and easy to understand.</p>
<p>Data Classification: Easy, Medium, and Hard</p>
<p>To classify data into easy, medium, and hard categories, we use the following criteria:</p>
<ol>
<li>Language Understanding: Assesses the complexity of the language used.Problems with straightforward, clear language are classified as easy, while those with technical or complex phrasing are considered medium or hard.2. Mathematical Complexity: Assesses the nature of mathematical operations required.Problems involving fundamental concepts are easy, whereas those requiring multiple or intricate operations are medium or hard.3. Reasoning Complexity: Evaluates the complexity of reasoning needed.Problems requiring straightforward application of concepts are easy, while those needing detailed logic or multi-step reasoning are medium or hard.4. Number of Variables: Evaluates the number of variables involved.Problems with a few variables are easy, whereas those with many or complex variables are medium or hard. 5. Conceptual Complexity: Assesses the depth of understanding required.Basic conceptual understanding is easy, while advanced or abstract concepts are medium or hard.</li>
</ol>
<p>Structured Problem Solving: Examples of Data Transformation</p>
<p>We have raw data in the format of Question and Final Answer, which is then converted into a Refined Solution.</p>
<p>These examples span various topics, including but not limited to Probability, Linear Algebra, Integration, and many more.Following are examples of few instances from the curated IndiMathQA dataset.</p>
<p>Example 1: Conic Sections Question: Find the equation of the hyperbola with foci (0, ±3) and vertices 0, ± √ 11 2</p>
<p>. Raw Solution: Given the foci on the y-axis, the equation of the hyperbola is of the form
y 2 a 2 − x 2 b 2 = 1.
Using the given vertices 0, ± Problem Analysis: We need to determine the standard form of the equation of the hyperbola given its foci and vertices.For a hyperbola centered at the origin and oriented along the y-axis, its general equation is
y 2 a 2 − x 2 b 2 = 1,
where: -The distance from the center to a vertex is a. -The distance from the center to a focus is c. -The relationship c 2 = a 2 + b 2 holds true.</p>
<p>Theoretical Framework: Key concepts and formulae for hyperbolas include: -Standard form of hyperbola equation: Simplifying by multiplying through by 100 to clear the denominators: 100y 2 − 44x 2 = 275 Hence, the equation of the hyperbola is:
100y 2 − 44x 2 = 275
Example 2: Probability Question: A committee of two persons is selected from two men and two women.What is the probability that the committee will have (a) no man?(b) one man?(c) two men?</p>
<p>Raw Solution: The total number of persons = 2 + 2 = 4. Out of these four persons, two can be selected in 4 C 2 ways.</p>
<p>(a) No men in the committee of two means there will be two women in the committee.</p>
<p>Out of two women, two can be selected in 2 C 2 = 1 way.Therefore,
P(no man) = 2 C 2 4 C 2 = 1 × 2 × 1 4 × 3 = 1 6
(b) One man in the committee means that there is one woman.One man out of 2 can be selected in 2 C 1 ways and one woman out of 2 can be selected in 2 C 1 ways.Together they can be selected in 2 C 1 × 2 C 1 ways.Therefore,
P(One man) = 2 C 1 × 2 C 1 4 C 2 = 2 × 2 2 × 3 = 2 3
(c) Two men can be selected in 2 C 2 way.Hence,
P(Two men) = 2 C 2 4 C 2 = 1 4 C 2 = 1 6 Final Answer:
• The probability that the committee will have no man is 1 6 .</p>
<p>• The probability that the committee will have one man is 2 3 .</p>
<p>• The probability that the committee will have two men is 1 6 .Refined Solution: Data Identification: • Total people: 2 men and 2 women.• Committee size: 2 persons.</p>
<p>Problem Analysis: • Calculate the total number of ways to form a committee of 2 persons from 4 people.• Determine the number of favorable outcomes for:</p>
<p>-No man in the committee.</p>
<p>-One man in the committee.</p>
<p>-Two men in the committee.</p>
<p>• Calculate the probability for each case by dividing the number of favourable outcomes by the total number of ways to form a committee.Theoretical Framework: • Use the combination formula C(n, r) which denotes the number of ways to choose r items from n items without regard to the order, given by C(n, r) = Methodology Development: 1.Total number of ways to form a committee of 2 out of 4 people: C(4, 2). 2. Number of ways to form a committee with:</p>
<p>• No man: C(2, 2) (choosing 2 women out of 2).</p>
<p>• One man: C(2, 1) × C(2, 1) (choosing 1 man out of 2 and 1 woman out of 2).• Two men: C(2, 2) (choosing 2 men out of 2).</p>
<p>Computation:</p>
<ol>
<li>Total number of ways to form the committee:
C(4, 2) = 4! 2!(4 − 2)! = 4 × 3 2 × 1 =6</li>
<li>Number of favorable outcomes:</li>
</ol>
<p>• No man:
C(2, 2) = 2! 2! × 0! = 1
• One man:
C(2, 1) × C(2, 1) = 2! 1! × (2 − 1)! × 2! 1! × (2 − 1)! = 2 × 2 = 4
• Two men:
C(2, 2) = 2! 2! × 0! = 1
3. Calculate the probabilities:</p>
<p>• Solution:</p>
<p>• The probability that the committee will have no man is 1 6 .• The probability that the committee will have one man is 2 3 .</p>
<p>• The probability that the committee will have two men is 1 6 .These refined solutions provide clear, step-by-step explanations of the problem, ensuring the process is thoroughly documented and easier to follow.This structured approach enhances the accuracy and clarity of solution generation using large language models (LLMs).</p>
<p>Consistency of Topics across Difficulties</p>
<p>In this section, we demonstrate that during our dataset preparation and categorization of questions into Easy, Medium, and Hard levels, we ensured consistent representation of mathematical topics across all complexity levels in both the MATHS and IndiMath datasets.Figure 3 and 4 illustrates the uniform distribution of each mathematical topic at each difficulty level, highlighting the LLM's ability to effectively learn and generalize across topics from easy to hard levels.remove Math dataset</p>
<p>Curriculum Learning Analysis</p>
<p>In this section, we present heatmap visualizations showing each model's final (best) performance for easy, medium, and hard questions in both languages, achieved through our experimentations.Figure 5 and 6 show that WizardMath consistently outperforms other open-source counterparts across all difficulties, while LLeMMA performs the worst.These results were obtained after we were able to counter the hallucination problem in LLeMMA to some extent through our strategies as discussed in the Ablation Study (Section 4) of the Main Paper.</p>
<p>Error Analysis: Demonstrating the effect of Curriculum Learning</p>
<p>Here we present the answers of the WizardMath 7B model before and after SFT based on Curriculum Learning.</p>
<p>• Question: A family has two children.What is the probability that both children are boys given that at least one of them is a boy? • Wizard Math Base Model Response: To solve the problem of finding the probability that both children are boys given that at least one of them is a boy, we assume that if one child is a boy, the other must also  be a boy.Therefore, the probability that both children are boys is 100%.</p>
<p>• Wizard Math SFT Easy Response: To solve the problem of finding the probability that both children are boys given that at least one of them is a boy, we use conditional probability.There are four possible combinations for two children: (Boy, Boy), (Boy, Girl), (Girl, Boy), and (Girl, Girl).Since we know at least one child is a boy, we eliminate the (Girl, Girl) scenario, leaving us with (Boy, Boy), (Boy, Girl), and (Girl, Boy).Each of these scenarios has an equal probability of 1/4.Thus, the probability of at least one child being a boy is 3/4.The probability that both children are boys given this condition is calculated as the probability of both being boys (1/4) divided by the probability of at least one being a boy (3/4), resulting in 1/2.Therefore, the probability that both children are boys given that at least one is a boy is 1/2.• Wizard Math SFT Easy+Medium Response: To solve the problem of finding the probability that both children are boys given that at least one of them is a boy, we use conditional probability.There are four possible combinations for two children: (Boy, Boy), (Boy, Girl), (Girl, Boy), and (Girl, Girl).Since we know at least one child is a boy, we eliminate the (Girl, Girl) scenario, leaving us with (Boy, Boy), (Boy, Girl), and (Girl, Boy).Each of these scenarios has an equal probability of 1/4.Thus, the probability of at least one child being a boy is 3/4.The probability that both children are boys given this condition is calculated as the probability of both being boys (1/4) divided by the probability of at least one being a boy (3/4), resulting in 1/3.Therefore, the probability that both children are boys given that at least one is a boy is 1/3.• Comments: The base model answer demonstrates its inability to apply probability logic correctly.After Curriculum learning with SFT on only Easy examples, the model's reasoning improved substantially, but the model faced an error in calculation at the very last step.This error was rectified when the model was fine-tuned on Easy+Medium, demonstrating the benefit of Curriculum Learning in teaching the LLM the basics of Math, like logical reasoning and calculations, before solving complex questions.</p>
<p>Annotation Analysis IndiMathQA</p>
<p>Our augmented dataset comprises question-answer pairs generated by GPT-4 and verified by five human annotators.This section provides an analysis of the annotation process and the consistency among annotators.</p>
<p>Evaluation Methodology and Inter-Annotator Agreement for GPT-4 Generated Responses</p>
<p>The evaluation methodology for GPT-4 generated responses involved a rigorous review process conducted by five independent human annotators.Each question-answer pair was meticulously evaluated for accuracy, relevance, and clarity.The verification process encompassed all aspects of the data, including data identification, problem analysis, theoretical framework, methodology development, computation, and the final solution.To ensure the reliability of these annotations, inter-annotator agreement was assessed using Fleiss' Kappa, focusing on two critical aspects.The first aspect was the validation of solution correctness, where the annotators assessed whether the generated solution appropriately matched the given question.The Fleiss' Kappa score for this validation was 0.71, indicating substantial agreement among the annotators and confirming the reliability of the model's output.The second aspect involved classifying the data into easy, medium, and hard categories based on criteria such as language understanding, mathematical complexity, reasoning complexity, number of variables, and conceptual complexity.The Fleiss' Kappa score for this classification task was 0.58, reflecting moderate agreement.This score suggests that while there was general consensus, some variation existed among the annotators, likely due to subjective interpretations of what constitutes a medium versus a hard problem.Such differences may arise from personal preferences or individual experiences with similar tasks.These Kappa scores collectively validate the robustness of the annotation process, demonstrating a high level of consistency in the human evaluation used to verify and classify GPT-4's generated responses.</p>
<p>Discrepancies and Resolutions</p>
<p>Discrepancies among the annotators' evaluations were resolved through a majority voting system.If a majority decision could not be reached due to close scores or ambiguity, a domain expert reviewed the responses to make the final decision.If the domain expert could not reach a consensus, the solution was manually corrected and re-evaluated.</p>
<p>Figure 1 :
1
Figure 1: Curriculum Learning with Structured Solutions: A Comprehensive Framework to Gradually Guide Models Through Complex Mathematical Challenges.</p>
<ol>
<li>Bilingual Combined Training: We propose the methodology of Bilingual Combined Training, where the Structured Solution Approach with Curriculum Learning is applied on a dataset containing both English and Hindi versions of Mathematical questions-answers.4. IndiMathQA Dataset Creation: We developed the In-diMathQA dataset by curating 598 math problems from NCERT 3 textbooks for grades 10-12, spanning 14 mathematical domains.Expert annotations categorized these into easy, medium, and hard problems.The dataset was expanded to 7,823 questions. 5. Performance Analysis of Multilingual LLMs: A comprehensive analysis of several LLMs, including five open-source English LLMs, three open-source Hindi LLMs, and two closed-source models, was conducted using both English and Hindi mathematical datasets to assess their capabilities and limitations.</li>
</ol>
<p>Figure 2 :
2
Figure 2: Overall Methodology: The top section illustrates our primary approach, which combines Curriculum Learning and Bilingual Integrated Training.The bottom section depicts the process of applying the decomposition strategy to the HAWP dataset.</p>
<p>Answer: {sample['solution']} New Answer: {sample['updated_solution']}One of the examples provided as few-shot example is given below:</p>
<p>Answer: {sample['solution']} New Answer: {sample['updated_solution']} One of the examples provided as few-shot example is given below: the quotients: 50 + 10 + 0.5 = 60.5 Final Answer: 968 divided by 16 equals 60.5.</p>
<p>Using the relationship b 2 = c 2 − a 2 , we find b 2</p>
<p>x 2 b 2 = 1 -Distance to the vertices: ±a -Distance to the foci: ±c -Relationship: c 2 = a 2 + b 2 Methodology Development: From the given foci and vertices, we can identify a and c.Using the relationship c 2 = a 2 + b 2 , we can find b and formulate the hyperbola's equation.the relationship c 2 = a 2 + b 2 : c 2 = a 2 + b 2 =⇒ 3 2 = Using the values of a 2 and b 2 , the equation of the hyperbola is: Using the values of a 2 and b 2 , the equation of the hyperbola is:</p>
<p>n−r)! .• Probability of an event P (E) = Number of favorable outcomes Total number of outcomes .</p>
<p>Figure 3 :
3
Figure 3: Bar graph showing topic distribution across each difficulty level extracted from the MATH dataset</p>
<p>Figure 4 :
4
Figure 4: Bar graph showing topic distribution across each difficulty level extracted from the IndiMathQA dataset</p>
<p>Figure 5 :
5
Figure 5: Heatmap showing English mathematical performance across Difficulties (Easy, Medium, Hard) on each Mathematical Models.</p>
<p>Figure 6 :
6
Figure 6: Heatmap showing Hindi mathematical performance across Difficulties (Easy, Medium, Hard) on each Mathematical Model.</p>
<p>Table 1 :
1Our proposed strategies are namely Structured Solution Cre-ation, Curriculum Learning, and Bilingual Training in Hindiand English. We explain the Bilingual Training Dataset Cre-ation in two phases: (i) Classification based on Complex-ity (required for Curriculum Learning), (ii) Structured Solu-tion Creation and Bilingual Translations. Finally, we demon-strate how we conducted curriculum learning based bilin-gual fine-tuning on our training datasets.
Performance of LLMs in Hindi Math questions using decomposition strategy.Bold values indicate improvements from the previous step.Underlined values show the highest performance of SLLMs for each operation.</p>
<p>Table 2 2
2
. Employing Bilingual Combined Training, where the model is trained on a combined dataset of both English and Hindi question-answer pairs.The outcomes of this bilingual training are detailed in Table3 and 4.For our evaluation of SLLMs in Hindi Math reasoning, we only evaluate performance on the Hindi version of Indi-MathQA and the HAWP dataset.For the purpose of clarity, we refer the Hindi version of IndiMathQA as HMKB and the English version as EMKB.(Table2)</p>
<p>Table 2 :
2
Performance Comparison of Open-Source and Closed-Source Models on English and Hindi Mathematical Benchmarks</p>
<p>Table 3 :
3
Performance of Bilingual Models on IndiMathQA Using SFT easy Training
ModelsIndiMathQAEMKBHMKBEasy Medium Hard Easy Medium HardLLaMA-7B44%35%23% 29%24%19%Llemma-7B21%14%8% 15%10%9%Mistral-7B50%39%29% 33%26%22%Mammoth-7B40%20%18% 44%35%27%WizardMath-7B 75%66%47% 72%57%45%Bilingual ModelsOpenHathi-7B 43%33%24% 40%37%31%Airavata25%16%13% 18%14%11%Hi-NOLIN40%30%21% 39%35%28%</p>
<p>Table 4 :
4
Performance of Bilingual Models on IndiMathQA Using SFT easy+medium Training
ConclusionThis research developed a Bilingual Math Problem Solverusing curriculum learning, query decomposition, and struc-tured solution generation. The Decomposition Strategyimproved reasoning by breaking down complex queries,Structured Solution addressed the problem of Hallucina-tions, while curriculum learning enhanced performance onmedium and hard problems. WizardMath-7B consistentlyoutperformed other SLLMs (Lightweight open-source)models and often surpassed closed-source models like Gem-ini 1.0 Pro with these strategies. Our findings demonstratethat integrating these methodologies significantly enhancesthe problem-solving capabilities of LLMs. Bilingual Paral-lel Training (Training in multiple languages) provided di-verse problem-solving perspectives, proving more effectivethan single-language training. This study shows how thesediverse methodologies can be used to address issues withLLMs in math problem-solving, and can effectively enhancetheir performance in Hindi.</p>
<p>Distribution of Math Questions Across Levels by Topic
Topics Binomial Theorem Conic Sections Limits and Derivatives Complex Numbers and Quadratic Equations Permutation Combination Probability Relations and Functions Sequence and Series Statistics Trigonometric Functions Sets
Code, Dataset and Appendix are available at https://github.com/midas-research/Multilingual-Mathematical-Reasoning.git
https://udiseplus.gov.in/udisereport/
NCERT is a major textbook for school students in India. https: //ncert.nic.in/
NCERT is a major textbook for school students in India. https: //ncert.nic.in/
AcknowledgmentsDr. Rajiv Ratn Shah is partly supported by the Infosys Center for AI, the Center of Design and New Media, and the Center of Excellence in Healthcare at Indraprastha Institute of Information Technology, Delhi.AppendixThe following appendix provides a detailed exploration of our research, including: Hyperparameters used and prompts developed, Explanations of key terms in the methodology that were not included in the main paper, Few-shot examples for certain prompts, Additional analysis of the IndiMathQA and MATH datasets (including question types and distribution), A comprehensive error analysis to demonstrate the effect of curriculum learning, and lastly, Annotation analysis of IndiMathQA (including inter-annotator agreements).This supplementary information offers a deeper understanding of our approach and findings.Ethics StatementThe datasets used for training and testing the LLM were sourced from publicly available repositories.We acknowledge the potential for bias inherent in language model training datasets.The collected data comes from NCERT textbooks, that are used across India for high school education, hence representing the extended demographic within India.LimitationsA limitation of our study is that we do not evaluate on all the available sources of data available.Future iterations of this research aim to expand the scope of evaluation, with the help of the research community to collect more comprehensive data from various available sources.Additionally, our study did not include the examination of romanized Hindi sentences, where Hindi word for elephant are written using the English alphabet as "Hathi."This form of input is prevalent in India, particularly when typing on electronic devices.Future research could beneficially extend to enhancing model performance on such inputs as well.The Easy/Medium/Hard distribution of questions was done manually, which can add bias to the distribution as we increase the dataset size.
S Ai, OpenHathi Series: An Approach To Build Bilingual LLMs Frugally. 2023</p>
<p>KG-CTG: citation generation through knowledge graph-guided large language models. A Anand, M Gupta, K Prasad, U Goel, N Lal, A Verma, R R Shah, International Conference on Big Data Analytics. Nature Switzerland ChamSpringer2023a</p>
<p>Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks. A Anand, M Gupta, K Prasad, N Singla, S Sanjeev, J Kumar, A R Shivam, R R Shah, 2023b</p>
<p>GeoVQA: A Comprehensive Multimodal Geometry Dataset for Secondary Education. A Anand, R Jaiswal, A Dharmadhikari, A Marathe, H Popat, H Mital, A R Nair, K Prasad, S Kumar, A Verma, R R Shah, R Zimmermann, IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR). 2024a. 2024</p>
<p>A Anand, R Jaiswal, A Dharmadhikari, A Marathe, H P Popat, H Mital, K Prasad, R R Shah, R Zimmermann, arXiv:2412.00846Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And Multistep Scoring. 2024barXiv preprint</p>
<p>MM-PhyQA: Multimodal Physics Question-Answering with Multi-image CoT Prompting. A Anand, J Kapuriya, A Singh, J Saraf, N Lal, A Verma, R Gupta, R Shah, Pacific-Asia Conference on Knowledge Discovery and Data Mining. Singapore SingaporeSpringer Nature2024c</p>
<p>Advances in Citation Text Generation: Leveraging Multi-Source Seq2Seq Models and Large Language Models. A Anand, A R Nair, K Prasad, V Narayan, N Lal, D Mahata, Y K Singla, R R Shah, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM '24. the 33rd ACM International Conference on Information and Knowledge Management, CIKM '24New York, NY, USAAssociation for Computing Machinery2024dISBN 9798400704369</p>
<p>Z Azerbayev, H Schoelkopf, K Paster, M D Santos, S Mcaleer, A Q Jiang, J Deng, S Biderman, S Welleck, arXiv:2310.10631Llemma: An Open Language Model For Mathematics. 2023</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>N Dave, D Kifer, C L Giles, A Mali, arXiv:2405.13209Investigating Symbolic Capabilities of Large Language Models. 2024arXiv preprint</p>
<p>Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages. J Gala, P A Chitale, R Ak, S Doddapaneni, V Gumma, A Kumar, J Nawale, A Sujatha, R Puduppully, V Raghavan, arXiv:2305.16307arXiv:2401.15006Airavata: Introducing Hindi Instruction-tuned LLM. Gala, J2023. 2024arXiv preprint</p>
<p>Z Gekhman, G Yona, R Aharoni, M Eyal, A Feder, R Reichart, J Herzig, arXiv:2405.05904Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?. 2024arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, arXiv:2310.06825Mistral 7B. 2023</p>
<p>Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More. K.-C Kao, R Wang, C.-J Hsieh, arXiv:2407.051342024Than-Two Unknowns? arXiv preprint</p>
<p>H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.20050Let's Verify Step by Step. 2023arXiv preprint</p>
<p>H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, arXiv:2308.09583WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. 2023</p>
<p>. N Research, 2023Introducing NOLIN</p>
<p>HAWP: a Dataset for Hindi Arithmetic Word Problem Solving. H Sharma, P Mishra, D Sharma, N Calzolari, F Béchet, P Blache, K Choukri, C Cieri, T Declerck, S Goggi, H Isahara, B Maegaard, J Mariani, H Mazo, Odijk, Proceedings of the Thirteenth Language Resources and Evaluation Conference. J Piperidis, S , the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2022</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>X Yue, X Qu, G Zhang, Y Fu, W Huang, H Sun, Y Su, W Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>A careful examination of large language model performance on grade school arithmetic. H Zhang, J Da, D Lee, V Robinson, C Wu, W Song, T Zhao, P Raja, D Slack, Q Lyu, arXiv:2405.00332arXiv:2310.06117Take a step back: Evoking reasoning via abstraction in large language models. 2024. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
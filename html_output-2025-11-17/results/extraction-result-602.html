<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-602 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-602</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-602</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-f6e6fba61b654c5b799a9efe3b5849ce1197aab3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f6e6fba61b654c5b799a9efe3b5849ce1197aab3" target="_blank">DeepProbLog: Neural Probabilistic Logic Programming</a></p>
                <p><strong>Paper Venue:</strong> BNAIC/BENELEARN</p>
                <p><strong>Paper TL;DR:</strong> This work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.</p>
                <p><strong>Paper Abstract:</strong> We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports both symbolic and subsymbolic representations and inference, 1) program induction, 2) probabilistic (logic) programming, and 3) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e602.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e602.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepProbLog: Neural Probabilistic Logic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic probabilistic logic programming language that extends ProbLog with neural predicates (neural annotated disjunctions and neural facts) so that neural network outputs are used as probabilistic choices inside a logic program and the whole system can be trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DeepProbLog is a hybrid reasoning system built by minimally extending the probabilistic logic programming language ProbLog with neural predicates. Neural predicates (neural annotated disjunctions and neural facts) connect external neural network models to ProbLog: at ground time the system evaluates the relevant neural networks on their inputs to obtain class-probability vectors which are instantiated as the probabilities of annotated disjunctions (ADs) or probabilistic facts. Inference follows the ProbLog pipeline: backward grounding with respect to a query, rewriting to a propositional formula, knowledge compilation to SDDs and transformation to an arithmetic circuit (AC) for weighted model counting. For learning, DeepProbLog uses the gradient semiring (via aProbLog) to compute partial derivatives ∂P(query)/∂p for probabilistic parameters on the AC and supplies those derivatives as upstream gradients into the neural networks; the chain rule then yields gradients for neural parameters which are optimized together with probabilistic parameters using standard gradient-based optimizers, enabling end-to-end training.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ProbLog probabilistic logic programming (facts, rules, annotated disjunctions, grounding, well-founded semantics), knowledge compilation to Sentential Decision Diagrams (SDDs) and conversion to arithmetic circuits (ACs) for weighted model counting; learning-from-entailment objective (training on query success probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>External neural networks used as neural predicates: arbitrary architectures (e.g., convolutional neural networks for MNIST digit recognition, recurrent networks for sequences). Neural outputs are softmax/sigmoid probability vectors; neural parameters trained via gradient descent (SGD/Adam) and backpropagation driven by the gradient provided from the logic component.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular integration where neural networks provide probability labels for probabilistic facts/ADs (neural annotated disjunctions / neural facts). At prediction grounding, relevant networks are evaluated to instantiate AD probabilities; compiled ACs using those probabilities perform probabilistic inference. For learning, aProbLog’s gradient semiring computes ∂P/∂(abstract probabilities) on the AC; these partial derivatives are used as upstream signals (∂Loss/∂p̂) and backpropagated through the neural networks (∂p̂/∂θ) via the chain rule, enabling joint optimization of logic (probabilistic) parameters and neural parameters — i.e., end-to-end differentiable training across the symbolic-probabilistic and neural parts.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines high-level symbolic/probabilistic reasoning with low-level perceptual learning: (1) supports joint reasoning over perception and symbolic structure (e.g., arithmetic constraints over digit images); (2) enables program induction-like behavior where only supervision on high-level queries trains latent neural classifiers (neural predicates) reused across tasks; (3) improved sample efficiency in tasks decomposable into perception+reasoning (e.g., addition) because the neural part learns lower-dimensional subtasks (digit classification) rather than whole composed outputs; (4) probabilistic treatment of uncertain perception allows principled uncertainty propagation through symbolic rules; (5) explanations and interpretable derivations at the logic level (proof structure, probabilistic contributions) while keeping flexible subsymbolic perception modules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MNIST-based compositional tasks (single-digit addition T1, multi-digit addition / multi-digit numbers T2, three-image sum T3, plus combined probabilistic + deep learning tasks e.g. noisy-label fraction estimation T5/T9); generally visual perception + symbolic arithmetic / program induction benchmarks constructed from MNIST.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>For the MNIST single-digit addition task (T1) reported test accuracies (percent): DeepProbLog: 97.20 ± 0.45 (N=30000), 92.18 ± 1.57 (N=3000), 67.19 ± 25.05 (N=300). (Units: percent classification/answer accuracy). Other tasks (T2, T3, T8, T9) are reported qualitatively and show that DeepProbLog generalizes to multi-digit addition and learns probabilistic parameters jointly with neural nets; precise numeric results for all tasks are given in the paper but T1 table is the clearest numeric comparative evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Baseline CNN for the T1 single-digit addition (predicting the sum directly): 93.46 ± 0.49 (N=30000), 78.32 ± 2.14 (N=3000), 23.64 ± 1.75 (N=300). (Units: percent accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Demonstrates improved sample efficiency and compositional generalization: (1) networks trained on single-digit addition can be reused without retraining to handle multi-digit addition (T2), showing compositional reuse of learned neural predicates; (2) better accuracy than an end-to-end CNN baseline particularly in low-data regimes (N=3000 and N=300), indicating stronger generalization from decomposed supervision; (3) however, purely providing logical supervision can lead to degenerate solutions (mode collapse) unless regularized (entropy regularization used in T3), indicating some fragility when supervision is only at symbolic level.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>The declarative ProbLog component supplies explicit symbolic structure and probabilistic semantics: proofs, grounded rules, and compiled ACs provide interpretable derivations and enable inspection of which probabilistic facts (including neural predicate outputs) contributed to a query probability. The neural components remain black-box perceptual modules, but are encapsulated by named neural predicates whose outputs are interpretable probability distributions used in logic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Identified limitations include: (1) risk of mode collapse when only high-level symbolic supervision is available (T3) requiring entropy/max-entropy regularization to avoid trivial solutions; (2) grounding/pruning removes irrelevant groundings, which can lead to instantiated ADs that do not sum to one because dropped classes still receive mass from the neural network — this mismatch must be handled (authors note it and proceed with instantiated ADs); (3) computational cost: knowledge compilation and AC construction (SDD -> AC) can be expensive for large grounded programs; (4) neural outputs must be normalized (softmax/sigmoid), and probabilistic parameters require clipping/renormalization; (5) the interpretability is limited by the black-box neural modules — explanations are symbolic but do not open the neural internals.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Semantics defined by reducing DeepProbLog programs to standard ProbLog semantics by instantiating neural ADs/facts into ADs/facts with probabilities obtained from neural networks; learning/inference theory rests on ProbLog's weighted model counting via knowledge compilation (SDDs -> ACs) and aProbLog's gradient semiring which provides exact derivatives ∂P(query)/∂(probabilistic parameters) on the AC; combined with the chain rule to obtain gradients w.r.t. neural parameters, yielding a principled end-to-end differentiable framework integrating logic, probability, and neural learning. Conceptually motivated by the principle that neuro-symbolic integration should subsume pure neural, logical and probabilistic methods as special cases and that the probabilistic formulation provides a natural differentiable objective (query likelihood) for joint optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Key representational primitives: neural annotated disjunctions (nADs) and neural facts which allow neural network outputs to act as probabilistic annotations in ProbLog. Implementation leverages ProbLog inference and aProbLog gradient extensions; experiments demonstrate sample-efficiency and compositional generalization on MNIST-based reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepProbLog: Neural Probabilistic Logic Programming', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProbLog <em>(Rating: 2)</em></li>
                <li>Algebraic ProbLog <em>(Rating: 2)</em></li>
                <li>Sentential Decision Diagrams <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-602",
    "paper_id": "paper-f6e6fba61b654c5b799a9efe3b5849ce1197aab3",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "DeepProbLog",
            "name_full": "DeepProbLog: Neural Probabilistic Logic Programming",
            "brief_description": "A neuro-symbolic probabilistic logic programming language that extends ProbLog with neural predicates (neural annotated disjunctions and neural facts) so that neural network outputs are used as probabilistic choices inside a logic program and the whole system can be trained end-to-end.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepProbLog",
            "system_description": "DeepProbLog is a hybrid reasoning system built by minimally extending the probabilistic logic programming language ProbLog with neural predicates. Neural predicates (neural annotated disjunctions and neural facts) connect external neural network models to ProbLog: at ground time the system evaluates the relevant neural networks on their inputs to obtain class-probability vectors which are instantiated as the probabilities of annotated disjunctions (ADs) or probabilistic facts. Inference follows the ProbLog pipeline: backward grounding with respect to a query, rewriting to a propositional formula, knowledge compilation to SDDs and transformation to an arithmetic circuit (AC) for weighted model counting. For learning, DeepProbLog uses the gradient semiring (via aProbLog) to compute partial derivatives ∂P(query)/∂p for probabilistic parameters on the AC and supplies those derivatives as upstream gradients into the neural networks; the chain rule then yields gradients for neural parameters which are optimized together with probabilistic parameters using standard gradient-based optimizers, enabling end-to-end training.",
            "declarative_component": "ProbLog probabilistic logic programming (facts, rules, annotated disjunctions, grounding, well-founded semantics), knowledge compilation to Sentential Decision Diagrams (SDDs) and conversion to arithmetic circuits (ACs) for weighted model counting; learning-from-entailment objective (training on query success probabilities).",
            "imperative_component": "External neural networks used as neural predicates: arbitrary architectures (e.g., convolutional neural networks for MNIST digit recognition, recurrent networks for sequences). Neural outputs are softmax/sigmoid probability vectors; neural parameters trained via gradient descent (SGD/Adam) and backpropagation driven by the gradient provided from the logic component.",
            "integration_method": "Modular integration where neural networks provide probability labels for probabilistic facts/ADs (neural annotated disjunctions / neural facts). At prediction grounding, relevant networks are evaluated to instantiate AD probabilities; compiled ACs using those probabilities perform probabilistic inference. For learning, aProbLog’s gradient semiring computes ∂P/∂(abstract probabilities) on the AC; these partial derivatives are used as upstream signals (∂Loss/∂p̂) and backpropagated through the neural networks (∂p̂/∂θ) via the chain rule, enabling joint optimization of logic (probabilistic) parameters and neural parameters — i.e., end-to-end differentiable training across the symbolic-probabilistic and neural parts.",
            "emergent_properties": "Combines high-level symbolic/probabilistic reasoning with low-level perceptual learning: (1) supports joint reasoning over perception and symbolic structure (e.g., arithmetic constraints over digit images); (2) enables program induction-like behavior where only supervision on high-level queries trains latent neural classifiers (neural predicates) reused across tasks; (3) improved sample efficiency in tasks decomposable into perception+reasoning (e.g., addition) because the neural part learns lower-dimensional subtasks (digit classification) rather than whole composed outputs; (4) probabilistic treatment of uncertain perception allows principled uncertainty propagation through symbolic rules; (5) explanations and interpretable derivations at the logic level (proof structure, probabilistic contributions) while keeping flexible subsymbolic perception modules.",
            "task_or_benchmark": "MNIST-based compositional tasks (single-digit addition T1, multi-digit addition / multi-digit numbers T2, three-image sum T3, plus combined probabilistic + deep learning tasks e.g. noisy-label fraction estimation T5/T9); generally visual perception + symbolic arithmetic / program induction benchmarks constructed from MNIST.",
            "hybrid_performance": "For the MNIST single-digit addition task (T1) reported test accuracies (percent): DeepProbLog: 97.20 ± 0.45 (N=30000), 92.18 ± 1.57 (N=3000), 67.19 ± 25.05 (N=300). (Units: percent classification/answer accuracy). Other tasks (T2, T3, T8, T9) are reported qualitatively and show that DeepProbLog generalizes to multi-digit addition and learns probabilistic parameters jointly with neural nets; precise numeric results for all tasks are given in the paper but T1 table is the clearest numeric comparative evidence.",
            "declarative_only_performance": null,
            "imperative_only_performance": "Baseline CNN for the T1 single-digit addition (predicting the sum directly): 93.46 ± 0.49 (N=30000), 78.32 ± 2.14 (N=3000), 23.64 ± 1.75 (N=300). (Units: percent accuracy).",
            "has_comparative_results": true,
            "generalization_properties": "Demonstrates improved sample efficiency and compositional generalization: (1) networks trained on single-digit addition can be reused without retraining to handle multi-digit addition (T2), showing compositional reuse of learned neural predicates; (2) better accuracy than an end-to-end CNN baseline particularly in low-data regimes (N=3000 and N=300), indicating stronger generalization from decomposed supervision; (3) however, purely providing logical supervision can lead to degenerate solutions (mode collapse) unless regularized (entropy regularization used in T3), indicating some fragility when supervision is only at symbolic level.",
            "interpretability_properties": "The declarative ProbLog component supplies explicit symbolic structure and probabilistic semantics: proofs, grounded rules, and compiled ACs provide interpretable derivations and enable inspection of which probabilistic facts (including neural predicate outputs) contributed to a query probability. The neural components remain black-box perceptual modules, but are encapsulated by named neural predicates whose outputs are interpretable probability distributions used in logic.",
            "limitations_or_failures": "Identified limitations include: (1) risk of mode collapse when only high-level symbolic supervision is available (T3) requiring entropy/max-entropy regularization to avoid trivial solutions; (2) grounding/pruning removes irrelevant groundings, which can lead to instantiated ADs that do not sum to one because dropped classes still receive mass from the neural network — this mismatch must be handled (authors note it and proceed with instantiated ADs); (3) computational cost: knowledge compilation and AC construction (SDD -&gt; AC) can be expensive for large grounded programs; (4) neural outputs must be normalized (softmax/sigmoid), and probabilistic parameters require clipping/renormalization; (5) the interpretability is limited by the black-box neural modules — explanations are symbolic but do not open the neural internals.",
            "theoretical_framework": "Semantics defined by reducing DeepProbLog programs to standard ProbLog semantics by instantiating neural ADs/facts into ADs/facts with probabilities obtained from neural networks; learning/inference theory rests on ProbLog's weighted model counting via knowledge compilation (SDDs -&gt; ACs) and aProbLog's gradient semiring which provides exact derivatives ∂P(query)/∂(probabilistic parameters) on the AC; combined with the chain rule to obtain gradients w.r.t. neural parameters, yielding a principled end-to-end differentiable framework integrating logic, probability, and neural learning. Conceptually motivated by the principle that neuro-symbolic integration should subsume pure neural, logical and probabilistic methods as special cases and that the probabilistic formulation provides a natural differentiable objective (query likelihood) for joint optimization.",
            "additional_notes": "Key representational primitives: neural annotated disjunctions (nADs) and neural facts which allow neural network outputs to act as probabilistic annotations in ProbLog. Implementation leverages ProbLog inference and aProbLog gradient extensions; experiments demonstrate sample-efficiency and compositional generalization on MNIST-based reasoning tasks.",
            "uuid": "e602.0",
            "source_info": {
                "paper_title": "DeepProbLog: Neural Probabilistic Logic Programming",
                "publication_date_yy_mm": "2018-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProbLog",
            "rating": 2
        },
        {
            "paper_title": "Algebraic ProbLog",
            "rating": 2
        },
        {
            "paper_title": "Sentential Decision Diagrams",
            "rating": 1
        }
    ],
    "cost": 0.012163499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Probabilistic Logic Programming in DeepProbLog*</h1>
<p>Robin Manhaeve ${ }^{\text {a,* }}$, Sebastijan Dumančiča ${ }^{\mathrm{a}}$, Angelika Kimmig ${ }^{\mathrm{b}}$, Thomas Demeester ${ }^{\mathrm{c}, 1}$, Luc De Raedt ${ }^{\mathrm{a}, 1}$<br>${ }^{a}$ KU Leuven<br>${ }^{b}$ Cardiff University<br>${ }^{c}$ Ghent University - imec</p>
<h4>Abstract</h4>
<p>We introduce DeepProbLog, a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques of the underlying probabilistic logic programming language ProbLog can be adapted for the new language. We theoretically and experimentally demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.</p>
<p>Keywords: logic, probability, neural networks, probabilistic logic programming, neuro-symbolic integration, learning and reasoning</p>
<h2>1. Introduction</h2>
<p>Many tasks in AI can be divided into roughly two categories: those that require low-level perception, and those that require high-level reasoning. At the same time, there is a growing consensus that being capable of tackling both types of tasks is essential to achieve true (artificial) intelligence [2]. Deep learning is empowering a new generation of intelligent systems that excel at low-level perception, where it is used to interpret images, text and speech with unprecedented accuracy. The success of deep learning has caused a lot of excitement and has also created the impression that deep learning can solve any problem in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>artificial intelligence. However, there is a growing awareness of the limitations of deep learning: deep learning requires large amounts of (the right kind of) data to train the network, it provides neither justifications nor explanations, and the models are black-boxes that can neither be understood nor modified by domain experts. Although there have been attempts to demonstrate reasoning-like behaviour with deep learning [3], their current reasoning abilities are nowhere close to what is possible with typical high-level reasoning approaches. The two most prominent frameworks for reasoning are logic and probability theory. While in the past, these were studied by separate communities in artificial intelligence, many researchers are working towards their integration, and aim at combining probability with logic and statistical learning; cf. the areas of statistical relational artificial intelligence [4, 5] and probabilistic logic programming [6].</p>
<p>The abilities of deep learning and statistical relational artificial intelligence approaches are complementary. While deep learning excels at low-level perception, probabilistic logics excel at high-level reasoning. As such, an integration of the two would have very promising properties. Recently, a number of researchers have revisited and modernized ideas originating from the field of neural-symbolic integration [7], searching for ways to combine the best of both worlds $[8,9,10,3]$, for example, by designing neural architectures representing differentiable counterparts of symbolic operations in classical reasoning tools. Yet, joining the full flexibility of high-level probabilistic and logical reasoning with the representational power of deep neural networks is still an open problem. Elsewhere [11], we have argued that neuro-symbolic integration should: 1) integrate neural networks with the two most prominent methods for reasoning, that is, logic and probability, and 2) that neuro-symbolic integrated methods should have the pure neural, logical and probabilistic methods as special cases.</p>
<p>With DeepProbLog, we tackle the neuro-symbolic challenge from this perspective. Furthermore, instead of integrating reasoning capabilities into a complex neural network architecture, we proceed the other way round. We start from an existing probabilistic logic programming language, ProbLog [12], and introduce the smallest extension that allows us to integrate neural networks: the neural predicate. The idea is simple: in a probabilistic logic, atomic expressions of the form $q\left(t_{1}, \ldots, t_{n}\right)$ (aka tuples in a relational database) have a probability $p$. We extend this idea by allowing atomic expressions to be labeled with neural networks whose outputs can be considered probability distributions. This simple idea is appealing as it allows us to retain all the essential components of the ProbLog language: the semantics, the inference mechanism, as well as the implementation.</p>
<p>Therefore, one should not only integrate logic with neural networks in neuro-symbolic computation, but also probability.</p>
<p>This effectively leads to an integration of probabilistic logics (hence statistical relational AI) with neural networks and opens up new abilities. Furthermore, although at first sight, this may appear as a complication, it actually can greatly simplify the integration of neural networks with logic. The reason for this is that the probabilistic framework provides a clear optimisation criterion, namely the probability of the training examples. Real-valued probabilistic quantities are</p>
<p>also well-suited for gradient-based training procedures, as opposed to discrete logic quantities.</p>
<h1>Example 1</h1>
<p>Before going into further detail, the following example illustrates the possibilities of this approach. Consider the predicate addition $(\mathrm{X}, \mathrm{Y}, \mathrm{Z})$, where X and Y are images of digits and Z is the natural number corresponding to the sum of these digits. The goal is that after training, DeepProbLog allows us to make a probabilistic estimate on the validity of, for example, the example addition( 8 ). While such a predicate can be learned directly by a standard neural classifier, such an approach cannot incorporate background knowledge such as the definition of the addition of two natural numbers. In DeepProbLog such knowledge can easily be encoded in rules such as</p>
<p>$$
\text { addition }\left(\mathrm{I}<em _mathrm_Y="\mathrm{Y">{\mathrm{X}}, \mathrm{I}</em>}}, \mathrm{~N<em _mathrm_X="\mathrm{X">{\mathrm{Z}}\right):-\operatorname{digit}\left(\mathrm{I}</em>}}, \mathrm{~N<em _mathrm_Y="\mathrm{Y">{\mathrm{X}}\right), \operatorname{digit}\left(\mathrm{I}</em>}}, \mathrm{~N<em _mathrm_Z="\mathrm{Z">{\mathrm{Y}}\right), \mathrm{N}</em>}} \text { is } \mathrm{N<em _mathrm_Y="\mathrm{Y">{\mathrm{X}}+\mathrm{N}</em>
$$}</p>
<p>with is the standard operator of logic programming to evaluate arithmetic expressions. All that needs to be learned in this case is the neural predicate digit which maps an image of a digit $I_{D}$ to the corresponding natural number $N_{D}$. The trained network can then be reused for arbitrary tasks involving digits. Our experiments show that this leads not only to new capabilities but also to significant performance improvements. An important advantage of this approach compared to standard image classification settings is that it can be extended to multi-digit numbers without additional training. We note that the single digit classifier (i.e., the neural predicate) is not explicitly trained by itself: its output can be considered a latent representation, as we only use training data with pairwise sums of digits.</p>
<p>To summarize, we introduce DeepProbLog which has a unique set of features: (i) it is a programming language that supports neural networks and machine learning and has a well-defined semantics (ii) it integrates logical reasoning with neural networks; so both symbolic and subsymbolic representations and inference; (iii) it integrates probabilistic modeling, programming and reasoning with neural networks (as DeepProbLog extends the probabilistic programming language ProbLog, which can be regarded as a very expressive directed graphical modeling language [4]); (iv) it can be used to learn a wide range of probabilistic logical neural models from examples, including inductive programming.</p>
<p>This paper is a significantly extended and completed version of our previous work [1] (NeurIPS, spotlight presentation). This extended version now contains the necessary deep learning and probabilistic logic programming background and a more in depth theoretical explanation. It also contains additional experiments (see Section 6): the MNIST addition experiments from the short version are completed with the new experiments T3 and T4, and we designed new experiments (T8 and T9) to further investigate the use of DeepProbLog on combined probabilistic learning and deep learning. The code is available at https://bitbucket.org/problog/deepproblog.</p>
<h1>2. Background</h1>
<h3>2.1. Logic programming concepts</h3>
<p>In this section, we briefly summarize basic logic programming concepts; see e.g., Lloyd [13] for more details. Atoms are expressions of the form $q\left(t_{1}, \ldots, t_{n}\right)$ where $q$ is a predicate (of arity $n$, or $q / n$ in shorthand notation) and the $t_{i}$ are terms. A literal is an atom or the negation $\neg q\left(t_{1}, \ldots, t_{n}\right)$ of an atom. A term $t$ is either a constant $c$, a variable $V$, or a structured term of the form $f\left(u_{1}, \ldots, u_{k}\right)$ where $f$ is a functor and the $u_{i}$ are terms. We follow the Prolog convention and let constants, functors and predicates start with a lower case character and variables with an upper case. A rule is an expression of the form $h:-b_{1}, \ldots, b_{n}$ where $h$ is an atom, the $b_{i}$ are literals, and all variables are universally quantified. Informally, the meaning of such a rule is that $h$ holds whenever the conjunction of the $b_{i}$ holds. Thus :- represents logical implication $(\leftarrow)$, and the comma (,) represents conjunction $(\wedge)$. Rules with an empty body $n=0$ are called facts. A logic program is a finite set of rules.</p>
<p>A substitution $\theta=\left{V_{1}=t_{1}, \ldots, V_{n}=t_{n}\right}$ is an assignment of terms $t_{i}$ to variables $V_{i}$. When applying a substitution $\theta$ to an expression $e$ we simultaneously replace all occurrences of $V_{i}$ by $t_{i}$ and denote the resulting expression as $e \theta$. Expressions that do not contain any variables are called ground. The Herbrand base of a logic program is the set of ground atoms that can be constructed using the predicates, functors and constants occurring in the program. ${ }^{2}$ Subsets of the Herbrand base are called Herbrand interpretations. A Herbrand interpretation is a model of a clause $h:-b_{1}, \ldots, b_{n}$. if for every substitution $\theta$ such that the conjunction $\left(b_{1}, \ldots, b_{n}\right) \theta$ holds in the interpretation, $h \theta$ is in the interpretation.</p>
<p>It is a model of a logic program if it is a model of all clauses in the program.
For negation-free programs, the semantics is given by the minimal such model, known as the least Herbrand model, which is unique. General logic programs use the notion of negation as failure, that is, the negation of an atom is true exactly if the atom cannot be derived from the program. These programs are not guaranteed to have a unique minimal Herbrand model, and several ways to define a canonical model have been studied. We follow the well-founded semantics here [14].</p>
<p>The main inference task in logic programming is to determine whether a given atom $q$, also called query (or goal), is true in the canonical model of a logic program $P$, denoted by $P \models q$. If the answer is yes (or no), we also say that the query succeeds (or fails). If such a query is not ground, inference asks for the existence of an answer substitution, that is, a substitution that grounds the query into an atom that is part of the canonical model.</p>
<h3>2.2. Deep Learning</h3>
<p>The following paragraphs provide a very brief introduction to deep learning, focusing on concepts needed for understanding our work. Extensive further</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>details can be found, e.g., in [15]. This section is meant to provide readers with little or no knowledge of deep learning, with a conceptual understanding of the main ideas. In particular, we will focus on the setting of supervised learning, where the model learns to map an input item to a particular output, based on input-output examples.</p>
<p>An artificial neural network is a highly parameterized and therefore very flexible non-linear mathematical function that can be 'trained' towards a particular desired behavior, by suitably adjusting its parameters.</p>
<p>During training, the model learns to capture from the input data the most informative 'features' for the task at hand. The need for 'feature engineering' in classical (or rather, non-neural) machine learning methods has therefore been replaced by 'architecture engineering', since a wide variety of neural network components are available to be composed into a suitable model.</p>
<p>Deep neural networks are often designed and trained in an 'end-to-end' fashion, whereby only the raw input and the final target are known during training, and all components of the model are jointly trained. For example, for the task of hand-written digit recognition, an input instance consists of a pixel image of a hand-written digit, whereas its target denotes the actual digit.</p>
<p>Consider a supervised learning problem, with a training set $\left{\left(\boldsymbol{x}<em i="i">{i}, \boldsymbol{y}</em>\right)\right}<em i="i">{i=1}^{N}$ containing $N$ i.i.d. input instances $\boldsymbol{x}</em> \mid \Theta)$.}$ and corresponding outputs $\boldsymbol{y}_{i}$. A model represented by a mapping function $\mathcal{M}$ with parameters $\Theta$, maps an input item $\boldsymbol{x}$ to the corresponding predicted output $\hat{\boldsymbol{y}}=\mathcal{M}(\boldsymbol{x</p>
<p>To quantify how strongly the predicted output $\hat{\boldsymbol{y}}$ deviates from the target output $\boldsymbol{y}$, a loss function $\mathcal{L}(\hat{\boldsymbol{y}}, \boldsymbol{y})$ is defined. Training the model then comes down to minimizing the expected loss $\hat{\mathcal{L}}=\frac{1}{N} \sum_{i} \mathcal{L}\left(\mathcal{M}\left(\boldsymbol{x}<em i="i">{i} \mid \Theta\right), \boldsymbol{y}</em>$ obtained at the output of the neural network.}\right)$ over the training set. In the specific setting of multiclass classification, each input instance corresponds to one out of a fixed set of $M$ output categories. The target vectors $\boldsymbol{y}$ are typically represented as one-hot vectors: all components are zero, except at index $m$ of the corresponding category. The predicted counterpart $\hat{\boldsymbol{y}}$ at the model's output is often obtained by applying a so-called softmax output function to intermediate real-valued scores $\mathbf{s</p>
<p>The $i$ 'th component of the softmax is defined as</p>
<p>$$
\operatorname{softmax}(\mathbf{s})<em i="i">{i}=\hat{y}</em>
$$}=\frac{e^{s_{i}}}{\sum_{j} e^{s_{j}}</p>
<p>The softmax outputs are well-suited to model a probability distribution (i.e., $0&lt;\hat{y}<em i="i">{i}&lt;1$ and $\sum</em>$ ) defined as} \hat{y}_{i}=1$ ). The standard corresponding loss function is the cross-entropy loss, which quantifies the deviation between the empirical output distribution $\hat{\boldsymbol{y}}$ (i.e., the softmax outputs) and the ground truth distribution (i.e., the one-hot target vector $\boldsymbol{y</p>
<p>$$
\mathcal{L}=-\sum_{j} y_{j} \log \hat{y}_{j}
$$</p>
<p>The most widely used optimization approaches for neural networks are variations of the gradient descent algorithm, in which the parameters $\Theta$ are iteratively</p>
<p>updated by taking small steps along the negative gradient of the loss. An estimate $\Theta_{n}$ at iteration $n$ is updated as $\Theta_{n+1}=\Theta_{n}-\lambda \nabla_{\Theta} \mathcal{L}$, in which the step size is controlled by the learning rate $\lambda$. Typically, training is not performed over the entire dataset per iteration, but instead over a smaller 'mini-batch' of instances. This is computationally more efficient and allows for a better exploration of parameter space. Importantly, the loss gradient can only be calculated if all components of the neural network are differentiable.</p>
<p>A deep neural network typically has a layer-wise architecture: the different layers correspond to nested differentiable functions in the overall mapping function $\mathcal{M}$. The 'forward pass' through the network corresponds to consecutively applying these layer functions to a given input to the network. The intermediate representations obtained by evaluating these layer functions are called hidden states. After a forward pass, the gradient with respect to all parameters can then be calculated by applying the chain rule. This happens during the so-called 'backward pass': the gradients are calculated from the output back to the first layer. As an illustration of how the chain rule is applied, consider the network function $\mathcal{M}(\boldsymbol{x} \mid \Theta)=\mathbf{g}\left(\mathbf{f}\left(\boldsymbol{x}, \theta_{f}\right), \theta_{g}\right)$, which contains a first layer represented by the vector function $\mathbf{f}$, and a second layer $\mathbf{g}$. For simplicity, say each layer has one trainable parameter, respectively written as $\theta_{f}$ and $\theta_{g}$. The derivative with respect to these parameters of a scalar loss function applied to the network output, becomes</p>
<p>$$
\nabla_{\Theta} \mathcal{L}(\mathcal{M}(\boldsymbol{x} \mid \Theta))=\left[\frac{d \mathcal{L}}{d \theta_{f}}, \frac{d \mathcal{L}}{d \theta_{g}}\right]=\left[\sum_{i} \frac{\partial \mathcal{L}}{\partial g_{i}} \sum_{j} \frac{\partial g_{i}}{\partial f_{j}} \frac{\partial f_{j}}{\partial \theta_{f}}, \sum_{i} \frac{\partial \mathcal{L}}{\partial g_{i}} \frac{\partial g_{i}}{\partial \theta_{g}}\right]
$$</p>
<p>in which the individual derivatives are evaluated based on the considered input $\boldsymbol{x}$ and current value of the parameters. The entire procedure to calculate the gradients is called the backpropagation algorithm. It requires a forward pass to calculate all intermediate representations up to the value of the loss. After that, in the backward pass, the gradients corresponding to all operations applied during the forward pass, are iteratively calculated, starting at the loss (i.e., with $\partial \mathcal{L} / \partial g_{i}$ in the example). As such, the gradients with respect to parameters at a given layer can be calculated as soon as the gradients due to all operations further in the network are known, as governed by the chain rule.</p>
<p>To summarize, a single iteration in the optimization happens as follows: 1) A minibatch is sampled from the training data. 2) The output of the neural network is calculated during the forward pass. 3) The loss is calculated based on that output and the target. 4) The gradients for the parameters in the neural network are calculated using backpropagation. 5) The parameters are updated using a gradient-based optimizer.</p>
<p>The most basic neural networks building block is the so-called fully-connected layer. It consists of a linear transformation with weight matrix $\mathbf{W}$ and bias vector $\mathbf{b}$, followed by applying a component-wise non-linear function, called the activation function. The input into such a layer can be the vector representation $\boldsymbol{x}$ of the actual input to the model, or the output $\boldsymbol{h}^{\mathrm{v}, \mathrm{v}, \mathrm{s}}$ from a previous layer $i$, which is called a hidden representation. Its output is calculated as</p>
<p>$\boldsymbol{h}^{<i+1>}=a\left(\mathbf{W} \boldsymbol{h}^{<i>}+\mathbf{b}\right)$, in which typical choices for the activation function $a$ are the Rectified Linear Unit (ReLU) defined as $a(x)=\max (0, x)$ or the hyperbolic tangent $a(x)=\tanh (x)$. In other cases, a sigmoid activation can be used, given by $\sigma(x)=\left(1+e^{-x}\right)^{-1}$. Another important type of neural network layer is the convolutional layer, which convolves the input to pass it to the next layer, by means of a kernel with trainable weights, typically much smaller than the input size. Convolutional layers, followed by a similar activation function, are often used in image recognition models, whereby subsequent layers learn to extract useful features for the given task, from local patterns up to more global and often interpretable patterns. An architecture well-suited for modeling sequences are the so-called recurrent neural networks (RNN). In short, these define a mapping from an input element in the considered sequence into a hidden representation. Every input element is encoded with the same neural network, called the RNN 'cell', such that the model can be applied to variable-length sequences. In order to explicitly model the sequential behavior, when encoding a given item in the sequence, the cell takes as input that item, as well as the hidden state obtained while encoding the previous input item. When training with this recurrent setup, the gradient propagates back through the entire sequence. When encoding long sequences, this may lead to very small gradients. An important type of RNN, well-equipped to deal with this so-called vanishing gradient problem, is the Long Short-term Memory (LSTM). The same problem is solved by a similar architecture called the GRU.</p>
<p>More technical details, as well as several other popular types of neural network components, are provided in [15].</p>
<p>Deep neural networks can become very expressive, especially when deeper, or with large hidden representations. To avoid overfitting, various regularization approaches have been developed. A widespread technique, also used in some of the presented experiments in this work, is called dropout. For those layers on which dropout is applied, during training a random sample of the layer outputs are set to zero in each iteration, while accordingly compensating the amplitude of the remaining activations. At inference time, i.e., when applying the trained model to held-out data, all activations are kept.</p>
<p>As mentioned, many choices are possible in terms of architecture and training: dimensions, types of layers, learning rates, regularization strength, etc. These are so-called hyper-parameters, and are typically 'tuned' by evaluating on a validation set, not used explicitly for gradient-based training of the network parameters, and still separate from the final test data.</p>
<h1>3. Introducing DeepProbLog</h1>
<p>We now recall the basics of probabilistic logic programming using ProbLog (see De Raedt and Kimmig [6] for more details), and then introduce our new language DeepProbLog.</p>
<h1>3.1. ProbLog</h1>
<h2>Definition 1 (ProbLog program)</h2>
<p>A ProbLog program consists of a set of ground probabilistic facts $\mathcal{F}$ of the form $p:: f$ where $p$ is a probability and $f$ a ground atom and a set of rules $\mathcal{R}$.</p>
<p>For instance, the following ProbLog program models a variant of the well-known alarm Bayesian network [16]:</p>
<p>$$
\begin{aligned}
&amp; 0.1:: \text { burglary. } \
&amp; 0.5:: \text { hears_alarm(mary). } \
&amp; 0.2:: \text { earthquake. } \
&amp; 0.4:: \text { hears_alarm(john). }
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
&amp; \text { alarm }: \text {-earthquake. } \
&amp; \text { alarm :- burglary. } \
&amp; \text { calls }(\mathrm{X}): \text {-alarm, hears_alarm }(\mathrm{X})
\end{aligned}
$$</p>
<p>Each probabilistic fact corresponds to an independent Boolean random variable that is true with probability $p$ and false with probability $1-p$. Every subset $F \subseteq \mathcal{F}$ defines a possible world $w_{F}=F \cup{h \theta \mid \mathcal{R} \cup F \models h \theta$ and $h \theta$ is ground $}$, that is, the world $w_{F}$ is the canonical model of the logic program obtained by adding $F$ to the set of rules $\mathcal{R}$, e.g.,</p>
<p>$$
w_{\text {{burglary, hears_alarm(mary)\ }}={\text { burglary, hears_alarm(mary) }} \cup{\text { alarm, calls(mary) }}
$$</p>
<p>To keep the presentation simple, we focus on the case of finitely many ground probabilistic facts, but note that the semantics is also well-defined for the countably infinite case. The probability $P\left(w_{F}\right)$ of such a possible world $w_{F}$ is given by the product of the probabilities of the truth values of the probabilistic facts:</p>
<p>$$
P\left(w_{F}\right)=\prod_{f_{i} \in F} p_{i} \prod_{f_{i} \in \mathcal{F} \backslash F}\left(1-p_{i}\right)
$$</p>
<p>For instance,</p>
<p>$$
P\left(w_{\text {{burglary, hears_alarm(mary)\ }}\right)=0.1 \times 0.5 \times(1-0.2) \times(1-0.4)=0.024
$$</p>
<p>The probability of a ground fact $q$, also called success probability of $q$, is then defined as the sum of the probabilities of all worlds containing $q$, i.e.,</p>
<p>$$
P(q)=\sum_{F \subseteq \mathcal{F}: q \in w_{F}} P\left(w_{F}\right)
$$</p>
<p>The probability of a query is also equal to the weighted model count (WMC) of the worlds where this query is true.</p>
<p>For ease of modeling, ProbLog supports non-ground probabilistic facts as a shortcut for introducing a set of ground probabilistic facts, as well as annotated disjunctions (ADs), which are expressions of the form</p>
<p>$$
p_{1}:: h_{1} ; \ldots ; p_{n}:: h_{n}:-b_{1}, \ldots, b_{m}
$$</p>
<p>where the $p_{i}$ are probabilities that sum to at most one, the $h_{i}$ are atoms, and the $b_{j}$ are literals. The meaning of an AD is that whenever all $b_{i}$ hold, the AD causes one of the $h_{j}$ to be true, or none of them with probability $1-\sum p_{i}$. Note that several of the $h_{i}$ may be true at the same time if they also appear as heads of other rules or ADs. This is convenient to model choices between different categorical variables, e.g. different severities of the earthquake:
$0.4::$ earthquake(none) ; $0.4::$ earthquake(mild) ; $0.2::$ earthquake(severe).
or without explicitly representing the event of no earthquake:</p>
<p>$$
0.4:: \text { earthquake(mild) ; } 0.2:: \text { earthquake(severe) }
$$</p>
<p>In which neither earthquake(mild) nor earthquake(severe) will be true with probability 0.4 . Annotated disjunctions do not change the expressivity of ProbLog, as they can alternatively be modeled through independent facts and logical rules; we refer to De Raedt and Kimmig [6] for technical details.</p>
<h1>3.2. DeepProbLog</h1>
<p>In ProbLog, the probabilities of all random choices are explicitly specified as part of probabilistic facts or annotated disjunctions. DeepProbLog extends ProbLog to basic random choices whose probabilities are specified through external functions implemented as neural networks.</p>
<h2>Definition 2 (Neural annotated disjunction)</h2>
<p>A neural $A D$ is an expression of the form</p>
<p>$$
n n\left(m_{r}, \vec{I}, O, \vec{d}\right):: r(\vec{I}, O)
$$</p>
<p>where $n n$ is a reserved functor, $m_{r}$ uniquely identifies a neural network model with $k$ inputs and $n$ outputs (i.e., its architecture as well as its trainable parameters) defining a probability distribution over $n$ classes, $\vec{I}=I_{1}, \ldots, I_{k}$ is a sequence of input variables, $O$ is the output variable, $\vec{d}=d_{1}, \ldots, d_{n}$ is a sequence of ground terms (the output domain of this neural network) and $r$ is a predicate.</p>
<p>A ground neural $A D$ is an expression of the form</p>
<p>$$
n n\left(m_{r}, \vec{i}, d_{1}\right):: r\left(\vec{i}, d_{1}\right) ; \ldots ; n n\left(m_{r}, \vec{i}, d_{n}\right):: r\left(\vec{i}, d_{n}\right)
$$</p>
<p>where $\vec{i}=i_{1}, \ldots, i_{k}$ is a sequence of ground terms (the input to the neural network) and $d_{1}, \ldots, d_{n}$ are ground terms (the output domain of this neural network).</p>
<p>The $n n\left(m_{r}, \vec{i}, d_{j}\right)$ term in the definition can be considered a function that returns the probability of class $d_{j}$ when evaluating the network $m_{r}$ on input $\vec{i}$. As such, a ground nAD can be instantiated into a normal AD by evaluating the neural network and replacing the functor with the calculated probability. For instance, in the MNIST addition example, we would specify the nAD</p>
<p>$$
\mathrm{nn}(\mathrm{~m} _ \text {digit, }[\mathrm{X}], \mathrm{Y},[\mathrm{O}, \ldots, 9]): \operatorname{digit}(\mathrm{X}, \mathrm{Y})
$$</p>
<p>where $m_{_} d i g i t$ is a network that classifies MNIST digits. Grounding this on an input image $\mathbf{2}$ would result in a ground nAD:</p>
<p>$$
\mathrm{nn}(\mathrm{~m} \text { _digit, }[\mathbf{2}], 0):: \operatorname{digit}(\mathbf{2}, 0) ; \ldots ; \mathrm{nn}(\mathrm{~m} \text { _digit, }[\mathbf{2}], 9):: \operatorname{digit}(\mathbf{2}, 9)
$$</p>
<p>Evaluating this would result in a ground AD:</p>
<p>$$
\mathrm{p}<em 9="9">{0}:: \operatorname{digit}(\mathbf{2}, 0) ; \ldots ; \mathrm{p}</em>, 9)
$$}:: \operatorname{digit}(\mathbf{2</p>
<p>Where $\left[p_{0}, \ldots, p_{9}\right]$ is the output vector of the $m_{\text {_ }}$ digit network when evaluated on 2 .
The neural network could take any shape, e.g., a convolutional network for image encoding, a recurrent network for sequence encoding, etc. However, its output layer, which feeds the corresponding neural predicate, needs to be normalized.</p>
<p>We consider an output domain size of two as a special case. Instead of the neural network having two probabilities at the output that sum to one, we can simplify this to a single probability, with the second one the complement of that probability. This difference coincides with the difference between a softmax and single-neuron sigmoid layer in a neural network. We call such an expression a neural fact.</p>
<h1>Definition 3 (Neural fact)</h1>
<p>A neural fact is an expression of the form</p>
<p>$$
n n\left(m_{r}, \vec{I}\right):: r(\vec{I})
$$</p>
<p>where $n n$ is a reserved functor, $m_{r}$ uniquely identifies a neural network model defining a probability distribution over $n$ classes, $\vec{I}=I_{1}, \ldots, I_{k}$ is a sequence of input variables and $r$ is a predicate.</p>
<p>A ground neural fact is an expression of the form</p>
<p>$$
n n\left(m_{r}, \vec{i}\right):: r(\vec{i})
$$</p>
<p>where $\vec{i}=i_{1}, \ldots, i_{k}$ is a sequence of ground terms (the input to the neural network).</p>
<p>To exemplify, we use a neural network that gives a measure of the similarity between two input images. We can encode this with the following neural fact:</p>
<p>$$
\mathrm{nn}(\mathrm{~m},[\mathrm{X}, \mathrm{Y}]): \operatorname{similar}(\mathrm{X}, \mathrm{Y})
$$</p>
<p>Grounding this on the input $\underline{\mathbf{Z}}$ and $\underline{\mathbf{S}}$ would result in the follow ground neural fact:</p>
<p>$$
\mathrm{nn}(\mathrm{~m} . \underline{\mathbf{Z}} \mathbf{S}): \text { similar }(\underline{\mathbf{Z}} \mathbf{S})
$$</p>
<p>Evaluating this would result in a ground probabilistic fact:</p>
<p>$$
\mathrm{p}: \text { : similar }(\underline{\mathbf{Z}} \mathbf{S})
$$</p>
<p>Where $p$ is the output of the $m$ network when evaluated on $\underline{\mathbf{Z}}$ and $\underline{\mathbf{S}}$.</p>
<h1>Definition 4 (DeepProbLog Program)</h1>
<p>A DeepProbLog program consists of a set of ground probabilistic facts $\mathcal{F}$, a set of ground neural ADs and ground neural facts $\mathcal{N}$, and a set of rules $\mathcal{R}$.</p>
<p>The semantics of a DeepProbLog program is given by the semantics of the ProbLog program obtained by replacing each nAD with the AD obtained by instantiating the probabilities as mentioned above. While the semantics is defined with respect to ground neural ADs and facts, as in ProbLog, we write non-ground such expressions if the intended grounding is clear from context.</p>
<h2>4. Inference</h2>
<p>This section explains how a DeepProbLog model is used for a given query at prediction time. First, we provide more detail on ProbLog inference [17]. Next, we describe how ProbLog inference is adapted in DeepProbLog.</p>
<h3>4.1. ProbLog Inference</h3>
<p>ProbLog inference proceeds in four steps. The first step is the grounding step, in which the logic program is grounded with respect to the query. This step uses backward reasoning to determine which ground rules are relevant to derive the truth value of the query, and may perform additional logical simplifications that do not affect the query's probability.</p>
<p>The second step rewrites the ground logic program into a formula in propositional logic that defines the truth value of the query in terms of the truth values of probabilistic facts. We can calculate the query success probability by performing weighted model counting (WMC) on this logic formula (cfr. Fierens et al. [17]). However, performing WMC on this logical formula directly is not efficient.</p>
<p>The third step is knowledge compilation [18]. During this step, the logic formula is transformed into a form that allows for efficient weighted model counting. The current ProbLog system uses Sentential Decision Diagrams (SDDs, Darwiche [19]), the most succinct suitable representation available today. SDDs, being a subset of d-DNNFs allow for polytime model counting ([18]). However, they also support polytime conjunction, disjunction and negation while being</p>
<p>more succinct than OBDDs (Darwiche [19]).
The fourth and final step transforms the SDD into an arithmetic circuit (AC). This is done by putting the probabilities of the probabilistic facts or their negations on the leaves, replacing the OR nodes with addition and the AND nodes by multiplication. The WMC is then calculated with an evaluation of the AC.</p>
<h1>Example 2</h1>
<p>In Figure 1, we apply the four steps of ProbLog inference on the earthquake example with query calls (mary).
In the first step, the non-ground program (Figure 1a) is grounded with respect to the query calls(mary). The result is shown in Figure 1b: the irrelevant fact hears_alarm(john) is omitted and the variable X in the calls rule is substituted with the constant mary. The resulting formula in the second step is</p>
<p>$$
\text { calls(mary) } \leftrightarrow \text { hears_alarm(mary) } \wedge(\text { burglary } \vee \text { earthquake })
$$</p>
<p>The WMC of this formula is shown in Figure 1c. However, it is not calculated by enumeration as shown here, but an AC is used instead. The AC derived in step four is shown in Figure 1d, where rounded grey rectangles depict variables corresponding to probabilistic facts, and the rounded red rectangle denotes the query atom defined by the formula. The white rectangles correspond to logical operators applied to their children. The intermediate results are shown in black next to the nodes in Figure 1d.</p>
<h3>4.2. DeepProbLog Inference</h3>
<p>The only change required for DeepProbLog inference is that we need to instantiate the ground nADs and neural facts into the corresponding ground ADs and ground facts. This is done in a separate step after grounding, where the parameters for the regular AD are determined by making a forward pass on the relevant neural network with the ground input.</p>
<h2>Example 3</h2>
<p>We illustrate this by evaluating the MNIST addition example (Figure 2a). The DeepProbLog program requires two lines: the first line defining the neural predicate, and the second line defining the addition. We evaluate it on the query addition[2.1.1]. In the first step, the DeepProbLog program is grounded into a ground DeepProbLog Program (Figure 2b). Note that the nADs are now all ground. As ProbLog only grounds the relevant part of the program, i.e. the part that can be used to prove the query, only the digits 0 and 1 are retained as the larger digits cannot sum to 1 . The next step is the only difference between ProbLog and DeepProbLog inference: instantiating the ground nADs into regular ground ADs, which could, for instance, produce an AD as shown in Figure 2c. The probabilities in the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">0.2::earthquake.</th>
<th style="text-align: left;">$0.2:$ :earthquake.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0.1::burglary.</td>
<td style="text-align: left;">$0.1:$ :burglary.</td>
</tr>
<tr>
<td style="text-align: left;">0.5::hears_alarm(mary).</td>
<td style="text-align: left;">$0.5:$ :hears_alarm(mary).</td>
</tr>
<tr>
<td style="text-align: left;">0.4::hears_alarm(john).</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">alarm :- earthquake.</td>
<td style="text-align: left;">alarm :- earthquake.</td>
</tr>
<tr>
<td style="text-align: left;">alarm :- burglary.</td>
<td style="text-align: left;">alarm :- burglary.</td>
</tr>
<tr>
<td style="text-align: left;">calls(X):-alarm,hears_alarm(X).</td>
<td style="text-align: left;">calls(mary):-alarm,hears_alarm(mary).</td>
</tr>
</tbody>
</table>
<p>(a) The ProbLog program.
(b) The relevant ground program.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models of calls(mary) $\leftrightarrow$ hears_alarm(mary) $\wedge$ (burglary $\vee$ earthquake)</th>
<th style="text-align: center;">w</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$}$</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">{hears_alarm(mary)}</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">{earthquake}</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr>
<td style="text-align: left;">{earthquake, hears_alarm(mary), calls(mary)}</td>
<td style="text-align: center;">$\mathbf{0 . 0 9}$</td>
</tr>
<tr>
<td style="text-align: left;">{burglary}</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">{burglary, hears_alarm(mary), calls(mary)}</td>
<td style="text-align: center;">$\mathbf{0 . 0 4}$</td>
</tr>
<tr>
<td style="text-align: left;">{burglary, earthquake}</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">{burglary, earthquake, hears_alarm(mary), calls(mary)}</td>
<td style="text-align: center;">$\mathbf{0 . 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">$\sum_{\text {calls(mary) } \in \text { model }} \mathbf{0 . 1 4}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(c) The weighted count of the models where calls(mary) is true.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Inference in ProbLog using query calls(mary) and the program in (a). (Example 2)</p>
<div class="codehilite"><pre><span></span><code><span class="nf">nn</span><span class="p">(</span><span class="s s-Atom">m_digit</span><span class="p">,</span> <span class="p">[</span><span class="nv">X</span><span class="p">],</span> <span class="nv">Y</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.</span><span class="p">.</span><span class="mf">.9</span><span class="p">])</span><span class="o">::</span> <span class="nf">digit</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">).</span>
<span class="nf">addition</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">,</span><span class="nv">Z</span><span class="p">)</span> <span class="o">:-</span> <span class="nf">digit</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">N1</span><span class="p">),</span> <span class="nf">digit</span><span class="p">(</span><span class="nv">Y</span><span class="p">,</span><span class="nv">N2</span><span class="p">),</span> <span class="nv">Z</span> <span class="o">is</span> <span class="nv">N1</span><span class="o">+</span><span class="nv">N2</span><span class="p">.</span>
</code></pre></div>

<p>(a) The DeepProbLog program.</p>
<div class="codehilite"><pre><span></span><code><span class="nf">nn</span><span class="p">(</span><span class="s s-Atom">m_digit</span><span class="p">,[</span><span class="mi">2</span><span class="p">],</span><span class="mi">0</span><span class="p">)</span><span class="o">::</span><span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span><span class="nf">nn</span><span class="p">(</span><span class="s s-Atom">m_digit</span><span class="p">,[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">::</span><span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span>
<span class="nf">nn</span><span class="p">(</span><span class="s s-Atom">m_digit</span><span class="p">,[</span><span class="mi">1</span><span class="p">],</span><span class="mi">0</span><span class="p">)</span><span class="o">::</span><span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span><span class="nf">nn</span><span class="p">(</span><span class="s s-Atom">m_digit</span><span class="p">,[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">::</span><span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span>
<span class="nf">addition</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">:-</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span>
<span class="nf">addition</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">:-</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">).</span>
</code></pre></div>

<p>(b) The ground DeepProbLog program.</p>
<div class="codehilite"><pre><span></span><code><span class="mf">0.8</span> <span class="o">::</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span> <span class="mf">0.1</span> <span class="o">::</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span>
<span class="mf">0.2</span> <span class="o">::</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span> <span class="mf">0.6</span> <span class="o">::</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span>
<span class="nf">addition</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">:-</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span>
<span class="nf">addition</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">:-</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="nf">digit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">).</span>
</code></pre></div>

<p>(c) The ground ProbLog program.</p>
<p>Figure 2: Inference in DeepProbLog (Example 3)
instantiated ADs do not sum to one, as the irrelevant terms (digit(2,2), ...,digit(2,9) and digit(1,2), ..., digit(1,9)) have been dropped in the grounding process, although the neural network still assigns probability mass to them. Inference then proceeds identically to that of ProbLog: the ground program is rewritten into a logical formula, this formula is compiled and transformed into an AC. Finally, this AC is evaluated to calculate the query probability.</p>
<h1>5. Learning in DeepProbLog</h1>
<p>We now introduce our approach to learn the parameters in DeepProbLog programs. The parameters include the learnable parameters of the neural network (which we will call neural parameters from now on) and the learnable parameters in the logic program (which we will refer to as probabilistic parameters). We use the learning from entailment setting [20]</p>
<h2>Definition 5</h2>
<p>Learning from entailment Given a DeepProbLog program with parameters $\Theta$, a set $\mathcal{Q}$ of pairs $(q, p)$ with $q$ a query and $p$ its desired success probability, and a loss function $\mathcal{L}$, compute:</p>
<p>$$
\underset{\Theta}{\arg \min } \frac{1}{|\mathcal{Q}|} \sum_{(q, p) \in \mathcal{Q}} \mathcal{L}(P(q \mid \Theta), p)
$$</p>
<p>In most of the experiments, unless mentioned otherwise, we only use positive examples for training (i.e., with desired success probability $p=1$ ). The model then needs to adjust the weights to maximize query probabilities $P_{\Theta}(q)$ for all training examples. This can be expressed by minimizing the average negative log likelihood of the query, whereby Definition 5 reduces to:</p>
<p>$$
\arg \min <em _in="\in" _mathcal_Q="\mathcal{Q" _q_="(q," p_="p)">{\Theta} \frac{1}{|\mathcal{Q}|} \sum</em>(q)
$$}}-\log P_{\Theta</p>
<p>The presented method however works for other choices in the loss function. For example, in experiment T9 (Section 6.3) the mean squared error (MSE) is used.</p>
<h1>5.1. Gradient descent in ProbLog</h1>
<p>In contrast to the earlier approach for ProbLog parameter learning in this setting by Gutmann et al. [21], we use gradient descent rather than EM. This allows for seamless integration with neural network training. The key insight here is that we can use the same AC that ProbLog uses for inference for gradient computations as well. We rely on the automatic differentiation capabilities already available in ProbLog to derive these gradients. More specifically, to compute the gradient with respect to the probabilistic logic program part, we rely on Algebraic ProbLog (aProbLog [22]), a generalization of the ProbLog language and inference to arbitrary commutative semirings, including the gradient semiring [23]. In the following, we provide the necessary background on aProbLog, discuss how to use it to compute gradients with respect to ProbLog parameters and extend the approach to DeepProbLog.
aProbLog and the gradient semiring. ProbLog annotates each probabilistic fact $f$ with the probability $P$ that $f$ is true, which implicitly also defines the probability $1-P$ that its negation $\neg f$ is true. It then uses the probability semiring with regular addition and multiplication as operators to compute the probability of a query on the AC constructed for this query, cf. Figure 1d. The probability semiring is defined as follows:</p>
<p>$$
\begin{aligned}
a \oplus b &amp; =a+b \
a \otimes b &amp; =a b \
e^{\oplus} &amp; =0 \
e^{\otimes} &amp; =1
\end{aligned}
$$</p>
<p>And the accompanying labeling function as:</p>
<p>$$
\begin{aligned}
L(f) &amp; =p &amp; &amp; \text { for } p:: f \
L(\neg f) &amp; =1-p &amp; &amp; \text { with } L(f)=p
\end{aligned}
$$</p>
<p>This idea is generalized in aProbLog to compute such values based on arbitrary commutative semirings. Instead of probability labels on facts, aProbLog uses a labeling function that explicitly associates values from the chosen semiring with</p>
<p>both facts and their negations, and combines these using semiring addition $\oplus$ and multiplication $\otimes$ on the AC. We use the gradient semiring, whose elements are tuples $\left(p, \frac{\partial p}{\partial \theta}\right)$, where $p$ is a probability (as in ProbLog), and $\frac{\partial p}{\partial \theta}$ is the partial derivative of that probability with respect to a parameter $\theta$, that is, the probability $p_{i}$ of a probabilistic fact with learnable probability, written as $t\left(p_{i}\right):: f_{i}$. This is easily extended to a vector of parameters $\vec{\theta}=\left[\theta_{1}, \ldots, \theta_{N}\right]^{T}$, the concatenation of all $N$ probabilistic parameters in the ground program, as it is easier and faster to process all gradients in one vector. Semiring addition $\oplus$, multiplication $\otimes$ and the neutral elements with respect to these operations are defined as follows:</p>
<p>$$
\begin{aligned}
\left(a_{1}, \overrightarrow{a_{2}}\right) \oplus\left(b_{1}, \overrightarrow{b_{2}}\right) &amp; =\left(a_{1}+b_{1}, \overrightarrow{a_{2}}+\overrightarrow{b_{2}}\right) \
\left(a_{1}, \overrightarrow{a_{2}}\right) \otimes\left(b_{1}, \overrightarrow{b_{2}}\right) &amp; =\left(a_{1} b_{1}, b_{1} \overrightarrow{a_{2}}+a_{1} \overrightarrow{b_{2}}\right) \
e^{\oplus} &amp; =(0, \overrightarrow{0}) \
e^{\oplus} &amp; =(1, \overrightarrow{0})
\end{aligned}
$$</p>
<p>Note that the first element of the tuple mimics ProbLog's probability computation, whereas the second simply computes gradients of these probabilities using derivative rules.</p>
<p>Gradient descent with aProbLog. To use the gradient semiring for gradient descent parameter learning in ProbLog, we first transform the ProbLog program into an aProbLog program by extending the label of each probabilistic fact $p:: f$ to include the probability $p$ as well as the gradient vector of $p$ with respect to the probabilities of all probabilistic facts and ADs in the program, i.e.,</p>
<p>$$
\begin{aligned}
L(f) &amp; =(p, \overrightarrow{0}) &amp; &amp; \text { for } p:: f \text { with fixed } p \
L\left(f_{i}\right) &amp; =\left(p_{i}, \mathbf{e}<em i="i">{i}\right) &amp; &amp; \text { for } t\left(p</em> \
L(\neg f) &amp; =(1-p,-\nabla p) &amp; &amp; \text { with } L(f)=(p, \nabla p)
\end{aligned}
$$}\right):: f_{i} \text { with learnable } p_{i</p>
<p>where the vector $\mathbf{e}_{i}$ has a 1 in the $i$ th position and 0 in all others. For fixed probabilities, the gradient does not depend on any parameters and thus is 0 . Note that after each update step, the probabilistic parameters are clipped to the $[0,1]$ range, and the parameters of an AD are re-normalized to ensure that they sum to one. For the other cases, we use the semiring labels as introduced above.</p>
<h1>Example 4</h1>
<p>Assume we want to learn the probabilities of earthquake and burglary in the example of Figure 1, while keeping those of the other facts fixed. Figure 3 shows the evaluation of the same AC as in Figure 1d, but with the gradient semiring. The nodes in the AC now also contain the gradient (the second element of the tuple). The result on the top node shows that</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The AC evaluated using the gradient semiring. (Example 4)
the partial derivative of the query is 0.45 and 0.4 w.r.t. the earthquake and burglary parameters respectively.</p>
<h1>5.2. Gradient descent for DeepProbLog</h1>
<p>Just as the only difference between inference in ProbLog and DeepProbLog is the evaluation of the nADs, the only difference between gradient descent in ProbLog and DeepProbLog is optimizing the neural parameters alongside the probabilistic parameters. As mentioned in the previous section, the probabilistic parameters $p_{i}$ in the logic program can be optimized by using the gradient semiring, which allows us to calculate $\partial P(q) / \partial p_{i}$. This gradient is then used to perform the update by using gradient descent. Note that since the outputs of the neural network are used as probabilities in the logic program and can be learned, we can view them as a kind of abstract parameters. However, although we can derive a gradient for these abstract parameters, we cannot optimize them directly, as the logic is unaware of the neural parameters that determine the value of these abstract parameters. Recall from Equation (1) that the gradient of the internal (neural) parameters in standard supervised learning can be derived using the chain rule in backpropagation. Below, we show how we can derive the gradient for these neural parameters of the loss applied to $P(q)$ (Definition 5), rather than a loss function defined directly on the output of the neural network.</p>
<p>Specifically, consider the case of a single neural annotated disjunction, with probabilities $\hat{p}_{i}$ (i.e., the aforementioned abstract parameters), calculated by evaluating a neural network with softmax output. The predicted probability that the query holds true, based on the current values of the neural and probabilistic parameters, is written $P(q)$. While training, true examples should yield a predicted query probability close to the expected query probability, which is expressed by means of a loss function $\mathcal{L}$ as introduced in Definition 5.</p>
<p>Application of the chain rule leads to</p>
<p>$$
\frac{d \mathcal{L}}{d \theta_{k}}=\frac{\partial \mathcal{L}}{\partial P(q)} \sum_{i} \frac{\partial P(q)}{\partial \hat{p}<em i="i">{i}} \frac{\partial \hat{p}</em>
$$}}{\partial \theta_{k}</p>
<p>where the derivative of the loss with respect to any trainable parameter $\theta_{k}$ in the neural network is decomposed into the partial derivative of the loss with</p>
<p>respect to the predicted output $P(q)$, the latter's derivative $\partial P(q) / \partial \hat{p}<em i="i">{i}$ with respect to each component of the annotated disjunction as obtained with the gradient semiring, and finally $\partial \hat{p}</em>$, to systematically obtain the loss gradients for all neural parameters.
Extending this approach to the situation of multiple neural predicates is straightforward. If the same neural network is used for different neural predicates (e.g. in Example 3), the final derivative is obtained by summing over the contributions of each neural predicate.} / \partial \theta_{k}$, the derivative of the neural network's output components with respect to the considered parameter. The latter is obtained by the standard application of the chain rule in the neural network. The backpropagation procedure in the neural network can thus be started by providing $\partial P(q) / \partial \hat{p}_{i</p>
<p>Then, standard gradient-based optimizers (e.g. SGD, Adam, ...) are used to update the parameters of the network. During gradient computation with aProbLog, the probabilities of neural ADs are kept constant. Furthermore, updates on neural ADs come from the neural network part of the model, where the use of a softmax output layer ensures a normalized distribution, hence not requiring the additional normalization as for non-neural ADs.</p>
<p>To extend the gradient semiring to DeepProbLog programs, we define it for nADs and neural facts. The label for the nAD is defined as:</p>
<p>$$
L\left(f_{i}\right)=\left(\hat{p}<em j="j">{j}, \mathbf{e}</em>
$$}\right) \quad \text { for } \ldots ; n n\left(m, \vec{i}, d_{j}\right):: r\left(\vec{i}, d_{j}\right) ; \ldots \text { a ground nAD </p>
<p>Where $d_{j}$ is the j-th domain element, $\hat{p}_{j}$, is the j-th element of the output of the neural network $m$ evaluated on input $\vec{i}$. The label for a neural fact is defined as:</p>
<p>$$
L\left(f_{i}\right)=\left(\hat{p}, \mathbf{e}_{j}\right) \quad \text { for } n n(m, \vec{i}):: r(\vec{i}) \text { a ground neural fact }
$$</p>
<p>where $\hat{p}$ is the output of the neural network $m$ evaluated on input $\vec{i}$. Since the first element of the tuple for nADs and neural facts is the evaluation of the neural networks as in Section 4.2, this change remains semantically equivalent.</p>
<h1>Example 5</h1>
<p>To demonstrate the learning pipeline (Figure 5), we will apply it on the MNIST addition example show in Section 4.2 with a small extension: some of the labels have been corrupted and are picked randomly from a uniform distribution over $[0,18]$. The goal is to also learn the fraction of noisy examples. The DeepProbLog program is given in Figure 4a. Grounding on the query addition $(a, b, 1)$ results in the ground DeepProbLog program shown in Figure 4b. The arithmetic circuit corresponding to the ground program is shown in Figure 4c. As can be seen, the neural networks already have a confident prediction for both images (being 0 and 1 respectively). The top right shows how the different partial derivatives that are calculated: one w.r.t. to the noisy parameter, ten for the evaluation of the neural network on input a and ten for the evaluation on input b.</p>
<div class="codehilite"><pre><span></span><code><span class="nf">nn</span><span class="p">(</span><span class="s s-Atom">classifier</span><span class="p">,</span> <span class="p">[</span><span class="nv">X</span><span class="p">],</span> <span class="nv">Y</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span> <span class="p">..</span> <span class="mi">9</span><span class="p">])</span> <span class="o">::</span> <span class="nf">digit</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">).</span>
<span class="nf">t</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span> <span class="o">::</span> <span class="s s-Atom">noisy</span><span class="p">.</span>
<span class="mi">1</span><span class="o">/</span><span class="mi">19</span> <span class="o">::</span> <span class="nb">uniform</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="p">;</span> <span class="p">...</span> <span class="p">;</span> <span class="mi">1</span><span class="o">/</span><span class="mi">19</span> <span class="o">::</span> <span class="nb">uniform</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">,</span><span class="mi">18</span><span class="p">).</span>
<span class="nf">addition</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">,</span><span class="nv">Z</span><span class="p">)</span> <span class="o">:-</span> <span class="s s-Atom">noisy</span><span class="p">,</span> <span class="nb">uniform</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">,</span><span class="nv">Z</span><span class="p">).</span>
<span class="nf">addition</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">Y</span><span class="p">,</span><span class="nv">Z</span><span class="p">)</span> <span class="o">:-</span> <span class="s s-Atom">\+noisy</span><span class="p">,</span> <span class="nf">digit</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="nv">N1</span><span class="p">),</span> <span class="nf">digit</span><span class="p">(</span><span class="nv">Y</span><span class="p">,</span><span class="nv">N2</span><span class="p">),</span> <span class="nv">Z</span> <span class="o">is</span> <span class="nv">N1</span><span class="o">+</span><span class="nv">N2</span><span class="p">.</span>
</code></pre></div>

<p>(a) The DeepProbLog program.
nn(classifier, [a],0) : : digit(a,0); nn(classifier, [a],1) : : digit(a,1).
nn(classifier, [b],0) : : digit(b,0); nn(classifier, [b],1) : : digit(b,1).
$\mathrm{t}(0.2)$ : :noisy.
1/19::uniform $(a, b, 1)$.
addition $(a, b, 1)$ :- noisy, uniform $(a, b, 1)$.
addition(a,b,1) :- +noisy, digit(a,0), digit(b,1).
addition(a,b,1) :- +noisy, digit(a,1), digit(b,0).
(b) The ground DeepProbLog program.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(c) The AC for query addition $(a, b, 1)$.</p>
<p>Figure 4: Parameter learning in DeepProbLog. (Example 5)
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The learning pipeline.</p>
<h1>6. Experimental Evaluation</h1>
<p>We perform three sets of experiments to demonstrate that DeepProbLog supports (i) logical reasoning and deep learning; (ii) program induction; and (iii) probabilistic inference and combined probabilistic and deep learning.</p>
<p>We provide implementation details at the end of this section and list all programs in Appendix A.</p>
<h3>6.1. Logical reasoning and deep learning</h3>
<p>To show that DeepProbLog supports both logical reasoning and deep learning, we extend the classic learning task on the MNIST dataset [24] to four more complex problems that require reasoning:</p>
<h2>T1: addition( $\mathbf{Z}, 8)$</h2>
<p>Instead of using labeled single digits, we train on pairs of images, labeled with the sum of the individual labels. This is the same as Example 3. The DeepProbLog program consists of the clause</p>
<p>$$
\text { addition }(X, Y, Z):-\operatorname{digit}(X, X 2), \operatorname{digit}(Y, Y 2), Z \text { is } X 2+Y 2
$$</p>
<p>and a neural AD for the digit/2 predicate, which classifies an MNIST image. We compare to a CNN baseline ${ }^{3}$ classifying the two images into the 19 possible sums.</p>
<p>Results. Figure 6 shows the learning curves for the baseline (orange) and DeepProbLog (blue) on the single-digit addition. We evaluated on 3 levels of data availability: 30000 examples, 3000 and 300 examples. As can be seen in the figures, DeepProbLog converges faster and achieves a higher accuracy than the baseline. In the case for $\mathrm{N}=30000$ (Figure 6a), the difference between the baseline and DeepProbLog is significant, but not immense. However, for $\mathrm{N}=3000$ and especially $\mathrm{N}=300$, the difference becomes more apparent.
The reason behind this disparity is that the baseline needs to learn making a decision for the combined input digits (and there are a 100 different sums possible), whereas the DeepProbLog's neural predicate only needs to recognize individual digits (with only 10 possibilities). Table 1 shows the average accuracy on the test set for the different models for different training set sizes.</p>
<p>T2: addition( $\mathbf{Z}, \mathbf{Z}, \mathbf{Z}, 63)$
The input consists of two lists of images, each element being a digit. Each list represents a multi-digit number. The label is the sum of the two numbers. The neural predicate remains the same. Learning the new predicate requires only a small change in the logic program. Because the CNN baseline cannot handle numbers of varying size, we fixed the size of the input to two-digit numbers.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: MNIST Single-Digit Addition (T1). The graphs show the accuracy on the validation set during training for different training set sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Number of training examples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">30000</td>
<td style="text-align: center;">3000</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">$93.46 \pm 0.49$</td>
<td style="text-align: center;">$78.32 \pm 2.14$</td>
<td style="text-align: center;">$23.64 \pm 1.75$</td>
</tr>
<tr>
<td style="text-align: left;">DeepProbLog</td>
<td style="text-align: center;">$97.20 \pm 0.45$</td>
<td style="text-align: center;">$92.18 \pm 1.57$</td>
<td style="text-align: center;">$67.19 \pm 25.05$</td>
</tr>
</tbody>
</table>
<p>Table 1: The accuracy on the test set for T1.</p>
<p>Results. First, we perform an experiment where we take the neural network trained in T1 and use it in this model without any further training. Evaluating it on the same test set, we achieve an accuracy that is not significantly different from training on the full dataset of T2. This demonstrates that the approach used in DeepProbLog causes it to generalize well beyond training data. Figure 7 shows the learning curves for the baseline (orange) and DeepProbLog (blue) on the multi-digit addition. DeepProbLog achieves a somewhat lower accuracy compared to the single digit problem due to the compounding effect of the classification error on the individual digits, but the model generalizes well. The baseline fails to learn from few examples ( 150 and 1500 ). It is able to learn with 15000 examples, but converges very slowly. Table 2 shows the average accuracy on the test set for the different models for different training set sizes.</p>
<h1>T3: addition[ 27</h1>
<p>The input consists of 3 MNIST images such that the last is the sum of the first two. This task demonstrates potential pitfalls of only providing supervision on the logic level. Namely, without any regularization, the neural network quickly learns to predict 0 for all digits, i.e., the model collapses to always predicting $0+0=0$, as it is a valid logical solution. To avoid this, we add a regularisation term based on entropy maximization (Equation 18, Section 6.4). The intuition behind this regularisation term is that it penalizes mode collapse by requiring the entropy of the average output distribution per batch to be high. As such, this term encourages exploration, but is only necessary to start the training of the neural networks.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We'd like to thank Paolo Frasconi for the interesting discussion and idea for a new baseline.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
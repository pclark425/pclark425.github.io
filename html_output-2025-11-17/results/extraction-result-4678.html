<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4678 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4678</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4678</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-270372057</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.05872v1.pdf" target="_blank">STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT-3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4678.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4678.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Behavior Cloning (BC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavior Cloning (T5-based) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based behavior cloning baseline (reported by prior work) using a T5-base transformer to directly predict actions from observations; used in this paper as a comparison baseline on ScienceWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Behavior Cloning (BC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer-based behavior cloning model that maps observations to actions; reported in prior work and compared against STARLING in ScienceWorld evaluations (reported as a state-of-the-art LLM-based baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>T5-base (reported as 11B parameter initialization with Macaw)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ScienceWorld (test variations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>30 science-domain tasks (e.g., change-of-state, classification, biology) where the agent must complete task-specific objectives expressed in text-game format.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No ablation w.r.t. memory for BC is reported in this paper; BC is only compared as a baseline in aggregate performance tables.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes that LLM-based approaches (including BC) do not necessarily improve representation learning performance over smaller GRU-based models in some settings; specific memory-related limitations for BC are not described.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>No explicit memory recommendations for BC are given; BC is used as a comparative baseline to show that STARLING's pretraining can outperform these LLM-based baselines on many ScienceWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4678.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4678.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text Decision Transformer (TDT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Decision Transformer (TDT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision-transformer style LLM approach that models trajectories to predict actions; included as an LLM-based baseline in comparisons on ScienceWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Text Decision Transformer (TDT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer-based sequence model applied to text-game trajectories (decision-transformer family); reported in prior work and used here as a state-of-the-art LLM-based comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>T5-base (reported as 11B parameter initialization with Macaw)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ScienceWorld (test variations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ScienceWorld tasks covering change-of-state, classification, and biology where agents must plan and act in text-based environments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No memory-focused ablation for TDT in this paper; TDT is included for aggregate performance comparison in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper reiterates prior findings that LLM-based representation learning does not always translate to better text-game performance; no TDT-specific memory failure modes are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>No explicit recommendations about memory usage for TDT are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4678.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4678.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM-GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALM (GPT-2 based) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based baseline (CALM) using GPT-2 reported in prior work and compared against STARLING on ScienceWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM-GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A GPT-2 based model (CALM) used for action prediction/decision-making in text games; included as a comparative LLM-based baseline (reported to use GPT-2 with ~131M parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (131M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ScienceWorld (test variations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation on multiple ScienceWorld tasks requiring textual interaction and procedural steps (e.g., boiling, classification).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No memory-specific ablation of CALM is reported in this paper; CALM is used only as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>No CALM-specific memory discussion in this paper; general limitations of LLM-based agents (e.g., not always improving performance) are reiterated.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>No explicit memory recommendations for CALM are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4678.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4678.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (qualitative mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (conversational large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational LLM (ChatGPT) discussed qualitatively for its ability to interact with interactive-fiction games; the authors report limitations in tracking object states and preconditions when using ChatGPT as a player.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A conversational LLM used in informal experiments to interact with Inform7-based interactive fiction; discussed qualitatively (not used as an experimental baseline with quantitative metrics in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (model not parameterized in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Inform7 / Inform7-generated interactive fiction games (STARLING games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Interacting with Inform7 games as a player (producing action sequences to complete game goals).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>none (no external memory beyond model context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>none stored outside of the LLM context (authors report that ChatGPT does not reliably track in-game object states)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>none beyond context updates across the chat; no explicit external memory update mechanism reported</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>none beyond using the LLM context for generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No ablation studies with ChatGPT are reported; discussion is qualitative describing failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors report ChatGPT struggles to keep track of states of in-game objects and preconditions (e.g., may not require the player to turn on the stove before using it), makes small factual errors, and produces non-deterministic behaviors; thus it has difficulty maintaining correct game state across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors note these limitations as evidence that additional mechanisms (state-tracking, validators, or external memory) may be necessary for LLM agents to reliably play Inform7-style games, but provide no detailed recipe; they suggest exploring models that better track state or integrating validators/world-models to correct factual/state errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4678.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4678.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based agents (general, prior work mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language model based agents for text games (general mentions and prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references several prior lines of work that apply LLMs to text games (action generation, world models, direct play), noting mixed results and specific limitations in representation and state-tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-based agents (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior works repurpose LLMs for action-generation, world-model construction, or direct gameplay in text games (citations include Yao et al., 2020; Tsai et al., 2023; Ammanabrolu et al., 2020; Wang et al., 2023b). The current paper positions STARLING relative to these approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Various text-game environments (TextWorld, Jericho/Zork, ScienceWorld) cited across prior work</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks vary per prior work: action generation, world-model learning, direct play in interactive-fiction environments and domain-specific science tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>This paper does not perform memory-focused ablations on the cited LLM-based agents, but cites literature showing mixed effectiveness of LLMs for representation learning in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General limitations summarized: LLMs often struggle with state-tracking, precondition enforcement, are prone to factual errors, and can be non-deterministic; LLM use does not automatically yield better performance than smaller, task-tailored architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors recommend using LLMs to generate auxiliary pre-training games (their STARLING approach) rather than relying solely on LLMs for in-game decision making; they also suggest future exploration of integrating validators, world models, or mechanisms to improve state-tracking and reduce factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models play text games well? current state-of-the-art and open questions <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Behavior cloned transformers are neurosymbolic reasoners <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 1)</em></li>
                <li>Building world models with language models for interactive fiction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4678",
    "paper_id": "paper-270372057",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "Behavior Cloning (BC)",
            "name_full": "Behavior Cloning (T5-based) baseline",
            "brief_description": "An LLM-based behavior cloning baseline (reported by prior work) using a T5-base transformer to directly predict actions from observations; used in this paper as a comparison baseline on ScienceWorld tasks.",
            "citation_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
            "mention_or_use": "use",
            "agent_name": "Behavior Cloning (BC)",
            "agent_description": "A transformer-based behavior cloning model that maps observations to actions; reported in prior work and compared against STARLING in ScienceWorld evaluations (reported as a state-of-the-art LLM-based baseline).",
            "llm_model_name": "T5-base (reported as 11B parameter initialization with Macaw)",
            "game_or_benchmark_name": "ScienceWorld (test variations)",
            "task_description": "30 science-domain tasks (e.g., change-of-state, classification, biology) where the agent must complete task-specific objectives expressed in text-game format.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No ablation w.r.t. memory for BC is reported in this paper; BC is only compared as a baseline in aggregate performance tables.",
            "challenges_or_limitations": "Paper notes that LLM-based approaches (including BC) do not necessarily improve representation learning performance over smaller GRU-based models in some settings; specific memory-related limitations for BC are not described.",
            "best_practices_or_recommendations": "No explicit memory recommendations for BC are given; BC is used as a comparative baseline to show that STARLING's pretraining can outperform these LLM-based baselines on many ScienceWorld tasks.",
            "uuid": "e4678.0",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Text Decision Transformer (TDT)",
            "name_full": "Text Decision Transformer (TDT)",
            "brief_description": "A decision-transformer style LLM approach that models trajectories to predict actions; included as an LLM-based baseline in comparisons on ScienceWorld.",
            "citation_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
            "mention_or_use": "use",
            "agent_name": "Text Decision Transformer (TDT)",
            "agent_description": "A transformer-based sequence model applied to text-game trajectories (decision-transformer family); reported in prior work and used here as a state-of-the-art LLM-based comparison.",
            "llm_model_name": "T5-base (reported as 11B parameter initialization with Macaw)",
            "game_or_benchmark_name": "ScienceWorld (test variations)",
            "task_description": "ScienceWorld tasks covering change-of-state, classification, and biology where agents must plan and act in text-based environments.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No memory-focused ablation for TDT in this paper; TDT is included for aggregate performance comparison in Table 3.",
            "challenges_or_limitations": "Paper reiterates prior findings that LLM-based representation learning does not always translate to better text-game performance; no TDT-specific memory failure modes are reported here.",
            "best_practices_or_recommendations": "No explicit recommendations about memory usage for TDT are provided in this paper.",
            "uuid": "e4678.1",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CALM-GPT2",
            "name_full": "CALM (GPT-2 based) baseline",
            "brief_description": "An LLM-based baseline (CALM) using GPT-2 reported in prior work and compared against STARLING on ScienceWorld tasks.",
            "citation_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
            "mention_or_use": "use",
            "agent_name": "CALM-GPT2",
            "agent_description": "A GPT-2 based model (CALM) used for action prediction/decision-making in text games; included as a comparative LLM-based baseline (reported to use GPT-2 with ~131M parameters).",
            "llm_model_name": "GPT-2 (131M parameters)",
            "game_or_benchmark_name": "ScienceWorld (test variations)",
            "task_description": "Evaluation on multiple ScienceWorld tasks requiring textual interaction and procedural steps (e.g., boiling, classification).",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No memory-specific ablation of CALM is reported in this paper; CALM is used only as a comparative baseline.",
            "challenges_or_limitations": "No CALM-specific memory discussion in this paper; general limitations of LLM-based agents (e.g., not always improving performance) are reiterated.",
            "best_practices_or_recommendations": "No explicit memory recommendations for CALM are given here.",
            "uuid": "e4678.2",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatGPT (qualitative mention)",
            "name_full": "ChatGPT (conversational large language model)",
            "brief_description": "A conversational LLM (ChatGPT) discussed qualitatively for its ability to interact with interactive-fiction games; the authors report limitations in tracking object states and preconditions when using ChatGPT as a player.",
            "citation_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
            "mention_or_use": "mention",
            "agent_name": "ChatGPT",
            "agent_description": "A conversational LLM used in informal experiments to interact with Inform7-based interactive fiction; discussed qualitatively (not used as an experimental baseline with quantitative metrics in this paper).",
            "llm_model_name": "ChatGPT (model not parameterized in this paper)",
            "game_or_benchmark_name": "Inform7 / Inform7-generated interactive fiction games (STARLING games)",
            "task_description": "Interacting with Inform7 games as a player (producing action sequences to complete game goals).",
            "memory_used": false,
            "memory_type": "none (no external memory beyond model context window)",
            "memory_representation": "none stored outside of the LLM context (authors report that ChatGPT does not reliably track in-game object states)",
            "memory_update_mechanism": "none beyond context updates across the chat; no explicit external memory update mechanism reported",
            "memory_retrieval_mechanism": "none beyond using the LLM context for generation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No ablation studies with ChatGPT are reported; discussion is qualitative describing failure modes.",
            "challenges_or_limitations": "Authors report ChatGPT struggles to keep track of states of in-game objects and preconditions (e.g., may not require the player to turn on the stove before using it), makes small factual errors, and produces non-deterministic behaviors; thus it has difficulty maintaining correct game state across steps.",
            "best_practices_or_recommendations": "Authors note these limitations as evidence that additional mechanisms (state-tracking, validators, or external memory) may be necessary for LLM agents to reliably play Inform7-style games, but provide no detailed recipe; they suggest exploring models that better track state or integrating validators/world-models to correct factual/state errors.",
            "uuid": "e4678.3",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-based agents (general, prior work mentions)",
            "name_full": "Large language model based agents for text games (general mentions and prior work)",
            "brief_description": "The paper references several prior lines of work that apply LLMs to text games (action generation, world models, direct play), noting mixed results and specific limitations in representation and state-tracking.",
            "citation_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
            "mention_or_use": "mention",
            "agent_name": "LLM-based agents (general)",
            "agent_description": "Prior works repurpose LLMs for action-generation, world-model construction, or direct gameplay in text games (citations include Yao et al., 2020; Tsai et al., 2023; Ammanabrolu et al., 2020; Wang et al., 2023b). The current paper positions STARLING relative to these approaches.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Various text-game environments (TextWorld, Jericho/Zork, ScienceWorld) cited across prior work",
            "task_description": "Tasks vary per prior work: action generation, world-model learning, direct play in interactive-fiction environments and domain-specific science tasks.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "This paper does not perform memory-focused ablations on the cited LLM-based agents, but cites literature showing mixed effectiveness of LLMs for representation learning in text games.",
            "challenges_or_limitations": "General limitations summarized: LLMs often struggle with state-tracking, precondition enforcement, are prone to factual errors, and can be non-deterministic; LLM use does not automatically yield better performance than smaller, task-tailored architectures.",
            "best_practices_or_recommendations": "Authors recommend using LLMs to generate auxiliary pre-training games (their STARLING approach) rather than relying solely on LLMs for in-game decision making; they also suggest future exploration of integrating validators, world models, or mechanisms to improve state-tracking and reduce factual errors.",
            "uuid": "e4678.4",
            "source_info": {
                "paper_title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models play text games well? current state-of-the-art and open questions",
            "rating": 2,
            "sanitized_title": "can_large_language_models_play_text_games_well_current_stateoftheart_and_open_questions"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Behavior cloned transformers are neurosymbolic reasoners",
            "rating": 2,
            "sanitized_title": "behavior_cloned_transformers_are_neurosymbolic_reasoners"
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 1,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Building world models with language models for interactive fiction",
            "rating": 1,
            "sanitized_title": "building_world_models_with_language_models_for_interactive_fiction"
        }
    ],
    "cost": 0.0130015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</p>
<p>Shreyas Basavatia sbasavatia3@gatech.edu 
Keerthiram Murugesan keerthiram.murugesan@ibm.com </p>
<p>Georgia Institute of Technology</p>
<p>IBM Research</p>
<p>STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models
2D0B809C95D3B5B3CA011797160EAD1F
Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents.Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills.In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment.These games let the agent hone their skills on a predefined set of tasks.We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision.Experimental results based on both the human participants and baseline text-based RL agents reveal that current stateof-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can.These results enforce STAR-LING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.* Equal Contributions  Work done while SB was a high school student at Pelham Memorial High School.Code and data can be found at https: //github.com/IBM/starling-agent.</p>
<p>Introduction</p>
<p>Interactive fiction games such as Zork can be utilized as an important test-bed to improve the generalization capabilities of text-based reinforcement learning (TBRL) agents (Hausknecht et al., 2020;Jansen, 2022).In these games, both the observed state of the game and the actions taken are in natural language.To play these games, the agents (or human players) need to understand the observed text from the environment and take relevant action toward the goal.These games encourage agents to understand the underlying state of the game and take actions to interact with the environment.In order to be successful, agents must use previously learned skills in new situations to complete an overarching goal.Current environments of interactive fiction games suffer from two major problems.First, environments such as TextWorld Commonsense measure simple commonsense reasoning based on one-hop relationships between entities (e.g., apple  refrigerator) (Murugesan et al., 2021a) but lack game complexity (besides a fewer number of games) to learn skills and generalize to novel domains.Second, environments such as ScienceWorld (even though many variations of task-based games are available) and Jericho are arXiv:2406.05872v1[cs.LG] 9 Jun 2024 domain-specific so agents that play these environments may perform well while conducting specific tasks like completing science experiments but lack generalized skills to apply them to other situations (Wang et al., 2022;Hausknecht et al., 2020).Most importantly, in order to generate a large number of games to train the RL agents to master skills in these environments, we will have to employ human annotators to manually design, generate, and deploy the game.Therefore, the purpose of this work is to develop an efficient approach that generates a large amount of text-based games to train the RL agents to master the desired skills and excel at the target environments such as TWC, ScienceWorld, etc.</p>
<p>As developing a set of text-based games is a time-intensive manual process, we propose Self-supervised Text-bAsed RL learnING, "STAR-LING", an interactive environment that utilizes Large Language Models (LLM) and an integrated interactive fiction game engine (Inform7 (Nelson, 2006)) to easily produce games in any domain.We generate a set of 100 text-based games using GPT3 (Brown et al., 2020) based on the input game ideas (seed list) that emphasize the need for the everyday skills such as boiling water, cooking pasta, etc. in (pre-)training text-based RL agents.These games require agents to use a specific sequence of actions to achieve the goal and successfully complete the game.For example, while cooking pasta, an agent must first gather the ingredients, fill pot with water, boil the water, and put the pasta in the pot.We then deploy the pre-trained RL agent on the target environments.This novel game-generation method can easily be used by others to create their own games and be adapted for future applications to build challenging RL agents in various domains.Figure 1 shows the overview of the proposed approach for self-supervised text-based reinforcement learning using LLM.</p>
<p>2 Self-supervised Text-based RL Self-supervised RL involves bootstrapping RL agents with auxiliary tasks in an unsupervised or semi-supervised setting to accelerate learning and generalize in the target tasks.With the recent interest in LLMs, in this paper, we consider LLMs as an alternative option to pre-train an RL agent with minimal human supervision.Unlike in the other textbased environments such as TextWorld (Ct et al., 2019), TextWorld Commonsense (TWC) (Muruge- san et al., 2021a), Jericho (Hausknecht et al., 2020), ScienceWorld (Wang et al., 2022), we utilize the skill generation capability of the large language models (Huang et al., 2022) to automatically generate text-based games based on the input game ideas with minimal human supervision.Our proposed approach for self-supervised TBRL, STARLING is an interactive text-based environment with assistance from LLM and enables the text-based RL agent to hone their extra-curricular skills1 .In this paper, we assume a seed list of game ideas is already available as input to STARLING.These game ideas are chosen to exhibit specific skills either for creating a generalized agent or targeting domain-specific environments.Optionally, the RL agent can generate a new set of game ideas during training, specific to the target domain to improve its performance.</p>
<p>Constructing Pre-training Games</p>
<p>In this section, we briefly describe how we generate the pre-training games from the game ideas using LLM and Inform7.Given the set of game ideas (seed) to LLM (Brown et al., 2020), we design a method that procedurally generates text-based games based on the interactive fiction game engine.In this paper, we use GPT3 as our LLM.Inform7 is an interactive fiction programming language that allows users to create interactive fiction games using natural language instructions (Nelson, 2006).Previous text-based environments such as TextWorld, Jericho, ScienceWorld, etc. use Inform7 (in the backend) to generate a handful of text-based games manually that require agents to explore the environment and take a sequence of actions to complete a goal such as cooking a pasta.Based on our observation from these environments, we find that the game generation can be modularized into four parts: setup, object creation, custom action, and reward assignment:</p>
<ol>
<li>
<p>Setup -defines basic properties about the game such as the room, entity types, any external libraries (Inform7), etc.</p>
</li>
<li>
<p>Object Creation -creates in-game entities such as bread or jelly.Each entity is placed in its proper location like the refrigerator or cabinet and assigned properties such as portable, open, or closed.</p>
</li>
<li>
<p>Custom actions -defines actions not native to Inform7.Each custom action checks for the pre-conditions and then executes the action by initiating the relevant state changes, and returning the proper observations to the agent.We utilize predefined action templates to incorporate custom actions during the game generation.</p>
</li>
<li>
<p>Rewards -assigns reward value for gathering the necessary entities and completing custom actions to achieve the goal.Once all the rewards are collected for each game, the game ends.</p>
</li>
</ol>
<p>Figure 2 shows the overview of the game generation using STARLING.When we feed a game idea from the seed list, STARLING prepares a prompt using natural language instruction and example game metadata as shown in Figure 3(a), with information about the setup, objects, custom actions, and rewards required for the game idea.We input this prompt to an LLM which generates the requested information as shown in Figure 3(b).We initiate each prompt for a game idea with the necessary objects that the agent needs and agents must collect those objects and use them to cook, clean, build, or complete the high-level task.In order to be successful in these games, agents must understand the properties, location, and affordances of objects in addition to the specific sequence of actions needed to accomplish the task.</p>
<p>Next, we write the output from the GPT3 output into a JSON file as shown in Figure 13 (supplementary).The objects, actions, and tasks from the GPT3 output correspond to the entities, custom actions, and verbs sections of the JSON file respectively.At this stage, the user may update or change game information in the JSON file.If the user approves the game-related data in the JSON file, we write the Inform7 code based on the JSON file.We then convert this code into an Inform7 game for a given game idea using the Glulx2 interpreter for interactive fiction games.</p>
<p>Parsing LLM Response</p>
<p>Since the response generated by LLM may not strictly follow the desired format, we follow additional steps to mitigate the irrelevant content in the response from LLM.First, we request a specific set of game-related data from LLM in a slotfilling style text generation to reduce the amount of long unstructured text generation (Rakotonirina et al., 2022).Since LLM are good at instructionfollowing when few-shot examples (input-output pairs) of a similar problem are given as a part of the prompt input, we add k-shot examples (k=3) to guide the LLM to generate a response.Figure 3(a) shows one of the three examples given as a part of the input prompt.Finally, during game compilation, the Glulx interpreter verifies whether the information extracted from the response adheres to the Inform7 programming language syntax.In addition, the pipeline for game generation provides an option for users to review the generated JSON file.When the generated game files still contain irrelevant content, we repeat the text generation multiple times to get the desired response from LLM.</p>
<p>Game Insights</p>
<p>Using the above approach repeatedly, we built a set of 100 games with minimal human supervision for training and evaluating the text-based RL agents with skills.These games have on average 2 skills and 4 rewarded states per game.These games have multiple sub-tasks which indicate that agents must utilize at least 2 skills (on average) for each game in the correct order.</p>
<p>Agents must take approximately 7 actions on average to complete each game, though some of these actions do not necessarily have to be completed in order (e.g. the agent can "turn on the stove" before "fill the pot with water" and vice versa).Games in TWC only require agents to gather objects and take actions to place them in their commonsense locations.These actions can often be completed in any order, whereas generated games, such as cooking pasta, require agents to gather objects and use other related skills in a specific sequence to achieve the final goal.</p>
<p>Experiments</p>
<p>In this section, we report the experimental results of the proposed approach: STARLING.We pre-train the RL agent on the generated 100 games (train split).We evaluate the pre-trained agent (STAR-LING) on three benchmark environments for textbased games: TextWorld Commonsense (TWC) with easy, medium, and hard difficulty levels (Murugesan et al., 2021a) 2) ScienceWorld with 4 tasks and variations (Wang et al., 2022).3) Zork1 from Jericho.</p>
<p>Text-based RL Agent</p>
<p>In this section, we briefly describe the text-based agent used for all the experiments.Based on the recent observation that using LLM to learn the underlying representation of text in the environment does not necessarily improve the performance (Wang et al., 2022), we follow previous works (He et al.,  We use individual GRUs (Cho et al., 2014) for the information from the text-based environment such as observed text, the content of the inventory, the description of the room where the agent currently is located, and a valid list of actions.We learn the state representation by concatenating the individual representations from their GRUs (Cho et al., 2014).We compute the action probability from both the state and action representations.We use Advantage Action-critic (A2C) to train the network (Mnih et al., 2016).In order to limit the impact of architecture and text-based RL algorithms, we consider these standard architectures for representation learning and RL algorithms that have proven to work well in text-based environments.We plan to study the impacts of different RL algorithms and diverse sets of representation learning approaches in our future work.All the results reported in this paper are averaged over 3 runs.</p>
<p>Pre-training Text-based RL Agent</p>
<p>We generate 100 games that demonstrate basic skills in these environments such as cooking pasta, painting the living room, boiling water, lighting a candle, etc.We split these 100 games into 75 games for training and 25 for evaluation.We trained the vanilla TBRL agent on these 75 pretraining games over 100 episodes (50 max.steps per episode) and evaluated it on 25 held-out games.</p>
<p>We compare the performance of the vanilla TBRL agent (pre-training) against both the random agent (picks random action at each step) and the Human performance.We use the mean normalized score and mean moves/steps taken by the agent for comparison.We collect the Human performance results based on the 48 participants (Section B). Figure 4 shows the training performance and Table 1 shows the evaluation results.We can see that human participants (high-school students) solved these games with a perfect normalized score of 1.0 indicating that these games are easy to solve.In order to successfully finish a game, an agent needs to take certain actions in a particular order.The order of actions taken decides the future states of the entities involved in the game.</p>
<p>Self-supervised Training of Text-based RL Agent</p>
<p>Next, we deploy the TBRL agent pre-trained on ronments.We call the pre-trained TBRL agent STARLING.We expect that STARLING will outperform the vanilla TBRL agent by utilizing the skills learned using LLM and boosting the performance and generalization capabilities to reach the goal of the target environments: ScienceWorld, TWC, and Zork1.</p>
<p>TextWorld Commonsense Environment</p>
<p>TextWorld Commonsense environment evaluates the agent on commonsense reasoning about everyday objects such as toothbrush, dirty towel, etc.The environment, based on Textworld engine (Ct et al., 2019), includes three difficulty levels: easy, medium, and hard depending on the number of objects to find and the number of rooms to explore.Each difficulty level includes 5 training games and 5 evaluation games similar to the distribution of the training games3 for a total of 30 games with a batch size of 1 for this experiment.We train the STAR-LING agent on these 15 games for 100 episodes with a maximum of 50 steps.</p>
<p>Figure 5 shows the training curves of the three difficulty levels in the TWC environment.We can see that the STARLING agent gets a boost in performance both in the scores achieved and the moves taken compared to the vanilla TBRL.This shows that the pre-training step using LLM in STARLING leverages the basic skills mastered using the 75 generated games.Table 2 confirms our hypothesis that the pre-training step in STARLING improves the overall performance across different difficulty levels.</p>
<p>ZORK1 Environment</p>
<p>Zork1 is a human-made interactive fictional game environment and one of the earliest known textbased games created based on the underworld characters with dark themes and characters such as dungeon, grue, elvish sword, etc. Zork1 is one of the 33 interactive games released as a part of the Jericho game suite.Unlike TWC and ScienceWorld, Zork1 includes a diverse set of locations (over 200 locations), larger action space, sparser rewards, and longer trajectories, making it a challenging environment.</p>
<p>In order to evaluate the effect of pre-training in the Zork1 environment, we simplified the game with "killing troll" as a final goal (Zahavy et al., 2018).The agent needs to find the lantern and sword from the house, locate the hidden passageway to the underworld, light the lantern, and kill the troll.Without the lantern and sword, the agent entering the troll room reaches the failure state with negative rewards.In addition to these intermediate rewards, the game includes additional rewards when the agent collects a jewel-encrusted golden egg from the tall tree in the forest (+5 score) and a painting from the art gallery in the house (+4 score).We train the agents on 100 episodes with a maximum step of 100 steps per episode.</p>
<p>ScienceWorld Environment</p>
<p>ScienceWorld environment evaluates the science reasoning abilities of the TBRL agents.It consists of several tasks from topics such as change of state, biology, classification, etc.We choose all the 30 science-domain tasks from the themes.Each of these tasks contains 10  1400 variations of the game and are split into 50% training, 25% for evaluation set, and 25% for test set.We train the STARLING agent with 100k maximum steps on a single environment (with a maximum of 100 steps per game play) 4 .</p>
<p>Figure 7 shows the training curves for both the 4 Unlike in the "number of moves taken" metric in the pre-training results, the number of moves taken in the Science-World measures how long the agent survived without reaching the failure state.An agent may reach a failure state if it takes an action that results in abruptly ending the game such as pouring water on the floor for the boiling task, etc  scores received and moves taken on the 4 tasks.We can see that STARLING outperforms vanilla TBRL on all the tasks.We notice that the pretraining steps improved the performance of STAR-LING in the first few episodes of the classification task (find a living thing).On the other hand, pretraining games such as boiling water, cooking pasta, planting a tree, etc. may have influenced the performance of STARLING in the later episodes of tasks: change of state -boiling and grow a fruit, by adapting the learned skills during pre-training to the target environment.</p>
<p>Table 3 shows the performance comparison of GRU-based and LLM-based models against STAR-LING on the test variations of the ScienceWorld.We compare the STARLING against the stateof-the-art LLM-based models Behavior cloning (BC), Text Decision Transformers (TDT), CALM-GPT2, etc (Wang et al., 2023a;Ammanabrolu and Hausknecht;Yao et al., 2020).We notice that our pretraining strategy has generally improved the performance of STARLING in the majority of the tasks.We outperform both GRU-based and LLMbased models.It is worth noting that both BC and TDT use T5-base (11 billion parameters) initialized with Macaw, CALM uses GPT2 with 131 million parameters, whereas, GRU-based STARLING uses approx 200K parameters.</p>
<p>Discussion</p>
<p>Our experiments on three text-based game environments show that pre-training these agents using LLM-generated games as auxiliary tasks generally boosts the performance of the agent.We notice that most of the performance gain achieved in the first few episodes of the gameplay on some game instances (e.g., training curves in ScienceWorld tasks find a living thing, TWC Easy, and Hard, Zork1), whereas, STARLING adapts the basic skills learned during the pre-training to the target environment to improve the final scores (e.g., training curves in ScienceWorld tasks: change of state -boiling and grow a fruit).We notice that STARLING avoids the failure states better than vanilla TBRL as can be seen in Zork1 and ScienceWorld.Similarly, STARLING tends to choose valid actions from the action space more effectively than vanilla TBRL (for example, see Figure 12 supplementary for the sample trajectories from ScienceWorld taken by vanilla TBRL and STARLING within the first few episodes for the task: changes of state -boiling).</p>
<p>Since these pre-training games lack navigational complexity that elicits skills such as planning, we observe that STARLING tends to suffer in games that require navigational skills (e.g., example, in ScienceWorld task find a non-living thing, TWC hard difficulty and Zork1).Since the pre-training games involve fewer sequences of actions (short trajectories) to collect the reward and reach the final goal, STARLING struggles when the target environment has longer trajectories to reach the goal.</p>
<p>On the other hand, STARLING tends to collect bonus scores in Zork1 that are reachable within fewer steps instead of just chasing the larger rewarded states (e.g., see Figure 6 right).</p>
<p>4 Related Work</p>
<p>Text-based Games</p>
<p>Text-based games provide a challenging benchmark for RL agents interacting with the environment in natural language.The common challenges for text-based RL are partial observability, combinatorial action space, sparse rewards, longhorizon planning, etc.To circumvent some of these challenges, it is typical that the games are often manually curated to evaluate a specific set of skills such as commonsense reasoning (Murugesan et al., 2021a), knowledge graphs (Ammanabrolu and Hausknecht;Murugesan et al., 2021b), exploration strategies (Ct et al., 2019), etc. Environments such as TextWorld Commonsense (Murugesan et al., 2021a) measure simple commonsense reasoning based on the one-hop relationship between a pair of everyday objects such (apple and refrigerator, dirty towel and laundry basket), but lacks diversity and complexity to learn a general set of skills.Environments such as ScienceWorld (Wang et al., 2022) are often domain-specific environments that require domain knowledge to perform well in these environments.Jericho (Hausknecht et al., 2020), on the other hand, includes humangenerated games that require a complex set of skills to show any progress in the gameplay.These environments are manually created by humans with very limited automation in the variations of game generation by replacing similar or related objects, changing the layout/orientation of the environment, etc.Unlike these environments, STARLING provides an approach to leverage the skill generation capability of LLMs (Huang et al., 2022) to automatically generate text-based games based on the input game ideas with minimal human supervision.</p>
<p>These games are generated automatically by requesting a specific set of game-related facts from LLM in a slot-filling style text generation (Rakotonirina et al., 2022).</p>
<p>Self-supervised RL</p>
<p>Self-supervised RL has been a popular topic in vision-based RL and robotic environments (Sekar et al., 2020;Li et al., 2022).To the best of our knowledge, we are the first to utilize LLM to generate the games to train text-based RL agents5 .Previous works have utilized LLM for action generation (Yao et al., 2020), play interactive fictional game (Tsai et al., 2023), build a world model (Ammanabrolu et al., 2020;Wang et al., 2023b), etc.These works showed that using LLM to learn the underlying representation of text in the environment does not necessarily improve the performance (Wang et al., 2022).It is shown that novel exploration strategies and efficient RL algorithms along with the learning model similar to the one in Fig- ure 11 outperform all the other LLM-based agents (Tuyls et al., 2021).On the other hand, generalist agents have been recently explored to generalize across multiple environments (Reed et al., 2022), but the performance of these agents on a diverse set of environments is less convincing (Cobbe et al., 2019).</p>
<p>Conclusion</p>
<p>In this paper, we proposed a novel self-supervised training for a text-based reinforcement learning agent, STARLING, with the help of the generalized skill generation capability of large language models like GPT3.We generated a set of text-based games that require agents to learn basic skills such as cooking pasta, boiling water, etc., and utilize sequential decision-making over the modality of text.The proposed STARLING uses the GPT3 pretrained language model to automatically generate these games.This approach can be used to create additional games or adapted to build games for new domains with minimal human intervention.We showed that the STARLING agent pre-trained on the games generated by LLM outperforms vanilla TBRL.We evaluated STARLING on three environments: ScienceWorld, TWC, and Zork1.In all these environments, STARLING showed enhanced skills in the target environment.</p>
<p>Human participants were volunteers from a local High School that agreed to participate in this study.This may have introduced a bias into the human participant data since all participants were high school educated, from one geographic region, between the ages of 15 and 18, and volunteers.Many of these participants complete homework assignments and assessments often which may make their reasoning skills better than potential participants outside of school.In the future, testing human participants from various geographic locations, age groups, and levels of education may reduce bias.The STAR-LING currently requires human intervention and/or the Game Validator (from the glulx compiler) to build functioning games.We will continue to work on building an end-to-end version of the STAR-LING, that can take a game idea and turn it into an interactive fiction game without any human intervention.This would speed up development time so a larger set of games can be created.</p>
<p>Large language models such as ChatGPT have been developed recently with the ability to interact with users in a manner conversationally similar to the interactions found in interactive fiction games.From our experimentation, ChatGPT struggles to keep track of the states of all in-game objects and the pre-conditions necessary to use those actions (e.g.ChatGPT does not always require the player to turn on the stove before using it) as well as Inform7-based games.In addition, it suffers from small factual errors, and is hard to reproduce the same result, through this could also be seen as a benefit.Despite these challenges exploring the use of models such as ChatGPT to interact with agents shows promise in the future.</p>
<p>Ethical Impact</p>
<p>We asked the human participants to play the games generated by STARLING to evaluate the game's complexity and clarity.After receiving IRB approval from a local High School and informed consent from each of the 48 human participants, we asked the participants to play five randomly assigned games via iplayif.com,an online interactive fiction game player.Players received the goal of the game and the list of admissible actions.We collected the number of steps that each player took and the score received for each game via Google form.We did not collect any personal information or personally identifiable information as a part of this study.</p>
<p>Since we use large language models such as GPT3 to generate a set of text-based games, the bias and other fairness/ethical concerns that come with the LLM may unintentionally transfer to the pre-trained agent.Additional mitigation steps may be required to filter harmful contents from the generated response.</p>
<p>Reproducibility</p>
<p>As an effort to encourage further research in selfsupervised text-based RL, we plan to release the 100 games generated as a part of this paper, the source code for STARLING, script to generate game-related files based on a set of game ideas and LLM (including game templates and metadata) as an open-source project.between the game engine and TextWorld Gym environment.This wrapper ensures that the user to freely define any object or action type and the environment works with any Glulx compiled game file without dependence on the TextWorld-generated metadata to track the state of objects throughout the game.The wrapper does this by parsing the observation state returned by game engine after every step to generate certain data-points like admissible commands, current score, last action, number of steps taken and inventory required by the TextWorld Gym environment.</p>
<p>B Human Participants</p>
<p>Humans are considered to have exemplary compositional skill learning so comparing their performance to pre-trained agent's performance is valuable to validate generated games's difficulty and effectiveness as a pre-training task.After receiving IRB approval from a local High School and informed consent from each of the 48 human participants, we asked the participants to play five randomly assigned games via iplayif.com,an online interactive fiction game player.Players received the goal of the game and the list of admissible actions.We collected the number of steps that each player took and the score received for each game via Google form.6     {"libraries" : [ {"name": "measured liquid", "author": "Emily Short"}, { "name": "modern conveniences", "author": "Emily Short"}],</p>
<p>C Pretraining Game Statistics</p>
<p>"modules" : ["scoring"],</p>
<p>"room" : { "name": "home kitchen", "description": ""}, "custom entities" : [ "Food is a kind of thing.Food is usually edible.Food can be raw or cooked.Food is usually raw."],</p>
<p>"entities" : [{"name": "pot", "type": "container", "properties": ["portable", "open"],</p>
<p>"location": "in the cabinet"}, { "name": "pasta", "type": "food", "properties": "", "location": "in the refrigerator"}, {"name": "sauce", "type": "food", "properties": "", "location": "in the refrigerator"}],</p>
<p>"scoring" : [ {"condition": "taking the pot for the first time", "increment": "1"}, {"condition": "taking the pasta for the first time", "increment": "1"}, {"condition": "taking the sauce for the first time", "increment": "1"}],</p>
<p>"actions" : [{"name": "", "declaration": { "command": "cook [something] with [something]", "alias": "cooking it with", "applicable_to": "one carried thing and one thing"}, "prerequisites": [],</p>
<p>"constraints": [{"condition": "the noun is not a food", "prompt": "You can't cook that."},{ "condition": "the second noun is not a stove", "prompt": "You can't cook that."}],</p>
<p>"definition": { "tasks": [ "increase score by 1"],</p>
<p>"prompt": "You cooked the [noun] with the [second noun]."}, "postrequisties": [] } ],</p>
<p>"end_game" : { "condition": "4", "task": "end the story finally", "tasks": ["look", "inventory", "open cabinet", "take pot", "drop pot", "open fridge", "take pasta", "drop pasta", "turn on sink", "turn off sink", "fill pot with water", "turn on stove", "turn off stove", "boil water in pot", "cook pasta with stove"]}}</p>
<p>Figure 13: Example JSON file produced for cooking pasta game idea.The libraries, modules, and room sections were part of the setup, the custom entities and entities sections correspond to object creation, the actions correspond to the custom actions, and the scoring and end game correspond to the rewards sections of each game.The entities section describes names, types, and properties of entities present in the game.The actions section defines custom actions including their declaration, alias, and constraints not part of Inform7 by default.The end-game section defines the maximum score and the list of admissible actions that the user can take.</p>
<p>Figure 1 :
1
Figure 1: Architecture diagram for Self-supervised Text-based Reinforcement Learning using LLM (STARLING).</p>
<p>Figure 2 :
2
Figure 2: Workflow of the STARLING Game Generator using large language model (GPT3).</p>
<p>Figure 3 :
3
Figure 3: (A) GPT3 input prompt for cooking games with one action example.The actual prompt contains four action examples.(B) GPT3 output for cooking pasta game idea.GPT3 reliably outputs accurate and necessary game information very similar to the input.</p>
<p>Figure 4 :
4
Figure 4: Training curves for pre-training step of STARLING depicting the normalized scores (left) and number of moves taken (right) of text-based reinforcement learning agents.</p>
<p>Figure 5 :
5
Figure 5: Training curves for TWC easy (left), medium (middle), and hard (right) games depicting the normalized scores (top) and number of moves (bottom) of both vanilla TBRL and STARLING agents.</p>
<p>Figure 6 :
6
Figure 6: (left) Performance of TBRL agents on Zork1.(right) Sample trajectory from STARLING showing the bonus points of (+4 scores) for collecting the painting in the art gallery within the first few episodes.</p>
<p>Figure 6 (
6
Figure 6 (left) shows the performance of both the STARLING and vanilla TBRL on the Zork1 environment.As in TWC, we can see that the pretraining step boosts the performance of STARLING in the first few episodes compared to the vanilla TBRL agent.As in ScienceWorld, STARLING successfully avoids the failure state compared to the vanilla TBRL agent.</p>
<p>Figure 7 :
7
Figure 7: Training curves for ScienceWorld -Boil Substance (Matter), Find a living thing (Classification), Find a non-living thing (Classification), and Grow a fruit (Biology) games depicting the scores (left) and number of moves (right) of text-based reinforcement learning agents.</p>
<p>Figure 8 :
8
Figure 8: (Left) shows the overview of the proposed self-supervised text-based reinforcement learning with large language model.(Right) shows the sample text-only agent play through of cooking pasta game.Players must use the boil skill at the correct time to be successful.</p>
<p>Figure 9 :Figure 10 :
910
Figure 9: Architecture diagram for Self-supervised Text-based Reinforcement Learning using LLM (STARLING).</p>
<p>Figure 11 :
11
Figure 11: Vanilla Text-based RL agent used in this paper.</p>
<p>Figure 12 :
12
Figure 12: Sample Trajectories taken by STARLING and Vanilla TBRL for the task: change of state (boiling) task in ScienceWorld.</p>
<p>Table 1 :
1AgentsMean Normalized Score Mean Moves TakenRandom0.050  0.0150  0.0Pre-training0.72  0.06328.105  1.876Human1.000  0.0009.640  5.620
Performance of random and pre-trained agents on a set of 25 unseen pre-training games after training on 75 pre-training games over 100 episodes.Mean Norm.Score (higher is better, normalized with maximum score achievable per game) and Mean Moves Taken (to achieve the goal, lower is better).</p>
<p>Table 2 :
2
Murugesan et al., 2021a;f vanilla TBRL (without pre-tYao et al., 2020;LING on the test set with the three difficulty levels of Textworld Commonsense (TWC).All the scores and moves are averaged over 3 runs.2016;Murugesanetal., 2021a; Ammanabrolu and  Hausknecht;Yao et al., 2020; Atzeni et al., 2022)and use GRU-based Vanilla TBRL agent to evaluate our proposed approach (Figure11supplementary).</p>
<p>Table 3 :
3
Performance comparison of STARLING against other GRU-based and LLM-based agents on test variations from ScienceWorld.</p>
<p>*indicates the reliance on a valid action list during evaluation.</p>
<p>In this work, we define the skills based on the auxiliary task such as "boil <object>", "fill <container>", "cook <object>", etc. We assume that LLM such as GPT3 has the necessary knowledge to generate text-based games based on these basic skills.
https://en.wikipedia.org/wiki/Glulx
In addition to the 5 evaluation games, TWC includes 5 test games from out-of-distribution. Since these games require external knowledge such as ConceptNet(Speer et al., 2017), ATOMIC(Hwang et al., 2021) etc., we exclude them from our experiments.
Several LLM-based interactive fictional environments for entertainment purposes exist(AID, 2019).
No personal information was collected as a part of this study.
A Additional DetailsIn order to generate games that require composing previously learned skills, we take inspiration from household chores, cooking, and maintenance tasks.We generate 100 game ideas and use the STAR-LING to generate a set of 100 games.We choose the game ideas carefully for the learning agent to utilize similar skills (ex.baking, mixing, spreading, using a hammer, etc) in new situations therefore forcing the agent to generalize skills and compose them with other skills.For example, while cooking pasta, an agent must learn how to boil water which is a skill that can be applied for a related game idea "brewing tea".LLMs such as GPT-3 are prone to making factual and grammatical errors, in addition to violating the specified format.We check for any errors in the generated game(s) using a Game Validator as a part of glulx compiler which uses depth first search (DFS) to explore all the possible trajectories in the game.To correct for minor errors and inconsistencies in each game, information from GPT-3 can also be optionally verified by the human authors in the JSON file.We found that, in cases when the created game has errors, restarting the game generation a few times usually results in a playable game.A.1 From GPT3 output to game JSONWe extract the information from GPT-3 using Python simple regular expression rules by first splitting the output into three sections: task sequence (ex.Open cabinet, take pot), objects (ex.Pot), and actions (ex.Fill pot with water).We add the task sequence to the list of admissible actions the player could execute within the game.We store the objects internally with a type, a location, a name, and a set of properties.We further split the actions section into default actions and custom actions which are actions native and non-native to inform7 respectively.Similar to the objects, we store each custom action internally with a name, a declaration, a definition, a set of constraints, a set of prerequisites, and a set of post-requisites.A.2 Inform7 Code GeneratorWe wrote a simple script based on the JSON structure and the action templates to generate the In-form7 Code for a given game idea.This script along with the other code &amp; data will be shared to the public in the near future.A.3 Modifying TextWorld Gym for STARLINGOpenAI Gym is a general reinforcement learning framework that acts as an interface between RL agents and Inform7-based STARLING game engine(Brockman et al., 2016).Gym connects environments with agents by using a monitor to keep track of every step, state of the game, the final score of agents, and the sample complexity or the amount of time an agent takes to learn.Most default environments in Gym support a continuous or discrete action space although interactive fiction games require combinatorial action spaces in natural language(Hausknecht et al., 2020).The TextWorld Gym customized the OpenAI Gym for interactive fiction games.In this work, we repurpose the custom Gym environment created for TextWorld environment with Inform7 object and action types.TextWorld's Gym environment only supports TextWorld-generated games which includes a Glulx compiled game file and a TextWorld-generated JSON file with game metadata defined in proprietary TextWorld classes.This restricted our ability to create games with objects and actions previously undefined in TextWorld environment.These objects and actions must be defined according to TextWorld's grammar and logic rules.This is a time consuming process and is prone to many errors.The goal of the STARLING Game Generator is to allow users to automate the game creation using LLM, and most importantly, create games without learning a new programming language or familiarizing themselves with any grammar rules.To get rid of these restrictions, an entirely new wrapper was created which acted as an interface
Ai dungeon. </p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. </p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>Keerthiram Murugesan, and Mrinmaya Sachan. 2022. Case-based reasoning for better generalization in text-adventure games. Mattia Atzeni, Shehzaad Dhuliawala, ICLR 2022</p>
<p>Openai gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016</p>
<p>Language models are fewshot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, . . Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Learning phrase representations using rnn encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>Quantifying generalization in reinforcement learning. Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, John Schulman, International Conference on Machine Learning. PMLR2019</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Ct, Akos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Stockholm, SwedenSpringer2019. 2018. July 13. 2018Revised Selected Papers 7</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Ct, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics2016</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>(comet-) atomic 2020: on symbolic and neural commonsense knowledge graphs. Jena D Hwang, Chandra Bhagavatula, Le Ronan, Jeff Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>A systematic survey of text worlds as embodied natural language environments. Peter Jansen, 10.18653/v1/2022.wordplay-1.1Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022). the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022)Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Does self-supervised learning really improve reinforcement learning from pixels?. Xiang Li, Jinghuan Shang, Srijan Das, Michael Ryoo, Advances in Neural Information Processing Systems. 202235</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, AAAI. 2021a</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2021b2</p>
<p>. Graham Nelson, 2006. Inform7</p>
<p>Can discrete information extraction prompts generalize across language models?. Nathanal Carraz Rakotonirina, Roberto Dessi, Fabio Petroni, Sebastian Riedel, Marco Baroni, The Eleventh International Conference on Learning Representations. 2022</p>
<p>. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gmez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimnez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, A generalist agent. Transactions on Machine Learning Research. 2022</p>
<p>Planning to explore via self-supervised world models. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak, International Conference on Machine Learning. PMLR2020</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Can large language models play text games well? current state-of-the-art and open questions. Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, Hongyuan Mei, arXiv:2304.028682023arXiv preprint</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, Sham M Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2021</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Ct, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Behavior cloned transformers are neurosymbolic reasoners. Ruoyao Wang, Peter Jansen, Marc-Alexandre Ct, Prithviraj Ammanabrolu, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational Linguistics2023a</p>
<p>Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre Ct, Peter Jansen, Bytesized32: A corpus and challenge task for generating task-specific world models expressed as text games. arXiv e-prints. 2023b2305</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, Advances in neural information processing systems. 201831</p>            </div>
        </div>

    </div>
</body>
</html>
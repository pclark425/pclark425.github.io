<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-809 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-809</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-809</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-17d648a0a8ec8a9b30c7bedccbc38b3ed0a5cdda</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/17d648a0a8ec8a9b30c7bedccbc38b3ed0a5cdda" target="_blank">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Rational meaning construction is proposed, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference and extends to integrate cognitively-motivated symbolic modules to provide a unified commonsense thinking interface from language.</p>
                <p><strong>Paper Abstract:</strong> How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e809.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e809.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rational Meaning Construction (PLoT+LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rational Meaning Construction: mapping natural language to a Probabilistic Language of Thought (PLoT) with LLM-based meaning functions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that translates natural language into probabilistic programs (a Probabilistic Language of Thought) using a language-to-code LLM, and performs probabilistic inference (belief-state/posterior sampling) in the resulting symbolic world model to support reasoning and question answering from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Rational Meaning Construction (PLoT implemented in Church, translation via Codex)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper implements a concrete architecture in which (1) a prewritten probabilistic generative world model (the PLoT) is expressed in the Church probabilistic programming language, (2) a language-to-code LLM (Codex) is few-shot prompted with the model and example translations to translate natural-language observations and questions into Church expressions (condition and query statements, and define constructs), and (3) Church performs probabilistic inference (rejection-sampling-based simulation) to produce posterior belief distributions over latent state variables and query outcomes. The system supports an interactive REPL-style dialogue that interleaves conditions and queries, defines new symbolic concepts, and returns full posterior distributions (belief states) over quantities of interest rather than single text responses.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic programs (Church PPL) / Probabilistic Language of Thought (PLoT)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>World models are implemented as Church probabilistic generative programs that sample latent variables (e.g., player strengths and laziness in the tug-of-war domain; discrete genealogical events in the kinship domain) and define deterministic/stochastic mechanisms that generate observable events. Observations are encoded as conditioning statements that constrain the generative model; questions are encoded as query expressions evaluated via Monte Carlo simulation. The representation is explicitly probabilistic (captures state uncertainty via posterior distributions) and symbolic (named entities, predicates, derived predicates and higher-order functions allow logical composition).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Meaning function / translation from natural language to probabilistic program expressions (condition/query/define); amortized pragmatics (mapping vague language to numeric thresholds/parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Codex (GPT-3-based language-to-code model)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State/posterior uncertainty (belief over latent world states) and uncertainty in mapping language to program parameters (amortized pragmatic uncertainty from the LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Probabilistic generative modeling with posterior sampling (Church rejection sampling / Monte Carlo simulation) for state uncertainty; LLM supplies conditional program forms and numeric thresholds which are treated as probabilistic conditions when used in inference (i.e., implicit amortized uncertainty through choice of parameter values); the paper explicitly uses Monte Carlo sampling (1000 samples in examples) to form belief distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Toy text domains: Tug-of-war dialogues and Kinship dialogue/world-building (language-only interactive scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Interactive, dialog-style natural-language exchanges about abstract toy domains implemented as probabilistic generative models (e.g., Bayesian tug-of-war tournament with latent strengths/laziness; kinship family-tree generation). These are not standardized text-game benchmarks (like TextWorld) but are text-based environments in which utterances provide observations/queries that are grounded into a symbolic probabilistic world model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This work demonstrates that: (1) LLMs trained on code (Codex) can serve as broad-coverage, context-sensitive meaning functions that translate open-ended natural language into executable probabilistic program statements tied to a symbolic world model; (2) by offloading reasoning to a probabilistic symbolic world model (Church PLoT), the system returns rich posterior belief states (distributions) and supports defeasible, context-sensitive inferences that align with human intuitions; (3) the approach naturally integrates symbolic compositionality (derived predicates, definitions) with probabilistic uncertainty (Monte Carlo posterior sampling), and amortizes pragmatic choices (e.g., numeric thresholds) through the LLM; and (4) while the paper demonstrates the approach on toy text domains and shows LLMs can choose reasonable parameterizations, scaling to richer planning or real text-game benchmarks and tighter LLM uncertainty integration are left as open engineering and research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Church: A Language for Generative Models <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-809",
    "paper_id": "paper-17d648a0a8ec8a9b30c7bedccbc38b3ed0a5cdda",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "Rational Meaning Construction (PLoT+LLM)",
            "name_full": "Rational Meaning Construction: mapping natural language to a Probabilistic Language of Thought (PLoT) with LLM-based meaning functions",
            "brief_description": "A framework that translates natural language into probabilistic programs (a Probabilistic Language of Thought) using a language-to-code LLM, and performs probabilistic inference (belief-state/posterior sampling) in the resulting symbolic world model to support reasoning and question answering from text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Rational Meaning Construction (PLoT implemented in Church, translation via Codex)",
            "system_description": "The paper implements a concrete architecture in which (1) a prewritten probabilistic generative world model (the PLoT) is expressed in the Church probabilistic programming language, (2) a language-to-code LLM (Codex) is few-shot prompted with the model and example translations to translate natural-language observations and questions into Church expressions (condition and query statements, and define constructs), and (3) Church performs probabilistic inference (rejection-sampling-based simulation) to produce posterior belief distributions over latent state variables and query outcomes. The system supports an interactive REPL-style dialogue that interleaves conditions and queries, defines new symbolic concepts, and returns full posterior distributions (belief states) over quantities of interest rather than single text responses.",
            "world_model_type": "Probabilistic programs (Church PPL) / Probabilistic Language of Thought (PLoT)",
            "world_model_description": "World models are implemented as Church probabilistic generative programs that sample latent variables (e.g., player strengths and laziness in the tug-of-war domain; discrete genealogical events in the kinship domain) and define deterministic/stochastic mechanisms that generate observable events. Observations are encoded as conditioning statements that constrain the generative model; questions are encoded as query expressions evaluated via Monte Carlo simulation. The representation is explicitly probabilistic (captures state uncertainty via posterior distributions) and symbolic (named entities, predicates, derived predicates and higher-order functions allow logical composition).",
            "uses_llm": true,
            "llm_role": "Meaning function / translation from natural language to probabilistic program expressions (condition/query/define); amortized pragmatics (mapping vague language to numeric thresholds/parameters)",
            "llm_model_name": "Codex (GPT-3-based language-to-code model)",
            "uncertainty_modeling": true,
            "uncertainty_type": "State/posterior uncertainty (belief over latent world states) and uncertainty in mapping language to program parameters (amortized pragmatic uncertainty from the LLM)",
            "uncertainty_method": "Probabilistic generative modeling with posterior sampling (Church rejection sampling / Monte Carlo simulation) for state uncertainty; LLM supplies conditional program forms and numeric thresholds which are treated as probabilistic conditions when used in inference (i.e., implicit amortized uncertainty through choice of parameter values); the paper explicitly uses Monte Carlo sampling (1000 samples in examples) to form belief distributions.",
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": "Toy text domains: Tug-of-war dialogues and Kinship dialogue/world-building (language-only interactive scenarios)",
            "text_environment_description": "Interactive, dialog-style natural-language exchanges about abstract toy domains implemented as probabilistic generative models (e.g., Bayesian tug-of-war tournament with latent strengths/laziness; kinship family-tree generation). These are not standardized text-game benchmarks (like TextWorld) but are text-based environments in which utterances provide observations/queries that are grounded into a symbolic probabilistic world model.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": "",
            "has_ablation_uncertainty": false,
            "ablation_results": "",
            "key_findings": "This work demonstrates that: (1) LLMs trained on code (Codex) can serve as broad-coverage, context-sensitive meaning functions that translate open-ended natural language into executable probabilistic program statements tied to a symbolic world model; (2) by offloading reasoning to a probabilistic symbolic world model (Church PLoT), the system returns rich posterior belief states (distributions) and supports defeasible, context-sensitive inferences that align with human intuitions; (3) the approach naturally integrates symbolic compositionality (derived predicates, definitions) with probabilistic uncertainty (Monte Carlo posterior sampling), and amortizes pragmatic choices (e.g., numeric thresholds) through the LLM; and (4) while the paper demonstrates the approach on toy text domains and shows LLMs can choose reasonable parameterizations, scaling to richer planning or real text-game benchmarks and tighter LLM uncertainty integration are left as open engineering and research directions.",
            "uuid": "e809.0",
            "source_info": {
                "paper_title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Church: A Language for Generative Models",
            "rating": 2,
            "sanitized_title": "church_a_language_for_generative_models"
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        }
    ],
    "cost": 0.0097565,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>From Word Models to World Models: <br> Translating from Natural Language to the Probabilistic Language of Thought</h1>
<p>Lionel Wong ${ }^{1 \star}$, Gabriel Grand ${ }^{1 \star}$, Alexander K. Lew ${ }^{1}$, Noah D. Goodman ${ }^{2}$, Vikash K. Mansinghka ${ }^{1}$, Jacob Andreas ${ }^{1}$, Joshua B. Tenenbaum ${ }^{1}$<br>*Equal contribution.<br>${ }^{1}$ MIT, ${ }^{2}$ Stanford</p>
<h4>Abstract</h4>
<p>How does language inform our downstream thinking? In particular, how do humans make meaning from language - and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT) - a general-purpose symbolic substrate for probabilistic, generative world modeling. Our architecture integrates two powerful computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for flexible commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework in action through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning about agents and their plans. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and goal-directed planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will help to situate contemporary developments in LLMs within a broader cognitive picture of human language and intelligence, providing a roadmap towards AI systems that synthesize the insights of both modern and classical computational perspectives.</p>
<h2>1 Introduction</h2>
<p>Language expresses the vast internal landscape of our thoughts. We use language to convey what we believe, what we are uncertain about, and what we do not know. We talk about what we see in the world around us, and what we imagine in real or wholly hypothetical futures. We discuss what we want and what we plan to do, and dissect what others want and what we think they will do. We build and pass on new bodies of knowledge in language - we ask questions and offer explanations, give commands and instructions, and propose and refute theories. Some of these ideas can be expressed in part through other means. But language stands apart for its flexibility and breadth, and its seeming proximity to our thoughts.</p>
<p>What is language? How does language get its meaning, and when should we say that a person or machine knows, understands, and can use it? What is the relationship between language and the rest of general cognition-what allows language to inform and support so much of thought? This paper focuses on these questions as they relate to human language and thought, in computational terms. What integrated cognitive theory can model how language relates to the other core systems of human cognition? If we seek to build AI systems that emulate how humans talk and think, what architecture can integrate language robustly into systems that support the full scope of our thought?</p>
<p>Theories of cognition have long considered human language and thinking to be deeply related, but fundamentally distinct. Thinking, in many traditional cognitive theories, revolves around goal-directed world modeling, inference, and decision making-constructing mental models of the world that reflect prior beliefs, can be updated from new observations, and support rational prediction and decision making toward's one's goals (Craik, 1967; Gentner \&amp; Stevens, 2014; Johnson-Laird, 1980, 1989; Lake, Ullman, Tenenbaum, \&amp; Gershman, 2017; Morgan, 1999; Nersessian et al., 2010). Language, in contrast, centers around communicating these thoughts to others, and receiving their thoughts in turn. In most linguistic theories, human languages are mappings between the internal representations of thought and an externalizable symbol system, which might be phonemes, signs, or glyphs (Frege, 1892; Heim \&amp; Kratzer, 1998; Lewis, 1976). To produce language is to map thoughts into these external symbols, and to understand language is to transduce from these external symbols back into the representations of thought.</p>
<p>The theoretical distinction between language and thought rests on multiple intersecting lines of evidence. Prior to learning language, infants are born equipped with a powerful toolkit for modeling and thinking about the world, including an understanding of physical objects and events, and the goals and actions of agents (Spelke, 2022; Spelke \&amp; Kinzler, 2007), and general abilities for learning statistics and structure (Saffran, Senghas, \&amp; Trueswell, 2001; Xu et al., 2021). Building on these foundations, children acquire language from relatively sparse input data, rapidly generalizing beyond the utterances they hear to produce and understand entirely new ones (Bloom, 2002; L. Gleitman, 1990; L. R. Gleitman, Cassidy, Nappa, Papafragou, \&amp; Trueswell, 2005; Landauer \&amp; Dumais, 1997; Pinker, 1998; L. Smith \&amp; Yu, 2008); they then use language to acquire new concepts they would not get merely from direct experience (Carey, 2009; Gopnik, 1996; Wellman \&amp; Gelman, 1992). Language and thought also appear to operate in distinct but interacting brain systems: neuroimaging and neurological studies reveal a "language network" specialized for processing sentences, functionally and anatomically separate from but closely connected to brain networks supporting other aspects of general cognition (Fedorenko \&amp; Varley, 2016; Mahowald et al., 2023).
'These empirical findings have shaped decades of computational models in cognitive science and AI. To model the expressiveness of human cognition, an influential computational paradigm suggests that humans compose and execute mental programs in an internal language of thought (Fodor, 1975), a structured symbolic substrate for representing conceptual knowledge that provides a general interface to algorithms for problem solving and reasoning. These symbolic systems are not merely logic engines; they support our probabilistic inferences, and rich intuitive simulations (Goodman, Tenenbaum, \&amp; Gerstenberg, 2014; Oaksford \&amp; Chater, 2007; Russell \&amp; Norvig, 2021). This paradigm underlies many of the success stories in cognitive science and related applications in AI. It has influenced models that capture how people draw causal and explanatory inferences about facts and observations (Pearl, 1988; Pearl et al., 2000), learn and generalize concepts from few examples (Lake et al., 2017); plan actions over long time horizons and under complex conditions (Kaelbling \&amp; Lozano-PÃ©rez, 2013; Russell \&amp; Norvig, 2021); imagine and predict the physical world (Battaglia, Hamrick, \&amp; Tenenbaum, 2013; Ullman, Spelke, Battaglia, \&amp; Tenenbaum, 2017); and reason about other agents with their own beliefs and goals (C. Baker, Saxe, \&amp; Tenenbaum, 2011). Within linguistics and natural language processing, in turn, this paradigm underlies semantic parsing systems designed to map from human language into symbolic computational representations. It has yielded AI systems that could follow instructions (Tellex et al., 2011) and answer natural language queries with respect to structured knowledge representations (Klein \&amp; Manning, 2003; Liang, 2016; Steedman, 2011; Y. W. Wong \&amp; Mooney, 2007); as well as cognitive models that capture how human children learn the grammar and meaning of expressions in their native language (Abend, Kwiatkowski, Smith, Goldwater, \&amp; Steedman, 2017; Chater \&amp; Manning, 2006; Frank, Goodman, \&amp; Tenenbaum, 2009; Gauthier, Levy, \&amp; Tenenbaum, 2018; Goldwater, Griffiths, \&amp; Johnson, 2009; Perfors, Tenenbaum, \&amp; Regier, 2011; Piantadosi, Tenenbaum, \&amp; Goodman, 2012).</p>
<p>Despite this progress, however, modular and symbolic models of language and thought have been dogged by persistent critiques of their scalability and scope. Cognitive and AI researchers over the years have carved off specific domains of world knowledge, constructing bespoke representations to model them without a general account of whether they would generalize to all of human knowledge, or how they could be scalably learned. Semantic parsing systems inherited these critiques, and faced additional challenges in implementing the mapping from sentences into symbolic representations. These mapping functions were either hand-engineered or learned from strong supervision on specific domains of language, limiting them to brittle, imperfect models of the breadth and complexity of real human discourse.</p>
<p>In just the last few years, a serious challenge has emerged to the traditional view of language and thought as distinct but interacting components of the mind, each modeled using structured representations. Large language models (LLMs) use a new generation of attention-based deep neural networks to learn the probabilistic distributions of words from vast datasets of human language, generally training on orders of magnitude more data than a human encounters in their lifetime (Bommasani et al., 2021; T. B. Brown et al., 2020; OpenAI, 2023c; Rae et al., 2021; Vaswani et al., 2017). The underlying computational objective that drives these models is not itself new. LLMs follow in the tradition of distributional approaches to discovering structure in language (Firth, 1957; Harris, 1954; Osgood, 1952), which seek to extract representations of meaning from statistical patterns in how words are used in context (Dumais et al., 2004; Griffiths, Steyvers, \&amp; Tenenbaum, 2007; Mikolov, Sutskever, Chen, Corrado, \&amp; Dean, 2013; Sahlgren, 2008). What is new, however, is the scale and scope of today's distributional vision, which has expanded in stages. A first generation of LLMs, trained specifically to predict words in context, produced such fluent language that they challenged traditional symbolic approaches to modeling language (Devlin, Chang, Lee, \&amp; Toutanova, 2018; Peters et al., 1802; Radford et al., 2019). Their qualitative success, as well as internal representational probes, suggested that linguistic structures sufficient for grammatically coherent language could be learned entirely from modeling the statistics of words (Piantadosi, 2023; Tenney, Das, \&amp; Pavlick, 2019). By scaling to even larger datasets and neural networks, LLMs appeared to learn not only the structure of language, but capacities for some kinds of thinking; they could learn new words in context, and extract patterns in language from a few examples that they could generalize locally to similar cases (T. B. Brown et al., 2020). The most recent LLMs have been trained not only to model the statistics of language but explicitly to reason, with targeted supervision on instruction following, writing code, and other forms of human dialog and feedback in conversational contexts (Chen et al., 2021; OpenAI, 2023a, 2023c; Ouyang et al., 2022). They produce such fluent language on a wide variety of tasks that many have begun to ask whether merely more training of this sort, with increasing scale, could learn representations sufficient for general intelligence (Bubeck et al., 2023). Proponents of the most extreme "scaling hypothesis" have argued that because language is used to express so much of human thought, a sufficiently large and performant predictive language model would effectively have to construct an internal model of all of cognition (Branwen, 2022).</p>
<p>This theoretical vision has sparked both excitement and controversy, but proponents and critics agree that it raises its own questions about its long-term scalability-most significantly, what will be required to close the outstanding gaps between today's LLMs and general cognitive models that reason systematically and consistently about the language they receive or produce. Current LLMs can produce impressive results on a set of linguistic inputs and then fail completely on others that make trivial alterations to the same underlying domain (Ullman, 2023); they mix confident answers to complex questions with equally confident, hallucinated language that does not reflect a consistent, calibrated notion of truth or belief (Bubeck et al., 2023; OpenAI, 2023c). These issues make it difficult to evaluate whether LLMs have acquired cognitive capacities such as social reasoning and theory of mind (Ullman, 2023), or to compare different kinds of world modeling and planning tasks (Valmeekam, Sreedharan, Marquez, Olmo, \&amp; Kambhampati, 2023). One approach to solving these problems is through additional data. Perhaps fully robust, systematic reasoning will finally emerge if models are trained on still more language, or supervised more explicitly on data from complex reasoning tasks. This scaling route raises practical questions about whether it will be possible to acquire enough data to train such a model, as well as theoretical questions whether more data and more parameters alone will in fact yield robust systems for thought. Another strategy in recent work seeks to build more robust cognitive capacities by augmenting LLMs with various external tools for structured representation and symbolic reasoning, such as calculators (Cobbe et al., 2021), logic engines (Weir \&amp; Van Durme, 2022), databases (Alon et al., 2022; Borgeaud et al., 2022; Izacard et al., 2022; Thoppilan et al., 2022), physics simulators (R. Liu et al., 2022), planners (B. Liu et al., 2023), and APIs for executing arbitrary code (Karpas et al., 2022; OpenAI, 2023c; Schick et al., 2023). But these new hybrid approaches resurrect many of the same long-term scalablity challenges that confronted earlier semantic parsing and knowledge representation systems, by designing a menagerie of bespoke representations and tools without a broader account of how they will scale towards general models of language and thought.</p>
<p>In this paper, we consider a different approach to integrating the strengths of modern language models and classic symbolic architectures, one that draws on but also runs counter to recent trends in AI, in a sense flipping these scaling questions on their head. Instead of trying to turn models trained to predict language into models that might genuinely think-filling each gap in reasoning we discover through yet more</p>
<h1>Approaches to language-informed thinking</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Human language understanding supports flexible inferences in a process we term language-informed thinking. Computational approaches to language-informed thinking sit on a neurosymbolic continuum: On one side, classical symbolic models (top right) yield systematic, structured inferences, but are typically limited to narrow linguistic domains and often require hand-engineering. On the other side, large language models (LLMs; top left) achieve remarkable facility with open-domain natural language, but struggle to ground reasoning in a consistent world state that supports coherent inferences, predictions and plans. Our rational meaning construction framework decomposes language-informed thinking into two modules: (1) A meaning function translates natural language into probabilistic programming language (PPL) statements that represent linguistic meaning with respect to a symbolic world model. (2) An inference function computes probabilities over the space of possible worlds consistent with and conditioned on information in the language. In the rest of this paper, we illustrate how our framework can combine the strengths of LLMs and PPLs, affording both broad coverage of natural language and a principled treatment of reasoning about uncertain events, outcomes, and scenarios.
data, new kinds of language training or linguistic prompting tricks, or by plugging in yet another external tool-we ask: what are the prospects for a unifying computational framework guided by the study of thought and language in the human mind and brain, as well as what we have learned from multiple eras of AI? Can we build intelligent architectures that use, learn and understand language as people do, informed by neuroscience constraints and developmental trajectories? That is, can we build models in which language is learned efficiently within one relatively small, modular computational system, which interfaces generally with other systems dedicated to robust world modeling and reasoning? What architecture lets language build on pre-existing capacities for symbolic world modeling and inference, while also allowing linguistic meanings and world knowledge to scaffold and bootstrap each other, as a learner's experiences and competences grow?</p>
<p>This paper attempts to show what such a model might look like-and how it can build theoretically and practically on the insights from both classical paradigms for language and thought, and the recent successes of statistical learning made by large language models. We propose a framework for intelligent computational architectures that reason about and learn from language, but we begin with a proposal for what it means to think. As in the traditional cognitive view, thinking at its core is constructing general-purpose representations for modeling the entities and events in the world, sufficient to support rational, coherent inferences under</p>
<p>uncertainty and planning actions that achieve our goals. We then consider how language relates to this architecture to support language-informed thinking-how language sets up world modeling and inference, to guide, constrain, and drive our downstream thought, and grow new thinking capacities.</p>
<p>Our proposal, which we call rational meaning construction, rests on the integration of two computational components, each which we suggest can be instantiated using modern computational tools-a probabilistic language of thought for constructing structured models of arbitrary situations, which supports supports coherent belief updating and inferences over them; and a general mechanism for taking natural language and constructing meaning from it, represented as distributions over expressions in this language of thought (Fig. 1). We propose that probabilistic programs can formally instantiate the first component. They offer a structured representation for expressing novel situations and arbitrary problems with respect to a meaningful model over possible world states, a coherent notion of conditional belief updating, and a systematic framework for inferences with respect to queries and goals. We propose, in turn, that meaning construction can be modeled as translation from utterances in language to expressions in a general probabilistic programming language. Theoretical and empirical results have long suggested that human languages implement locally compositional, efficiently learnable mappings between symbolic representations of thought and external symbol systems. We therefore propose that code-trained large language models can be viewed as in-principle implementations of broad, context-sensitive, and resource-rational meaning functions, in that they can be used to efficiently infer distributions between language and programs from stored, prior patterns in the background distribution of language and code. By integrating these two components, we propose that this paradigm suggests a general framework by which language can meaningfully relate to many fundamental aspects of cognition, modeling how we might condition on language to systematically update our beliefs, pose new questions and goals in language, and convey structured background information or even define new relevant concepts about a situation or about the world.</p>
<p>In Section 2, we give an overview of this framework, describing the overall structure and more detailed rationale behind the computational components we build on in the remainder of this paper. We then describe a concrete but minimal implementation of this framework using contemporary probabilistic programming and language modeling tools, intended to demonstrate the basic computational components of this approach and elucidate the scope and scalability of the broader proposal.</p>
<p>Given this general paradigm, we first illustrate the potential breadth of this approach for integrating meaning construction and reasoning, showing how it might address a core set of computational and cognitive domains that we communicate about in language (Fig. 2). Each of these examples uses minimal pedagogical examples intended to suggest how this approach integrates language with important bodies of work from computational cognitive science and artificial intelligence. We first show how this framework can condition on language in order to describe and reason about uncertain situations with respect to an ongoing discourse (Section 2.2), then show how this approach can be extended to reason about relational systems (Section 3.1), physical and perceptual scenes (Section 3.2), and social situations involving agents with goals and plans (Section 3.3).</p>
<p>We then turn to how this approach might begin to address core scalability challenges that confront traditional approaches to modeling thinking as symbol processing, whether logical or probabilistic. In Section 4, we show how language can support growing knowledge autonomously, without hand engineering, by using the rational meaning construction framework to construct a broad range of new concepts in existing models and even whole new world models, which in turn support coherent downstream reasoning.</p>
<p>Ultimately, this paper is a prospective one, and the examples presented here are intended to convey a sufficiently concrete proposal to suggest avenues for future work. In Section 5, we outline what we see as some of the most significant open questions and future directions raised by this framework. These include theoretical questions that relate our approach to classical models of language, open cognitive directions for extending this approach to model language acquisition and production, and important engineering directions necessary for scaling inference, robust translation, and learning under this general paradigm. Finally, in Section 6, we conclude by looking ahead to the longer-term implications of this proposal for modeling intelligent systems that use, understand, and think about language as we do.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Understanding language in four domains of reasoning that form the core of this paper. Probabilistic reasoning requires integrating sparse evidence to predict the outcomes of uncertain events, like the winners of tug-of-war matches. Relational reasoning involves maintaining and updating coherent beliefs about structured domains, like family trees, based on relational information. Perceptual and physical reasoning links language to our sensory and intuitive physical knowledge of objects in the external world, such as kitchen items on a tabletop. Social reasoning involves reasoning about the minds of other intelligent agents, such as how their goals, preferences, and circumstances shape their actions as they navigate in the world. Across all the domains, we present a unified framework that translates language into code in a probabilistic programming language to facilitate human-like reasoning.</p>
<h1>2 Overview of the key ideas</h1>
<p>The central goal of this paper is to propose a new computational framework, rational meaning construction, which relates language to thought. This framework licenses a concrete class of computational architectures for building intelligent systems that use language, which we propose can be implemented using modern AI tools. In this section, we briefly overview the key ideas that form the basis of this proposal. We draw on three observations from a rational, probabilistic perspective on biological intelligence and human language:</p>
<p>A rational perspective on intelligent thought. Biological intelligence encompasses many computational capacities. The foundational notion of thought we focus on here centers on rational inference and decision making in service of one's goals (Anderson, 1990; Chater \&amp; Oaksford, 1999). Under this perspective, thought comprises systems for modeling the world. These internal world models allow us to infer the particulars of a situation from whatever information is at hand, evaluate alternative world states and imagine possible future ones, and decide on actions that might bring one towards valuable future states in the world. Following extensive work in computational cognitive science, we view the world models that support biological intelligence as structured and probabilistic (Goodman et al., 2014; Griffiths, Chater, Kemp, Perfors, \&amp; Tenenbaum, 2010; Lake et al., 2017), designed to integrate the noisy evidence an agent receives into causal, explanatory models that allow them to maintain coherent beliefs about the world and generalizably infer consistent, useful predictions and plans. This basic, underlying view of intelligent thought draws on empirical evidence from essentially every species with a brain, from bees (Biernaskie, Walker, \&amp; Gegear, 2009; R. F. Wang \&amp; Spelke, 2002), to zebrafish Bolton et al. (2019); R. E. Johnson et al. (2020), mice (English, Nejad, Sommerfelt, Yanik, \&amp; von der Behrens, 2023), birds (Isomura, Parr, \&amp; Friston, 2019), and primates (Khalvati, Kiani, \&amp; Rao, 2021). Informally, a rational view of thought can be summarized as the ability to solve useful problems given our internal models of the world, ranging from navigation and foraging to physical prediction and social reasoning. Against this overarching picture of thought, human intelligence further stands out for its flexibility and expressiveness. We invent our own problems along with new approaches to solving them, rather than sticking to a limited set of largely innate goals and strategies (Tomasello, 2022). A few other species, non-human primates, dolphins, and some birds, are creative problem-solvers and problem-creators, but none come close to the range of goals humans can adopt (Chu \&amp; Schulz, 2023). Uniquely in the natural world, humans think about and come to understand problems far beyond the narrow range necessary for our immediate survival, considering goals and questions that draw on abstract, culturally constructed, and even entirely hypothetical systems for modeling and conceptualizing the world (Dennett, 2017).</p>
<p>A rational perspective on language. As with thought, language also encompasses many systems and capacities. This paper focuses on the class of problems we refer to as language-informed thinking, the general means by which language informs the inferences and decisions of an intelligent agent. We take a broadly rational perspective on language-we consider language to be a system of goal-directed actions for externalizing and communicating thoughts to other intelligent beings (Chater \&amp; Manning, 2006; Gibson, 2014; Goodman \&amp; Frank, 2016). In this context, we frame the problem of deriving meaning as inferring the mappings between a language's system of external communicative signals into the representations of rational thought. It is worth highlighting that thought does not require language and is distinct from language in the human brain (Mahowald et al., 2023). Non-human species, and pre-verbal infants (Spelke, 2022), are clearly capable of modeling the world towards their inferences and goals without language. But for humans, language clearly plays a profound role in determining the problems we think about, and how we think about them. Our natural languages allow us to communicate an extraordinarily broad range of our thoughts about the problems we pose and solve, including our abstract and general world knowledge, our specific beliefs about a situation, the particular questions or goals we have or want to pose to others, and our approaches to reasoning about them.</p>
<p>A resource-rational perspective on language and thought. Finally, our integrated computational approach to language and thought builds on extensive evidence that humans are resource-rational thinkersunder finite constraints of time and memory, we rationally allocate computational resources in order to make useful inferences and plans (S. J. Gershman, Horvitz, \&amp; Tenenbaum, 2015; Lieder \&amp; Griffiths, 2019). Resource rational agents amortize computational effort across prior experience and problems, storing and</p>
<p>reusing prior computation towards similar new problems that we encounter in the future (S. Gershman \&amp; Goodman, 2014; Le, Baydin, \&amp; Wood, 2017). Certain domains of inferences share more structure than others, and evidence suggests that we therefore heavily amortize them. Prior work, for instance, suggests that computations involved in basic perceptual activities (Brooke-Wilson, 2023; Dasgupta \&amp; Gershman, 2021; Fodor, 1983), such as object recognition under common lighting conditions, are highly amortizable from reusable patterns in computation that are learnable and shared across a background distribution of perceptual instances. This view suggests why fast, bottom-up pattern recognition models have made great advances in modeling perception in recent years, while it has proved much more challenging to amortize the wide range of flexible inferences required for arbitrary problem solving.</p>
<p>We propose an analogous resource-rational perspective on the kinds of computation implicated in languageinformed thought. Under almost every theoretical and empirical account of linguistic structure and semantics, the mappings between language and meanings should be highly amortizable across the background distribution of language - there are structured, systematic, and learnable patterns in how units of language map onto units of thought. The idea that meaning construction should be highly amortizable follows from our view on language itself as an efficient communicative system. Extensive empirical evidence suggests that communicative pressures shape how language maps onto meanings at every level of linguistic structure, from individual morphemes (Bybee, 1985) to patterns in how common syntactic frames communicate meaning (L. Gleitman, 1990; Grimshaw, 1981), and even reusable pragmatic implications present across common discourse situations (White, Mu, \&amp; Goodman, 2020). But while we take the view that a resource-rational agent should intelligently learn and reuse prior computation when possible, we do not view language-informed thinking, or thinking in general, as solely a matter of learning and interpolating over statistical patterns from prior experience. When we think, including when we think about the meanings we recover from language - to update our beliefs, to follow instructions, or to answer questions posed in language - we must be able to flexibly model arbitrary situations and support capacities for general problem solving, including inference, planning, and simulation, under a wide range of new and unencountered circumstances.</p>
<p>The efficient learnability of human language also highlights that, in many senses, the computational relationship between language and thought in humans is almost the inverse of that in today's LLMs. For humans, language could be characterized as an emergent property of thinking. Infants can model the world and draw inferences well before they know language (Gopnik, 1996; Spelke, 2022), and reliably acquire complete linguistic capabilities from exposure to relatively tiny amounts of language (R. Brown, 1973). Congenitally-Deaf humans born with no language input spontaneously develop languages to communicate their thoughts, with the same basic hallmarks of mature natural languages (Goldin-Meadow, 2012; Pyers, Shusterman, Senghas, Spelke, \&amp; Emmorey, 2010; Senghas, Kita, \&amp; Ozyurek, 2004). This paper seeks to understand and model the cognitive and computational structures underlying this human scaling route to intelligence and language use - one that begins with robust capacities for thought, and scaffolds language efficiently on top of them to then offer a powerful tool for driving and constructing new thought.</p>
<h1>2.1 Our proposal: A framework for modeling rational meaning construction</h1>
<p>The perspective we offer above draws from theoretical and empirical work that precedes this paper. Our core contribution in this paper is to propose a new computational framework in light of these observations, that seeks to unify prior symbolic, probabilistic inference and statistical learning traditions and to take advantage of the clear computational advances made by modern LLMs as learned statistical models of language. We describe a framework for rational meaning construction in which linguistic meaning is formalized as a context-sensitive mapping from natural language to a distribution over expressions in a probabilistic language of thought (PLoT) for rational world modeling and inference. Under this framework, we then propose that large language models trained on language and code can be used to implement meaning functions in a resource-rational architecture - they can implement learned, broad-coverage mappings between language and code; and they can be understood as part of a human-like, resource-rational system that efficiently infers these mappings using stored patterns amortized from the prior joint distribution over language and code. This motivates the concrete architecture we propose and illustrate throughout the remainder of this paper, and its two main components for modeling thinking and modeling language relative to thinking-or how language informs thinking.</p>
<h1>2.1.1 Modeling thinking</h1>
<p>We propose implementing thinking using probabilistic programs as a general representational substrate for building world models and specifying rational inferences over them. This proposal builds on prior work in cognitive science and AI formalizing how a broad class of problems can be expressed as probabilistic programs (Chater \&amp; Manning, 2006; Goodman et al., 2014), following a generic inference query motif (Goodman, Mansinghka, Roy, Bonawitz, \&amp; Tenenbaum, 2008) - a probabilistic program that combines a generative world model that models abstract, causal beliefs about probable world states; specific evidence that an agent conditions on; and a particular query being posed as the question or goal for thinking. Inference to solve a problem consists of formally computing or sampling from a probability distribution over answers to this question, specified by the world model and conditions. This computational proposal forms the backbone of the probabilistic language of thought model of general human cognition (Goodman et al., 2014), and has been used empirically to model a wide range of human inferences, including those that draw on visual perception (V. K. Mansinghka, Kulkarni, Perov, \&amp; Tenenbaum, 2013), physical simulation (Battaglia et al., 2013), and social reasoning (C. Baker et al., 2011). It is designed explicitly to formalize a central property of human thought - the capacity to expressively and flexibly pose problems involving entirely novel situations and goals, and to solve them relative to a computable representation of the world and internal belief.</p>
<h3>2.1.2 Modeling language relative to thought</h3>
<p>Given this model for thought, we propose formalizing rational meaning construction as a broad-coverage, contextual translation function that maps language into a distribution over expressions in a probabilistic language of thought. This proposal builds most closely on and draws inspiration from efforts to articulate a probable world semantics for natural language in prior work (Goodman \&amp; Lassiter, 2015), in order to express how language could compactly convey uncertain propositions and vague meanings with respect to a formal probabilistic generative model. It also builds on the longer history of symbolic semantic theories we overview in the introduction, including formal semantics theories that model language as mapping into formal propositions over possible worlds (eg. Heim and Kratzer (1998); Lewis (1976)), and semantic parsing systems (eg. Abend et al. (2017); Klein and Manning (2003); Liang (2016); Steedman (2001); Y. W. Wong and Mooney (2007)) that map language into formally executable program expressions.</p>
<p>Our goal, however, is to broaden and generalize these framings to suggest a general framework for modeling how language can interface with and inform such a broad swatch of human cognition. By positing that meaning is a general mapping between sentences and expressions in a probabilistic language of thought, we believe that a rational meaning construction approach can elaborate on and concretely model core desiderata of a coherent theory of linguistic meaning - modeling how meanings drive inferences about what is true and probable; formalizing how language can pose propositions and queries that are then evaluated with respect to an internal model over probable worlds; and relating meaning to the general computational systems for representing, thinking about, and receiving new information about the world from broader cognition.</p>
<p>This proposal suggests a wide class of possible architectures that map from language into probabilistic programs - in principle, any general mapping function that expresses a distribution over programs conditioned on sentences in context. Under this umbrella of possible implementations, we propose finally that large language-to-code models can be used to generally instantiate these meaning functions. Unlike prior semantic parsers or attempts to hand implement mappings between language and code, LLMs offer a concrete means of instantiating far more broad-coverage mappings between human sentences and meanings than have been previously possible. They are also context-sensitive, in that they can construct meanings for an utterance that condition both on the general distribution of language and thought and a local linguistic and thinking context. They can condition translation on a local discourse context, when prompted with prior utterances, and on a local problem under consideration, when prompted with existing code in a probabilistic program.</p>
<p>By using LLMs to map between language and code, this proposal is also closely related to the recent lines of work we review in the introduction that seek to augment and connect LLMs with various structured and symbolic reasoning tools - both domain-specific reasoning engines like planners and physics engines (eg. B. Liu et al. (2023); R. Liu et al. (2022)), and more general APIs for code execution (eg. Karpas et al. (2022); OpenAI (2023b); Schick et al. (2023)). As we demonstrate throughout this paper, however, we propose that the probabilistic language of thought can offer a cognitively-motivated, unifying symbolic substrate for</p>
<p>interfacing between language and many core aspects associated with general cognition. It provides a general motif for structuring and constructing generative world models, which can nest calls to other domain-specific systems (such as planners and physics engines); and an overarching framework for modeling how diverse kinds of observations can be used to update these models and answer new queries, framed as Bayesian conditioning and inference. With respect to the more general landscape of large statistical language models, this proposal finally suggests one way to situate the strengths of LLMs into a more human-like, modular framework for language-informed thinking. Rather than look to statistical patterns to capture all of the ways we think, plan, and reason about language, this resource-rational approach seeks to ground distributional aspects of language into a framework that can leverage learned prior patterns when they are useful - while also modeling how language can construct and relate to coherent world models and algorithms for explicit, novel decision making and inference.</p>
<h1>2.1.3 Illustrating the architecture by example</h1>
<p>This general architecture is best explained through concrete implemented examples, which we give in the next sections. For each of the four domains of reasoning shown in Fig. 2, we work through a representative dialog between a speaker of English and our language-informed thinking computational architecture, which could stand in for how we model another human being's understanding and thinking about the speaker's language, or the ways we hope a human-like AI system would similarly respond.</p>
<p>For pedagogical reasons, we have chosen to implement these examples using one particular probabilistic programming language and one particular language-to-code model. These particular tools are not necessarily the most performant or scalable AI solutions; nor the best accounts we have of the corresponding components of human architecture. Nevertheless, they are familiar and simple, and provide the most direct route we know to illustrate our ideas in ways others can also experiment with. To elaborate on these choices:</p>
<ul>
<li>The probabilistic language of thought we use to express inference problems is Church (Goodman et al., 2008), a Turing-universal probabilistic programming language constructed on top of the functional programming language Scheme. We have used the WebChurch dialect which implements several general inference procedures, but we have chosen the simplest and most general - and least efficient - approach based on rejection sampling: Inference is based on drawing samples from the prior over world states described by the generative model, and rejecting those that fail satisfy the constraints of any observation conditions. The samples that remain constitute a posterior sample over possible worlds consistent with the observed information, sufficient to answer the queries under consideration in the language discourse. Other similarly functional PPLs such as WebPPL or Gen could have been chosen instead. In Section 5, we discuss future directions for extending and scaling inference beyond these simple illustrative implementations.</li>
<li>The language-to-code model we use to amortize meaning construction over programs is Codex model (Chen et al., 2021), a GPT-3-based language model fine-tuned on source code, which provides pairings between natural language and code with comments, drawn from programs on GitHub and other sources. Since the release of Codex, many other language-to-code models have been developed, and more recent versions of GPT-based language models are now routinely trained on large amounts of source code; we believe these could be used to similar effect. In Section 5, we also discuss future directions for more cognitively plausible training and updating of neural models that amortize inference in joint distributions over natural language and probabilistic languages of thought.</li>
</ul>
<p>Finally, before turning to the examples, we want to add an important note about our intentions and goals. The examples are designed to be illustrative and pedagogical - we choose them for their simplicity and clarity, and to show how prior empirical and computational work from cognitive science can be related under this general framework to language. Each example gestures at a larger domain of reasoning, but, of course, each domain is much broader than what we can implement here. Each example is also representative of a wide class of computational cognitive models that can be instantiated in a probabilistic language of thought, and which we propose can be integrated with natural language inputs and outputs under a rational meaning construction framework. In each section we therefore also discuss how this framework might be scaled, and what more work may be necessary, to scale from these examples towards a richer model of language in relation to those domains.</p>
<p>We also hope that these examples, and other variations that elaborate on them and on the core domains of reasoning we discuss here, will offer useful starting points for more rigorous, systematic, cognitivelyoriented evaluation and interpretation of the reasoning processes emergent in large language models and other language-based AI systems. In our own preliminary evaluations of these domains, we find that current large language models show many of the properties we discuss in the introduction. In some cases they appear to approximate implicitly the representations and algorithms we seek to model explicitly. In others, particularly with more complex modifications beyond these simple examples, we find that large language models left to their own devices produce outputs that diverge from our intuitions. We seek here to model the representations with which people make meaning from language in relation to all of these domains, but hope that these frameworks will be useful for understanding other computational systems that use language as well, including interpreting the representations that large language models already learn or should seek to acquire.</p>
<h1>Graphical conventions</h1>
<p>Throughout the examples presented in this paper:
$\rightarrow$ Translations mapping from language into probabilistic programs, produced by Codex, are indicated by a neural network icon.</p>
<p>Probabilistic inferences, performed by Church, are indicated by a cog icon.</p>
<h1>2.2 Understanding language with probabilistic reasoning</h1>
<p>To illustrate our framework, let's consider a concrete scenario that involves reasoning from language in the face of uncertainty. Suppose a friend is telling you about a tug-of-war tournament that took place the prior weekend in which the authors were participating:</p>
<p>Right off the bat, Josh won against Lio. He then proceeded to claim victory against Alex. Even working as a team, Lio and Alex still could not beat Josh!</p>
<p>In order to understand this story, it is useful to construct a little mental model: there are different players, they face each other solo or in teams, and based on his track record, Josh appears to be particularly strong. Now, suppose your friend tells you about a newcomer: In a huge upset, Gabe managed to best Josh in the fourth round. Maybe Gabe is even stronger than Josh! Or, perhaps Josh was simply feeling lazy in the last match, in which case, Gabe might not actually be so strong. To clarify, you might ask a question, Who is stronger: Gabe or Josh? Your friend's answer, which might itself express uncertainty, will nevertheless provide further information for you to incorporate into your understanding.</p>
<p>In making meaning from language about a scenario like the above, you are engaging in probabilistic reasoning: integrating over different possibilities in order to infer likely explanations. People are remarkably proficient at making inferences from exactly this kind of sparse evidence. Sometimes, we acquire this evidence through direct experience - by watching the tournament, for instance - but often, this kind of information comes to us through language that cues us to update our beliefs accordingly. Critically, in order to reason consistently, we need to represent core aspects of the situation: who are the different actors, what events took place, and what inferences have we already made? To this end, it is extremely useful to have a world model, which we defined earlier as a probabilistic generative model that encapsulates the key mechanics of a domain and facilitates coherent, causal explanations of events. In this section, our aim is to further formalize what exactly we mean by world models and how large-scale neural models might serve as an interface between natural language and these kinds of cognitive representations.</p>
<p>World models as generative programs. The core of each example in this paper is a probabilistic generative model that defines the mechanics of a domain. For the purposes of this demonstration, and throughout Section 3, we focus on reasoning from language given a pre-specified world model. Later, in Section 4, we show how language can be used to grow out and construct new world models.</p>
<p>As a playground for this initial demonstration, we consider the "Bayesian tug-of-war," a classic experimental domain in cognitive science that requires making inferences about the latent traits of individuals from sparse evidence. Prior work establishes that Bayesian inference in a probabilistic generative model closely captures people's predictions about scenarios in the tug-of-war (Gerstenberg \&amp; Goodman, 2012; Goodman et al., 2014), and that simple sentences can be mapped onto queries in this model (Goodman \&amp; Lassiter, 2015). Here, we build on this work to give an account for how people might turn open-ended natural language into statements in the probabilistic language-of-thought.</p>
<p>In tug-of-war, we start with a generative model of a tournament in which players of varying strengths compete in a series of matches, facing off either solo or as part of fluid teams (Fig. 3A). Each player has a latent strength value randomly sampled from a Gaussian distribution (with parameters arbitrarily chosen as $\mu=50$ and $\sigma=20$ ). As an observer, our goal is to infer the latent strength of each individual based on their win/loss record. However, players sometimes don't pull at their full strength and each player has a different intrinsic "laziness" value (uniformly sampled from the interval $[0,1]$ ) that describes how likely they are to be lethargic in a given match. The full Church code for the tug-of-war is given in Appendix A.1.1.</p>
<p>Linguistic meanings as probabilistic program expressions. While the generative model defines the generic mechanics of the domain, we want to be able to talk about specific people and events. In our framework, we focus on two kinds of linguistic utterances:</p>
<p>Observations provide information about people, objects, and events in the world; e.g., "Josh faced off against Lio and won." In our framework, we translate observations into condition statements in Church, which update the state of the world model to reflect new facts. Note that condition statements have no return value; instead, they constrain the world model such that downstream inferences must be consistent with respect to the conditioning statement.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(B) Translation examples for LLM prompting</p>
<p>John and Mary won against Tom and Sue.
(condition (won-against '(john mary) '(tom sue)))
Sue is very strong!
(condition (&gt; (strength 'sue) 75))
If Sue played against Tom, who would win?
(query (won-against '(sue) '(tom)))
<img alt="img-3.jpeg" src="img-3.jpeg" />
(C) Natural language
<img alt="img-4.jpeg" src="img-4.jpeg" />
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Josh won against Lio.</th>
<th style="text-align: center;">(condition (won-against '(josh) '(lio)))</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Josh proceeded to claim victory against Alex.</td>
<td style="text-align: center;">(condition (won-against '(josh) '(alex)))</td>
</tr>
<tr>
<td style="text-align: center;">Even working as a team, Lio and Alex still could not beat Josh.</td>
<td style="text-align: center;">(condition (not (won-against '(lio alex) <br> '(josh)))</td>
</tr>
<tr>
<td style="text-align: center;">How strong is Josh?</td>
<td style="text-align: center;">(query (strength 'josh)) <br> Query (won-against '(gabe) '(josh)))</td>
</tr>
<tr>
<td style="text-align: center;">What are the odds of Gabe beating Josh?</td>
<td style="text-align: center;">(query (won-against '(gabe) '(josh)))</td>
</tr>
</tbody>
</table>
<p>Figure 3: Illustration of probabilistic reasoning via language-to-code translation in the tug-of-war domain. (A) The generative model defines two latent traits, strength and laziness, and specifies how these interact to determine team-strength. By combining (A) and (B), we can few-shot prompt an LLM to translate open-ended natural language (C) into Church statements (D) that capture linguistic meaning with respect to the domain. The resulting probabilistic inferences transparently represent the model's beliefs and naturally capture human-like intuitions about players' latent traits.</p>
<p>Questions seek information in the face of uncertainty about the world; e.g., "Would Josh beat Gabe if they played again?" In our framework, we translate questions into query statements in Church, which evaluate the quantity of interest. Calling query triggers a probabilistic computation that simulates possible worlds under the model, constrained by any observations so far. The query expression is evaluated in each simulated world, yielding multiple samples that form a posterior distribution over the value of interest.</p>
<p>Throughout the examples in this work, we freely interleave query and condition statements, much as questions might occasionally arise between statements of fact in a natural dialogue. Implementationally, this behavior is achieved through a read-evaluate-print loop (REPL) inspired by Venture's (V. Mansinghka, Selsam, \&amp; Perov, 2014), that evaluates queries against all condition statements that have appeared up to that point in the dialogue history. In our model, we assume that the user specifies whether each utterance is a condition or a query, but LLMs could likely classify unannotated utterances accurately.</p>
<p>Translating from natural language to program expressions. Inspired by the work of Goodman and Lassiter (2015), if we had some way to translate linguistic utterances into probabilistic program statements, we could perform a wide variety of probabilistic inferences from plain English. Up until recently, however, it was unclear how to construct a meaning function sufficiently general to translate open-ended natural language into highly structured expressions compatible with a Church model. Our core observation is that language-code LLMs have many of the properties necessary to serve as a useful meaning function: broad-coverage exposure to natural language, a robust capacity to model joint language-code text distributions, and the ability to quickly grasp domain-specific syntax and semantics from a few examples.</p>
<p>In this work, we leverage the few-shot prompting capabilities of one such LLM, the Codex model from OpenAI, to induce a translation model from English to Church code. As it turns out, we only need to provide a small handful of example translations (represented in Fig. 3B) to achieve a variety of interesting behaviors. To translate a new language utterance to Church, we simply concatenate the generative model (full text in Appendix A.1.1) and the translation examples (full text in Appendix A.1.2) into a prompt whose final line is the utterance. We then generate from Codex, which, based on the comment-code pattern in the prompt, infers that the completion should be written in Church, using the function definitions and constructs provided in the prompt.</p>
<p>Notice the high degree of variation in phrasing and lexical choice in Fig. 3C; none of the utterances contain "won" or "against," yet Codex still maps these to the won-against function. Here, we start to see some of the advantages of using an LLM over more traditional semantic parsing techniques like CCG parsers (Artzi, Lee, \&amp; Zettlemoyer, 2015; Artzi \&amp; Zettlemoyer, 2013). Because the model is pre-trained on a vast amount of linguistic data, it fluently handles many different kinds of linguistic variation. However, by including the Church generative model in the prompt, we can effectively constrain the output space; the model infers that the generated code should use the functions defined in the generative model.</p>
<p>As a semantic parsing tool, this combination of pre-training and prompting manages to achieve broad invariance to spurious linguistic variation while remaining sensitive to wording choices that might affect meaning. We can see this tradeoff at work in Fig. 3C, where the translation uses a negation, closely reflecting the structure of "Lio and Alex still could not beat Josh." Of course, there are multiple aspects of the utterance that this translation does not capture (e.g., "Even working as a team..." suggests that Lio and Alex's efforts were well-coordinated; as opposed to something like, "Stepping on each other's toes the whole match...," which would imply the opposite). Our point is not that the LLM translation perfectly captures all aspects of the utterance meaning, but rather, that it encodes those that are relevant to and compatible with the domain model so as to facilitate downstream reasoning.</p>
<p>Reasoning about scenarios with probabilistic inference. So far, we've illustrated how we might condition a PLoT model on natural language, but what about reasoning? After hearing the information in Fig. 3C, we might assume that the player named Josh is quite strong. Exactly how strong is Josh, though? And how likely is it that he would beat another player who isn't Lio or Alex? Just as we used Codex to translate facts into condition statements, we can use it to translate questions into query statements in Church. The Church inference engine then automatically simulates scenarios (in this case, 1000 times) that are consistent with the given condition statements in order to produce an approximate posterior distribution over each query.</p>
<p>By offloading reasoning from the LLM to the PLoT, we can obtain a much richer picture of our model's beliefs about the world (Fig. 3D). While the LLM alone can only respond with textual statements like "Josh is very strong," Church gives us an entire probability density over Josh's strength (on expectation, he is a little less than one standard deviation above the average strength $=50$ ). Likewise, we can easily obtain a distribution over the outcomes of a Gabe-Josh match (given Josh's strong track record, our model finds Gabe's chances slim, at $23.90 \%$ ). Critically, Church is doing much of the heavy lifting of inference in the background in order to produce these posterior distributions.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 4: Reasoning about a pair of hypothetical scenarios with language-code translation. In a world where Josh is often lazy, Gabe's win is counteracted by a high likelihood that Josh threw the match. Conversely, in a world where Josh is rarely lazy, Gabe's win is surprising and suggests a high strength value. Rational meaning construction with an LLM appropriately resolves the linguistic meaning of these two scenarios, selecting reasonable probability parameters for the conditioning statements. Meanwhile, probabilistic inference about Gabe's strength is finely sensitive to the implications of these competing hypotheses.</p>
<p>In addition to providing useful interpretability, reasoning in Church models is sensitive to each new piece of information. Much like human learners, Church models can flexibly update their beliefs when presented with low-probability or unanticipated events. Picking up our tug-of-war saga, consider the plot twist in Fig. 4:</p>
<p>In a huge upset, Gabe managed to best Josh in the fourth round.
How might this new information shape our interpretation of the match 4 outcome? If Josh is likely to be lazy, then it's possible that Gabe simply got lucky and wasn't so strong after all. If, on the other hand, Josh is rarely lazy, we might start to regard Gabe as particularly strong. In Fig. 4, we can observe how Church reasons about these two possibilities, shifting the probability density over Gabe's strength left if Josh is likely lazy and right if Josh is rarely lazy.</p>
<p>Note how, in order to translate a phrase like "Josh has a propensity to slack off," Codex must choose a particular probability threshold. This choice is arbitrary and, while there is no "correct" answer, we see that Codex is able to choose valid probability values between $[0,1]$ that feel appropriate to the wording: a</p>
<p>"propensity to slack off" doesn't necessarily imply that someone slacks off all the time, while, in contrast, "rarely lazy" offers more certainty. Indeed, across many different contexts, we observe that Codex is able to pick reasonable parameter values that respect both the language and the parametrization of defined distributions. We consider these inferences to represent a form of "amortized pragmatics" (Goodman \&amp; Lassiter, 2015), which we will revisit in Section 5.</p>
<p>Putting it together: the power of probabilistic reasoning. We conclude this section with a final example that underscores the flexibility of our framework to model complex reasoning from language and foreshadows multiple themes that we will revisit later in the paper. Consider the dialogue in Fig. 5, in which the students and faculty team up to face one another. The interlocutor poses two questions: "Is Gabe stronger than the weakest player on the faculty team?" and "Who would win in a match between the students and the faculty?" As we saw in the prior tug-of-war examples, the answers to both of these questions are expressed as probability distributions derived from simulation of the generative tug-of-war model. Moreover, in both cases, the introduction of new information flips the model's belief state in a way that aligns with human intuitions. In this way, the PLoT framework is natively capable of defeasible inference-a phenomenon of human reasoning that was of great interest to early AI pioneers of non-monotonic logics (Ginsberg, 1987; McCarthy, 1980).</p>
<p>A key advantage of our framework is that achieving these kinds of defeasible and flexible inferences from natural language reduces to grounding utterances into appropriate condition and query statements. While the observations and questions in Fig. 5 are semantically more complex than those that appeared in the prior examples, and though there are many degrees of freedom involved in the translation problem, we confirm that an appropriately-prompted LLM can produce translations that intuitively capture the meaning of each utterance with respect to the tug-of-war domain. Moreover, as we saw in Fig. 4, Codex is able to amortize certain pragmatic inferences in resolving "pretty strong" to a threshold of strength $&gt;60$, "real slackers" to a threshold of laziness $&gt;0.9$, and "several of the faculty" to count $&gt;=3$. How far can we go with these kinds of amortizations? Throughout Section 3 and Section 4, we will see examples of context-driven amortizations across different domains; and in Section 5, we will regroup to discuss how these different examples of amortization might inform our theories of language understanding and pragmatics.</p>
<p>In this dialogue, we also give a preview of define, a powerful construct in our framework that is discussed in depth in Section 4. Just as people come up with terms like "20th-century pragmatists" or "Meatless Monday" to pick out entire hierarchies of people, things, and events, a core feature of the probabilistic LoT is the ability to define new concepts that can later be referenced symbolically. In the Fig. 5 dialogue, language about team structures defines two new concepts, faculty-team and student-team, that facilitate concise translation of language like, "Is Gabe stronger than the weakest player on the faculty team?" Moreover, while faculty-team is a static list, other defined concepts can ground out in functions that take arguments. In fact, stronger-than?, which is defined in the prompt (Appendix A.1.2), is one such example, illustrating how programming languages are well-suited to capture the infinite productivity of language that arises through structured composition. Through this lens, we can start to imagine how our tug-of-war world model might be expanded to ground many new kinds of language:</p>
<ul>
<li>The tug-of-war tournament is organized into three leagues for novices, amateurs, and professionals. In order to be considered a professional, a player must win 20 one-on-one matches against other professionals.</li>
<li>Players often get increasingly tired over the course of a tournament, though some players have more stamina than others.</li>
<li>The tournament has an entry fee of $\$ 20$ per contestant and a grand prize of $\$ 10,000$ for the winning team.</li>
</ul>
<p>How can we grow our world models to incorporate new language, or even construct new world models entirely from scratch? In Section 4, we revisit the tug-of-war domain with an eye to precisely these questions.</p>
<p>Conclusions. As an introductory example, the tug-of-war domain serves as a minimal illustration of the kind of reasoning from language that our framework is concerned with. Our goal here was to build intuition</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 5: In this final tug-of-war dialogue, natural language plays three interlaced roles in interfacing with the language-of-thought. Definitions (purple) introduce new concepts, such as specific player-teams, that can later be referenced symbolically. Observations (blue) translate into condition statements that probabilistically constrain the world state, sometimes amortizing the resolution of linguistic ambiguity (e.g., "pretty strong" or "real slackers"). Finally, questions (green) translate into queries that trigger inference by probabilistic simulation over possible worlds that is both sensitive to and consistent with prior definitions and observations.</p>
<p>for our general approach: by translating natural language into condition and query statements as inputs to a probabilistic inference engine, we can achieve forms of reasoning from language that are consistent with respect to a mental model of the world. Nonetheless, in scaling this approach beyond the toy domain of tug-of-war, many questions arise. How does probabilistic inference relate to models of relational and deductive reasoning of the sort that classical AI approaches excel at? How do we ground linguistic meaning in the visual and physical world? And how does language understanding inform our actions and interactions with other agents through goal-directed planning? In Section 3, we will progressively expand our scope to touch on each of these questions and show that, in each case, new kinds of language understanding and reasoning can be naturally incorporated into our framework.</p>
<h1>3 Understanding and reasoning about language with world models</h1>
<p>In this section, we illustrate how the general framework we propose in Section 2 can be applied and extended to integrate natural language with core domains of human-like thought. In each, we build on the idea that language that conveys observations and questions about uncertain situations, constructing meanings from a generative world modeling program that supports probabilistic reasoning. In Section 3.1, we show how this approach can be integrated to understand language that conveys structured, logical lexical relations. In Section 3.2, we show how generative programs that support perceptual and physical simulation can be used to ground language about scenes into visual world. Finally, in Section 3.3, we consider language about agents with preferences and goals, and show how we can make meaning from sentences with respect to a generative program that supports planning.</p>
<h3>3.1 Language for logical and relational reasoning</h3>
<p>In the previous section, we examined how translation from natural language into the probabilistic language of thought naturally captures a certain form of reasoning in which uncertainty plays a key role. How does this framework relate to earlier computational theories of reasoning, such as classical AI approaches to logical and relational reasoning (Russell \&amp; Norvig, 2021)? Historically, systems like Prolog (Colmerauer, Kanoui, Pasero, \&amp; Roussel, 1972; Philippe, 1972) were designed for similar goals to ours here, to allow people to directly interact with computers via natural language (French, originally), specifying only the background knowledge and goals for computation without the algorithmic details (Colmerauer \&amp; Roussel, 1996). In this section, we demonstrate how the PLoT not only fully supports the style of deductive, logical reasoning characteristic of classical AI, but extends it to support inductive inferences as well. Moreover, we argue that many kinds of real-world reasoning problems that are traditionally modeled using structured logic-based approaches actually require a mix of both symbolic and probabilistic reasoning. In doing so, we aim to illustrate how our approach of translating from natural language to the PLoT fluidly integrates both kinds of reasoning in a way that comes naturally to people, but that has proved elusive for both traditional deductive programming systems and purely statistical language models.</p>
<p>Language about kinship relations. Suppose you are again with your friend from Section 2.2, who is telling you about a part of their extended family. "Avery has a sister named Blake, and their father is named Charlie," your friend says. Immediately, you start to sketch a picture in your mind of this family, which you can update on-the-fly as you get more information: "Charlie is the grandfather of Dana." At this point, you can infer that one of Charlie's kids is also Dana's parent, but which one? In the absence of additional information, it's a toss-up between Avery and Blake, with some outside chance that there could be another, unmentioned sibling who is Dana's parent. Hearing that "Blake has two kids" might initially shift your beliefs towards Blake. However, upon learning that "Dana is an only child," you'd have to rule Blake out entirely! This kind of relational reasoning, which freely intermixes deductive and inductive inferences, comes quite naturally to people. How do we make such rich inferences from a relatively sparse sequence of words?</p>
<p>In this section, our domain of interest will be kinship: relationships between people in a family. The kinship domain provides fertile ground for the study of logical reasoning for several reasons. First, during development, one of the first domains where we learn about logical relations is in describing families (Elkind, 1962; Piaget, 1951). Language has evolved to describe family structures in highly economical terms that naturally express composition (e.g., my mother's father is my grandfather) and symmetry (e.g., if Avery is my cousin, then I am Avery's cousin; together, we are cousins). Nevertheless, while certain kinship references are relatively straightforward (e.g., "Blake's mother"), others involve ambiguity (e.g., "Blake's uncle" could refer to the brother of either of Blake's parents; or even, perhaps, a close older male who is not related by blood or marriage). Finally, kinship reasoning freely intermixes deductive and inductive inferences: for instance, "Charlie has a grandson named Dana" deductively implies the existence of a child of Charlie who is also a parent to Dana; and it inductively implies that Charlie was possibly partnered at some point, such that Dana might have another grandparent in the picture. Traditional logical accounts of reasoning in this domain capture the deductive inferences but not the inductive inferences in cases like this. People, in contrast, routinely make statements such as "This is Kendall, the partner of Avery's niece" with the expectation that others will draw roughly the same inferences they would in building a mental model of this family: Avery has</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 6: Illustration of a simple kinship domain theory and conceptual system implemented in Church. (i) The generative model specifies a process by which individuals form couples and have children to form family trees. Each tree represents a "possible world" in which certain relationships hold. (ii) These relationships are expressed using predicates in a conceptual system that supports quantificational logic and composition, giving rise to an expressive domain semantics that aligns well with natural language.
a brother or sister, and that sibling has a female child, and Kendall is that person's partner. In sum, the kinship domain offers a rich set of relations and possible inferences, and comes equipped with an extensive natural language vocabulary, making it an ideal playground to explore our translation hypothesis.</p>
<p>World models of kinship as probabilistic generative programs. Owing to the richness of the domain, recent years have seen a steady interest in computational cognitive models of various aspects of kinship, ranging from development and acquisition of kinship terms across cultures (Mitchell \&amp; Jordan, 2021; Mollica \&amp; Piantadosi, 2022), tradeoffs in communicative efficiency in natural (Jones, 2010; Kemp \&amp; Regier, 2012) and artificial (K. Smith, Frank, Rolando, Kirby, \&amp; Loy, 2020) kinship systems, and probabilistic inferences about kinship relations from sparse evidence (Katz, Goodman, Kersting, Kemp, \&amp; Tenenbaum, 2008). In this work, our primary interest is in how people represent and reason about kinship relations conditioned on language. Following Katz et al. (2008), we construct an intuitive domain theory of kinship using a probabilistic generative model and a small number of rules that form a conceptual system.</p>
<p>As in Section 2.2, our kinship domain theory is expressed as a generative model in Church. In the Bayesian tug-of-war, the generative model consisted of random variables over continuous quantities like strength and laziness. In contrast, in this section, our generative model specifies a series of discrete random choices that describe events in a family's genealogy: people are born, find partners, have children, and the process repeats. All of these events involve random choices that shape the makeup of the family tree.</p>
<p>Fig. 6 (i) shows a schematic of the kinship generative domain theory. When a person is born, they are assigned a unique person-id, a name ${ }^{1}$ sampled from a list of gender-neutral names, and a gender sampled from {male, female}. Next, with fixed $p=0.5$, the person partners with a new individual from outside the family. Finally, if partnered, the couple has $n={0,1,2,3}$ children, with the number of kids drawn from a geometric distribution $(p=0.5)$. This process repeats recursively until a full family tree is generated. To support efficient inference using Church's generic sampling algorithms, we cap the trees at 3 generations and limit each couple to 3 children. Further implementation details of the generative model can be found in Appendix A.2.1.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>As with any computational model of a social phenomenon, this toy kinship model is reductive of many important nuances of identities and relationships. For instance, while the model includes both same- and opposite-gender couples, these couples never split, so step-relations aren't well-captured. While these kinds of compromises are designed to keep inference tractable, still others stem from limitations of the language itself. For example, many colloquial English kinship terms are gender-binary (e.g., mother, grandfather, daughter), so instantiating them as truth-conditional predicates coerces the generative model towards traditional gender assignments. Similarly, many English names carry strong gender associations, which NLP systems trained on large linguistic corpora pick up on (Caliskan, Bryson, \&amp; Narayanan, 2017; Grand, Blank, Pereira, \&amp; Fedorenko, 2022). In our examples, we intentionally select gender-neutral names (e.g., Avery, Blake, Charlie, Dana) to emphasize that these naming-based gender inferences are deliberately not part of the reasoning task.</p>
<p>To summarize, language both reflects and constrains our intuitive theories of complex domains like kinship (Sapir, 1929; Whorf, 1956; c.f. Gentner \&amp; Goldin-Meadow, 2003 for a review of contemporary perspectives on linguistic relativity), and these tradeoffs manifest concretely in the toy model presented in this section. Fortunately, where this initial "off-the-shelf" kinship model lacks social and cultural nuance, our framework offers opportunities to extend and modify these areas. In section Section 4.1, we look at ways of growing our kinship model to include concepts from non-English-speaking cultures and more inclusive concepts of gender.</p>
<p>Relational meanings as program statements. Given a generative model of family trees, we can define a rich conceptual system to make statements about relationships between individuals. Our conceptual system consists primarily of a dozen-odd derived predicates that are binary operators over pairs of names; e.g., (father-of? 'charlie 'blake) is true iff Charlie is the father of Blake in a particular tree instance. ${ }^{2}$ These derived predicates build on a small number of low-level accessor functions that operate directly on nodes in the tree data structure. For instance, (children-of 'blake) returns a list of names corresponding to the children of Blake in the tree. Finally, our conceptual system includes several higher-order functions, like map-tree, filter-tree, and exists that take custom predicates as inputs and return a boolean. These functions facilitate the expression of a rich compositional semantics by allowing for compound predicates containing conjunctions and disjunctions. Fig. 6 (ii) illustrates several examples of the kinds of statements that can be made using combinations of derived predicates, low-level accessors, and higher-order functions. The full set of definitions making up the conceptual system is given in Appendix A.2.3.</p>
<p>Translating from language to program expressions. As in Section 2.2, we use a handful of paired natural language / code examples (Appendix A.2.4) to induce a meaning function via Codex. Because the prompt also includes the generative model source code and the full set of derived predicates, the LLM is able to resolve statements like "Blake has two kids" to the appropriate function (in this case, children-of) using the available definitions. Moreover, we observe zero-shot generalization to linguistic constructs that are not explicitly defined in the prompt, such as the concept of an "only child" (Fig. 7).</p>
<p>Putting it together: Reasoning from language about kinship relations. What is the purpose of all of this domain-specific machinery that we have now built up? The answer is two-fold. First, the generative domain theory compactly captures the key dynamics of our domain, allowing us to reason about a combinatorially vast space of possible family trees. Meanwhile, the conceptual system serves as a higher-level program interface, defining certain relationships that we would like to be able to talk about. Finally, the large language model bridges the domain model with natural language, providing a flexible and context-aware way to ground language into conditioning and query statements.</p>
<p>In Fig. 7, we can see how these components come together to facilitate naturalistic reasoning from language about kinship relations. Each natural language utterance translates to a condition statement in Church that serves as a constraint on family trees. With each successive condition, our uncertainty decreases and our picture of the family tree in question starts to crystallize. Samples from the conditioned domain theory model therefore serve as hypotheses about possible worlds that are consistent with the information provided through language. Furthermore, the distribution over conditioned samples provides a principled way to reason about</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Note that because our model includes same-gender couples, Blake may have one father, two fathers, or no fathers. Blake also may not exist in the tree in the first place! Crucially, these aspects of the generative model don't matter to the derived predicate, which simply evaluates whether the relationship in question holds somewhere in the tree.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
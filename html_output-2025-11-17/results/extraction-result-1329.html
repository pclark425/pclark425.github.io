<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1329 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1329</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1329</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-261530044</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.01909v1.pdf" target="_blank">A Survey on Physics Informed Reinforcement Learning: Review and Open Problems</a></p>
                <p><strong>Paper Abstract:</strong> The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e., observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1329.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1329.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenSim (OpenSim-RL / L2M2019 environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomechanics / musculoskeletal simulation environment referenced as a training/evaluation platform for physiologically accurate motion and musculoskeletal control in RL studies (used e.g. for musculoskeletal simulation and locomotion tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>OpenSim (OpenSim-RL / L2M2019)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A musculoskeletal simulation environment used for physiologically accurate human motion and locomotion tasks; referenced in the survey as the environment for musculoskeletal simulation / L2M2019 benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology (biomechanics / musculoskeletal simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not specified in this survey (referenced as a physiology-accurate musculoskeletal simulator / benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>survey does not give low-level fidelity specs; referenced use implies detailed musculoskeletal dynamics (muscle models, joint dynamics) typical of OpenSim; the paper does not state which features were modeled or omitted</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>reinforcement learning agents for locomotion / musculoskeletal control (as used by referenced works)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>learn physiologically plausible locomotion / musculoskeletal control policies</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>not specified (used primarily as training/evaluation environment in referenced studies)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>no explicit discussion in the survey about minimum OpenSim fidelity required for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>none reported in the survey for OpenSim specifically</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1329.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFT-based custom simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Custom DFT (Density Functional Theory) based simulator / evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DFT-based evaluation/simulation used as a physics-based reward/evaluator for molecular structure optimization tasks in RL (i.e. physically-correct structural prediction via DFT calculations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-guided reinforcement learning for 3D molecular structures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Custom DFT-based calculation (DFT evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Custom simulation/evaluation using density functional theory calculations to provide physically-correct rewards/energies for molecular structure optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>molecular physics / computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity physics evaluator (quantum-chemistry level) as used for reward computation in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>uses first-principles DFT calculations to evaluate candidate molecular configurations; exact computational settings not specified in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agent for single-molecule 3D structure optimization (referenced works used DDPG and similar agents)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>optimize 3D molecular conformations / structures with physically-correct energy objective</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>not specified (DFT used as ground-truth physics-based reward during training/evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>survey notes DFT used as physically-correct reward but does not discuss minimal necessary fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1329.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMSOL-based custom</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Custom COMSOL-based simulator (multiphysics custom environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom, COMSOL-based simulation environment cited in the survey as used by an autonomous manufacturing / physics-guided RL study (custom multiphysics simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A physics-guided reinforcement learning framework for an autonomous manufacturing system with expensive data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Custom COMSOL-based simulator</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Custom multiphysics simulation environment built on COMSOL (used in referenced work for domain-specific physics simulation in autonomous manufacturing experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multiphysics / possibly thermomechanics or acoustics depending on the study</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not explicitly quantified in the survey; referred to as a COMSOL-based custom high-fidelity multiphysics simulator</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>survey gives no low-level detail; being COMSOL-based implies PDE-level multiphysics modeling (but exact physics modules or discretization not described)</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agent for autonomous manufacturing / policy optimization trained with COMSOL-simulated data in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>optimize manufacturing process parameters under physically-modelled constraints</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>target real-world manufacturing system (survey notes sim-to-real transfer was a goal in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>survey does not state explicit minimal fidelity requirements; notes COMSOL used to generate realistic simulated data</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>no explicit failure cases reported in the survey for this COMSOL-based setup</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1329.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IEEE distribution benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IEEE distribution system benchmarks (IEEE 9/33/141-bus etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard IEEE distribution and transmission test systems used as benchmarks for power/voltage control tasks in PIRL works (used for training/evaluation in power-system studies).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>IEEE distribution system benchmarks (simulation instantiations)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Standard power-system benchmark networks (e.g., IEEE 9-bus, 33-bus, 141-bus) used to evaluate voltage/stability control RL algorithms; typically simulated in power-system toolchains or MATLAB/Simulink.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>electrical power systems / circuits</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not specified in survey; these benchmarks represent canonical test cases that can be simulated with detailed power-flow and dynamic models (fidelity depends on simulator used)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>survey does not list specifics; possible inclusion of transient dynamics and standard component models when instantiated in MATLAB/SIMULINK or power-system tools</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agents for voltage control / transient stability enhancement referenced in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>control/stabilize power-system voltages and transient behavior</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>evaluation on IEEE benchmark cases (and in some works toward real systems), but survey does not report specific real-world transfer metrics</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>no explicit minimum-fidelity statement in the survey for IEEE benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>none specifically reported in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1329.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATLAB/Simulink</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATLAB / Simulink custom simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MATLAB/Simulink environments referenced as platforms for training/evaluating RL agents in control and power applications (e.g., power/grid control, district cooling modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MATLAB / Simulink (custom models)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>General-purpose engineering simulation environment used to implement power-system, thermal, and control models for RL training and evaluation in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>control systems / power systems / thermodynamics (depending on the model implemented)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not specified in survey; fidelity is model-dependent (from simple lumped models to detailed component-level dynamic simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>survey does not give specifics; MATLAB/Simulink often supports continuous-time dynamic models, control blocks, and specialized toolboxes for power/thermal dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agents for control tasks (e.g., inverter PQ control, district cooling control) trained/evaluated using Simulink models in referenced studies</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>control and regulation of power/thermal systems (e.g., voltage control, cooling system regulation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>target real-world devices/systems in cited works (survey notes Simulink used for training/evaluation but does not give numeric transfer results)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>no explicit minimal-fidelity conclusions provided in the survey regarding Simulink models</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>not reported in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1329.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differentiable simulators (NimblePhysics / Redner / custom DS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable physics simulators (e.g., NimblePhysics, Redner, custom differentiable simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differentiable simulators are referenced as simulation engines that provide analytic or automatic gradients of simulation outcomes w.r.t. actions/parameters, enabling gradient-based policy optimization and model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Differentiable simulators (NimblePhysics, Redner, other DS)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Physics simulators adapted to be differentiable (provide gradients), used in PIRL for accelerating policy learning, model identification, and to support differentiable loss computation through physical simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics / vision (depending on simulator capabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>survey does not assign a single fidelity level; differentiable sims trade off features and performance â€” they are discussed as providing useful gradients rather than being defined by fidelity alone</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>survey notes DS provide analytic/auto-differentiation of simulation outcomes, allow many environments to run in parallel and can include rigid-body contacts and rendering (specific physical effects depend on the particular implementation like NimblePhysics/Redner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>policy networks or model-based components that leverage simulation gradients for improved optimization (referenced works include agents trained with DS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>policy optimization and model identification using simulation gradients (e.g., policy improvement via differentiable simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>some referenced works aim for sim-to-real transfer; DS are used to refine models by comparing simulated and real observations</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>the survey highlights benefits of DS (gradient access) but does not prescribe minimal physics fidelity required for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>survey mentions DS have issues that some algorithms alleviate (e.g., Xu et al. propose methods to alleviate DS issues) but does not document concrete fidelity-failure cases in detail</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1329.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used rigid-body physics simulator with contact dynamics commonly used in RL benchmarks for dynamic control and locomotion tasks; referenced frequently in the surveyed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Rigid-body dynamics simulator supporting articulated bodies and contact, commonly used for locomotion and control benchmarks (Ant, HalfCheetah, Humanoid, Walker2d, Cassie).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity rigid-body dynamics with contact; offers efficient simulation of articulated mechanisms but abstracts away detailed continuum physics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>provides contact dynamics, joint dynamics, and actuator models, but not continuum/thermodynamic effects; survey lists MuJoCo as a standard benchmark platform</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agents for locomotion and dynamic control (PPO, SAC, DDPG variants) trained in MuJoCo-based tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>learn dynamic control and locomotion policies (not explicit scientific reasoning tasks like thermodynamics or circuits)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>some works use MuJoCo for sim-to-real experiments (survey mentions sim-to-real aims broadly), but no specific transfer metrics are given in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>survey does not discuss minimum MuJoCo fidelity needed for successful real-world transfer</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>survey does not report specific MuJoCo failure cases; it notes many works use customized environments instead of standard benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1329.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SUMO / CARLA (traffic/vehicle sims)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SUMO (Simulation of Urban MObility) and CARLA (CAR Learning to Act)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traffic micro/macro simulators cited as training and evaluation platforms for vehicular control and autonomous driving RL problems (ramp merging, connected automated vehicles).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>SUMO and CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>SUMO: microscopic/macroscopic traffic flow simulator used for traffic control tasks; CARLA: high-fidelity urban driving simulator used for autonomous driving RL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>traffic dynamics / autonomous vehicles</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>survey does not quantify fidelity; SUMO is typically a traffic flow microsimulator (modeling vehicle interactions), CARLA is a high-fidelity urban driving simulator (visual sensors, dynamics), but exact settings vary by study</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>survey does not give details; SUMO provides traffic flow and vehicle kinematics modeling, CARLA provides detailed urban environment, sensor simulation and vehicle dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL controllers for ramp metering, safe merging, and connected automated vehicle control</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>traffic control, ramp merging, and safe autonomous driving policy learning</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world driving/traffic control scenarios or other simulators; survey does not report quantitative transfer outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>no explicit statement about minimal fidelity required for transfer in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>survey notes many works use customized environments and that sim-to-real gap is a concern, but does not enumerate specific SUMO/CARLA failure cases</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1329.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1329.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-phase flow / thermal custom</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Custom two-phase flow / thermal simulators (as used in heat/mass transfer and cooling papers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Custom simulations and physics models used in thermodynamics / fluid-dynamics PIRL studies (e.g., two-phase flow interfacial area prediction, district cooling, data-center thermal models).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Custom thermal / two-phase flow simulators (domain-specific custom models)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Custom physics models/simulators for fluid/thermal systems used to generate data and rewards for RL agents in heat/mass transfer and cooling control applications; implementations vary by study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>thermodynamics / fluid dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not specified in the survey; typically these are domain-specific physics models (ranging from reduced-order/empirical to PDE-resolved CFD), survey does not specify which fidelity was used per paper</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>survey references inclusion of physical constraints (momentum equation, pressure Poisson, boundary conditions) in reward design for flow reconstruction, but does not detail solver fidelity, discretization, or turbulence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL agents for flow-field reconstruction, district cooling control, and two-phase flow interfacial area prediction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>predict/optimize thermal/fluid behavior (e.g., interfacial area in two-phase flow, cooling system regulation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real systems or higher-fidelity models in referenced works (survey mentions sim-to-real and offline-online training strategies but gives no numeric transfer outcomes)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>the survey notes physics-informed models improve sample efficiency but does not state explicit minimal fidelity requirements for thermal/fluid tasks</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>no explicit documented failure cases in the survey for low-fidelity thermal/fluid simulators</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Physics Informed Reinforcement Learning: Review and Open Problems', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Real-world reinforcement learning via multifidelity simulators <em>(Rating: 2)</em></li>
                <li>Differentiable physics models for real-world offline model-based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer with neural-augmented robot simulation <em>(Rating: 2)</em></li>
                <li>A physics-guided reinforcement learning framework for an autonomous manufacturing system with expensive data. <em>(Rating: 2)</em></li>
                <li>Physics-guided reinforcement learning for 3D molecular structures <em>(Rating: 2)</em></li>
                <li>Towards stochastic modeling for two-phase flow interfacial area predictions: A physics-informed reinforcement learning approach <em>(Rating: 2)</em></li>
                <li>PhysQ: A Physics Informed Reinforcement Learning Framework for Building Control <em>(Rating: 1)</em></li>
                <li>Phyllis: Physics-Informed Lifelong Reinforcement Learning for Data Center Cooling Control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1329",
    "paper_id": "paper-261530044",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "OpenSim",
            "name_full": "OpenSim (OpenSim-RL / L2M2019 environment)",
            "brief_description": "A biomechanics / musculoskeletal simulation environment referenced as a training/evaluation platform for physiologically accurate motion and musculoskeletal control in RL studies (used e.g. for musculoskeletal simulation and locomotion tasks).",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "OpenSim (OpenSim-RL / L2M2019)",
            "simulator_description": "A musculoskeletal simulation environment used for physiologically accurate human motion and locomotion tasks; referenced in the survey as the environment for musculoskeletal simulation / L2M2019 benchmark.",
            "scientific_domain": "biology (biomechanics / musculoskeletal simulation)",
            "fidelity_level": "not specified in this survey (referenced as a physiology-accurate musculoskeletal simulator / benchmark)",
            "fidelity_characteristics": "survey does not give low-level fidelity specs; referenced use implies detailed musculoskeletal dynamics (muscle models, joint dynamics) typical of OpenSim; the paper does not state which features were modeled or omitted",
            "model_or_agent_name": null,
            "model_description": "reinforcement learning agents for locomotion / musculoskeletal control (as used by referenced works)",
            "reasoning_task": "learn physiologically plausible locomotion / musculoskeletal control policies",
            "training_performance": null,
            "transfer_target": "not specified (used primarily as training/evaluation environment in referenced studies)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "no explicit discussion in the survey about minimum OpenSim fidelity required for transfer",
            "failure_cases": "none reported in the survey for OpenSim specifically",
            "uuid": "e1329.0",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "DFT-based custom simulator",
            "name_full": "Custom DFT (Density Functional Theory) based simulator / evaluator",
            "brief_description": "A DFT-based evaluation/simulation used as a physics-based reward/evaluator for molecular structure optimization tasks in RL (i.e. physically-correct structural prediction via DFT calculations).",
            "citation_title": "Physics-guided reinforcement learning for 3D molecular structures",
            "mention_or_use": "mention",
            "simulator_name": "Custom DFT-based calculation (DFT evaluator)",
            "simulator_description": "Custom simulation/evaluation using density functional theory calculations to provide physically-correct rewards/energies for molecular structure optimization.",
            "scientific_domain": "molecular physics / computational chemistry",
            "fidelity_level": "high-fidelity physics evaluator (quantum-chemistry level) as used for reward computation in referenced work",
            "fidelity_characteristics": "uses first-principles DFT calculations to evaluate candidate molecular configurations; exact computational settings not specified in the survey",
            "model_or_agent_name": null,
            "model_description": "RL agent for single-molecule 3D structure optimization (referenced works used DDPG and similar agents)",
            "reasoning_task": "optimize 3D molecular conformations / structures with physically-correct energy objective",
            "training_performance": null,
            "transfer_target": "not specified (DFT used as ground-truth physics-based reward during training/evaluation)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "survey notes DFT used as physically-correct reward but does not discuss minimal necessary fidelity",
            "failure_cases": "not reported in survey",
            "uuid": "e1329.1",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "COMSOL-based custom",
            "name_full": "Custom COMSOL-based simulator (multiphysics custom environment)",
            "brief_description": "A custom, COMSOL-based simulation environment cited in the survey as used by an autonomous manufacturing / physics-guided RL study (custom multiphysics simulation).",
            "citation_title": "A physics-guided reinforcement learning framework for an autonomous manufacturing system with expensive data.",
            "mention_or_use": "mention",
            "simulator_name": "Custom COMSOL-based simulator",
            "simulator_description": "Custom multiphysics simulation environment built on COMSOL (used in referenced work for domain-specific physics simulation in autonomous manufacturing experiments).",
            "scientific_domain": "multiphysics / possibly thermomechanics or acoustics depending on the study",
            "fidelity_level": "not explicitly quantified in the survey; referred to as a COMSOL-based custom high-fidelity multiphysics simulator",
            "fidelity_characteristics": "survey gives no low-level detail; being COMSOL-based implies PDE-level multiphysics modeling (but exact physics modules or discretization not described)",
            "model_or_agent_name": null,
            "model_description": "RL agent for autonomous manufacturing / policy optimization trained with COMSOL-simulated data in referenced work",
            "reasoning_task": "optimize manufacturing process parameters under physically-modelled constraints",
            "training_performance": null,
            "transfer_target": "target real-world manufacturing system (survey notes sim-to-real transfer was a goal in referenced work)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "survey does not state explicit minimal fidelity requirements; notes COMSOL used to generate realistic simulated data",
            "failure_cases": "no explicit failure cases reported in the survey for this COMSOL-based setup",
            "uuid": "e1329.2",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "IEEE distribution benchmarks",
            "name_full": "IEEE distribution system benchmarks (IEEE 9/33/141-bus etc.)",
            "brief_description": "Standard IEEE distribution and transmission test systems used as benchmarks for power/voltage control tasks in PIRL works (used for training/evaluation in power-system studies).",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "IEEE distribution system benchmarks (simulation instantiations)",
            "simulator_description": "Standard power-system benchmark networks (e.g., IEEE 9-bus, 33-bus, 141-bus) used to evaluate voltage/stability control RL algorithms; typically simulated in power-system toolchains or MATLAB/Simulink.",
            "scientific_domain": "electrical power systems / circuits",
            "fidelity_level": "not specified in survey; these benchmarks represent canonical test cases that can be simulated with detailed power-flow and dynamic models (fidelity depends on simulator used)",
            "fidelity_characteristics": "survey does not list specifics; possible inclusion of transient dynamics and standard component models when instantiated in MATLAB/SIMULINK or power-system tools",
            "model_or_agent_name": null,
            "model_description": "RL agents for voltage control / transient stability enhancement referenced in the survey",
            "reasoning_task": "control/stabilize power-system voltages and transient behavior",
            "training_performance": null,
            "transfer_target": "evaluation on IEEE benchmark cases (and in some works toward real systems), but survey does not report specific real-world transfer metrics",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "no explicit minimum-fidelity statement in the survey for IEEE benchmarks",
            "failure_cases": "none specifically reported in the survey",
            "uuid": "e1329.3",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "MATLAB/Simulink",
            "name_full": "MATLAB / Simulink custom simulations",
            "brief_description": "MATLAB/Simulink environments referenced as platforms for training/evaluating RL agents in control and power applications (e.g., power/grid control, district cooling modeling).",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "MATLAB / Simulink (custom models)",
            "simulator_description": "General-purpose engineering simulation environment used to implement power-system, thermal, and control models for RL training and evaluation in referenced works.",
            "scientific_domain": "control systems / power systems / thermodynamics (depending on the model implemented)",
            "fidelity_level": "not specified in survey; fidelity is model-dependent (from simple lumped models to detailed component-level dynamic simulations)",
            "fidelity_characteristics": "survey does not give specifics; MATLAB/Simulink often supports continuous-time dynamic models, control blocks, and specialized toolboxes for power/thermal dynamics",
            "model_or_agent_name": null,
            "model_description": "RL agents for control tasks (e.g., inverter PQ control, district cooling control) trained/evaluated using Simulink models in referenced studies",
            "reasoning_task": "control and regulation of power/thermal systems (e.g., voltage control, cooling system regulation)",
            "training_performance": null,
            "transfer_target": "target real-world devices/systems in cited works (survey notes Simulink used for training/evaluation but does not give numeric transfer results)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "no explicit minimal-fidelity conclusions provided in the survey regarding Simulink models",
            "failure_cases": "not reported in the survey",
            "uuid": "e1329.4",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Differentiable simulators (NimblePhysics / Redner / custom DS)",
            "name_full": "Differentiable physics simulators (e.g., NimblePhysics, Redner, custom differentiable simulators)",
            "brief_description": "Differentiable simulators are referenced as simulation engines that provide analytic or automatic gradients of simulation outcomes w.r.t. actions/parameters, enabling gradient-based policy optimization and model learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "Differentiable simulators (NimblePhysics, Redner, other DS)",
            "simulator_description": "Physics simulators adapted to be differentiable (provide gradients), used in PIRL for accelerating policy learning, model identification, and to support differentiable loss computation through physical simulation.",
            "scientific_domain": "mechanics / robotics / vision (depending on simulator capabilities)",
            "fidelity_level": "survey does not assign a single fidelity level; differentiable sims trade off features and performance â€” they are discussed as providing useful gradients rather than being defined by fidelity alone",
            "fidelity_characteristics": "survey notes DS provide analytic/auto-differentiation of simulation outcomes, allow many environments to run in parallel and can include rigid-body contacts and rendering (specific physical effects depend on the particular implementation like NimblePhysics/Redner)",
            "model_or_agent_name": null,
            "model_description": "policy networks or model-based components that leverage simulation gradients for improved optimization (referenced works include agents trained with DS)",
            "reasoning_task": "policy optimization and model identification using simulation gradients (e.g., policy improvement via differentiable simulation)",
            "training_performance": null,
            "transfer_target": "some referenced works aim for sim-to-real transfer; DS are used to refine models by comparing simulated and real observations",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "the survey highlights benefits of DS (gradient access) but does not prescribe minimal physics fidelity required for transfer",
            "failure_cases": "survey mentions DS have issues that some algorithms alleviate (e.g., Xu et al. propose methods to alleviate DS issues) but does not document concrete fidelity-failure cases in detail",
            "uuid": "e1329.5",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact)",
            "brief_description": "A widely used rigid-body physics simulator with contact dynamics commonly used in RL benchmarks for dynamic control and locomotion tasks; referenced frequently in the surveyed literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "Rigid-body dynamics simulator supporting articulated bodies and contact, commonly used for locomotion and control benchmarks (Ant, HalfCheetah, Humanoid, Walker2d, Cassie).",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "medium-fidelity rigid-body dynamics with contact; offers efficient simulation of articulated mechanisms but abstracts away detailed continuum physics",
            "fidelity_characteristics": "provides contact dynamics, joint dynamics, and actuator models, but not continuum/thermodynamic effects; survey lists MuJoCo as a standard benchmark platform",
            "model_or_agent_name": null,
            "model_description": "RL agents for locomotion and dynamic control (PPO, SAC, DDPG variants) trained in MuJoCo-based tasks",
            "reasoning_task": "learn dynamic control and locomotion policies (not explicit scientific reasoning tasks like thermodynamics or circuits)",
            "training_performance": null,
            "transfer_target": "some works use MuJoCo for sim-to-real experiments (survey mentions sim-to-real aims broadly), but no specific transfer metrics are given in the survey",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "survey does not discuss minimum MuJoCo fidelity needed for successful real-world transfer",
            "failure_cases": "survey does not report specific MuJoCo failure cases; it notes many works use customized environments instead of standard benchmarks",
            "uuid": "e1329.6",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "SUMO / CARLA (traffic/vehicle sims)",
            "name_full": "SUMO (Simulation of Urban MObility) and CARLA (CAR Learning to Act)",
            "brief_description": "Traffic micro/macro simulators cited as training and evaluation platforms for vehicular control and autonomous driving RL problems (ramp merging, connected automated vehicles).",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "SUMO and CARLA",
            "simulator_description": "SUMO: microscopic/macroscopic traffic flow simulator used for traffic control tasks; CARLA: high-fidelity urban driving simulator used for autonomous driving RL experiments.",
            "scientific_domain": "traffic dynamics / autonomous vehicles",
            "fidelity_level": "survey does not quantify fidelity; SUMO is typically a traffic flow microsimulator (modeling vehicle interactions), CARLA is a high-fidelity urban driving simulator (visual sensors, dynamics), but exact settings vary by study",
            "fidelity_characteristics": "survey does not give details; SUMO provides traffic flow and vehicle kinematics modeling, CARLA provides detailed urban environment, sensor simulation and vehicle dynamics",
            "model_or_agent_name": null,
            "model_description": "RL controllers for ramp metering, safe merging, and connected automated vehicle control",
            "reasoning_task": "traffic control, ramp merging, and safe autonomous driving policy learning",
            "training_performance": null,
            "transfer_target": "real-world driving/traffic control scenarios or other simulators; survey does not report quantitative transfer outcomes",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "no explicit statement about minimal fidelity required for transfer in the survey",
            "failure_cases": "survey notes many works use customized environments and that sim-to-real gap is a concern, but does not enumerate specific SUMO/CARLA failure cases",
            "uuid": "e1329.7",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Two-phase flow / thermal custom",
            "name_full": "Custom two-phase flow / thermal simulators (as used in heat/mass transfer and cooling papers)",
            "brief_description": "Custom simulations and physics models used in thermodynamics / fluid-dynamics PIRL studies (e.g., two-phase flow interfacial area prediction, district cooling, data-center thermal models).",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "Custom thermal / two-phase flow simulators (domain-specific custom models)",
            "simulator_description": "Custom physics models/simulators for fluid/thermal systems used to generate data and rewards for RL agents in heat/mass transfer and cooling control applications; implementations vary by study.",
            "scientific_domain": "thermodynamics / fluid dynamics",
            "fidelity_level": "not specified in the survey; typically these are domain-specific physics models (ranging from reduced-order/empirical to PDE-resolved CFD), survey does not specify which fidelity was used per paper",
            "fidelity_characteristics": "survey references inclusion of physical constraints (momentum equation, pressure Poisson, boundary conditions) in reward design for flow reconstruction, but does not detail solver fidelity, discretization, or turbulence modeling",
            "model_or_agent_name": null,
            "model_description": "RL agents for flow-field reconstruction, district cooling control, and two-phase flow interfacial area prediction",
            "reasoning_task": "predict/optimize thermal/fluid behavior (e.g., interfacial area in two-phase flow, cooling system regulation)",
            "training_performance": null,
            "transfer_target": "real systems or higher-fidelity models in referenced works (survey mentions sim-to-real and offline-online training strategies but gives no numeric transfer outcomes)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "the survey notes physics-informed models improve sample efficiency but does not state explicit minimal fidelity requirements for thermal/fluid tasks",
            "failure_cases": "no explicit documented failure cases in the survey for low-fidelity thermal/fluid simulators",
            "uuid": "e1329.8",
            "source_info": {
                "paper_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Real-world reinforcement learning via multifidelity simulators",
            "rating": 2,
            "sanitized_title": "realworld_reinforcement_learning_via_multifidelity_simulators"
        },
        {
            "paper_title": "Differentiable physics models for real-world offline model-based reinforcement learning",
            "rating": 2,
            "sanitized_title": "differentiable_physics_models_for_realworld_offline_modelbased_reinforcement_learning"
        },
        {
            "paper_title": "Sim-to-real transfer with neural-augmented robot simulation",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_with_neuralaugmented_robot_simulation"
        },
        {
            "paper_title": "A physics-guided reinforcement learning framework for an autonomous manufacturing system with expensive data.",
            "rating": 2,
            "sanitized_title": "a_physicsguided_reinforcement_learning_framework_for_an_autonomous_manufacturing_system_with_expensive_data"
        },
        {
            "paper_title": "Physics-guided reinforcement learning for 3D molecular structures",
            "rating": 2,
            "sanitized_title": "physicsguided_reinforcement_learning_for_3d_molecular_structures"
        },
        {
            "paper_title": "Towards stochastic modeling for two-phase flow interfacial area predictions: A physics-informed reinforcement learning approach",
            "rating": 2,
            "sanitized_title": "towards_stochastic_modeling_for_twophase_flow_interfacial_area_predictions_a_physicsinformed_reinforcement_learning_approach"
        },
        {
            "paper_title": "PhysQ: A Physics Informed Reinforcement Learning Framework for Building Control",
            "rating": 1,
            "sanitized_title": "physq_a_physics_informed_reinforcement_learning_framework_for_building_control"
        },
        {
            "paper_title": "Phyllis: Physics-Informed Lifelong Reinforcement Learning for Data Center Cooling Control",
            "rating": 1,
            "sanitized_title": "phyllis_physicsinformed_lifelong_reinforcement_learning_for_data_center_cooling_control"
        }
    ],
    "cost": 0.0205055,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Physics Informed Reinforcement Learning: Review and Open Problems</p>
<p>Member, IEEEC Banerjee 
Member, IEEEK Nguyen 
Senior Member, IEEEC Fookes 
Senior Member, IEEEM Raissi 
A Survey on Physics Informed Reinforcement Learning: Review and Open Problems
1Index Terms-Physics-informedReinforcement LearningMachine learningNeural NetworkDeep Learning âœ¦
The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e. observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.</p>
<p>INTRODUCTION</p>
<p>Through trial-and-error interactions with the environment, Reinforcement Learning (RL) offers a promising approach to solving decision-making and optimization problems. Over the past few years, RL has accomplished impressive feats in handling difficult tasks, in such domains as autonomous driving [119,16], locomotion control [99,129], robotics [71,94], continuous control [5,6,7], and multi-agent systems and control [39,15]. A majority of these successful approaches are purely data-driven and leverage trial-and-error to freely explore the search space. RL methods work well in simulations, but they struggle with real-world data because of the disconnection between simulated setups and the complexities of real world systems. Major RL challenges [33], that are consistently addressed in latest research includes sample efficiency [91,9], high dimensional continuous state and action spaces [34,118], safe exploration [41,48], multiobjective and well-defined reward function [65,10], perfect simulators and learned model [27,96] and policy transfer from offline pre-training [72,131].</p>
<p>When it comes to machine learning, incorporating mathematical physics into the models can lead to more meaningful solutions. This approach, known as physics-informed machine learning, helps neural networks learn from incomplete physics information and imperfect data more efficiently, resulting in faster training times and better â€¢ C. Banerjee generalization. Additionally, it can assist in tackling high dimensionality applications and ensure that the resulting solution is physically sound and follows the underlying physical law [60,8,52]. Among the various sub-fields of ML, RL is the natural candidate for incorporating physics information since most RL-based solutions deal with realworld problems and have an explainable physical structure. Recent research has seen substantial improvement in addressing the RL challenges by incorporating physics information in the training pipeline. For example, PIRL approaches seek to use physics to reduce high-dimensional continuous states with intuitive representations and better simulation. A low-dimensional representation adhering to physical model PDEs is learned in [45], while [12] uses features from a supervised surrogate model. Learning a good world model is a quicker and safer alternative to training RL agents in the real world. [103] incorporate physics into the network for better world models, and [128] utilize a highlevel specification robot morphology and physics for rapid model identification.</p>
<p>A well-defined reward function is crucial for successful reinforcement learning, PIRL approaches also seek to incorporate physical constraints into the design for safe learning and more efficient reward functions. For example, in [68] the designed reward incorporates IMU sensor data, imbibing inertial constraints, while in [75] the physics informed reward is designed to satisfy explicit operational targets. To ensure safe exploration during training and deployment, works such as [133,141] learn a data-driven barrier certificate based on physical property-based losses and a set of unsafe state vectors.</p>
<p>There are several lines of PIRL research dedicated to exploring more efficient exploration of the search space and effective policy deployment for real-world systems. Some approaches were developed to improve simulators for sample efficiency and better sim to real transfer [1,81]. Carefully selecting task-specific state representations [59,51], reward functions [13,14], and action spaces [124,141] has been shown to improve both the time to convergence and performance. To sum it up, integrating underlying physics about the learning task structure has been found to improve performance and accelerate convergence.</p>
<p>Physics-informed Reinforcement Learning (PIRL) has been a growing trend in the literature, as demonstrated in the increasing number of papers published in this area over the past six years, as shown in Figure 1. The bar chart indicates that this field is gaining more attention, and we can anticipate even more in the future. Our contributions in this paper are summarized as follows: 1) Taxonomy: We propose a unified taxonomy to investigate what physics knowledge/processes are modeled, how they are represented, and the strategies to incorporate them into RL approaches. 2) Algorithmic Review: We present state-of-the-art approaches on the physics information guided/ physics informed RL methods, using unified notations, simplified functional diagrams and discussion on latest literature. 3) Training and evaluation benchmark Review: We analyze the evaluation benchmarks used in the reviewed literature, thus presenting popular evaluation and benchmark platforms/ suites for understanding the popular trend and also for easy reference. 4) Analysis: We delve deep into a wide range of model based and model free RL applications over diverse domains. We analyze in detail how physics information is integrated into specific RL approaches, what physical processes have been modeled and incorporated, and what network architectures or network augmentations have been utilized to incorporate physics. 5) Open Problems: We summarize our perspectives on the challenges, open research questions and directions for future research. </p>
<p>Difference to other survey papers:</p>
<p>George et al. [60] provided one of the most comprehensive reviews on machine learning (ML) in the context of physics-informed (PI) methods, but approaches in the RL domain has not been discussed. The work by Hao et al. [52] also provided an overview of physics-informed machine learning, where the authors briefly touch upon the topic of PIRL. Another recent study by Eesserer et al. [35] showcased the use of prior knowledge to guide reinforcement learning (RL) algorithms, specific to robotic applications. The authors categorize knowledge into three types: expert knowledge, world knowledge, and scientific knowledge. Our paper offers a focused and comprehensive review specially on the RL approaches that utilize the structure, properties, or constraints unique to the underlying physics of a process/system. Our scope of application domains is not limited to robotics, but also spanning to motion control, molecular structure optimization, safe exploration, and robot manipulation. The rest of this paper is organized as follows. In Section Â§ 2, we provide a brief overview of the Physics informed ML paradigm. In Section Â§ 3, we present RL fundamentals/ framework in Â§ 3.1 and provide a definition with an intuitive introduction to PIRL in Â§ 3.2. Most importantly we introduce a comprehensive taxonomy in Â§ 3.3 threading together physics information types, PIRL methods that implement those information and RL pipeline as a backbone. Later in Â§ 3.4 we present and elaborate on two additional categories: Learning architecture and Bias, through which the implementation side of the literature is explained more precisely. In Section Â§ 4 we present an elaborate review and analysis of latest PIRL literature. In Section Â§ 5, we discuss the different open problems, challenges and research directions that may be addresses in future works by interested researchers. Finally Section Â§ 6 concludes the paper.</p>
<p>PHYSICS-INFORMED MACHINE LEARNING (PIML): AN OVERVIEW</p>
<p>The aim of PIML is to merge mathematical physics models and observational data seamlessly in the learning process. This helps to guide the process towards finding a physically consistent solution even in complex scenarios that are partially observed, uncertain, and high-dimensional [62,52,26]. Adding physics knowledge to machine learning models has numerous benefits, as discussed in [62,89]. This information captures the vital physical principles of the process being modeled and brings following advantages 1) Ensures that the ML model is consistent both physically and scientifically. 2) Increases data efficiency in model training, meaning that the model can be trained with fewer data inputs. 3) Accelerates the model training process, allowing models to converge faster to an optimal solution. 4) Increases the generalizability of trained models, enabling them to make better predictions for scenarios that were not seen during the training phase. 5) Enhances the transparency and interpretability of models, making them more trustworthy and explainable.</p>
<p>According to literature, there are three strategies for integrating physics knowledge or priors into machine learning models: observational bias, learning bias, and inductive bias.</p>
<p>Observational bias: This approach uses multi-modal data that reflects the physical principles governing their generation [82,61,77,132]. The deep neural network (DNN) is trained directly on observed data, with the goal of capturing the underlying physical process. The training data can come from various sources such as direct observations, simulation or physical equation-generated data, maps, or extracted physics data induction.</p>
<p>Learning bias: One way to reinforce prior knowledge of physics is through soft penalty constraints. This approach involves adding extra terms to the loss function that are based on the physics of the process, such as momentum or conservation of mass. An example of this is physics-informed neural networks (PINN), which combine information from measurements and partial differential equations (PDEs) by embedding the PDEs into the neural network's loss function using automatic differentiation [60]. Some prominent examples of soft penalty based approaches includes statistically constrained GAN [127], physics-informed auto-encoders [37] and encoding invariances by soft constraints in the loss function InvNet [110].</p>
<p>Inductive biases: Custom neural network-induced 'hard' constraints can incorporate prior knowledge into models. For instance, Hamiltonian NN [47] draws inspiration from Hamiltonian mechanics and trains models to respect exact conservation laws, resulting in better inductive biases. Lagrangian Neural Networks (LNNs) [25] introduced by Cranmer et al. can parameterize arbitrary Lagrangians using neural networks, even when canonical momenta are unknown or difficult to compute. Meng et al. [90] uses a Bayesian framework to learn functional priors from data and physics with a PI-GAN, followed by estimating the posterior PI-GAN's latent space using the Hamiltonian Monte Carlo (HMC) method. Additionally, DeepONets [82] networks are used in PDE agnostic physical problems.</p>
<p>PHYSICS-INFORMED REINFORCEMENT LEARN-ING: FUNDAMENTALS, TAXONOMY AND EXAMPLES</p>
<p>In this section, we will explain how physics information can be integrated into reinforcement learning applications. Figure 2: Agent-environment framework, of RL paradigm. Here the reward generating function and the system/ plant is abstracted as the environment. And the control policy (e.g. a DNN) and the learning algorithm, forms the RL agent.</p>
<p>RL fundamentals</p>
<p>RL algorithms use reward signals from the environment to learn the best strategy to solve a task through trial and error. They effectively solve sequential decision-making problems that follow the Markov Decision Process (MDP) framework. In the RL paradigm, there are two main players: the agent and the environment. The environment refers to the world where the agent resides and interacts. Through agent-environment interactions the agent perceives the state of the world and decides on the appropriate action to take.</p>
<p>The agent-environment RL framework, see Fig. 2, is a large abstraction of the problem of goal-directed learning from interaction [115]. The details of control apparatus, sensors, and memory are abstracted into three signals between the agent and the environment: the control/ action, the state and the reward. Though typically, the agents computes the rewards, but by the current convention anything that cannot be changed arbitrarily by the agent is considered outside of it and hence the reward function is shown as a part of the environment.</p>
<p>MDP is typically represented by the tuple (S, A, R, P, Î³), where S represents the states of the environment, A represents set of actions that the RL agent can take. Reward function may be typically represented as R(s t+1 , a t ) a function of next state and current action. The function generates the reward due to action induced state transition from s t to s t+1 . P (s t+1 |s t , a t ) is the environment model that returns the probability of transitioning to state s t+1 from s t . Finally the discount factor Î³ âˆˆ [0, 1], determines the amount of emphasis given to the immediate rewards relative to that of future rewards.</p>
<p>The RL framework typically organizes the agent's interactions with the environment into episodes. In each episode, the agent starts at a particular initial state s 1 sampled from an initial distribution p(s 1 ), which is part of the state space S of the MDP. At each timestep t, the agent observes the current state s t âˆˆ S and samples an action a t âˆˆ A from its latest policy Ï€ Ï• (a t |s t ) based on the state s t , where Ï• represents the policy parameters. The action space of the MDP is denoted by A. Next, the agent applies the action a t into the environment, which results in a new state s t+1 given by the dynamics of the MDP, i.e., s t+1 âˆ¼ p(s t+1 |s t , a t ). The agent also receives a reward r t = R(s t+1 , a t ), which can be construed as the desirability of a certain state transition from the context of the given task. The above process is repeated up to a certain time horizon T , which may also be infinite. The agent-environment interaction is recorded as a trajectory, and the closed-loop trajectory distribution for the episode t = 1, Â· Â· Â· , T can be represented by,
p Ï• (Ï„ ) =p Ï• (s 1 , a 1 , s 2 , a 2 , Â· Â· Â· , s T , a T , s T +1 )(1)
=p(s 1 ) T t=1 Ï€ Ï• (a t |s t )p(s t+1 |s t , a t ),</p>
<p>where Ï„ = (s 1 , a 1 , s 2 , a 2 , Â· Â· Â· , s T , a T , s T +1 ) represents the sequence of states and control actions. The objective is to find an optimal policy represented by the parameter,
Ï• * = arg max Ï• E Ï„ âˆ¼p Ï• (Ï„ ) T t=1 Î³ t R(a t , s t+1 ) J (Ï•) ,(3)
which maximizes the objective function J (Ï•), Î³ is a parameter called discount factor, where 0 â‰¤ Î³ â‰¤ 1. Î³ determines the present value of the future reward, i.e., a reward received at k timesteps in the future is worth only Î³ kâˆ’1 times what it would be worth if received immediately.</p>
<p>Model-free and model-based RL: In RL, algorithms can be classified based on whether the environment model is available during policy optimization. The environment dynamics are represented as p(s t+1 , r t ) = P r(s t+1 , r t |s t , a t ), which means that given a state and action, the environment model can predict the state transition and the corresponding reward. Access to an environment model allows the agent to plan and choose between options and also improves sample efficiency compared to model-free approaches. However, the downside is that the environment's groundtruth model is typically not available, and learning a perfect model of the real world is challenging. Additionally, any bias in the learned model can lead to good performance in the learned model but poor performance in the real environment.</p>
<p>Online, Off-policy and Offline RL: Online RL algorithms, e.g. PPO, TRPO, and A3C, optimize policies by using only data collected while following the latest policy, creating an approximator for the state or action value functions, used to update the policy. Off-policy RL algorithms, e.g. SAC, TD3 and IPNS, involve the agent updating its policy and other networks using data collected at any point during training. This data is stored in a buffer called the experience replay buffer and is in the form of tuples. Mini-batches are sampled from the buffer and used for the training process. Offline RL algorithms use a fixed dataset called D collected by a policy Ï€ Î¶ to learn the optimal policy. This allows for the use of large datasets collected previously.</p>
<p>Combining model-free/model-based with online/offpolicy/offline categorization, typical RL architectures can be presented as Fig. 3.</p>
<p>PIRL: Introduction</p>
<p>Definition</p>
<p>The concept of physics-informed RL involves incorporating physics structures, priors, and real-world physical variables into the policy learning or optimization process. Physics induction helps improve the effectiveness, sample efficiency and accelerated training of RL algorithms/ approaches, for complex problem-solving and real-world deployment. Depending on the specific problem or scenario, different physics priors can be integrated using various RL methods at different stages of the RL framework, see Fig. 4.</p>
<p>Intuitive introduction to physics priors in RL</p>
<p>Physics priors come in different forms, like intuitive physical rules or constrains, underlying mathematical/ guiding equations and physics simulators, to name a few. Here we discuss a couple of intuitive examples. In [128], the physical characteristics of the system were utilized as priors. The high-level specifications of a robot's morphology such as the number and connectivity structure of links were used as physics priors. This feature based representation of the system dynamics enabled rapid model identification in this model based RL setup. In another example, pertaining to adaptive cruise control problem, [59] (see Fig.5), physics information in the form of "jam-avoiding distance" (based on desired physical parameters e.g. velocity and acceleration constraints, minimum jam avoiding distance etc.) is included in state space input to the RL agent. Physics info. incorporation results in a RL controller which performs with less collisions and enables more equidistant travel. Figure 5: An illustrative example of physics incorporation in RL application, [59]. Here the RL agent is fed with an additional state variable: jam avoiding distance, which is based on desired physical parameters and primary state variables.</p>
<p>PIRL Taxonomy</p>
<p>Physics information (types): representation of physics priors</p>
<p>There are different types/ forms of physics information, e.g. mathematical representation of the physical system like PDE/ODE and physics enriched simulators. Based on the type of the physics information representation, works can be typically categorized as follows.</p>
<p>1) Differential and algebraic equations (DAE): Many works use system dynamics representations, such as partial/ordinary differential equations (PDE/ ODE) and boundary conditions (BC), as physics priors primarily through PINN and other special networks. For example in transient voltage control application [40], a PINN is trained using PDE of transient process. The PINN learns a physical constraint which it transfers to the loss term of the RL algorithm. 2) Barrier certificate and physical constraints (BPC): It is imperative to regulate agent exploration in safety-critical applications of reinforcement learning. One way it is addressed in recent research is through the use of optimization-based control theoretic constraints. Use of concepts like control Lyapunov function (CLF) [74,23], barrier certificate/ barrier function (BF), control barrier function/ certificate (CBF/ CBC) [19,11] is made in recent safety critical RL applications. Barrier certificate is generally used to establish a safe set of desired states for a system. A control barrier function is then employed to devise a control law that keeps the states within the safety set. In certain scenarios barrier functions are represented as NNs and learned through data driven approaches [141,140]. In above control theoretic approaches the system dynamics either partial or learnable and safety sets represent the primary physical information. For more details on CBFs refer [2]. Additionally safety in the learning process may also be ensured by incorporating physical constraints into the RL loss [76,17]. 3) Physics parameters, primitives and physical variables (PPV):</p>
<p>Physics values extracted/ derived from the environ-ment or system has been directly used by RL agents in form of physics parameters [113], dynamic movement (physics) primitives [3], physical state [59] and physical target [75]. For example in [75], the reward is created to meet two physical objectives/ targets: operation cost and self-energy sustainability. In an adaptive cruise control problem [59], authors use desired physical parameters e.g. velocity and acceleration constraints and minimum jam avoiding distance, as a state space input. 4) Offline data and representation (ODR): For the improvement simulator based training, especially during simto-real transfer, non-task-specific-policy data collected from real robot has been used to train RL agents in offline setting along with simulators [46] and as hardware data to seed simulators [81]. Another popular way of extracting physics information from environment is learning physically relevant low dimensional representation from observations [45,12]. For example, in [45], PINN is used to extract physically relevant information about the hidden state of the system, which is further used to learn a Q-function for policy optimization. 5) Physics simulator and model (PS): Simulators provide a easy way of experimenting with RL algorithms without exposing the agent e.g. a robot to the wear and tear of the real environment.</p>
<p>Apart from serving as test-beds for RL algorithms, simulators are also used alongside RL algorithms to impart physical correctness or physics awareness in the data or training process. For example in order to improve motion capture and imitation of given motion clips, [20] have used rigid body physics simulations to solve the rigid body poses closely following the motion capture clip. In [42], using a physics simulator, a residual agent is able to learn how to improve user input in order to achieve a task while staying true to the original input and expert-recorded trajectories.</p>
<p>In the MBRL setting the system model can be: 1) completely known, 2) partially known or 3) completely unknown. RL algorithms typically addresses the last two types, since it deals with environments whose dynamics is complex and difficult to ascertain through classical approaches. In such cases a DNN based datadriven approach is generally utilized to learn the system model completely or enrich the existing partial or basic model of the environment. In [51] a data driven surrogate traffic flow model is learned that generates synthetic data. This data is later used by the agent in an offline learning process, followed by an online control process. In [103] learns environment and reward models by using Lagrangian NNs [25]. LNNs are models that are able to Lagrangian functions straight from data gathered from agent-environment interactions. 6) Physical properties (PPR): Fundamental knowledge regarding the physical structure or properties pertaining to a system has been used in a number of works. For example system morphology, system symmetry [54] 3.3.2 PIRL methods: physics prior augmentations to RL PIRL methods highlights and discusses about the different components of the typical RL paradigm e.g. state space, action space, reward function and agent networks (policy and value function N/W), that has been directly modified/ augmented through the incorporation of physics information. 1) State design: This category is concerned with the observed state space of the environment or model. The PIRL approaches, typically modifies or expands the state representation in order to make it more instructive. Works include state fusion using additional information from environment [59] and other agents [112], state as extracted features from robust representation [12], learned surrogate model generated data as state [51] and state constraints [138]. 2) Action regulation: This pertains to modifying the action value, which is often achieved through PIRL approaches that impose constraints on the action value to ensure safety protocols are implemented [76,19]. 3) Reward design: It concerns approaches that induce physics information through effective reward design or augmentation of existing reward functions with bonuses or penalties [28,83]. 4) Augment policy or value N/W: These PIRL approaches incorporate physics principles via methods like, adjusting the update rules and losses of the policy [4,87], value functions [93,98] and making direct changes to their underlying network structure [14]. Works with novel physics based losses [92,130] and constraints for policy or value function learning [40] are also included. 5) Augment simulator or model: This category encompasses those works that develops improved simulators through incorporation of underlying physics knowledge thereby allowing for more accurate simulation of real-world environments. Works include physics based augmentation of DNN based learnable models for accurate system model learning [70,103], improved simulators for sim-to-real transfer [46,81] and physics informed learning for partially known environment model [78].</p>
<p>RL Pipeline</p>
<p>A typical RL pipeline can be represented into four functional stages namely, the problem representation, learning strategy, network design, training and trained policy deployment. These stages are elaborated as follows: 1. Problem Representation: In this stage, a real-world problem is modeled as a Markov Decision Process (MDP) and thereby described using formal RL terms. The main challenge is to choose the right observation vector, define the reward function, and specify the action space for the RL agent so that it can perform the specified task properly.  Here the problem is first modeled as a MDP, clearly defining the state, action and reward spaces. Followed by selecting the RL algorithm as Learning strategy and then selecting/ designing the policy and/or value networks in network design stage. Finally the agent is trained using default/ custom loss function in training stage and finally deployed.</p>
<p>Training:</p>
<p>The policy and allied networks are trained in this stage. It also represents training augmentation approaches like Sim-to-real, that helps is reducing discrepancy between simulated and real worlds. 5. Trained policy deployment: At this stage the policy is completely trained and is deployed for solving the concerned task. </p>
<p>Further categorization</p>
<p>In this section we introduce a couple of additional categorizations: Bias and Learning architecture. These categories are not part of the taxonomy that we have discussed in the previous section, see Fig.7. They provide an additional perspective to the PIRL approaches presented here.</p>
<p>Bias</p>
<p>PI approaches in ML paradigm, mentions of different kind of biases or categories of methods of physics incorporation in ML models. In order to relate to that existing taxonomy used in PIML methods, in Table 2 and Table 3., we include corresponding bias categories to each of the PIRL entries.</p>
<p>Learning architecture</p>
<p>We also categorize PIRL algorithms based on the alterations that they introduce to the conventional RL learning architecture to incorporate physics information/ priors. As listed and discussed below they help us understand the PIRL methods from an architectural point of view. In the literature review section we use the aid of such learning architecture categories to group and discuss the PIRL methods.</p>
<p>1) Safety filter: This category includes approaches that has a PI based module which regulates the agent's exploration ensuring safety constraints, for reference see Fig. 8(a). In this typical architecture the safety-filter module takes action a t from RL agent Ï€ Ï† , and state information (s t ) and refines the action, givingÃ£ t . 2) PI reward: This category includes approaches where physics information is used to modify the reward function, see Fig.8(b) for reference. Here the PI-reward module augments agent's extrinsic reward (r t ) with a physics information based intrinsic component, giving r t . 3) Residual learning: Residual RL is an architecture which typically consists of two controllers: a human designed controller and a learned policy [58]. In PIRL setting the architecture consists of a physics informed controller Ï€ Ïˆ along with the data-driven DNN based policy Ï€ Ï† , called residual RL agent, see Fig. 8(c). 4) Physics embedded network: In this category physics information e.g. system dynamics is directly incorporated in the policy or value function networks, see Fig.8(d) for reference. 5) Differentiable simulator: Here the approaches have use differentiable physics simulators, which are nonconventional/ or adapted simulators and explicitly provides loss gradients of simulation outcome w.r.t. control action, see Fig.8(e) for reference. 6) Sim-to-Real: In Sim-to-real architecture, the agent is first trained on a simulator or source domain and is later transferred to a target domain for deployment. In certain cases the transfer is followed by fine-tuning at the target domain, see erence. In a hierarchical RL (HRL) setting a long horizon decision making task is broken into simpler subtasks autonomously. In curriculum learning a complex task is solved by learning to solve a series of increasingly difficult tasks. In both HRL and CRL physics is typically incorporated into all the policy (including meta and sub-policies) and value networks. Approaches here are mostly extensions of physicsembedded networks (Fig.8(d)), as used in non-HRL/ CRL settings. 9) Data augmentation: This category includes approaches where the input state is replaced with a different or augmented form of it, e.g. low dimensional representation so as to derive special and physically relevant features out of it. See Fig.8(i) for reference. In this typical architecture, the state vector s t+1 is transformed into an augmented representation z t+1 . Physically relevant features are then extracted from it and used by the RL agent (Ï€ Ï† ). 10) PI model identification: This architecture represents those PIRL approaches, especially in data-driven MBRL setting where physics information is directly incorporated into the model identification process. For reference see Fig.8(j).</p>
<p>PIRL: REVIEW AND ANALYSIS</p>
<p>In this section we provide a indepth review of latest works in PIRL, followed by a review of the popular datasets. We also include an analysis of the algorithms and their derivatives, and discuss crucial insights.</p>
<p>Algorithmic review</p>
<p>We provide a detailed overview of the PIRL approaches as identified by our literature review in Table 2 and Table 3. We have structured our discussion according to the methods of the introduced taxonomy (see Â§ 3.3) since they form a bridge between the physics information sources and practical applications. We also use learning architecture categories as introduced in 3.4.2, to better explain the PIRL methods.</p>
<p>State design:</p>
<p>Vehicular traffic control applications have used physics priors to design the state representations. While controlling connected automated vehicles (CAVs), [112] proposed the use of surrounding information from downstream vehicles and roadside geometry, by embedding them in the state representation, see Fig. 10. The physics-informed state fusion approach integrates received information as DRL state (input features) i.e. for the i th CAV, DRL state is given
as s t i = e t i , Ï• t i , Î´q âˆ’t i , Î´d âˆ’t i , k t i ,
which are deviation values, (from left): lateral, angular, weighed equilibrium spacing and speed, and road curvature information.</p>
<p>Jurj et al. [59] makes use of physical information like jamavoiding distance to train RL agent, in order to improve collision avoidance of vehicles with adaptive cruise control. In ramp metering control, [51] utilizes an offline-online policy training process, where the offline training data consists of historical data and synthetic data generated from a physical traffic flow model. Figure 9: Taxonomy, the diagram connects PI types with PIRL methods and then to the RL pipeline backbone. The connection thickness represents the quantity of work done which corresponds to those components/ categories. Figure 10: Example of state design, through physics incorporation. Distributed control framework for connected automated vehicles [112]. Here information from downstream vehicles and roadway geometry information are incorporated as physics prior knowledge through state fusion.</p>
<p>In [12] a physics informed graphical representationenabled, global graph attention (GGAT) network is trained to model power flow calculation process. Informative features are then extracted from the GGAT layer (as representation N/W) and transferred used in the policy training process. While [45], uses PINNs based on thermal dynamics of buildings for learning better heating control strategies. Dealing with aircraft conflict resolution problem, [139] composed intruder's information e.g. speed and heading angle into an image state representation. This image now constitutes of the physics prior and serves as the input feature for RL based learning. In [138], the authors proposed a safe reinforcement learning algorithm using barrier functions for distributed MPC nonlinear multi-robot systems, with state constraints. [95], incorporates trained model alongside control barrier certificates, which restrict policies and prohibits exploration of the RL agent into certain undesirable sections of the state space. In case of a safety breach due to nonstationarity, the Lyapunov stability conditions ensures the re-establishment of safety. Figure 11: Example of action regulation, using physics priors. In [141], a barrier certification system receives RL control policy generated control actions and refines them sequentially using a barrier certificate to satisfy operational constraints.</p>
<p>Action regulation:</p>
<p>Many safety critical applications have used physics based constraints and other information in action regulation. These kind of approaches can be categorized under shielded RL/ safety filter, where a type of safety shield or barrier function is employed to check the actions. For safe power system control [141] proposes a framework for learning a stabilizing controller that satisfies predefined safety regions, see Fig. 11. Combining a model-free controller and a barrier-certification system, using a NN based barrier function, i.e. neural barrier certificate (NBC). Given a training set they learn a NBC B Ïµ (x) and filtered (regulated) control action F Ïˆ u , jointly holding the following condition
(âˆ€x âˆˆ S 0 , B Ïµ (x) â‰¤ 0) âˆ§ (âˆ€x âˆˆ S u , B Ïµ (x) &gt; 0) âˆ§(âˆ€x âˆˆ x|B Ïµ (x) = 0, L f (x,u RL ) B Ïµ (x) &lt; 0) where L f (x,u RL ) B Ïµ (x)
is the Lie derivative of B Ïµ (x), and Ï•, Ïµ are NN parameters. S 0 , S u are set of initial states and unsafe states respectively. [19] introduces a hybrid approach of MFRL and MBRL using CBF, with provision of online learning of unknown system dynamics. It assumes availability of a set of safe states. In a MARL setting, [11] introduced cooperative and non-cooperative CBFs in a collision-avoid problem, which includes both cooperative agents and obstacles. Also in MARL setting, [17] proposed efficient active voltage controller of photovoltaics (PVs) enabled with shielding mechanism. Which ensures safe actions of battery energy storage systems (BESSs) during training. [136] deals with controlling a district cooling system (DCS), with complex thermal dynamic model and uncertainties from regulation signals and cooling demands. The proposed safe controller a hybrid of barrier function and DRL and helps avoid unsafe explorations and improves training efficiency. [76] proposed a safe RL framework for adaptive cruise control, based on a safety-supervision module. The authors used the underlying system dynamics and exclusion-zone requirement to construct a safety set, for constraining the learning exploration.</p>
<p>In a highway motion planning setting for autonomous vehicles [124] proposed a CBF-DRL hybrid approach. Certain works like [13] and [14] have introduced multiple physics based artifacts to ensure safe learning in autonomous agents. Both of them used residual control based architecture merging physical model and data driven control. Additionally it also leverages physics model guided reward. [14] extends the work by [13] and introduces physics model guided policy and value network editing in addition to the physics based reward. In [32], the authors integrate learning a task space policy with a model based inverse dynamics controller, which translates task space actions into joint-level controls. This enables the RL policy to learn actions in task space.</p>
<p>Reward design:</p>
<p>In sim-to-real setting [113] proposed a reward specification framework based on composing probabilistic periodic costs on basic forces and velocities, see Fig. 12. The framework defines a parametric reward function for common robotic (bipedal) gaits. Dealing with periodic robot behavior, the absolute time reward function is here defined in terms of a cycle time variable Ï• (which cycles over time period of [0, 1], as R(s, Ï•). The updated reward function as given below, is defined as a biased sum of n reward components R i (s, Ï•), each capturing a desired robot gait characteristic. Ï•) is a product of phase-coefficient c i , phase indicator I i (Ï•) and phase reward measurement q i (s).
R(s, Ï•) = Î² + Î£R i (s, Ï•), where R i (s, Ï•) = c i Ã— I i (Ï•) Ã— q i (s) each R i (s,
In [18], the authors introduced a RL-PIDL hybrid framework, to learn MFGs, which generalize well and manage Figure 12: Example of physics incorporation in reward design. In [113] a reward function design framework was introduced, that describe robot gaits as a periodic phase sequence such that each of which rewards or penalizes a particular physical system measurement.</p>
<p>can complex multi-agent systems applications. The physics based reward component (= evolution of population density/ mean-field state) is approximated using PINN. To better mimic natural human locomotion [68], designed reward function based on physical and experimental information: trajectory optimization rewards, and bio-inspired rewards. In a similar task of imitation of human motion but from motion clip, [20] proposes a physics-based controller using DRL. A rigid body physics simulator is used to solve rigid body poses that closely follows the motion capture (mocap) clip frames. In a similar work [100], a data driven RL framework was introduced for training control policies for simulated characters. Refernce motions are used to define imitation reward and the task goal defines task specific reward.</p>
<p>[75] leverages a federated MADRL approach for energy management in multi-microgrid settings. The reward is designed to satisfy two physical targets: operation cost and self energy sufficiency. [135] proposed a DRL based method for reconstruction of flow fields from noisy data. Physical constraints like momentum equation, pressure Poisson equation and boundary conditions are used for designing the reward function. [134] proposed physics based reward shaping for wireless navigation applications. They used a cost function augmented with physically motivated costs like costs for link-state monotonicity, for angle of arrival direction following, and for SNR increasing. In single molecule 3D structure optimization problem, [22] used physics based DFT calculation is used as reward function, for physically correct structural prediction. In [74], the authors used temporal logic through a finite state automata (FSA), control Lyapunov and barrier function for ensuring effective and safe RL in complex environments.The FSA simultaneously provides rewards, objectives and safety constraints to the framework components.</p>
<p>Addressing the problem of dexterous manipulation of objects in virtual environments, [42] trained the agent in a residual setting using hybrid model-free RL-IL approach. Using a physics simulator and a pose estimation reward the agent learns to refine the user input to achieve a task while keeping the motion close to the input and the expert demonstrations. [83] tackles physically valid 3D pose estimation from egocentric video. The authors utilized a combination of kinematics and dynamics approach, whereby the residual of the action against a learned kinematics model is outputted by the dynamics-based model. In [56],the authors proposed inclusion of physics based intrinsic reward for improved policy optimization of RL algorithms.</p>
<p>In the context of predicting interfacial area in two-phase flow, [28] proposed. The two-phase flow physics information is infused into the underlying MDP framework, which is then uses RL strategies to describe behavior of flow dynamics. The work introduces multiple rewards based on physical interfacial area transport models, other physical parameters and data. In a work concerning optimization of nuclear fuel assembly [101], the authors introduce a reward shaping approach in RL optimization, which is based on physical tactics used by fuel designers. These tactics include moving fuel rods in assembly to meet certain constraints and objectives.</p>
<p>A number of works have used physics through multiple PIRL methods. Apart from reward design they have infused physics information through state design [112] and action regulation [14,13,124]. They have been discussed in previous sections and hence not repeated. In MBRL setting, using structure of underlying physics, and building upon Lagrangian neural network (LLN) [25], Ramesh et al. [103] learned the system model via datadriven approach, see Fig. 13. Concerning systems obeying Lagrangian mechanics, the state consists of generalized coordinates q and velocitiesq. Lagrangian, which is a scalar is defined as</p>
<p>Augment simulator or model:
L(q,q, t) = T (q,q) âˆ’ V(q)
where T (q,q) is kinetic energy and V(q) is the potential energy. And so the Lagrangian equation of motion can be written as Ï„ = M (q)q + C(q,q)q + G(q), wherÃ«
q = M âˆ’1 (q)(Ï„ âˆ’ C(q,q)q âˆ’ G(q))
where C(q,q)q is Coriolis term, G(q) is gravitational term and Ï„ is motor torque. In the NN implementation, separate networks are used for learning V(q) and L(q), leveraging which the acceleration (q) quantity is generated. The output state derivative (q,q) is then integrated using 2 nd -order Runge-Kutta to compute next state. Concerning a sim-to-real setting, in [46] authors train a recurrent neural network on the differences between robotic trajectories in simulated and actual environments. This model is further used to improve the simulator. For improved transfer to real environment, [81] collected hardware data (positions and calculated system velocities) to seed the simulator, for training control policies. [1] proposes a framework for autonomous manufacturing of acoustic meta-material, while leveraging physics informed RL and transfer learning. A physics guided simulation engine is used to train the agent in source task and then fine-tuned in a data-driven fashion in the target task.</p>
<p>[88] introduced a PINN based gravity model for training of dynamically informed RL agents. [106] uses surrogate models that capture primary physics of the system, as a starting point of training DRL agent. In a curriculum learning setting, they train an agent to first track limit cycles in a velocity space for a representative non-holonomic system and then further trained on a small simulation dataset. [128] combines linear dynamic models of physical systems with optimism driven exploration. Here the features for the linear models obtained from robot morphology and the exploration is done using MPC.</p>
<p>A number of works introduced novel models are better representations of real world physics and serves as better simulators and ensures effective sim to real transfers. [108] introduced learnable physics models which supports accurate predictions and efficient generalization across distinct physical systems. Concerning dynamic control with partially known underlying physics (governing laws), [78] proposed a physics informed learning architecture, for environment model. ODEs and PDEs serves as the primary source of physics for these models. [121] uses entity abstraction to integrate graphical models, symbolic computation and NNs in a MBRL agent. The framework presents object-centric perception, prediction and planning which helps agents to generalize to physical tasks not encountered before. [70] proposes a context aware dynamics model which is adaptable to change in dynamics. They break the problem of learning the environment dynamics model into two stages: learning context latent vector and predicting next state conditioned on it.</p>
<p>In micro-grid power control problem, [111] combines model-based analytical proof and reinforcement learning. Here model-based derivations are used to narrow the learning space of the RL agen, reducing training complexity significantly. In visual model based RL, [121] models a scene in terms of entities and their local interactions, thus better generalizing to physical task the learner has not seen before. Similar to learning entity abstractions, in [70] the authors tackles the challenge of learning a generalizable global model through: learning context latent vector, capturing local dynamics and predicting next state conditioned on the encoded vector. Addressing dynamic control problem in MBRL setting, [78] leveraged physical laws (in form of canonical ODE/ PDE) and environmental constraints to mitigate model bias issue and sample inefficiency. In autonomous driving safe ramp merging problem, [120] embedded probabilistic CBF in RL policy in order to learn safe policies, that also optimize the performance of the vehicle. Typically CBFs need good approximation of car's model. Here the probabilistic CBF is used as an estimate of the model uncertainty. [22] incorporates physics through reward design as well as through simulator augmentation, and has been discussed in previous section. Figure 14: Example, augmentation of policy using physics information. In [4], given an observation s t from the environment, a neural dynamic policy generates w i.e. the weights of basis function and g which is a goal for the robot, for a function f Î¸ . This function is then used by an open loop controller to generate a set of actions from the robot to execute in the environment and collect next states and rewards to train the policy.</p>
<p>Augment policy and/or value N/W:</p>
<p>In [4], Bahl et al. proposes Neural Dynamic Policies (NDP) where they incorporate dynamical system as a differentiable layer in the policy network, see Fig. 14. In NDP, a NN Î¦ takes an input state (s t ) and predicts parameters of the dynamical system (i.e. (w, g) ). Which are then used to solve second-order differential equationÃ¿ = Î±(Î²(g âˆ’ y) âˆ’(y)) + f (x), to obtain system states (y,áº,Ã¿)., which represents the behavior of the dynamic system, given a state goal g. Here Î±, Î² are global parameters allowing critical damping of system and f is a non-linear forcing function which primarily captures the shape of trajectory. Depending on robot's coordinate system an inverse controller may also be used to convert y to a, i.e. a = â„¦(y,áº,Ã¿) . The NDPs thus can be defined as Ï€(a|s; Î¸) â‰œ â„¦(DE(Î¦(s; Î¸))), where DE(w, g) â†’ {y,áº,Ã¿} here DE(w, g) represents solution of the differential equation.</p>
<p>Extending this work to hierarchical deep policy learning framework, [3] introduced H-NDP which forms a curriculum by learning local dynamical system-based policies on small state-space region and then refines them into global dynamical system based policy. Given the accurate dynamics and constraint of the system [140] introduces control barrier certificates into actor-critic RL framework, for learning safe policies in dynamical systems. [87] proposes a method for generating highly agile and visually guided locomotion behaviors. They leverage MFRL while using model based optimization of ground reaction forces, as a behavior regularizer.</p>
<p>In [31] proposes an approach of safe exploration using CLBF without explicitly employing any dynamic model. The approach approximate the RL critic as a CLBF, from data samples and parameterized with DNNs. Both the actor and critic satisfies reachability and safety guarantees. [93] combines PINN with RL, where the value function is treated as a PINN to solve Hamilton-Jacobi-Bellman (HJB) PDE. It enables the RL algorithm to exploit the physics of environment aswell as optimal control to improve learning and convergence. [98] proposes an optimization method for freeform nanophotonic devices, by combining adjoint based methods (ABM) and RL. In this work the value network is initialized with adjoint gradient predicting network during initialization of RL process. Cao et al. [14] have used physics model to influence reward function, as well as edit policy and value networks as necessary. The work has been mentioned before in reward design.</p>
<p>To improve policy optimization, [92] used differentiable simulators to directly compute the analytic gradient of the policy's value function w.r.t. the actions generated by it. This gradient information is used to monotonically improve the policy's value function. Gao et al. [40] proposes a transient voltage control approach, by integrating physical and data-driven models of power system. They also uses the constraint of the physical model on the data-driven model to speed up convergence. A PINN trained using PDE of transient process acts as the physical model and contributes directly to the loss of the RL algorithm.</p>
<p>Xu et al. [130] presents an efficient differentiable simulator (DS) with a new policy training algorithm which can effectively leverage simulation gradients. The learning algorithm alleviates issues inherent in DS while allowing many physical environments to be run in parallel. [17] incorporates physics through action regulation and penalty signal to agent, and has been discussed in previous section.</p>
<p>In MBRL setting, [85] leverage differentiable physicsbased simulation and differentiable rendering. By comparing raw observations between simulated and real world, the initial learned system model is continually updated, producing a more physically consistent model. In data center (DC) cooling control application, [123] proposed a lifelong-RL approach under evolving DC environment. It leverages physical laws of thermodynamics and the system and models the DC thermal transition and power usage through data collected online. Utilizing learned state transition and reward models it accelerates online adaptation.</p>
<p>Working with a nominal system model, [23] presented an RL framework where the agent learns model uncertainty in multiple general dynamic constraints, e.g. CLF and CBF, through data-driven training. A quadratic program then solves for the control that satisfies the safety constraints under learned model uncertainty. </p>
<p>Review of simulation/ evaluation benchmarks</p>
<p>In Table 4, we present the different training and evaluation benchmarks that has been used in the reviewed PIRL literature. We list the important insights from the table:</p>
<ol>
<li>A majority works dealing with dynamic control have used OpenAI Gym [128], Safe Gym [133], MuJoCo [121,142], Pybullet [31] and Deep mind control suite environments [108,103], which are standard benchmarks in RL . Works dealing specifically with traffic management have used platforms like SUMO [124] and CARLA [120]. 2. Works dealing with power and voltage management problems have used IEEE distribution system benchmarks [17,40] to evaluate proposed algorithms. Alternatively in some works MATLAB/ SIMULINK plat-form is also used for training or evaluating RL agents [111] 3. One crucial observation is that a huge number of work have used customized or adapted environments for training and evaluation and have not used conventional environments [74,24,84].</li>
</ol>
<p>Analysis</p>
<p>Research trend and statistics</p>
<p>Use of RL algorithms: As is evident from Fig.15 (a), PPO [109] and its variants are the most preferred RL algorithm, followed by DDPG [114]. Among the comparatively new algorithms SAC [49] is preferred over TD3 [38]. Types of physics priors used: In Fig.15 (b), we can see that physics information takes the form of physics simulator, system models, barrier certificates and physical constraints, in a majority of works. PI types "Barrier certificate constraints and physical constraint" and "Physics simulator and models" dominates in more that 60% of works in "Action regulation" and "Augment policy and value N/W" PIRL methods.</p>
<p>Learning architecture and bias: In Fig.15 (c) we visualize the relationship between PIRL learning architectures (sec: 3.4.2) and the three biases through which physics is typically incorporated in PIML approaches. In architectures "PI reward" and "safety filter", physics is incorporated strictly through "learning bias", signifying the heavy use of constraints, regularizers and specialized loss functions. While "Physics embedded network" incorporates physics information through "inductive bias", i.e. through imposition of hard constraints through use specialized and custom physics embodied networks. Fig.15 (d) almost 85% of the application problem dealt with PIRL approaches relates to controller or policy design. "Miscellaneous control" includes optimal policy/ controller learning approaches for different application sectors like energy management [75,111] and data-center cooling [123], and accounts to majority of applications. "Safe control and exploration", includes those works concerning with safety critical systems, ensuring safe exploration and policy learning, accounts for 25%. "Dynamic control", includes control of dynamic systems, including robot systems and amounts to about 23% of all works surveyed. Other specific applications include optimization/ prediction [22,28], motion capture/simulation [124,20] and improvement of general policy optimization approaches [46,81] through physics incorporation.</p>
<p>Application domains: In</p>
<p>RL challenges addressed</p>
<p>In this section we will discuss and elaborate on how recent physics incorporation in RL algorithms have addressed certain open problems of the RL paradigm.</p>
<p>1) Sample efficiency: RL approaches need a huge number of agent-environment interaction and related data to work. One effective way of dealing with this problem is to use a surrogate for the real environment in the form of a simulator or learned model via data-driven approaches. PIRL approaches incorporate physics to augment simulators thus reducing the sim-to-real gap, thereby bringing down online evaluation cycles [85,1]. Also physics incorporation during system identification or model learning phase in MBRL help reduce sample efficiency through learning a truer to real environment using lesser training samples [108,121]. 2) Curse of dimensionality: RL algorithms become less efficient both in training and performing on environment defined with high-dimensional and continuous state and action spaces, known as the 'curse of dimensionality'. Typically dimensionality reduction techniques are used to encode the large state or action vectors into low dimensional representations. The RL algorithm is then trained in this low dimensional setting. PIRL approaches extract underlying physics information from environment through learning physically relevant low dimensional representation from high dimensional observation or state space [45,12]. In [45], a PINN is utilized to extract physically relevant information about the system's hidden state, which is then used to learn a Q-function for policy optimization. 3) Safety exploration: Safe reinforcement learning involves learning control policies that guarantee system per- formance and respect safety constraints during both exploration and policy deployment.</p>
<p>In safety-critical applications using reinforcement learning, it's crucial to regulate agent exploration. Control Lyapunov function (CLF) [74,23], barrier certificate/ barrier function (BF), control barrier function/ certifi-cate (CBF/ CBC) [19,11] are commonly used concepts. Barrier certificates define safe states, while control barrier functions ensure states stay in the safety set. These approaches are typically used for systems with partial or learnable dynamics model and generally a known set of safe states/ actions. 4) Partial observability or imperfect measurement: Partial observability is a setting where due to noise, missing information, or outside interference, an RL agent is unable to obtain the complete states needed to understand the environment. PIRL approaches modify or enhance the state representation to provide more useful information, in cases of missing or inadequate information. This may involve state fusion, which incorporates additional physics or geographical information from the environment [59] or other agents [112]. 5) Under-defined reward function: Defining the reward function is critical in creating MDPs and ensuring the effectiveness and efficiency of RL algorithms. However, since they are created by humans, there is a risk of them being under-defined and not guiding the RL algorithm effectively in policy optimization. PIRL approaches introduce physics information through effective reward design or augmentation of existing reward functions with bonuses or penalties [28,83,42,113]. For example, in a sim-to-real setting, [113] proposed a framework for specifying rewards that combines probabilistic costs associated with primary forces and velocities. The framework creates a parametric reward function for common robotic gaits, in biped robots.</p>
<p>OPEN CHALLENGES AND RESEARCH DIREC-</p>
<p>TIONS</p>
<p>High Dimensional Spaces</p>
<p>A large number of real world tasks deals with high dimensional and continuous state and action spaces. One popular method to address this high dimensionality issue is to compress the state space (or action space) vectors into low dimensional vectors. A PI based approach may learn high quality environment representations using deep networks and extract physically relevant low dimensional features from them. But learning a compressed and informative latent space from high dimensional continuous state (or action) space still remains a hurdle. Also learning physically relevant representation is still an open problem. Future research should address this issue and try to devise approaches that helps to incorporate or take guidance of underlying physics during representation learning or feature extraction, so as to make them both informative and physically pertinent.</p>
<p>Safety in Complex and Uncertain Environments</p>
<p>In the realm of safe reinforcement learning, striking a balance between the complexity of the environment and ensuring safety is always a challenge. Current physics informed approaches uses different control theoretic concepts e.g. CBFs to ensure safe exploration and learning of the RL  [20,100,83] agent. But these approaches are limited by the approximated model of the system and the prior knowledge about safe state sets. There has been a lot of research for better system identification or model learning through physics incorporation. But most works do not generalize well to different tasks and environments. To summarize, future works should address these crucial research goals: 1) model agnostic safe exploration and control using RL agents in complex and uncertain environments and 2) devise generalized approach of incorporating physics in data-driven Model learning.</p>
<p>Choice of physics prior</p>
<p>Choice of the physics prior is very crucial for the PIRL algorithm. But such choice is difficult and requires extensive study of the system and may vary extensively from one case to another even in same domains. To enhance efficacy, devising a comprehensive framework with physics information to manage novel physical tasks is preferable rather than dealing with tasks individually.</p>
<p>Evaluation and bench-marking platform</p>
<p>Currently, PIRL doesn't have comprehensive benchmarking and evaluation environments to test and compare new physics approaches before induction. This limitation makes it challenging to assess the quality and uniqueness of new works. Additionally, most PIRL works rely on customized environments related to a particular domain, making it difficult to compare PIRL algorithms fairly. Moreover, PIRL application cases are diverse, and the physics information chosen is specific to a domain, requiring extensive study and domain expertise to understand and compare such works.</p>
<p>CONCLUSIONS</p>
<p>This paper presents a state-of-the-art reinforcement learning paradigm, known as physics-informed reinforcement learning (PIRL). By leveraging both data-driven techniques and knowledge of underlying physical principles, PIRL is capable of improving the effectiveness, sample efficiency and accelerated training of RL algorithms/ approaches, for complex problem-solving and real-world deployment. We have created two taxonomies that categorize conventional PIRL methods based on physics prior/information type and physics prior induction (RL methods), providing a framework for understanding this approach. To help readers comprehend the physics involved in solving RL tasks, we have included various explanatory images from recent papers and summarized their characteristics in Tables 2 and 3. Additionally, we have provided a benchmark-summary table 4 detailing the training and evaluation benchmarks used for PIRL evaluation. Our objective is to simplify the complex concepts of existing PIRL approaches, making them more accessible for use in various domains. Finally, we discuss the limitations and unanswered questions of current PIRL work, encouraging further research in this area.</p>
<p>ACKNOWLEDGMENT</p>
<p>Figure 1 :
1PIRL papers published over years. This statistic graph the exponential growth of PIRL papers over last six years.</p>
<p>Figure 3 :
3Typical RL architectures, based on model use and interaction with the environment.</p>
<p>Figure 4 :
4Map of physics incorporation (PI) in the conventional Reinforcement Learning (RL) framework.</p>
<p>2 .
2Learning strategy: In this stage, the decisions are made regarding the type of agent-environment interaction e.g. in terms of environment model use, learning architecture and the choice of RL algorithm. 3. Network design: Here the finer details of the learning framework are decided and customized where needed. Decisions are made regarding the type of constituent units (e.g. layer types, network depth etc.) of underlying Policy and value function networks.</p>
<p>Figure 6 :
6Deep Reinforcement Learning Pipeline.</p>
<p>Figure 7 :
7PIRL taxonomy and further categories. Physics information (types), the RL methods that incorporate them and the underlying RL pipeline constitutes the PIRLtaxonomy, seeFig. 9. bias (sec. 3.4.1) and Learning architecture (sec. 3.4.2) are two additional categories which has been introduced to better explain the implementation of PIRL.</p>
<p>Fig. 8 (Figure 8 :
88f) for reference. 7) Physics variable: This architecture encompasses all those approaches where physical parameters, variables or primitives are introduced to augment components (e.g. states and reward) of the RL framework. For reference see Fig.8(g). 8) Hierarchical RL: This category includes hierarchical and curriculum learning based approaches, Fig.8(h) for ref-Typical RL architectures with physics information incorporation (a) Safety filter (b) PI-reward (c) Residual agent (d) Physics embedded network (e) Differentiable simulator (f) Sim-to-Real (g) Physics variable (h) Hierarchical RL (i) Data augmentation (j) PI model identification. To keep illustrations simple, we have not included ancillary networks e.g. value function networks above.</p>
<p>Figure 13 :
13Example, augmentation of learnable model using physics information. The figure shows system dynamics learning network structured using a LNN[103] and next state calculations using Ralston's method. Here the PINN (LNN) based dynamics model and reward model , are learned via data-driven method.</p>
<p>Figure 15 :
15Statistical analysis of PIRL literature. (a) Statistic of type of RL algorithms used, (b) Statistic of PI types used in each PIRL method, (c) Statistic of PIRL learning architectures and related biases, (d) Statistic of PIRL applications in different domains.</p>
<p>, K. Nguyen, and C. Fookes are with Queensland University of Technology, Australia. Maziar Raissi is with University of Colorado Boulder, USA. E-mail: {c.banerjee, k.nguyenthanh, c.fookes}@qut.edu.au, maziar.raissi@colorado.edu</p>
<p>Table 1 :
1A list of abbreviations used in this article.Abbreviations </p>
<p>FSA 
Finite State Automata 
FEA 
Finite Element Analysis 
CFD 
Computational Fluid Dynamics 
MDP 
Markov Decision Process 
MBRL 
Model based Reinforcement Learning 
MFRL 
Model Free Reinforcement Learning 
CBF 
Control Barrier Function 
CBC 
Control Barrier Certificate 
NBC 
Neural Barrier Certificate 
CLBF 
Control Lyapunov Barrier Function 
NBC 
Neural Barrier Certificate 
DFT 
Density Functional Theory 
AC 
Actor Critic 
MPC 
Model Predictive Control 
DDP 
Differential Dynamic Programming 
NPG 
Natural Policy Gradient 
TL 
Temporal Logic 
DMP 
Dynamic Movement Primitive 
WBTG 
Whole Body Trajectory Generator 
DPG 
Deterministic Policy Gradient 
DPPO 
Distributed proximal Policy optimization 
ABM 
Adjoint based method 
APG 
Analytic Policy Gradient 
WBIC 
Whole Body Impulse Controller 
LNN 
Lagrangian Neural Network </p>
<p>Table 2 :
2Summary of PIRL literature -Model Free.Ref. 
Year 
Context/ Application 
RL Algorithm 
Learning arch. 
Bias 
Physics information 
PIRL methods 
RL pipeline </p>
<p>[20] 
2018 
Motion capture 
PPO 
Physics reward 
Learning 
Physics simulator 
Reward design 
Problem representation 
[100] 
2018 
Motion control 
PPO [109] 
Physics reward 
Learning 
Physics simulator 
Reward design 
Problem representation 
[46] 
2018 
Policy optimization 
PPO 
Sim-to-Real 
Observational 
Offline data 
Augment simulator 
Training 
[81] 
2018 
Policy optimization 
NPG [126] (C)  *<br />
Sim-to-Real 
Observational 
Offline data 
Augment simulator 
Training </p>
<p>[22] 
2019 
Molecular structure optimization 
DDPG 
Physics reward 
Learning 
DFT (PS) 
Reward design 
Problem representation 
Augment simulator 
Training 
[74] 
2019 
Safe exploration and control 
PPO 
Residual RL 
Learning 
CBF, CLF, FSA/TL (BPC) 
Reward design 
Problem representation 
Augment policy 
Learning strategy </p>
<p>[4] 
2020 
Dynamic system control 
PPO 
Phy. embed. N/W 
Inductive 
DMP (PPV) 
Augment policy 
Network design 
[42] 
2020 
Dexterous manipulations 
PPO 
Residual RL 
Observational 
Physics simulator 
Reward design 
Problem representation 
[83] 
2020 
3D Ego pose estimation 
PPO 
Physics reward 
Learning 
Physics simulator 
State, Reward design 
Problem representation </p>
<p>[3] 
2021 
Dynamic system control 
PPO 
Hierarchical RL 
Inductive 
DMP (PPV) 
Augment policy 
Network design 
[87] 
2021 
Dynamic system control 
PPO 
Hierarchical RL 
Learning 
WBIC (PPV) 
Augment policy 
Learning strategy 
[1] 
2021 
Manufacturing 
SARSA [116] 
Sim-to-Real 
Observational 
Physics engine 
Augment simulator 
Training 
[113] 
2021 
Dynamic system control 
PPO 
Phy. variable 
Learning 
Physics parameters 
Reward design 
Problem representation 
[76] 
2021 
Safe exploration and control 
NFQ [104] 
Safety filter 
Learning 
Physical constraint 
Action regulation 
Problem representation 
[59] 
2021 
Safe cruise control 
SAC 
Phy. variable 
Observational 
Physical state (PPV) 
State design 
Problem representation 
[92] 
2021 
Policy optimization 
DPG (C) 
Diff. Simulator 
Learning 
Physics simulator 
Augment policy 
Learning strategy 
[101] 
2021 
Optimization, nuclear engineering 
DQN, PPO 
Physics reward 
Learning bias 
Physical properties (PPR) 
Reward design 
Problem representation 
[139] 
2021 
Air-traffic control 
PPO 
Data augmentation 
Observational 
Representation (ODR) 
State design 
Problem representation </p>
<p>[124] 
2022 
Motion planner 
PPO + AC [67] 
Safety filter 
Learning 
CBF (BPC) 
Action regulation 
Problem representation 
Reward design 
[17] 
2022 
Active voltage control 
TD3 (C) 
Safety filter 
Learning 
Physical constraints 
Penalty function 
Problem representation 
Action regulation 
[28] 
2022 
Interfacial structure prediction 
DDPG 
Off-policy 
Learning 
Physics model 
Reward design 
Problem representation 
[40] 
2022 
Transient voltage control 
DQN 
PINN loss 
Learning 
PDE (DAE) 
Augment policy 
Learning strategy 
[45] 
2022 
Building control 
Q-learning (C) 
Data augment 
Observational 
Representation (ODR) 
State design 
Problem representation 
[51] 
2022 
Traffic control 
Q-Learning 
Data augment 
Observational 
Physics model 
State design 
Problem representation 
[88] 
2022 
Safe exploration and control 
SAC 
Sim-to-Real 
Observational 
Physics model 
Augment simulator 
Training 
[56] 
2022 
Dynamic system control 
SAC (etc.) 
Physics reward 
Learning 
Barrier function 
Reward design 
Problem representation 
[130] 
2022 
Policy Learning 
Actor-critic (C) 
Diff. Simulator 
Learning 
Physics simulator 
Augment policy 
Learning strategy </p>
<p>[13] 
2023 
Safe exploration and control 
DDPG 
Residual RL 
Learning 
Physics model 
Reward design 
Problem representation 
Action regulation 
[14] 
2023 
Safe exploration and control 
DDPG 
Residual RL 
Inductive 
Physics model 
Reward design 
Problem representation 
Action regulation 
Inductive 
N/W editing (Aug. pol.) 
Network design 
[12] 
2023 
Robust voltage control 
SAC 
Data augment 
Observational 
Representation (ODR) 
State design 
Problem representation 
[18] 
2023 
Mean field games 
DDPG 
Physics reward 
Learning 
Physics model 
Reward design 
Problem representation 
[133] 
2023 
Safe exploration and control 
PPO (C) 
Safety filter 
Learning 
NBC (BPC) 
Augment policy 
Training 
[141] 
2023 
Power system stability enhancement 
Custom 
Safety filter 
Learning 
NBC (BPC) 
Action regulation 
Problem representation 
[31] 
2023 
Safe exploration and control 
AC (C) 
Safety filter 
Learning 
CLBF [107, 29] (BPC) 
Augment value N/W 
Training 
[112] 
2023 
Connected automated vehicles 
DPPO 
Physics variable 
Observational 
Physical state (PPV) 
State design 
Problem representation 
Learning 
Reward design 
[68] 
2023 
Musculoskeletal simulation 
SAC (C) 
Physics variable 
Learning 
Physical value 
Reward design 
Problem representation 
[75] 
2023 
Energy management 
MADRL(C) 
Physics variable 
Learning 
Physical target 
Reward design 
Problem representation 
[93] 
2023 
Policy optimization 
PPO 
Phy. embed N/W 
Inductive 
PDE (DAE) 
Augment value N/W 
Network design 
[135] 
2023 
Flow field reconstruction 
A3C 
Physics reward 
Learning 
Physical constraints 
Reward design 
Problem representation 
[98] 
2023 
Freeform nanophotonic devices 
Ïµâˆ’ greedy Q 
Phy. embed N/W 
Inductive 
ABM 
Augment value N/W 
Network design 
[106] 
2023 
Dynamic system control 
DPG 
Curriculum learning 
Learning 
Physics model 
Augment simulator 
Training 
[111] 
2023 
Energy management 
TD3 
Sim-to-Real 
Observational 
Physics model 
Augment simulator 
Learning strategy 
[134] 
2023 
Robot wireless navigation 
PPO 
Physics reward 
Learning 
Physical value 
Reward design 
Problem representation </p>
<p>C  *  represents custom versions of the adjacent conventional algorithms. </p>
<p>Table 3 :
3Summary of PIRL literature -Model based C * represents custom versions of the adjacent conventional algorithms.Ref. 
Year 
Context/ Application 
Algorithm 
Learning arch. 
Bias 
Physics information 
PIRL method 
RL pipeline </p>
<p>[128] 
2016 
Exploration and control 
-
Model learning 
Observational 
Sys. morphology (PPR) 
Augment model 
Learning strategy </p>
<p>[108] 
2018 
Dynamic system control 
-
Model learning 
Inductive 
Physics model 
Augment model 
Learning strategy 
[95] 
2019 
Safe navigation 
-
Safety filter 
Learning 
CBC (BPC) 
Action regulation 
Problem representation 
[19] 
2019 
Safe exploration and control 
TRPO, DDPG 
Residual RL 
Learning 
CBF (BPC) 
Action regulation 
Problem representation </p>
<p>[121] 
2020 
Control (visual RL) 
-
Model learning 
Observational 
Entity abstraction (ODR) 
Augment model 
Learning strategy 
[70] 
2020 
Dynamic system control 
-
Model learning 
Observational 
Context encoding (ODR) 
Augment model 
Learning strategy 
[23] 
2020 
Safe exploration and control 
DDPG [114] 
Safety filter 
Learning 
CBF, CLF, QP (BPC) 
augment policy 
Learning strategy </p>
<p>[78] 
2021 
Dynamic system control 
Dyna + TD3(C)  *<br />
Model identification 
Learning 
PDE/ ODE, BC (DAE) 
Augment model 
Learning strategy 
[32] 
2021 
Dynamic system control 
PPO 
Residual-RL 
Learning 
Physics model 
Action regulation 
Problem representation 
[11] 
2021 
Multi agent collision avoidance 
MADDPG (C) 
Safety filter 
Learning 
CBF (BPC) 
Action regulation 
Problem representation </p>
<p>[85] 
2022 
Dynamic system control 
TD3(C) 
Sim-to-Real 
Learning 
Physics simulator 
Augment policy 
Learning strategy 
[120] 
2022 
Traffic control 
AC[86] 
Safety filter 
Learning 
CBF (BPC) 
Augment model 
Learning strategy 
[140] 
2022 
Safe exploration and control 
DDPG 
Safety filter 
Learning 
CBC (BPC) 
Augment policy 
Learning strategy 
[138] 
2022 
Distributed MPC 
AC [57] 
Safety filter 
Learning 
CBF (BPC) 
State design 
Problem representation </p>
<p>[103] 
2023 
Dynamic system control 
Dreamer [50] 
Phy. embed. N/W 
Inductive 
Physics model 
Augment model 
Network design 
[24] 
2023 
Safe exploration and control 
-
Safety filter 
Learning 
CBF (BPC) 
Augment model 
Learning strategy 
[54] 
2023 
Attitude control 
-
Phy. embed N/W 
Inductive 
System symmetry (PPR) 
Augment model 
Network design 
[123] 
2023 
Data center cooling 
SAC 
Model identification 
Learning 
Physics laws (PPR) 
Augment model 
Learning strategy 
[136] 
2023 
Cooling system control 
DDPG 
Residual RL 
Learning 
CBF (BPC) 
Action regulation 
Problem representation </p>
<p>Table 4 :
4Summary of PIRL training/ evaluation benchmarks. Humanoid standup, Swimmer, Hopper Inverted and Inverted Double Pendulum (v4) Pybullet Franka Panda, Flexiv Rizon (also real world robots) [85] NimblePhysics[125], Redner[73] (Differentiable sim.)Simulator/ platform 
Specific environment/ system name 
Reference 
OpenAI Gym 
Pusher, Striker, ErgoReacher 
[46] 
OpenAI Gym 
Mountain Car, Lunar Lander (continuous) 
[56] 
OpenAI Gym 
Cart-Pole, Pendulum (simple and double) 
[128] 
OpenAI Gym 
Cart-pole 
[13] 
OpenAI Gym 
Cart-pole and Quadruped robot 
[14] 
OpenAI Gym 
CartPole, Pendulum 
[78] 
OpenAI Gym 
Inverted Pendulum (pendulum âˆ’ v0), 
[19] 
OpenAI Gym 
Mountain car (cont.), Pendulum, Cart pole 
[140] 
OpenAI Gym 
Simulated car following [53] 
MuJoCo 
Ant, HalfCheetah, Humanoid, Walker2d 
[93] 
MuJoCo 
Cassie-MuJoCo-sim [105] 
[113, 32] 
6 DoF Kinova Jaco [44] 
[4, 3] 
MuJoCo 
HalfCheetah, Ant, 
[70] 
CrippledHalfCheetah, and SlimHumanoid [142] 
MuJoCo 
Block stacking task [55] 
[121] 
OpenAI Gym 
CartPole, Pendulum 
OpenSim-RL[64] 
L2M2019 environment 
[68] 
Safety gym [137] 
Point, car and Doggo goal 
[133] 
-
Cart pole swing up, Ant 
[130] 
-
Humanoid, Humanoid MTU 
-
Autonomous driving system 
[18] 
Deep control suite [117] 
Pendulum, Cartpole, Walker2d 
[108] 
Acrobot, Swimmer, Cheetah 
-
JACO arm (real world) 
Deep control suite 
Reacher, Pendulum, Cartpole, 
[103] 
Cart-2-pole, Acrobot, 
Cart-3-pole and Acro-3-bot 
-
Rabbit[21] 
[23] 
MARL env. [80] 
Multi-agent particle env. 
[11] 
ADROIT[102] 
Shadow dexterous hand 
[42] 
-
First-Person Hand Action Benchmark[43] 
MuJoCo 
Door opening, in-hand manipulation, 
tool use and object relocation 
SUMO[79], METANET[69] 
-
[51] 
SUMO 
-
[124] 
CARLA[30] 
-
[120] 
Gazebo[66] 
Quadrotor (IF 750A) 
[54] 
IEEE Distribution 
IEEE 33-bus and 141-bus distribution networks 
[17] 
system benchmarks 
IEEE 33-node system 
[12, 17] 
IEEE 9-bus standard system 
[40] 
-
Custom (COMSOL based) 
[1] 
-
Custom (DFT based) 
[22] 
-
Custom (based on [122]) 
[45] 
-
Custom (based on [63]) 
[59] 
-
Custom 
[75, 88, 28] 
-
Custom 
[98, 112, 134] 
-
Custom 
[135, 136, 141] 
-
Custom 
[74, 24, 84] 
-
Custom 
[123, 18] 
Open AI Gym 
Custom (based on geometries of Nuclear reactor) 
[101] 
MATLAB-Simulink 
Custom 
[111, 138] 
-
Custom [143] 
[92] 
MATLAB 
Cruise control 
[76] 
Pygame 
Custom 
[139] 
-
Custom (Unicycle, Car-following) 
[36] 
-
Brushbot, Quadrotor (sim) 
[95] 
Phantom manipulation platform 
[81] 
Pybullet 
2 finger gripper 
gym-pybullet-drones[97] 
[31] 
-
Custom MOCAP </p>
<p>This research was partly supported by the Advance Queensland Industry Research Fellowship AQIRF024-2021RD4.
A physics-guided reinforcement learning framework for an autonomous manufacturing system with expensive data. Alam Md Ferdous, 2021Md Ferdous Alam et al. "A physics-guided rein- forcement learning framework for an autonomous manufacturing system with expensive data." In: 2021</p>
<p>American Control Conference (ACC). 2021. American Control Conference (ACC). 2021, pp. 484-490.</p>
<p>Control barrier functions: Theory and applications. D Aaron, Ames, 18th European control conference (ECCAaron D Ames et al. "Control barrier functions: The- ory and applications." In: 2019 18th European control conference (ECC). 2019, pp. 3420-3431.</p>
<p>Hierarchical neural dynamic policies. Shikhar Bahl, Abhinav Gupta, Deepak Pathak, arXiv:2107.05627arXiv preprintShikhar Bahl, Abhinav Gupta, and Deepak Pathak. "Hierarchical neural dynamic policies." In: arXiv preprint arXiv:2107.05627 (2021).</p>
<p>Neural dynamic policies for endto-end sensorimotor learning. Shikhar Bahl, Advances in Neural Information Processing Systems. 33Shikhar Bahl et al. "Neural dynamic policies for end- to-end sensorimotor learning." In: Advances in Neural Information Processing Systems 33 (2020), pp. 5058- 5069.</p>
<p>Boosting Exploration in Actor-Critic Algorithms by Incentivizing Plausible Novel States. Chayan Banerjee, Zhiyong Chen, arXiv:2210.00211arXiv preprintand Nasimul NomanChayan Banerjee, Zhiyong Chen, and Nasimul No- man. "Boosting Exploration in Actor-Critic Algo- rithms by Incentivizing Plausible Novel States." In: arXiv preprint arXiv:2210.00211 (2022).</p>
<p>Improved soft actor-critic: Mixing prioritized off-policy samples with on-policy experiences. Chayan Banerjee, Zhiyong Chen, IEEE Transactions on Neural Networks and Learning Systems. and Nasimul NomanChayan Banerjee, Zhiyong Chen, and Nasimul No- man. "Improved soft actor-critic: Mixing prioritized off-policy samples with on-policy experiences." In: IEEE Transactions on Neural Networks and Learning Systems (2022).</p>
<p>Optimal Actor-Critic Policy With Optimized Training Datasets. Chayan Banerjee, IEEE Transactions on Emerging Topics in Computational Intelligence. 6Chayan Banerjee et al. "Optimal Actor-Critic Policy With Optimized Training Datasets." In: IEEE Trans- actions on Emerging Topics in Computational Intelligence 6.6 (2022), pp. 1324-1334.</p>
<p>Physics-Informed Computer Vision: A Review and Perspectives. Chayan Banerjee, arXiv:2305.18035eess.IVChayan Banerjee et al. Physics-Informed Computer Vi- sion: A Review and Perspectives. 2023. arXiv: 2305 . 18035 [eess.IV].</p>
<p>Distributed distributional deterministic policy gradients. Gabriel Barth-Maron, arXiv:1804.08617arXiv preprintGabriel Barth-Maron et al. "Distributed distribu- tional deterministic policy gradients." In: arXiv preprint arXiv:1804.08617 (2018).</p>
<p>The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications. Serena Booth, AAAI Conference on Artificial Intelligence. 37Serena Booth et al. "The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications." In: AAAI Conference on Artificial Intelligence. Vol. 37. 5. 2023, pp. 5920-5929.</p>
<p>Safe multi-agent reinforcement learning through decentralized multiple control barrier functions. Zhiyuan Cai, arXiv:2103.12553arXiv preprintZhiyuan Cai et al. "Safe multi-agent reinforcement learning through decentralized multiple control bar- rier functions." In: arXiv preprint arXiv:2103.12553 (2021).</p>
<p>Physics-informed Graphical Representation-enabled Deep Reinforcement Learning for Robust Distribution System Voltage Control. Di Cao, IEEE Transactions on Smart Grid. Di Cao et al. "Physics-informed Graphical Representation-enabled Deep Reinforcement Learning for Robust Distribution System Voltage Control." In: IEEE Transactions on Smart Grid (2023).</p>
<p>Physical Deep Reinforcement Learning Towards Safety Guarantee. Hongpeng Cao, arXiv:2303.16860arXiv preprintHongpeng Cao et al. "Physical Deep Reinforce- ment Learning Towards Safety Guarantee." In: arXiv preprint arXiv:2303.16860 (2023).</p>
<p>Physical Deep Reinforcement Learning: Safety and Unknown Unknowns. Hongpeng Cao, arXiv:2305.16614arXiv preprintHongpeng Cao et al. "Physical Deep Reinforce- ment Learning: Safety and Unknown Unknowns." In: arXiv preprint arXiv:2305.16614 (2023).</p>
<p>Off-policy learning for adaptive optimal output synchronization of heterogeneous multiagent systems. Ci Chen, Automatica. 119Ci Chen et al. "Off-policy learning for adaptive opti- mal output synchronization of heterogeneous multi- agent systems." In: Automatica 119 (2020), p. 109081. ISSN: 0005-1098.</p>
<p>Model-free deep reinforcement learning for urban autonomous driving. Jianyu Chen, Bodi Yuan, Masayoshi Tomizuka, 2019 IEEE intelligent transportation systems conference (ITSC). Jianyu Chen, Bodi Yuan, and Masayoshi Tomizuka. "Model-free deep reinforcement learning for urban autonomous driving." In: 2019 IEEE intelligent trans- portation systems conference (ITSC). 2019, pp. 2765- 2771.</p>
<p>Physics-Shielded Multi-Agent Deep Reinforcement Learning for Safe Active Voltage Control with Photovoltaic/Battery Energy Storage Systems. Pengcheng Chen, IEEE Transactions on Smart Grid. Pengcheng Chen et al. "Physics-Shielded Multi- Agent Deep Reinforcement Learning for Safe Active Voltage Control with Photovoltaic/Battery Energy Storage Systems." In: IEEE Transactions on Smart Grid (2022).</p>
<p>A Hybrid Framework of Reinforcement Learning and Physics-Informed Deep Learning for Spatiotemporal Mean Field Games. Xu Chen, Shuo Liu, Xuan Di, 2023 International Conference on Autonomous Agents and Multiagent Systems. Xu Chen, Shuo Liu, and Xuan Di. "A Hybrid Framework of Reinforcement Learning and Physics- Informed Deep Learning for Spatiotemporal Mean Field Games." In: 2023 International Conference on Autonomous Agents and Multiagent Systems. 2023, pp. 1079-1087.</p>
<p>End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. Richard Cheng, AAAI conference on artificial intelligence. 33Richard Cheng et al. "End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks." In: AAAI conference on artificial intelligence. Vol. 33. 01. 2019, pp. 3387-3395.</p>
<p>Physics-based motion capture imitation with deep reinforcement learning. Nuttapong Chentanez, 11th ACM SIGGRAPH Conference on Motion, Interaction and Games. Nuttapong Chentanez et al. "Physics-based motion capture imitation with deep reinforcement learning." In: 11th ACM SIGGRAPH Conference on Motion, Inter- action and Games. 2018, pp. 1-10.</p>
<p>Rabbit: A testbed for advanced control theory. Christine Chevallereau, IEEE Control Systems Magazine. 23Christine Chevallereau et al. "Rabbit: A testbed for advanced control theory." In: IEEE Control Systems Magazine 23.5 (2003), pp. 57-79.</p>
<p>Physics-guided reinforcement learning for 3D molecular structures. Youngwoo Cho, Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS). Youngwoo Cho et al. "Physics-guided reinforcement learning for 3D molecular structures." In: Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS). 2019.</p>
<p>Reinforcement learning for safetycritical control under model uncertainty, using control lyapunov functions and control barrier functions. Jason Choi, arXiv:2004.07584arXiv preprintJason Choi et al. "Reinforcement learning for safety- critical control under model uncertainty, using con- trol lyapunov functions and control barrier func- tions." In: arXiv preprint arXiv:2004.07584 (2020).</p>
<p>Safe exploration in model-based reinforcement learning using control barrier functions. H Max, Calin Cohen, Belta, In: Automatica. 147110684Max H Cohen and Calin Belta. "Safe exploration in model-based reinforcement learning using con- trol barrier functions." In: Automatica 147 (2023), p. 110684.</p>
<p>Lagrangian neural networks. Miles Cranmer, arXiv:2003.04630arXiv preprintMiles Cranmer et al. "Lagrangian neural networks." In: arXiv preprint arXiv:2003.04630 (2020).</p>
<p>Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next. Salvatore Cuomo, arXiv:2201.05624arXiv preprintSalvatore Cuomo et al. "Scientific Machine Learn- ing through Physics-Informed Neural Networks: Where we are and What's next." In: arXiv preprint arXiv:2201.05624 (2022).</p>
<p>Real-world reinforcement learning via multifidelity simulators. Mark Cutler, J Thomas, Jonathan P Walsh, How, IEEE Transactions on Robotics. 31Mark Cutler, Thomas J Walsh, and Jonathan P How. "Real-world reinforcement learning via multifidelity simulators." In: IEEE Transactions on Robotics 31.3 (2015), pp. 655-671.</p>
<p>Towards stochastic modeling for two-phase flow interfacial area predictions: A physics-informed reinforcement learning approach. Zhuoran Dang, Mamoru Ishii, International Journal of Heat and Mass Transfer. 192122919Zhuoran Dang and Mamoru Ishii. "Towards stochas- tic modeling for two-phase flow interfacial area pre- dictions: A physics-informed reinforcement learning approach." In: International Journal of Heat and Mass Transfer 192 (2022), p. 122919.</p>
<p>Safe nonlinear control using robust neural lyapunov-barrier functions. Charles Dawson, PMLR. 2022Conference on Robot Learning. Charles Dawson et al. "Safe nonlinear control using robust neural lyapunov-barrier functions." In: Con- ference on Robot Learning. PMLR. 2022, pp. 1724-1735.</p>
<p>CARLA: An open urban driving simulator. Alexey Dosovitskiy, Conference on robot learning. Alexey Dosovitskiy et al. "CARLA: An open urban driving simulator." In: Conference on robot learning. PMLR. 2017, pp. 1-16.</p>
<p>Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions. Desong Du, arXiv:2305.09793arXiv preprintDesong Du et al. "Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Func- tions." In: arXiv preprint arXiv:2305.09793 (2023).</p>
<p>Learning task space actions for bipedal locomotion. Helei Duan, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021. Helei Duan et al. "Learning task space actions for bipedal locomotion." In: 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021, pp. 1276-1282.</p>
<p>Challenges of realworld reinforcement learning: definitions, benchmarks and analysis. Gabriel Dulac-Arnold, Machine Learning. 110Gabriel Dulac-Arnold et al. "Challenges of real- world reinforcement learning: definitions, bench- marks and analysis." In: Machine Learning 110.9 (2021), pp. 2419-2468.</p>
<p>Deep reinforcement learning in large discrete action spaces. Gabriel Dulac-Arnold, arXiv:1512.07679arXiv preprintGabriel Dulac-Arnold et al. "Deep reinforcement learning in large discrete action spaces." In: arXiv preprint arXiv:1512.07679 (2015).</p>
<p>Guided Reinforcement Learning: A Review and Evaluation for Efficient and Effective Real-World Robotics. Julian EeÃŸerer, IEEE Robotics &amp; Automation Magazine. Julian EEÃŸerer et al. "Guided Reinforcement Learn- ing: A Review and Evaluation for Efficient and Ef- fective Real-World Robotics." In: IEEE Robotics &amp; Automation Magazine (2022).</p>
<p>Safe reinforcement learning using robust control barrier functions. Yousef Emam, IEEE Robotics and Automation Letters. 99Yousef Emam et al. "Safe reinforcement learning using robust control barrier functions." In: IEEE Robotics and Automation Letters 99 (2022), pp. 1-8.</p>
<p>Physics-informed autoencoders for Lyapunov-stable fluid flow prediction. Michael N Benjamin Erichson, Michael W Muehlebach, Mahoney, arXiv:1905.10866arXiv preprintN Benjamin Erichson, Michael Muehlebach, and Michael W Mahoney. "Physics-informed autoen- coders for Lyapunov-stable fluid flow prediction." In: arXiv preprint arXiv:1905.10866 (2019).</p>
<p>Addressing function approximation error in actor-critic methods. Scott Fujimoto, Herke Hoof, David Meger, International conference on machine learning. PMLR. Scott Fujimoto, Herke Hoof, and David Meger. "Ad- dressing function approximation error in actor-critic methods." In: International conference on machine learn- ing. PMLR. 2018, pp. 1587-1596.</p>
<p>On Passivity, Reinforcement Learning, and Higher Order Learning in Multiagent Finite Games. Bolin Gao, Lacra Pavel, IEEE Transactions on Automatic Control. 66Bolin Gao and Lacra Pavel. "On Passivity, Rein- forcement Learning, and Higher Order Learning in Multiagent Finite Games." In: IEEE Transactions on Automatic Control 66.1 (2021), pp. 121-136.</p>
<p>Transient Voltage Control Based on Physics-Informed Reinforcement Learning. Jiemai Gao, IEEE Journal of Radio Frequency Identification. 6Jiemai Gao et al. "Transient Voltage Control Based on Physics-Informed Reinforcement Learning." In: IEEE Journal of Radio Frequency Identification 6 (2022), pp. 905-910.</p>
<p>Safe exploration of state and action spaces in reinforcement learning. Javier Garcia, Fernando FernÃ¡ndez, In: Journal of Artificial Intelligence Research. 45Javier Garcia and Fernando FernÃ¡ndez. "Safe explo- ration of state and action spaces in reinforcement learning." In: Journal of Artificial Intelligence Research 45 (2012), pp. 515-564.</p>
<p>Physics-based dexterous manipulations with estimated hand poses and residual reinforcement learning. Guillermo Garcia-Hernando, Edward Johns, Tae-Kyun Kim, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020. Guillermo Garcia-Hernando, Edward Johns, and Tae-Kyun Kim. "Physics-based dexterous manipula- tions with estimated hand poses and residual rein- forcement learning." In: 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020, pp. 9561-9568.</p>
<p>First-person hand action benchmark with rgb-d videos and 3d hand pose annotations. Guillermo Garcia-Hernando, IEEE conference on computer vision and pattern recognition. Guillermo Garcia-Hernando et al. "First-person hand action benchmark with rgb-d videos and 3d hand pose annotations." In: IEEE conference on computer vision and pattern recognition. 2018, pp. 409-419.</p>
<p>Divide-and-conquer reinforcement learning. Dibya Ghosh, arXiv:1711.09874arXiv preprintDibya Ghosh et al. "Divide-and-conquer reinforce- ment learning." In: arXiv preprint arXiv:1711.09874 (2017).</p>
<p>PhysQ: A Physics Informed Reinforcement Learning Framework for Building Control. Gargya Gokhale, Bert Claessens, Chris Develder, arXiv:2211.11830arXiv preprintGargya Gokhale, Bert Claessens, and Chris Develder. "PhysQ: A Physics Informed Reinforcement Learn- ing Framework for Building Control." In: arXiv preprint arXiv:2211.11830 (2022).</p>
<p>Sim-to-real transfer with neural-augmented robot simulation. Florian Golemo, In: Conference on Robot Learning. PMLR. Florian Golemo et al. "Sim-to-real transfer with neural-augmented robot simulation." In: Conference on Robot Learning. PMLR. 2018, pp. 817-828.</p>
<p>Hamiltonian neural networks. Samuel Greydanus, Misko Dzamba, Jason Yosinski, Advances in neural information processing systems. 32Samuel Greydanus, Misko Dzamba, and Jason Yosin- ski. "Hamiltonian neural networks." In: Advances in neural information processing systems 32 (2019).</p>
<p>A review of safe reinforcement learning: Methods, theory and applications. Shangding Gu, arXiv:2205.10330arXiv preprintShangding Gu et al. "A review of safe reinforce- ment learning: Methods, theory and applications." In: arXiv preprint arXiv:2205.10330 (2022).</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, In: International conference on machine learning. PMLR. Tuomas Haarnoja et al. "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." In: International conference on ma- chine learning. PMLR. 2018, pp. 1861-1870.</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, arXiv:1912.01603arXiv preprintDanijar Hafner et al. "Dream to control: Learning behaviors by latent imagination." In: arXiv preprint arXiv:1912.01603 (2019).</p>
<p>A physics-informed reinforcement learning-based strategy for local and coordinated ramp metering. Yu Han, In: Transportation Research Part C: Emerging Technologies. 137103584Yu Han et al. "A physics-informed reinforcement learning-based strategy for local and coordinated ramp metering." In: Transportation Research Part C: Emerging Technologies 137 (2022), p. 103584.</p>
<p>Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications. Zhongkai Hao, arXiv:2211.08064arXiv preprintZhongkai Hao et al. "Physics-Informed Machine Learning: A Survey on Problems, Methods and Ap- plications." In: arXiv preprint arXiv:2211.08064 (2022).</p>
<p>Databased fuel-economy optimization of connected automated trucks in traffic. Chaozhe R He, GÃ¡bor Ge Jin, Orosz, Annual American Control Conference (ACC). Chaozhe R He, I Ge Jin, and GÃ¡bor Orosz. "Data- based fuel-economy optimization of connected au- tomated trucks in traffic." In: 2018 Annual American Control Conference (ACC). 2018, pp. 5576-5581.</p>
<p>Symmetry-informed Reinforcement Learning and Its Application to the Attitude Control of Quadrotors. Junchang Huang, IEEE Transactions on Artificial Intelligence. Junchang Huang et al. "Symmetry-informed Rein- forcement Learning and Its Application to the Atti- tude Control of Quadrotors." In: IEEE Transactions on Artificial Intelligence (2023).</p>
<p>Reasoning about physical interactions with object-oriented prediction and planning. Michael Janner, arXiv:1812.10972arXiv preprintMichael Janner et al. "Reasoning about physical in- teractions with object-oriented prediction and plan- ning." In: arXiv preprint arXiv:1812.10972 (2018).</p>
<p>Physics informed intrinsic rewards in reinforcement learning. Jiazhou Jiang, Minyue Fu, Zhiyong Chen, 2022 Australian &amp; New Zealand Control Conference (ANZCC). 2022. Jiazhou Jiang, Minyue Fu, and Zhiyong Chen. "Physics informed intrinsic rewards in reinforcement learning." In: 2022 Australian &amp; New Zealand Control Conference (ANZCC). 2022, pp. 74-69.</p>
<p>Cooperative adaptive optimal output regulation of nonlinear discrete-time multi-agent systems. Yi Jiang, In: Automatica. 121109149Yi Jiang et al. "Cooperative adaptive optimal out- put regulation of nonlinear discrete-time multi-agent systems." In: Automatica 121 (2020), p. 109149.</p>
<p>Residual reinforcement learning for robot control. Tobias Johannink, 2019 International Conference on Robotics and Automation (ICRA). Tobias Johannink et al. "Residual reinforcement learning for robot control." In: 2019 International Conference on Robotics and Automation (ICRA). 2019, pp. 6023-6029.</p>
<p>Increasing the safety of adaptive cruise control using physics-guided reinforcement learning. Jurj Sorin Liviu, In: Energies. 147572Sorin Liviu Jurj et al. "Increasing the safety of adap- tive cruise control using physics-guided reinforce- ment learning." In: Energies 14.22 (2021), p. 7572.</p>
<p>Physics-informed machine learning. George Em Karniadakis, Nature Reviews Physics. 36George Em Karniadakis et al. "Physics-informed ma- chine learning." In: Nature Reviews Physics 3.6 (2021), pp. 422-440.</p>
<p>A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries. Ali Kashefi, Davis Rempe, Leonidas J Guibas, Physics of Fluids. 3327104Ali Kashefi, Davis Rempe, and Leonidas J Guibas. "A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries." In: Physics of Fluids 33.2 (2021), p. 027104.</p>
<p>Physics-informed machine learning: case studies for weather and climate modelling. K Kashinath, In: Philosophical Transactions of the Royal Society A. 379K Kashinath et al. "Physics-informed machine learn- ing: case studies for weather and climate modelling." In: Philosophical Transactions of the Royal Society A 379 (2021), pp. 1-36.</p>
<p>Jam-avoiding adaptive cruise control (ACC) and its impact on traffic dynamics. Arne Kesting, Traffic and Granular Flow'05. SpringerArne Kesting et al. "Jam-avoiding adaptive cruise control (ACC) and its impact on traffic dynamics." In: Traffic and Granular Flow'05. Springer. 2007, pp. 633- 643.</p>
<p>Learning to run challenge: Synthesizing physiologically accurate motion using deep reinforcement learning. Åukasz KidziÅ„ski, The NIPS'17 Competition: Building Intelligent Systems. SpringerÅukasz KidziÅ„ski et al. "Learning to run challenge: Synthesizing physiologically accurate motion using deep reinforcement learning." In: The NIPS'17 Com- petition: Building Intelligent Systems. Springer. 2018, pp. 101-120.</p>
<p>Reward (mis) design for autonomous driving. Knox W Bradley, In: Artificial Intelligence. 316103829W Bradley Knox et al. "Reward (mis) design for autonomous driving." In: Artificial Intelligence 316 (2023), p. 103829.</p>
<p>IEEE/RSJ international conference on intelligent robots and systems (IROS)(IEEE Cat. Nathan Koenig, Andrew Howard, 3Design and use paradigms for gazebo, an open-source multirobot simulatorNathan Koenig and Andrew Howard. "Design and use paradigms for gazebo, an open-source multi- robot simulator." In: 2004 IEEE/RSJ international con- ference on intelligent robots and systems (IROS)(IEEE Cat. No. 04CH37566). Vol. 3. 2004, pp. 2149-2154.</p>
<p>Actor-critic algorithms. Vijay Konda, John Tsitsiklis, Advances in neural information processing systems. 12Vijay Konda and John Tsitsiklis. "Actor-critic algo- rithms." In: Advances in neural information processing systems 12 (1999).</p>
<p>Inertia-Constrained Reinforcement Learning to Enhance Human Motor Control Modeling. Soroush Korivand, Nader Jalili, Jiaqi Gong, In: Sensors. 232698Soroush Korivand, Nader Jalili, and Jiaqi Gong. "Inertia-Constrained Reinforcement Learning to En- hance Human Motor Control Modeling." In: Sensors 23.5 (2023), p. 2698.</p>
<p>Traffic flow modeling of large-scale motorway networks using the macroscopic modeling tool METANET. Apostolos Kotsialos, IEEE Transactions on intelligent transportation systems. 3Apostolos Kotsialos et al. "Traffic flow modeling of large-scale motorway networks using the macro- scopic modeling tool METANET." In: IEEE Transac- tions on intelligent transportation systems 3.4 (2002), pp. 282-292.</p>
<p>Context-aware dynamics model for generalization in model-based reinforcement learning. Kimin Lee, PMLR. 2020International Conference on Machine Learning. Kimin Lee et al. "Context-aware dynamics model for generalization in model-based reinforcement learn- ing." In: International Conference on Machine Learning. PMLR. 2020, pp. 5757-5766.</p>
<p>End-to-end training of deep visuomotor policies. Sergey Levine, In: The Journal of Machine Learning Research. 17Sergey Levine et al. "End-to-end training of deep visuomotor policies." In: The Journal of Machine Learn- ing Research 17.1 (2016), pp. 1334-1373.</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Sergey Levine, arXiv:2005.01643arXiv preprintSergey Levine et al. "Offline reinforcement learning: Tutorial, review, and perspectives on open prob- lems." In: arXiv preprint arXiv:2005.01643 (2020).</p>
<p>Differentiable monte carlo ray tracing through edge sampling. Tzu-Mao Li, In: ACM Transactions on Graphics (TOG). 37Tzu-Mao Li et al. "Differentiable monte carlo ray tracing through edge sampling." In: ACM Transac- tions on Graphics (TOG) 37.6 (2018), pp. 1-11.</p>
<p>Temporal logic guided safe reinforcement learning using control barrier functions. Xiao Li, Calin Belta, arXiv:1903.09885arXiv preprintXiao Li and Calin Belta. "Temporal logic guided safe reinforcement learning using control barrier func- tions." In: arXiv preprint arXiv:1903.09885 (2019).</p>
<p>Federated multiagent deep reinforcement learning approach via physics-informed reward for multimicrogrid energy management. Yuanzheng Li, IEEE Transactions on Neural Networks and Learning Systems. Yuanzheng Li et al. "Federated multiagent deep re- inforcement learning approach via physics-informed reward for multimicrogrid energy management." In: IEEE Transactions on Neural Networks and Learning Systems (2023).</p>
<p>Safe reinforcement learning using robust action governor. Yutong Li, PMLR. 2021In: Learning for Dynamics and ControlYutong Li et al. "Safe reinforcement learning using robust action governor." In: Learning for Dynamics and Control. PMLR. 2021, pp. 1093-1104.</p>
<p>Fourier neural operator for parametric partial differential equations. Zongyi Li, arXiv:2010.08895arXiv preprintZongyi Li et al. "Fourier neural operator for paramet- ric partial differential equations." In: arXiv preprint arXiv:2010.08895 (2020).</p>
<p>Physics-informed Dyna-style model-based deep reinforcement learning for dynamic control. Yang Xin, Jian-Xun Liu, Wang, Royal Society A. 47720210618Xin-Yang Liu and Jian-Xun Wang. "Physics-informed Dyna-style model-based deep reinforcement learn- ing for dynamic control." In: Royal Society A 477.2255 (2021), p. 20210618.</p>
<p>In: 2018 21st international conference on intelligent transportation systems (ITSC). Pablo Alvarez Lopez, Microscopic traffic simulation using sumoPablo Alvarez Lopez et al. "Microscopic traffic simu- lation using sumo." In: 2018 21st international confer- ence on intelligent transportation systems (ITSC). 2018, pp. 2575-2582.</p>
<p>Multi-agent actor-critic for mixed cooperative-competitive environments. Ryan Lowe, Advances in neural information processing systems. 30Ryan Lowe et al. "Multi-agent actor-critic for mixed cooperative-competitive environments." In: Advances in neural information processing systems 30 (2017).</p>
<p>Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system. Kendall Lowrey, 2018 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR). Kendall Lowrey et al. "Reinforcement learning for non-prehensile manipulation: Transfer from simula- tion to physical system." In: 2018 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR). 2018, pp. 35-42.</p>
<p>Learning nonlinear operators via Deep-ONet based on the universal approximation theorem of operators. Lu Lu, Nature Machine Intelligence. 3Lu Lu et al. "Learning nonlinear operators via Deep- ONet based on the universal approximation theo- rem of operators." In: Nature Machine Intelligence 3.3 (2021), pp. 218-229.</p>
<p>Kinematics-guided reinforcement learning for object-aware 3d ego-pose estimation. Zhengyi Luo, arXiv:2011.04837arXiv preprintZhengyi Luo et al. "Kinematics-guided reinforce- ment learning for object-aware 3d ego-pose estima- tion." In: arXiv preprint arXiv:2011.04837 (2020).</p>
<p>Differentiable physics models for real-world offline model-based reinforcement learning. Michael Lutter, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021. Michael Lutter et al. "Differentiable physics mod- els for real-world offline model-based reinforcement learning." In: 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021, pp. 4163-4170.</p>
<p>SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering. Jun Lv, arXiv:2210.15185arXiv preprintJun Lv et al. "SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics- Based Simulation and Rendering." In: arXiv preprint arXiv:2210.15185 (2022).</p>
<p>Model-based constrained reinforcement learning using generalized control barrier function. Haitong Ma, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021. Haitong Ma et al. "Model-based constrained rein- forcement learning using generalized control bar- rier function." In: 2021 IEEE/RSJ International Con- ference on Intelligent Robots and Systems (IROS). 2021, pp. 4552-4559.</p>
<p>Learning to jump from pixels. B Gabriel, Margolis, arXiv:2110.15344arXiv preprintGabriel B Margolis et al. "Learning to jump from pixels." In: arXiv preprint arXiv:2110.15344 (2021).</p>
<p>Reinforcement learning and orbit-discovery enhanced by smallbody physics-informed neural network gravity models. John Martin, Hanspeter Schaub, AIAA SCITECH 2022 Forum. 2022. 2272John Martin and Hanspeter Schaub. "Reinforcement learning and orbit-discovery enhanced by small- body physics-informed neural network gravity mod- els." In: AIAA SCITECH 2022 Forum. 2022, p. 2272.</p>
<p>When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning. Chuizheng Meng, arXiv:2203.16797arXiv preprintChuizheng Meng et al. "When Physics Meets Ma- chine Learning: A Survey of Physics-Informed Ma- chine Learning." In: arXiv preprint arXiv:2203.16797 (2022).</p>
<p>Learning functional priors and posteriors from data and physics. Xuhui Meng, In: Journal of Computational Physics. 457111073Xuhui Meng et al. "Learning functional priors and posteriors from data and physics." In: Journal of Computational Physics 457 (2022), p. 111073.</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, arXiv:1312.5602arXiv preprintVolodymyr Mnih et al. "Playing atari with deep reinforcement learning." In: arXiv preprint arXiv:1312.5602 (2013).</p>
<p>Pods: Policy optimization via differentiable simulation. Miguel Angel Zamora Mora, PMLR. 2021International Conference on Machine Learning. Miguel Angel Zamora Mora et al. "Pods: Policy optimization via differentiable simulation." In: Inter- national Conference on Machine Learning. PMLR. 2021, pp. 7805-7817.</p>
<p>Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO). Amartya Mukherjee, Jun Liu, arXiv:2302.00237arXiv preprintAmartya Mukherjee and Jun Liu. "Bridging Physics- Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Pol- icy Optimization (HJBPPO)." In: arXiv preprint arXiv:2302.00237 (2023).</p>
<p>Continuous-discrete reinforcement learning for hybrid control in robotics. Michael Neunert, Conference on Robot Learning (2020). Michael Neunert et al. "Continuous-discrete rein- forcement learning for hybrid control in robotics." In: Conference on Robot Learning (2020), pp. 735-751.</p>
<p>Barrier-certified adaptive reinforcement learning with applications to brushbot navigation. Motoya Ohnishi, IEEE Transactions on robotics. 35Motoya Ohnishi et al. "Barrier-certified adaptive re- inforcement learning with applications to brushbot navigation." In: IEEE Transactions on robotics 35.5 (2019), pp. 1186-1205.</p>
<p>Simulation-based reinforcement learning for real-world autonomous driving. BÅ‚aÅ¼ej OsiÅ„ski, 2020 IEEE international conference on robotics and automation (ICRA). 2020. BÅ‚aÅ¼ej OsiÅ„ski et al. "Simulation-based reinforcement learning for real-world autonomous driving." In: 2020 IEEE international conference on robotics and au- tomation (ICRA). 2020, pp. 6411-6418.</p>
<p>Learning to fly-a gym environment with pybullet physics for reinforcement learning of multi-agent quadcopter control. Jacopo Panerati, 2021Jacopo Panerati et al. "Learning to fly-a gym en- vironment with pybullet physics for reinforcement learning of multi-agent quadcopter control." In: 2021</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021, pp. 7512-7519.</p>
<p>Physics-informed reinforcement learning for sample-efficient optimization of freeform nanophotonic devices. Chaejin Park, arXiv:2306.04108arXiv preprintChaejin Park et al. "Physics-informed reinforce- ment learning for sample-efficient optimization of freeform nanophotonic devices." In: arXiv preprint arXiv:2306.04108 (2023).</p>
<p>DeepLoco: Dynamic locomotion skills using hierarchical deep reinforcement learning. Xue Bin, Peng, In: ACM Transactions on Graphics (TOG). 364Xue Bin Peng et al. "DeepLoco: Dynamic locomotion skills using hierarchical deep reinforcement learn- ing." In: ACM Transactions on Graphics (TOG) 36.4 (2017), pp. 1-13.</p>
<p>Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. Xue Bin, Peng, In: ACM Transactions On Graphics (TOG). 37Xue Bin Peng et al. "Deepmimic: Example-guided deep reinforcement learning of physics-based char- acter skills." In: ACM Transactions On Graphics (TOG) 37.4 (2018), pp. 1-14.</p>
<p>Physics-informed reinforcement learning optimization of nuclear assembly design. Majdi I Radaideh, Nuclear Engineering and Design. 372110966Majdi I Radaideh et al. "Physics-informed reinforce- ment learning optimization of nuclear assembly de- sign." In: Nuclear Engineering and Design 372 (2021), p. 110966.</p>
<p>Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. Aravind Rajeswaran, arXiv:1709.10087arXiv preprintAravind Rajeswaran et al. "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations." In: arXiv preprint arXiv:1709.10087 (2017).</p>
<p>Physics-Informed Model-Based Reinforcement Learning. Adithya Ramesh, Balaraman Ravindran, PMLR. 2023Learning for Dynamics and Control Conference. Adithya Ramesh and Balaraman Ravindran. "Physics-Informed Model-Based Reinforcement Learning." In: Learning for Dynamics and Control Conference. PMLR. 2023, pp. 26-37.</p>
<p>Neural fitted Q iteration-first experiences with a data efficient neural reinforcement learning method. Martin Riedmiller, 16th European Conference on Machine Learning. SpringerMartin Riedmiller. "Neural fitted Q iteration-first ex- periences with a data efficient neural reinforcement learning method." In: 16th European Conference on Machine Learning. Springer. 2005, pp. 317-328.</p>
<p>Cassie-mujoco-sim. Year Published/ Last Updated. Agility robotics. Agility robotics. Cassie-mujoco-sim. Year Published/ Last Updated. URL: https : / / github . com / osudrl / cassie-mujoco-sim.</p>
<p>Physics-informed reinforcement learning for motion control of a fish-like swimming robot. Colin Rodwell, Phanindra Tallapragada, In: Scientific Reports. 13Colin Rodwell and Phanindra Tallapragada. "Physics-informed reinforcement learning for motion control of a fish-like swimming robot." In: Scientific Reports 13.1 (2023), pp. 1-17.</p>
<p>Stabilization with guaranteed safety using control Lyapunov-barrier function. Muhammad Zakiyullah Romdlony, Bayu Jayawardhana, In: Automatica. 66Muhammad Zakiyullah Romdlony and Bayu Jayawardhana. "Stabilization with guaranteed safety using control Lyapunov-barrier function." In: Auto- matica 66 (2016), pp. 39-47.</p>
<p>Graph networks as learnable physics engines for inference and control. Alvaro Sanchez-Gonzalez, International Conference on Machine Learning. PMLR. Alvaro Sanchez-Gonzalez et al. "Graph networks as learnable physics engines for inference and con- trol." In: International Conference on Machine Learning. PMLR. 2018, pp. 4470-4479.</p>
<p>Proximal policy optimization algorithms. John Schulman, arXiv:1707.06347arXiv preprintJohn Schulman et al. "Proximal policy optimiza- tion algorithms." In: arXiv preprint arXiv:1707.06347 (2017).</p>
<p>Encoding invariances in deep generative models. Viraj Shah, arXiv:1906.01626arXiv preprintViraj Shah et al. "Encoding invariances in deep gen- erative models." In: arXiv preprint arXiv:1906.01626 (2019).</p>
<p>Inverter PQ Control with Trajectory Tracking Capability for Microgrids Based on Physicsinformed Reinforcement Learning. Buxin She, IEEE Transactions on Smart Grid. Buxin She et al. "Inverter PQ Control with Trajectory Tracking Capability for Microgrids Based on Physics- informed Reinforcement Learning." In: IEEE Transac- tions on Smart Grid (2023).</p>
<p>Physics-informed deep reinforcement learning-based integrated two-dimensional car-following control strategy for connected automated vehicles. Haotian Shi, In: Knowledge-Based Systems. 269110485Haotian Shi et al. "Physics-informed deep reinforce- ment learning-based integrated two-dimensional car-following control strategy for connected auto- mated vehicles." In: Knowledge-Based Systems 269 (2023), p. 110485.</p>
<p>Sim-to-real learning of all common bipedal gaits via periodic reward composition. Jonah Siekmann, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021. Jonah Siekmann et al. "Sim-to-real learning of all common bipedal gaits via periodic reward com- position." In: 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021, pp. 7309-7315.</p>
<p>Deterministic policy gradient algorithms. David Silver, International conference on machine learning. David Silver et al. "Deterministic policy gradient algorithms." In: International conference on machine learning. Pmlr. 2014, pp. 387-395.</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Reinforcement learning: an introduction MIT Press. S Richard, Andrew G Sutton, Barto, 22447Cambridge, MARichard S Sutton and Andrew G Barto. "Reinforce- ment learning: an introduction MIT Press." In: Cam- bridge, MA 22447 (1998).</p>
<p>Deepmind control suite. Yuval Tassa, arXiv:1801.00690arXiv preprintYuval Tassa et al. "Deepmind control suite." In: arXiv preprint arXiv:1801.00690 (2018).</p>
<p>Action assembly: Sparse imitation learning for text based games with combinatorial action spaces. Chen Tessler, arXiv:1905.09700arXiv preprintChen Tessler et al. "Action assembly: Sparse imita- tion learning for text based games with combinato- rial action spaces." In: arXiv preprint arXiv:1905.09700 (2019).</p>
<p>End-to-end model-free reinforcement learning for urban driving using implicit affordances. Marin Toromanoff, Emilie Wirbel, Fabien Moutarde, IEEE/CVF conference on computer vision and pattern recognition. Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. "End-to-end model-free reinforcement learning for urban driving using implicit affor- dances." In: IEEE/CVF conference on computer vision and pattern recognition. 2020, pp. 7153-7162.</p>
<p>Safe Reinforcement Learning with Probabilistic Control Barrier Functions for Ramp Merging. Soumith Udatha, Yiwei Lyu, John Dolan, arXiv:2212.00618arXiv preprintSoumith Udatha, Yiwei Lyu, and John Dolan. "Safe Reinforcement Learning with Probabilistic Control Barrier Functions for Ramp Merging." In: arXiv preprint arXiv:2212.00618 (2022).</p>
<p>Entity abstraction in visual model-based reinforcement learning. Rishi Veerapaneni, PMLR. 2020Conference on Robot Learning. Rishi Veerapaneni et al. "Entity abstraction in visual model-based reinforcement learning." In: Conference on Robot Learning. PMLR. 2020, pp. 1439-1456.</p>
<p>Experimental demonstration of frequency regulation by commercial buildings-Part I: Modeling and hierarchical control design. Evangelos Vrettos, IEEE Transactions on Smart Grid. 9Evangelos Vrettos et al. "Experimental demonstra- tion of frequency regulation by commercial build- ings-Part I: Modeling and hierarchical control de- sign." In: IEEE Transactions on Smart Grid 9.4 (2016), pp. 3213-3223.</p>
<p>Phyllis: Physics-Informed Lifelong Reinforcement Learning for Data Center Cooling Control. Ruihang Wang, 14th ACM International Conference on Future Energy Systems. 2023. Ruihang Wang et al. "Phyllis: Physics-Informed Life- long Reinforcement Learning for Data Center Cool- ing Control." In: 14th ACM International Conference on Future Energy Systems. 2023, pp. 114-126.</p>
<p>Ensuring safety of learning-based motion planners using control barrier functions. Xiao Wang, IEEE Robotics and Automation Letters. 72Xiao Wang. "Ensuring safety of learning-based mo- tion planners using control barrier functions." In: IEEE Robotics and Automation Letters 7.2 (2022), pp. 4773-4780.</p>
<p>Fast and feature-complete differentiable physics for articulated rigid bodies with contact. Keenon Werling, arXiv:2103.16021arXiv preprintKeenon Werling et al. "Fast and feature-complete dif- ferentiable physics for articulated rigid bodies with contact." In: arXiv preprint arXiv:2103.16021 (2021).</p>
<p>Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 8Ronald J Williams. "Simple statistical gradient- following algorithms for connectionist reinforcement learning." In: Machine learning 8 (1992), pp. 229-256.</p>
<p>Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. Jin-Long Wu, In: Journal of Computational Physics. 406109209Jin-Long Wu et al. "Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems." In: Journal of Computa- tional Physics 406 (2020), p. 109209.</p>
<p>Model-based reinforcement learning with parametrized physical models and optimismdriven exploration. Chris Xie, 2016 IEEE international conference on robotics and automation (ICRA). Chris Xie et al. "Model-based reinforcement learning with parametrized physical models and optimism- driven exploration." In: 2016 IEEE international conference on robotics and automation (ICRA). 2016, pp. 504-511.</p>
<p>Feedback control for cassie with deep reinforcement learning. Zhaoming Xie, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Zhaoming Xie et al. "Feedback control for cassie with deep reinforcement learning." In: 2018 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) (2018), pp. 1241-1246.</p>
<p>Accelerated policy learning with parallel differentiable simulation. Jie Xu, arXiv:2204.07137arXiv preprintJie Xu et al. "Accelerated policy learning with par- allel differentiable simulation." In: arXiv preprint arXiv:2204.07137 (2022).</p>
<p>Representation matters: Offline pretraining for sequential decision making. Mengjiao Yang, Ofir Nachum, PMLR. 2021International Conference on Machine Learning. Mengjiao Yang and Ofir Nachum. "Representation matters: Offline pretraining for sequential decision making." In: International Conference on Machine Learning. PMLR. 2021, pp. 11784-11794.</p>
<p>Conditional deep surrogate models for stochastic, high-dimensional, and multi-fidelity systems. Yibo Yang, Paris Perdikaris, In: Computational Mechanics. 64Yibo Yang and Paris Perdikaris. "Conditional deep surrogate models for stochastic, high-dimensional, and multi-fidelity systems." In: Computational Me- chanics 64.2 (2019), pp. 417-434.</p>
<p>Model-Free Safe Reinforcement Learning through Neural Barrier Certificate. Yujie Yang, IEEE Robotics and Automation Letters. Yujie Yang et al. "Model-Free Safe Reinforcement Learning through Neural Barrier Certificate." In: IEEE Robotics and Automation Letters (2023).</p>
<p>Generalizable Wireless Navigation through Physics-Informed Reinforcement Learning in Wireless Digital Twin. Mingsheng Yin, arXiv:2306.06766arXiv preprintMingsheng Yin et al. "Generalizable Wireless Nav- igation through Physics-Informed Reinforcement Learning in Wireless Digital Twin." In: arXiv preprint arXiv:2306.06766 (2023).</p>
<p>Physics-guided deep reinforcement learning for flow field denoising. Z Mustafa, Yousif, arXiv:2302.09559arXiv preprintMustafa Z Yousif et al. "Physics-guided deep re- inforcement learning for flow field denoising." In: arXiv preprint arXiv:2302.09559 (2023).</p>
<p>District cooling system control for providing regulation services based on safe reinforcement learning with barrier functions. Peipei Yu, Hongcai Zhang, Yonghua Song, In: Applied Energy. 347121396Peipei Yu, Hongcai Zhang, and Yonghua Song. "Dis- trict cooling system control for providing regula- tion services based on safe reinforcement learning with barrier functions." In: Applied Energy 347 (2023), p. 121396.</p>
<p>Safe-control-gym: A unified benchmark suite for safe learning-based control and reinforcement learning. Zhaocong Yuan, arXiv:2109.06325arXiv preprintZhaocong Yuan et al. "Safe-control-gym: A uni- fied benchmark suite for safe learning-based con- trol and reinforcement learning." In: arXiv preprint arXiv:2109.06325 (2021).</p>
<p>Barrier Function-based Safe Reinforcement Learning for Formation Control of Mobile Robots. Xinglong Zhang, 2022 International Conference on Robotics and Automation (ICRA). 2022. Xinglong Zhang et al. "Barrier Function-based Safe Reinforcement Learning for Formation Control of Mobile Robots." In: 2022 International Conference on Robotics and Automation (ICRA). 2022, pp. 5532-5538.</p>
<p>Physics informed deep reinforcement learning for aircraft conflict resolution. Peng Zhao, Yongming Liu, IEEE Transactions on Intelligent Transportation Systems. 23Peng Zhao and Yongming Liu. "Physics informed deep reinforcement learning for aircraft conflict reso- lution." In: IEEE Transactions on Intelligent Transporta- tion Systems 23.7 (2021), pp. 8288-8301.</p>
<p>Safe reinforcement learning for dynamical systems using barrier certificates. Qingye Zhao, Yi Zhang, Xuandong Li, Connection Science. 34Qingye Zhao, Yi Zhang, and Xuandong Li. "Safe reinforcement learning for dynamical systems using barrier certificates." In: Connection Science 34.1 (2022), pp. 2822-2844.</p>
<p>A Barrier-Certificated Reinforcement Learning Approach for Enhancing Power System Transient Stability. Tianqiao Zhao, Jianhui Wang, Meng Yue, IEEE Transactions on Power Systems. Tianqiao Zhao, Jianhui Wang, and Meng Yue. "A Barrier-Certificated Reinforcement Learning Ap- proach for Enhancing Power System Transient Sta- bility." In: IEEE Transactions on Power Systems (2023).</p>
<p>Environment Probing Interaction Policies. Wenxuan Zhou, Lerrel Pinto, Abhinav Gupta, International Conference on Learning Representations. Wenxuan Zhou, Lerrel Pinto, and Abhinav Gupta. "Environment Probing Interaction Policies." In: Inter- national Conference on Learning Representations. 2018.</p>
<p>Puppetmaster: robotic animation of marionettes. Simon Zimmermann, In: ACM Transactions on Graphics (TOG). 38Simon Zimmermann et al. "Puppetmaster: robotic animation of marionettes." In: ACM Transactions on Graphics (TOG) 38.4 (2019), pp. 1-11.</p>            </div>
        </div>

    </div>
</body>
</html>
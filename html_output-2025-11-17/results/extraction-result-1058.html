<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1058 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1058</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1058</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-265445462</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.14379v1.pdf" target="_blank">Robot Learning in the Era of Foundation Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1058.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1058.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1 (Robotics Transformer for Real-World Control at Scale)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model trained on large-scale real robot demonstration data to map image+instruction tokens to robot actions for multi-task manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RT-1 agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Vision-language-action transformer trained by behavioral cloning on a large dataset of human/teleoperated robot demonstrations (imitation learning / supervised fine-tuning of VLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot / robotic agent (evaluated on real robots and demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Office and kitchen manipulation environments (multi-task real-world)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-world domestic/office scenes containing household objects and tasks (multi-object desktop and kitchen manipulation). Complexity arises from contact-rich interactions, object variety, and real sensor noise.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of distinct tasks and objects in dataset; presence of contact-rich interactions and continuous robot motion (paper reports dataset scale: ~130k demonstrations referenced elsewhere in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Scale and diversity of demonstrations across robots, objects and environments (dataset size ~130k demonstrations referenced); multi-scene collection over months.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Execution success rate / task success (manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes RT-1 leverages large, diverse demonstration data to improve generalization across tasks/environments; high variation in demonstrations is treated as a way to handle environment variability though low-level control and dynamics remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Large-scale imitation learning / behavioral cloning on multi-task real-world demonstration dataset; multi-task fine-tuning of vision-language models.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Reported to transfer web-scale vision-language knowledge to robotic control and improve zero-shot/transfer capabilities across many manipulation tasks, but exact numeric gains not stated in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High data usage (~130k demonstrations referenced); not sample-efficient in low-data regimes per survey discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large, diverse real demonstration datasets plus VLM architectures improve cross-task generalization in manipulation; however, foundation models still lack robust low-level dynamics control and need dynamics data to further improve performance under physical variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1058.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboGen (Generative-simulation data generation for robot learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative-simulation method that synthesizes (effectively infinite) simulated robot data to automate dataset creation for training robot policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RoboGen-trained agents (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents trained in simulation using data generated by RoboGen; learning paradigm typically imitation learning / supervised on generated trajectories and/or RL in sim.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents (physics simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated household and manipulation task environments (RFUniverse-style / generated scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated simulated household tasks including object interactions (fruit picking, cloth folding, wiping, pouring). Complexity provided by multi-step tasks and contact dynamics simulated; variation introduced by procedural generation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number and variety of atomic operations supported (~87 atomic operations in RFUniverse cited); number of task types (multiple household tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation parameters enabling many scene and object instantiations (effectively unbounded simulated instances); domain randomization degree determined by generator.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task completion / transfer performance when porting sim-trained models to target tasks (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey highlights RoboGen's approach to trade off expensive real data by increasing simulated variation; increased simulated variation aims to improve generalization but real-world dynamics mismatch remains a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Generative simulation / procedural data generation; train in simulation possibly with domain randomization; large-scale supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Intended to improve generalization by providing massive simulated variation; survey reports the method as a step towards automation but no numeric generalization metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relies on generating large volumes of synthetic interactions (high sample usage in sim but intended to reduce real-data needs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using generative simulation to expand dataset scale/variation is promising to handle environment variability, but sim-to-real transfer and capturing realistic dynamics remain challenging trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1058.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFUniverse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RFUniverse (physics-based action-centric environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-based simulated environment that supports a rich set of atomic operations and primitive object types for training embodied agents on daily household tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agents trained in RFUniverse</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents trained in simulation using imitation learning and RL on atomic operations (pick/place, pouring, folding), typically evaluated on multi-task household scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>RFUniverse simulated household environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Visually and physically plausible simulation supporting 87 atomic operations and 8 primitive object types; tasks include fruit picking, cloth folding, sponge wiping, milk pouring—complexity arises from articulated/deformable objects and contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of atomic operations (87), number of primitive object types (8), and diversity of task types.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (contact-rich, deformable objects included)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variety of tasks and object types; ability to generate diverse scenarios but explicit counts of environment instances not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success across simulated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey frames RFUniverse as providing complexity (contact-rich interactions) and variation (many operation types) to train agents; highlights that adding physics and diverse primitives improves realism but increases training difficulty and need for dynamics-aware models.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Simulation-based training with many atomic operations; imitation and RL in physics-rich sim.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Used to demonstrate usability across various tasks; provides capability to test policies on multiple operation types though concrete generalization numbers not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Requires many simulated interactions due to contact complexity; sample counts not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Physics-rich simulated environments with many atomic operations increase task realism and prepare agents for varied real-world scenarios, but increase the need for dynamics data and more sophisticated training (world models, dynamics-aware learning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1058.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPORT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPORT (Language-conditioned Imitation Agent combining CLIP and Transporter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>End-to-end language-conditioned imitation learning agent that combines CLIP for semantic grounding with Transporter for spatial precision to solve desktop manipulation tasks specified by language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CLIPORT agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Autoregressive imitation-learning agent using visual-language embeddings (CLIP) and spatial action models (Transporter); trained on paired image-instruction-action data.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated and real robotic agent (manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Language-specified desktop manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Structured desktop scenes with objects arranged for pick/place and simple multi-step routines; complexity arises from language grounding and precise spatial interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of language-specified tasks and objects in dataset (CLIPORT benchmark tasks number referenced qualitatively); requirement for spatial precision.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation in natural language instructions and object appearances; dataset includes multiple language-task pairs but explicit counts not given in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success rate on language-conditioned manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey describes CLIPORT as achieving good performance by separating semantic understanding and spatial precision—implying a trade-off where semantic complexity (language variation) can be mitigated by strong perception modules while spatial complexity requires spatial inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Language-conditioned imitation learning (end-to-end), leveraging pretrained visual-language models (CLIP) and transport-based spatial modules.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Shows improved generalization across language instructions compared to baselines due to CLIP semantic features; survey gives qualitative assessment only.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained on moderate-sized task-specific datasets (hundreds of demonstrations), not quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Modularizing semantic grounding and spatial reasoning helps handle language variation and spatial complexity; pretrained VLMs improve semantic generalization but spatial inductive biases remain important for precise control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1058.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIMA (Transformer-based robotic agent using multimodal prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based agent that interprets multimodal prompts (interleaved language and images/video frames) and outputs motor actions autoregressively for various manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VIMA: Robot Manipulation with Multimodal Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VIMA agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer agent using multimodal prompt conditioning and imitation learning; maps multimodal prompts to sequences of motor actions (behavioral cloning).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (simulated and physical manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multimodal-prompted manipulation environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Tasks represented as sequences of images and language interleaved into prompts; complexity stems from temporally extended goals and necessity to interpret multimodal context.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Prompt complexity (length and multimodal content), number of task steps, and degrees-of-freedom of actions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Diversity of multimodal prompts, number of different tasks and objects used in training; survey indicates many tasks formulated as multimodal prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success / success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey emphasizes that expressing tasks as multimodal prompts allows VIMA to handle variation in instructions and visual scenes, but integrating perception and action remains challenging in highly complex contact-rich domains.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multimodal prompt-based imitation learning with transformer architectures and multimodal encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Authors claim broad expressivity and generalization across multimodal task formulations; survey reports qualitative improvements without specific numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Depends on multi-task demonstration datasets; sample counts not detailed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multimodal prompt representations enable concise specification of diverse tasks and support generalization across instruction styles and visual contexts; however, handling physical dynamics and contact-rich manipulation still requires additional dynamics-aware data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1058.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOMA-Force</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOMA-Force (Visual-force imitation for mobile manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An imitation learning approach that binds visual perception to force control to learn contact-rich tasks with high robustness and low contact forces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MOMA-Force agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learns perceptual representations combined with imitation learning and admittance whole-body control to produce force-aware policies for contact-rich mobile manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical mobile manipulator</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Contact-rich manipulation tasks (mobile manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Tasks that require whole-body contact control (e.g., pushing, wiping) in real-world settings; complexity comes from contact dynamics and whole-body coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Contact richness (presence of force interactions), number of manipulation primitives, and success rate on multiple complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Different contact scenarios and object types; survey notes multiple complex tasks learned but does not quantify count.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate and contact force magnitude (robustness and controllability)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>reported qualitatively as 'high success rate and small contact force' (no numeric value in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey suggests that incorporating force information (dynamics) helps agents handle complexity in contact-rich tasks and improves robustness to variation, implying a positive role of dynamics-aware modalities in coping with environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Perceptual representation learning + imitation learning + admittance whole-body control.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Demonstrated ability to learn multiple complex contact-rich tasks with robust contact force control; exact generalization metrics not supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fusing visual and force modalities and using dynamics-aware controllers significantly improves performance on contact-rich tasks and robustness to perturbations, highlighting the importance of dynamics data for generalization in complex environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1058.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E (An embodied multimodal language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal model that incorporates continuous sensor modalities (vision, proprioception, etc.) into a large language model to establish links between words and perception for sequential robot planning and QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM-E: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PaLM-E-based agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM grounded with continuous sensor inputs (visual, proprioceptive) trained end-to-end for tasks including sequential robot operation planning, VQA and captioning; uses transformer/Large language model architecture with multimodal encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>embodied agent / robotic agent (simulated and real)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General embodied tasks spanning manipulation, perception and planning</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Multimodal embodied settings where agent must combine vision, continuous states and language to plan sequences of operations; complexity arises from combining modalities and long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Multimodal input dimensionality (visual frames, continuous state vectors), number of sequential steps in plans.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Diversity of sensor observations and tasks in pretraining/fine-tuning; survey references many downstream tasks but doesn't quantify variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task planning success, VQA accuracy, sequential operation execution quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey highlights PaLM-E as an approach to bridge perception and language for embodied reasoning; increasing multimodal complexity requires larger models and diverse data, improving reasoning but exacerbating computation and data demands.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>End-to-end multimodal fine-tuning of LLM with continuous sensor encodings; transfer from pre-trained language models.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Demonstrated ability to perform varied embodied tasks by grounding LLMs with sensors; survey gives qualitative claims without numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Large pretrained models leveraged; sample efficiency relative to small models not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating continuous sensor modalities into LLMs improves embodied reasoning and planning across varied tasks, but requires significant multimodal data and computation; highlights trade-off between model scale and environment variation handled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1058.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan (LLM + value of pre-trained skills for real-world execution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines language models with pre-trained low-level skills' value functions to ground high-level language commands into feasible real-world robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SayCan agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses a language model to propose high-level actions and uses pre-trained skill value functions (learned from real-world experience) to score feasibility, effectively combining semantic planning (LLM) with learned low-level affordances (value-based).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot / robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Real-world manipulation environments with pre-trained skills</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Everyday real environments where pre-trained low-level controllers exist; complexity due to long-horizon goals and need to ground abstract language into feasible motor skills.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of pre-trained skills available; complexity of task decomposition required to accomplish long-horizon commands.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation in environment contexts and tasks where pre-trained skills must be feasible; degree of novelty encountered relative to skill repertoire.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task completion rate when combining LLM proposals with skill feasibility scoring</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey frames SayCan as addressing a trade-off: LLMs provide broad semantic knowledge (helping handle variation), while value functions of pre-trained skills constrain proposals to feasible actions in complex environments—thus mitigating failure modes from high variation or high complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Combine LLM planning with pre-trained low-level skills evaluated via learned value functions (hybrid LLM+RL/skill library).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Reported to enable LLMs to execute abstract long-term commands by leveraging real-world skill values; survey provides qualitative description only.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relies on pre-trained skills (which require separate data collection); sample efficiency of LLM proposals improved because skills reduce need for retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hybrid approaches that combine high-level LLM reasoning with feasibility-aware low-level skills provide a practical trade-off to handle both environment complexity and variation, improving real-world executability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1058.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statler</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statler (State-maintaining LLM framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that gives LLMs a memory/world-state representation (world model reader and writer instances) enabling them to maintain changing world states over time for embodied reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Statler-based LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two interacting LLM instances (reader and writer) maintain and update an explicit world-state memory, enabling temporal stateful reasoning for planning and execution correction.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>embodied agent / robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Dynamic environments requiring stateful, time-extended reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where world states evolve during execution and need to be tracked (e.g., long-horizon tasks with changing object states and partial observability); complexity driven by temporal dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Length of temporal horizon and number of state variables to be tracked; requirement to maintain and update memory over time.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variability in state transitions and types of events that must be remembered; not quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success improvement due to stateful memory and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey indicates that enabling LLMs with world-state memory reduces failures in dynamic/complex environments by allowing persistent representations over time, thus helping generalization across temporal variations.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Architectural augmentation adding explicit state memory (reader/writer LLM instances) to support embodied reasoning and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey reports improved embodied reasoning and planning performance in simulated domains and real robots when world-state memory is available; numeric details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing LLMs with explicit, updateable world-state memory helps agents handle temporal complexity and partial observability, a key factor when environments vary over time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1058.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NavGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NavGPT (LLM-based instruction-tracking navigation agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based navigation agent performing zero-shot sequential action prediction for visual-and-language navigation, demonstrating reasoning in navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NavGPT agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses large language models to maintain instruction tracking and predict next navigation actions in partially observable visual navigation tasks (zero-shot sequential action prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated or mobile robotic agent (navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Visual-and-language navigation (VLN) environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable indoor navigation scenes where the agent must interpret natural language and visual inputs to navigate; complexity from long horizon, partial observability, and scene variation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sequence length (number of steps), partial observability, number of landmarks/objects to recognize; datasets have thousands of trajectories (e.g., R2R etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation across environment layouts, language instructions, and scene appearances (many datasets with multiple buildings/scenes); procedural or dataset-based variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate / Oracle Success / SPL / Navigation Error (m)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes LLM-based navigation benefits from reasoning for long-horizon tasks, but performance depends on alignment between LLM inference and perception modules; high environmental variation increases perceptual demands and can reduce navigation accuracy without robust cross-modal grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Zero-shot LLM inference supplemented by visual perception modules; sometimes trained with speaker-driven augmentation and pragmatic reasoning or self-supervised imitation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Claims of improved reasoning and zero-shot capabilities in VLN benchmarks qualitatively, but no numeric values in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can provide strong sequential reasoning for navigation, particularly in long-horizon tasks; however, dense environment variation and partial observability necessitate strong perception grounding and possibly modular architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1058.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ManiSkill2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ManiSkill2 (maniskill2 benchmark for generalizable manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark (ManiSkill2) comprising multiple manipulation tasks designed to test generalization and robustness of generalizable manipulation skills across objects and scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Benchmarked embodied agents (ManiSkill2 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Various learning algorithms (RL, imitation learning, fine-tuned VLMs) are evaluated on a set of 20 manipulation tasks that expose challenges like long-horizon planning, contact dynamics and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated and real robotic agents (benchmark evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ManiSkill2 manipulation benchmark environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>20 operational tasks covering distinct manipulation challenges (object categories, articulated objects, dexterous interactions); complexity derived from task diversity and contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of tasks (20), task-specific degrees of difficulty (long-horizon, contact-rich), diversity of object categories.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Holdout tasks and holdout object instances; survey references splitting into holdout tasks for evaluation (e.g., 88 holdout tasks in ActivityPrograms example), indicating explicit variation testing protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Plan success rate and execution success rate across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey emphasizes ManiSkill2 as a tool to probe generalization: higher environment complexity (contact-rich, long-horizon) exposes limitations in generalization unless models are fine-tuned or use staged training strategies; more variation requires stronger pretraining/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Benchmark used to evaluate varied training strategies including two-stage fine-tuning, multi-task training, and modular skill learning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey reports that two-stage fine-tuning strategies can improve generalization on ManiSkill2 tasks, but does not give numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Benchmarks that contain both complex tasks and held-out variations (ManiSkill2) reveal gaps in generalization for current foundation-model-based approaches and motivate staged fine-tuning and more diverse datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1058.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboCat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboCat (Self-improving foundation agent for robotic manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundational, goal-conditional decision transformer agent that can self-generate data for subsequent training and support autonomous improvement loops in manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RoboCat agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Visual goal-conditional decision transformer trained via offline data; demonstrated capability to generalize to new tasks and robots and to generate data to bootstrap further training (self-improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated/robotic agent (manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Manipulation benchmarks and multi-robot/robot variants</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Goal-conditioned manipulation environments with diverse object types and tasks; complexity includes cross-robot transfer and varied goal specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Diversity of goal specifications, number of robot platforms supported, and manipulation task variation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Cross-robot datasets and ability to generate new training data to handle environment variation; survey notes self-generation of data but no quantitative measure.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Generalization to new tasks and robots; ability to improve via self-generated data</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey suggests self-improvement loops (generate-then-train) help cope with environment variation and complexity by expanding training coverage, but quality and dynamics realism of generated data are key trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Offline decision transformer training with self-generated data augmentation for continual improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Reported qualitatively to generalize to new tasks and robots and to enable data generation for further training; numeric performance not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-improvement via data generation is a promising strategy to increase environment variation coverage and thus generalization, but relies on generated data fidelity and may not address low-level dynamics mismatch without real dynamics data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1058.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e1058.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LACO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LACO (Linguistically Conditional Collision Function)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to learn collision functions conditioned on language using only single-view images, language cues, and robot configurations to plan in contact-aware scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language-Conditioned Path Planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LACO-enabled planning agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that use learned, language-conditioned collision functions (supervised learning from images+language) to reason about collisions and plan trajectories that respect task semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent / motion planner (simulated/real)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Contact-aware path planning environments (single-view perception)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where collision semantics depend on object/task context and must be inferred from single-view images and language; complexity arises from partial observability and semantics-conditioned feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Perception ambiguity (single view), number of obstacle/goal configurations, semantic complexity of instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Diversity of visual-linguistic contexts and robot configurations used for training; degree of environment novelty at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Collision-aware planning success / feasibility of planned trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes that learning language-conditioned collision functions helps bridge semantic instructions and geometric constraints, enabling planning under semantic variation; trade-off is reliance on perceptual accuracy from limited views.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised learning of collision functions conditioned on language and image inputs; used in planning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Claimed to enable planning from single views and language cues in varied scenarios; no numeric benchmarks provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating language into collision reasoning permits agents to handle semantic variability in planning, but perceptual uncertainty from limited views is a limiting factor in highly complex or highly variable environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning in the Era of Foundation Models: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-1: Robotics Transformer for Real-World Control at Scale <em>(Rating: 2)</em></li>
                <li>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation <em>(Rating: 2)</em></li>
                <li>RFUniverse: A physics-based action-centric interactive environment for everyday household tasks <em>(Rating: 2)</em></li>
                <li>CLIPORT: What and Where Pathways for Robotic Manipulation <em>(Rating: 2)</em></li>
                <li>VIMA: Robot Manipulation with Multimodal Prompts <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>ManiSkill2: A unified benchmark for generalizable manipulation skills <em>(Rating: 2)</em></li>
                <li>Do as I can, not as I say: Grounding language in robotic affordances (SayCan) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1058",
    "paper_id": "paper-265445462",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "RT-1",
            "name_full": "RT-1 (Robotics Transformer for Real-World Control at Scale)",
            "brief_description": "A vision-language-action model trained on large-scale real robot demonstration data to map image+instruction tokens to robot actions for multi-task manipulation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "RT-1 agent",
            "agent_description": "Vision-language-action transformer trained by behavioral cloning on a large dataset of human/teleoperated robot demonstrations (imitation learning / supervised fine-tuning of VLMs).",
            "agent_type": "physical robot / robotic agent (evaluated on real robots and demonstrations)",
            "environment_name": "Office and kitchen manipulation environments (multi-task real-world)",
            "environment_description": "Real-world domestic/office scenes containing household objects and tasks (multi-object desktop and kitchen manipulation). Complexity arises from contact-rich interactions, object variety, and real sensor noise.",
            "complexity_measure": "Number of distinct tasks and objects in dataset; presence of contact-rich interactions and continuous robot motion (paper reports dataset scale: ~130k demonstrations referenced elsewhere in survey).",
            "complexity_level": "medium-to-high",
            "variation_measure": "Scale and diversity of demonstrations across robots, objects and environments (dataset size ~130k demonstrations referenced); multi-scene collection over months.",
            "variation_level": "high",
            "performance_metric": "Execution success rate / task success (manipulation)",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes RT-1 leverages large, diverse demonstration data to improve generalization across tasks/environments; high variation in demonstrations is treated as a way to handle environment variability though low-level control and dynamics remain challenges.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Large-scale imitation learning / behavioral cloning on multi-task real-world demonstration dataset; multi-task fine-tuning of vision-language models.",
            "generalization_tested": true,
            "generalization_results": "Reported to transfer web-scale vision-language knowledge to robotic control and improve zero-shot/transfer capabilities across many manipulation tasks, but exact numeric gains not stated in survey.",
            "sample_efficiency": "High data usage (~130k demonstrations referenced); not sample-efficient in low-data regimes per survey discussion.",
            "key_findings": "Large, diverse real demonstration datasets plus VLM architectures improve cross-task generalization in manipulation; however, foundation models still lack robust low-level dynamics control and need dynamics data to further improve performance under physical variation.",
            "uuid": "e1058.0",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RoboGen",
            "name_full": "RoboGen (Generative-simulation data generation for robot learning)",
            "brief_description": "A generative-simulation method that synthesizes (effectively infinite) simulated robot data to automate dataset creation for training robot policies.",
            "citation_title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
            "mention_or_use": "mention",
            "agent_name": "RoboGen-trained agents (simulated)",
            "agent_description": "Agents trained in simulation using data generated by RoboGen; learning paradigm typically imitation learning / supervised on generated trajectories and/or RL in sim.",
            "agent_type": "simulated agents (physics simulation)",
            "environment_name": "Simulated household and manipulation task environments (RFUniverse-style / generated scenes)",
            "environment_description": "Procedurally generated simulated household tasks including object interactions (fruit picking, cloth folding, wiping, pouring). Complexity provided by multi-step tasks and contact dynamics simulated; variation introduced by procedural generation.",
            "complexity_measure": "Number and variety of atomic operations supported (~87 atomic operations in RFUniverse cited); number of task types (multiple household tasks).",
            "complexity_level": "medium",
            "variation_measure": "Procedural generation parameters enabling many scene and object instantiations (effectively unbounded simulated instances); domain randomization degree determined by generator.",
            "variation_level": "high",
            "performance_metric": "Task completion / transfer performance when porting sim-trained models to target tasks (implicit)",
            "performance_value": null,
            "complexity_variation_relationship": "Survey highlights RoboGen's approach to trade off expensive real data by increasing simulated variation; increased simulated variation aims to improve generalization but real-world dynamics mismatch remains a concern.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Generative simulation / procedural data generation; train in simulation possibly with domain randomization; large-scale supervised learning.",
            "generalization_tested": true,
            "generalization_results": "Intended to improve generalization by providing massive simulated variation; survey reports the method as a step towards automation but no numeric generalization metrics provided.",
            "sample_efficiency": "Relies on generating large volumes of synthetic interactions (high sample usage in sim but intended to reduce real-data needs).",
            "key_findings": "Using generative simulation to expand dataset scale/variation is promising to handle environment variability, but sim-to-real transfer and capturing realistic dynamics remain challenging trade-offs.",
            "uuid": "e1058.1",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RFUniverse",
            "name_full": "RFUniverse (physics-based action-centric environment)",
            "brief_description": "A physics-based simulated environment that supports a rich set of atomic operations and primitive object types for training embodied agents on daily household tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Agents trained in RFUniverse",
            "agent_description": "Agents trained in simulation using imitation learning and RL on atomic operations (pick/place, pouring, folding), typically evaluated on multi-task household scenarios.",
            "agent_type": "simulated agent",
            "environment_name": "RFUniverse simulated household environment",
            "environment_description": "Visually and physically plausible simulation supporting 87 atomic operations and 8 primitive object types; tasks include fruit picking, cloth folding, sponge wiping, milk pouring—complexity arises from articulated/deformable objects and contact dynamics.",
            "complexity_measure": "Number of atomic operations (87), number of primitive object types (8), and diversity of task types.",
            "complexity_level": "high (contact-rich, deformable objects included)",
            "variation_measure": "Variety of tasks and object types; ability to generate diverse scenarios but explicit counts of environment instances not provided.",
            "variation_level": "medium-to-high",
            "performance_metric": "Task success across simulated tasks",
            "performance_value": null,
            "complexity_variation_relationship": "Survey frames RFUniverse as providing complexity (contact-rich interactions) and variation (many operation types) to train agents; highlights that adding physics and diverse primitives improves realism but increases training difficulty and need for dynamics-aware models.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Simulation-based training with many atomic operations; imitation and RL in physics-rich sim.",
            "generalization_tested": true,
            "generalization_results": "Used to demonstrate usability across various tasks; provides capability to test policies on multiple operation types though concrete generalization numbers not reported in survey.",
            "sample_efficiency": "Requires many simulated interactions due to contact complexity; sample counts not specified.",
            "key_findings": "Physics-rich simulated environments with many atomic operations increase task realism and prepare agents for varied real-world scenarios, but increase the need for dynamics data and more sophisticated training (world models, dynamics-aware learning).",
            "uuid": "e1058.2",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CLIPORT",
            "name_full": "CLIPORT (Language-conditioned Imitation Agent combining CLIP and Transporter)",
            "brief_description": "End-to-end language-conditioned imitation learning agent that combines CLIP for semantic grounding with Transporter for spatial precision to solve desktop manipulation tasks specified by language.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "CLIPORT agent",
            "agent_description": "Autoregressive imitation-learning agent using visual-language embeddings (CLIP) and spatial action models (Transporter); trained on paired image-instruction-action data.",
            "agent_type": "simulated and real robotic agent (manipulation)",
            "environment_name": "Language-specified desktop manipulation tasks",
            "environment_description": "Structured desktop scenes with objects arranged for pick/place and simple multi-step routines; complexity arises from language grounding and precise spatial interactions.",
            "complexity_measure": "Number of language-specified tasks and objects in dataset (CLIPORT benchmark tasks number referenced qualitatively); requirement for spatial precision.",
            "complexity_level": "medium",
            "variation_measure": "Variation in natural language instructions and object appearances; dataset includes multiple language-task pairs but explicit counts not given in survey.",
            "variation_level": "medium",
            "performance_metric": "Task success rate on language-conditioned manipulation",
            "performance_value": null,
            "complexity_variation_relationship": "Survey describes CLIPORT as achieving good performance by separating semantic understanding and spatial precision—implying a trade-off where semantic complexity (language variation) can be mitigated by strong perception modules while spatial complexity requires spatial inductive bias.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Language-conditioned imitation learning (end-to-end), leveraging pretrained visual-language models (CLIP) and transport-based spatial modules.",
            "generalization_tested": true,
            "generalization_results": "Shows improved generalization across language instructions compared to baselines due to CLIP semantic features; survey gives qualitative assessment only.",
            "sample_efficiency": "Trained on moderate-sized task-specific datasets (hundreds of demonstrations), not quantified in survey.",
            "key_findings": "Modularizing semantic grounding and spatial reasoning helps handle language variation and spatial complexity; pretrained VLMs improve semantic generalization but spatial inductive biases remain important for precise control.",
            "uuid": "e1058.3",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "VIMA",
            "name_full": "VIMA (Transformer-based robotic agent using multimodal prompts)",
            "brief_description": "Transformer-based agent that interprets multimodal prompts (interleaved language and images/video frames) and outputs motor actions autoregressively for various manipulation tasks.",
            "citation_title": "VIMA: Robot Manipulation with Multimodal Prompts",
            "mention_or_use": "mention",
            "agent_name": "VIMA agent",
            "agent_description": "Transformer agent using multimodal prompt conditioning and imitation learning; maps multimodal prompts to sequences of motor actions (behavioral cloning).",
            "agent_type": "robotic agent (simulated and physical manipulation)",
            "environment_name": "Multimodal-prompted manipulation environments",
            "environment_description": "Tasks represented as sequences of images and language interleaved into prompts; complexity stems from temporally extended goals and necessity to interpret multimodal context.",
            "complexity_measure": "Prompt complexity (length and multimodal content), number of task steps, and degrees-of-freedom of actions.",
            "complexity_level": "medium-to-high",
            "variation_measure": "Diversity of multimodal prompts, number of different tasks and objects used in training; survey indicates many tasks formulated as multimodal prompts.",
            "variation_level": "high",
            "performance_metric": "Task success / success rate",
            "performance_value": null,
            "complexity_variation_relationship": "Survey emphasizes that expressing tasks as multimodal prompts allows VIMA to handle variation in instructions and visual scenes, but integrating perception and action remains challenging in highly complex contact-rich domains.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multimodal prompt-based imitation learning with transformer architectures and multimodal encoders.",
            "generalization_tested": true,
            "generalization_results": "Authors claim broad expressivity and generalization across multimodal task formulations; survey reports qualitative improvements without specific numeric metrics.",
            "sample_efficiency": "Depends on multi-task demonstration datasets; sample counts not detailed in survey.",
            "key_findings": "Multimodal prompt representations enable concise specification of diverse tasks and support generalization across instruction styles and visual contexts; however, handling physical dynamics and contact-rich manipulation still requires additional dynamics-aware data.",
            "uuid": "e1058.4",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MOMA-Force",
            "name_full": "MOMA-Force (Visual-force imitation for mobile manipulation)",
            "brief_description": "An imitation learning approach that binds visual perception to force control to learn contact-rich tasks with high robustness and low contact forces.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "MOMA-Force agent",
            "agent_description": "Learns perceptual representations combined with imitation learning and admittance whole-body control to produce force-aware policies for contact-rich mobile manipulation.",
            "agent_type": "physical mobile manipulator",
            "environment_name": "Contact-rich manipulation tasks (mobile manipulation)",
            "environment_description": "Tasks that require whole-body contact control (e.g., pushing, wiping) in real-world settings; complexity comes from contact dynamics and whole-body coordination.",
            "complexity_measure": "Contact richness (presence of force interactions), number of manipulation primitives, and success rate on multiple complex tasks.",
            "complexity_level": "high",
            "variation_measure": "Different contact scenarios and object types; survey notes multiple complex tasks learned but does not quantify count.",
            "variation_level": "medium",
            "performance_metric": "Success rate and contact force magnitude (robustness and controllability)",
            "performance_value": "reported qualitatively as 'high success rate and small contact force' (no numeric value in survey)",
            "complexity_variation_relationship": "Survey suggests that incorporating force information (dynamics) helps agents handle complexity in contact-rich tasks and improves robustness to variation, implying a positive role of dynamics-aware modalities in coping with environment variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Perceptual representation learning + imitation learning + admittance whole-body control.",
            "generalization_tested": true,
            "generalization_results": "Demonstrated ability to learn multiple complex contact-rich tasks with robust contact force control; exact generalization metrics not supplied.",
            "sample_efficiency": null,
            "key_findings": "Fusing visual and force modalities and using dynamics-aware controllers significantly improves performance on contact-rich tasks and robustness to perturbations, highlighting the importance of dynamics data for generalization in complex environments.",
            "uuid": "e1058.5",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E (An embodied multimodal language model)",
            "brief_description": "A multimodal model that incorporates continuous sensor modalities (vision, proprioception, etc.) into a large language model to establish links between words and perception for sequential robot planning and QA.",
            "citation_title": "PaLM-E: An embodied multimodal language model",
            "mention_or_use": "mention",
            "agent_name": "PaLM-E-based agents",
            "agent_description": "LLM grounded with continuous sensor inputs (visual, proprioceptive) trained end-to-end for tasks including sequential robot operation planning, VQA and captioning; uses transformer/Large language model architecture with multimodal encoders.",
            "agent_type": "embodied agent / robotic agent (simulated and real)",
            "environment_name": "General embodied tasks spanning manipulation, perception and planning",
            "environment_description": "Multimodal embodied settings where agent must combine vision, continuous states and language to plan sequences of operations; complexity arises from combining modalities and long-horizon planning.",
            "complexity_measure": "Multimodal input dimensionality (visual frames, continuous state vectors), number of sequential steps in plans.",
            "complexity_level": "high",
            "variation_measure": "Diversity of sensor observations and tasks in pretraining/fine-tuning; survey references many downstream tasks but doesn't quantify variation.",
            "variation_level": "high",
            "performance_metric": "Task planning success, VQA accuracy, sequential operation execution quality",
            "performance_value": null,
            "complexity_variation_relationship": "Survey highlights PaLM-E as an approach to bridge perception and language for embodied reasoning; increasing multimodal complexity requires larger models and diverse data, improving reasoning but exacerbating computation and data demands.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "End-to-end multimodal fine-tuning of LLM with continuous sensor encodings; transfer from pre-trained language models.",
            "generalization_tested": true,
            "generalization_results": "Demonstrated ability to perform varied embodied tasks by grounding LLMs with sensors; survey gives qualitative claims without numeric metrics.",
            "sample_efficiency": "Large pretrained models leveraged; sample efficiency relative to small models not quantified.",
            "key_findings": "Integrating continuous sensor modalities into LLMs improves embodied reasoning and planning across varied tasks, but requires significant multimodal data and computation; highlights trade-off between model scale and environment variation handled.",
            "uuid": "e1058.6",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan (LLM + value of pre-trained skills for real-world execution)",
            "brief_description": "A method that combines language models with pre-trained low-level skills' value functions to ground high-level language commands into feasible real-world robot actions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "SayCan agent",
            "agent_description": "Uses a language model to propose high-level actions and uses pre-trained skill value functions (learned from real-world experience) to score feasibility, effectively combining semantic planning (LLM) with learned low-level affordances (value-based).",
            "agent_type": "physical robot / robotic agent",
            "environment_name": "Real-world manipulation environments with pre-trained skills",
            "environment_description": "Everyday real environments where pre-trained low-level controllers exist; complexity due to long-horizon goals and need to ground abstract language into feasible motor skills.",
            "complexity_measure": "Number of pre-trained skills available; complexity of task decomposition required to accomplish long-horizon commands.",
            "complexity_level": "medium-to-high",
            "variation_measure": "Variation in environment contexts and tasks where pre-trained skills must be feasible; degree of novelty encountered relative to skill repertoire.",
            "variation_level": "medium",
            "performance_metric": "Task completion rate when combining LLM proposals with skill feasibility scoring",
            "performance_value": null,
            "complexity_variation_relationship": "Survey frames SayCan as addressing a trade-off: LLMs provide broad semantic knowledge (helping handle variation), while value functions of pre-trained skills constrain proposals to feasible actions in complex environments—thus mitigating failure modes from high variation or high complexity.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Combine LLM planning with pre-trained low-level skills evaluated via learned value functions (hybrid LLM+RL/skill library).",
            "generalization_tested": true,
            "generalization_results": "Reported to enable LLMs to execute abstract long-term commands by leveraging real-world skill values; survey provides qualitative description only.",
            "sample_efficiency": "Relies on pre-trained skills (which require separate data collection); sample efficiency of LLM proposals improved because skills reduce need for retraining.",
            "key_findings": "Hybrid approaches that combine high-level LLM reasoning with feasibility-aware low-level skills provide a practical trade-off to handle both environment complexity and variation, improving real-world executability.",
            "uuid": "e1058.7",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Statler",
            "name_full": "Statler (State-maintaining LLM framework)",
            "brief_description": "A framework that gives LLMs a memory/world-state representation (world model reader and writer instances) enabling them to maintain changing world states over time for embodied reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Statler-based LLM agents",
            "agent_description": "Two interacting LLM instances (reader and writer) maintain and update an explicit world-state memory, enabling temporal stateful reasoning for planning and execution correction.",
            "agent_type": "embodied agent / robotic agent",
            "environment_name": "Dynamic environments requiring stateful, time-extended reasoning",
            "environment_description": "Environments where world states evolve during execution and need to be tracked (e.g., long-horizon tasks with changing object states and partial observability); complexity driven by temporal dependencies.",
            "complexity_measure": "Length of temporal horizon and number of state variables to be tracked; requirement to maintain and update memory over time.",
            "complexity_level": "high",
            "variation_measure": "Variability in state transitions and types of events that must be remembered; not quantified in survey.",
            "variation_level": "medium",
            "performance_metric": "Task success improvement due to stateful memory and reasoning",
            "performance_value": null,
            "complexity_variation_relationship": "Survey indicates that enabling LLMs with world-state memory reduces failures in dynamic/complex environments by allowing persistent representations over time, thus helping generalization across temporal variations.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Architectural augmentation adding explicit state memory (reader/writer LLM instances) to support embodied reasoning and planning.",
            "generalization_tested": true,
            "generalization_results": "Survey reports improved embodied reasoning and planning performance in simulated domains and real robots when world-state memory is available; numeric details not provided.",
            "sample_efficiency": null,
            "key_findings": "Providing LLMs with explicit, updateable world-state memory helps agents handle temporal complexity and partial observability, a key factor when environments vary over time.",
            "uuid": "e1058.8",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "NavGPT",
            "name_full": "NavGPT (LLM-based instruction-tracking navigation agent)",
            "brief_description": "An LLM-based navigation agent performing zero-shot sequential action prediction for visual-and-language navigation, demonstrating reasoning in navigation tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "NavGPT agent",
            "agent_description": "Uses large language models to maintain instruction tracking and predict next navigation actions in partially observable visual navigation tasks (zero-shot sequential action prediction).",
            "agent_type": "simulated or mobile robotic agent (navigation)",
            "environment_name": "Visual-and-language navigation (VLN) environments",
            "environment_description": "Partially observable indoor navigation scenes where the agent must interpret natural language and visual inputs to navigate; complexity from long horizon, partial observability, and scene variation.",
            "complexity_measure": "Sequence length (number of steps), partial observability, number of landmarks/objects to recognize; datasets have thousands of trajectories (e.g., R2R etc.).",
            "complexity_level": "high",
            "variation_measure": "Variation across environment layouts, language instructions, and scene appearances (many datasets with multiple buildings/scenes); procedural or dataset-based variation.",
            "variation_level": "high",
            "performance_metric": "Success rate / Oracle Success / SPL / Navigation Error (m)",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes LLM-based navigation benefits from reasoning for long-horizon tasks, but performance depends on alignment between LLM inference and perception modules; high environmental variation increases perceptual demands and can reduce navigation accuracy without robust cross-modal grounding.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Zero-shot LLM inference supplemented by visual perception modules; sometimes trained with speaker-driven augmentation and pragmatic reasoning or self-supervised imitation.",
            "generalization_tested": true,
            "generalization_results": "Claims of improved reasoning and zero-shot capabilities in VLN benchmarks qualitatively, but no numeric values in survey.",
            "sample_efficiency": null,
            "key_findings": "LLMs can provide strong sequential reasoning for navigation, particularly in long-horizon tasks; however, dense environment variation and partial observability necessitate strong perception grounding and possibly modular architectures.",
            "uuid": "e1058.9",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ManiSkill2",
            "name_full": "ManiSkill2 (maniskill2 benchmark for generalizable manipulation)",
            "brief_description": "A benchmark (ManiSkill2) comprising multiple manipulation tasks designed to test generalization and robustness of generalizable manipulation skills across objects and scenes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Benchmarked embodied agents (ManiSkill2 tasks)",
            "agent_description": "Various learning algorithms (RL, imitation learning, fine-tuned VLMs) are evaluated on a set of 20 manipulation tasks that expose challenges like long-horizon planning, contact dynamics and generalization.",
            "agent_type": "simulated and real robotic agents (benchmark evaluation)",
            "environment_name": "ManiSkill2 manipulation benchmark environments",
            "environment_description": "20 operational tasks covering distinct manipulation challenges (object categories, articulated objects, dexterous interactions); complexity derived from task diversity and contact dynamics.",
            "complexity_measure": "Number of tasks (20), task-specific degrees of difficulty (long-horizon, contact-rich), diversity of object categories.",
            "complexity_level": "high",
            "variation_measure": "Holdout tasks and holdout object instances; survey references splitting into holdout tasks for evaluation (e.g., 88 holdout tasks in ActivityPrograms example), indicating explicit variation testing protocols.",
            "variation_level": "high",
            "performance_metric": "Plan success rate and execution success rate across tasks",
            "performance_value": null,
            "complexity_variation_relationship": "Survey emphasizes ManiSkill2 as a tool to probe generalization: higher environment complexity (contact-rich, long-horizon) exposes limitations in generalization unless models are fine-tuned or use staged training strategies; more variation requires stronger pretraining/fine-tuning.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Benchmark used to evaluate varied training strategies including two-stage fine-tuning, multi-task training, and modular skill learning.",
            "generalization_tested": true,
            "generalization_results": "Survey reports that two-stage fine-tuning strategies can improve generalization on ManiSkill2 tasks, but does not give numeric results.",
            "sample_efficiency": null,
            "key_findings": "Benchmarks that contain both complex tasks and held-out variations (ManiSkill2) reveal gaps in generalization for current foundation-model-based approaches and motivate staged fine-tuning and more diverse datasets.",
            "uuid": "e1058.10",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RoboCat",
            "name_full": "RoboCat (Self-improving foundation agent for robotic manipulation)",
            "brief_description": "A foundational, goal-conditional decision transformer agent that can self-generate data for subsequent training and support autonomous improvement loops in manipulation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "RoboCat agent",
            "agent_description": "Visual goal-conditional decision transformer trained via offline data; demonstrated capability to generalize to new tasks and robots and to generate data to bootstrap further training (self-improvement).",
            "agent_type": "simulated/robotic agent (manipulation)",
            "environment_name": "Manipulation benchmarks and multi-robot/robot variants",
            "environment_description": "Goal-conditioned manipulation environments with diverse object types and tasks; complexity includes cross-robot transfer and varied goal specifications.",
            "complexity_measure": "Diversity of goal specifications, number of robot platforms supported, and manipulation task variation.",
            "complexity_level": "medium-to-high",
            "variation_measure": "Cross-robot datasets and ability to generate new training data to handle environment variation; survey notes self-generation of data but no quantitative measure.",
            "variation_level": "high",
            "performance_metric": "Generalization to new tasks and robots; ability to improve via self-generated data",
            "performance_value": null,
            "complexity_variation_relationship": "Survey suggests self-improvement loops (generate-then-train) help cope with environment variation and complexity by expanding training coverage, but quality and dynamics realism of generated data are key trade-offs.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Offline decision transformer training with self-generated data augmentation for continual improvement.",
            "generalization_tested": true,
            "generalization_results": "Reported qualitatively to generalize to new tasks and robots and to enable data generation for further training; numeric performance not provided in survey.",
            "sample_efficiency": null,
            "key_findings": "Self-improvement via data generation is a promising strategy to increase environment variation coverage and thus generalization, but relies on generated data fidelity and may not address low-level dynamics mismatch without real dynamics data.",
            "uuid": "e1058.11",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LACO",
            "name_full": "LACO (Linguistically Conditional Collision Function)",
            "brief_description": "A method to learn collision functions conditioned on language using only single-view images, language cues, and robot configurations to plan in contact-aware scenarios.",
            "citation_title": "Language-Conditioned Path Planning",
            "mention_or_use": "mention",
            "agent_name": "LACO-enabled planning agents",
            "agent_description": "Agents that use learned, language-conditioned collision functions (supervised learning from images+language) to reason about collisions and plan trajectories that respect task semantics.",
            "agent_type": "robotic agent / motion planner (simulated/real)",
            "environment_name": "Contact-aware path planning environments (single-view perception)",
            "environment_description": "Environments where collision semantics depend on object/task context and must be inferred from single-view images and language; complexity arises from partial observability and semantics-conditioned feasibility.",
            "complexity_measure": "Perception ambiguity (single view), number of obstacle/goal configurations, semantic complexity of instructions.",
            "complexity_level": "medium",
            "variation_measure": "Diversity of visual-linguistic contexts and robot configurations used for training; degree of environment novelty at test time.",
            "variation_level": "medium",
            "performance_metric": "Collision-aware planning success / feasibility of planned trajectories",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes that learning language-conditioned collision functions helps bridge semantic instructions and geometric constraints, enabling planning under semantic variation; trade-off is reliance on perceptual accuracy from limited views.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Supervised learning of collision functions conditioned on language and image inputs; used in planning loop.",
            "generalization_tested": true,
            "generalization_results": "Claimed to enable planning from single views and language cues in varied scenarios; no numeric benchmarks provided in survey.",
            "sample_efficiency": null,
            "key_findings": "Incorporating language into collision reasoning permits agents to handle semantic variability in planning, but perceptual uncertainty from limited views is a limiting factor in highly complex or highly variable environments.",
            "uuid": "e1058.12",
            "source_info": {
                "paper_title": "Robot Learning in the Era of Foundation Models: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-1: Robotics Transformer for Real-World Control at Scale",
            "rating": 2,
            "sanitized_title": "rt1_robotics_transformer_for_realworld_control_at_scale"
        },
        {
            "paper_title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
            "rating": 2,
            "sanitized_title": "robogen_towards_unleashing_infinite_data_for_automated_robot_learning_via_generative_simulation"
        },
        {
            "paper_title": "RFUniverse: A physics-based action-centric interactive environment for everyday household tasks",
            "rating": 2,
            "sanitized_title": "rfuniverse_a_physicsbased_actioncentric_interactive_environment_for_everyday_household_tasks"
        },
        {
            "paper_title": "CLIPORT: What and Where Pathways for Robotic Manipulation",
            "rating": 2,
            "sanitized_title": "cliport_what_and_where_pathways_for_robotic_manipulation"
        },
        {
            "paper_title": "VIMA: Robot Manipulation with Multimodal Prompts",
            "rating": 2,
            "sanitized_title": "vima_robot_manipulation_with_multimodal_prompts"
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "ManiSkill2: A unified benchmark for generalizable manipulation skills",
            "rating": 2,
            "sanitized_title": "maniskill2_a_unified_benchmark_for_generalizable_manipulation_skills"
        },
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances (SayCan)",
            "rating": 1,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances_saycan"
        }
    ],
    "cost": 0.026875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Robot Learning in the Era of Foundation Models: A Survey</p>
<p>Xuan Xiao 
National Key Laboratory of Autonomous Intelligent Unmanned Systems
Tongji University
ShanghaiChina</p>
<p>Frontiers Science Center for Intelligent Autonomous Systems
Ministry of Education
Tongji University
ShanghaiChina</p>
<p>Institute of Acoustics
School of Physics Science and Engineering
Tongji University
ShanghaiChina</p>
<p>Jiahang Liu 
National Key Laboratory of Autonomous Intelligent Unmanned Systems
Tongji University
ShanghaiChina</p>
<p>Frontiers Science Center for Intelligent Autonomous Systems
Ministry of Education
Tongji University
ShanghaiChina</p>
<p>College of Electronics and Information Engineering
Tongji University
ShanghaiChina</p>
<p>Zhipeng Wang 
National Key Laboratory of Autonomous Intelligent Unmanned Systems
Tongji University
ShanghaiChina</p>
<p>Frontiers Science Center for Intelligent Autonomous Systems
Ministry of Education
Tongji University
ShanghaiChina</p>
<p>College of Electronics and Information Engineering
Tongji University
ShanghaiChina</p>
<p>Yanmin Zhou 
National Key Laboratory of Autonomous Intelligent Unmanned Systems
Tongji University
ShanghaiChina</p>
<p>Frontiers Science Center for Intelligent Autonomous Systems
Ministry of Education
Tongji University
ShanghaiChina</p>
<p>College of Electronics and Information Engineering
Tongji University
ShanghaiChina</p>
<p>Yong Qi 
School of Electronic Information and Artificial Intelligence
Shaanxi University of Science and Technology
ShaanxiChina</p>
<p>Qian Cheng 
National Key Laboratory of Autonomous Intelligent Unmanned Systems
Tongji University
ShanghaiChina</p>
<p>Frontiers Science Center for Intelligent Autonomous Systems
Ministry of Education
Tongji University
ShanghaiChina</p>
<p>College of Electronics and Information Engineering
Tongji University
ShanghaiChina</p>
<p>Institute of Acoustics
School of Physics Science and Engineering
Tongji University
ShanghaiChina</p>
<p>Bin He 
National Key Laboratory of Autonomous Intelligent Unmanned Systems
Tongji University
ShanghaiChina</p>
<p>Frontiers Science Center for Intelligent Autonomous Systems
Ministry of Education
Tongji University
ShanghaiChina</p>
<p>Shuo Jiang 
National Key Laboratory of Autonomous Intelligent Unmanned Systems
Tongji University
ShanghaiChina</p>
<p>Frontiers Science Center for Intelligent Autonomous Systems
Ministry of Education
Tongji University
ShanghaiChina</p>
<p>College of Electronics and Information Engineering
Tongji University
ShanghaiChina</p>
<p>Robot Learning in the Era of Foundation Models: A Survey
8969E5949A23AA79310196164FD34613Robot LearningFoundation ModelsEmbodied AI
The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI).Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application.However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics.The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas.Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework.In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios.Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed.This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.</p>
<p>Introduction</p>
<p>Robots have played an important role in various scenarios including industrial [1] , medical [2] , service [3] and special robot [4] industry.With the increasing task complexity and working environment variability, the demands for robot tasks have shifted from fixed automation to general artificial intelligence, where robot learning will be the core enabling techniques of the autonomous systems [5] .</p>
<p>The field of robot learning lies at the junction of machine learning and robotics.Its primary focus is enabling robots to acquire new skills or adapt to their environment by utilizing learning algorithms.Some examples of skills that learning algorithms target include sensorimotor skills (like locomotion, grasping and active object classification), interaction skills (such as co-manipulating objects with humans) and linguistic skills (including the grounded and situated meaning of human language).Learning can be acquired through autonomous self-exploration or through demonstrated teaching.</p>
<p>Extensive research studies have been performed on how to realize robot learning.Traditional robot learning techniques are generally divided into imitation learning [6][7][8][9] and reinforcement learning [10,11] .Imitation learning enables robots to learn new skills by mimicking human behavior, while reinforcement learning allows robots to optimize the outcome of skill execution.Imitation learning is an important way to initialize and improve learning efficiency in reinforcement learning.However, they all suffer from certain limitations.Imitation learning is a straightforward and stable form of supervised learning.It requires labeled behavior data and is unable to surpass human-level performance.Although reinforcement learning has the potential to surpass human-level performance, it requires defining the reward function, addressing the policy exploration challenge, and may encounter convergence issues and instability during training.The characteristics of robot learning is that robots are physically embodied and environmentally situated.Thus, a critical challenge in robot learning is to close the perception-action loop in the practical scenarios and there are still some problems in the practical application of traditional robot learning, including insufficient generalization of tasks, insufficient environmental adaptability, low execution accuracy, and lack of planning and reasoning capabilities [12][13][14] .</p>
<p>Recently, with the release of ChatGPT, foundation models, particularly multi-modal foundation models, presented both opportunities and challenges for robot learning.LLMs have demonstrated significant potential in achieving human-level intelligence, thereby leading to a surge of research in robotics based on LLMs.Leveraging multi-domain prior knowledge, LLMs have made breakthroughs in understanding complex task, engaging in continuous dialogue, and performing zero-sample reasoning.However, LLMs still lack the general perception ability of the external environment, which can be addressed by employing the multi-modal foundation model incorporating 2D&amp;3D vision, LiDAR, voice, inertial motion unit (IMU), etc.To fully unleash the potential of the foundational model and address the current challenges in robot learning, so that robots can learn human behavior and skillfully undertake a series of tasks, researchers have developed task-specific robot learning architectures.However, these models were proposed independently and relatively recently.Upon thorough investigation, existing surveys on robot learning are primarily focused on a single task and predominantly rely on traditional methods [5,[15][16][17][18][19] .There are few literature reviews on multi-task robot learning based on foundation models.Therefore, it is crucial to construct an overall summary analysis of existing LLM-based robot learning work, which is of great significance for a comprehensive understanding of the field and to provide inspiration for future research [20] .</p>
<p>We organize this survey according to the solutions of the following three research questions.What platforms does the embodied AI need?What foundation model algorithms, strategies, and mechanisms are currently used for downstream tasks in robot learning?What are promising future research areas in robotics learning for current research questions?In this paper, we systematically survey the relatively new field of foundation model-based robot learning, establish a clear taxonomy for the existing research in the field, focusing on the four aspects of robot learning: manipulation, navigation, planning, and reasoning.We also identify several challenges in this field and discuss potential future directions.The article follows the organizational structure presented below (Fig. 1).</p>
<p>We select references based on the following criteria: 1) Our primary objective is to encompass all significant milestones in robot learning during the advent of foundation models across diverse tasks.2) the diversity and significance of subfields and research groups should be considered.3) A valuable source entails conducting a reverse search beginning with highly influential publications [21] .Fig. 1.Overall structure of the survey.</p>
<p>Overview</p>
<p>Technical Evolution</p>
<p>In the exploration and discovery of robot learning, it has gone through different stages of historical evolution (Fig. 2).</p>
<p>They can be summarized as the following four categories: robot learning through teaching programming (TP) including kinesthetic teaching and teleoperation, reinforcement learning in interaction, embodied imitation learning [22,23] and learning in AIGC-based generative models [24][25][26] .Fig. 2. Technical Evolution [27][28][29][30] .</p>
<p>Kinesthetic</p>
<p>teaching [31] involves physically maneuvering the robot to perform the necessary actions, while the robot's onboard sensors record status information, which is then used to generate training data for the machine learning model.Although this method is relatively simple, the quality of the demonstration relies heavily on the operator's ability to perform movements with flexibility and smoothness.While it proves highly effective for robotic arms, its application becomes more challenging on other platforms, such as legged robots or dexterous hands.Another demonstration method is teleoperation [32] , which enables trajectory learning, task learning, grabbing or more advanced tasks, by providing external input to the robot through handles, graphical interfaces or other methods.There are currently a variety of interactive devices (such as haptic devices or VR interactive devices).In contrast to kinesthetic teaching, teleoperation allows for remote implementation, eliminating the need for the user and the robot to be physically present at the same site.Limitations of teleoperation include the additional work required to develop input interfaces, a longer user training process, and usability risks posed by external devices.</p>
<p>Reinforcement learning involves determining optimal actions in a given situation to maximize the digital revenue signal.Rather than being explicitly instructed on which actions to take, learners must independently identify the actions that yield the greatest benefits.Reinforcement learning focuses on finding a balance between exploring unknown territory and leveraging current knowledge.A typical framework for a reinforcement learning scenario: an agent takes an action in the environment, and the action is interpreted as a reward and state representation, which is then fed back to the agent [33] .</p>
<p>Embodied imitation learning [34] mainly involves robots learning by observing the actions of the demonstrator (such as watching videos).In this method, the presenter utilizes their own body to perform the task, while an external device records their movements.This approach proves to be the most straightforward for presenters, as it does not necessitate any training in the presentation process.Furthermore, this approach can be extended to robots with multiple degrees of freedom, including non-anthropomorphic robots.However, enabling this approach involves mapping human actions to robot-executable actions, which presents challenges such as occlusion, rapid movement, and sensor noise during demonstrations.</p>
<p>By integrating foundation models [25] with robots, based on their embodied intelligence, zero-sample training is achieved for motion planning and execution in real-world scenarios, resulting in human-like motion control logic and capabilities.This type of robot obtains general knowledge from foundation models and addresses cognitive deficiencies through perceptual intelligence, such as vision.This marks the initial steps towards the industrialization of embodied intelligent robots.</p>
<p>Preliminary Preparations</p>
<p>Developing a learning model for robots is not an easy task [35,36] , given the challenges of technical issues [37,38] including computing resources, algorithms and data.A feasible approach is to perform incremental development or experimental verification on the basis of existing models.In this section, we briefly organize publicly available resources [39][40][41][42][43][44][45] for robot learning, including simulators, datasets [46][47][48][49] , foundation models [36] .</p>
<p>As developing and testing applications with real robots is expensive and time-consuming, simulation has emerged as a crucial component in the field of robotics application development.The validation of applications in simulation prior to deployment on robots can shorten iteration time by identifying potential issues at an early stage.Simulation facilitates the testing of corner cases or scenarios that could pose risks in the real world.To validate and assess the performance and effectiveness of robot learning, it is beneficial to conduct initial testing in a simulator before transitioning to the real-world environment.Several simulation frameworks [50] currently exist for training in robot learning simulations and are summarized in Appendix Table 1.</p>
<p>To validate the correlation performance in the simulation experiment, it is essential to have a dataset.There are two types of data sets: static data and dynamic data.Static data refers to related data sets collected on the Internet, and dynamic data refers to data generated through interactions with robots in a real-world environment.Currently, the majority of existing datasets consist of static data, and the specifics of prominent datasets are summarized in Appendix Table 2. Obtaining dynamic data is challenging, and it is relatively scarce.</p>
<p>Foundation models [51] are inseparable from the support of computing power, algorithms and data.The chip determines the computing power.Higher-performance chips are necessary for the training and construction of the neural network in foundation models.Research institutes currently employ diverse algorithms to implement foundation models.However, the primary difficulty lies in acquiring highquality data.High-quality data plays a crucial role in facilitating AI training and tuning.The following is a summary of notable foundation models in Appendix Table 3.</p>
<p>Due to the substantial cost of model pre-training, utilizing public API [52,53] allows for remote execution of inference tasks.</p>
<p>Downstream Tasks</p>
<p>The target and output of robot learning is to help the robot tactfully fulfill a certain task via acquiring a behavior or skill.Common target tasks include robot manipulation, navigation, mission planning, and reasoning (Fig. 3).This section investigates and reviews how different works combine foundation models for different downstream tasks to achieve robot learning goals [54,55] .Fig. 3. Downstream Tasks For Robot Learning.</p>
<p>Manipulation</p>
<p>Characteristic and Challenges</p>
<p>Tasks, despite being commonly perceived as simple and routine, such as washing dishes, cutting vegetables, and packing clothes, are still challenging for robots and are at the forefront of robotics research [56] .The robot manipulation problem deals with how a robot learns to manipulate its surrounding environment [5] .It can be formulated as the ability to connect a starting state to a goal state through successive actions, during which the manipulation task is represented by a set of points denoting the starting and goal states, along with the constraints imposed on the transition states [57] .Furthermore, the ability to make physical contact is critical to the development of manipulation skills.Currently, robots can only effectively grab and release certain types of objects and perform a variety of simple manipulation actions such as throwing, sliding, pushing, and poking.Challenges emerge when these actions need to be performed in uncluttered environments or when more complex interactions are necessary [14] .Foundation models have the capability to engage in interactive dialogues with users, receive and process various types of data, including images, text, and speech.They can utilize multimodal information, such as vision, to guide the actions of the robot or generate corresponding code or action sequences.These capabilities enable robots to effectively adapt to intricate and ever-changing environments, eliminating the need for defining every single possible scenario in advance [55] .In this section, we survey the recent research results and latest progress of foundation models in the direction of robot manipulation, including learning of robotic skill primitives, learning of complex robot manipulation tasks, and robot manipulation learning with multimodal fusion.</p>
<p>Methods l Learning of Robotic Skill Primitives</p>
<p>The approach involves breaking down the problem of robot skill learning into several easily comprehensible skill primitives or movement primitives.Subsequently, suitable and efficient learning methods are devised for these skill primitives, allowing the robot to acquire manipulation skills by combining these primitives.This enables the robot to adapt to new environments and generalize its abilities to perform novel tasks.Researchers are currently exploring the utilization of LLM guidance to facilitate the learning of action primitives for skill completion [58,59] .</p>
<p>The ActivityPrograms knowledge base [60] collected by Puig et al. has 292 different high-level tasks in the knowledge base.This work lays the foundation for further exploration by future generations.Huang et al. [61] randomly sampled 88 holdout tasks from the aforementioned knowledge base for evaluation, and the remaining 204 tasks were used as a demonstration set, from which they were used as examples for prompting language models.For supervised fine-tuning baselines, these tasks were utilized in fine-tuning the pretrained language model.Brohan et al. [62] collected a large and diverse dataset of robot trajectories, including multiple tasks, objects, and environments.The main dataset contains 130,000 robot demonstrations, which were performed by 13 robots over a 17-month period.The robot conducted large-scale data collection in a series of office and kitchen.Fu et al. [63] proposed a novel physics-based actioncentric environment, RFUniverse, for robot learning of daily housework tasks.RFUniverse supports 87 atomic operations and interactions between 8 primitive object types in a visually and physically plausible way.To demonstrate the usability of the simulated environment, the learned algorithm is performed on various types of tasks, namely fruit picking for manipulation, folding cloth and sponge wiping, stair chasing for locomotion, room cleaning for multi-agent collaboration, milk pouring, and hand lift for cloning behavior from VR interface.Vaucher et al. [22] extracted 28 kinds of atomic operations for synthetic operations of chemical experiments using three models, which are rule-based action extraction from Pistachio, rulebased action extraction from NLP, and a combination of the above two methods.Wu et al. [64] propose to use pick-andplace and pick-and-throw as action primitives, arguing that they are well suited for household cleaning, and thus propose a personalized robot assistance model with a large language model [58] .Gu et al. introduced the benchmark maniskill2, comprising a series of 20 operational tasks specifically designed to tackle the major challenges faced by researchers when utilizing general operational skill benchmarks [65] .Gao et al. proposed a two-stage fine-tuning strategy to further enhance the model's generalization ability based on the maniskill2 benchmark [59] .Furthermore, there are additional pertinent works that can be referenced [66][67][68][69] .l Learning of Complex Manipulation Tasks for Robots With the expanding scope of application scenarios and increasing performance requirements for robots, the demand for completing increasingly complex and precise manipulation tasks has risen.Therefore, the research goal is to empower robots with the capability to autonomously learn from their environment and independently accomplish complex manipulation tasks.This includes acquiring knowledge, accumulating experience, continuously updating and expanding manipulation skills in order to successfully execute complex tasks.</p>
<p>Wang et al. proposed RoboGen, a method for achieving robot automation learning by generating simulated infinite data [70] .Ma et al. proposed the Eureka method, which utilizes LLMs for human-like reward design.They demonstrated, for the first time, a simulated shadow hand capable of performing pen spinning techniques [30] .Shridhar et al. propose CLIPORT, a language-conditioned imitation learning agent that combines the broad semantic understanding of CLIP with the spatial precision of Transporter, an end-to-end framework capable of solving various language-specified desktop tasks [71] .Khandelwal et al. build a simple baseline EmbCLIP without task-specific architectures, inductive biases (e.g. using semantic maps), auxiliary tasks during training, or depth maps, but the improved baseline performs remarkably well across a range of tasks and simulator [72] .Wang et al. access pre-trained visual language models in robot manipulation, using a combinatorial classification grammar that parses sentences into operational procedures in a domain-specific language, which allows visual language models to correspond category or attribute descriptions to pixels, thus opening up the learning of visual and action strategies [73] .Xiao et al. introduced Data-Driven Instructional Augmentation with Linguistic Condition Control (DIAL) and generalized it to 60 new instructions not seen in the original dataset.This effectively incorporates internet-scale knowledge into existing datasets with limited real annotations [69] .Huang et al. used LLMs to infer affordances and constraints through language, used its code writing ability to interact with VLM to generate 3D value maps, integrated knowledge into observation space, and used value maps to model zero-shot synthesis of closed-loop robot trajectories.Robust to perturbations, the model is used to perform a variety of manipulation tasks [74] .Mo et al. proposed SeeAsk, an openworld interactive vision-based system that can master specific goals through vague natural language instructions [75] .Ze et al. proposed GNFactor, a visual behavioral cloning agent for multi-task robot manipulation [76] .Zheng et al. designed the canonicalized manipulation space and proposed a two-stage framework for synthesizing humanlike manipulation animations covering rigid and articulated object categories [77] .Extensive research has also been conducted on dexterous manipulation [78][79][80][81][82][83][84][85][86][87][88][89][90][91][92][93][94][95][96] .l Robot Manipulation Learning with Multimodal Fusion</p>
<p>While multimodality [97] addresses the challenge of agent perception, it falls short in addressing the issue of cognition.In various scenarios like an intelligent customer service, users often provide information through multiple modalities.Multimodality holds significant perceived value, but it still has substantial challenges to overcome in solving fundamental issues.Multimodality represents a prominent future trend wherein foundation models will progressively align with multimodal approaches, enabling future agents to operate within a multimodal environment [98] .</p>
<p>Yang et al. proposed MOMA-Force, an imitation method for visual binding force, using perceptual representation learning, imitation learning, and admittance whole-body control to enhance the robustness and controllability of the system.MOMA-Force enables mobile manipulators to learn multiple complex contact-rich tasks with high success rate and small contact force [99] .Jiang et al. demonstrate that a wide range of robotic manipulation tasks can be expressed with multimodal prompts, and many robotic manipulation tasks can be represented as multimodal prompts interleaved with language and images or video frames.The authors design a transformer-based robotic agent, VIMA, that processes these prompts and outputs motor actions autoregressively [100] .Sferrazza et al. achieve robot generalization manipulation through multimodal learning that fuses vision and touch.AudioPaLM combines text-based and speech-based language models [101] .Brohan et  al. propose to jointly fine-tune state-of-the-art visuallanguage models on robot trajectory data and Internet-scale visual-language tasks.The authors present RT-2: Vision-Language-Action Model Transferring Network Knowledge to Robot Control [102] .RobotGPT provides the perception, cognition, decision, execution capabilities with embodied AI [103] .Luo et al. proposed a physics-based humanoid controller that enables high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy inputs (e.g., pose estimates generated from videos or speech) and unexpected falls [104] .Gandhi et al. conducted the first largescale study of the interaction between sound and robot movement.The authors created the largest available soundaction-vision dataset using the robotics platform Tilt-Bot, with 15,000 interactions on 60 objects [105] .Peng introduces Kosmos-2, a multimodal large language model (MLLM) that supports perceptual object descriptions (such as bounding boxes) and new capabilities for integrating text into the visual world [106] .Radosavovic et al. proposed an RPT model, which is a Transformer that operates on a sequence of sensorimotor tokens.Given a sequence of camera images, proprioceptive robot states, and past actions, it is assumed that if the robot can predict what is missing, then it has acquired a good model of the physical world that allows it to act [107] .Li et al. systematically studied how visual, auditory, and tactile perception can jointly help robots solve complex operating tasks [108] .LEE et al. use graph neural networks to generate master odor maps (POMs) that preserve perceptual relationships and enable odor quality predictions for previously uncharacterized odors [109] .Guzey et al. proposed visually stimulated tactile adaptation in order to learn tactile flexibility, thereby correcting errors and adapting to changing situations [110] .</p>
<p>Datasets and Metrics</p>
<p>Datasets.In order to improve the generalization ability of robot manipulation learning, researchers collected multitask, multi-scenario data sets so that the robot can be trained to quickly generalize various scenarios.At the same time, the test datasets evaluate the manipulation capabilities of these robots guided by the foundation model.There are several datasets available for testing, as presented in Appendix Table 4.</p>
<p>Metrics.In order to improve robot performance and manipulation level, the key is to continuously optimize the performance of the robot learning system.Then the core issue is to reasonably evaluate the performance of the robot.Issues encountered in the comprehensive evaluation process: improvement of single evaluation indicators, integration of multiple evaluation indicators, weight distribution of each indicator, and reliability of evaluation results.</p>
<p>Manipulation performance is evaluated by: (1) Plan success rate, which measures whether the skills selected by the model are correct for the instructions, regardless of whether they are actually successfully executed.(2) Execution success rate, which measures whether the entire system actually successfully executed the required instructions [62,102] .</p>
<p>Problems</p>
<p>Despite making considerable progress, significant problems still exist.Robot movements are continuous and complex, with more physical interactions and manipulation causality.</p>
<p>A comprehensive library of skills and actions needs to be built.In order to complete downstream target tasks, a skill library needs to be built.High-level instructions are broken down into atomic actions.The existing skill action library suffers from a long-tail distribution problem.A more comprehensive skill action library needs to be established to assist robot skill learning.Robots can manipulate objects of any size, shape, and material in any scene.For example, a robot can pour a specified volume of any liquid and complete specified tasks in a dynamically changing environment.Objects that are difficult to manipulate in the 3D world, such as articulated, deformable and transparent objects, can be manipulated.The robot can solve the problems of occlusion and obstacle avoidance when grasping.The robot can reason about spatial relationships and contact dynamics, accurately drive high-degree-of-freedom arms, and apply the appropriate force to stably grasp objects without breaking them.</p>
<p>Current robotic object manipulation applications rely heavily on human programming and have poor autonomy, which is unrealistic in the long run.Vision-based intuitive physics, open set grasping with semantic representation, and planning under partial observability and uncertainty will be the future trends of robot grasping [15] .</p>
<p>Navigation</p>
<p>Characteristic and Challenges</p>
<p>Several research projects focus on developing autonomous robots capable of navigating in different environments.However, little research has specifically addressed the navigation challenges faced by legged robots in indoor environments, especially when relying on singlecamera vision.The legged robot is able to traverse uneven surfaces and overcome obstacles, such as stairs, that traditional wheeled robots typically cannot reach.In order to realize the autonomous navigation of mobile robots in unknown dynamic environments, mobile robots need to solve three basic problems, namely "Where am I?", "What is my surrounding?",and "What should I do next?".In order to solve these three core basic problems, a series of core technologies such as environmental perception, map creation, autonomous positioning, and motion planning are required [111,112] .Foundation models have reasoning and decision-making capabilities, and multi-modal foundation models can handle rich information modalities, and encode information of different modalities into the same vector space, which is conducive to cross-modal information processing.Benefiting from these considerations, foundation models help to deal with problems such as motion planning and environment perception of robots.In this section, we survey recent research results and state-ofthe-art advances in the direction of robotic navigation with foundation models, including robotic environment perception, map creation, autonomous localization, and motion planning.</p>
<p>Methods l Environment Perception</p>
<p>Environmental perception refers to the process in which a robot, during its movement, perceives the surrounding environment through various devices.It can collect real-time environmental information and adjust its position and pose accordingly based on the actual situation.</p>
<p>Lynch et al. proposed a robot framework for real-time natural language interaction in a real environment.The relevant resources are open source (datasets, environments, benchmarks, and policies).Behavioral cloning training is performed on the dataset of annotated trajectories, and the generated Policies proficiently execute orders of magnitude more commands than previous work [113] .Methods that align the embedding space of different modalities (in this case, IMU data) with the visual embedding space enable the VLM to understand and reason about these additional modalities without the need for retraining.The results show that using multiple modalities as input can improve VLM's scene understanding and enhance its overall performance in various tasks [114] .The agent follows real-life navigation instructions, then recognizes the location described in natural language and finds the object at the target location [115] .Hong et al. proposed a predictor to generate a set of candidate waypoints during navigation, bridging the gap between learning in discrete and continuous environments for visual and language navigation [116] .Tan et al. use environment dropout for back-translation to learn unseen environments in navigation [117] .Qi et al. propose an object and action perception model that can flexibly match objectcentered or action-centered instructions with their corresponding visual perception or action direction in the process [118] .Various other studies can serve as references [119- 121] .l Map Creation Map construction can be accomplished either through the utilization of metric maps or by employing symbols that represent the robot's position in its reference frame.</p>
<p>Gervet et al. proposed a semantic visual navigation method, compared six typical approaches of classical, modular and end-to-end learning methods without any experience, maps or instruments, through large-scale empirical research, found that modular learning is a reliable way to navigate to objects [122] .To address the lack of spatial accuracy of classical geometric maps, Huang et al. proposed spatial map representation VLMaps, which fuses pretrained visual-linguistic features with 3D reconstructions of the physical world.VLMaps can be autonomously constructed from the robot's video sources using standard exploration methods and can support natural language indexing of maps without labeled data [123] .An et al. proposed a multi-modal map pre-training paradigm BEVbert for language-guided navigation, which has spatial awareness capabilities [124] .Jia et al. proposed a neural SLAM method that for the first time exploits multiple modes to explore, predict perceptible semantic maps, and plan them simultaneously [125] .Zhang et al. propose a hierarchical object-to-region (HOZ) environment map to guide the agent in a coarse-to-fine manner.Additionally, an online learning method is proposed to update the HOZ new environment based on real-time observations.l Autonomous Localization Robot localization refers to a robot's ability to accurately determine its position and orientation within a predefined reference frame.Inspired by the recent success of open-vocabulary models for image classification, Gadre et al. investigate a simple framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to language-driven zeroshot object navigation without fine-tuning (L-ZSON) [126] .Keetha et al. have developed a universal VPR solution -a technology that works across a variety of structured and unstructured environments without the need for any retraining or fine-tuning [127] .Based on the WAY dialogue dataset, Hahn et al. focus on the LED task (locating the observer from the dialogue history) [128] .Two agents (tourist and tour guide) interact through natural language to allow tourists to navigate to the correct location [129] .l Motion Planning Motion planning is the process by which a robot intelligently selects an optimal sequence of actions to move towards a target location based on its current position within a reference frame, considering changes in the environment.Hong et al. proposed a time-aware recurrent bert model to maintain cross-modal state information in order to solve the problem that the bert architecture in visual language navigation is difficult to use for partially observable Markov decision-making processes, which need to rely on historical attention and decision-making [130] .Manglani studies the development of visual segmentation and path planning algorithms for autonomous navigation and obstacle avoidance systems in domestic environments [111] .Lin et al. propose to provide action prompts for VLN agents to enable explicit learning of action-level modality alignment for navigation.Specifically, action prompts are defined as a pair of modality-aligned image sub-prompts and text subprompts, and when navigation starts, a set of action prompts related to an instruction is retrieved from a pre-built library of action prompts and passed through a prompt encoder to get hint features.Then, the prompt features are concatenated with raw instruction features and fed to a multi-layer transformer for action prediction.Using a contrastive language-image pre-training (CLIP) model, a modality alignment loss and a sequential consistency loss are introduced to enhance the alignment of action prompts and force the agent to sequentially attend to related prompts [131] .Traditional path planning algorithms focus only on collisionfree paths, limiting their applicability in contact-rich tasks.To address this limitation, Xie et al. proposed Linguistically Conditional Collision Function (LACO), a novel method to learn collision functions using only single-view images, language cues, and robot configurations [132] .Zhou et al. introduce NavGPT, an LLM-based instruction tracking navigation agent, to demonstrate the reasoning ability of large language models in complex scenes by performing zero-shot sequential action prediction for visual and language navigation [133] .Qian et al. propose the March-In-Chat model, which allows communication with LLMs in real time and perform dynamic planning based on room-andobject aware scene perceiver.The model outperforms previous models on the REVERIE benchmark [134] .Fried et al. experimentally demonstrated that the three parts of the proposed method (speaker-driven data augmentation, pragmatic reasoning, and panoramic action space) greatly improved the performance of the baseline instruction follower [135] .Wang et al. proposed enhanced cross-modal matching and self-supervised imitation learning methods to solve cross-modal grounding, ill-posed feedback and generalization problems [136] .Nguyen et al. [137] develop a memory-enhanced neural agent.Imitation learning algorithms teach agents to avoid repeating past mistakes.The agent also completes object finding tasks by requesting and interpreting natural language and visual assistance.</p>
<p>Datasets and Metrics</p>
<p>Datasets.In order to promote the development of robot navigation tasks, researchers have collected a large amount of data from multiple aspects such as instructions, scenes, objects, etc.This plays a very important role in the development of navigation and also lays the foundation for the transfer from virtual to real environments.There are many datasets that can be used for navigation.See Appendix Table 5 below.</p>
<p>Metrics.Navigation performance is evaluated by: (1) Success / Oracle Success Rate (%).(2) Navigation Error (m).</p>
<p>(3) SPL (Success weighted by Path Length).( 4) CLS (Coverage weighted by Length Score), measuring fidelity to the reference path.( 5) nDTW (normalized Dynamic Time).( 6) SDTW (Success weighted by normalized Dynamic Time Warping) [138][139][140] .</p>
<p>Problems</p>
<p>To solve the problem of high obstacles, it is worth considering investigating partially observable search methods.Potential solutions would involve broadening the scope of the study to cover a wider range of domestic environments (including stairs and uneven surfaces), outdoor environments and complex unknown environments.In addition, a more general large-scale navigation model can be designed that can be widely applied to any robot, similar to LLMs and VLMs for processing any text or image.</p>
<p>In addition, the efficiency of multi-target navigation needs to be improved, scene knowledge needs to be better applied in navigation, and the efficiency of unknown scene navigation needs to be improved.Techniques for optimizing system efficiency in real-time scenarios will also be explored, taking into account factors such as robot stability, dynamic obstacle avoidance, and resource constraints.</p>
<p>Path planning in unknown scenarios faces the following problems: lack of global map, generalization of unknown environments, and sensor noise in real-life applications.The lack of global map includes the following aspects: lack of global SLAM map, how to learn prior knowledge of layout, and in what form to store knowledge.The generalization of unknown environments includes the following aspects: limited training rooms and infinite unknown environments, transfer and generalization of knowledge.Sensor noise in real-life applications includes the following aspects: depth loss, RGB blur, and detection and segmentation errors.</p>
<p>Task Planning</p>
<p>Characteristic and Challenges</p>
<p>Depending on the difficulty of the task, the task can be completed by a single robot or by multiple robots.Many of these problems require multiple robots to work together in multi-robot systems.Long-term tasks for a single robot need to be broken down into smaller tasks that can be accomplished with simple operations.Task planning for multiple robots working together needs to consider the relationship between tasks, robot capabilities and cooperation, and other challenges.In addition, the agent cannot hallucinate when planning tasks.Within the bounds of common sense, agents can learn from previous tasks, selfcorrect and improve, and continue to learn.This is a challenging task.The true realization of intelligent robots will be based on the perception of the environment, dynamic learning, and continuous updating.In this section, we survey the recent research results and the latest progress of foundation models in the direction of robot task planning, including single-robot task decomposition, multi-robot task planning, and self-correction, self-improvement, and continuous learning of agents.</p>
<p>Methods</p>
<p>l Single-Robot Task Decomposition Task planning.For complex tasks, the best solution is to split it into several simple tasks, and then solve them one by one.Generally, the effect will be good.LLMs have shown good zero-shot generalization ability.State-of-the-art chatbots can provide plausible answers to many common questions that arise in everyday life.However, so far, LLMs cannot reliably solve long-term planning problems [141] .</p>
<p>Liu et al. introduce LLM+P, the first framework to incorporate the strengths of the classical planner into LLM [142] .Wu et al. aligned the LLM with the visual perception model, and generated executable action sequences based on the objects existing in the real scene [143] .Lin et al. proposed a language-based planning framework, Text2Motion, based on which robots can solve sequential manipulation tasks for long-horizon reasoning.This framework uses the feasibility heuristic encoded in the skill library Q function to guide the task planning of LLMs, and Geometric dependencies among skills are resolved by performing geometric feasibility planning during the search process [144] .Wake et al. use ChatGPT to convert natural language instructions into executable robotic actions in longstep scenarios [145] .EmbodiedGPT introduces an efficient training method to generate high-quality plans.The paradigm of extracting task-relevant features from LLMgenerated planning queries is introduced to form a closed loop between high-level planning and low-level control [146] .Ruan et al. proposed a structured framework customized for LLM-based artificial intelligence agents, within which two different types of agents were designed: one-step agents and sequential agents, to perform the reasoning process.The framework is then instantiated using LLMs and evaluated on their task planning and tool usage abilities on typical tasks [147] .Zhen et al. propose a task planning method that combines human expertise with LLMs.And the LLM prompt template is designed, which has stronger expressive ability to represent structured professional knowledge.Further, a method of gradually decomposing tasks to generate task trees is proposed.And a strategy for decoupling robot task planning is designed [148] .Gu et al. studied a modular approach to handle the long-view movement operation task for object rearrangement, decomposing the complete task into a series of subtasks [149] .Obinata et al. describe a strategy for implementing a robotic system capable of performing general service robot (GPSR) tasks in robocup@home [150] .Shi et al. developed RoboCook, an intelligent robotic system that can use multiple tools for long-horizon elastoplastic object manipulation [151] .There are some other works that can be referenced [61] .l Multi-Robot Task Planning For multiple robots, relationships between tasks needs to be considered during task planning, robot capabilities and cooperation, and other challenges.Task planning for multirobot teams usually involves three sub-problems: task decomposition, task allocation and task scheduling, which are introduced here to consider situations where robots sequence their own tasks to satisfy various constraints.While each of these problems can be solved individually, the interdependencies between these subproblems are often considered in the context of integrated tasks to provide solutions that are often better than solving each subproblem independently [18,152,153] .</p>
<p>LLMs have shown impressive planning capabilities in single-agent specific tasks across domains, yet their planning and communication capabilities in multi-agent cooperation remain unclear.A novel framework for multi-agent cooperation using LLMs is proposed, which enables agents to plan, communicate and cooperate with other agents or humans to effectively complete long-horizon tasks [154] .RoCo is a unified approach for multi-robot collaboration that leverages pre-trained LLMs for high-level communication and low-level path planning.The authors introduce the RoCoBench benchmark, which includes various challenges in collaborative task scenarios.The method demonstrates its utility on RoCoBench [155] .Qian et al. carefully divided the development process into four different chronological stages: design, coding, testing and recording.Each stage involves a team of agents, such as programmers, code reviewers, and test engineers, facilitating collaborative conversations and facilitating a seamless workflow.The chat chain acts as a facilitator, breaking down each stage into atomic subtasks [156] .Using multi-agent competition, the simple goal of hide-and-seek, and large-scale standard reinforcement learning algorithms, Bowen Baker et al. found that the agent created a self-supervised automated course, triggering multiple rounds of different emergent strategies, many of which required complex tool use and coordination [157] .Zhang et al. proposed a framework for multi-agent cooperation using LLMs.Experiments have proven that LLM-based agents that communicate verbally can win more trust and cooperate more effectively with humans [154] .l Self-Correction and Self-Improvement</p>
<p>The ability to automatically detect and analyze failed executions is critical for explainable and robust robotic systems.This requires the agent to self-criticize and continuously learn from previous tasks.Correct behavior within the bounds of common sense.Based on the large language model, directional error correction and directional improvement can be done.In task planning, it is necessary not only to have the current state, but also to have memory, experience, reflection and summary, and world knowledge.Agents can get feedback based on actions.For large language models, reinforcement learning from human feedback is an extremely simple environment.</p>
<p>Pan et al. analyzed and categorized a series of work of LLM's self-correction, including training time, generation time and post-correction [158] .Sharma et al. show how language can be used to update the latent cost of the planner to improve task performance.This method can use the language to correct the plan in two ways: adding constraints or specifying intermediate subgoals for the planner [159] .Liu et al. introduced a framework, REFLECT, to leverage the power of LLMs to explain robot failures.According to the explanation, the mission planner will generate an executable plan for the robot to correct the fault and complete the mission.Evaluating the framework on the RoBoFail dataset of failure scenarios, experiments demonstrate that the LLMbased framework is capable of generating informative failure explanations to aid in successful corrective planning [160] .Shinn et al. proposed a novel framework, reflexion, to learn from trial and error through dynamic memory and self-reflection to make better decisions in subsequent experiments [161] .Bousmalis et al. proposed RoboCat, a foundational agent for robotic manipulation, as a visual goal-conditional decision transformer.The authors demonstrate the ability to generalize to new tasks and robots.It also demonstrates the use of the trained model to generate data for subsequent training iterations, providing the basic building blocks for autonomous improvement loops [162] .Ning et al. explore whether LLMs has the ability to identify its own mistakes without resorting to external resources.In particular, the research focuses on whether they can be used to identify individual errors in step-by-step reasoning.The authors propose a zero-shot verification scheme to identify such errors.This verification scheme is then used to perform weighted voting on different generated answers, improving question answering performance [163] .Olausson et al. analyzed the ability of GPT3.5 and GPT4 to perform selfrepair on APPS.The approach is to self-repair with separate code and feedback models.APPS is a dataset consisting of various coding challenges.By evaluating the strategy, it was found that the effectiveness of self-repair was only seen in GPT4.It was also observed that self-repair is bottlenecked by the feedback phase [164] .Continual learning serves as a means to consistently foster self-improvement and selfoptimization [165] .Guo et al. proposed DoReMi, a novel language model base framework that can instantly detect and recover from inconsistencies between planning and execution [166] .In order to enable LLMs to integrate into the environment autonomously, Peng et al. proposed the selfdriven grounding framework to automatically and gradually lay the foundation for LLMs through self-driven skill learning [167] .</p>
<p>There are some other works that can be referenced [168- 175] , such as WebShop [176] , InterCode [177] , Collie [178] .</p>
<p>Datasets and Metrics</p>
<p>Datasets.Researchers in the field of robot task planning rely on datasets to develop, validate, and test their planning algorithms and systems.These datasets may include real-world data collected from robots or simulated data generated within robot environments [179] .Here is an example list in Appendix Table 6.</p>
<p>Metrics.Task planning performance is evaluated by: (1) Task Success Rate.(2) Task Durations.(3) Task Diversity.(4) Task Difficulty.(5) Task Dependencies.</p>
<p>Problems</p>
<p>More effective strategies for self-improvement and enhancement need to be explored [180] .LLMs automatically detects when and how to apply the planner; LLMs reduces reliance on human information during planning [142] .In addition, the planning time efficiency of the model needs to be improved.The correctness and executability of generated plans need to be significantly improved.Multimodal models are explored for mission planning, which can naturally support extending the planning system to higherdimensional observation spaces.</p>
<p>A significant advantage of utilizing the latest LLMs is their ability to adapt to various operating environments through several learnings and user feedback.These capabilities not only eliminate the need for extensive data collection or model retraining, but also allow users to make adjustments that promote safe and robust mission planning.The ability to effectively adapt to user feedback may be due in part to learning methods that combine model behavior with human intent.Additionally, the output of large models can be adjusted with a reasonable amount of feedback.The ability of LLMs to reflect the semantic content of user feedback provides means for users to communicate their intentions to the system.Therefore, this aspect helps lay the foundation for a user-friendly system.Delving into this ability to adjust contributes to the user-friendliness of the system [145] .</p>
<p>Reasoning</p>
<p>Characteristic and Challenges</p>
<p>Reasoning requires the reasoner (robot) to have an explicit representation of various parts or aspects of its environment in order to reason.Robots are increasingly transitioning from specialized single-task machines to general-purpose systems operating in diverse and dynamic environments.To solve the challenges associated with realworld interactions, robots must effectively generalize knowledge, learn, and remain transparent in their decisionmaking processes.However, there are few studies that specifically address the reasoning challenges faced by robots in interactive environments.This survey aims to investigate reasoning robot system technologies that enable robots to encode and use knowledge, including concepts, facts, ideas, and beliefs about the world.Continuously sensing, understanding, and generalizing knowledge enables robots to identify meaningful patterns shared across problems and environments to perform a variety of real-world tasks more efficiently [19] .LLMs provide promising tools for robots to perform complex reasoning tasks [181] .In this section, we survey the recent research results and latest progress of foundation models in the direction of robot reasoning, including robot logical reasoning, common sense reasoning, affordance reasoning, and personalized reasoning [182][183][184][185] .</p>
<p>Methods l Logical Reasoning</p>
<p>Machine logical reasoning refers to the ability of a machine to draw new conclusions through the derivation of known facts and reasoning rules.In logical reasoning, the machine needs to take into account the logical relationship between facts, such as If A is true, then B is also true, One of A and B must be chosen, etc.In addition, machines need to be able to understand fuzzy information and take into account the possibilities of different situations and choose the most reasonable conclusion from them [186] .Chain of thoughts can significantly improve the complex reasoning capabilities of large language models [187] .Tree of thoughts can be used for tasks that require exploration, strategic foresight, or where initial decision-making play a key role [188] .</p>
<p>Driess et al. implemented a PaLM-E multimodal model that plugs real-world continuous sensor modalities into a LLM to establish a link between words and perception.The inputs to the specific LLM are multimodal sentences that interweave visual, continuous state estimation, and textual input encodings.These encodings are trained end-to-end in conjunction with a pre-trained LLM to perform a variety of specific tasks, including sequential robot operation planning, visual questioning and answering, and captioning [189] .Brohan et al. argue that one of the keys to the success of general robotics models lies in combining high-performance architectures that learn a large variety of robotics data for training on open tasks [62] .Liang et al. proposed a method to generate policies based on LLMs.By inputting natural language instructions, the LLM trained by code completion can write robot policy codes.These policies are reactive policies and waypoint-based policies.Features include exhibiting spatial-geometric reasoning, generalizing new instructions, and assigning exact values to ambiguous descriptions based on context [168] .Yao et al. explore that with the support of chain of thoughts prompting and external knowledge bases, LLMs generates reasoning trajectories and task-specific actions in an interactive manner, thereby exerting greater synergy between the two [190] .</p>
<p>There are still some valuable methods for reasoning that are worth studying, including accumulative reasoning [191] , context learning [192] and causal reinforcement learning [193] .l Commonsense Reasoning LLMs combined with knowledge graphs or langchain is a way to try common sense reasoning.LLMs are blackbox models that typically fail to acquire and capture factual knowledge.In contrast, knowledge graphs etc. are structured knowledge models that explicitly store rich factual knowledge.Knowledge graphs can enhance LLMs by providing external knowledge for reasoning and interpretability.At the same time, knowledge graphs are inherently difficult to construct and evolve, posing challenges to existing methods for generating new knowledge and representing unseen knowledge in knowledge graphs.Therefore, it is complementary to combine LLMs and knowledge graphs to play their respective advantages at the same time [194] .LLMs and knowledge graphs (KGs) can complement each other, such that LLMs can be used for KG construction or completion, while existing KGs can be used for different tasks, such as making LLMs outputs interpretable or fact-checking in a neural-symbolic manner.Text2KGBench is a benchmark that evaluates the ability of language models to generate knowledge graphs from ontology-guided natural language text [195] .Kim et al. proposed KG-GPT, a general framework that utilizes LLM for knowledge graph reasoning [196] .Zhu et al proposed AutoKG, a method based on multi-agents, using LLM for knowledge graph construction and reasoning [197] .Yang et al. proposed to enhance the large language model KGLLM by developing knowledge graphs, and provided a solution to enhance the factual reasoning ability of LLMs [198] .Ren et al. propose KNOWNO, a framework for measuring and adjusting uncertainty in LLM planners so that they know when they don't know and seek help when needed.KNOWNO builds on conformal prediction theory to provide statistical guarantees for task completion.At the same time human assistance in complex multi-step planning setups is minimized [199] .Zellers et al. proposed QLeT: a model that learns physical commonsense knowledge through interaction and then uses this knowledge to construct language.Authors decompose QLeT into a physical dynamics model and a separate language model.Using a dynamics model as an interface to a language model, QLeT can read a sentence, neurally simulate what might happen next, and then communicate that result through a textsymbolic representation or natural language [200] .Zhu et al. proposed 3D-VisTA, a LLM with 3D world recognition capability, capable of answering questions based on a 3D world model.The project team also released the ScanScribe dataset, a 3D model-text dataset [201] .One approach to address inference problems is by incorporating vector databases.Both LlamaIndex and Langchain are working diligently to develop a data-augmented retrieval system, which could be further enhanced with a contextual agent.Yohei introduces the concept of incorporating relevant contextual information (task context), which may have nuances differing from those of the conventional semantic similarity algorithm offered by vector databases.Tang et al. proposed GraphGPT, which utilizes LLMs for graph instruction tuning [202] .l Affordance Reasoning</p>
<p>The ability to reason about affordances enables robots to choose actions that are appropriate for a given object and produce a desired effect.The Dreamer algorithm has recently shown great promise for learning from a small number of interactions by planning in a learned world model, outperforming pure reinforcement learning in video games.Learning world models to predict the outcomes of potential actions can be planned in imagination, reducing the amount of trial and error required in real settings [203] .Robots need primary knowledge of the world in which to act.LLMs can be used to score potential subsequent actions during task planning, or even directly generate action sequences without natural language instructions from additional domain information.Singh et al. propose a procedural LLM hint structure that enables plan generation to work across environments, robot functions, and tasks.LLMs are prompted by program-like specifications of operations and objects available in the environment, as well as example programs that can be executed.Situational awareness is introduced in LLM-based robot task planning [204] .Ahn et al. proposed SayCan, the value function of pre-trained skills to obtain the results of interaction with the environment, based on the real world.SayCan incorporates real-world experience into LLMs through the value function of pretrained skills, enabling LLMs to execute real-world abstract, long-term commands on robots.This method implements language models to provide high-level semantic knowledge and provide pre-trained low-level skills to constrain the model to propose natural language actions that are both feasible and appropriate to the context [205] .Yoneda et al. published the Statler framework, which enables LLMs to have representations of world states that change over time while maintaining memory.The framework has two generic LLM instances: a world model reader and a world model writer.These two parts interact with and maintain the world state.With access to this world-state memory, the Statler framework improves the ability of existing LLMs to reason over longer periods of time, independent of context length constraints.Experiments on simulated domains and real robots show that the proposed method improves the state-ofthe-art in LLM-based robot inference [206] .Gao et al. finetuned VLM on PhysObjects to improve its understanding of physical object concepts by capturing human priors on these concepts from visual appearance.Combining this physicsbased VLM with a large language model-based robot planner into an interactive framework improves mission success rates [207] .Tang et al. proposed a knowledge condition detection framework CoTDet for affordance knowledge prompts for task-driven object detection [208] .Strategic robotic pursuit-avoidance requires exploiting the dynamics of interactions and planning through uncertainty in physical states and underlying intentions [209] .l Personalized Reasoning</p>
<p>The emergence of large language models marks a revolutionary breakthrough in artificial intelligence.A major leap forward in the capabilities of general artificial intelligence will change how personalization is implemented.On one hand, it will change the way humans interact with personalized systems.On the other hand, it will also greatly expand the scope of personalization [210] .The growing use of LLMs in conversational agents has sparked interest in the personality exhibited by data-trained models, as personality significantly influences communication effectiveness.Therefore, Safdari et al. proposed a comprehensive approach to test the personality traits expressed in the text generated by LLMs.Experiments find the reliability and validity of LLMs for simulating personality for larger and fine-tuned models.The personalities in the LLM output can be shaped in desired dimensions to mimic specific personality traits [211] .Huang et al. proposed to assess the empathic ability of LLMs, that is, how their feelings change when they encounter a specific situation.After experimental evaluation, LLMs can usually respond appropriately to some situations, although there are some biases.Still, they don't match human emotional behavior to make connections between similar situations.This paper expects to contribute to the advancement of LLMs to better adapt to human emotional behavior, thereby enhancing the practicality and applicability of intelligent assistants [212] .Wu et al. used TidyBot, a robot to learn personal preferences to personalize the cleaning of a room, where the robot used the planning and perception of language combined with the summarization capabilities of a LLM to pick up objects, determine where to place them, and organize the room [64] .Ding et al.Learn universal human priors for dexterous manipulation from human preferences [213] .Deng et al. present Socratis, a social response benchmark that tests the ability of state-of-the-art multimodal large language models to generate emotional reasons for a given IC pair [214] .Lian et al. proposed the first multimodal LLM in affective computing, called AffectGPT.The goal is to address the long-standing challenge of label ambiguity and chart a path toward more reliable technology [215] .</p>
<p>Datasets and Metrics</p>
<p>Datasets.Robot reasoning datasets typically involve tasks that require problem-solving abilities such as logical reasoning and common-sense reasoning.These datasets are designed to evaluate the robot's ability to make inferences in various scenarios.Here are some examples of robot reasoning datasets in Appendix Table 7.</p>
<p>Metrics.The evaluation indicators for robot reasoning datasets typically depend on the specific dataset and task.However, common evaluation metrics and indicators for many reasoning datasets.</p>
<p>Reasoning performance is evaluated by: (1) Accuracy.6) BLEU (Bilingual Evaluation Understudy): Commonly used in machine translation tasks, BLEU measures the similarity between predicted and reference answers.</p>
<p>Problems</p>
<p>Robot learning goals are to master how to learn, combine advanced pattern recognition with model-based reasoning, and develop common sense intelligence.With the advancement of learning and the improvement of intelligence [216] , research on robot reasoning has gradually deepened.Reasoning is an abstract, advanced form of thinking.The objective basis of reasoning is the relationship between objective things.At present, the reasoning capabilities of robots are weak (including causal reasoning [217] , spatiotemporal reasoning, real-time reasoning, geometric reasoning, world model, world knowledge, common sense reasoning, understanding of physical constraints, etc.).One very important point is that the robot lacks reverse reasoning, and the basic model may reason from left to right.The current model is based on text-visual speech, so it does not directly reason about information such as touch, and more powerful multi-modal models are needed when available.</p>
<p>Solutions that can be tried are as follows: Beyond statistical correlation, reasoning about system dynamics and causality; meta-learning with limited data; rapid learning to adapt to dynamic, uncertain environments; learning across heterogeneous tasks and domains ; Developing systems that know their limitations and know how to ask for help; Developing systems that can deeply understand and synthesize complex textual and narrative information; Conducting deep moral and social reasoning about realworld problems.</p>
<p>Discussion</p>
<p>Despite the significant progress made in robot learning based on LLMs techniques, the field still faces numerous challenges in terms of both technical aspects and ethic aspects.In the following section, we will outline the major challenges, potential solutions as well as potential future directions.We hope that the highlight aspects can serve as inspiration for future research investigations in the robot learning area.</p>
<p>Robot Hardware and Software Decoupling</p>
<p>The basic requirement for a foundation model is a unified architectural framework.However, there is significant variation in robot hardware, making it crucial to achieve bottom-level uniformity or decoupling.To facilitate the advancement of robot technology, it is crucial to ensure the synchronization of form with function [218] , which implies that software and hardware must evolve simultaneously.Nevertheless, numerous challenges persist in the development of software and hardware.On one hand, due to the diversity of manufacturers' specifications and private parameters, it becomes challenging to individually program and manage robots when (re)configuring them to achieve desired tasks or formations.This dilemma often occurs in industrial and operational fields.On the other hand, the construction of current robot models and databases largely depends on the hardware structure of the robots.The majority of robot databases are constructed by collecting specific data pertaining to individual robots.Consequently, trained robot models exhibit optimal performance solely on the particular robot they are trained for.Put simply, existing robot models are limited by the hardware structure of the robots and software algorithms.</p>
<p>Ideally, there is a need to decouple robot software and hardware, and concurrently, collect databases encompassing various robot models.The separation of the logical and physical components would foster increased software innovations and facilitate a potentially more accessible robot market, inevitably resulting in reduced non-recurring engineering costs.In a decoupled architecture, robot hardware and software can be developed and updated independently, without restrictions.Additionally, the most recent high-performance software algorithms can be efficiently implemented across various robot models.Confronted with diverse hardware configurations and unpredictable operating environments, users can develop and program robots of different types without the need for a comprehensive understanding of the specific hardware employed by each robot.Compatible APIs, predefined libraries, and programming software can be developed and enhanced.The abstraction level of robot software needs to be improved to increase the efficiency and effectiveness of robot operations, enabling robot applications to run robustly in dynamic environments [219] .</p>
<p>Dynamic Data for Interaction with the Environment</p>
<p>When establishing a specialized large-scale robot model, it is necessary to utilize dynamic and diverse data [220] , including dynamics data, during the training or fine-tuning process.Meanwhile, robots are anticipated to embody key attributes including agility, cost-effectiveness, diversity, environmental adaptability, and plasticity, which empower them to execute tasks like fastening garments and tying shoelaces.Interactive environments [221] and abundant dynamic data are imperative for sufficiently training and assessing robots possessing these aforementioned attributes.However, the realization of authentic robot scenarios and the acquisition of real-time data present formidable challenges.Therefore, collaboration and resource sharing among laboratories worldwide assume paramount importance in propelling responsible and open advancements in robotics research.Data collection methods may encompass several approaches, including action library aggregation, teleoperation, and imitation learning.The crux of the challenge lies in dissociating the collected data from the specific robot model employed, thus ensuring its perpetual validity.Additionally, the existing training process in robot learning suffers from an inadequacy of effective and abundant data.Establishing multimodal databases that can be utilized for robot learning training is of paramount importance.</p>
<p>In order to better understand the environment, recognize objects, and perform tasks, current robot models often only focus on the fusion of top-level data such as vision and language in multimodal fusion, while neglecting bottom-level data such as dynamics data related to interaction with the environment.In real-world environments, there is no doubt that mechanics directly determine the stability of grasping.To improve the accuracy, generalization, robustness, and address issues related to force perception in robot models, it is necessary to construct large-scale dynamics data during training.Dynamics data includes information such as position (XYZ, rotation angles), acceleration, and forces (magnitude and torque), etc.Based on this information, robots can easily manipulate objects of different materials and weights accurately.Understanding the robot's dynamics model allows the model to comprehend the robot from a bottom-up approach during the training process of existing robot learning.Incorporating dynamics data into the robot's foundation model also enables the model to generate more realistic and physically meaningful data.</p>
<p>Robot Generalization</p>
<p>Robot models need to demonstrate better generalization ability, surpassing the semantic, visual, and other multimodal understanding of the robot data they encounter.It requires robots to perform operational tasks on objects or scenes in the robot data that they have never seen before.This necessitates the utilization of knowledge derived from network data for operation.Robot models should not only maintain performance on the original tasks in robot data but also improve performance in previously unseen scenarios.Accordingly, we present the following key elements.</p>
<p>Currently, the advancements in robotic research have yet to reach optimal levels of efficacy.A clear divergence is evident between human intelligence and that of robots, especially when evaluating areas such as image recognition and the competency of platforms like ChatGPT in question-answering scenarios.The enhancement of both the precision and efficiency of robotic learning stands as a critical imperative.While modern robotic learning methodologies have attained certain milestones, the scope for refining execution success rates remains substantial.</p>
<p>The environment is expected to become increasingly complex.Robots need to adapt to various scenarios to perform a variety of tasks.It is difficult to achieve long-term autonomy in complex environments, so lifelong learning is required.Currently, robots are only capable of performing a limited number of skilled tasks and are not generalizable across a variety of tasks [222] .On the one hand, Robots will adapt to different scenarios including layout configuration, visual texture, light source changes, time simulation, low cost, data enhancement, demand personalization, and program automation.In virtual-real transfer, methods that may be used include domain adaptation, meta-learning, transfer learning, knowledge distillation, and world models.Legged robots have made a lot of progress in traditional methods, but they are lacking in combining with foundation models.On the other hand, Robots can observe, understand, practice in interaction with their environment, and thus selfexam, diagnose, and repair.The robot can not only collect data by itself, analyze the data, and then analyze the cause of the failure and then solve the problem.Robots can have selfdriven learning awareness and can continue to learn [165] .</p>
<p>Currently, foundation models are developed and trained in unmanned environments.In the future, when they are utilized in environments with human presence, interactions between robots and humans will need to take human factors into consideration.It requires human-robot alignment and considers emotional factors.It also considers biomimetic learning, cross-learning, and the co-development of the brain and the body.</p>
<p>AI alignment is worthy of in-depth research.The goals of AI systems are required to be consistent with human values and interests.If the values of AI and humans cannot be aligned, the following problems will arise.Behaviors that do not meet the goals of human intentions, out of control, harming human interests, and making wrong choices in the conflict of multiple setting goals [157,223] .</p>
<p>Researchers can combine functionality with expressive capabilities and explore methods to improve robots from this perspective.Functionality is manifested in mobility, dexterity, perception, and intelligence.Considering the relationship between individuals and occupations, we can liberate people from dangerous situations and enhance the technological level of humanity.</p>
<p>While current foundation models are capable of engaging in high-level semantic conversations, they fall short of humans in low-level control aspects such as movement and operation.Consequently, bionics is deemed significant.Biomimetic learning is a highly worthwhile field of research in order to design and develop more powerful robots.It draws inspiration from biological systems, especially animals and organisms.By observing how animals and organisms move, perceive their environment, and interact with it, researchers attempt to replicate these mechanisms in robots.They utilize biomimetic materials, such as artificial muscles and soft robot components, to enable robots to imitate the adaptability and flexibility of living organisms.The objective of researchers is not only to replicate the physical characteristics of animals but also to understand their behavioral patterns.This includes understanding how animals make decisions, solve problems, and exhibit intelligent behavior.Overall, the aim of biomimetic learning in robotics is to push the limits of robot capabilities by harnessing natural wisdom and integrating biological principles into the design and control of robots, making them more versatile and adaptable to various tasks and environments.</p>
<p>Robots can achieve dexterous movement and manipulation.It has been found through research that the perception-action circuit is the center of cognition, and the body uses the perception-motor system to generate intelligence in the interaction with the environment.Intelligent robots span multiple directions in intelligent disciplines, such as cognitive science, psychology, brain science, and sociology.Therefore, the focus is on interdisciplinary integration, analogous to the developmental process of human intelligence.In multimodal environmental interaction, by opening up the links and loops between morphology, perception, behavior and learning, robots can realize active perception and autonomous learning.</p>
<p>Being conscious is hard, but the body and brain of robot should develop together [218,224] .In robot learning, the robot lacks common sense and lacks the physical constraints of learning real scenes from the environment.Robot learning is to make robots intelligent.In addition to perceptual and motor capabilities, which have been studied in the past, cognitive abilities are also very significant.Referring to the development process of human intelligence, the brain and body develop simultaneously.Intelligent robots not only have perception and motor intelligence, but also cognitive intelligence with autonomous consciousness.In addition, robots lack the ability to learn actively.For example, in completing the specified target tasks, it can actively ask what it needs to do next in order to achieve a certain task.Active learning may regain favor as autonomous contextual agents actively reveal what they don't know in order to prioritize [225] .</p>
<p>Multimodal Interaction</p>
<p>Currently, the interaction data of robots mainly relies on visual and textual information, lacking diversity.In order to enhance the robot's perception of the environment and its interaction with it, it is necessary to integrate multimodal data from various sensory modalities, including visual, auditory, tactile, olfactory, gustatory, and other sensory inputs.Although multimodal data brings great hopes for advancing robot technology, it also comes with some problems and challenges.Specifically, different modalities have different data formats, structures, and features, posing challenges for the collaborative interaction among different modalities.The main issues involved are multimodal representation, multimodal mapping, multimodal alignment, and multimodal fusion.</p>
<p>To address these problems, several considerations can be taken into account.Research can be conducted to tackle challenges related to large-scale environments, multitasking, and strong interaction.Tasks with higher interactivity can be verified and demonstrated in more open and complex environments, such as simulators and multi-platform settings.Various fusion strategies can be explored to effectively combine information from different modalities, including hierarchical and multilingual fusion, as well as scalable fusion.Advanced deep learning models, such as multimodal transformers, can be developed to efficiently fuse and extract meaningful information from different sensory modalities.Cross-modal learning techniques can be explored to enable robots to learn meaningful correlations between different modalities, thereby improving their understanding of the environment.Transparent and interpretable multimodal fusion models can be developed to facilitate human understanding and trust in robot decisions.The capability of robots to understand the semantic of multimodal data, including scene understanding [15] and context comprehension, can also be enhanced.Additionally, during the process of converting multimodal content, foundation models may lose information or generate errors, leading to biased results.This requires foundation models to achieve strong multimodal understanding, addressing the current issues of unreliability in intelligent agents, as well as information loss and biases.</p>
<p>Exclusive Foundation Models for Robots</p>
<p>Foundation models have achieved many gratifying results in text, language, etc., but in the field of robotics, the development of foundation models is slow.There is an urgent need to develop usable and effective general-purpose foundation models and special-purpose foundation models of robots.In order for robots to reach human-like levels of capability, it is necessary to collect robot data for every object, environment, task, and situation.In addition to the issue of robot data, the following problems and challenges also need to be overcome.</p>
<p>Multimodal large language models suffer from the catastrophic forgetting problem.Fine-tuning of a model can improve specific performance, but as fine-tuning proceeds, the model begins to exhibit hallucinations, resulting in a significant loss of generality [226] .The hallucination problem of foundation models, that is, when the text generated by the model does not follow the original text (Faithfulness) or does not conform to the facts (Factualness) [227] .As a result, the robot gives wrong judgments or operations in the process of performing tasks in combination with the foundation model [228] .Currently, in order to obtain reasonable answers in multi-turn dialogues with foundation models, multiple prompts need to be provided.Therefore, it is necessary to design foundation models that do not rely on human prompts.Furthermore, foundation models need to have better contextual understanding and causal understanding [229,230] .</p>
<p>Combining large models and small models is also a method that can be tried [231] .Large models are well suited to handle Corner Cases, while small models trained by modeling and for scenarios cannot exhaustively cover the whole scenario, and this part of the work can be handled by the general understanding and strong generalization and inference capabilities of large models.The combination of large model and small model can improve the inference efficiency without reducing the inference ability.</p>
<p>Foundation models for low level control need to be designed.Due to the lack of data for low-level controllers in the training corpora, most of the existing robot foundation models have been applied to robots as either a semantic planner or to interact with robots using human-designed action primitives [232] .Having a large embodied model enables one-step control of the lowest level.</p>
<p>The evaluation of embodied intelligent agents should shift from task-oriented evaluation to capability-value evaluation.Human discrimination tests evaluate AI based on human observations, as represented by the classic Turing test.</p>
<p>Qualitative testing can only be done through human observation and there is no quantitative testing [233][234][235][236][237][238][239] .</p>
<p>Computation Efficiency</p>
<p>Computing efficiency [240] requires continuous evolution and improvement.Optimization aspects while ensuring high performance mainly include the following three aspects: computing costs need to be reduced, computing resources need to be reduced, and computing time needs to be reduced.</p>
<p>There has been a staggering increase in computational power requirements.However, storage performance significantly falls behind that of processors.These two factors contribute to the problems of the computational wall and the storage wall.In response to the above challenges, researchers are committed to addressing issues related to novel AI storage and computing technologies.These technologies aim to break through the storage and computational bottlenecks of AI calculations, improve computational efficiency, and mainly include new applications, computing frameworks, storage-computing architectures, and cloud infrastructure technologies.In terms of hardware architecture, there are two aspects: von Neumann architecture and novel storage-computing architecture.The novel storage-computing architecture can focus on integrated storage-computing chips, neuromorphic chips, and so on.</p>
<p>The models involved in the robot system require efficient computing, including the following aspects: cloudedge-device integration and smart chips.Cloud-edge-device integration requires unified management, cloud-edge collaboration, and resource allocation.Smart chips need to be small in size, low in power consumption, and high in performance.</p>
<p>The existing foundation models have a huge amount of parameters, and the training of parameters requires the support of many high-performance graphics cards, which requires a huge cost.In order to reduce the cost and the amount of parameters of the model, a lightweight foundation model is designed while ensuring the performance [241] .Additionally, further compression can be performed on the network structure of large-scale models.In addition to adjusting the parameter size, extraction methods such as knowledge distillation, network pruning, low-rank parameter decomposition, and quantization can also be employed.</p>
<p>Robot Safety and Ethics</p>
<p>Robot security includes two aspects: physical security and data security.Physical safety refers to the fact that some misunderstandings when combining LLMs with robots may lead to unexpected chain reactions.For example, the robot received instructions to cook Western food in the oven, but it cooked Chinese food.As a result, it turned on the gas and accidentally caught fire.The specificity of robot tasks lies in the constant interaction with the environment during task execution, and robot safety becomes particularly crucial when humans are present in the environment.In the process of completing the specified target tasks, the robot lacks safety guarantee due to physical constraints and other real environment restrictions, and there may be accidents such as collisions, extrusions, and damage to mechanical parts, potentially causing harm to humans.</p>
<p>There is also data security, which involves data privacy.Privacy risks in robot foundation model development and application primarily come from the information contained in the original training data and the powerful inference capabilities of the models.Developers need to ensure that robot foundation models do not cause privacy breaches and carefully evaluate the potential ethical issues they may bring about.In addition to the data bias in the LLMs being trained, every user will also have information security concerns when training the foundation model and uploading the data to the cloud.The intellectual property rights and data security cannot be guaranteed.This requires the use of data desensitization.In addition to the risks of sensitive information leakage and content infringement, the following risks also exist: model denial of service, model theft, training data poisoning, model hallucination, and attacker prompt injection.Therefore, it is necessary to establish a comprehensive and multi-faceted security evaluation, detection, and defense system.</p>
<p>Ethically, the behavior of robots should comply with social and legal norms.Developing autonomous robots with self-awareness is also a promising direction.There is growing concern about ethical and safety aspects of robotic learning, including fairness, transparency, and robustness.It is important to pay attention to biases and toxicity in the training samples of robot foundation models to ensure that robots' behavior does not lead to discrimination or unfairness.</p>
<p>Conclusion</p>
<p>This paper provides an overview of the key challenges of robot learning and the types of algorithms that combine robot learning with foundation models developed to address these challenges.We outline the development and evolution of related technologies for robot learning, as well as the prerequisites such as datasets and computing resources required.We divide these key robot learning challenges into four categories according to downstream tasks, namely manipulation, navigation, planning, and reasoning.With the development of foundational models, they have demonstrated significant progress in robot applications and promising humanoid intelligence [242] .These findings present a bright future for foundational models in robot applications.Last but not least, discussions were conducted, which explained the current problems and challenges of robot learning, and proposed research directions in the future, including robot hardware and software decoupling, dynamic data for interaction with the environment, exclusive foundation models for robots, and so on.Habi tat3.0 [245] BEH AVIO R-100 [246 ] BEH AVIO R-1K [247] iGibson 1.0 [248] AI2-THO R 2.0 [24 9] BabyAI [250]   PyBulle t PyR obot [ 251] Isaac Sim [25 2] RFUn iverse [253]   Uni sim [ 254] Simu lator Objects Scenes Data Volume Task Skills Open X-Embodiment [255] Demonstrations 4435.41GBManipulation 527 HoloAssist [256] RGB, depth, head pose, 3D hand pose, eye gaze, audio, and IMU, text ARMBench [257] Images and videos 190K 1 235K Object segmentation, Object Identification, and Defect Detection RT-1 [62] Instruction and image tokenization 3 130K Manipulation 744</p>
<p>RoboTurk [258] Demonstrations 2 137h Block Lifting (lifting), Bin Picking (picking), and Nut-andpeg Assembly (assembly) Raven [259] Images and RPM 1120000 images and 70000 RPM Reasoning, VQA RoboNet [260] Video frames 4 15 million video frames Manipulation GSO [261] 3D images 1030 13G Manipulation, Navigation and so on Meta-world [262] Video, trajectory 50 Manipulation 50 RLBench [263] RGB, depth, and segmentation masks Manipulation 100</p>
<p>M2DGR [264] RGB image, 3D point cloud, inertial data, GNSS signals 36 sequences (about 1TB) SLAM OBJECTFOLDER 2.0 [265] Object files (containing the complete multisensory profile) 1000 1000 object files Object scale estimation, Contact localization, and Shape reconstruction Google Brain Robot Data [266] Images ~800k grasp attempts Manipulation Dex-Net 2.0 [267] Point cloud 1500 6.7 million point clouds Manipulation Bridge Data [268] Demonstrations (video)</p>
<p>Robotics dataset community</p>
<p>Daily</p>
<p>Interactive Manipulation [270] Position, orientation, force, and torque of objects 3354 trails Manipulation 33</p>
<p>Robot @ Home [271] Intensity images, depth images, and 3D point clouds</p>
<p>Mobile robotics and computer vision</p>
<p>ImageNet [273] Image 1.4 million+ images</p>
<p>Computer vision</p>
<p>EgoNet [273] Image 100+ 1.5 million video frames Manipulation OakInk-Image [274] Image 100 230K image frames</p>
<p>Hand Mesh Recovery and Hand-Object Pose Estimation OakInk-Shape [274] Obj models 62K handobject poses and models Grasp Generation, Intentbased Interaction Generation, and Handover Generation HANDAL [48] Image frames 210 306K image frames Manipulation ScanScribe [201] 3D scan and text 56.1k 1,185 2995 RGB-D scans 3D vision-language grounding Sound-Action-Vision Dataset [105] Sound, RGBD, tracking location ChatGLM [275] Text Tsinghua T5 [276] Text 60M-11B 1T ü Google GPT3 [277] Text 175B 300B û OpenAI LaMDA [278] Text 1.56T words 2B-137B 2.81T û 57.7 days 1024 TPU-v3 chips Google LLaMA [279] Text 4.7T 7B-65B 1.4T ü 21 days 2048 A100 GPU Meta MOSS [280] Text 700B words 16B ü Fudan Universit y InternLM [281] Text 1.6T 7B, 20B, 104B  [287,288] Imag e 11M image s, 1.1B mask s</p>
<p>91M-636M</p>
<p>ü 68 hours 256 GPUs Meta DINOv2 [289] Imag e 1.2B image s ~1B ü 2 days 8 V100-32GB GPUs</p>
<p>Meta</p>
<p>ViT [290] Imag e 317M image s 86M ü Google VideoMAE V2 [291] Video 1.35 M clips 1B ü 14 days 64 A100 GPUs Nanjing Universit y Multilingual Models BLOOM [292] Text 341B token s of text 176B 366B ü 3.5 months 48 nodes with 8 NVIDIA A100 80GB GPUs Google Vicuna [293] Text 70K sampl es 13B ü 1 day 8 A100 GPUs UC Berkeley, etc. Claude û Anthropi c Multimodal Models GPT-4 [294] Text, Imag e 175B 13000B û OpenAI GPT-4V [295] Text, Imag e û OpenAI MiniGPT-5 [296] Text, Imag e 7B ü 4 A6000 GPUs Universit y of Californi a, Santa Cruz ERNIE Bot [297] Text, Imag e 4TB Chine se text corpo ra</p>
<p>100B-260B</p>
<p>û Baidu CLIP [298] Text, Imag e</p>
<p>400M</p>
<p>(imag e, text) pairs ~1.6B ü OpenAI DALL-E [299] Text, Imag e 250M textimage s 12B ü OpenAI</p>
<p>ViLT [300] Text  [319] 77 46200 46200 ü ü ü ü û CMU dataset [320] 150+ 50567 Abo ut 1G 50K 7Do F ü ü û Google grasping dataset [266] 800000 650 K 7Do F ü ü û</p>
<p>Dex-Net 4.0 [321] 25 5M 5M ü 103.16G û û û JACQUA RD [322] 11k  Min Liu et al. [325] 324 25D oF û û ü HOI4D [326] 800 610 2.4M 2.4M ü ü ü ü DexGrasp Net [83] 5355</p>
<p>1.32 M û ü</p>
<p>Graspinganything [32 7] ∼3M 1M 1M sam ples ∼600 M û Table 5 Visual Language Navigation Datasets Data type Size Scenes Annotation Objects Instructions Buildings NavigationNet [328] Image Hundreds of thousands of images 15 Each scene with 1-3 rooms</p>
<p>( 2 )
2
F1 Score: F1 score is calculated based on the overlap between predicted answers and ground truth answers.(3) Exact Match (EM): Calculates the percentage of questions answered exactly correctly.(4) Top-k Accuracy.(5) Mean Reciprocal Rank (MRR): Measures the quality of the topranked answer.It calculates the average reciprocal rank of the first correct answer in the ranked list of answers.(</p>
<p>Table 2 .
2
Comprehensive Robot Datasets Data Type
MineHabiHabiiGibsoOMNIiGibson AI2-MiniGrPyBulleGazeOmniRFUnUniDojotattatn 2.0GIBSTHOidtboverseiversesimSimSimONR 2.0DatasMineReplHabiHumaBEHiGibsoniTHOetDojoicaCtatn VRAVIOdatasetR,ADSyntdemosR-1KRoboheticTHOScenR,esProc
This work was supported by National Key Research and Development Program of China under Grant 2020AAA0108905, National Natural Science Foundation of China (Grant No. 62088101, 52105033), and the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0100).Embodied AgentsPalm-E[189]Text, Imag e 12B, 84B, 562Bû Google VIMA[310]Text VoxPoser[312]Text, Imag e (Based on GPT4) ü Stanford Universit y VOYAGER[313 ]Text, Imag e (Based on GPT4) ü NVIDIA DIAL[69]Text, Imag e 80k trajec toriesGato[314]Text, Imag e, etc. 63M Episo desRoboCat[162]Text, Imag e, etc.1.2Bü DeepMin d MOMA-Force[99]Visio n, Force û ByteDan ce Co-LLM-Agents[154]Text (Based on GPT4)ü Universit y of Massach usetts Amherst CLIPORT[71]Text 240 885 885 720 0 gras ps M2DGR[264]Image, IMU, VI sensor, LiDAR, GNSS 1220.6GB36 DISCOMAN[329]Image, depth, IMU, grid 200 long sequences ALFRED[138]Images, instructions 8055 expert demonstration episodes 120 25k ü VXN[330]Image, text, audio 9.6M episodes 58 21 (category) SCAND[331]Rosbags, LiDAR, Images, Visual Odometry, Wheel odometry 8.7hour/138 trajectories 1656 MuSoHu[332]Image, LiDAR, IMU, audio, depth 300 trajectories 17 labels R2R[139]Image, text 7189 trajectories 90 22kBnB[333]Image, text 1.4M 140k 0.7M REVERIE[140]Image, text 10318 panoramas 132 4140 21702 86Task Duration RangeDataset Size (GB) TaPA[143]Grounded planning with physical scene constraintImage, textDaily tasks 15k instructionsRoboCook[151]Complex long-horizon soft body manipulation tasksPoint clouds Complex long-horizon soft body manipulation tasks 19.73GB RLBench[263]An ambitious largescale benchmark and learning environment Image, depth, joint angles, velocities and forces 100 Manipulation tasks PackIt[334]A dataset of hard packing problemsPoint cloudThe geometry of objects and plan actions for manipulating7.7GBSheetCopilot[335]A dataset as a foundation for assessing the spreadsheet control capabilities Text 221 Spreadsheet control tasks Kinetics[336]A dataset for inverse kinematics of robotic manipulatorsCoordinates of the robotic manipulator end effectorInverse kinematics 15k data HMDB[337]A large human motion database Video Human action tasks 2GB UCF[338]An action recognition data set of realistic action videos Video 101 Action recognition tasks 6.5GBCALVIN[339]A benchmark to learn long-horizon languageconditioned tasks Image, depth, tactile image 34 Languageconditioned continuous control tasks1.3GBIKEA assembly dataset[340]A multi-modal and multi-view video dataset of assembly tasks 3 RGB views, one depth stream, atomic actions, human poses, object segments, object tracking, and extrinsic camera calibration Assembly tasks 406GB Epic-Kitchens[341]A large-scale dataset in first-person (egocentric) vision Image, audio 700 Daily activities in the kitchen 1.1TB TUM Kitchen DataSet[342]A dataset of everyday manipulation activities for motion tracking and action recognition Image, motion capture data, RFID tag readings Action recognition tasks MPII Cooking Activities Dataset[343]A dataset which distinguishes 65hours videoYoucook2[345]A large task-oriented, instructional video dataset Video 2000 Cooking tasks35.1GBKitchen(CMU)[346 ]A dataset contains multimodal measures of the human activity of subjects performing the tasks involved in cooking and food preparation.Video, audio, IMU 5The human activity of subjects performing the tasks involved in cooking and food preparation MMACT[347]A large-scale dataset for cross modal learning on human action understanding RGB, Keypoints, Acceleration, Gyroscope, Orientation, Wi-Fi, Pressure 37 3 major groups: complex actions, simple action, desk actions 40K+ instances FLOBOT Perception[348]A real-world dataset for robotic cleaning in public spaces Image, depth, LiDAR
. G-Z Yang, R J Full, N Jacobstein, Ten robotics technologies of the year [M</p>
<p>A decade retrospective of medical robotics research from 2010 to. P E Dupont, B J Nelson, M Goldfarb, 2020</p>
<p>. Science Robotics. 66080172021</p>
<p>Robots for the people, by the people: Personalizing human-machine interaction. C Clabaugh, M Matarić, </p>
<p>. Science Robotics. 32174512018</p>
<p>A review of ground-based robotic systems for the characterization of nuclear environments. I Tsitsimpelis, C J Taylor, B Lennox, </p>
<p>. Progress in nuclear energy. 1112019</p>
<p>A review of robot learning for manipulation: Challenges, representations, and algorithms. O Kroemer, S Niekum, G Konidaris, The Journal of Machine Learning Research. 2212021</p>
<p>One-shot hierarchical imitation learning of compound visuomotor tasks. T Yu, P Abbeel, S Levine, </p>
<p>Continuous relaxation of symbolic planner for one-shot imitation. D-A Huang, D Xu, Y Zhu, 2019</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>O2a: oneshot observational learning with action vectors. L Pauly, W C Agboh, D C Hogg, </p>
<p>. Frontiers in Robotics and AI. 86863682021</p>
<p>Imitation learning: A survey of learning methods. A Hussein, M M Gaber, E Elyan, </p>
<p>ACM Computing Surveys (CSUR). 201750</p>
<p>Unifying Offline Reinforcement Learning and Imitation Learning from Observations. A Li, B Boots, C-A Cheng, Mahalo, arXiv:2303.171562023arXiv preprint</p>
<p>Hierarchical reinforcement learning: A comprehensive survey. S Pateria, B Subagdja, A-H Tan, </p>
<p>. ACM Computing Surveys (CSUR). 5452021</p>
<p>Grasp learning: Models, methods, and performance. R Platt, Robotics, and Autonomous Systems. 62023Annual Review of Control</p>
<p>. G-Z Yang, </p>
<p>Trends and challenges in robot manipulation. A Billard, D Kragic, </p>
<p>. Science. 364644684142019</p>
<p>Robotic grasping from classical to modern: A survey. H Zhang, J Tang, S Sun, arXiv:2202.036312022arXiv preprint</p>
<p>Core challenges of social robot navigation: A survey. C Mavrogiannis, F Baldini, A Wang, </p>
<p>. ACM Transactions on Human-Robot Interaction. 1232023</p>
<p>Recent trends in task and motion planning for robotics: A Survey. H Guo, F Wu, Y Qin, </p>
<p>ACM Computing Surveys. 2023</p>
<p>Multiple mobile robot task and motion planning: A survey. L Antonyshyn, J Silveira, S Givigi, </p>
<p>. ACM Computing Surveys. 55102023</p>
<p>A survey of Semantic Reasoning frameworks for robotic systems. W Liu, A Daruna, M Patel, Robotics and Autonomous Systems. 1591042942023</p>
<p>A Survey on Large Language Model based Autonomous Agents. L Wang, C Ma, X Feng, </p>
<p>Robot learning from randomized simulations: A review. F Muratore, F Ramos, G Turk, </p>
<p>. Frontiers in Robotics and AI. 312022</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. C Vaucher, F Zipoli, J Geluykens, </p>
<p>. Nature communications. 11136012020</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, </p>
<p>Foundational Models Defining a New Era in Vision: A Survey and Outlook. M Awais, M Naseer, S Khan, arXiv:2307.137212023arXiv preprint</p>
<p>A survey of large language models. W Zhao, X Zhou, K Li, J , </p>
<p>Bayesian Flow Networks. A Graves, T Srivastava R K, Atkinson, arXiv:2308.070372023arXiv preprint</p>
<p>Robot learning from demonstration: kinesthetic teaching vs. B Akgun, K Subramanian, </p>
<p>Unpublished manuscript. 201126</p>
<p>Deep Reinforcement Learning with Skill Library : Exploring with Temporal Abstractions and coarse approximate Dynamics Models. A Agarwal, 2018</p>
<p>One-shot visual imitation learning via meta. C Finn, T Yu, T Zhang, 2017</p>
<p>Human-Level Reward Design via Coding Large Language Models. Y J Ma, W Liang, G Wang, arXiv:2310.129312023arXiv preprint</p>
<p>Trajectories and keyframes for kinesthetic teaching: A humanrobot interaction. B Akgun, M Cakmak, J W Yoo, Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction. the seventh annual ACM/IEEE international conference on Human-Robot Interaction2012</p>
<p>Teleoperation and Visualization Interfaces for Remote Intervention in Space. P Kazanzides, B P Vagvolgyi, W Pryor, Frontiers in Robotics and AI. 87479172021</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, Advances in neural information processing systems. 201629</p>
<p>Foundation Model is Efficient Multimodal Multitask Model Selector. F Meng, W Shao, Z Peng, arXiv:2308.062622023arXiv preprint</p>
<p>A Survey on Multimodal Large Language Models. S Yin, C Fu, S Zhao, arXiv:2306.135492023arXiv preprint</p>
<p>Review of large vision models and visual prompt engineering. J Wang, Z Liu, L Zhao, arXiv:2307.008552023arXiv preprint</p>
<p>Challenges and Applications of Large Language Models. J Kaddour, J Harris, M Mozes, arXiv:2307.101692023arXiv preprint</p>
<p>WebArena: A Realistic Web Environment for Building Autonomous Agents. S Zhou, F F Xu, H Zhu, arXiv:2307.138542023arXiv preprint</p>
<p>Vision-language pretraining: Basics, recent advances, and future trends. Z Gan, L Li, C Li, 202214Foundations and Trends® in Computer Graphics and Vision</p>
<p>Towards natural language interfaces for data visualization: A survey. L Shen, E Shen, Y Luo, IEEE transactions on visualization and computer graphics. 2022</p>
<p>Efficient large-scale language model training on gpu clusters using megatron. D Narayanan, M Shoeybi, J Casper, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and Analysis2021</p>
<p>A survey of embodied ai: From simulators to research tasks. J Duan, S Yu, H L Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 20222</p>
<p>FogROS2-SGC: A ROS2 Cloud Robotics Platform for Secure Global Connectivity. K Chen, R Hoque, K Dharmarajan, </p>
<p>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. A Awadalla, I Gao, J Gardner, arXiv:2308.013902023arXiv preprint</p>
<p>Train Offline, Test Online: A Real Robot Learning Benchmark. G Zhou, V Dean, M K Srirama, </p>
<p>MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Visionand-Language Navigation. X Liang, L Ma, S Guo, arXiv:2306.103222023arXiv preprint</p>
<p>HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions. A Guo, B Wen, J Yuan, </p>
<p>On Human Grasping and Manipulation in Kitchens: Automated Annotation, Insights, and Metrics for Effective Data Collection. N Elangovan, R Godoy, F Sanches, 2023</p>
<p>IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M-A Côté, </p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. J Yang, H Jin, R Tang, arXiv:2304.137122023arXiv preprint</p>
<p>Completing tasks by connecting foundation models with millions of apis. Y Liang, C Wu, T Song, arXiv:2303.164342023arXiv preprint</p>
<p>ToolLLM: Facilitating Large Language Models to Master 16000+ Realworld APIs. Y Qin, S Liang, Y Ye, arXiv:2307.167892023arXiv preprint</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, arXiv:2206.076822022arXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, </p>
<p>. Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>Robot manipulation: Perception, planning, and control. R Tedrake, </p>
<p>. Downloaded On March, 2021</p>
<p>Toward next-generation learned robot manipulation. J Cui, J Trinkle, </p>
<p>. Science Robotics. 65494612021</p>
<p>Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. M J Kim, J Wu, C Finn, arXiv:2307.059592023arXiv preprint</p>
<p>A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI. F Gao, X Li, J Yu, arXiv:2307.113432023arXiv preprint</p>
<p>X Puig, K Ra, M Boben, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018Simulating household activities via</p>
<p>Language models as zero-shot planners. W Huang, P Abbeel, D Pathak, International Conference on Machine Learning. 2022</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, arXiv:2212.068172022arXiv preprint</p>
<p>Rfuniverse: A physicsbased action-centric interactive environment for everyday household tasks. H Fu, W Xu, H Xue, arXiv:2202.001992022arXiv preprint</p>
<p>Tidybot: Personalized robot assistance with large language models. J Wu, R Antonova, A Kan, arXiv:2305.056582023arXiv preprint</p>
<p>Maniskill2: A unified benchmark for generalizable manipulation skills. J Gu, F Xiang, X Li, </p>
<p>Chain-of-Thought Predictive Control. Z Jia, F Liu, V Thumuluri, </p>
<p>Task-Driven Reinforcement Learning With Action Primitives for Long-Horizon Manipulation Skills. H Wang, H Zhang, L Li, IEEE Transactions on Cybernetics. 2023</p>
<p>Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition. H Ha, P Florence, S Song, arXiv:2307.145352023arXiv preprint</p>
<p>Robotic skill acquisition via instruction augmentation with vision-language models. T Xiao, H Chan, P Sermanet, arXiv:2211.117362022arXiv preprint</p>
<p>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation. Y Wang, Z Xian, F Chen, arXiv:2311.014552023arXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, 2022</p>
<p>Simple but effective: Clip embeddings. A Khandelwal, L Weihs, R Mottaghi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Programmatically Grounded, Compositionally Generalizable Robotic Manipulation. R Wang, J Mao, J Hsu, arXiv:2304.138262023arXiv preprint</p>
<p>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. W Huang, C Wang, R Zhang, arXiv:2307.059732023arXiv preprint</p>
<p>Towards Open-World Interactive Disambiguation for Robotic. Y Mo, H Zhang, T Kong, 2023IEEE</p>
<p>Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. Y Ze, G Yan, Y-H Wu, arXiv:2308.168912023arXiv preprint</p>
<p>CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis. J Zheng, Q Zheng, L Fang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Open-Vocabulary Mobile Manipulation. S Yenamandra, A Ramachandran, K Yadav, arXiv:2306.115652023arXiv preprint</p>
<p>Semantic exploration from language abstractions and pretrained representations. A Tam, N Rabinowitz, A Lampinen, Advances in neural information processing systems. 202235</p>
<p>UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning. W Wan, H Geng, Y Liu, arXiv:2304.004642023arXiv preprint</p>
<p>Online augmentation of learned grasp sequence policies for more adaptable and data-efficient in-hand manipulation. E K Gordon, R S Zarrin, arXiv:2304.020522023arXiv preprint</p>
<p>Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations. S Haldar, J Pari, A Rai, </p>
<p>Dexgraspnet: A large-scale robotic dexterous grasp dataset for general objects based on simulation. R Wang, J Zhang, J Chen, 2023IEEE</p>
<p>Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play. I Guzey, B Evans, S Chintala, arXiv:2303.120762023arXiv preprint</p>
<p>DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects. C Bao, H Xu, Y Qin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Tactile Pose Feedback for Closed-loop Manipulation Tasks. K Ota, S Jain, M Zhang, </p>
<p>Leap hand: Low-cost, efficient, and anthropomorphic hand for robot learning. K Shaw, D Pathak, </p>
<p>. Icra Submission, 2023</p>
<p>DEFT: Dexterous Fine-Tuning for Real. A Kannan, K Shaw, S Bahl, /7th Annual Conference on Robot Learning. 2023</p>
<p>On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills. Y Han, M Xie, Y Zhao, </p>
<p>Learning Generalizable Visuomotor Skills from Pre-trained Networks. X Lin, J So, S Mahalingam, </p>
<p>Dynamic Handover: Throw and Catch with Bimanual. B Huang, Y Chen, T Wang, /7th Annual Conference on Robot Learning. 2023</p>
<p>Deep Imitation Learning for Humanoid Loco-manipulation through Human Teleoperation. M Seo, S Han, K Sim, arXiv:2309.019522023arXiv preprint</p>
<p>A system for general inhand object re-orientation. T Chen, J Xu, P Agrawal, 2022</p>
<p>Learning Humanoid Locomotion with Transformers. I Radosavovic, T Xiao, B Zhang, </p>
<p>How can LLMs transform the robotic design process. F Stella, Della Santina, C Hughes, J , </p>
<p>. Nature Machine Intelligence. 2023</p>
<p>Rotating without Seeing: Towards In-hand Dexterity through Touch. Z-H Yin, B Huang, Y Qin, arXiv:2303.108802023arXiv preprint</p>
<p>Next-gpt: Any-to-any multimodal llm. S Wu, H Fei, L Qu, </p>
<p>Multimodal Foundation Models: From Specialists to General-Purpose Assistants. C Li, Z Gan, Z Yang, arXiv:2309.100202023arXiv preprint</p>
<p>MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation. T Yang, Y Jing, H Wu, arXiv:2308.036242023arXiv preprint</p>
<p>VIMA: Robot Manipulation with Multimodal Prompts. Y Jiang, A Gupta, Z Zhang, 2023</p>
<p>AudioPaLM: A Large Language Model That Can Speak and Listen. P K Rubenstein, C Asawaroengchai, D D Nguyen, arXiv:2306.129252023arXiv preprint</p>
<p>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. A Brohan, N Brown, J Carbajal, arXiv:2307.158182023arXiv preprint</p>
<p>From chatgpt to robot intelligence. H M He, Robotgpt, 2023</p>
<p>Perpetual Humanoid Control for Real-time Simulated Avatars. Z Luo, J Cao, A Winkler, arXiv:2305.064562023arXiv preprint</p>
<p>Swoosh! Rattle! Thump!--Actions that Sound. D Gandhi, A Gupta, L Pinto, arXiv:2007.018512020arXiv preprint</p>
<p>Kosmos-2: Grounding Multimodal Large Language Models to the World. Z Peng, Wang W Dong, L , arXiv:2306.148242023arXiv preprint</p>
<p>Robot Learning with Sensorimotor Pre-training. I Radosavovic, B Shi, L Fu, arXiv:2306.100072023arXiv preprint</p>
<p>H Li, Y Zhang, J Zhu, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation. Supplementary Materials</p>
<p>A principal odor map unifies diverse tasks in olfactory perception. B K Lee, E J Mayhew, B Sanchez-Lengeling, </p>
<p>. Science. 38166612023</p>
<p>See to Touch: Learning Tactile Dexterity through Visual Incentives. I Guzey, Y Dai, B Evans, arXiv:2309.123002023arXiv preprint</p>
<p>Real-time Vision-based Navigation for a Robot in an Indoor Environment. S Manglani, arXiv:2307.006662023arXiv preprint</p>
<p>What determines our navigational abilities. T Wolbers, M Hegarty, Trends in cognitive sciences. 1432010</p>
<p>Interactive language: Talking to robots in real time. C Lynch, A Wahid, J Tompson, </p>
<p>. IEEE Robotics and Automation Letters. 2023</p>
<p>Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception. R Tavassoli, M Amani, R Akhavian, </p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street. H Chen, A Suhr, D Misra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Bridging the gap between learning in discrete and continuous environments for vision-and. Y Hong, Z Wang, Q Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. H Tan, L Yu, M Bansal, </p>
<p>Object-and-action aware model for. Y Qi, Z Pan, S Zhang, 2020Springer</p>
<p>Visual semantic navigation using scene priors. W Yang, X Wang, A Farhadi, arXiv:1810.065432018arXiv preprint</p>
<p>Learning object relation graph and tentative policy. H Du, X Yu, L Zheng, ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part VII 16</p>
<p>Layout-Based Causal Inference for Object. S Zhang, X Song, W Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Navigating to objects in the real world. T Gervet, S Chintala, D Batra, </p>
<p>. Science Robotics. 87969912023</p>
<p>Visual language maps for robot. C Huang, O Mees, A Zeng, 2023IEEE</p>
<p>BEVBert: Topo-Metric Map Pre-training for Language-guided Navigation. D An, Y Qi, Y Li, </p>
<p>Learning to act with affordance-aware multimodal neural. Z Jia, K Lin, Y Zhao, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Cows on pasture: Baselines and benchmarks for languagedriven zero-shot object navigation. M Gadre S Y, Wortsman, G Ilharco, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>AnyLoc: Towards Universal Visual Place Recognition. N Keetha, A Mishra, J Karhade, </p>
<p>Where are you? localization from embodied dialog. M Hahn, J Krantz, D Batra, arXiv:2011.082772020arXiv preprint</p>
<p>Talk the walk: Navigating new york city through grounded dialogue. De Vries, H Shuster, K Batra, D , arXiv:1807.033672018arXiv preprint</p>
<p>Vln bert: A recurrent vision-and-language. Y Hong, Q Wu, Y Qi, Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. the IEEE/CVF conference on Computer Vision and Pattern Recognition2021</p>
<p>Adapt: Vision-language navigation with modality-aligned action. B Lin, Y Zhu, Z Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Language-Conditioned Path Planning. A Xie, Y Lee, P Abbeel, arXiv:2308.168932023arXiv preprint</p>
<p>Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. G Zhou, Y Hong, Q Wu, Navgpt, arXiv:2305.169862023arXiv preprint</p>
<p>March in Chat: Interactive Prompting for Remote Embodied Referring Expression. Y Qiao, Y Qi, Z Yu, arXiv:2308.101412023arXiv preprint</p>
<p>Speaker-follower models for vision-and-language navigation. D Fried, R Hu, V Cirik, </p>
<p>. Advances in neural information processing systems. 312018</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision. X Wang, Q Huang, A Celikyilmaz, 2019</p>
<p>anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. K Nguyen, Iii H Daumé, Help, arXiv:1909.018712019arXiv preprint</p>
<p>Alfred: A benchmark for interpreting grounded instructions. M Shridhar, J Thomason, D Gordon, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Vision-andlanguage navigation: Interpreting visuallygrounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Reverie: Remote embodied visual referring expression in real indoor. Y Qi, Q Wu, P Anderson, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. S Yang, O Nachum, Y Du, arXiv:2303.041292023arXiv preprint</p>
<p>Llm+ p: Empowering large language models with optimal planning proficiency. B Liu, Y Jiang, X Zhang, </p>
<p>Embodied Task Planning with Large Language Models. Z Wu, Z Wang, X Xu, arXiv:2307.018482023arXiv preprint</p>
<p>Text2motion: From natural language instructions to feasible plans. K Lin, C Agia, T Migimatsu, arXiv:2303.121532023arXiv preprint</p>
<p>Chatgpt empowered long-step robot control in various environments: A case application. N Wake, A Kanehira, K Sasabuchi, arXiv:2304.038932023arXiv preprint</p>
<p>Embodiedgpt: Visionlanguage pre-training via embodied chain of thought. Y Mu, Q Zhang, M Hu, arXiv:2305.150212023arXiv preprint</p>
<p>TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents. J Ruan, Y Chen, B Zhang, arXiv:2308.034272023arXiv preprint</p>
<p>Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures. Y Zhen, S Bi, L Xing-Tong, arXiv:2306.051712023arXiv preprint</p>
<p>Multi-skill mobile manipulation for object rearrangement. J Gu, D S Chaplot, H Su, arXiv:2209.027782022arXiv preprint</p>
<p>Foundation Model based Open Vocabulary Task Planning and Executive System for General Purpose Service Robots. Y Obinata, N Kanazawa, K Kawaharazuka, arXiv:2308.033572023arXiv preprint</p>
<p>RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools. H Shi, H Xu, S Clarke, arXiv:2306.144472023arXiv preprint</p>
<p>All in One: Multi-Task Prompting for Graph Neural Networks. X Sun, H Cheng, J Li, 2023</p>
<p>Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf. Y Xu, S Wang, P Li, arXiv:2309.046582023arXiv preprint</p>
<p>Building Cooperative Embodied Agents Modularly with Large Language Models. H Zhang, W Du, J Shan, arXiv:2307.024852023arXiv preprint</p>
<p>Dialectic Multi-Robot Collaboration with Large Language Models. Z Mandi, S Jain, Song S Roco, arXiv:2307.047382023arXiv preprint</p>
<p>Communicative agents for software development. C Qian, X Cong, C Yang, arXiv:2307.079242023arXiv preprint</p>
<p>Emergent tool use from multi-agent autocurricula. B Baker, I Kanitscheider, T Markov, arXiv:1909.075282019arXiv preprint</p>
<p>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. L Pan, M Saxon, W Xu, </p>
<p>Correcting robot plans with natural language feedback. P Sharma, B Sundaralingam, V Blukis, arXiv:2204.051862022arXiv preprint</p>
<p>Reflect: Summarizing robot experiences for failure explanation and correction. Z Liu, A Bahety, S Song, arXiv:2306.157242023arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and selfreflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation. K Bousmalis, G Vezzani, D Rao, arXiv:2306.117062023arXiv preprint</p>
<p>Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. N Miao, Y W Teh, Rainforth T Selfcheck, arXiv:2308.004362023arXiv preprint</p>
<p>Demystifying GPT Self-Repair for Code Generation. T X Olausson, J P Inala, C Wang, arXiv:2306.098962023arXiv preprint</p>
<p>A comprehensive survey of continual learning: Theory, method and application. L Wang, X Zhang, H Su, arXiv:2302.004872023arXiv preprint</p>
<p>Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment. Y Guo, Y-J Wang, L Zha, arXiv:2307.003292023arXiv preprint</p>
<p>Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning. S Peng, X Hu, Q Yi, arXiv:2309.013522023arXiv preprint</p>
<p>Code as policies: Language model programs. J Liang, W Huang, F Xia, 2023IEEE</p>
<p>Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification. N Guha, M F Chen, K Bhatia, arXiv:2307.110312023arXiv preprint</p>
<p>Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey. S Dou, J Shan, H Jia, arXiv:2308.011912023arXiv preprint</p>
<p>Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. D Jiang, X Ren, B Y Lin, Llm-Blender, arXiv:2306.025612023arXiv preprint</p>
<p>Codegeex: A pretrained model for code generation with multilingual evaluations on humaneval-x. Q Zheng, X Xia, X Zou, arXiv:2303.175682023arXiv preprint</p>
<p>Large Language Models as Optimizers. C Yang, X Wang, Y Lu, arXiv:2309.034092023arXiv preprint</p>
<p>Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions. H Yang, S Yue, Y He, arXiv:2306.022242023arXiv preprint</p>
<p>Code generation with generated tests. B Chen, F Zhang, A Nguyen, arXiv:2207.103972022arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, Advances in neural information processing systems. 202235</p>
<p>InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. J Yang, A Prabhakar, K Narasimhan, </p>
<p>COLLIE: Systematic Construction of Constrained Text Generation Tasks. S Yao, H Chen, A W Hanjie, </p>
<p>MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations. A Mandlekar, S Nasiriany, B Wen, arXiv:2310.175962023arXiv preprint</p>
<p>Hybrid hierarchical learning for solving complex sequential tasks using the robotic manipulation network. E Triantafyllidis, F Acero, Z Liu, </p>
<p>. Nature Machine Intelligence. 2023</p>
<p>Cognitive Architectures for Language Agents. T Sumers, S Yao, K Narasimhan, arXiv:2309.024272023arXiv preprint</p>
<p>Towards reasoning in large language models: A survey. J Huang, Chang K C-C , arXiv:2212.104032022arXiv preprint</p>
<p>LISA: Reasoning Segmentation via Large Language Model. X Lai, Z Tian, Y Chen, arXiv:2308.006922023arXiv preprint</p>
<p>Reasoning with language model prompting: A survey. S Qiao, Y Ou, N Zhang, arXiv:2212.095972022arXiv preprint</p>
<p>Hypothesis Search: Inductive Reasoning with Language Models. R Wang, E Zelikman, G Poesia, arXiv:2309.056602023arXiv preprint</p>
<p>Purposive learning: Robot reasoning about the meanings of human activities. G Cheng, K Ramirez-Amaro, M Beetz, </p>
<p>. Science Robotics. 42615302019</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, Advances in neural information processing systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, arXiv:2305.106012023arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, arXiv:2303.033782023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, arXiv:2210.036292022arXiv preprint</p>
<p>Cumulative Reasoning With Large Language Models. Y Zhang, J Yang, Y Yuan, arXiv:2308.043712023arXiv preprint</p>
<p>CausalLM is not optimal for in-context learning. N Ding, T Levinboim, J Wu, arXiv:2308.069122023arXiv preprint</p>
<p>Causal Reinforcement Learning: A Survey. Z Deng, J Jiang, G Long, arXiv:2307.014522023arXiv preprint</p>
<p>Unifying Large Language Models and Knowledge Graphs: A Roadmap. S Pan, L Luo, Y Wang, arXiv:2306.083022023arXiv preprint</p>
<p>Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text. N Mihindukulasooriya, S Tiwari, C F Enguix, </p>
<p>KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models. J Kim, Y Kwon, Y Jo, arXiv:2310.112202023arXiv preprint</p>
<p>LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities. Y Zhu, X Wang, J Chen, arXiv:2305.131682023arXiv preprint</p>
<p>ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling. L Yang, H Chen, Z Li, arXiv:2306.114892023arXiv preprint</p>
<p>Robots that ask for help: Uncertainty alignment for large language model planners. Z Ren, A Dixit, A Bodrova, </p>
<p>PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world. R Zellers, A Holtzman, M Peters, arXiv:2106.001882021arXiv preprint</p>
<p>3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment. Z Zhu, X Ma, Y Chen, </p>
<p>GraphGPT: Graph Instruction Tuning for Large Language Models. J Tang, Y Yang, W Wei, </p>
<p>Daydreamer: World models for physical robot. P Wu, A Escontrela, D Hafner, 2023</p>
<p>I Singh, V Blukis, A Mousavian, Generating situated robot task plans using large language models. ICRA</p>
<p>. IEEE. 2023</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, arXiv:2204.016912022arXiv preprint</p>
<p>Statler: Statemaintaining language models for embodied reasoning. T Yoneda, J Fang, P Li, arXiv:2306.178402023arXiv preprint</p>
<p>Physically Grounded Vision-Language Models for Robotic Manipulation. J Gao, B Sarkar, F Xia, arXiv:2309.025612023arXiv preprint</p>
<p>CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection. J Tang, G Zheng, J Yu, arXiv:2309.010932023arXiv preprint</p>
<p>Learning Vision-based Pursuit-Evasion Robot Policies. A Bajcsy, A Loquercio, A Kumar, </p>
<p>When large language models meet personalization: Perspectives of challenges and opportunities. J Chen, Z Liu, X Huang, arXiv:2307.163762023arXiv preprint</p>
<p>Personality traits in large language models. M Safdari, G Serapio-García, C Crepy, </p>
<p>Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench. J Huang, -T, M H Lam, E J Li, </p>
<p>Learning a Universal Human Prior for Dexterous Manipulation from Human Preference. Z Ding, Y Chen, A Z Ren, arXiv:2304.046022023arXiv preprint</p>
<p>Socratis: Are large multimodal models emotionally aware. K Deng, A Ray, R Tan, arXiv:2308.167412023arXiv preprint</p>
<p>Explainable Multimodal Emotion Reasoning. Z Lian, L Sun, M Xu, </p>
<p>From attribution maps to human-understandable explanations through Concept Relevance Propagation. R Achtibat, M Dreyer, I Eisenbraun, </p>
<p>. Nature Machine Intelligence. 20239</p>
<p>Zero-shot causal learning. H Nilforoshan, M Moor, Y Roohani, </p>
<p>Will robots be bodies with brains or brains with bodies. Y Mengüç, N Correll, R Kramer, </p>
<p>. Science Robotics. 21245272017</p>
<p>Software variability in service robotics. S García, D Strüber, D Brugali, Empirical Software Engineering. 282242023</p>
<p>Emerging wearable interfaces and algorithms for hand gesture recognition: A survey. S Jiang, P Kang, X Song, IEEE Reviews in Biomedical Engineering. 152021</p>
<p>Dual Stream Meta Learning for Road Surface Classification and Riding Event Detection on Shared Bikes. S Jiang, Z Strout, B He, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2023</p>
<p>Learning a Flexible Neural Energy Function With a Unique Minimum for Globally Stable and Accurate Demonstration Learning. Z Jin, W Si, A Liu, </p>
<p>. IEEE Transactions on Robotics. 2023</p>
<p>Supervising strong learners by amplifying weak experts. P Christiano, B Shlegeris, D Amodei, </p>
<p>The feasibility of artificial consciousness through the lens of neuroscience. J Aru, M Larkum, J Shine, arXiv:2306.009152023arXiv preprint</p>
<p>A survey of deep active learning. P Ren, Y Xiao, X Chang, </p>
<p>. ACM Computing Surveys (CSUR). 5492021</p>
<p>Investigating the Catastrophic Forgetting in Multimodal Large Language Models. Y Zhai, S Tong, X Li, </p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, </p>
<p>. ACM Computing Surveys. 55122023</p>
<p>Detecting and Preventing Hallucinations in Large Vision Language Models. A Gunjal, J Yin, E Bas, arXiv:2308.063942023arXiv preprint</p>
<p>The Rise and Potential of Large Language Model Based Agents: A Survey. Z Xi, Chen W Guo, X , arXiv:2309.07864v22023arXiv preprint</p>
<p>The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision. Z Yang, L Li, K Lin, </p>
<p>Small models are valuable plug-ins for large language models. C Xu, Y Xu, S Wang, </p>
<p>Language to Rewards for Robotic Skill Synthesis. W Yu, N Gileadi, C Fu, arXiv:2306.086472023arXiv preprint</p>
<p>The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions. Y Peng, J Han, Z Zhang, </p>
<p>. Engineering, 2023</p>
<p>Through the Lens of Core Competency: Survey on Evaluation of Large Language Models. Z Zhuang, Q Chen, L Ma, arXiv:2308.079022023arXiv preprint</p>
<p>AgentBench: Evaluating LLMs as Agents. X Liu, H Yu, H Zhang, arXiv:2308.036882023arXiv preprint</p>
<p>LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. F Dalvi, M Hasanain, S Boughorbel, arXiv:2308.049452023arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, arXiv:2307.031092023arXiv preprint</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, arXiv:2302.040232023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, arXiv:2206.046152022arXiv preprint</p>
<p>Large Language Model for Science: A Study on P vs. Q Dong, L Dong, K Xu, arXiv:2309.056892023arXiv preprint</p>
<p>A Survey on Model Compression for Large Language Models. X Zhu, J Li, Y Liu, arXiv:2308.076332023arXiv preprint</p>
<p>Human-like systematic generalization through a meta-learning neural network. M Lake, M Baroni, </p>
<p>. Nature. 2023</p>
<p>Building open-ended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, Advances in neural information processing systems. 202235</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, Advances in neural information processing systems. 202134</p>
<p>HABITAT 3.0: A CO-HABITAT FOR HUMANS. X Puig, E Undersander, A Szot, 2023</p>
<p>Behavior: Benchmark for everyday household activities in virtual, interactive. S Srivastava, C Li, M Lingelbach, 2022</p>
<p>Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. C Li, R Zhang, J Wong, 2023</p>
<p>iGibson 1.0: A simulation environment for interactive tasks in large realistic. B Shen, F Xia, C Li, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, arXiv:1712.054742017arXiv preprint</p>
<p>A platform to study the sample efficiency of grounded language learning. M Chevalier-Boisvert, D Bahdanau, S Lahlou, arXiv:1810.082722018arXiv preprint</p>
<p>Pyrobot: An open-source robotics framework for research and benchmarking. A Murali, T Chen, K Alwala, arXiv:1906.082362019arXiv preprint</p>
<p>Isaac gym: High performance gpu-based physics simulation for robot learning. V Makoviychuk, L Wawrzyniak, Y Guo, arXiv:2108.104702021arXiv preprint</p>
<p>Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI. H Fu, W Xu, R Ye, </p>
<p>Learning Interactive Real-World Simulators. M Yang, Y Du, K Ghasemipour, arXiv:2310.061142023arXiv preprint</p>
<p>Open X-Embodiment: Robotic learning datasets and RT-X models. A Padalkar, A Pooley, A Jain, arXiv:2310.088642023arXiv preprint</p>
<p>HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World. X Wang, T Kwon, M Rad, 2023</p>
<p>ARMBench: An object-centric benchmark dataset for robotic manipulation. C Mitash, F Wang, S Lu, arXiv:2303.163822023arXiv preprint</p>
<p>Roboturk: A crowdsourcing platform for robotic skill learning through imitation. A Mandlekar, Y Zhu, A Garg, PMLR. 2018</p>
<p>Raven: A dataset for relational and analogical visual. C Zhang, F Gao, B Jia, 2019</p>
<p>Robonet: Largescale multi-robot learning. S Dasari, F Ebert, S Tian, arXiv:1910.112152019arXiv preprint</p>
<p>Google scanned objects: A high-quality dataset of 3d scanned household items. L Downs, A Francis, N Koenig, /2022 International Conference on Robotics and Automation (ICRA). </p>
<p>. IEEE. 2022</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement. T Yu, D Quillen, Z He, 2020PMLR</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, Arrojo D R, </p>
<p>. IEEE Robotics and Automation Letters. 522020</p>
<p>M2dgr: A multi-sensor and multi-scenario slam dataset for ground robots. J Yin, A Li, T Li, </p>
<p>. IEEE Robotics and Automation Letters. 722021</p>
<p>Objectfolder 2.0: A multisensory object dataset for sim2real transfer. R Gao, Z Si, Y-Y Chang, 2022</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, </p>
<p>. The International journal of robotics research. 374-52018</p>
<p>Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. J Mahler, J Liang, S Niyaz, arXiv:1703.093122017arXiv preprint</p>
<p>Bridge data: Boosting generalization of robotic skills with crossdomain datasets. F Ebert, Y Yang, K Schmeckpeper, </p>
<p>RH20T: A Robotic Dataset for Learning Diverse Skills in One-Shot. H-S Fang, H Fang, Z Tang, </p>
<p>A dataset of daily interactive manipulation. Y Huang, Y Sun, The International journal of robotics research. 3882019</p>
<p>Robot@ home, a robotic dataset for semantic mapping of home environments. J R Ruiz-Sarmiento, C Galindo, J González-Jiménez, The International journal of robotics research. 3622017</p>
<p>Teach: Task-driven embodied agents that. A Padmakumar, J Thomason, A Shrivastava, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Exploring Visual Pretraining for Robot Manipulation: Datasets, Models and Methods. Y Jing, X Zhu, X Liu, arXiv:2308.036202023arXiv preprint</p>
<p>OakInk: A large-scale knowledge repository for understanding hand. L Yang, K Li, X Zhan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, arXiv:2210.024142022arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, The Journal of Machine Learning Research. 2112020</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Advances in neural information processing systems. 202033</p>
<p>Language models for dialog applications. R Thoppilan, De Freitas, D Hall, J , arXiv:2201.082392022arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, arXiv:2302.139712023arXiv preprint</p>
<p>Training conversational language models from synthetic data. T Sun, X Zhang, Z He, arXiv:2307.1502020237arXiv preprint</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. I Team, 2023</p>
<p>Baichuan 2: Open large-scale language models. A Yang, B Xiao, B Wang, arXiv:2309.103052023arXiv preprint</p>
<p>Qwen technical report. J Bai, S Bai, Y Chu, arXiv:2309.166092023arXiv preprint</p>
<p>PanGu-{\Sigma}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing. X Ren, P Zhou, X Meng, arXiv:2303.108452023arXiv preprint</p>
<p>Colossal-ai: A unified deep learning system for large. S Li, H Liu, Z Bian, Proceedings of the 52nd International Conference on Parallel Processing. the 52nd International Conference on Parallel Processing2023</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, 2023</p>
<p>. A Kirillov, E Mintun, N Ravi, arXiv:2304.026432023arXiv preprint</p>
<p>A Comprehensive Survey on Segment Anything Model for Vision and Beyond. C Zhang, L Liu, Y Cui, arXiv:2305.081962023arXiv preprint</p>
<p>Learning robust visual features without supervision. M Oquab, T Darcet, T Moutakanni, arXiv:2304.071932023arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, arXiv:2010.119292020arXiv preprint</p>
<p>Videomae v2: Scaling video masked autoencoders with dual masking. L Wang, B Huang, Z Zhao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. T L Scao, Fan A Akiki, C , arXiv:2211.051002022arXiv preprint</p>
<p>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. W-L Chiang, Z Li, Z Lin, </p>
<p>GPT-4 Technical. arXiv:2303.08774v32023OpenAIarXiv preprint</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision. Z Yang, L Li, K Lin, </p>
<p>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. K Zheng, X He, X E Wang, arXiv:2310.022392023arXiv preprint</p>
<p>Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pretraining for language understanding and generation. S Wang, Y Sun, Y Xiang, arXiv:2112.127312021arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, 2021</p>
<p>Zero-shot textto-image generation. A Ramesh, M Pavlov, G Goh, International Conference on Machine Learning. PMLR2021</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. W Kim, B Son, I Kim, </p>
<p>Meta-transformer: A unified framework for multimodal learning. Y Zhang, K Gong, K Zhang, </p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, </p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision. D Shah, B Osiński, S Levine, 2023</p>
<p>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. W Huang, C Wang, R Zhang, arXiv:2307.059732023arXiv preprint</p>
<p>Voyager: An openended embodied agent with large language models. G Wang, Y Xie, Y Jiang, arXiv:2305.162912023arXiv preprint</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, arXiv:2205.061752022arXiv preprint</p>
<p>Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model. S Huang, Z Jiang, H Dong, arXiv:2305.111762023arXiv preprint</p>
<p>Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. J Yang, W Tan, C Jin, arXiv:2306.057162023arXiv preprint</p>
<p>Robotic grasping of novel objects using vision. A Saxena, J Driemeyer, A Y Ng, The International journal of robotics research. 2722008</p>
<p>Efficient grasping from rgbd images: Learning using a new rectangle. Y Jiang, S Moseson, A Saxena, 2011</p>
<p>. IEEE International conference on robotics and automation. IEEE. 2011</p>
<p>Benchmarking in manipulation research: Using the Yale-CMU-Berkeley object and model set. B Calli, A Walsman, A Singh, </p>
<p>. IEEE Robotics &amp; Automation Magazine. 2232015</p>
<p>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot. L Pinto, A Gupta, IEEE international conference on robotics and automation. 2016. 2016IEEEICRA</p>
<p>Learning ambidextrous robot grasping policies. J Mahler, M Matl, V Satish, </p>
<p>. Science Robotics. 42649842019</p>
<p>Jacquard: A large scale dataset for robotic grasp detection. A Depierre, E Dellandréa, L Chen, 2018</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2018</p>
<p>Graspnet-1billion: A large-scale benchmark for general object grasping. H-S Fang, C Wang, M Gou, 2020</p>
<p>W Gao, R Tedrake, Kpam-Sc, Generalizable manipulation planning using keypoint affordance and shape completion. ICRA</p>
<p>. IEEE. 2021</p>
<p>Generating grasp poses for a high-dof gripper. M Liu, Z Pan, K Xu, C]//2019</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>HOI4D: A 4D egocentric dataset for category-level human. Y Liu, Y Liu, C Jiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Grasp-Anything: Large-scale Grasp Dataset from Foundation Models. D Vuong, M N Vu, H Le, arXiv:2309.098182023arXiv preprint</p>
<p>NavigationNet: A large-scale interactive indoor navigation dataset. H Huang, Y Shen, J Sun, </p>
<p>DISCOMAN: Dataset of Indoor SCenes for Odometry. P Kirsanov, A Gaskarov, F Konokhov, </p>
<p>Mapping And Navigation. 2019</p>
<p>Towards versatile embodied navigation. H Wang, W Liang, L Gool, Advances in neural information processing systems. 202235</p>
<p>Socially compliant navigation dataset (scand): A large-scale dataset of demonstrations for social navigation. H Karnan, A Nair, X Xiao, </p>
<p>. IEEE Robotics and Automation Letters. 742022</p>
<p>Toward Human-Like Social Robot Navigation: A Large-Scale, Multi-Modal, Social Human Navigation Dataset. M Nguyen, M Nazeri, A Payandeh, arXiv:2303.148802023arXiv preprint</p>
<p>Airbert: Indomain pretraining for vision-and. P-L Guhur, M Tapaswi, S Chen, 2021</p>
<p>A Goyal, J Deng, Packit, International Conference on Machine Learning. PMLR2020</p>
<p>SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. H Li, J Su, Y Chen, arXiv:2305.193082023arXiv preprint</p>
<p>Utilization of multilayer perceptron for determining the inverse kinematics of an industrial robotic manipulator. S B Šegota, N Anđelić, V Mrzljak, International Journal of Advanced Robotic Systems. 18417298814209252832021</p>
<p>HMDB: a large video database for human motion recognition. H Kuehne, H Jhuang, E Garrote, 2011IEEE</p>
<p>UCF101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A R Zamir, M Shah, arXiv:1212.04022012arXiv preprint</p>
<p>A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, </p>
<p>. IEEE Robotics and Automation Letters. 732022</p>
<p>The ikea asm dataset: Understanding people assembling furniture through actions. Y Ben-Shabat, X Yu, F Saleh, 2021</p>
<p>Scaling egocentric vision: The epic-kitchens. D Damen, H Doughty, G M Farinella, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>The TUM kitchen data set of everyday manipulation activities for motion tracking and action recognition. M Tenorth, J Bandouch, M Beetz, IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops. IEEE2009. 2009</p>
<p>A database for fine grained activity detection of cooking. M Rohrbach, S Amin, M Andriluka, IEEE. 2012</p>
<p>Coin: A large-scale dataset for comprehensive. Y Tang, D Ding, Y Rao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Towards automatic learning of procedures from web instructional. L Zhou, C Xu, J Corso, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Guide to the carnegie mellon university multimodal activity (cmu-mmac) database. F De La Torre, J Hodgins, A Bargteil, 2009</p>
<p>Mmact: A large-scale dataset for cross modal human action. Q Kong, Z Wu, Z Deng, 2019</p>
<p>Robot perception of static and dynamic objects with an autonomous floor scrubber. Z Yan, S Schreiberhuber, G Halmetschlager, </p>
<p>Benchmarking progress to infant-Level physical reasoning in AI. L Weihs, A Yuile, R Baillargeon, Transactions on Machine Learning Research. 2022</p>
<p>Unicorn on rainbow: A universal commonsense reasoning. N Lourie, Le Bras, R Bhagavatula, C , </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1656 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1656</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1656</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-259937271</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.07862v1.pdf" target="_blank">Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality</a></p>
                <p><strong>Paper Abstract:</strong> Simulation-to-real is the task of training and developing machine learning models and deploying them in real settings with minimal additional training. This approach is becoming increasingly popular in fields such as robotics. However, there is often a gap between the simulated environment and the real world, and machine learning models trained in simulation may not perform as well in the real world. We propose a framework that utilizes a message-passing pipeline to minimize the information gap between simulation and reality. The message-passing pipeline is comprised of three modules: scene understanding, robot planning, and performance validation. First, the scene understanding module aims to match the scene layout between the real environment set-up and its digital twin. Then, the robot planning module solves a robotic task through trial and error in the simulation. Finally, the performance validation module varies the planning results by constantly checking the status difference of the robot and object status between the real set-up and the simulation. In the experiment, we perform a case study that requires a robot to make a cup of coffee. Results show that the robot is able to complete the task under our framework successfully. The robot follows the steps programmed into its system and utilizes its actuators to interact with the coffee machine and other tools required for the task. The results of this case study demonstrate the potential benefits of our method that drive robots for tasks that require precision and efficiency. Further research in this area could lead to the development of even more versatile and adaptable robots, opening up new possibilities for automation in various industries.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1656.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1656.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim2Plan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A message-passing framework that constructs a digital twin in Nvidia Omniverse, uses vision-based scene understanding (OWL-ViT / Grounding DINO) and geometric inference, plans motions with Riemannian Motion Policies + RRT, and validates execution with joint-state checks and MiniGPT-4 VQA to enable zero-shot sim-to-real transfer for a multi-step manipulation (coffee-making) task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Fixed-base robot arm with gripper (coffee-making setup)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A robot arm equipped with a gripper operating in a bounded tabletop workspace, instrumented by a single RGB monocular camera. Used to pick/place objects (cup, coffee capsule) and interact with a coffee machine.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Nvidia Omniverse</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A high-fidelity digital-twin platform (Omniverse) used to simulate rigid bodies, soft bodies, articulated objects, fluids, lighting/ray-traced rendering, and physics interactions; supports Python scripting and external libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity physics simulation with photorealistic (ray-traced) rendering</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, soft-body dynamics, fluid simulation (claimed), articulated bodies, gravity, friction, collisions, lighting and ray-traced visual rendering, camera intrinsics and viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No explicit modeling or reporting of sensor noise, no depth sensor (only monocular RGB used), limited reporting of actuator low-level dynamics or delays, potential simplifications in internal coffee-machine mechanisms and fluid-handling precision; background/environment variability and human interaction effects not explicitly modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Controlled tabletop experimental platform with fixed camera and robot positions in some trials; objects include coffee machine, cup, and coffee capsule; experiments run with both fixed and randomized object positions; perception via single RGB camera.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Zero-shot transfer of motion planning and manipulation subtasks (pick up cup/capsule, place objects, operate coffee machine) from simulation to real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Planner-based approach: perception using pretrained object detectors (OWL-ViT, Grounding DINO) + geometric 2D->3D inference with priors; control via Riemannian Motion Policies (RMP) and path planning via Rapidly-exploring Random Tree (RRT). No reinforcement learning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate (%) over 20 trials (overall task completion and subtask pick-up success); also reported perception 3D position error (<= 5 mm).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Fixed scene overall success: 90% (18/20). Randomized positions: capsule pick-up success 83.3%; cup pick-up success 76.6%; overall completion under randomization: 75% (15/20).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual appearance / lighting differences, missing depth sensing (monocular-only), unmodeled sensor noise, potential mismatch in contact/friction parameters, unmodeled actuator/controller delays or dynamics, calibration errors between simulated and real coordinate frames, and environmental variability (background, human interactions) described as typical gap sources.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Detailed digital twin construction in Omniverse, accurate scene understanding combining prior geometry and object detectors, 3D position estimates controlled within ~5 mm, robust planner/controller design (RMP + RRT), continuous performance validation (joint state comparisons) and VQA-driven task verification (MiniGPT-4), and an immediate stop policy when large deviations are detected.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Yes — the paper emphasizes that achieving 3D position prediction error within about 5 mm was sufficient for precise motion planning; additionally the use of high-fidelity physics and close scene matching (digital twin) are highlighted as important for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a high-fidelity digital twin, accurate perception (<=5 mm), robust planner (RMP+RRT), and continuous validation, Sim2Plan achieves zero-shot sim-to-real transfer for a complex multi-step manipulation task with high success (90% fixed, ~75% randomized) without real-world fine-tuning; message-passing between simulation and reality narrows the information gap and enables safe, efficient deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1656.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1656.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IsaacGym_ref</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Isaac gym: High performance gpu-based physics simulation for robot learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced GPU-accelerated physics simulation platform (Isaac Gym) designed for high-performance robot learning and sim-to-real research; mentioned in related work as an enabling simulator for sim-to-real training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Isaac gym: High performance gpu-based physics simulation for robot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>High-performance GPU-based physics simulator for robot learning (as described in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1656.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1656.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgileLocomotion_ref</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real: Learning agile locomotion for quadruped robots</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work on sim-to-real transfer applied to quadruped locomotion that demonstrated domain randomization and other techniques to transfer agile locomotion policies from simulation to real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real: Learning agile locomotion for quadruped robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1656.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1656.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim2RealSurvey_ref</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim2real in robotics and automation: Applications and challenges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced survey/perspective on sim-to-real approaches, applications, and challenges in robotics and automation; cited in related work to contextualize Sim2Plan.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim2real in robotics and automation: Applications and challenges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Sim2real in robotics and automation: Applications and challenges <em>(Rating: 2)</em></li>
                <li>Isaac gym: High performance gpu-based physics simulation for robot learning <em>(Rating: 2)</em></li>
                <li>Sim2real grasp pose estimation for adaptive robotic applications <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1656",
    "paper_id": "paper-259937271",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Sim2Plan",
            "name_full": "Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality",
            "brief_description": "A message-passing framework that constructs a digital twin in Nvidia Omniverse, uses vision-based scene understanding (OWL-ViT / Grounding DINO) and geometric inference, plans motions with Riemannian Motion Policies + RRT, and validates execution with joint-state checks and MiniGPT-4 VQA to enable zero-shot sim-to-real transfer for a multi-step manipulation (coffee-making) task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Fixed-base robot arm with gripper (coffee-making setup)",
            "agent_system_description": "A robot arm equipped with a gripper operating in a bounded tabletop workspace, instrumented by a single RGB monocular camera. Used to pick/place objects (cup, coffee capsule) and interact with a coffee machine.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "Nvidia Omniverse",
            "virtual_environment_description": "A high-fidelity digital-twin platform (Omniverse) used to simulate rigid bodies, soft bodies, articulated objects, fluids, lighting/ray-traced rendering, and physics interactions; supports Python scripting and external libraries.",
            "simulation_fidelity_level": "High-fidelity physics simulation with photorealistic (ray-traced) rendering",
            "fidelity_aspects_modeled": "Rigid-body dynamics, soft-body dynamics, fluid simulation (claimed), articulated bodies, gravity, friction, collisions, lighting and ray-traced visual rendering, camera intrinsics and viewpoints.",
            "fidelity_aspects_simplified": "No explicit modeling or reporting of sensor noise, no depth sensor (only monocular RGB used), limited reporting of actuator low-level dynamics or delays, potential simplifications in internal coffee-machine mechanisms and fluid-handling precision; background/environment variability and human interaction effects not explicitly modeled.",
            "real_environment_description": "Controlled tabletop experimental platform with fixed camera and robot positions in some trials; objects include coffee machine, cup, and coffee capsule; experiments run with both fixed and randomized object positions; perception via single RGB camera.",
            "task_or_skill_transferred": "Zero-shot transfer of motion planning and manipulation subtasks (pick up cup/capsule, place objects, operate coffee machine) from simulation to real robot.",
            "training_method": "Planner-based approach: perception using pretrained object detectors (OWL-ViT, Grounding DINO) + geometric 2D-&gt;3D inference with priors; control via Riemannian Motion Policies (RMP) and path planning via Rapidly-exploring Random Tree (RRT). No reinforcement learning reported.",
            "transfer_success_metric": "Task success rate (%) over 20 trials (overall task completion and subtask pick-up success); also reported perception 3D position error (&lt;= 5 mm).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Fixed scene overall success: 90% (18/20). Randomized positions: capsule pick-up success 83.3%; cup pick-up success 76.6%; overall completion under randomization: 75% (15/20).",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Visual appearance / lighting differences, missing depth sensing (monocular-only), unmodeled sensor noise, potential mismatch in contact/friction parameters, unmodeled actuator/controller delays or dynamics, calibration errors between simulated and real coordinate frames, and environmental variability (background, human interactions) described as typical gap sources.",
            "transfer_enabling_conditions": "Detailed digital twin construction in Omniverse, accurate scene understanding combining prior geometry and object detectors, 3D position estimates controlled within ~5 mm, robust planner/controller design (RMP + RRT), continuous performance validation (joint state comparisons) and VQA-driven task verification (MiniGPT-4), and an immediate stop policy when large deviations are detected.",
            "fidelity_requirements_identified": "Yes — the paper emphasizes that achieving 3D position prediction error within about 5 mm was sufficient for precise motion planning; additionally the use of high-fidelity physics and close scene matching (digital twin) are highlighted as important for transfer.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Using a high-fidelity digital twin, accurate perception (&lt;=5 mm), robust planner (RMP+RRT), and continuous validation, Sim2Plan achieves zero-shot sim-to-real transfer for a complex multi-step manipulation task with high success (90% fixed, ~75% randomized) without real-world fine-tuning; message-passing between simulation and reality narrows the information gap and enables safe, efficient deployment.",
            "uuid": "e1656.0",
            "source_info": {
                "paper_title": "Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "IsaacGym_ref",
            "name_full": "Isaac gym: High performance gpu-based physics simulation for robot learning",
            "brief_description": "Referenced GPU-accelerated physics simulation platform (Isaac Gym) designed for high-performance robot learning and sim-to-real research; mentioned in related work as an enabling simulator for sim-to-real training.",
            "citation_title": "Isaac gym: High performance gpu-based physics simulation for robot learning",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": null,
            "virtual_environment_name": "Isaac Gym",
            "virtual_environment_description": "High-performance GPU-based physics simulator for robot learning (as described in the cited work).",
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": null,
            "uuid": "e1656.1",
            "source_info": {
                "paper_title": "Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "AgileLocomotion_ref",
            "name_full": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "brief_description": "Referenced prior work on sim-to-real transfer applied to quadruped locomotion that demonstrated domain randomization and other techniques to transfer agile locomotion policies from simulation to real robots.",
            "citation_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": null,
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": null,
            "uuid": "e1656.2",
            "source_info": {
                "paper_title": "Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Sim2RealSurvey_ref",
            "name_full": "Sim2real in robotics and automation: Applications and challenges",
            "brief_description": "Referenced survey/perspective on sim-to-real approaches, applications, and challenges in robotics and automation; cited in related work to contextualize Sim2Plan.",
            "citation_title": "Sim2real in robotics and automation: Applications and challenges",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": null,
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": null,
            "uuid": "e1656.3",
            "source_info": {
                "paper_title": "Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim2real in robotics and automation: Applications and challenges",
            "rating": 2,
            "sanitized_title": "sim2real_in_robotics_and_automation_applications_and_challenges"
        },
        {
            "paper_title": "Isaac gym: High performance gpu-based physics simulation for robot learning",
            "rating": 2,
            "sanitized_title": "isaac_gym_high_performance_gpubased_physics_simulation_for_robot_learning"
        },
        {
            "paper_title": "Sim2real grasp pose estimation for adaptive robotic applications",
            "rating": 2,
            "sanitized_title": "sim2real_grasp_pose_estimation_for_adaptive_robotic_applications"
        }
    ],
    "cost": 0.012137,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality
15 Jul 2023</p>
<p>Yizhou Zhao yizhouzhao@g.ucla.edu 
Department of Statistics
University of California
Los Angeles</p>
<p>Yuanhong Zeng 
Department of Statistics
University of California
Los Angeles</p>
<p>Qian Long 
Department of Statistics
University of California
Los Angeles</p>
<p>Ying Nian Wu 
Department of Statistics
University of California
Los Angeles</p>
<p>Song-Chun Zhu 
Department of Statistics
University of California
Los Angeles</p>
<p>Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality
15 Jul 20239FE1AC8FF3FCA1630B22CD54ED9E2211arXiv:2307.07862v1[cs.RO]Digital twinRobotics3-DAutomation
Simulation-to-real is the task of training and developing machine learning models and deploying them in real settings with minimal additional training.This approach is becoming increasingly popular in fields such as robotics.However, there is often a gap between the simulated environment and the real world, and machine learning models trained in simulation may not perform as well in the real world.We propose a framework that utilizes a message-passing pipeline to minimize the information gap between simulation and reality.The message-passing pipeline is comprised of three modules: scene understanding, robot planning, and performance validation.First, the scene understanding module aims to match the scene layout between the real environment set-up and its digital twin.Then, the robot planning module solves a robotic task through trial and error in the simulation.Finally, the performance validation module varies the planning results by constantly checking the status difference of the robot and object status between the real set-up and the simulation.In the experiment, we perform a case study that requires a robot to make a cup of coffee.Results show that the robot is able to complete the task under our framework successfully.The robot follows the steps programmed into its system and utilizes its actuators to interact with the coffee machine and other tools required for the task.A noteworthy observation from the experiment is the speed and accuracy with which the robot completed the task.The robot can make a cup of coffee relatively quickly compared with traditional robot planning and control methods, and its movements were precise and efficient.Overall, the results of this case study demonstrate the potential benefits of our method that drive robots for tasks that require precision and efficiency.Further research in this area could lead to the development of even more versatile and adaptable robots, opening up new possibilities for automation in various industries.</p>
<p>Fig. 1.An overview of our Sim2Plan framework.A high-level schematic of the three main components that make up the Sim2Plan framework: the experiment platform, the simulation engine, and the message-passing pipeline.</p>
<p>Introduction</p>
<p>Traditionally, training a robot in a real setting involves designing the task, examining and setting up hardware, programming the robot, and testing the performance [9].These steps require careful planning, design, and execution, as well as ongoing evaluation and refinement.In general, training a robot in the real world can be expensive in terms of cost and time, especially when optimal performance and safety need to be ensured.</p>
<p>Recently, Simulation-to-reality (Sim2Real) has been active in robotics.Driven by advances in physics-based simulation, machine learning, and AI-based benchmarking, Sim2Real techniques make it more efficient and accessible for robot training and application in the real world.Compared with traditional robot training methods, Sim2Real can be less expensive than traditional robot training methods because they do not require as many environmental resources [7].It can also increase safety, enable faster iteration, allow more precise and complex control, and improve the robot's performance from more training data.</p>
<p>Although Sim2Real techniques offer many advantages for robot training and deployment, they also introduce some challenges related to the Sim2Real gap.For example, simulated environments may not perfectly match the real world: simulation can be hard to perfectly replicate the complexities and nuances of the real world [7].Besides, robots trained in simulation may struggle to generalize their learned behaviors and policies to new, unseen environments in the real world, which limits the robot's performance and adaptability in diverse environments [19].In addition, real-world environments are inherently uncertain and variable, with unpredictable factors such as lighting, background, and human interactions [9].</p>
<p>To address the challenges introduced by the Sim2Real gap, we introduce Sim2Plan, a framework that leverages the strengths of simulation for robot motion planning in real-world deployment.Sim2Plan combines advanced simulation techniques with transfer learning and domain adaptation to enable robots to learn from simulated environments and transfer those learned behaviors to the real world.</p>
<p>Specifically, Sim2Plan consists of three main components: a real world experiment platform, a simulated environment, and a message-passing pipeline.The core idea behind Sim2Plan is to use the message-passing pipeline to interchange the information between the real-world experiment and the simulation while constantly checking and correcting the information gap.The messagepassing pipeline (composed by scene understanding, robot motion planning, and performance validation) is targeted to match the scenes, robots, and experiments between reality and simulation.The scene understanding module collects data from real experiments and constructs a digital twin in simulation.It also provides essential information for the robot motion planning module, which generates actions for the robot both in simulation and in the physical environment.Lastly, the performance validation Module evaluates the robot's actual behavior against its simulated counterpart, allowing for continuous improvement and optimization of the system.Overall, our framework enables efficient and accurate robot control through seamless integration between virtual and real-world environments.</p>
<p>We evaluate Sim2Plan on a coffee-making case study.This study requires the robot to perform object manipulation tasks through motion planning.Sim2Plan demonstrate its effectiveness in improving the performance and generalization of robotic systems.Our results show that Sim2Plan can significantly reduce the amount of real-world training time required while enabling robots to perform effectively in diverse and challenging randomized environments.</p>
<p>Overall, Sim2Plan represents a promising approach to addressing the challenges introduced by the Sim2Real gap, and has the potential to significantly improve the efficiency and effectiveness of robot training and deployment in the real world.</p>
<p>Related Work</p>
<p>A considerable body of research is related to our study, including work on simulating real-world environments for robots, motion planning algorithms, and embodied AI simulation.</p>
<p>Sim2Real</p>
<p>Recent advances in simulation-to-reality (Sim2Real) transfer have enabled robots to learn complex manipulation tasks in simulation and apply these skills to the real world.For instance, researchers have used Sim2Real transfer to teach robots to grasp objects with greater accuracy [8], navigate through challenging environments [21], and even perform tasks like pouring liquid into a cup [6].These advances are made possible by using machine learning algorithms to train robots in simulation and then fine-tuning the learned skills in the real world.Additionally, advancements in hardware technology, such as high-fidelity simulators and robust robotic systems [14] , have contributed to the success of Sim2Real transfer.As a result, Sim2Real transfer is becoming an increasingly popular approach for developing more capable and versatile robots that can operate effectively in dynamic, real-world environments.</p>
<p>Robot Motion Planning</p>
<p>Recent studies have focused on developing more advanced algorithms for robot motion planning that can handle increasingly complex scenarios.Some of these approaches involve using machine learning techniques to generate plans based on past experience [16], while others leverage cloud computing resources to distribute computationally intensive tasks among multiple servers [22].Other areas of interest include improving plan robustness to uncertainty and ensuring safe interactions with humans in shared workspaces [5].Ultimately, the goal of these efforts is to enable robots to perform tasks autonomously and efficiently in dynamic and uncertain environments [11].</p>
<p>Embodied AI Simulators</p>
<p>Embodied AI is intelligence that emerges through interacting with environments [4].The growing interest in embodied AI fosters the development of embodied AI simulators, which serve as benchmarks [2] to train and develop intelligent systems before deploying them in the real world.The simulators typically address three typical AI research tasks: visual exploration, visual navigation, and embodied question-answering [3].In visual exploration, the agent navigates through the environment, processes visual information, identifies objects, and learns their spatial relationships [10].In visual navigation, the agent knows to plan its route, avoid obstacles, and adapt its strategy based on environmental changes [23].Finally, embodied QA tasks involve AI agents answering questions or reasoning about their environment based on their egocentric perceptions.</p>
<p>Framework</p>
<p>In this section, we will discuss the various components that make up our Sim2Plan framework, including establishing an experimental platform in the real world, creating a simulated environment, and implementing a robust messaging-passing pipeline.We show these in Figure 1.</p>
<p>The experiment platform serves as the interface for the real-world robot environment (Section 3.1).The simulation part acts as the digital twin of the experiment platform (Section 3.2), allowing for accurate modeling and prediction of system behavior.Finally, the core element of the framework is the messagepassing pipeline (Section 4), which facilitates seamless communication between the experiment platform and the simulation engine.</p>
<p>Experiment Platform</p>
<p>The Sim2Plan framework requires the creation of a physical experimentation platform in which the simulated models can be tested in the real world.</p>
<p>Robot.Setting up a robot in a physics space requires careful consideration of several factors, such as the size and shape of the workspace, the type of tasks the robot needs to perform, the sensors required to perceive its surroundings, and the actuators necessary for motion control.</p>
<p>To set up the robot for our experiment, we followed several steps.Firstly, we designed a fixed area as our workspace, where the robot would perform its tasks.This was necessary to ensure that the robot would operate within a defined and controlled environment, which would help us to measure its performance accurately.</p>
<p>Next, we chose the robot arm as our primary training target.The robot arm is a crucial component that enables the robot to manipulate objects and perform tasks in the environment.By training the arm, we could help the robot develop the skills needed to perform its functions effectively.</p>
<p>Finally, we select the gripper as the end-effector for the robot.The gripper is a device that allows the robot to grasp and manipulate objects, while the endeffector is the component attached to the end of the robot arm and is responsible for performing specific tasks, such as picking up and moving objects.By carefully selecting the gripper and end-effector, we could ensure that the robot has the necessary tools to perform its tasks effectively and efficiently.</p>
<p>Sensor.We use a single RGB camera without a depth sensor as the sole sensor in the scene for the following reasons:</p>
<p>Simplicity: A monocular camera setup is often the most straightforward option, requiring fewer resources and less complex calibration than stereo or multicamera systems.This makes it suitable for smaller projects or prototyping purposes where complexity may not be desirable.</p>
<p>Portability: By relying exclusively on an RGB camera, the system becomes highly portable since no additional sensors need to be integrated into the setup.This allows for quick deployment across multiple platforms or environments without significant modifications.</p>
<p>Versatility: Despite being a basic configuration, an RGB camera can still capture valuable information for various perception tasks, including object detection, segmentation, and even Simultaneous Localization And Mapping (SLAM).These algorithms rely heavily on visual cues from images, making the RGB camera a sufficient input data source.</p>
<p>While other configurations might offer greater robustness or accuracy, an RGB camera remains a practical choice due to its ease of implementation, affordability, and broad applicability.As technology advances, these benefits continue to make it a viable option for many real-world scenarios.</p>
<p>Object.When considering the interaction between the robot's tool and the objects in the scene, we must account for their physical properties.Our framework will focus on three distinct categories: rigid bodies, soft bodies, and fluids.Each type presents unique challenges when attempting to manipulate or interact with the environment.</p>
<p>Rigid bodies: objects made up entirely of solid material, such as metal or plastic, are classified as rigid bodies.They maintain their shape under external forces and not deform unless subjected to extreme stress or impact.Manipulating rigid bodies requires careful consideration of their mass distribution, center of gravity, and friction coefficients.Tools designed for rigid bodies typically have high stiffness and low compliance to minimize deflection and ensure stable interactions.Examples of rigid bodies encountered in our scenario could include cups, coffee machines, or furniture pieces.</p>
<p>Soft bodies: Unlike rigid bodies, soft bodies exhibit some degree of elasticity or compressibility.Soft materials, such as foam, rubber, or fabric, behave differently than hard solids when subjected to external loads.Interacting with soft bodies demands special attention to contact mechanics, deformation modeling, and damping effects.</p>
<p>Fluids: Fluid would introduce new complexities into the equation due to its continuous nature and nonlinear behavior.Flow patterns, turbulence, and viscosity variations play crucial roles in understanding how fluids respond to pressure, temperature, or velocity changes.In addition, robots operating within fluid environments need to cope with issues related to buoyancy, drag, and other dynamic properties.</p>
<p>Simulation</p>
<p>In this section, we will describe the setup of the digital twin of the experiment in the simulation environment, which involves retaining simulated robot, sensor, and objects.Simulation Engine.To create a comprehensive simulated training program for robots, we choose Nvidia Omniverse [17] as the development platform for our Sim2Plan.Omniverse boasts cutting-edge simulation features that enable efficient and dependable representation of rigid bodies, soft bodies, articulated objects, and fluids.Furthermore, the platform supports seamless integration of Python scripts, enabling access to a vast array of open-source and third-party libraries.Another advantage of using Omniverse lies in its advanced ray tracing technology, allowing for breathtakingly realistic renderings.Digital Twin.A digital twin is a virtual replica of a physical entity created through computer simulations and sensor data [20].The purpose of establishing a digital twin is to provide a dynamic, interactive representation of the original object, allowing for better analysis, prediction, and optimization of its performance.First, we meticulously scrutinize the surroundings and concentrate on configuring the digital twin for the robot, camera, and task items (see Figure 2).Afterward, we utilize the digital twin to reenact diverse situations and gauge the system's execution for the experiment.</p>
<p>Message-Passing Pipeline: An Experiment</p>
<p>In this section, we test our Sim2Plan framework in a case study: make coffee by a coffee machine.Then, we introduce how we set up the digital twin, discuss implementing the message-passing pipeline, and demonstrate the results.</p>
<p>Preparation</p>
<p>To create a coffee-making experiment's digital twin, we first gather information about its physical properties, such as dimensions, materials used, and internal components.Then, we use computer-aided design (CAD) software to model the coffee machine digitally.Next, we simulate the behavior of the coffee machine using physics engines in Omniverse.These simulations consider gravity, friction, and other forces acting on the device during operation.</p>
<p>The experiment is captured from multiple angles to give a comprehensive setup overview (see Figure 3).The global view provides a broad perspective of the experimental arrangement, showcasing the interaction between the physical objects and the virtual representation.Meanwhile, the camera view offers a closer look at specific aspects of the experiment, highlighting details that would be applied for object detection.Lastly, the simulation view displays the digital twin of the investigation, allowing us to visualize the system's inner workings and plan the robot's behavior.Together, these views offer comprehensive insights into the experiment and enable more informed decision-making.</p>
<p>Scene understanding</p>
<p>Prior knowledge.We first utilize prior knowledge to gain a basic understanding of the scene layout.Prior knowledge refers to the fixed measures within the scene, such as the sizes ŝi of objects like the table, robot, coffee machine, cup, and coffee capsule.Since the camera, table, and robot positions remain unchanged throughout the experiment, we also measure their respective positions pi .By incorporating these measurements into our analysis, we enhance our ability to interpret the visual input from the camera view and obtain a clearer picture of the scene.
Ŝprior = {ŝ table , ŝrobot , ŝcoffee machine , ŝcup , ŝcapsule } (1) Pprior = {p table , pcamera , probot }(2)
Object detection.To better understand the scene layout, we employ the object detection module to identify objects present in the scene.Two state-of-the-art deep learning-based algorithms are used for this purpose: Open-Vocabulary Object Detection (OWL-ViT) [15] and Grounding DINO [13].Both methods take the image and text prompt as the inputs and leverage powerful vision transformers to detect and localize objects within the image frame, providing accurate bounding boxes and class labels for each detected instance.This information serves as crucial contextual awareness for subsequent tasks involving manipulation planning and execution.</p>
<p>Vision inference.Besides using the object detection module, we also gather relevant metadata about the camera to ensure optimal calibration and accuracy.Precisely, we determine the camera's focal length (f x , f y ), resolution, and principal point (c x , c y ), which are essential parameters for correcting lens distortion and projecting 3D points onto the 2D image plane.With all this information, we have a solid foundation for building a reliable and effective perception system tailored to our needs.Figure 4 demonstrates the steps involved in estimating the 3D position of an object from a 2D camera view.The process begins with a computer vision model Fig. 4. Obtaining object 3D position from 2D camera view.A visual explanation of the process used to estimate the 3D position of an object from a single 2D image captured by a camera.After the getting the bounding box (thus the object center) in the image space, we can project the point p0 to the ground (table) in 3D space.Since the size of the object ŝ0 and the position of the camera ĥ0 are known as prior knowledge, we can thus determine the the object's 3D position d0.</p>
<p>(left side), which uses the input image to predict the location of the object.Once the bounding box in the image space of the object is obtained, the next step (right side) utilizes geometric principles to calculate the 3D position of the object relative to the camera's field of view.Specifically, this requires knowledge of the camera's intrinsic parameters (e.g., focal length, principal point) and extrinsic parameters (e.g., rotation matrix, translation vector).These values allow us to project the bounding box onto the 3D world coordinate frame, resulting in the final estimated position of the object in 3D space.</p>
<p>Compared to Owl-Vit, the Ground-DINO model performs better in detecting objects such as cups, coffee machines, and coffee capsules.Leveraging vision inference techniques, the final prediction error for the 3D position of these objects can be controlled within 5 mm, enabling precise robot motion planning.This level of accuracy is sufficient for our real-world experiment.</p>
<p>Robot Motion Planning</p>
<p>After obtaining the scene information from the prior knowledge and the vision module, we use the Riemannian Motion Policy (RMP) [18] as the motion policy controller for the robot in its digital twin simulation.RMP has the following advantages.Firstly, RMP considers the geometry of the configuration space (cspace), which allows for smooth and efficient trajectories even when working close to singularities or other nonlinear regions.Secondly, RMP ensures that the resulting motions satisfy constraints on joint velocities, accelerations, and torques, making it suitable for robots with limited dynamic capabilities.Thirdly, RMP enables real-time optimization of motion plans based on sensor feedback, enabling adaptive behaviors that respond to environmental changes.Finally, RMP simplifies the design of complex motion sequences, reducing the computational burden required for generating feasible solutions.</p>
<p>After applying the RMP as the motion policy controller, we generate collisionfree paths for the robot using Rapidly-exploring Random Tree (RRT) [12].The RRT algorithm constructs a tree data structure that grows randomly in the high-dimensional configuration space until it reaches a solution.New nodes are sampled randomly at each iteration and connected to existing ones if they lie within a certain distance threshold.We then check whether newly added nodes violate collision constraints with environmental obstacles.If so, we reject them; otherwise, we add them to the tree.Once the tree spans the entire configuration space, we extract a valid path between the initial and final configurations.Our approach leverages the strengths of both RMP and RRT, allowing us to achieve safe and efficient motion planning under uncertainty.</p>
<p>Performance Validation</p>
<p>We assess the robot's performance by verifying its configuration and task completion against simulations.By comparing the actual robot configuration with the planned one, we ensure that the physical robot adheres to the desired path generated during simulation.Additionally, we validate the successful completion of subtasks by evaluating the task configuration in the real environment.These checks enable us to confirm that the robot operates correctly and achieves its intended goals, thereby improving overall reliability and effectiveness.</p>
<p>Robot Configuration.We continually compare the actual joint states j i (and gripper state g i ) with those predicted by the simulation.By doing so, we can ensure that the physical robot follows the intended instructions and performs according to expectations.The deviations or errors identified during this process can be addressed and corrected.If a large deviation is detected, we immediately stop the task execution to prevent the failure case from causing any safety concerns.Through this validation step, we can improve the reliability, effectiveness, and safety of the robotic system.</p>
<p>Task Completion.In addition, we continually compare the robot's performance in the real world with simulation by verifying the completion of subtasks.To ensure that each subtask, such as pick up the cup or place the cup, is executed correctly, we apply visual question answering (VQA) techniques [1] to verify the goal conditions from the camera's perspective.Specifically, we employ the newly released MiniGPT-4 [24] module to perform the VQA task in practice.This allows us to accurately assess the robot's ability to accomplish specific subtasks and identify discrepancies between the simulated and real-world environments from vision-based prediction.</p>
<p>Figure 5 illustrates verifying subtask completion using Visual Question Answering (VQA).To do this, we first compare the current robot configuration with the planned one after the planning and execution stages of a subtask.Next, we employ the MiniGPT-4 model to analyze the real-time visual input from the camera.Then, based on the specific context of each subtask, we formulate appropriate prompts for the VQA module and receive the corresponding answers from MiniGPT-4.Subsequently, we examine the keywords in these responses (such as yes or no) to confirm the successful completion of the subtask.This method provides an accurate and timely evaluation of the robot's progress, ensuring that it meets the requirements of each step along the way.</p>
<p>Results</p>
<p>Our experiments have demonstrated the effectiveness of our Sim2Plan framework for zero-shot robot motion planning (without training the robot in real space).In the first set of trials, where the positions of the coffee machine, coffee capsule, and cup are fixed, our framework achieved a remarkable 90% success rate out of 20 attempts.When the positions of the capsule and cup are randomized, our framework could still successfully pick up the items 83.3% and 76.6% of the time, respectively.Despite the increased difficulty due to randomization, our framework managed to complete 75% of a total of 20 trials.Overall, these results highlight the robustness and adaptability of our Sim2Plan framework in various environments and situations, making it a promising tool for robotic manipulation tasks.</p>
<p>Figure 6 presents several examples of subtasks performed by a robot from a side camera view in the real world and their corresponding screenshots taken from the simulation environment.Each block displays two images side by side, with the left one being the real-world snapshot and the right one showing the simulated scenario.These comparisons showcase how closely the simulation matches the real-world environment, demonstrating the validity of our proposed framework for zero-shot robot motion planning.</p>
<p>Furthermore, our proposed method also has practical benefits when applied to real-world settings.Since the robot was trained in a zero-shot manner during testing, it did not require any additional fine-tuning or retraining after being deployed to new environments.This means that the robot could quickly adapt to new situations without incurring significant delays or costs associated with retraining.By leveraging the motion planning from the digital twin, our method effectively reduces the amount of time required to train robots for specific tasks, making them more versatile and useful in a wide range of industries.</p>
<p>Conclusion</p>
<p>Sim2Real is an increasingly popular approach in robotics that involves training and developing machine learning models in a simulated environment and deploying them in the real world with minimal additional training.However, there is often a gap between the simulated environment and the real world, limiting the effectiveness of machine learning models trained in simulation.</p>
<p>In this work, we present Sim2Plan, a framework that utilizes a multi-stage message-passing pipeline that has been proposed to minimize the information gap between simulation and reality.Sim2Plan includes modules such as scene understanding, robot planning, and performance validation, and it is demonstrated through a case study involving a robot making a cup of coffee.The results of the experiment suggest that the proposed framework has the potential to drive robots for tasks that require precision and efficiency and could lead to the development of more versatile and adaptable robots that could increase productivity in various industries.</p>
<p>Fig. 2 .
2
Fig. 2. Digital twin components.An overview of the key elements involved in creating a comprehensive digital twin, including rigid bodies, lighting, fluids, cameras, articulation bodies, and soft bodies.</p>
<p>Fig. 3 .
3
Fig. 3. Different views of the experiment.The global view shows the overall setting of the experiment.The camera view shows what can be seen from the camera.And the simulation view shows the digital twin of the experiment.</p>
<p>Fig. 5 .
5
Fig. 5. Task completion verification by MiniGPT-4: the image from the camera view and the prompt are input, and we check the keyword in the response (answer) to verify the task is complete.</p>
<p>Fig. 6 .
6
Fig. 6.Real-world side camera view vs. simulated screenshot for subtask examples.Comparison between real-world images captured through a side camera and their simulated counterparts in the virtual environment.</p>
<p>Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>A survey of embodied ai: From simulators to research tasks. J Duan, S Yu, H L Tan, H Zhu, C Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 2022</p>
<p>Autonomous agents as embodied ai. S Franklin, Cybernetics &amp; Systems. 2861997</p>
<p>Joint mind modeling for explanation generation in complex human-robot collaborative tasks. X Gao, R Gong, Y Zhao, S Wang, T Shu, S.-C Zhu, 2020 29th IEEE international conference on robot and human interactive communication (RO-MAN). IEEE2020</p>
<p>R Gong, J Huang, Y Zhao, H Geng, X Gao, Q Wu, W Ai, Z Zhou, D Terzopoulos, S.-C Zhu, arXiv:2304.04321A benchmark for language-grounded task learning with continuous states in realistic 3d scenes. 2023arXiv preprint</p>
<p>Sim2real in robotics and automation: Applications and challenges. S Höfer, K Bekris, A Handa, J C Gamboa, M Mozifian, F Golemo, C Atkeson, D Fox, K Goldberg, J Leonard, IEEE transactions on automation science and engineering. 1822021</p>
<p>D Horváth, K Bocsi, G Erdős, Z Istenes, arXiv:2211.01048Sim2real grasp pose estimation for adaptive robotic applications. 2022arXiv preprint</p>
<p>How to train your robot with deep reinforcement learning: lessons we have learned. J Ibarz, J Tan, C Finn, M Kalakrishnan, P Pastor, S Levine, The International Journal of Robotics Research. 404-52021</p>
<p>Learning to act with affordance-aware multimodal neural slam. Z Jia, K Lin, Y Zhao, Q Gao, G Thattai, G S Sukhatme, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Robot motion planning. J.-C Latombe, 2012Springer Science &amp; Business Media124</p>
<p>Rapidly-exploring random trees: Progress and prospects: Steven m. lavalle, iowa state university, a james j. kuffner, jr., university of tokyo, tokyo, japan. Algorithmic and computational robotics. S M Lavalle, J J Kuffner, 2001</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, C Li, J Yang, H Su, J Zhu, arXiv:2303.054992023arXiv preprint</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Simple open-vocabulary object detection with vision transformers. M Minderer, A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy, A Mahendran, A Arnab, M Dehghani, Z Shen, arXiv:2205.062302022arXiv preprint</p>
<p>A machine learning approach for feature-sensitive motion planning. Algorithmic Foundations of Robotics VI. M Morales, L Tapia, R Pearce, S Rodriguez, N M Amato, 200517</p>
<p>N D Ratliff, J Issac, D Kappler, S Birchfield, D Fox, arXiv:1801.02854Riemannian motion policies. 2018arXiv preprint</p>
<p>J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.10332Sim-to-real: Learning agile locomotion for quadruped robots. 2018arXiv preprint</p>
<p>Digital twin in industry: State-of-the-art. F Tao, H Zhang, A Liu, A Y Nee, IEEE Transactions on industrial informatics. 1542018</p>
<p>Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. J Truong, S Chernova, D Batra, IEEE Robotics and Automation Letters. 622021</p>
<p>Robot control as a service-towards cloud-based motion planning and control for industrial robots. A Vick, V Vonásek, R Pěnička, J Krüger, 2015 10th International Workshop on Robot Motion and Control (RoMoCo). IEEE2015</p>
<p>Luminous: Indoor scene generation for embodied ai challenges. Y Zhao, K Lin, Z Jia, Q Gao, G Thattai, J Thomason, G S Sukhatme, arXiv:2111.055272021arXiv preprint</p>
<p>Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1615 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1615</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1615</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-28971016</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1706.01120v2.pdf" target="_blank">Evolving imputation strategies for missing data in classification problems with TPOT</a></p>
                <p><strong>Paper Abstract:</strong> Missing data has a ubiquitous presence in real-life applications of machine learning techniques. Imputation methods are algorithms conceived for restoring missing values in the data, based on other entries in the database. The choice of the imputation method has an influence on the performance of the machine learning technique, e.g., it influences the accuracy of the classification algorithm applied to the data. Therefore, selecting and applying the right imputation method is important and usually requires a substantial amount of human intervention. In this paper we propose the use of genetic programming techniques to search for the right combination of imputation and classification algorithms. We build our work on the recently introduced Python-based TPOT library, and incorporate a heterogeneous set of imputation algorithms as part of the machine learning pipeline search. We show that genetic programming can automatically find increasingly better pipelines that include the most effective combinations of imputation methods, feature pre-processing, and classifiers for a variety of classification problems with missing data.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1615.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1615.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary computation paradigm that evolves computer programs (commonly represented as trees) using population-based search with biologically inspired operators such as crossover and mutation to discover programs that solve a target task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming (general)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GP frames candidate solutions as executable program structures (trees). Evolution proceeds over a population using selection (often fitness-based), crossover (swapping subtrees between parents), and mutation (randomly altering nodes or terminals) to explore program-space; objectives can include task performance and program complexity (bloat control). The paper cites GP as the conceptual basis for evolving sklearn pipelines and for prior work that uses GP to create imputation functions or regression models for missing-data imputation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs / program trees (executable pipelines or symbolic expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Described generically: swapping subtrees between two parent program trees (standard GP subtree crossover). The paper refers to subtree/one-point style crossover on tree/list representations used by TPOT/DEAP but does not formalize a novel crossover variant for GP beyond standard practice.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Described generically: altering a node (function or terminal) or parameter in the program tree (e.g., replacing a terminal parameter with another feasible value); the paper notes mutation on terminals (parameters) as used in TPOT/DEAP but does not introduce a novel mutation operator.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>general program synthesis / automated program construction; here applied to machine-learning pipeline construction and imputation-function synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>The paper cites prior GP works and classical imputation methods as baselines in related work (e.g., regression/KNN/tree based imputation methods used in cited comparative studies).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GP is appropriate for evolving executable program-like solutions (sklearn pipelines or symbolic regressors for imputation). The paper uses GP to search the space of ML pipelines including imputation steps; however it does not provide quantitative measures of novelty, creativity, or diversity produced by GP operators nor formal analysis of how crossover/mutation affect novelty or executability beyond qualitative description.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving imputation strategies for missing data in classification problems with TPOT', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1615.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1615.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPOT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TPOT (Tree-based Pipeline Optimization Tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python tool that uses multi-objective genetic programming to automatically evolve machineâ€‘learning pipelines composed of scikit-learn primitives (preprocessors, feature selectors, classifiers).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TPOT (Tree-based Pipeline Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TPOT represents sklearn pipelines as strongly-typed tree/list individuals and applies a multi-objective GP (NSGA2) to evolve pipelines that trade off classification accuracy (evaluated with balanced 3-fold cross-validation) and pipeline complexity (shorter pipelines preferred). The implementation uses DEAP for evolutionary operators; each generation selects high-performing individuals via NSGA2, applies crossover and mutation to create offspring, and maintains a hall-of-fame Pareto front of solutions. In this paper TPOT is extended by adding imputation operators as primitives so the GP can include imputation steps within evolved pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs / sklearn pipeline trees (executable pipeline specifications)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>One-point / subtree crossover on the list/tree representation implemented by DEAP: select a node in each parent tree (nodes are pipeline primitives or sub-pipelines) and exchange the subtrees, preserving strong typing so offspring remain valid sklearn pipeline compositions. The paper gives an explicit example of crossing preprocessing nodes including their contained imputer primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Terminal/parameter mutation and subtree mutation via DEAP: the paper describes mutation that modifies a terminal node (e.g., a numeric hyperparameter) to another feasible value within its type/range, and also indicates mutation can replace nodes respecting the TPOT grammar so resulting pipelines remain type-correct.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Pipeline performance measured as classification accuracy (balanced 3-fold cross-validation on training partition, final evaluation on held-out DB-Test). The paper treats accuracy as the primary fitness metric; pipeline syntactic validity (strong typing) enforces executability but no separate numerical executability metric is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td>TPOT maintains a Pareto front (via NSGA2) trading off accuracy and pipeline length (complexity); the paper describes this Pareto-front usage qualitatively but does not present a quantitative frontier characterization between novelty and executability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Automated machine-learning pipeline synthesis for supervised classification on benchmark datasets (Randal Olson benchmark set); specifically extended here to pipelines that include imputation for missing-data problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>1) TPOT run on complete databases without imputation primitives (regular TPOT), 2) prior imputation methods and classifiers reported in related work. The paper also references comparisons in prior TPOT work vs random forest.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TPOT + added imputation primitives successfully evolves pipelines that include imputation methods and often ameliorate accuracy degradation due to injected missing data. Crossover and mutation are the active search operators (DEAP's one-point/subtree crossover and parameter mutation), and the NSGA2 multi-objective selection encourages both accuracy and short pipelines; the paper does not quantify operator-wise contributions (no ablation or operator-effect statistics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving imputation strategies for missing data in classification problems with TPOT', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1615.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1615.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DEAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DEAP (Distributed Evolutionary Algorithms in Python)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python library/framework that provides building blocks to implement evolutionary algorithms (including GP) and supplies standard genetic operators such as crossover and mutation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DEAP: Evolutionary Algorithms Made Easy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DEAP evolutionary framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DEAP supplies the GP implementation used by TPOT: representation of individuals as lists/trees, genetic operators (one-point/subtree crossover, mutations of nodes/terminals), selection schemes, and tools for strongly-typed GP. TPOT relies on DEAP to perform mating (crossover) and mutation operations over pipeline representations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs / list/tree representations of pipelines and parameter terminals</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>DEAP's one-point/subtree crossover over list/tree structures: select node(s) in each parent and swap the corresponding subtrees; TPOT leverages this to exchange preprocessing/imputer/classifier subtrees.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>DEAP supports node/terminal mutation; TPOT examples show terminal mutation replacing a hyperparameter value (e.g., changing MultinomialNB alpha from 1 to 10); other mutations can replace a primitive with another type-correct primitive.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Framework/tooling for implementing evolutionary algorithms; here used for evolving ML pipelines in TPOT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not applicable; DEAP is the underlying operator implementation used by TPOT rather than an evaluated baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DEAP implements the standard GP genetic operators used in TPOT (subtree/one-point crossover and parameter/terminal mutation) and enables strongly-typed GP to ensure offspring pipelines are syntactically/executable valid; the paper gives concrete examples of crossover and parameter mutation applied to pipeline trees.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving imputation strategies for missing data in classification problems with TPOT', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1615.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1615.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tran2015</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple imputation for missing data using genetic programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited prior work that applies genetic programming to perform multiple imputation for missing data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multiple imputations for missing data using genetic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-based multiple imputation (Tran et al. 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The referenced work uses genetic programming to create imputation models; it aims to perform multiple imputations for missing data by evolving symbolic models/regressors that predict missing values. The paper is cited in related work as an example of GP applied specifically to imputation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>symbolic regression programs / imputation functions (numeric predictors)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Evaluated by imputation quality and downstream classification accuracy (as reported in that cited work); the present paper does not re-report quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Missing-data imputation for supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in the cited study to standard imputation techniques (regression-based methods, tree-based methods) as reported by the original authors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that GP can be effective at designing imputation models and often outperforms some classical methods; the current paper references this work but does not reproduce its metrics nor analyze crossover/mutation effects in that specific study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving imputation strategies for missing data in classification problems with TPOT', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1615.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1615.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tran2016</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A genetic programming-based imputation method for classification with missing data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited conference paper that proposes a GP-based imputation approach and evaluates it with respect to classification accuracy and imputation error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A genetic programming-based imputation method for classification with missing data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-based imputation (Tran et al. 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The cited work extends GP to implement an imputation mechanism for missing values, evaluating both downstream classification accuracy and the error between imputed and real values; the present paper lists it among prior GP-for-imputation contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>symbolic regressors / imputation programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Reported in the original work as imputation error and classification accuracy, but not re-stated here.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Missing-data imputation for classification</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with other imputation techniques (details in the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as prior art demonstrating GP can design competitive imputation methods; the present article does not provide further operator-level details or novelty/diversity metrics from that study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving imputation strategies for missing data in classification problems with TPOT', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1615.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1615.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeResende2016</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time series imputation using genetic programming and Lagrange interpolation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study that first imputes time-series with interpolation and then evolves a regression function using GP to refine imputations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Time series imputation using genetic programming and Lagrange interpolation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-based time-series imputation (De Resende et al. 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The referenced method imputes missing time-series values using Lagrange interpolation then applies genetic programming to evolve regression functions to improve imputation; performance is compared to mean, cluster and KNN imputation in that study.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>symbolic regression programs for time-series imputation</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Evaluated by downstream imputation accuracy and comparative classification/regression performance in the cited work; not re-reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Time-series imputation</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Lagrange interpolation alone, mean, cluster, and KNN imputation in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as another example where GP-based imputation often yields superior results; the present paper uses these citations as motivation but does not re-evaluate operator-level behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving imputation strategies for missing data in classification problems with TPOT', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multiple imputations for missing data using genetic programming. <em>(Rating: 2)</em></li>
                <li>A genetic programming-based imputation method for classification with missing data. <em>(Rating: 2)</em></li>
                <li>Time series imputation using genetic programming and Lagrange interpolation. <em>(Rating: 2)</em></li>
                <li>TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning. <em>(Rating: 2)</em></li>
                <li>DEAP: Evolutionary Algorithms Made Easy. <em>(Rating: 1)</em></li>
                <li>Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1615",
    "paper_id": "paper-28971016",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "GP",
            "name_full": "Genetic Programming",
            "brief_description": "An evolutionary computation paradigm that evolves computer programs (commonly represented as trees) using population-based search with biologically inspired operators such as crossover and mutation to discover programs that solve a target task.",
            "citation_title": "Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems.",
            "mention_or_use": "mention",
            "system_name": "Genetic Programming (general)",
            "system_description": "GP frames candidate solutions as executable program structures (trees). Evolution proceeds over a population using selection (often fitness-based), crossover (swapping subtrees between parents), and mutation (randomly altering nodes or terminals) to explore program-space; objectives can include task performance and program complexity (bloat control). The paper cites GP as the conceptual basis for evolving sklearn pipelines and for prior work that uses GP to create imputation functions or regression models for missing-data imputation.",
            "input_type": "programs / program trees (executable pipelines or symbolic expressions)",
            "crossover_operation": "Described generically: swapping subtrees between two parent program trees (standard GP subtree crossover). The paper refers to subtree/one-point style crossover on tree/list representations used by TPOT/DEAP but does not formalize a novel crossover variant for GP beyond standard practice.",
            "mutation_operation": "Described generically: altering a node (function or terminal) or parameter in the program tree (e.g., replacing a terminal parameter with another feasible value); the paper notes mutation on terminals (parameters) as used in TPOT/DEAP but does not introduce a novel mutation operator.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "general program synthesis / automated program construction; here applied to machine-learning pipeline construction and imputation-function synthesis",
            "comparison_baseline": "The paper cites prior GP works and classical imputation methods as baselines in related work (e.g., regression/KNN/tree based imputation methods used in cited comparative studies).",
            "key_findings": "GP is appropriate for evolving executable program-like solutions (sklearn pipelines or symbolic regressors for imputation). The paper uses GP to search the space of ML pipelines including imputation steps; however it does not provide quantitative measures of novelty, creativity, or diversity produced by GP operators nor formal analysis of how crossover/mutation affect novelty or executability beyond qualitative description.",
            "uuid": "e1615.0",
            "source_info": {
                "paper_title": "Evolving imputation strategies for missing data in classification problems with TPOT",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "TPOT",
            "name_full": "TPOT (Tree-based Pipeline Optimization Tool)",
            "brief_description": "A Python tool that uses multi-objective genetic programming to automatically evolve machineâ€‘learning pipelines composed of scikit-learn primitives (preprocessors, feature selectors, classifiers).",
            "citation_title": "TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning.",
            "mention_or_use": "use",
            "system_name": "TPOT (Tree-based Pipeline Optimization)",
            "system_description": "TPOT represents sklearn pipelines as strongly-typed tree/list individuals and applies a multi-objective GP (NSGA2) to evolve pipelines that trade off classification accuracy (evaluated with balanced 3-fold cross-validation) and pipeline complexity (shorter pipelines preferred). The implementation uses DEAP for evolutionary operators; each generation selects high-performing individuals via NSGA2, applies crossover and mutation to create offspring, and maintains a hall-of-fame Pareto front of solutions. In this paper TPOT is extended by adding imputation operators as primitives so the GP can include imputation steps within evolved pipelines.",
            "input_type": "programs / sklearn pipeline trees (executable pipeline specifications)",
            "crossover_operation": "One-point / subtree crossover on the list/tree representation implemented by DEAP: select a node in each parent tree (nodes are pipeline primitives or sub-pipelines) and exchange the subtrees, preserving strong typing so offspring remain valid sklearn pipeline compositions. The paper gives an explicit example of crossing preprocessing nodes including their contained imputer primitives.",
            "mutation_operation": "Terminal/parameter mutation and subtree mutation via DEAP: the paper describes mutation that modifies a terminal node (e.g., a numeric hyperparameter) to another feasible value within its type/range, and also indicates mutation can replace nodes respecting the TPOT grammar so resulting pipelines remain type-correct.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Pipeline performance measured as classification accuracy (balanced 3-fold cross-validation on training partition, final evaluation on held-out DB-Test). The paper treats accuracy as the primary fitness metric; pipeline syntactic validity (strong typing) enforces executability but no separate numerical executability metric is reported.",
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": "TPOT maintains a Pareto front (via NSGA2) trading off accuracy and pipeline length (complexity); the paper describes this Pareto-front usage qualitatively but does not present a quantitative frontier characterization between novelty and executability.",
            "benchmark_or_domain": "Automated machine-learning pipeline synthesis for supervised classification on benchmark datasets (Randal Olson benchmark set); specifically extended here to pipelines that include imputation for missing-data problems.",
            "comparison_baseline": "1) TPOT run on complete databases without imputation primitives (regular TPOT), 2) prior imputation methods and classifiers reported in related work. The paper also references comparisons in prior TPOT work vs random forest.",
            "key_findings": "TPOT + added imputation primitives successfully evolves pipelines that include imputation methods and often ameliorate accuracy degradation due to injected missing data. Crossover and mutation are the active search operators (DEAP's one-point/subtree crossover and parameter mutation), and the NSGA2 multi-objective selection encourages both accuracy and short pipelines; the paper does not quantify operator-wise contributions (no ablation or operator-effect statistics).",
            "uuid": "e1615.1",
            "source_info": {
                "paper_title": "Evolving imputation strategies for missing data in classification problems with TPOT",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "DEAP",
            "name_full": "DEAP (Distributed Evolutionary Algorithms in Python)",
            "brief_description": "A Python library/framework that provides building blocks to implement evolutionary algorithms (including GP) and supplies standard genetic operators such as crossover and mutation.",
            "citation_title": "DEAP: Evolutionary Algorithms Made Easy.",
            "mention_or_use": "use",
            "system_name": "DEAP evolutionary framework",
            "system_description": "DEAP supplies the GP implementation used by TPOT: representation of individuals as lists/trees, genetic operators (one-point/subtree crossover, mutations of nodes/terminals), selection schemes, and tools for strongly-typed GP. TPOT relies on DEAP to perform mating (crossover) and mutation operations over pipeline representations.",
            "input_type": "programs / list/tree representations of pipelines and parameter terminals",
            "crossover_operation": "DEAP's one-point/subtree crossover over list/tree structures: select node(s) in each parent and swap the corresponding subtrees; TPOT leverages this to exchange preprocessing/imputer/classifier subtrees.",
            "mutation_operation": "DEAP supports node/terminal mutation; TPOT examples show terminal mutation replacing a hyperparameter value (e.g., changing MultinomialNB alpha from 1 to 10); other mutations can replace a primitive with another type-correct primitive.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Framework/tooling for implementing evolutionary algorithms; here used for evolving ML pipelines in TPOT.",
            "comparison_baseline": "Not applicable; DEAP is the underlying operator implementation used by TPOT rather than an evaluated baseline.",
            "key_findings": "DEAP implements the standard GP genetic operators used in TPOT (subtree/one-point crossover and parameter/terminal mutation) and enables strongly-typed GP to ensure offspring pipelines are syntactically/executable valid; the paper gives concrete examples of crossover and parameter mutation applied to pipeline trees.",
            "uuid": "e1615.2",
            "source_info": {
                "paper_title": "Evolving imputation strategies for missing data in classification problems with TPOT",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "Tran2015",
            "name_full": "Multiple imputation for missing data using genetic programming",
            "brief_description": "A cited prior work that applies genetic programming to perform multiple imputation for missing data.",
            "citation_title": "Multiple imputations for missing data using genetic programming.",
            "mention_or_use": "mention",
            "system_name": "GP-based multiple imputation (Tran et al. 2015)",
            "system_description": "The referenced work uses genetic programming to create imputation models; it aims to perform multiple imputations for missing data by evolving symbolic models/regressors that predict missing values. The paper is cited in related work as an example of GP applied specifically to imputation.",
            "input_type": "symbolic regression programs / imputation functions (numeric predictors)",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Evaluated by imputation quality and downstream classification accuracy (as reported in that cited work); the present paper does not re-report quantitative results.",
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Missing-data imputation for supervised learning",
            "comparison_baseline": "Compared in the cited study to standard imputation techniques (regression-based methods, tree-based methods) as reported by the original authors.",
            "key_findings": "Cited as evidence that GP can be effective at designing imputation models and often outperforms some classical methods; the current paper references this work but does not reproduce its metrics nor analyze crossover/mutation effects in that specific study.",
            "uuid": "e1615.3",
            "source_info": {
                "paper_title": "Evolving imputation strategies for missing data in classification problems with TPOT",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "Tran2016",
            "name_full": "A genetic programming-based imputation method for classification with missing data",
            "brief_description": "A cited conference paper that proposes a GP-based imputation approach and evaluates it with respect to classification accuracy and imputation error.",
            "citation_title": "A genetic programming-based imputation method for classification with missing data.",
            "mention_or_use": "mention",
            "system_name": "GP-based imputation (Tran et al. 2016)",
            "system_description": "The cited work extends GP to implement an imputation mechanism for missing values, evaluating both downstream classification accuracy and the error between imputed and real values; the present paper lists it among prior GP-for-imputation contributions.",
            "input_type": "symbolic regressors / imputation programs",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Reported in the original work as imputation error and classification accuracy, but not re-stated here.",
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Missing-data imputation for classification",
            "comparison_baseline": "Compared with other imputation techniques (details in the cited paper).",
            "key_findings": "Referenced as prior art demonstrating GP can design competitive imputation methods; the present article does not provide further operator-level details or novelty/diversity metrics from that study.",
            "uuid": "e1615.4",
            "source_info": {
                "paper_title": "Evolving imputation strategies for missing data in classification problems with TPOT",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "DeResende2016",
            "name_full": "Time series imputation using genetic programming and Lagrange interpolation",
            "brief_description": "A cited study that first imputes time-series with interpolation and then evolves a regression function using GP to refine imputations.",
            "citation_title": "Time series imputation using genetic programming and Lagrange interpolation.",
            "mention_or_use": "mention",
            "system_name": "GP-based time-series imputation (De Resende et al. 2016)",
            "system_description": "The referenced method imputes missing time-series values using Lagrange interpolation then applies genetic programming to evolve regression functions to improve imputation; performance is compared to mean, cluster and KNN imputation in that study.",
            "input_type": "symbolic regression programs for time-series imputation",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Evaluated by downstream imputation accuracy and comparative classification/regression performance in the cited work; not re-reported here.",
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Time-series imputation",
            "comparison_baseline": "Compared against Lagrange interpolation alone, mean, cluster, and KNN imputation in the cited paper.",
            "key_findings": "Cited as another example where GP-based imputation often yields superior results; the present paper uses these citations as motivation but does not re-evaluate operator-level behaviors.",
            "uuid": "e1615.5",
            "source_info": {
                "paper_title": "Evolving imputation strategies for missing data in classification problems with TPOT",
                "publication_date_yy_mm": "2017-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multiple imputations for missing data using genetic programming.",
            "rating": 2,
            "sanitized_title": "multiple_imputations_for_missing_data_using_genetic_programming"
        },
        {
            "paper_title": "A genetic programming-based imputation method for classification with missing data.",
            "rating": 2,
            "sanitized_title": "a_genetic_programmingbased_imputation_method_for_classification_with_missing_data"
        },
        {
            "paper_title": "Time series imputation using genetic programming and Lagrange interpolation.",
            "rating": 2,
            "sanitized_title": "time_series_imputation_using_genetic_programming_and_lagrange_interpolation"
        },
        {
            "paper_title": "TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning.",
            "rating": 2,
            "sanitized_title": "tpot_a_treebased_pipeline_optimization_tool_for_automating_machine_learning"
        },
        {
            "paper_title": "DEAP: Evolutionary Algorithms Made Easy.",
            "rating": 1,
            "sanitized_title": "deap_evolutionary_algorithms_made_easy"
        },
        {
            "paper_title": "Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems.",
            "rating": 1,
            "sanitized_title": "genetic_programming_a_paradigm_for_genetically_breeding_populations_of_computer_programs_to_solve_problems"
        }
    ],
    "cost": 0.0130885,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evolving imputation strategies for missing data in classification problems with TPOT
14 Aug 2017</p>
<p>Unai Garciarena 
Roberto Santana 
Alexander Mendiburu </p>
<p>Intelligent Systems Group Univ. of the Basque Country (UPV/EHU
San SebastianSpain</p>
<p>Intelligent Systems Group Univ. of the Basque Country (UPV/EHU)
San SebastianSpain</p>
<p>Intelligent Systems Group Univ. of the Basque Country (UPV/EHU)
San SebastianSpain</p>
<p>Evolving imputation strategies for missing data in classification problems with TPOT
14 Aug 2017genetic programmingmissing dataimputation methodssupervised classification
Missing data has a ubiquitous presence in real-life applications of machine learning techniques. Imputation methods are algorithms conceived for restoring missing values in the data, based on other entries in the database. The choice of the imputation method has an influence on the performance of the machine learning technique, e.g., it influences the accuracy of the classification algorithm applied to the data. Therefore, selecting and applying the right imputation method is important and usually requires a substantial amount of human intervention. In this paper we propose the use of genetic programming techniques to search for the right combination of imputation and classification algorithms. We build our work on the recently introduced Python-based TPOT library, and incorporate a heterogeneous set of imputation algorithms as part of the machine learning pipeline search. We show that genetic programming can automatically find increasingly better pipelines that include the most effective combinations of imputation methods, feature preprocessing, and classifiers for a variety of classification problems with missing data.keywords: genetic programming, missing data, imputation methods, supervised classification Previous studies have shown that the characteristics of missing data and those of the selected imputer are strongly related to information quality[6,21], as should be expected. However, no generic</p>
<p>Introduction</p>
<p>Missing data can cause considerable losses in data quality, and therefore, information, in any kind of database (DB) in, potentially, any domain. This issue consists of a DB which does not contain all the information it is supposed to. There may be many reasons for this phenomenon, e.g., a human operator that failed to record a value, or a machine that was unable to transmit the values it recorded. Additionally, information that is not expected to be present can be identified as missing data if each case is not properly studied. Among others, this reason presents the study of missing data as a key to successfully prepare the data a given machine learning algorithm is fed with. Imputation [28,19] is one of the most used strategies for the missing data problem. These algorithms attempt to guess what the original, non-present, values were in the first place. To impute the missing data, the information available in the rest of the database (both in the defective observations and the complete ones) is used.</p>
<p>procedure providing a potential final user with the necessary knowledge to straightforwardly recognize a good imputer for a given dataset has been proposed. Although the use of imputation methods is an essential requirement for the effective application of machine learning techniques, the choice of the right imputer is not trivial. The right choice of the imputation method will almost certainly depend on the type of classification problem at hand, or the distribution of the missing data.</p>
<p>In this paper we propose the use of a genetic programming (GP) [17,18,4] strategy to search for the right combination of imputation and classification algorithms in such a way that the resulting pipeline can effectively deal with missing data. We build our work on the recently introduced Python-based TPOT library [25], which is able to evolve machine learning pipelines which consist of preprocessing, feature selection, and classification methods. While the preprocessing methods included in TPOT are diverse, imputation methods are not explicitly included.</p>
<p>We incorporate a heterogeneous set of imputation algorithms as part of the machine learning pipeline search. We inject missing data into a set of representative databases with a varying number of cases and variables, and show that the GP approach is able to consistently improve the classification accuracy along generations. Moreover, we investigate the relationship between the imputation methods and classification algorithms that appear in the best evolved pipelines.</p>
<p>The article is structured as follows: In the next section we introduce the missing data problem and discuss imputation approaches. Section 3 presents our contribution, including a description of the TPOT library, the imputation methods we investigate and the algorithm used to simulate missing data in the complete databases. Section 4 introduces the experimental framework used to evaluate our proposal and presents and discusses the numerical results. The main contributions of the paper are summarized in Section 5, which also includes some lines for future research.</p>
<p>The missing data problem and the imputation approach</p>
<p>Missing data can be found in almost any kind of imaginable information collection. Different sources of missing data can result in different missingness patterns in the data. Most research works [29,21] usually distinguish three divisions when it comes to classifying the arrangements described by missing values:</p>
<p>â€¢ Missing Completely At Random (MCAR): When the probability of losing a value is unrelated to any information, i.e., when it is random, the missing data is cataloged as MCAR.</p>
<p>â€¢ Missing At Random (MAR): the MAR tag is used when a pattern can be identified, this is, we can find a common factor in all (or, at least a considerable amount of) the observations with missing values.</p>
<p>â€¢ Missing Not At Random (MNAR): This missing data type is similar to MAR. However, in this case the values causing others to be missing are not known. This type is easily confused with MCAR.</p>
<p>The identification of the missing data pattern has proven to be key in data preprocessing. The distinctive characteristics in MCAR have made it a unique benchmark for imputation testing, since it has proven to show more contrast between imputation methods than other configurations [13]. However, features in current databases tend to be interrelated. The result of this is that, even if missingness is caused by a reason usually linked to MCAR, the missing data class is relatively close to MAR.</p>
<p>Once we have correctly identified which one of the previous missing data types is the cause, one solution from the multiple ones available can be chosen. Depending on the characteristics of the database, deleting incomplete observations could be an acceptable answer to the problem. An example of this is when there is very little defectiveness. However, in other cases, the data loss is not worth it. This is where imputation methods arise.</p>
<p>Imputation methods</p>
<p>Since its formalization [27], imputation has seen its status grow, and nowadays it is considered an essential preprocessing method when operating with incomplete data [6,21]. A common scenario for the application of imputation is when the data is intended to undergo a machine learning process, whose capability to deal with data with absent entries is commonly limited.</p>
<p>Imputation methods are usually classified in two groups. The simple and the complex ones. Simple methods correspond to straightforward operations, such as assigning the mean or median of each variable to all its lost items. When the percentage of lost entries is low, a simple method could be a good choice in terms of information loss avoidance and time consumption.</p>
<p>However, simple methods may not be a valid strategy when the amount of missing data for any of the variables rises. Computing the mean of a feature in such cases would cause a significant reduction in the variance of the distribution of the data in the corresponding variable, which would result in a feature that contributes almost nothing when building a classification model. For this reason, more complex algorithms have been developed, with the main goal of producing more accurate results, taking into account the closeness to the original value. These more sophisticated methods are usually based on multiple imputation [28], model creation [14], or similarity computation [3] 2.2 Imputation methods for classification Several works have studied the effect of imputation over a (mostly supervised) classification [21,6]. These works usually select one or more databases to which missing data is introduced. Then the data is imputed, and used for classification. Finally, the imputation quality is measured based on the classification accuracy, computed via cross-validation. The results of these works usually suggest that complex imputation outperforms simpler strategies. For example, the study performed in [21] investigated the relations between imputation methods and supervised classification algorithms. This work discerns three types of classifiers in rule inducting (e.g., decision trees), approximate models (SVM, MLP), and lazy (KNN) and applies a total of 23 classifiers on previously imputed databases treated by 14 imputation methods, from which 4 could be considered as simple strategies and the other 10 use a considerable amount of information and computational effort. The authors created a ranking based on the scores obtained from each database imputed with different methods. For the rule induction and approximate model classifiers (which include 19 out of 23 classifiers), any of the simple imputers failed to obtain a better rank than fourth, while for the lazy learning test obtained the best results. In [20], a similar experiment was performed, but with a reduced set of classifiers (3). In this case, the experiment was performed over 22 databases, and, for the 3 Ã— 22 = 66 combinations, in only in 12 of them the simple methods obtained a better result, about 18%.</p>
<p>An alternative approach to estimate quality of the imputer is to compare the imputed matrix to the original one using some distance measure [32,15]. Generally, the normalized root mean square error is used. However, since the goal of this work is based on the classification algorithm performance, these approaches are out of scope.</p>
<p>Genetic programming</p>
<p>Genetic Programming (GP) was introduced in the early 90's [17] as a paradigm of automatically "creating" computing solutions for any class of problems. Instead of having a human-developed program, this concept proposes to seek for a solution in a search space where all possible programs can be found. This idea implements an evolutionary algorithm [33] to perform such exploration, considering evaluable programs as individuals (shaped as trees), and contemplating typical genetic operators such as crossover and mutation. GP has proven to be a good alternative to human knowledge when it comes to the automatic generation of programs, as it has sometimes produced programs as good as human-made versions, or has even improved classical program developments and implementation processes [16].</p>
<p>Related Work</p>
<p>There have been previous approaches that apply GP to design imputation methods. For example, in [31], a regression imputation method is developed using GP. They point out the suitability of GP to design a symbolic regression procedure since it is a non-parametric regressive model, which seeks for both an optimal structure and parameters. The method was compared to other three techniques based on regression and two tree-based algorithms, and tested regarding the accuracy that different classifiers obtained after being trained with data treated by the imputation methods. The used classi- The results showed that the proposed method consistently offered significantly better performance compared to the selected benchmark.</p>
<p>In [30], the same authors proposed a similar method, with a more explicit multiple imputation implementation, and this time they used the accuracy and the error between the imputed and the real values as a metric to evaluate the imputation method. Again, the proposed method regularly beat the other methods.</p>
<p>[9] focuses its efforts on time-series data. The authors exploit this characteristic by firstly imputing the dataset with Lagrange interpolation, and then perform a regression function computation with a genetic programming approach. The results of the method are compared to other IMs, such as mean, cluster, and KNN imputation, and only KNN seemed to obtain better results in some occasions. Anyhow, the proposed method offered the best results more frequently.</p>
<p>Notice that the approach we propose in this paper is different. We evolve combinations of different steps involved in the classification process. We improve these combinations by adding the possibility to contain imputation methods within them.</p>
<p>Evolving automatic ML pipelines for databases with missing data</p>
<p>In this paper we address the question of how to design a machine learning solution, involving different modules, for classification problems with missing data. In these problems, a database of observations (or cases), each of which corresponds to a row in the database, is given. Each observation has an associated class. The general goal is to learn to predict the class solely using the information contained in the observations. Generally, classifiers require the observations to be complete. Therefore, a solution to the missing data problem would include the choice of an imputation method. Furthermore, realistic machine learning pipelines also add steps in which the data is preprocessed in other ways (e.g., PCA and variable normalization), or subsets of features are selected from the original data.</p>
<p>There are several possible combinations of imputation, preprocessing, feature selection and classification methods. For a given database, trying all possible combinations to identify the one that maximizes a given criterion, e.g., the classification accuracy, is often unfeasible. Search-based methods are a feasible alternative in this scenario. Therefore, we focus on automatically generating a machine learning pipeline for maximizing the classification accuracy in problems with missing data. Figure 1 shows a diagram of the different components involved in our approach to this problem. Three of these components are already included in the TPOT implementation (Classification method, feature selection, and preprocessors). Our contribution lies in the design and implementation of the fourth one (Imputation methods).</p>
<p>We organize the presentation of our proposal in three steps: First, we describe the TPOT library, the work in which our approach is inspired. Secondly, we explain the characteristics of the imputation methods we implemented, which are the building blocks of our approach. Thirdly, we introduce the method conceived to inject missing data into the original databases.</p>
<p>TPOT, a platform for automatic parameter selection</p>
<p>TPOT (Tree-based Pipeline Optimization) [25] progressively evolves machine learning pipelines for regression and classification problems. The program implements a multi-objective GP algorithm [5,7] where the programs correspond to pipelines of the popular scikit-learn (sklearn) [26] library.</p>
<p>TPOT defines all the machine learning techniques it uses as GP primitives, and organizes them in a tree structure. These constructions are the elements evolved by the underlying genetic component in the core of the project.</p>
<p>The other component of the TPOT project is the DEAP (Distributed Evolutionary Algorithms in Python) library [12]. This package endows TPOT with the method to implement genetic programming over the sklearn-based pipelines. DEAP is basically a framework that simplifies the task of creating fast evolutionary prototypes, which perfectly suits TPOT. The contribution of DEAP on this specific task of serving TPOT is explained in the following paragraphs.</p>
<p>The overall workflow is not far from the standard GP procedure. As an initial step, a population is randomly initialized, where each individual is a tree-shaped sklearn pipeline. Next, TPOT evaluates every tree regarding the accuracy that the components inside the tree managed to obtain, computed via balanced 3-fold cross-validation. Then, the best scoring individuals are selected (via the NSGA2 strategy [10]) to take part in the creation of the next population. For this step, the top trees are susceptible to being subjected to crossover and mutation. Once this sub-process is finished, the whole method is rerun. The algorithm stops when the number of specified generations is reached. Every generation, the algorithm updates a "hall of fame", a Pareto front comprising all solutions in every generation.</p>
<p>As previously stated, GP, as implemented in TPOT, pursues two objectives rather than simply maximizing the classification accuracy. TPOT also attempts to keep trees as simple as possible. This constraint addresses the necessary issue of regulating the complexity of the final solution, resulting in a potentially considerable computation-time gain. Without this restriction, the algorithm could generate intractable pipelines, composed by heaps of sklearn methods. Obviously, this multi-objective feature generates a Pareto front as an output, from which TPOT selects the individual with the highest accuracy. Therefore, even though the second objective has had a relevant role during the TPOT general run, it remains unused in the last selection.</p>
<p>Despite being recently introduced software, TPOT has been successfully applied to different problems [25,24,11]. In [25], it was tested using artificial data sets and the (real-world) CGEMS Prostate Cancer Data Set. In [24], it was compared to a random forest classifier over 150 real databases from different sources. TPOT obtained significantly better results than the random forest classifier in 21 of them, and significantly worse in 4. Finally, for 125, no significant differences were found.</p>
<p>The following code shows an example product of TPOT, a sklearn representation for a program that includes an imputation method, a preprocessing step and a classifier with different parameters: This is a strongly typed string, since the parameters are individually and explicitly linked to specific parameters of specific functions. This is necessary, for example, to introduce parameters of the required types, and avoid entering a Boolean where an array is expected. However, this notation is not practical, and it is therefore converted to a more tractable notation to undergo processes such as genetic operations or conversion to sklearn pipeline: As it can be seen the trees are coded as lists of lists (brackets denote lists in python syntax). In this case, "input matrix" would be our data, which would firstly be treated by the "MFImputer". Then "MaxAbsScaler" would be applied to the imputed matrix, with "True" as the argument for the "copy" parameter. Finally, the data would be used for classification, via "MultinomialNB", with "1", and "True", as parameters for the alpha, and prior class probability learning (fit prior), respectively.</p>
<p>DEAP straightforwardly applies simple operators, such as one-point-crossover, over this kind of lists. For instance, if the algorithm decided to mate this input with another similar one, it would simply select a node and cross it with another node with similar characteristics from another tree.</p>
<p>For this instance, we will be crossing the preprocessing nodes, with the imputer within it; "MaxAb-sScaler" (with "MFImputer") and "FastICA" (with "MICEImputer"): </p>
<p>Implementing imputation methods</p>
<p>Instead of exhaustively evaluating combinations of imputation methods and classifiers over databases with missing values, we exploit TPOT to search for the most effective combination. We recall that there is an important difference between preprocessing methods as implemented in TPOT, and imputation methods. The former are conceived for transforming complete data. Imputation methods are oriented to replace the missing data and, when used in the wrong way, they can introduce important biases in the information. In these cases, the results of the classification process are not only influenced by the quality of the classifier but also by the adequacy of the imputation method. This highlights the relevance, and also the difficulty, of incorporating the imputation analysis to the automation of the ML pipeline.</p>
<p>In our implementation, we evaluate six imputation methods. Firstly, four simple algorithms; mean, median and most frequent which are self explaining, and are already implemented in sklearn, and max. imputer, which computes the maximum value of a variable from the available data and imputes it on all the missing parts. In addition, we incorporate two complex imputation methods that have been extensively applied as a previous step for classification.</p>
<p>MICE [8] (Multiple Imputation based on Chained Equations) clearly differentiates two ideas on its operations. Firstly, it performs a simple imputation (e.g., mean) with the objective of re-imputing each missing value with a regression method until a certain threshold is met. In other words, variables are imputed with a straightforward method, to later use the complete DB to re-impute each originally missing values via regression. Multiple Imputation is the other concept influencing the design of the method, as the previously explained re-imputation procedure is performed m times, resulting in so many outcomes. A final product is computed from all the primary imputations. This method has proven to be top class in missing data restoration when that information is going to be fed to a machine learning algorithm, regarding classification accuracy [2]. However, the computational cost could be considered an enormous drawback when time is a key aspect.</p>
<p>The EM (Expectation-Maximization) method [14] is also the consequence of the combination between a base method and multiple imputation. To start with, an EM approach is used to compute various predefined statistical parameters from the known data, which is assumed to have a normal distribution. Similarly to the previous method, the originally missing values are re-imputed with models built from mentioned parameters, iteratively. Once again, this algorithm is operated multiple times, and the results are combined similarly. This method also produces relatively good imputation, even if it is usually outperformed by MICE. Nevertheless, the elapsed time is considerably smaller compared to MICE, which results in a much better imputation quality/elapsed time ratio.</p>
<p>These two complex methods have been implemented using two R packages, each one respectively described in the two aforementioned references to the algorithms.</p>
<p>Injecting missing data in the database</p>
<p>Previous studies [13] have shown that MCAR missing data type introduction maximizes the contrast produced by posterior imputation and supervised classification. For this reason, this missing data configuration is the most suitable one to measure the imputation performance in this case. For this instance, we randomly selected certain positions in the complete database, and changed their values to "NaN". Halting was conditioned to a certain percentage of missingness, over the total quantity of values in the database.</p>
<p>Experiments</p>
<p>In this section, we validate the results of our approach. The goals of our experiments are twofold. First, we would like to investigate whether the evolved pipelines are able to produce high classification accuracies even when missing data is introduced. To this end, we compare the accuracies obtained by our approach to those produced by TPOT over the complete databases. Our expectation is that missing data will produce a deleterious effect in the accuracy. Still, we expect this effect to be ameliorated by the use of imputation methods. We will assume that deleting observations with missing data is not a general and realistic approach, since, even if a dataset has a small percentage of MD and the incomplete cases are well distributed among the cases, removing observations can produce significant decrease in the number of available overall information.</p>
<p>The second goal of our experiments is trying to shed some light over the problem of discovering relations between imputation and classification. We analyze the best solutions produced by the introduced approach to determine whether there exist dependencies between imputation and classification methods at the time of producing highly accurate pipelines. This question is relevant in order to produce general recommendations at the time of combining imputers and classifiers.</p>
<p>This section is organized as follows. The next subsection presents a global view of the experimental design. Subsequently, the benchmark of classification problems and the parameters of the GP approach are presented. The two closing subsections respectively address the above-mentioned research questions, presenting the numerical results of the algorithms and discussing them.</p>
<p>Experimental design</p>
<p>The general work-flow of the experiment is described in the following steps:</p>
<ol>
<li>
<p>Missing data is introduced into the selected database. 2. The incomplete DB is split into two databases (DB-Train and DB-Test). We use a random and unique partition of the dataset through all experimentation.</p>
</li>
<li>
<p>DB-Train is used to evolve a pipeline using TPOT.</p>
</li>
<li>
<p>The best pipeline among those evolved by TPOT is tested using DB-Test. This is performed in order to obtain an accuracy, which is regarded as the score of the pipeline. We use accuracy to remain consistent with both, the TPOT metric system, and the metric used in [23].</p>
</li>
<li>
<p>The best pipeline is stored along with its corresponding score, in addition to the other pipelines and accuracies generated from the rest of the benchmark.</p>
</li>
</ol>
<p>We notice that using a unique partition of the database could eventually bias the results of the comparison to the specific characteristics of the training set. Varying the train/test partition could be used to increase the accuracy of our experimentation but a larger set of experiments would be required to account for this extra source of variability, and this was not possible due to computational constraints.</p>
<p>The complete experimental process was repeated 20 times to avoid biased results, which could be produced due to the stochastic component of the GP algorithm.</p>
<p>TPOT provides a natural framework to address the pipeline optimization problem. Using other optimization algorithms would require the design of specialized operators to search in the same space of pipelines. Additionally, TPOT uses an internal tree representation for its individuals, which suits perfectly genetic programming, resulting in a more than appropriate framework to make our contribution on.</p>
<p>Benchmark classification problems and parameters of the GP approach</p>
<p>This design was applied to a set of 23 databases of those available from http://www.randalolson.com/data/benchmarks/. These databases were originally used in [22] to investigate the performance of TPOT. The original collection contains 151 databases, ranging from 420 Bytes to 628 Mega Bytes. From this original collection we arbitrarily chose 23 (all the databases starting with the letter "a"), which range from 680B to 2.5MB, resulting in a set that contains float and integer variables, and combines binary and multi-class databases.</p>
<p>In our experiments, the GP algorithm used a population of 100 individuals, and a maximum of 50 generations. The choice of the number of evaluations was due to the computational time constraints in the experiments. The percentage of missing data introduced was 7% as in [13]. This way, we can also roughly compare the effectiveness of the search algorithm compared to a full search. The initial database split (see Section 4.1) is set at 75%. Moreover, this experimentation did not consider the XGBoost classifier, which is optional for TPOT.</p>
<p>Additionally, we performed the same experimentation with the same parameters once more, only this time missing data was not introduced, and, obviously, the imputation operators were not used to evolve the pipelines. In other words, the experiments were also run with regular TPOT.</p>
<p>Influence of the missing data on the accuracy results</p>
<p>The goal of the first study is to compare the pipelines generated using the original TPOT (without the addition of imputation methods) and our proposal, for two different types of databases: with and without missing values. We expect that injecting missing data will degrade the performance of the classifiers. However, we would like to measure how this degradation is alleviated by the use of imputation methods.</p>
<p>The outcome of the comparison is summarized in Figure 2. This box-plot compares, for each database, the accuracies produced by the best evolved pipelines, discerning the results obtained with and without missing data injection. Note that, for this, and the subsequent figures, the database names have been substituted by indexes, in alphabetical order.</p>
<p>As expected, pipelines evolved from databases with missing values performed worse, overall. However, there are some exceptions. For example, pipelines developed from DB7 and DB20 obtained better results generally across all the 20 runs of the experiment. To determine if differences described in Figure 2 were statistically significant, the 20 outcomes run over each incomplete database were subjected to a statistical test, in which they were compared to their counterpart obtained from that same database, in its complete version.  Table 1: p-values generated by applying the Dunn statistical test to each pair of database accuracies, over all 20 runs. Samples that generate a p-value under 0.05 are considered to be significantly different. A blue cell represents a database that generated remarkably better results on its original version compared to its incomplete variant. A non-highlighted cell means that there were found no differences between accuracies generated from the two versions of the database.</p>
<p>On the right combination of imputation methods and classification algorithms</p>
<p>Once the effect of missing data has been analyzed, the next step focuses on determining whether our approach can be useful to identify how imputers and classifiers interact. To this end, we analyze the results on incomplete databases. Figures 3 and 4 show heatmaps where darker colors correspond to higher appearance frequency. Note that the figures show compressed names for imputers and classifiers. Full names can be found in Figure 1. Figure 3 represents the amount of times an imputer was part of the best pipeline generated for a specific database in all 20 runs of the experiment. Figure 4 represents the amount of times an imputer/classifier pair was comprised in the best pipelines. Figure 4 also displays a number, which stands for the amount of different databases it was found in, i.e., a combination would only get one point for each database, even if it was present in more than one run for the same data. These numbers serve as a descriptor of how robust a particular combination of imputers and classifiers was at the time of dealing with missing data in a diverse set of databases.</p>
<p>An analysis of Figure 3 reveals the existing uncertainty when choosing imputation methods. Even though 23 databases were considered, only two of them showed preference for one IM, appearing the Mean  23  22  21  20  19  18  17  16  15  14  13  12  11  latter in more than 11 occasions. Another remarkable fact resides in the good performance offered by the simple imputation methods, as opposed to the usual result lines presented in the state-of-art.</p>
<p>MnI EMI MdI MFI MICEMaxI</p>
<p>In terms of the global behavior of the classifiers, Figure 4 shows that the Gradient Boosting classifier produces good results regardless of the imputation method it is combined with, except for EM and Mean.</p>
<p>Discussion</p>
<p>In Figure 2, we can see how overall databases with no missing data introduced offered better classification accuracies than their incomplete versions. However, if we take a closer look, some pipelines that obtained better performance generated from databases with missing values introduced and imputed stand out (DB7 is a clear example). At a first glance, this behavior could seem indicative of good imputation, since the classification accuracy has increased. Nevertheless, in those cases imputation produced overconfidence on the classifier, which makes the model believe that there is less uncertainty than there really is, potentially leading to miss-classification. This is why several runs of the GP algorithm are necessary, not to be misled by uncommon behaviors and outliers, such as the one that obtained a 100% accuracy in database 1.</p>
<p>In any case, imputation seems to have softened the effects of missing values. Overall, databases that generated non-top accuracies had similar results between original and incomplete versions of data, and only some of the top accuracies (about 90%) offered significantly different results (DB5 and DB22). The fact that in most cases the generated accuracies from both complete and incomplete databases vary in similar ranges makes us believe that imputation is a more-than-valid strategy to consider when facing a missing data problem.</p>
<p>Results in these figures are statistically supported by Concerning the information represented in Figure 3, there is not clear indication that an imputation method clearly and consistently outperforms the other ones. More than one strategy had a major scoring difference over the rest in at least one database, (MICE on 21, most frequent on 22, and mean on 2, for example). This backs up what other works of the state-of-the-art conclude, that there is not a universally better imputation method regarding posterior classification accuracy.</p>
<p>Nevertheless, a quick analysis of the previous figure indicates most frequent imputation as the method that at the end of the process generated the best accuracies, overall.</p>
<p>This observation is partially supported by Figure 4. In this case, even though most frequent imputation is repeatedly found in the best evolved pipelines, it is median imputation which, combined with different classification methods, appears in more distinct databases. However, all imputation methods were present in most part of database pipelines, at least once. We can see that Gradient Boosting benefited median and most frequent methods when combined, but in other cases, it is median imputation which clearly outperforms the most frequent strategy.</p>
<p>One possible explanation of these results is that they could be product of the small amount of missing data introduced, since the performance of the simple methods in some cases tends to drastically decrease as the missingness increases, while more sophisticated methods suffer these effects in a softer manner [1].</p>
<p>Conclusions</p>
<p>Automatic generation of machine learning pipelines is one of the relevant topics to extend the scope of application and efficiency of machine learning software. Missing data is a frequent difficulty found at the time of applying machine learning algorithms. In this paper we have proposed a framework to automatically select imputation methods as part of classification pipelines.</p>
<p>We have also investigated the relationship between imputers and classifiers using an approach, that to the knowledge of the authors, has not been explored before. This strategy involves the application of a GP algorithm to evolve the best performing pipelines, and conduct a statistical analysis of these pipelines.</p>
<p>Another contribution of our work is to show how TPOT can be extended to deal with missing data in any circumstances. Previously, only classifiers that permit missingness on their input (e.g., XG-Boost) could classify databases with missing data, but with the addition of an imputation step, any classifier can. We have shown that both, simple and complex imputation methods, can be added to TPOT, giving the GP algorithm a considerably expanded search space.</p>
<p>As a general conclusion, we can state that automatic configuration of pipelines can be useful to identify which are the particular combinations of imputers and classifiers more appropriate for a particular database. This result is important, since, as confirmed by previous studies, there is not a superior imputation method for all databases.</p>
<p>Further work</p>
<p>This work has added new functionality to an existing python library. However, more work on the analysis part should be addressed. For example, one question to be determinedd is how the number of generations influence the quality of the optimal pipelines.</p>
<p>Additionally, an extension of the imputer set should be considered. In this instance, four simple methods have been proposed, complemented by two more sophisticated techniques. The latter have a significantly higher time consumption rate, which should correspond with a greater score, but this was not the case. This is why the addition of more imputation methods, whose complexity pays off, is required.</p>
<p>Besides, beyond the scope of this work, TPOT could be extended from a bi-objective to a triobjective algorithm. This third component to be taken into consideration would be time. In the design we have introduced in this paper, TPOT simply focuses on obtaining the best accuracy and having short pipelines. However, pipeline shortness does not guarantee speed. Thus, pipeline execution time could be added as a new objective to optimize Finally, and although GP looks naturally suited for this task, other optimization algorithms could be tested in this problem.</p>
<p>Acknowledgments</p>
<p>This work was supported by IT-609-13 program (Basque Government) and TIN2016-78365-R (Spanish Ministry of Economy, Industry and Competitiveness) projects, while Unai Garciarena holds a predoctoral grant from the University of the Basque Country. We also thank Borja Calvo for useful advice on the statistical analysis conducted in the paper. Additionally, we thank anonymous reviewers of a previous version of this paper submitted to the SSBSE-2017 conference. Some of their comments and suggestions have been added to this manuscript.</p>
<p>Figure 1 :
1TPOT Workflow fiers were K Nearest Neighbors, NaÃ¯ve Bayes, Support Vector Machine and Multi Layer Perceptron.</p>
<p>the same expression would be coded internally in the program, before being processed</p>
<p>this operation could be performed with any nodes, as long as the individuals keep matching with the grammatic defined in TPOT.This kind of operation can also be performed with other altering strategies, i.e., mutation. In this case, we modify a terminal node of the tree (a parameter of a method) to another feasible value:[ ' Mu ltin o m ialNB ' , [ ' MaxAbsScaler ' , [ ' MFImputer ' , ' i n p u t m a t r i x ' ] , T r u e ] , 1 , T r u e ] In this example, we modify the node representing the "alpha" parameter in "MultinomialNB" (the first one) from 1 to 10, another selection in the range of plausible values: [ ' Mu ltin o m ialNB ' , [ ' MaxAbsScaler ' , [ ' MFImputer ' , ' i n p u t m a t r i x ' ] , T r u e ] , 1 0 , T r u e ]</p>
<p>Figure 2 :
2Box plot produced using the accuracies obtained using the top pipelines in each one of the 20 runs of the experiment, for each DB. This plot also discerns databases that had no missingness injected and corresponding databases with missing values introduced and imputed.Finally, some configurations involving missing values have managed to produce surprisingly good results, as proven by the top outliers in databases 1 and 19.</p>
<p>Figure 3 :Figure 4 :
34Frequency of imputers in the best evolved pipelines for each database. As it can be seen in the color scale, the darker the color, the more appearances a method had in an optimal pipeline. Average appearance rate for imputer/classifier pairs in the best evolved pipelines. Darker color correspond to higher appearance frequency. Number refers to the amount of different databases (out of23) where each pair appears in at least one of the pipelines.</p>
<p>Table 1
1summarizes the </p>
<p>Table 1 .
1This table confirms that 10 out of 23 databases showed significantly disparate results. The fact that the other 13 did not show these differences backs up the theory that, even if missing values harm data quality, imputation methods can be considered a viable solution.</p>
<p>The treatment of missing values and its effect on classifier accuracy. E Acuna, C Rodrguez, Classification, clustering, and data mining applications. SpringerE. Acuna and C. Rodrguez. The treatment of missing values and its effect on classifier ac- curacy. In Classification, clustering, and data mining applications, pages 639-647. Springer, 2004.</p>
<p>A comparison of imputation techniques for handling missing predictor values in a risk model with a binary outcome. G Ambler, R Z Omar, P Royston, Statistical methods in medical research. 163G. Ambler, R. Z. Omar, and P. Royston. A comparison of imputation techniques for handling missing predictor values in a risk model with a binary outcome. Statistical methods in medical research, 16(3):277-298, 2007.</p>
<p>A review of hot deck imputation for survey non-response. R R Andridge, R J Little, International statistical review. 781R. R. Andridge and R. J. Little. A review of hot deck imputation for survey non-response. International statistical review, 78(1):40-64, 2010.</p>
<p>Genetic programming: an introduction. W Banzhaf, P Nordin, R E Keller, F D Francone, Morgan Kaufmann San Francisco1W. Banzhaf, P. Nordin, R. E. Keller, and F. D. Francone. Genetic programming: an introduc- tion, volume 1. Morgan Kaufmann San Francisco, 1998.</p>
<p>Incremental evolution of autonomous controllers for unmanned aerial vehicles using multi-objective genetic programming. G J Barlow, C K Oh, E Grant, Cybernetics and Intelligent Systems, 2004 IEEE Conference on. IEEE2G. J. Barlow, C. K. Oh, and E. Grant. Incremental evolution of autonomous controllers for unmanned aerial vehicles using multi-objective genetic programming. In Cybernetics and Intelligent Systems, 2004 IEEE Conference on, volume 2, pages 689-694. IEEE, 2004.</p>
<p>An analysis of four missing data treatment methods for supervised learning. G E Batista, M C Monard, Applied Artificial Intelligence. 175-6G. E. Batista and M. C. Monard. An analysis of four missing data treatment methods for supervised learning. Applied Artificial Intelligence, 17(5-6):519-533, 2003.</p>
<p>Multiobjective genetic programming: Reducing bloat using SPEA2. S Bleuler, M Brack, L Thiele, E Zitzler, Proceedings of the 2001 Congress on. the 2001 Congress onIEEE1Evolutionary ComputationS. Bleuler, M. Brack, L. Thiele, and E. Zitzler. Multiobjective genetic programming: Reducing bloat using SPEA2. In Evolutionary Computation, 2001. Proceedings of the 2001 Congress on, volume 1, pages 536-543. IEEE, 2001.</p>
<p>MICE: Multivariate Imputation by Chained Equations in R. S V Buuren, K Groothuis-Oudshoorn, Journal of Statistical Software. 453S. v. Buuren and K. Groothuis-Oudshoorn. MICE: Multivariate Imputation by Chained Equa- tions in R. Journal of Statistical Software, 45(3):1-67, 2011.</p>
<p>Time series imputation using genetic programming and Lagrange interpolation. D C De Resende, D L Santana, F M F Lobato, Intelligent Systems (BRACIS), 2016 5th Brazilian Conference on Intelligent Systems. IEEED. C. de Resende, d. L. de Santana, and F. M. F. Lobato. Time series imputation using genetic programming and Lagrange interpolation. In Intelligent Systems (BRACIS), 2016 5th Brazilian Conference on Intelligent Systems, pages 169-174. IEEE, 2016.</p>
<p>A fast and elitist multiobjective genetic algorithm: NSGA-II. K Deb, A Pratap, S Agarwal, T Meyarivan, IEEE transactions on evolutionary computation. 62K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on evolutionary computation, 6(2):182-197, 2002.</p>
<p>Experiments on the DCASE Challenge 2016: Acoustic scene classification and sound event detection in real life recording. B Elizalde, A Kumar, A Shah, R Badlani, E Vincent, B Raj, I Lane, arXiv:1607.06706arXiv preprintB. Elizalde, A. Kumar, A. Shah, R. Badlani, E. Vincent, B. Raj, and I. Lane. Experiments on the DCASE Challenge 2016: Acoustic scene classification and sound event detection in real life recording. arXiv preprint arXiv:1607.06706, 2016.</p>
<p>DEAP: Evolutionary Algorithms Made Easy. F.-A Fortin, F.-M D Rainville, M.-A Gardner, M Parizeau, C Gagn, Journal of Machine Learning Research. 13F.-A. Fortin, F.-M. D. Rainville, M.-A. Gardner, M. Parizeau, and C. Gagn. DEAP: Evolu- tionary Algorithms Made Easy. Journal of Machine Learning Research, 13:2171-2175, July 2012.</p>
<p>An extensive analysis of the interaction between missing data types, imputation methods, and supervised classifiers. U Garciarena, R Santana, Expert Systems with Applications. 89U. Garciarena and R. Santana. An extensive analysis of the interaction between missing data types, imputation methods, and supervised classifiers. Expert Systems with Applications, 89:52-65, Dec. 2017.</p>
<p>Amelia II: A program for missing data. J Honaker, G King, M Blackwell, Journal of statistical software. 457J. Honaker, G. King, and M. Blackwell. Amelia II: A program for missing data. Journal of statistical software, 45(7):1-47, 2011.</p>
<p>Missing value estimation for DNA microarray gene expression data: local least squares imputation. H Kim, G H Golub, H Park, Bioinformatics. 212H. Kim, G. H. Golub, and H. Park. Missing value estimation for DNA microarray gene ex- pression data: local least squares imputation. Bioinformatics, 21(2):187-198, 2005.</p>
<p>Annual "Humies" Awards For Human-Competitive Results. J Koza, J. Koza. Annual "Humies" Awards For Human-Competitive Results. 2017.</p>
<p>Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems. J R Koza, Stanford University, Department of Computer ScienceJ. R. Koza. Genetic programming: A paradigm for genetically breeding populations of com- puter programs to solve problems. Stanford University, Department of Computer Science, 1990.</p>
<p>Foundations of genetic programming. W B Langdon, R Poli, Springer Science &amp; Business MediaW. B. Langdon and R. Poli. Foundations of genetic programming. Springer Science &amp; Business Media, 2013.</p>
<p>Statistical analysis with missing data. R J Little, D B Rubin, John Wiley &amp; SonsR. J. Little and D. B. Rubin. Statistical analysis with missing data. John Wiley &amp; Sons, 2014.</p>
<p>A study on the use of imputation methods for experimentation with Radial Basis Function Network classifiers handling missing attribute values: the good synergy between RBFNs and EventCovering method. J Luengo, S Garca, F Herrera, Neural Networks. 233J. Luengo, S. Garca, and F. Herrera. A study on the use of imputation methods for experimen- tation with Radial Basis Function Network classifiers handling missing attribute values: the good synergy between RBFNs and EventCovering method. Neural Networks, 23(3):406-418, 2010.</p>
<p>On the choice of the best imputation methods for missing values considering three groups of classification methods. J Luengo, S Garca, F Herrera, Knowledge and Information Systems. 321J. Luengo, S. Garca, and F. Herrera. On the choice of the best imputation methods for miss- ing values considering three groups of classification methods. Knowledge and Information Systems, 32(1):77-108, 2012.</p>
<p>. R S Olson, R. S. Olson. Benchmark. 2017.</p>
<p>Evaluation of a tree-based pipeline optimization tool for automating data science. R S Olson, N Bartley, R J Urbanowicz, J H Moore, Proceedings of the 2016 on Genetic and Evolutionary Computation Conference. the 2016 on Genetic and Evolutionary Computation ConferenceACMR. S. Olson, N. Bartley, R. J. Urbanowicz, and J. H. Moore. Evaluation of a tree-based pipeline optimization tool for automating data science. In Proceedings of the 2016 on Genetic and Evolutionary Computation Conference, pages 485-492. ACM, 2016.</p>
<p>TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning. R S Olson, J H Moore, Workshop on Automatic Machine Learning. R. S. Olson and J. H. Moore. TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning. In Workshop on Automatic Machine Learning, pages 66-74, 2016.</p>
<p>R S Olson, R J Urbanowicz, P C Andrews, N A Lavender, L C Kidd, J H Moore, 10.1007/978-3-319-31204-09Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016. Porto, PortugalSpringer International PublishingProceedings, Part IR. S. Olson, R. J. Urbanowicz, P. C. Andrews, N. A. Lavender, L. C. Kidd, and J. H. Moore. Applications of Evolutionary Computation: 19th European Conference, EvoAppli- cations 2016, Porto, Portugal, March 30 April 1, 2016, Proceedings, Part I. pages 123-137. Springer International Publishing, 2016. DOI: 10.1007/978-3-319-31204-0 9.</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, The Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret- tenhofer, R. Weiss, and V. Dubourg. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12:2825-2830, 2011.</p>
<p>Multiple imputations in sample surveys-a phenomenological Bayesian approach to nonresponse. D B Rubin, Proceedings of the survey research methods section of the American Statistical Association. the survey research methods section of the American Statistical AssociationAmerican Statistical Association1D. B. Rubin. Multiple imputations in sample surveys-a phenomenological Bayesian approach to nonresponse. In Proceedings of the survey research methods section of the American Statis- tical Association, volume 1, pages 20-34. American Statistical Association, 1978.</p>
<p>Multiple imputation for nonresponse in surveys. D B Rubin, John Wiley &amp; Sons81D. B. Rubin. Multiple imputation for nonresponse in surveys, volume 81. John Wiley &amp; Sons, 2004.</p>
<p>Missing data: our view of the state of the art. J L Schafer, J W Graham, Psychological methods. 72147J. L. Schafer and J. W. Graham. Missing data: our view of the state of the art. Psychological methods, 7(2):147, 2002.</p>
<p>Multiple imputation for missing data using genetic programming. C T Tran, M Zhang, P Andreae, Proceedings of the 2015 annual conference on genetic and evolutionary computation. the 2015 annual conference on genetic and evolutionary computationACMC. T. Tran, M. Zhang, and P. Andreae. Multiple imputation for missing data using genetic programming. In Proceedings of the 2015 annual conference on genetic and evolutionary computation, pages 583-590. ACM, 2015.</p>
<p>A genetic programming-based imputation method for classification with missing data. C T Tran, M Zhang, P Andreae, European Conference on Genetic Programming. SpringerC. T. Tran, M. Zhang, and P. Andreae. A genetic programming-based imputation method for classification with missing data. In European Conference on Genetic Programming, pages 149-163. Springer, 2016.</p>
<p>Missing value estimation methods for DNA microarrays. O Troyanskaya, M Cantor, G Sherlock, P Brown, T Hastie, R Tibshirani, D Botstein, R B Altman, Bioinformatics. 176O. Troyanskaya, M. Cantor, G. Sherlock, P. Brown, T. Hastie, R. Tibshirani, D. Botstein, and R. B. Altman. Missing value estimation methods for DNA microarrays. Bioinformatics, 17(6):520-525, 2001.</p>
<p>A genetic algorithm tutorial. D Whitley, Statistics and computing. 42D. Whitley. A genetic algorithm tutorial. Statistics and computing, 4(2):65-85, 1994.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2394 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2394</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2394</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6" target="_blank">Unifying Large Language Models and Knowledge Graphs: A Roadmap</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Knowledge and Data Engineering</p>
                <p><strong>Paper TL;DR:</strong> A forward-looking roadmap for the unification of LLMs and KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2394.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2394.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMET</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses transformer-based language models to generate new commonsense knowledge tuples (triples) by training on seed KG facts and autoregressively producing additional triples to expand commonsense knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>COMET</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Autoregressive transformer trained on existing commonsense knowledge triples to generate novel triples (head, relation, tail). It treats KG generation as a text generation problem, using prompts derived from seed tuples and training a sequence model to produce additional plausible triples that can be added to a KG. Underlying approach: LLM-based generative model (transformer) fine-tuned for knowledge generation and KG completion.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>commonsense knowledge graphs / AI</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended knowledge generation / discovery</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2394.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BertNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BertNet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework mentioned for automatic KG construction empowered by LLMs that generates prompts and searches a language model to extract consistent knowledge to build KGs with diverse and novel relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BertNet</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Framework that (per the paper) requires minimal relation definitions, automatically constructs diverse prompts, and performs an efficient search/querying procedure over a given LLM to produce candidate facts for inclusion in a KG. Underlying approach: prompt-driven LLM querying and automated prompt generation to distill structured facts from model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>knowledge graph construction / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>end-to-end KG construction / distillation of implicit model knowledge into explicit facts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2394.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RooNest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RooNest</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method/framework referenced that automatically generates knowledge graphs by distilling knowledge from large language models, producing diverse new relations and facts not present in existing extractive pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RooNest</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that queries LLMs with automatically generated prompts or templates to retrieve candidate facts and aggregates them into an explicit KG; emphasizes diversity and novelty of relations produced via LLM-driven generation. Underlying approach: LLM-based generation plus filtering/aggregation to construct KG entries.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>knowledge graph construction / commonsense knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended discovery / KG distillation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2394.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>West et al. distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic knowledge distillation from LLMs (West et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic knowledge distillation framework described that fine-tunes a smaller 'student' LLM by distilling commonsense facts from a larger LLM, then uses the student to generate explicit KG facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic knowledge distillation (West et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage distillation: large teacher LLM is probed for knowledge, outputs are used to fine-tune a smaller student LLM which is then used to autoregressively generate structured facts for KG population. Underlying approach: LLM prompting + distillation + generation to produce explicit symbolic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>knowledge distillation / KG construction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended knowledge extraction and consolidation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2394.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoKG</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based system (mentioned) that designs prompts for various KG construction tasks (entity typing, linking, relation extraction) and uses ChatGPT/GPT-4 to perform KG construction via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoKG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt-engineering pipeline that encodes KG construction subtasks into few-shot or instruction prompts for closed-source LLMs (ChatGPT/GPT-4) to output candidate entities, types, links, and relations; relies on LLM generation and downstream normalization/verification to build or extend KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>knowledge graph construction / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>task-driven entity/relation extraction and KG population</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2394.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PiVE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PiVE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting + verification framework that uses an LLM to generate KG content and a smaller LLM (e.g., T5) to iteratively verify and correct errors in the LLM-generated KG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PiVE (Prompting with Iterative Verification and Editing)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage system: (1) a large LLM generates candidate KG triples from prompts; (2) a smaller, cheaper LLM performs iterative verification and correction on generated triples to reduce errors and improve consistency. Underlying approach: LLM generation + verification loop for higher-quality KG construction.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>knowledge graph construction / LLM verification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended generation with post-hoc validation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2394.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that provides programmatic APIs/interfaces to allow LLMs (e.g., ChatGPT) to reason directly on KGs and perform multi-step question answering or structured operations over graph data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>StructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Interface design that exposes KG operations via APIs so an LLM can call graph traversal, retrieval, and manipulation primitives; LLMs act as controllers that plan and execute multi-step KG reasoning through these APIs. Underlying approach: LLM-as-agent plus structured API for KG interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>KG reasoning / LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>multi-step structured reasoning / agentic exploration of graph data</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2394.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Think-on-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Think-on-graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play framework where LLM agents iteratively execute beam searches on knowledge graphs to discover reasoning paths and generate answers; LLMs are used as agents performing search and synthesis over KG structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Think-on-graph</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agentic framework: LLM issues actions to traverse or expand KG nodes/edges, uses beam search to explore multiple reasoning paths, and aggregates/verbalizes discovered paths as part of answer or hypothesis generation. Underlying approach: LLM-based agent with search over structured KG (graph search + LLM planning).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>KG reasoning / LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>structured path discovery / hypothesis-like path generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2394.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentTuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentTuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuning datasets and methods to guide LLM agents to perform reasoning and actions on knowledge graphs, improving the agentic capability of LLMs to interact with structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentTuning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Creates instruction-tuning corpora that teach LLMs how to perform KG operations and multi-step reasoning (e.g., retrieve, plan, traverse). Underlying approach: supervised instruction tuning of LLMs to act as agents for structured-data reasoning and to produce procedural/stepwise outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>LLM instruction tuning / KG agents</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>agentic planning and structured reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2394.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KD-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KD-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that iteratively retrieves facts from KGs and produces faithful chain-of-thought traces that guide LLMs to generate answers grounded by KG facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KD-CoT (Knowledge Distilled Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative retrieval and explanation pipeline: retrieve KG facts relevant to a query, generate chain-of-thought style intermediate reasoning steps grounded on those facts, and use these traces to constrain or inform the final answer generation. Underlying approach: hybrid KG retrieval + LLM-generated reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>KG-grounded reasoning / explainable LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>faithful, grounded multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2394.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2394.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KSL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method mentioned that teaches LLMs to search on knowledge graphs to retrieve relevant facts and then generate answers; an agentic search-and-generate paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KSL (KG Search Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach where LLMs are trained or prompted to perform explicit search actions over a KG (e.g., selecting edges/paths), retrieving facts that are then used as context for answer generation; underlying approach: LLM-driven graph search plus text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>KG reasoning / LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>path-finding and retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unifying Large Language Models and Knowledge Graphs: A Roadmap', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2394",
    "paper_id": "paper-9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "COMET",
            "name_full": "COMET",
            "brief_description": "A system that uses transformer-based language models to generate new commonsense knowledge tuples (triples) by training on seed KG facts and autoregressively producing additional triples to expand commonsense knowledge graphs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "COMET",
            "system_description": "Autoregressive transformer trained on existing commonsense knowledge triples to generate novel triples (head, relation, tail). It treats KG generation as a text generation problem, using prompts derived from seed tuples and training a sequence model to produce additional plausible triples that can be added to a KG. Underlying approach: LLM-based generative model (transformer) fine-tuned for knowledge generation and KG completion.",
            "research_domain": "commonsense knowledge graphs / AI",
            "problem_type": "open-ended knowledge generation / discovery",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.0",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "BertNet",
            "name_full": "BertNet",
            "brief_description": "A framework mentioned for automatic KG construction empowered by LLMs that generates prompts and searches a language model to extract consistent knowledge to build KGs with diverse and novel relations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "BertNet",
            "system_description": "Framework that (per the paper) requires minimal relation definitions, automatically constructs diverse prompts, and performs an efficient search/querying procedure over a given LLM to produce candidate facts for inclusion in a KG. Underlying approach: prompt-driven LLM querying and automated prompt generation to distill structured facts from model outputs.",
            "research_domain": "knowledge graph construction / NLP",
            "problem_type": "end-to-end KG construction / distillation of implicit model knowledge into explicit facts",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.1",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "RooNest",
            "name_full": "RooNest",
            "brief_description": "A method/framework referenced that automatically generates knowledge graphs by distilling knowledge from large language models, producing diverse new relations and facts not present in existing extractive pipelines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "RooNest",
            "system_description": "Pipeline that queries LLMs with automatically generated prompts or templates to retrieve candidate facts and aggregates them into an explicit KG; emphasizes diversity and novelty of relations produced via LLM-driven generation. Underlying approach: LLM-based generation plus filtering/aggregation to construct KG entries.",
            "research_domain": "knowledge graph construction / commonsense knowledge",
            "problem_type": "open-ended discovery / KG distillation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.2",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "West et al. distillation",
            "name_full": "Symbolic knowledge distillation from LLMs (West et al.)",
            "brief_description": "A symbolic knowledge distillation framework described that fine-tunes a smaller 'student' LLM by distilling commonsense facts from a larger LLM, then uses the student to generate explicit KG facts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Symbolic knowledge distillation (West et al.)",
            "system_description": "Two-stage distillation: large teacher LLM is probed for knowledge, outputs are used to fine-tune a smaller student LLM which is then used to autoregressively generate structured facts for KG population. Underlying approach: LLM prompting + distillation + generation to produce explicit symbolic knowledge.",
            "research_domain": "knowledge distillation / KG construction",
            "problem_type": "open-ended knowledge extraction and consolidation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.3",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "AutoKG",
            "name_full": "AutoKG",
            "brief_description": "A prompt-based system (mentioned) that designs prompts for various KG construction tasks (entity typing, linking, relation extraction) and uses ChatGPT/GPT-4 to perform KG construction via prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AutoKG",
            "system_description": "Prompt-engineering pipeline that encodes KG construction subtasks into few-shot or instruction prompts for closed-source LLMs (ChatGPT/GPT-4) to output candidate entities, types, links, and relations; relies on LLM generation and downstream normalization/verification to build or extend KGs.",
            "research_domain": "knowledge graph construction / NLP",
            "problem_type": "task-driven entity/relation extraction and KG population",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.4",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PiVE",
            "name_full": "PiVE",
            "brief_description": "A prompting + verification framework that uses an LLM to generate KG content and a smaller LLM (e.g., T5) to iteratively verify and correct errors in the LLM-generated KG.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "PiVE (Prompting with Iterative Verification and Editing)",
            "system_description": "Two-stage system: (1) a large LLM generates candidate KG triples from prompts; (2) a smaller, cheaper LLM performs iterative verification and correction on generated triples to reduce errors and improve consistency. Underlying approach: LLM generation + verification loop for higher-quality KG construction.",
            "research_domain": "knowledge graph construction / LLM verification",
            "problem_type": "open-ended generation with post-hoc validation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.5",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "StructGPT",
            "name_full": "StructGPT",
            "brief_description": "A system that provides programmatic APIs/interfaces to allow LLMs (e.g., ChatGPT) to reason directly on KGs and perform multi-step question answering or structured operations over graph data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "StructGPT",
            "system_description": "Interface design that exposes KG operations via APIs so an LLM can call graph traversal, retrieval, and manipulation primitives; LLMs act as controllers that plan and execute multi-step KG reasoning through these APIs. Underlying approach: LLM-as-agent plus structured API for KG interaction.",
            "research_domain": "KG reasoning / LLM agents",
            "problem_type": "multi-step structured reasoning / agentic exploration of graph data",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.6",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Think-on-graph",
            "name_full": "Think-on-graph",
            "brief_description": "A plug-and-play framework where LLM agents iteratively execute beam searches on knowledge graphs to discover reasoning paths and generate answers; LLMs are used as agents performing search and synthesis over KG structure.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Think-on-graph",
            "system_description": "Agentic framework: LLM issues actions to traverse or expand KG nodes/edges, uses beam search to explore multiple reasoning paths, and aggregates/verbalizes discovered paths as part of answer or hypothesis generation. Underlying approach: LLM-based agent with search over structured KG (graph search + LLM planning).",
            "research_domain": "KG reasoning / LLM agents",
            "problem_type": "structured path discovery / hypothesis-like path generation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.7",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "AgentTuning",
            "name_full": "AgentTuning",
            "brief_description": "Instruction-tuning datasets and methods to guide LLM agents to perform reasoning and actions on knowledge graphs, improving the agentic capability of LLMs to interact with structured data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AgentTuning",
            "system_description": "Creates instruction-tuning corpora that teach LLMs how to perform KG operations and multi-step reasoning (e.g., retrieve, plan, traverse). Underlying approach: supervised instruction tuning of LLMs to act as agents for structured-data reasoning and to produce procedural/stepwise outputs.",
            "research_domain": "LLM instruction tuning / KG agents",
            "problem_type": "agentic planning and structured reasoning",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.8",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "KD-CoT",
            "name_full": "KD-CoT",
            "brief_description": "A method that iteratively retrieves facts from KGs and produces faithful chain-of-thought traces that guide LLMs to generate answers grounded by KG facts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "KD-CoT (Knowledge Distilled Chain-of-Thought)",
            "system_description": "Iterative retrieval and explanation pipeline: retrieve KG facts relevant to a query, generate chain-of-thought style intermediate reasoning steps grounded on those facts, and use these traces to constrain or inform the final answer generation. Underlying approach: hybrid KG retrieval + LLM-generated reasoning traces.",
            "research_domain": "KG-grounded reasoning / explainable LLMs",
            "problem_type": "faithful, grounded multi-step reasoning",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.9",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "KSL",
            "name_full": "KSL",
            "brief_description": "A method mentioned that teaches LLMs to search on knowledge graphs to retrieve relevant facts and then generate answers; an agentic search-and-generate paradigm.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "KSL (KG Search Learning)",
            "system_description": "Approach where LLMs are trained or prompted to perform explicit search actions over a KG (e.g., selecting edges/paths), retrieving facts that are then used as context for answer generation; underlying approach: LLM-driven graph search plus text generation.",
            "research_domain": "KG reasoning / LLM agents",
            "problem_type": "path-finding and retrieval-augmented generation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2394.10",
            "source_info": {
                "paper_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01833975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unifying Large Language Models and Knowledge Graphs: A Roadmap</h1>
<p>Shirui Pan, Senior Member, IEEE, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEE</p>
<h4>Abstract</h4>
<p>Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unity LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.</p>
<p>Index TermsNatural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap, Bidirectional Reasoning.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) ${ }^{1}$ (e.g., BERT [1], RoBERTA [2], and T5 [3]), pre-trained on the large-scale corpus, have shown great performance in various natural language processing (NLP) tasks, such as question answering [4], machine translation [5], and text generation [6]. Recently, the dramatically increasing model size further enables the LLMs with the emergent ability [7], paving the road for applying LLMs as Artificial General Intelligence (AGI). Advanced LLMs like ChatGPT ${ }^{2}$ and PaLM2 ${ }^{3}$, with billions of parameters, exhibit great potential in many complex practical tasks, such as education [8], code generation [9] and recommendation [10].</p>
<ul>
<li>Shirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia. Email: s.pan@griffith.edu.au;</li>
<li>Linhao Luo and Yufei Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.</li>
<li>Chen Chen is with the Nanyang Technological University, Singapore. Email: s190009@ntu.edu.sg.</li>
<li>Jiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.</li>
<li>Xindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China. Email: xwu@hfut.edu.cn.</li>
<li>Shirui Pan and Linhao Luo contributed equally to this work.</li>
<li>
<p>Corresponding Author: Xindong Wu.</p>
</li>
<li>
<p>LLMs are also known as pre-trained language models (PLMs).</p>
</li>
<li>https://openai.com/blog/chatgpt</li>
<li>https://ai.google/discover/palm2
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ul>
<p>Fig. 1. Summarization of the pros and cons for LLMs and KGs. LLM pros: General Knowledge [11], Language Processing [12], Generalizability [13]; LLM cons: Implicit Knowledge [14], Hallucination [15], Indecisiveness [16], Black-box [17], Lacking Domain-specific/New Knowledge [18]. KG pros: Structural Knowledge [19], Accuracy [20], Decisiveness [21], Interpretability [22], Domain-specific Knowledge [23], Evolving Knowledge [24]; KG cons: Incompleteness [25], Lacking Language Understanding [26], Unseen Facts [27]. Pros. and Cons. are selected based on their representativeness. Detailed discussion can be found in Appendix A.</p>
<p>Despite their success in many applications, LLMs have been criticized for their lack of factual knowledge. Specifically, LLMs memorize facts and knowledge contained in the training corpus [14]. However, further studies reveal that LLMs are not able to recall facts and often experience hallucinations by generating statements that are factually</p>
<p>incorrect [15], [28]. For example, LLMs might say "Einstein discovered gravity in 1687" when asked, "When did Einstein discover gravity?", which contradicts the fact that Isaac Newton formulated the gravitational theory. This issue severely impairs the trustworthiness of LLMs.</p>
<p>As black-box models, LLMs are also criticized for their lack of interpretability. LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs. Moreover, LLMs perform reasoning by a probability model, which is an indecisive process [16]. The specific patterns and functions LLMs used to arrive at predictions or decisions are not directly accessible or explainable to humans [17]. Even though some LLMs are equipped to explain their predictions by applying chain-of-thought [29], their reasoning explanations also suffer from the hallucination issue [30]. This severely impairs the application of LLMs in high-stakes scenarios, such as medical diagnosis and legal judgment. For instance, in a medical diagnosis scenario, LLMs may incorrectly diagnose a disease and provide explanations that contradict medical commonsense. This raises another issue that LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data [18].</p>
<p>To address the above issues, a potential solution is to incorporate knowledge graphs (KGs) into LLMs. Knowledge graphs (KGs), storing enormous facts in the way of triples, i.e., (head entity, relation, tail entity), are a structured and decisive manner of knowledge representation (e.g., Wikidata [20], YAGO [31], and NELL [32]). KGs are crucial for various applications as they offer accurate explicit knowledge [19]. Besides, they are renowned for their symbolic reasoning ability [22], which generates interpretable results. KGs can also actively evolve with new knowledge continuously added in [24]. Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge [23].</p>
<p>Nevertheless, KGs are difficult to construct [25], and current approaches in KGs [27], [33], [34] are inadequate in handling the incomplete and dynamically changing nature of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig. 1, respectively.</p>
<p>Recently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practitioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In KG-enhanced LLMs, KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge [35]-[37], but also used for analyzing LLMs and providing interpretability [14], [38], [39]. In LLM-augmented KGs, LLMs have been used in various KG-related tasks, e.g., KG embedding [40], KG completion [26], KG construction [41], KG-to-text generation [42], and KGQA [43], to improve the performance and facilitate the application of KGs. In Synergized $L L M+K G$, researchers marries the merits of LLMs
and KGs to mutually enhance performance in knowledge representation [44] and reasoning [45], [46]. Although there are some surveys on knowledge-enhanced LLMs [47]-[49], which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrating KGs for LLMs and the potential role of LLMs in KG applications.</p>
<p>In this article, we present a forward-looking roadmap for unifying both LLMs and KGs, to leverage their respective strengths and overcome the limitations of each approach, for various downstream tasks. We propose detailed categorization, conduct comprehensive reviews, and pinpoint emerging directions in these fast-growing fields. Our main contributions are summarized as follows:</p>
<p>1) Roadmap. We present a forward-looking roadmap for integrating LLMs and KGs. Our roadmap, consisting of three general frameworks to unify LLMs and KGs, namely, KG-enhanced LLMs, LLMaugmented KGs, and Synergized LLMs + KGs, provides guidelines for the unification of these two distinct but complementary technologies.
2) Categorization and review. For each integration framework of our roadmap, we present a detailed categorization and novel taxonomies of research on unifying LLMs and KGs. In each category, we review the research from the perspectives of different integration strategies and tasks, which provides more insights into each framework.
3) Coverage of emerging advances. We cover the advanced techniques in both LLMs and KGs. We include the discussion of state-of-the-art LLMs like ChatGPT and GPT-4 as well as the novel KGs e.g., multi-modal knowledge graphs.
4) Summary of challenges and future directions. We highlight the challenges in existing research and present several promising future research directions.</p>
<p>The rest of this article is organized as follows. Section 2 first explains the background of LLMs and KGs. Section 3 introduces the roadmap and the overall categorization of this article. Section 4 presents the different KGs-enhanced LLM approaches. Section 5 describes the possible LLMaugmented KG methods. Section 6 shows the approaches of synergizing LLMs and KGs. Section 7 discusses the challenges and future research directions. Finally, Section 8 concludes this paper.</p>
<h2>2 BACKGROUND</h2>
<p>In this section, we will first briefly introduce a few representative large language models (LLMs) and discuss the prompt engineering that efficiently uses LLMs for varieties of applications. Then, we illustrate the concept of knowledge graphs (KGs) and present different categories of KGs.</p>
<h3>2.1 Large Language models (LLMs)</h3>
<p>Large language models (LLMs) pre-trained on large-scale corpus have shown great potential in various NLP tasks [13]. As shown in Fig. 3, most LLMs derive from the Transformer design [50], which contains the encoder and decoder</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Representative large language models (LLMs) in recent years. Open-source models are represented by solid squares, while closed source models are represented by hollow squares.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. An illustration of the Transformer-based LLMs with self-attention mechanism.</p>
<p>Modules empowered by a self-attention mechanism. Based on the architecture structure, LLMs can be categorized into three groups: 1) <em>encoder-only LLMs</em>, 2) <em>encoder-decoder LLMs</em>, and 3) <em>decoder-only LLMs</em>. As shown in Fig. 2, we summarize several representative LLMs with different model architectures, model sizes, and open-source availabilities.</p>
<h3>2.1.1 Encoder-only LLMs</h3>
<p>Encoder-only large language models only use the encoder to encode the sentence and understand the relationships between words. The common training paradigm for these models is to predict the mask words in an input sentence. This method is unsupervised and can be trained on the large-scale corpus. Encoder-only LLMs like BERT [1], ALBERT [51], RoBERTa [2], and ELECTRA [52] require adding an extra prediction head to resolve downstream tasks. These models are most effective for tasks that require understanding the entire sentence, such as text classification [26] and named entity recognition [53].</p>
<h3>2.1.2 Encoder-decoder LLMs</h3>
<p>Encoder-decoder large language models adopt both the encoder and decoder module. The encoder module is responsible for encoding the input sentence into a hidden space, and the decoder is used to generate the target output text. The training strategies in encoder-decoder LLMs can be more flexible. For example, T5 [3] is pre-trained by masking and predicting spans of masking words. UL2 [54] unifies several training targets such as different masking spans and masking frequencies. Encoder-decoder LLMs (e.g., T0 [55], ST-MoE [56], and GLM-130B [57]) are able to directly resolve tasks that generate sentences based on some context, such as summarization, translation, and question answering [58].</p>
<h3>2.1.3 Decoder-only LLMs</h3>
<p>Decoder-only large language models only adopt the decoder module to generate target output text. The training paradigm for these models is to predict the next word in the sentence. Large-scale decoder-only LLMs can generally perform downstream tasks from a few examples or simple instructions, without adding prediction heads or finetuning [59]. Many state-of-the-art LLMs (e.g., Chat-GPT [60] and GPT-4) follow the decoder-only architecture. However, since these models are closed-source, it is challenging for academic researchers to conduct further research. Recently, Alpaca and Vicuna are released as open-source decoder-only LLMs. These models are finetuned based on LLaMA [61] and achieve comparable performance with ChatGPT and GPT-4.</p>
<h3>2.1.4 Prompt Engineering</h3>
<p>Prompt engineering is a novel field that focuses on creating and refining prompts to maximize the effectiveness of large language models (LLMs) across various applications and research areas [62]. As shown in Fig. 4, a prompt is a sequence</p>
<ol>
<li>https://openai.com/product/gpt-4</li>
<li>https://github.com/tatsu-lab/stanford_alpaca</li>
<li>https://lmsys.org/blog/2023-03-30-vicuna/</li>
</ol>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. An example of sentiment classification prompt.</p>
<p>of natural language inputs for LLMs that are specified for the task, such as sentiment classification. A prompt could contain several elements, i.e., 1) <em>Instruction</em>, 2) <em>Context</em>, and 3) <em>Input Text</em>. <em>Instruction</em> is a short sentence that instructs the model to perform a specific task. <em>Context</em> provides the context for the input text or few-shot examples. <em>Input Text</em> is the text that needs to be processed by the model.</p>
<p>Prompt engineering seeks to improve the capacity of large large language models (e.g., ChatGPT) in diverse complex tasks such as question answering, sentiment classification, and common sense reasoning. Chain-of-thought (CoT) prompt [63] enables complex reasoning capabilities through intermediate reasoning steps. Prompt engineering also enables the integration of structural data like knowledge graphs (KGs) into LLMs. Li et al. [64] simply linearizes the KGs and uses templates to convert the KGs into passages. Mindmap [65] designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning on it. Prompt offers a simple way to utilize the potential of LLMs without finetuning. Proficiency in prompt engineering leads to a better understanding of the strengths and weaknesses of LLMs.</p>
<h3>2.2 Knowledge Graphs (KGs)</h3>
<p>Knowledge graphs (KGs) store structured knowledge as a collection of triples $$K\mathcal{G} = {(h, r, t) \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}}$$, where $$\mathcal{E}$$ and $$\mathcal{R}$$ respectively denote the set of entities and relations. Existing knowledge graphs (KGs) can be classified into four groups based on the stored information: 1) <em>encyclopedic KGs</em>, 2) <em>commonsense KGs</em>, 3) <em>domain-specific KGs</em>, and 4) <em>multimodal KGs</em>. We illustrate the examples of KGs of different categories in Fig. 5.</p>
<h4>2.2.1 Encyclopedic Knowledge Graphs</h4>
<p>Encyclopedic knowledge graphs are the most ubiquitous KGs, which represent the general knowledge in real-world. Encyclopedic knowledge graphs are often constructed by integrating information from diverse and extensive sources, including human experts, encyclopedias, and databases. Wikidata [20] is one of the most widely used encyclopedic knowledge graphs, which incorporates varieties of knowledge extracted from articles on Wikipedia. Other typical encyclopedic knowledge graphs, like Freebase [66], Dbpedia [67], and YAGO [31] are also derived from Wikipedia. In addition, NELL [32] is a continuously improving encyclopedic knowledge graph, which automatically extracts knowledge from the web, and uses that knowledge to improve its performance over time. There are several encyclopedic knowledge graphs available in languages other than English such as CN-DBpedia [68] and Vikidia [69]. The largest knowledge graph, named Knowledge Occean (KO)<sup>7</sup>, currently contains 4,8784,3636 entities and 17,3115,8349 relations in both English and Chinese.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Examples of different categories' knowledge graphs, i.e., <em>encyclopedic KGs</em>, <em>commonsense KGs</em>, <em>domain-specific KGs</em>, and <em>multi-modal KGs</em>.</p>
<h3>2.2.2 Commonsense Knowledge Graphs</h3>
<p>Commonsense knowledge graphs formulate the knowledge about daily concepts, e.g., objects, and events, as well as their relationships [70]. Compared with encyclopedic knowledge graphs, commonsense knowledge graphs often model the tacit knowledge extracted from text such as (<em>Car, UsedFor, Drive</em>). ConceptNet [71] contains a wide range of commonsense concepts and relations, which can help computers understand the meanings of words people use. ATOMIC [72], [73] and ASER [74] focus on the causal effects between events, which can be used for commonsense reasoning. Some other commonsense knowledge graphs, such as TransOMCS [75] and CausalBanK [76] are automatically constructed to provide commonsense knowledge.</p>
<p><sup>7</sup> https://ko.zhonghuapu.com/</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. The general roadmap of unifying KGs and LLMs. (a.) KG-enhanced LLMs. (b.) LLM-augmented KGs. (c.) Synergized LLMs + KGs.</p>
<p>TABLE 1
Representative applications of using LLMs and KGs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">KGs</th>
<th style="text-align: center;">URL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT/GPT-4</td>
<td style="text-align: center;">Chat Bot</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">https://shorturl.at/cmsE0</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE 3.0</td>
<td style="text-align: center;">Chat Bot</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">https://shorturl.at/sCLV9</td>
</tr>
<tr>
<td style="text-align: left;">Bard</td>
<td style="text-align: center;">Chat Bot</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">https://shorturl.at/pDUy6</td>
</tr>
<tr>
<td style="text-align: left;">Firefly</td>
<td style="text-align: center;">Photo Editing</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">https://shorturl.at/fkzJV</td>
</tr>
<tr>
<td style="text-align: left;">AutoGPT</td>
<td style="text-align: center;">AI Assistant</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">https://shorturl.at/bbz6V</td>
</tr>
<tr>
<td style="text-align: left;">Copilot</td>
<td style="text-align: center;">Coding Assistant</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">https://shorturl.at/ikLUV</td>
</tr>
<tr>
<td style="text-align: left;">New Bing</td>
<td style="text-align: center;">Web Search</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">https://shorturl.at/biinps</td>
</tr>
<tr>
<td style="text-align: left;">Shop.ai</td>
<td style="text-align: center;">Recommendation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">https://shorturl.at/aiC17</td>
</tr>
<tr>
<td style="text-align: left;">Wikidata</td>
<td style="text-align: center;">Knowledge Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">https://shorturl.at/lyMY5</td>
</tr>
<tr>
<td style="text-align: left;">KO</td>
<td style="text-align: center;">Knowledge Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">https://shorturl.at/ss238</td>
</tr>
<tr>
<td style="text-align: left;">OpenBG</td>
<td style="text-align: center;">Recommendation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">https://shorturl.at/pDMV9</td>
</tr>
<tr>
<td style="text-align: left;">Doctor.ai</td>
<td style="text-align: center;">Health Care Assistant</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">https://shorturl.at/dhJK0</td>
</tr>
</tbody>
</table>
<h3>2.2.3 Domain-specific Knowledge Graphs</h3>
<p>Domain-specific knowledge graphs are often constructed to represent knowledge in a specific domain, e.g., medical, biology, and finance [23]. Compared with encyclopedic knowledge graphs, domain-specific knowledge graphs are often smaller in size, but more accurate and reliable. For example, UMLS [77] is a domain-specific knowledge graph in the medical domain, which contains biomedical concepts and their relationships. In addition, there are some domainspecific knowledge graphs in other domains, such as finance [78], geology [79], biology [80], chemistry [81] and genealogy [82].</p>
<h3>2.2.4 Multi-modal Knowledge Graphs.</h3>
<p>Unlike conventional knowledge graphs that only contain textual information, multi-modal knowledge graphs represent facts in multiple modalities such as images, sounds, and videos [83]. For example, IMGpedia [84], MMKG [85], and Richpedia [86] incorporate both the text and image information into the knowledge graphs. These knowledge graphs can be used for various multi-modal tasks such as image-text matching [87], visual question answering [88], and recommendation [89].</p>
<h3>2.3 Applications</h3>
<p>LLMs as KGs have been widely applied in various real-world applications. We summarize some representative applications of using LLMs and KGs in Table 1. ChatGPT/GPT-4 are LLM-based chatbots that can communicate with humans in a natural dialogue format. To improve knowledge awareness of LLMs, ERNIE 3.0 and Bard incorporate KGs into their chatbot applications. Instead of</p>
<p>Chatbot. Firefly develops a photo editing application that allows users to edit photos by using natural language descriptions. Copilot, New Bing, and Shop.ai adopt LLMs to empower their applications in the areas of coding assistant, web search, and recommendation, respectively. Wikidata and KO are two representative knowledge graph applications that are used to provide external knowledge. OpenBG [90] is a knowledge graph designed for recommendation. Doctor.ai develops a health care assistant that incorporates LLMs and KGs to provide medical advice.</p>
<h2>3 Roadmap \&amp; Categorization</h2>
<p>In this section, we first present a road map of explicit frameworks that unify LLMs and KGs. Then, we present the categorization of research on unifying LLMs and KGs.</p>
<h3>3.1 Roadmap</h3>
<p>The roadmap of unifying KGs and LLMs is illustrated in Fig. 6. In the roadmap, we identify three frameworks for the unification of LLMs and KGs, including KG-enhanced LLMs, LLM-augmented KGs, and Synergized LLMs + KGs. The KG-enhanced LLMs and LLM-augmented KGs are two parallel frameworks that aim to enhance the capabilities of LLMs and KGs, respectively. Building upon these frameworks, Synergized LLMs + KGs is a unified framework that aims to synergize LLMs and KGs to mutually enhance each other.</p>
<h3>3.1.1 KG-enhanced LLMs</h3>
<p>LLMs are renowned for their ability to learn knowledge from large-scale corpus and achieve state-of-the-art performance in various NLP tasks. However, LLMs are often criticized for their hallucination issues [15], and lacking of interpretability. To address these issues, researchers have proposed to enhance LLMs with knowledge graphs (KGs).</p>
<p>KGs store enormous knowledge in an explicit and structured way, which can be used to enhance the knowledge awareness of LLMs. Some researchers have proposed to incorporate KGs into LLMs during the pre-training stage, which can help LLMs learn knowledge from KGs [35], [91]. Other researchers have proposed to incorporate KGs into LLMs during the inference stage. By retrieving knowledge from KGs, it can significantly improve the performance of LLMs in accessing domain-specific knowledge [92]. To improve the interpretability of LLMs, researchers also utilize</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. The general framework of the Synergized LLMs + KGs, which contains four layers: 1) Data, 2) Synergized Model, 3) Technique, and 4) Application.</p>
<p>KGs to interpret the facts [14] and the reasoning process of LLMs [38].</p>
<h3>3.1.2 LLM-augmented KGs</h3>
<p>KGs store structure knowledge playing an essential role in many real-world applications [19]. Existing methods in KGs fall short of handling incomplete KGs [33] and processing text corpus to construct KGs [93]. With the generalizability of LLMs, many researchers are trying to harness the power of LLMs for addressing KG-related tasks.</p>
<p>The most straightforward way to apply LLMs as text encoders for KG-related tasks. Researchers take advantage of LLMs to process the textual corpus in the KGs and then use the representations of the text to enrich KGs representation [94]. Some studies also use LLMs to process the original corpus and extract relations and entities for KG construction [95]. Recent studies try to design a KG prompt that can effectively convert structural KGs into a format that can be comprehended by LLMs. In this way, LLMs can be directly applied to KG-related tasks, e.g., KG completion [96] and KG reasoning [97].</p>
<h3>3.1.3 Synergized LLMs + KGs</h3>
<p>The synergy of LLMs and KGs has attracted increasing attention from researchers these years [40], [42]. LLMs and KGs are two inherently complementary techniques, which should be unified into a general framework to mutually enhance each other.</p>
<p>To further explore the unification, we propose a unified framework of the synergized LLMs + KGs in Fig. 7. The unified framework contains four layers: 1) Data, 2) Synergized Model, 3) Technique, and 4) Application. In the Data layer, LLMs and KGs are used to process the textual and structural data, respectively. With the development of multi-modal LLMs [98] and KGs [99], this framework can be extended to process multi-modal data, such as video, audio, and images. In the Synergized Model layer, LLMs and KGs could synergize with each other to improve their capabilities. In Technique layer, related techniques that have been used in LLMs and KGs can be incorporated into this framework to further enhance the performance. In the Application layer, LLMs and KGs can be integrated to address various real-world applications, such as search engines [100], recommender systems [10], and AI assistants [101].</p>
<h3>3.2 Categorization</h3>
<p>To better understand the research on unifying LLMs and KGs, we further provide a fine-grained categorization for each framework in the roadmap. Specifically, we focus on different ways of integrating KGs and LLMs, i.e., KG-enhanced LLMs, KG-augmented LLMs, and Synergized LLMs + KGs. The fine-grained categorization of the research is illustrated in Fig. 8.</p>
<p><strong>KG-enhanced LLMs.</strong> Integrating KGs can enhance the performance and interpretability of LLMs in various downstream tasks. We categorize the research on KG-enhanced LLMs into three groups:</p>
<ol>
<li><strong>KG-enhanced LLM pre-training</strong> includes works that apply KGs during the pre-training stage and improve the knowledge expression of LLMs.</li>
<li><strong>KG-enhanced LLM inference</strong> includes research that utilizes KGs during the inference stage of LLMs, which enables LLMs to access the latest knowledge without retraining.</li>
<li><strong>KG-enhanced LLM interpretability</strong> includes works that use KGs to understand the knowledge learned by LLMs and interpret the reasoning process of LLMs.</li>
</ol>
<p><strong>LLM-augmented KGs.</strong> LLMs can be applied to augment various KG-related tasks. We categorize the research on LLM-augmented KGs into five groups based on the task types:</p>
<ol>
<li><strong>LLM-augmented KG embedding</strong> includes studies that apply LLMs to enrich representations of KGs by encoding the textual descriptions of entities and relations.</li>
<li><strong>LLM-augmented KG completion</strong> includes papers that utilize LLMs to encode text or generate facts for better KGC performance.</li>
<li><strong>LLM-augmented KG construction</strong> includes works that apply LLMs to address the entity discovery, coreference resolution, and relation extraction tasks for KG construction.</li>
<li><strong>LLM-augmented KG-to-text Generation</strong> includes research that utilizes LLMs to generate natural language that describes the facts from KGs.</li>
<li><strong>LLM-augmented KG question answering</strong> includes studies that apply LLMs to bridge the gap between natural language questions and retrieve answers from KGs.</li>
</ol>
<p><strong>Synergized LLMs + KGs.</strong> The synergy of LLMs and KGs aims to integrate LLMs and KGs into a unified framework to mutually enhance each other. In this categorization, we review the recent attempts of Synergized LLMs + KGs from the perspectives of knowledge representation and reasoning.</p>
<p>In the following sections (Sec 4, 5, and 6), we will provide details on these categorizations.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Fine-grained categorization of research on unifying large language models (LLMs) with knowledge graphs (KGs).</p>
<h2>4 KG-enhanced LLMs</h2>
<p>Large language models (LLMs) achieve promising results in many natural language processing tasks. However, LLMs have been criticized for their lack of practical knowledge and tendency to generate factual errors during inference. To address this issue, researchers have proposed integrating knowledge graphs (KGs) to enhance LLMs. In this section, we first introduce the KG-enhanced LLM pre-training, which aims to inject knowledge into LLMs during the pre-training stage. Then, we introduce the KG-enhanced LLM inference, which enables LLMs to consider the latest knowledge while generating sentences. Finally, we introduce the KG-enhanced LLM interpretability, which aims to improve the interpretability of LLMs by using KGs. Table 2 summarizes the typical methods that integrate KGs for LLMs.</p>
<h3>4.1 KG-enhanced LLM Pre-training</h3>
<p>Existing large language models mostly rely on unsupervised training on the large-scale corpus. While these models may exhibit impressive performance on downstream tasks, they often lack practical knowledge relevant to the real world. Previous works that integrate KGs into large language models can be categorized into three parts: 1) <em>Integrating KGs into training objective</em>, 2) <em>Integrating KGs into LLM inputs</em>, and 3) <em>KGs Instruction-tuning</em>.</p>
<h4>4.1.1 Integrating KGs into Training Objective</h4>
<p>The research efforts in this category focus on designing novel knowledge-aware training objectives. An intuitive idea is to expose more knowledge entities in the pre-training objective. GLM [102] leverages the knowledge graph structure to assign a masking probability. Specifically, entities that can be reached within a certain number of hops are considered to be the most important entities for learning, and they are given a higher masking probability during pre-training. Furthermore, E-BERT [103] further controls the balance between the token-level and entity-level training losses. The training loss values are used as indications of the learning process for token and entity, which dynamically determines their ratio for the next training epochs. SKEP [124] also follows a similar fusion to inject sentiment knowledge during LLMs pre-training. SKEP first determines words with positive and negative sentiment by utilizing PMI along with a predefined set of seed sentiment words. Then, it assigns a higher masking probability to those identified.</p>
<p>TABLE 2
Summary of KG-enhanced LLM methods.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Method</th>
<th>Year</th>
<th>KG</th>
<th>Technique</th>
</tr>
</thead>
<tbody>
<tr>
<td>KG-enhanced LLM pre-training</td>
<td>ERNiff [35]</td>
<td>2019</td>
<td>E</td>
<td>Integrating KGs into Training Objective</td>
</tr>
<tr>
<td></td>
<td>LLM [102]</td>
<td>2020</td>
<td>E</td>
<td>Integrating KGs into Training Objective</td>
</tr>
<tr>
<td></td>
<td>Meet [103]</td>
<td>2020</td>
<td>M</td>
<td>Integrating KGs into Training Objective</td>
</tr>
<tr>
<td></td>
<td>KEPLER [40]</td>
<td>2021</td>
<td>E</td>
<td>Integrating KGs into Training Objective</td>
</tr>
<tr>
<td></td>
<td>Unsupervised LLM [104]</td>
<td>2021</td>
<td>M</td>
<td>Integrating KGs into Training Objective</td>
</tr>
<tr>
<td></td>
<td>KALA [105]</td>
<td>2022</td>
<td>M</td>
<td>Integrating KGs into Training Objective</td>
</tr>
<tr>
<td></td>
<td>SMILES [106]</td>
<td>2020</td>
<td>E</td>
<td>Integrating KGs into Training Objective</td>
</tr>
<tr>
<td></td>
<td>K-BERT [36]</td>
<td>2020</td>
<td>E + M</td>
<td>Integrating KGs into Language Model Inputs</td>
</tr>
<tr>
<td></td>
<td>CALARX [107]</td>
<td>2020</td>
<td>E</td>
<td>Integrating KGs into Language Model Inputs</td>
</tr>
<tr>
<td></td>
<td>ERNiff3.0 [101]</td>
<td>2021</td>
<td>E + M</td>
<td>Integrating KGs into Language Model Inputs</td>
</tr>
<tr>
<td></td>
<td>DILLIM [108]</td>
<td>2022</td>
<td>E</td>
<td>Integrating KGs into Language Model Inputs</td>
</tr>
<tr>
<td></td>
<td>KP-PLM [109]</td>
<td>2022</td>
<td>E</td>
<td>KGs Instruction-tuning</td>
</tr>
<tr>
<td></td>
<td>OnePronge [110]</td>
<td>2022</td>
<td>E + M</td>
<td>KGs Instruction-tuning</td>
</tr>
<tr>
<td></td>
<td>CHARRED [111]</td>
<td>2023</td>
<td>M</td>
<td>KGs Instruction-tuning</td>
</tr>
<tr>
<td></td>
<td>Rob [112]</td>
<td>2023</td>
<td>E</td>
<td>KGs Instruction-tuning</td>
</tr>
<tr>
<td>KG-enhanced LLM inference</td>
<td>KALA [113]</td>
<td>2019</td>
<td>E</td>
<td>KGs for LLM prehog</td>
</tr>
<tr>
<td></td>
<td>REALM [114]</td>
<td>2020</td>
<td>E</td>
<td>Removal augmented knowledge fusion</td>
</tr>
<tr>
<td></td>
<td>RAC [92]</td>
<td>2020</td>
<td>E</td>
<td>Removal augmented knowledge fusion</td>
</tr>
<tr>
<td></td>
<td>EMAT [115]</td>
<td>2022</td>
<td>E</td>
<td>Removal augmented knowledge fusion</td>
</tr>
<tr>
<td></td>
<td>Li et al. [64]</td>
<td>2023</td>
<td>C</td>
<td>KGs Prompting</td>
</tr>
<tr>
<td></td>
<td>Shaoheqi [65]</td>
<td>2023</td>
<td>E + M</td>
<td>KGs Prompting</td>
</tr>
<tr>
<td></td>
<td>ChatBots [116]</td>
<td>2023</td>
<td>E + M</td>
<td>KGs Prompting</td>
</tr>
<tr>
<td></td>
<td>LiJK [117]</td>
<td>2023</td>
<td>M + K + M</td>
<td>KGs Prompting</td>
</tr>
<tr>
<td>KG-enhanced LLM interpretability</td>
<td>LiAMA [54]</td>
<td>2019</td>
<td>E</td>
<td>KGs for LLM prehog</td>
</tr>
<tr>
<td></td>
<td>LPMoN [108]</td>
<td>2022</td>
<td>E</td>
<td>KGs for LLM prehog</td>
</tr>
<tr>
<td></td>
<td>Anngrenge [118]</td>
<td>2020</td>
<td>E</td>
<td>KGs for LLM prehog</td>
</tr>
<tr>
<td></td>
<td>ModLAWA [120]</td>
<td>2022</td>
<td>M</td>
<td>KGs for LLM prehog</td>
</tr>
<tr>
<td></td>
<td>LUM-layered [121]</td>
<td>2023</td>
<td>E + M</td>
<td>KGs for LLM prehog</td>
</tr>
<tr>
<td></td>
<td>KugTert [58]</td>
<td>2019</td>
<td>E</td>
<td>KGs for LLM analysis</td>
</tr>
<tr>
<td></td>
<td>Interpretion [122]</td>
<td>2021</td>
<td>E</td>
<td>KGs for LLM analysis</td>
</tr>
<tr>
<td></td>
<td>knowledge-awareness [28]</td>
<td>2021</td>
<td>E</td>
<td>KGs for LLM analysis</td>
</tr>
<tr>
<td></td>
<td>Shanks et al. [123]</td>
<td>2022</td>
<td>E</td>
<td>KGs for LLM analysis</td>
</tr>
</tbody>
</table>
<p>E: Encyclopedic Knowledge Graphs, E: Conformance Knowledge Graphs, M: Domain-Specific Knowledge Graphs.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. Injecting KG information into LLMs training objective via text-knowledge alignment loss, where <em>h</em> denotes the hidden representation generated by LLMs.</p>
<p>Sentiment words in the word masking objective.</p>
<p>The other line of work explicitly leverages the connections with knowledge and input text. As shown in Fig. 9, ERNIE [35] proposes a novel word-entity alignment training objective as a pre-training objective. Specifically, ERNIE feeds both sentences and corresponding entities mentioned in the text into LLMs, and then trains the LLMs to predict alignment links between textual tokens and entities in knowledge graphs. Similarly, KALM [91] enhances the input tokens by incorporating entity embeddings and includes an entity prediction pre-training task in addition to the token-only pre-training objective. This approach aims to improve the ability of LLMs to capture knowledge related to entities. Finally, KEPLER [40] directly employs both knowledge graph embedding training objective and Masked token pre-training objective into a shared transformer-based encoder. Deterministic LLM [104] focuses on pre-training language models to capture <em>deterministic</em> factual knowledge. It only masks the span that has a deterministic entity as the question and introduces additional clue contrast learning and clue classification objective. WKLM [106] first replaces entities in the text with other same-type entities and then feeds them into LLMs. The model is further pre-trained to distinguish whether the entities have been replaced or not.</p>
<h3>4.1.2 Integrating KGs into LLM Inputs</h3>
<p>As shown in Fig. 10, this kind of research focuses on introducing relevant knowledge sub-graph into the inputs of LLMs. Given a knowledge graph triple and the corresponding sentences, ERNIE 3.0 [101] represents the triple as a sequence of tokens and directly concatenates them with the sentences. It further randomly masks either the relation token in the triple or tokens in the sentences to better combine knowledge with textual representations. However, such direct knowledge triple concatenation method allows the tokens in the sentence to intensively interact with the tokens in the knowledge sub-graph, which could result in <em>Knowledge Noise</em> [36]. To solve this issue, K-BERT [36] takes the first step to inject the knowledge triple into the sentence via a <em>visible matrix</em> where only the knowledge entities have access to the knowledge triple information, while the tokens in the sentences can only see each other in the self-attention module. To further reduce <em>Knowledge Noise</em>, Colake [107] proposes a unified word-knowledge graph (shown in Fig. 10) where the tokens in the input sentences form a fully connected word graph where tokens aligned with knowledge entities are connected with their neighboring entities.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. Injecting KG information into LLMs inputs using graph structure.</p>
<p>The above methods can indeed inject a large amount of knowledge into LLMs. However, they mostly focus on popular entities and overlook the low-frequent and long-tail ones. DkLLM [108] aims to improve the LLMs representations towards those entities. DkLLM first proposes a novel measurement to determine long-tail entities and then replaces these selected entities in the text with pseudo token embedding as new input to the large language models. Furthermore, Dict-BERT [125] proposes to leverage external dictionaries to solve this issue. Specifically, Dict-BERT improves the representation quality of rare words by appending their definitions from the dictionary at the end of input text and trains the language model to locally align rare word representations in input sentences and dictionary definitions as well as to discriminate whether the input text and definition are correctly mapped.</p>
<h3>4.1.3 KGs Instruction-tuning</h3>
<p>Instead of injecting factual knowledge into LLMs, the KGs Instruction-tuning aims to fine-tune LLMs to better comprehend the structure of KGs and effectively follow user instructions to conduct complex tasks. KGs Instruction-tuning utilizes both facts and the structure of KGs to create instruction-tuning datasets. LLMs finetuned on these datasets can extract both factual and structural knowledge from KGs, enhancing the reasoning ability of LLMs. KPPLM [109] first designs several prompt templates to transfer structural graphs into natural language text. Then, two self-supervised tasks are proposed to finetune LLMs to further leverage the knowledge from these prompts. OntoPrompt [110] proposes an ontology-enhanced prompt-tuning that can place knowledge of entities into the context of LLMs, which are further finetuned on several downstream tasks. ChatKBQA [111] finetunes LLMs on KG structure to generate logical queries, which can be executed on KGs to obtain answers. To better reason on graphs, RoG [112] presents a planning-retrieval-reasoning framework. RoG is finetuned on KG structure to generate relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid</p>
<p>reasoning paths from the KGs for LLMs to conduct faithful reasoning and generate interpretable results.</p>
<p>KGs Instruction-tuning can better leverage the knowledge from KGs for downstream tasks. However, it requires retraining the models, which is time-consuming and requires lots of resources.</p>
<h3>4.2 KG-enhanced LLM Inference</h3>
<p>The above methods could effectively fuse knowledge into LLMs. However, real-world knowledge is subject to change and the limitation of these approaches is that they do not permit updates to the incorporated knowledge without retraining the model. As a result, they may not generalize well to the unseen knowledge during inference [126]. Therefore, considerable research has been devoted to keeping the knowledge space and text space separate and injecting the knowledge while inference. These methods mostly focus on the Question Answering (QA) tasks, because QA requires the model to capture both textual semantic meanings and up-to-date real-world knowledge.</p>
<h3>4.2.1 Retrieval-Augmented Knowledge Fusion</h3>
<p>Retrieval-Augmented Knowledge Fusion is a popular method to inject knowledge into LLMs during inference. The key idea is to retrieve relevant knowledge from a large corpus and then fuse the retrieved knowledge into LLMs. As shown in Fig. 11, RAG [92] proposes to combine nonparametric and parametric modules to handle the external knowledge. Given the input text, RAG first searches for relevant KG in the non-parametric module via MIPS to obtain several documents. RAG then treats these documents as hidden variables $z$ and feeds them into the output generator, empowered by Seq2Seq LLMs, as additional context information. The research indicates that using different retrieved documents as conditions at different generation steps performs better than only using a single document to guide the whole generation process. The experimental results show that RAG outperforms other parametric-only and non-parametric-only baseline models in open-domain QA. RAG can also generate more specific, diverse, and factual text than other parameter-only baselines. Story-fragments [127] further improves architecture by adding an additional module to determine salient knowledge entities and fuse them into the generator to improve the quality of generated long stories. EMAT [115] further improves the efficiency of such a system by encoding external knowledge into a keyvalue memory and exploiting the fast maximum inner product search for memory querying. REALM [114] proposes a novel knowledge retriever to help the model to retrieve and attend over documents from a large corpus during the pretraining stage and successfully improves the performance of open-domain question answering. KGLM [113] selects the facts from a knowledge graph using the current context to generate factual sentences. With the help of an external knowledge graph, KGLM could describe facts using out-ofdomain words or phrases.</p>
<h3>4.2.2 KGs Prompting</h3>
<p>To better feed the KG structure into the LLM during inference, KGs prompting aims to design a crafted prompt that</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11. Retrieving external knowledge to enhance the LLM generation.
converts structured KGs into text sequences, which can be fed as context into LLMs. In this way, LLMs can better take advantage of the structure of KGs to perform reasoning. Li et al. [64] adopt the pre-defined template to convert each triple into a short sentence, which can be understood by LLMs for reasoning. Mindmap [65] designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning by consolidating the facts in KGs and the implicit knowledge from LLMs. ChatRule [116] samples several relation paths from KGs, which are verbalized and fed into LLMs. Then, LLMs are prompted to generate meaningful logical rules that can be used for reasoning. CoK [117] proposes a chain-of-knowledge prompting that uses a sequence of triples to elicit the reasoning ability of LLMs to reach the final answer.</p>
<p>KGs prompting presents a simple way to synergize LLMs and KGs. By using the prompt, we can easily harness the power of LLMs to perform reasoning based on KGs without retraining the models. However, the prompt is usually designed manually, which requires lots of human effort.</p>
<h3>4.3 Comparison between KG-enhanced LLM Pretraining and Inference</h3>
<p>KG-enhanced LLM Pre-training methods commonly enrich large-amount of unlabeled corpus with semantically relevant real-world knowledge. These methods allow the knowledge representations to be aligned with appropriate linguistic context and explicitly train LLMs to leverage those knowledge from scratch. When applying the resulting LLMs to downstream knowledge-intensive tasks, they should achieve optimal performance. In contrast, KGenhanced LLM inference methods only present the knowledge to LLMs in the inference stage and the underlying LLMs may not be trained to fully leverage these knowledge when conducting downstream tasks, potentially resulting in sub-optimal model performance.</p>
<p>However, real-world knowledge is dynamic and requires frequent updates. Despite being effective, the KG-enhanced LLM Pre-training methods never permit knowledge updates or editing without model re-training. As a result, the KG-enhanced LLM Pre-training methods could generalize poorly to recent or unseen knowledge. KG-enhanced LLM inference methods can easily maintain knowledge updates by changing the inference inputs. These methods help improve LLMs performance on new knowledge and domains.</p>
<p>In summary, when to use these methods depends on the application scenarios. If one wishes to apply LLMs to han-</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 12. The general framework of using knowledge graph for language model probing.
dle time-insensitive knowledge in particular domains (e.g., commonsense and reasoning knowledge), KG-enhanced LLM Pre-training methods should be considered. Otherwise, KG-enhanced LLM inference methods can be used to handle open-domain knowledge with frequent updates.</p>
<h3>4.4 KG-enhanced LLM Interpretability</h3>
<p>Although LLMs have achieved remarkable success in many NLP tasks, they are still criticized for their lack of interpretability. The large language model (LLM) interpretability refers to the understanding and explanation of the inner workings and decision-making processes of a large language model [17]. This can improve the trustworthiness of LLMs and facilitate their applications in high-stakes scenarios such as medical diagnosis and legal judgment. Knowledge graphs (KGs) represent the knowledge structurally and can provide good interpretability for the reasoning results. Therefore, researchers try to utilize KGs to improve the interpretability of LLMs, which can be roughly grouped into two categories: 1) KGs for language model probing, and 2) KGs for language model analysis.</p>
<h3>4.4.1 KGs for LLM Probing</h3>
<p>The large language model (LLM) probing aims to understand the knowledge stored in LLMs. LLMs, trained on large-scale corpus, are often known as containing enormous knowledge. However, LLMs store the knowledge in a hidden way, making it hard to figure out the stored knowledge. Moreover, LLMs suffer from the hallucination problem [15], which results in generating statements that contradict facts. This issue significantly affects the reliability of LLMs. Therefore, it is necessary to probe and verify the knowledge stored in LLMs.</p>
<p>LAMA [14] is the first work to probe the knowledge in LLMs by using KGs. As shown in Fig. 12, LAMA first converts the facts in KGs into cloze statements by a predefined prompt template and then uses LLMs to predict the missing entity. The prediction results are used to evaluate the knowledge stored in LLMs. For example, we try to probe whether LLMs know the fact (Obama, profession, president). We first convert the fact triple into a cloze question "Obama's profession is _." with the object masked. Then, we test if the LLMs can predict the object "president" correctly.</p>
<p>However, LAMA ignores the fact that the prompts are inappropriate. For example, the prompt "Obama worked as a _" may be more favorable to the prediction of the blank by the language models than "Obama is a _ by profession".
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 13. The general framework of using knowledge graph for language model analysis.</p>
<p>Thus, LPAQA [118] proposes a mining and paraphrasingbased method to automatically generate high-quality and diverse prompts for a more accurate assessment of the knowledge contained in the language model. Moreover, Adolphs et al. [128] attempt to use examples to make the language model understand the query, and experiments obtain substantial improvements for BERT-large on the TREx data. Unlike using manually defined prompt templates, Autoprompt [119] proposes an automated method, which is based on the gradient-guided search to create prompts. LLM-facteval [121] designs a systematic framework that automatically generates probing questions from KGs. The generated questions are then used to evaluate the factual knowledge stored in LLMs.</p>
<p>Instead of probing the general knowledge by using the encyclopedic and commonsense knowledge graphs, BioLAMA [129] and MedLAMA [120] probe the medical knowledge in LLMs by using medical knowledge graphs. Alex et al. [130] investigate the capacity of LLMs to retain less popular factual knowledge. They select unpopular facts from Wikidata knowledge graphs which have lowfrequency clicked entities. These facts are then used for the evaluation, where the results indicate that LLMs encounter difficulties with such knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail.</p>
<h3>4.4.2 KGs for LLM Analysis</h3>
<p>Knowledge graphs (KGs) for pre-train language models (LLMs) analysis aims to answer the following questions such as "how do LLMs generate the results?", and "how do the function and structure work in LLMs?". To analyze the inference process of LLMs, as shown in Fig. 13, KagNet [38] and QA-GNN [131] make the results generated by LLMs at each reasoning step grounded by knowledge graphs. In this way, the reasoning process of LLMs can be explained by extracting the graph structure from KGs. Shaobo et al. [123] investigate how LLMs generate the results correctly. They adopt the causal-inspired analysis from facts extracted from KGs. This analysis quantitatively measures the word patterns that LLMs depend on to generate the results. The results show that LLMs generate the missing factual more by the positionally closed words rather than the knowledgedependent words. Thus, they claim that LLMs are inadequate to memorize factual knowledge because of the inaccurate dependence. To interpret the training of LLMs, Swamy</p>
<p>TABLE 3
Summary of representative LLM-augmented KG methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Year</th>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Technique</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLM-augmented KG embedding</td>
<td style="text-align: center;">Pietrasic-KGE [99]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">LLMs as Text Encoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KEPLER [49]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">LLMs as Text Encoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pietrasic et al. [132]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">LLM-augmented KG</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Huang et al. [135]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">LLMs as Text Encoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CalliG [134]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">LLMs as Text Encoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LMiK [139]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">LLMs for Joint Text and KG Embedding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">kNN-KGE [136]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">LLMs for Joint Text and KG Embedding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lamisulici [137]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">E + E + KD</td>
<td style="text-align: center;">LLMs for Joint Text and KG Embedding</td>
</tr>
<tr>
<td style="text-align: center;">LLM-augmented KG completion</td>
<td style="text-align: center;">KG-BERT [26]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Joint Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">kNN-KGE [138]</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Joint Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PIGG [139]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Joint Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LAM [140]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Joint Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MEM-KGC [141]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">MLM Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OpenWorld-KGC [142]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">MLM Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NoM [143]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Separated Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NonKGC [144]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Separated Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LP-BERT [145]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Separated Encoding</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GenKGC [96]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">LLM on decoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KLEP [146]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">LLM on decoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KG-SJD [147]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">LLM on decoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ananiki [151]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">LLM on Decoders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ELMO [148]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Nested Entity Recognition</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GenerativeSVR [149]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">Nested Entity Recognition</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LDET [150]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Entity Typing</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M-NaTspan [151]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Entity Typing</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kcg [152]</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Entity Linking</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RefvaKit [153]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Entity Linking</td>
</tr>
<tr>
<td style="text-align: center;">LLM-augmented KG construction</td>
<td style="text-align: center;">RooCK [154]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">CR (Entire Document)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rastero [155]</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">CR (Written document)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHLM [156]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">CR (Cross-document)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Crooof-R [157]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">CR (Cross-document)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CR-BL [158]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">CR (Cross-document)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NonBE [159]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">BE (Sentence-level)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Curriculum-BS [160]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">BE (Sentence-level)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBSS-NM [161]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">BE (Documented document)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kumar et al. [99]</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">End-to-End Construction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLM-augmented KG</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">End-to-End Construction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Couplan [162]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">End-to-End Construction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FIVE [163]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">D + KD</td>
<td style="text-align: center;">End-to-End Construction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMET [164]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Distilling KGs from LLMs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RooNest [165]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Distilling KGs from LLMs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nass et al. [166]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Distilling KGs from LLMs</td>
</tr>
<tr>
<td style="text-align: center;">LLM-augmented KG-to-text Generation</td>
<td style="text-align: center;">Ribeiro et al. [167]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">Leveraging Knowledge from LLMs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RIBG-Novo [168]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">D + KD</td>
<td style="text-align: center;">Leveraging Knowledge from LLMs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LoSP [169]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">Leveraging Knowledge from LLMs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gowhita [170]</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Constructing KG text aligned Corpus</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KGPY [171]</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">KD</td>
<td style="text-align: center;">Constructing KG text aligned Corpus</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Labovnikov et al. [172]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Entity-Netation Extractor</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nass et al. [173]</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Entity-Netation Extractor</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QA-LNNI [174]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Entity-Netation Extractor</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nao et al. [176]</td>
<td style="text-align: center;">2019</td>
<td style="text-align: center;">E + D + KD</td>
<td style="text-align: center;">Entity-Netation Extractor</td>
</tr>
<tr>
<td style="text-align: center;">LLM-augmented KGQA</td>
<td style="text-align: center;">DEKG-NI [175]</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Answer Reunions</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRLK [177]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Answer Reunions</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRAI-NI [178]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Answer Reunions</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GroosKK [179]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Answer Reunions</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Res3MLG [178]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Answer Reunions</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LinkKGQA [43]</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">Answer Reunions</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">K-Fincolor-only LLMs, D- Uncolor-only LLMs, KD- Encolor-encoder LLMs.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>et al. [122] adopt the language model during pre-training to generate knowledge graphs. The knowledge acquired by LLMs during training can be unveiled by the facts in KGs explicitly. To explore how implicit knowledge is stored in parameters of LLMs, Dai et al. [39] propose the concept of knowledge neurons. Specifically, activation of the identified knowledge neurons is highly correlated with knowledge expression. Thus, they explore the knowledge and facts represented by each neuron by suppressing and amplifying knowledge neurons.</p>
<h2>5 LLM-AUGMENTED KGs</h2>
<p>Knowledge graphs are famous for representing knowledge in a structural manner. They have been applied in many downstream tasks such as question answering, recommendation, and web search. However, the conventional KGs are often incomplete and existing methods often lack considering textual information. To address these issues, recent research has explored integrating LLMs to augment KGs to consider the textual information and improve the performance in downstream tasks. In this section, we will introduce the recent research on LLM-augmented KGs. We will introduce the methods that integrate LLMs for KG embedding, KG completion, KG construction, KG-to-text generation, and KG question answering, respectively. Representative works are summarized in Table 3.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Fig. 14. LLMs as text encoder for knowledge graph embedding (KGE).</p>
<h3>5.1 LLM-augmented KG Embedding</h3>
<p>Knowledge graph embedding (KGE) aims to map each entity and relation into a low-dimensional vector (embedding) space. These embeddings contain both semantic and structural information of KGs, which can be utilized for various tasks such as question answering [180], reasoning [38], and recommendation [181]. Conventional knowledge graph embedding methods mainly rely on the structural information of KGs to optimize a scoring function defined on embeddings (e.g., TransE [33], and DisMult [182]). However, these approaches often fall short in representing unseen entities and long-tailed relations due to their limited structural connectivity [183], [184]. To address this issue, as shown in Fig. 14, recent research adopts LLMs to enrich representations of KGs by encoding the textual descriptions of entities and relations [40], [94].</p>
<h3>5.1.1 LLMs as Text Encoders</h3>
<p>Pretrain-KGE [94] is a representative method that follows the framework shown in Fig. 14. Given a triple $(h, r, t)$ from KGs, it firsts uses a LLM encoder to encode the textual descriptions of entities $h, t$, and relations $r$ into representations as</p>
<p>$$
e_{h}=\operatorname{LLM}\left(\operatorname{Text}<em t="t">{h}\right), e</em>}=\operatorname{LLM}\left(\operatorname{Text<em r="r">{t}\right), e</em>\right)
$$}=\operatorname{LLM}\left(\operatorname{Text}_{r</p>
<p>where $e_{h}, e_{r}$, and $e_{t}$ denotes the initial embeddings of entities $h, t$, and relations $r$, respectively. Pretrain-KGE uses the BERT as the LLM encoder in experiments. Then, the initial embeddings are fed into a KGE model to generate the final embeddings $v_{h}, v_{r}$, and $v_{t}$. During the KGE training phase, they optimize the KGE model by following the standard KGE loss function as</p>
<p>$$
\mathcal{L}=\left[\gamma+f\left(v_{h}, v_{r}, v_{t}\right)-f\left(v_{h}^{\prime}, v_{r}^{\prime}, v_{t}^{\prime}\right)\right]
$$</p>
<p>where $f$ is the KGE scoring function, $\gamma$ is a margin hyperparameter, and $v_{h}^{\prime}, v_{r}^{\prime}$, and $v_{t}^{\prime}$ are the negative samples. In this way, the KGE model could learn adequate structure information, while reserving partial knowledge from LLM enabling better knowledge graph embedding. KEPLER [40] offers a unified model for knowledge embedding and pre-trained language representation. This model not only generates effective text-enhanced knowledge embedding</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Fig. 15. LLMs for joint text and knowledge graph embedding.
using powerful LLMs but also seamlessly integrates factual knowledge into LLMs. Nayyeri et al. [132] use LLMs to generate the world-level, sentence-level, and document-level representations. They are integrated with graph structure embeddings into a unified vector by Dihedron and Quaternion representations of 4D hypercomplex numbers. Huang et al. [133] combine LLMs with other vision and graph encoders to learn multi-modal knowledge graph embedding that enhances the performance of downstream tasks. CoDEx [134] presents a novel loss function empowered by LLMs that guides the KGE models in measuring the likelihood of triples by considering the textual information. The proposed loss function is agnostic to model structure that can be incorporated with any KGE model.</p>
<h3>5.1.2 LLMs for Joint Text and KG Embedding</h3>
<p>Instead of using KGE model to consider graph structure, another line of methods directly employs LLMs to incorporate both the graph structure and textual information into the embedding space simultaneously. As shown in Fig. 15, $k \mathrm{NN}-\mathrm{KGE}$ [136] treats the entities and relations as special tokens in the LLM. During training, it transfers each triple $(h, r, t)$ and corresponding text descriptions into a sentence $x$ as</p>
<p>$$
x=\left[\begin{array}{llll}
\text { CLS } &amp; h \text { Text }<em t="t">{h}[\text { SEP }] &amp; r[\text { SEP }] &amp; {[\text { MASK }]} \text { Text }</em>]
\end{array}\right.
$$}[\text { SEP </p>
<p>where the tailed entities are replaced by [MASK]. The sentence is fed into a LLM, which then finetunes the model to predict the masked entity, formulated as</p>
<p>$$
P_{L L M}(t \mid h, r)=P({\mathrm{MASK}}=\mathrm{t} \mid x, \Theta)
$$</p>
<p>where $\Theta$ denotes the parameters of the LLM. The LLM is optimized to maximize the probability of the correct entity $t$. After training, the corresponding token representations in LLMs are used as embeddings for entities and relations. Similarly, LMKE [135] proposes a contrastive learning method to improve the learning of embeddings generated by LLMs for KGE. Meanwhile, to better capture graph structure, LambdaKG [137] samples 1-hop neighbor entities and concatenates their tokens with the triple as a sentence feeding into LLMs.</p>
<h3>5.2 LLM-augmented KG Completion</h3>
<p>Knowledge Graph Completion (KGC) refers to the task of inferring missing facts in a given knowledge graph. Similar to KGE, conventional KGC methods mainly focused on
the structure of the KG, without considering the extensive textual information. However, the recent integration of LLMs enables KGC methods to encode text or generate facts for better KGC performance. These methods fall into two distinct categories based on their utilization styles: 1) LLM as Encoders (PaE), and 2) LLM as Generators (PaG).</p>
<h3>5.2.1 LLM as Encoders (PaE).</h3>
<p>As shown in Fig. 16 (a), (b), and (c), this line of work first uses encoder-only LLMs to encode textual information as well as KG facts. Then, they predict the plausibility of the triples or masked entities by feeding the encoded representation into a prediction head, which could be a simple MLP or conventional KG score function (e.g., TransE [33] and TransR [185]).</p>
<p>Joint Encoding. Since the encoder-only LLMs (e.g., Bert [1]) are well at encoding text sequences, KG-BERT [26] represents a triple $(h, r, t)$ as a text sequence and encodes it with LLM Fig. 16(a).</p>
<p>$$
x=\left[\begin{array}{llll}
\text { CLS } &amp; \text { Text }<em r="r">{h} &amp; {[\text { SEP }]} \text { Text }</em>,
\end{array}\right.
$$} &amp; {[\text { SEP }]} \text { Text }_{t} &amp; {[\text { SEP }]</p>
<p>The final hidden state of the [CLS] token is fed into a classifier to predict the possibility of the triple, formulated as</p>
<p>$$
s=\sigma\left(\operatorname{MLP}\left(e_{[\mathrm{CLS}]}\right)\right)
$$</p>
<p>where $\sigma(\cdot)$ denotes the sigmoid function and $e_{[\mathrm{CLS}]}$ denotes the representation encoded by LLMs. To improve the efficacy of KG-BERT, MTL-KGC [138] proposed a MultiTask Learning for the KGC framework which incorporates additional auxiliary tasks into the model's training, i.e. prediction (RP) and relevance ranking (RR). PKGC [139] assesses the validity of a triplet $(h, r, t)$ by transforming the triple and its supporting information into natural language sentences with pre-defined templates. These sentences are then processed by LLMs for binary classification. The supporting information of the triplet is derived from the attributes of $h$ and $t$ with a verbalizing function. For instance, if the triple is (Lebron James, member of sports team, Lakers), the information regarding Lebron James is verbalized as "Lebron James: American basketball player". LASS [140] observes that language semantics and graph structures are equally vital to KGC. As a result, LASS is proposed to jointly learn two types of embeddings: semantic embedding and structure embedding. In this method, the full text of a triple is forwarded to the LLM, and the mean pooling of the corresponding LLM outputs for $h, r$, and $t$ are separately calculated. These embeddings are then passed to a graphbased method, i.e. TransE, to reconstruct the KG structures.</p>
<p>MLM Encoding. Instead of encoding the full text of a triple, many works introduce the concept of Masked Language Model (MLM) to encode KG text (Fig. 16(b)). MEMKGC [141] uses Masked Entity Model (MEM) classification mechanism to predict the masked entities of the triple. The input text is in the form of</p>
<p>$$
x=\left[\begin{array}{llll}
\text { CLS } &amp; \text { Text }<em r="r">{h} &amp; {[\text { SEP }]} \text { Text }</em> \
&amp; \text { [MASK }
\end{array}\right. \text { [SEP] }
$$} &amp; {[\text { SEP }]</p>
<p>Similar to Eq. 4, it tries to maximize the probability that the masked entity is the correct entity $t$. Additionally, to enable the model to learn unseen entities, MEM-KGC integrates</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Fig. 16. The general framework of adopting LLMs as encoders (PaE) for KG Completion.</p>
<p>multitask learning for entities and super-class prediction based on the text description of entities:</p>
<p>$$x = [\text{CLS}] \text{ [MASK]} \text{ [SEP]} \text{ Text}_h \text{ [SEP]}.\qquad (8)$$</p>
<p>OpenWorld KGC [142] expands the MEM-KGC model to address the challenges of open-world KGC with a pipeline framework, where two sequential MLM-based modules are defined: Entity Description Prediction (EDP), an auxiliary module that predicts a corresponding entity with a given textual description; Incomplete Triple Prediction (ITP), the target module that predicts a plausible entity for a given incomplete triple (h, r, ?). EDP first encodes the triple with Eq. 8 and generates the final hidden state, which is then forwarded into ITP as an embedding of the head entity in Eq. 7 to predict target entities.</p>
<p><strong>Separated Encoding.</strong> As shown in Fig. 16(c), these methods involve partitioning a triple (h, r, t) into two distinct parts, i.e. (h, r) and t, which can be expressed as</p>
<p>$$x_{(h,r)} = [\text{CLS}] \text{ Text}_h \text{ [SEP]} \text{ Text}_r \text{ [SEP]},\tag{9}$$</p>
<p>$$x_t = [\text{CLS}] \text{ Text}_t \text{ [SEP]}.\tag{10}$$</p>
<p>Then the two parts are encoded separately by LLMs, and the final hidden states of the [CLS] tokens are used as the representations of (h, r) and t, respectively. The representations are then fed into a scoring function to predict the possibility of the triple, formulated as</p>
<p>$$s = f_{\text{score}}(e_{(h,r)}, e_t),\tag{11}$$</p>
<p>where fscore denotes the score function like TransE.</p>
<p>StAR [143] applies Siamese-style textual encoders on their text, encoding them into separate contextualized representations. To avoid the combinatorial explosion of textual encoding approaches, e.g., KG-BERT, StAR employs a scoring module that involves both deterministic classifier and spatial measurement for representation and structure learning respectively, which also enhances structured knowledge by exploring the spatial characteristics. SimKGC [144] is</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Fig. 17. The general framework of adopting LLMs as decoders (PaG) for KG Completion. The En. and De. denote the encoder and decoder, respectively.</p>
<p>another instance of leveraging a Siamese textual encoder to encode textual representations. Following the encoding process, SimKGC applies contrastive learning techniques to these representations. This process involves computing the similarity between the encoded representations of a given triple and its positive and negative samples. In particular, the similarity between the encoded representation of the triple and the positive sample is maximized, while the similarity between the encoded representation of the triple and the negative sample is minimized. This enables SimKGC to learn a representation space that separates plausible and implausible triples. To avoid overfitting textural information, CSPromp-KG [186] employs parameter-efficient prompt learning for KGC.</p>
<p>LP-BERT [145] is a hybrid KGC method that combines both MLM Encoding and Separated Encoding. This approach consists of two stages, namely pre-training and fine-tuning. During pre-training, the method utilizes the standard MLM mechanism to pre-train a LLM with KGC data. During the fine-tuning stage, the LLM encodes both parts and is optimized using a contrastive learning strategy (similar to SimKGC [144]).</p>
<h3>5.2.2 LLM as Generators (PaG).</h3>
<p>Recent works use LLMs as sequence-to-sequence generators in KGC. As presented in Fig. 17 (a) and (b), these approaches involve encoder-decoder or decoder-only LLMs. The LLMs receive a sequence text input of the query triple (h, r, ?), and generate the text of tail entity t directly.</p>
<p>GenKGC [96] uses the large language model BART [5] as the backbone model. Inspired by the in-context learning approach used in GPT-3 [59], where the model concatenates relevant samples to learn correct output answers, GenKGC proposes a relation-guided demonstration technique that includes triples with the same relation to facilitating the model's learning process. In addition, during generation, an entity-aware hierarchical decoding method is proposed to reduce the time complexity. KGT5 [146] introduces a</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Fig. 18. The framework of prompt-based PaG for KG Completion.
novel KGC model that fulfils four key requirements of such models: scalability, quality, versatility, and simplicity. To address these objectives, the proposed model employs a straightforward T5 small architecture. The model is distinct from previous KGC methods, in which it is randomly initialized rather than using pre-trained models. KG-S2S [147] is a comprehensive framework that can be applied to various types of KGC tasks, including Static KGC, Temporal KGC, and Few-shot KGC. To achieve this objective, KG-S2S reformulates the standard triple KG fact by introducing an additional element, forming a quadruple $(h, r, t, m)$, where $m$ represents the additional "condition" element. Although different KGC tasks may refer to different conditions, they typically have a similar textual format, which enables unification across different KGC tasks. The KG-S2S approach incorporates various techniques such as entity description, soft prompt, and Seq2Seq Dropout to improve the model's performance. In addition, it utilizes constrained decoding to ensure the generated entities are valid. For closed-source LLMs (e.g., ChatGPT and GPT-4), AutoKG adopts prompt engineering to design customized prompts [93]. As shown in Fig. 18, these prompts contain the task description, fewshot examples, and test input, which instruct LLMs to predict the tail entity for KG completion.</p>
<p>Comparison between PaE and PaG. LLMs as Encoders (PaE) applies an additional prediction head on the top of the representation encoded by LLMs. Therefore, the PaE framework is much easier to finetune since we can only optimize the prediction heads and freeze the LLMs. Moreover, the output of the prediction can be easily specified and integrated with existing KGC functions for different KGC tasks. However, during the inference stage, the PaE requires to compute a score for every candidate in KGs, which could be computationally expensive. Besides, they cannot generalize to unseen entities. Furthermore, the PaE requires the representation output of the LLMs, whereas some state-of-the-art LLMs (e.g. GPT-4 ${ }^{1}$ ) are closed sources and do not grant access to the representation output.</p>
<p>LLMs as Generators (PaG), on the other hand, which does not need the prediction head, can be used without finetuning or access to representations. Therefore, the framework of PaG is suitable for all kinds of LLMs. In addition, PaG directly generates the tail entity, making it efficient
in inference without ranking all the candidates and easily generalizing to unseen entities. But, the challenge of PaG is that the generated entities could be diverse and not lie in KGs. What is more, the time of a single inference is longer due to the auto-regressive generation. Last, how to design a powerful prompt that feeds KGs into LLMs is still an open question. Consequently, while PaG has demonstrated promising results for KGC tasks, the trade-off between model complexity and computational efficiency must be carefully considered when selecting an appropriate LLMbased KGC framework.</p>
<h3>5.2.3 Model Analysis</h3>
<p>Justin et al. [187] provide a comprehensive analysis of KGC methods integrated with LLMs. Their research investigates the quality of LLM embeddings and finds that they are suboptimal for effective entity ranking. In response, they propose several techniques for processing embeddings to improve their suitability for candidate retrieval. The study also compares different model selection dimensions, such as Embedding Extraction, Query Entity Extraction, and Language Model Selection. Lastly, the authors propose a framework that effectively adapts LLM for knowledge graph completion.</p>
<h3>5.3 LLM-augmented KG Construction</h3>
<p>Knowledge graph construction involves creating a structured representation of knowledge within a specific domain. This includes identifying entities and their relationships with each other. The process of knowledge graph construction typically involves multiple stages, including 1) entity discovery, 2) coreference resolution, and 3) relation extraction. Fig 19 presents the general framework of applying LLMs for each stage in KG construction. More recent approaches have explored 4) end-to-end knowledge graph construction, which involves constructing a complete knowledge graph in one step or directly 5) distilling knowledge graphs from LLMs.</p>
<h3>5.3.1 Entity Discovery</h3>
<p>Entity discovery in KG construction refers to the process of identifying and extracting entities from unstructured data sources, such as text documents, web pages, or social media posts, and incorporating them to construct knowledge graphs.</p>
<p>Named Entity Recognition (NER) involves identifying and tagging named entities in text data with their positions and classifications. The named entities include people, organizations, locations, and other types of entities. The state-of-the-art NER methods usually employ LLMs to leverage their contextual understanding and linguistic knowledge for accurate entity recognition and classification. There are three NER sub-tasks based on the types of NER spans identified, i.e., flat NER, nested NER, and discontinuous NER. 1) Flat NER is to identify non-overlapping named entities from input text. It is usually conceptualized as a sequence labelling problem where each token in the text is assigned a unique label based on its position in the sequence [1], [148], [188], [189]. 2) Nested NER considers complex scenarios which allow a token to belong to multiple entities. The spanbased method [190]-[194] is a popular branch of nested</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Fig. 19. The general framework of LLM-based KG construction.</p>
<p>NER which involves enumerating all candidate spans and classifying them into entity types (including a non-entity type). Parsing-based methods [195]-[197] reveal similarities between nested NER and constituency parsing tasks (predicting nested and non-overlapping spans), and propose to integrate the insights of constituency parsing into nested NER. 3) Discontinuous NER identifies named entities that may not be contiguous in the text. To address this challenge, [198] uses the LLM output to identify entity fragments and determine whether they are overlapped or in succession.</p>
<p>Unlike the task-specific methods, GenerativeNER [149] uses a sequence-to-sequence LLM with a pointer mechanism to generate an entity sequence, which is capable of solving all three types of NER sub-tasks.</p>
<p>Entity Typing (ET) aims to provide fine-grained and ultra-grained type information for a given entity mentioned in context. These methods usually utilize LLM to encode mentions, context and types. LDET [150] applies pretrained ELMo embeddings [148] for word representation and adopts LSTM as its sentence and mention encoders. BOX4Types [151] recognizes the importance of type dependency and uses BERT to represent the hidden vector and each type in a hyperrectangular (box) space. LRN [199] considers extrinsic and intrinsic dependencies between labels. It encodes the context and entity with BERT and employs these output embeddings to conduct deductive and inductive reasoning. MLMET [200] uses predefined patterns to construct input samples for the BERT MLM and employs [MASK] to predict context-dependent hypernyms of the mention, which can be viewed as type labels. PL [201] and DFET [202] utilize prompt learning for entity typing. LITE [203] formulates entity typing as textual inference and uses RoBERTa-large-MNLI as the backbone network.</p>
<p>Entity Linking (EL), as known as entity disambiguation, involves linking entity mentions appearing in the text to their corresponding entities in a knowledge graph. [204] proposed BERT-based end-to-end EL systems that jointly discover and link entities. ELQ [152] employs a fast biencoder architecture to jointly perform mention detection
and linking in one pass for downstream question answering systems. Unlike previous models that frame EL as matching in vector space, GENRE [205] formulates it as a sequence-tosequence problem, autoregressively generating a version of the input markup-annotated with the unique identifiers of an entity expressed in natural language. GENRE is extended to its multilingual version mGENRE [206]. Considering the efficiency challenges of generative EL approaches, [207] parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. ReFinED [153] proposes an efficient zero-shot-capable EL approach by taking advantage of fine-grained entity types and entity descriptions which are processed by a LLM-based encoder.</p>
<h3>5.3.2 Coreference Resolution (CR)</h3>
<p>Coreference resolution is to find all expressions (i.e., mentions) that refer to the same entity or event in a text.</p>
<p>Within-document CR refers to the CR sub-task where all these mentions are in a single document. Mandar et al. [154] initialize LLM-based coreferences resolution by replacing the previous LSTM encoder [208] with BERT. This work is followed by the introduction of SpanBERT [155] which is pre-trained on BERT architecture with a span-based masked language model (MLM). Inspired by these works, Tuan Manh et al. [209] present a strong baseline by incorporating the SpanBERT encoder into a non-LLM approach e2ecoref [208]. CorefBERT leverages Mention Reference Prediction (MRP) task which masks one or several mentions and requires the model to predict the masked mention's corresponding referents. CorefQA [210] formulates coreference resolution as a question answering task, where contextual queries are generated for each candidate mention and the coreferent spans are extracted from the document using the queries. Tuan Manh et al. [211] introduce a gating mechanism and a noisy training method to extract information from event mentions using the SpanBERT encoder.</p>
<p>In order to reduce the large memory footprint faced by large LLM-based NER models, Yuval et al. [212] and Raghuveer el al. [213] proposed start-to-end and approximation models, respectively, both utilizing bilinear functions to calculate mention and antecedent scores with reduced reliance on span-level representations.</p>
<p>Cross-document CR refers to the sub-task where the mentions refer to the same entity or event might be across multiple documents. CDML [156] proposes a cross document language modeling method which pre-trains a Longformer [214] encoder on concatenated related documents and employs an MLP for binary classification to determine whether a pair of mentions is coreferent or not. CrossCR [157] utilizes an end-to-end model for cross-document coreference resolution which pre-trained the mention scorer on gold mention spans and uses a pairwise scorer to compare mentions with all spans across all documents. CR-RL [158] proposes an actor-critic deep reinforcement learning-based coreference resolver for cross-document CR.</p>
<h3>5.3.3 Relation Extraction (RE)</h3>
<p>Relation extraction involves identifying semantic relationships between entities mentioned in natural language text. There are two types of relation extraction methods, i.e.</p>
<p>sentence-level RE and document-level RE, according to the scope of the text analyzed.</p>
<p>Sentence-level RE focuses on identifying relations between entities within a single sentence. Peng et al. [159] and TRE [215] introduce LLM to improve the performance of relation extraction models. BERT-MTB [216] learns relation representations based on BERT by performing the matching-the-blanks task and incorporating designed objectives for relation extraction. Curriculum-RE [160] utilizes curriculum learning to improve relation extraction models by gradually increasing the difficulty of the data during training. RECENT [217] introduces SpanBERT and exploits entity type restriction to reduce the noisy candidate relation types. Jiewen [218] extends RECENT by combining both the entity information and the label information into sentence-level embeddings, which enables the embedding to be entitylabel aware.</p>
<p>Document-level RE (DocRE) aims to extract relations between entities across multiple sentences within a document. Hong et al. [219] propose a strong baseline for DocRE by replacing the BiLSTM backbone with LLMs. HIN [220] use LLM to encode and aggregate entity representation at different levels, including entity, sentence, and document levels. GLRE [221] is a global-to-local network, which uses LLM to encode the document information in terms of entity global and local representations as well as context relation representations. SIRE [222] uses two LLM-based encoders to extract intra-sentence and inter-sentence relations. LSR [223] and GAIN [224] propose graph-based approaches which induce graph structures on top of LLM to better extract relations. DocuNet [225] formulates DocRE as a semantic segmentation task and introduces a U-Net [226] on the LLM encoder to capture local and global dependencies between entities. ATLOP [227] focuses on the multi-label problems in DocRE, which could be handled with two techniques, i.e., adaptive thresholding for classifier and localized context pooling for LLM. DREEAM [161] further extends and improves ATLOP by incorporating evidence information.
End-to-End KG Construction. Currently, researchers are exploring the use of LLMs for end-to-end KG construction. Kumar et al. [95] propose a unified approach to build KGs from raw text, which contains two LLMs powered components. They first finetune a LLM on named entity recognition tasks to make it capable of recognizing entities in raw text. Then, they propose another "2-model BERT" for solving the relation extraction task, which contains two BERT-based classifiers. The first classifier learns the relation class whereas the second binary classifier learns the direction of the relations between the two entities. The predicted triples and relations are then used to construct the KG. Guo et al. [162] propose an end-to-end knowledge extraction model based on BERT, which can be applied to construct KGs from Classical Chinese text. Grapher [41] presents a novel end-to-end multi-stage system. It first utilizes LLMs to generate KG entities, followed by a simple relation construction head, enabling efficient KG construction from the textual description. PiVE [163] proposes a prompting with an iterative verification framework that utilizes a smaller LLM like T5 to correct the errors in KGs generated by a larger LLM (e.g., ChatGPT). To further explore advanced LLMs, AutoKG design several prompts for different KG</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Fig. 20. The general framework of distilling KGs from LLMs.
construction tasks (e.g., entity typing, entity linking, and relation extraction). Then, it adopts the prompt to perform KG construction using ChatGPT and GPT-4.</p>
<h3>5.3.4 Distilling Knowledge Graphs from LLMs</h3>
<p>LLMs have been shown to implicitly encode massive knowledge [14]. As shown in Fig. 20, some research aims to distill knowledge from LLMs to construct KGs. COMET [164] proposes a commonsense transformer model that constructs commonsense KGs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a LLM learns to adapt its learned representations to knowledge generation, and produces novel tuples that are high quality. Experimental results reveal that implicit knowledge from LLMs is transferred to generate explicit knowledge in commonsense KGs. BertNet [165] proposes a novel framework for automatic KG construction empowered by LLMs. It requires only the minimal definition of relations as inputs and automatically generates diverse prompts, and performs an efficient knowledge search within a given LLM for consistent outputs. The constructed KGs show competitive quality, diversity, and novelty with a richer set of new and complex relations, which cannot be extracted by previous methods. West et al. [166] propose a symbolic knowledge distillation framework that distills symbolic knowledge from LLMs. They first finetune a small student LLM by distilling commonsense facts from a large LLM like GPT-3. Then, the student LLM is utilized to generate commonsense KGs.</p>
<h3>5.4 LLM-augmented KG-to-text Generation</h3>
<p>The goal of Knowledge-graph-to-text (KG-to-text) generation is to generate high-quality texts that accurately and consistently describe the input knowledge graph information [228]. KG-to-text generation connects knowledge graphs and texts, significantly improving the applicability of KG in more realistic NLG scenarios, including storytelling [229] and knowledge-grounded dialogue [230]. However, it is challenging and costly to collect large amounts of graph-text parallel data, resulting in insufficient training and poor generation quality. Thus, many research efforts resort to either: 1) leverage knowledge from LLMs or 2) construct large-scale weakly-supervised KG-text corpus to solve this issue.</p>
<h3>5.4.1 Leveraging Knowledge from LLMs</h3>
<p>As pioneering research efforts in using LLMs for KG-to-Text generation, Ribeiro et al. [167] and Kale and Rastogi [231] directly fine-tune various LLMs, including BART and T5, with the goal of transferring LLMs knowledge for this</p>
<p><img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Fig. 21. The general framework of KG-to-text generation.
task. As shown in Fig. 21, both works simply represent the input graph as a linear traversal and find that such a naive approach successfully outperforms many existing state-of-the-art KG-to-text generation systems. Interestingly, Ribeiro et al. [167] also find that continue pre-training could further improve model performance. However, these methods are unable to explicitly incorporate rich graph semantics in KGs. To enhance LLMs with KG structure information, JointGT [42] proposes to inject KG structure-preserving representations into the Seq2Seq large language models. Given input sub-KGs and corresponding text, JointGT first represents the KG entities and their relations as a sequence of tokens, then concatenate them with the textual tokens which are fed into LLM. After the standard self-attention module, JointGT then uses a pooling layer to obtain the contextual semantic representations of knowledge entities and relations. Finally, these pooled KG representations are then aggregated in another structure-aware self-attention layer. JointGT also deploys additional pre-training objectives, including KG and text reconstruction tasks given masked inputs, to improve the alignment between text and graph information. Li et al. [168] focus on the few-shot scenario. It first employs a novel breadth-first search (BFS) strategy to better traverse the input KG structure and feed the enhanced linearized graph representations into LLMs for high-quality generated outputs, then aligns the GCNbased and LLM-based KG entity representation. Colas et al. [169] first transform the graph into its appropriate representation before linearizing the graph. Next, each KG node is encoded via a global attention mechanism, followed by a graph-aware attention module, ultimately being decoded into a sequence of tokens. Different from these works, KGBART [37] keeps the structure of KGs and leverages the graph attention to aggregate the rich concept semantics in the sub-KG, which enhances the model generalization on unseen concept sets.</p>
<h3>5.4.2 Constructing large weakly KG-text aligned Corpus</h3>
<p>Although LLMs have achieved remarkable empirical success, their unsupervised pre-training objectives are not necessarily aligned well with the task of KG-to-text generation, motivating researchers to develop large-scale KG-text aligned corpus. Jin et al. [170] propose a 1.3M unsupervised KG-to-graph training data from Wikipedia. Specifically, they first detect the entities appearing in the text via hyperlinks and named entity detectors, and then only add text that shares a common set of entities with the corresponding
knowledge graph, similar to the idea of distance supervision in the relation extraction task [232]. They also provide a 1,000+ human annotated KG-to-Text test data to verify the effectiveness of the pre-trained KG-to-Text models. Similarly, Chen et al. [171] also propose a KG-grounded text corpus collected from the English Wikidump. To ensure the connection between KG and text, they only extract sentences with at least two Wikipedia anchor links. Then, they use the entities from those links to query their surrounding neighbors in WikiData and calculate the lexical overlapping between these neighbors and the original sentences. Finally, only highly overlapped pairs are selected. The authors explore both graph-based and sequence-based encoders and identify their advantages in various different tasks and settings.</p>
<h3>5.5 LLM-augmented KG Question Answering</h3>
<p>Knowledge graph question answering (KGQA) aims to find answers to natural language questions based on the structured facts stored in knowledge graphs [233], [234]. The inevitable challenge in KGQA is to retrieve related facts and extend the reasoning advantage of KGs to QA. Therefore, recent studies adopt LLMs to bridge the gap between natural language questions and structured knowledge graphs [174], [175], [235]. The general framework of applying LLMs for KGQA is illustrated in Fig. 22, where LLMs can be used as 1) entity/relation extractors, and 2) answer reasoners.</p>
<h3>5.5.1 LLMs as Entity/relation Extractors</h3>
<p>Entity/relation extractors are designed to identify entities and relationships mentioned in natural language questions and retrieve related facts in KGs. Given the proficiency in language comprehension, LLMs can be effectively utilized for this purpose. Lukovnikov et al. [172] are the first to utilize LLMs as classifiers for relation prediction, resulting in a notable improvement in performance compared to shallow neural networks. Nan et al. [174] introduce two LLM-based KGQA frameworks that adopt LLMs to detect mentioned entities and relations. Then, they query the answer in KGs using the extracted entity-relation pairs. QA-GNN [131] uses LLMs to encode the question and candidate answer pairs, which are adopted to estimate the importance of relative KG entities. The entities are retrieved to form a subgraph, where an answer reasoning is conducted by a graph neural network. Luo et al. [173] use LLMs to calculate the similarities between relations and questions to retrieve related facts, formulated as</p>
<p>$$
s(r, q)=\operatorname{LLM}(r)^{\top} \operatorname{LLM}(q)
$$</p>
<p>where $q$ denotes the question, $r$ denotes the relation, and $\operatorname{LLM}(\cdot)$ would generate representation for $q$ and $r$, respectively. Furthermore, Zhang et al. [236] propose a LLM-based path retriever to retrieve question-related relations hop-byhop and construct several paths. The probability of each path can be calculated as</p>
<p>$$
P(p \mid q)=\prod_{t=1}^{|p|} s\left(r_{t}, q\right)
$$</p>
<p>where $p$ denotes the path, and $r_{t}$ denotes the relation at the $t$-th hop of $p$. The retrieved relations and paths can be used</p>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Fig. 22. The general framework of applying LLMs for knowledge graph question answering (KGQA).
as context knowledge to improve the performance of answer reasoners as</p>
<p>$$
P(a \mid q)=\sum_{p \in \mathcal{P}} P(a \mid p) P(p \mid q)
$$</p>
<p>where $\mathcal{P}$ denotes retrieved paths and $a$ denotes the answer.</p>
<h3>5.5.2 LLMs as Answer Reasoners</h3>
<p>Answer reasoners are designed to reason over the retrieved facts and generate answers. LLMs can be used as answer reasoners to generate answers directly. For example, as shown in Fig. 3 22, DEKCOR [175] concatenates the retrieved facts with questions and candidate answers as</p>
<p>$$
x=\left[\text { CLS }\right] q[\mathrm{SEP}] \text { Related Facts [SEP] } a[\mathrm{SEP}]
$$</p>
<p>where $a$ denotes candidate answers. Then, it feeds them into LLMs to predict answer scores. After utilizing LLMs to generate the representation of $x$ as QA context, DRLK [176] proposes a Dynamic Hierarchical Reasoner to capture the interactions between QA context and answers for answer prediction. Yan et al. [235] propose a LLM-based KGQA framework consisting of two stages: (1) retrieve related facts from KGs and (2) generate answers based on the retrieved facts. The first stage is similar to the entity/relation extractors. Given a candidate answer entity $a$, it extracts a series of paths $p_{1}, \ldots, p_{n}$ from KGs. But the second stage is a LLM-based answer reasoner. It first verbalizes the paths by using the entity names and relation names in KGs. Then, it concatenates the question $q$ and all paths $p_{1}, \ldots, p_{n}$ to make an input sample as</p>
<p>$$
x=\left[\text { CLS }\right] q\left[\text { SEP] } p_{1}[\text { SEP }] \cdots[\text { SEP }] p_{n}[\text { SEP }]\right.
$$</p>
<p>These paths are regarded as the related facts for the candidate answer $a$. Finally, it uses LLMs to predict whether the hypothesis: " $a$ is the answer of $q$ " is supported by those facts, which is formulated as</p>
<p>$$
\begin{aligned}
e_{[\text {CLS }]} &amp; =\operatorname{LLM}(x) \
s &amp; =\sigma\left(\operatorname{MLP}\left(e_{[\text {CLS })}\right)\right)
\end{aligned}
$$</p>
<p>where it encodes $x$ using a LLM and feeds representation corresponding to [CLS] token for binary classification, and $\sigma(\cdot)$ denotes the sigmoid function.</p>
<p>TABLE 4
Summary of methods that synergize KGs and LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Year</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Synergized Knowledge representation</td>
<td style="text-align: center;">JointGT [42]</td>
<td style="text-align: center;">2021</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KEPLER [40]</td>
<td style="text-align: center;">2021</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRAGON [44]</td>
<td style="text-align: center;">2022</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HKLM [238]</td>
<td style="text-align: center;">2023</td>
</tr>
<tr>
<td style="text-align: center;">Synergized Reasoning</td>
<td style="text-align: center;">LARK [45]</td>
<td style="text-align: center;">2023</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Siyuan et al. [46]</td>
<td style="text-align: center;">2023</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KSL [239]</td>
<td style="text-align: center;">2023</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">StructGPT [237]</td>
<td style="text-align: center;">2023</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Think-on-graph [240]</td>
<td style="text-align: center;">2023</td>
</tr>
</tbody>
</table>
<p>To better guide LLMs reason through KGs, OreoLM [177] proposes a Knowledge Interaction Layer (KIL) which is inserted amid LLM layers. KIL interacts with a KG reasoning module, where it discovers different reasoning paths, and then the reasoning module can reason over the paths to generate answers. GreaseLM [178] fuses the representations from LLMs and graph neural networks to effectively reason over KG facts and language context. UniKGQA [43] unifies the facts retrieval and reasoning into a unified framework. UniKGQA consists of two modules. The first module is a semantic matching module that uses a LLM to match questions with their corresponding relations semantically. The second module is a matching information propagation module, which propagates the matching information along directed edges on KGs for answer reasoning. Similarly, ReLMKG [179] performs joint reasoning on a large language model and the associated knowledge graph. The question and verbalized paths are encoded by the language model, and different layers of the language model produce outputs that guide a graph neural network to perform message passing. This process utilizes the explicit knowledge contained in the structured knowledge graph for reasoning purposes. StructGPT [237] adopts a customized interface to allow large language models (e.g., ChatGPT) directly reasoning on KGs to perform multi-step question answering.</p>
<h2>6 Synergized LLMs + KGs</h2>
<p>The synergy of LLMs and KGs has attracted increasing attention these years, which marries the merits of LLMs and KGs to mutually enhance performance in various downstream applications. For example, LLMs can be used to understand natural language, while KGs are treated as a knowledge base, which provides factual knowledge. The unification of LLMs and KGs could result in a powerful model for knowledge representation and reasoning.</p>
<p>In this section, we will discuss the state-of-the-art Synergized LLMs + KGs from two perspectives: 1) Synergized Knowledge Representation, and 2) Synergized Reasoning. Representative works are summarized in Table 4.</p>
<h3>6.1 Synergized Knowledge Representation</h3>
<p>Text corpus and knowledge graphs both contain enormous knowledge. However, the knowledge in text corpus is usually implicit and unstructured, while the knowledge in KGs is explicit and structured. Synergized Knowledge Representation aims to design a synergized model that can</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Fig. 23. Synergized knowledge representation by additional KG fusion modules.</p>
<p>effectively represent knowledge from both LLMs and KGs. The synergized model can provide a better understanding of the knowledge from both sources, making it valuable for many downstream tasks.</p>
<p>To jointly represent the knowledge, researchers propose the synergized models by introducing additional KG fusion modules, which are jointly trained with LLMs. As shown in Fig. 23, ERNIE [35] proposes a textual-knowledge dual encoder architecture where a <em>T-encoder</em> first encodes the input sentences, then a <em>K-encoder</em> processes knowledge graphs which are fused them with the textual representation from the <em>T-encoder</em>. BERT-MK [241] employs a similar dual-encoder architecture but it introduces additional information of neighboring entities in the knowledge encoder component during the pre-training of LLMs. However, some of the neighboring entities in KGs may not be relevant to the input text, resulting in extra redundancy and noise. Coke-BERT [242] focuses on this issue and proposes a GNN-based module to filter out irrelevant KG entities using the input text. JAKET [243] proposes to fuse the entity information in the middle of the large language model.</p>
<p>KEPLER [40] presents a unified model for knowledge embedding and pre-trained language representation. In KEPLER, they encode textual entity descriptions with a LLM as their embeddings, and then jointly optimize the knowledge embedding and language modeling objectives. JointGT [42] proposes a graph-text joint representation learning model, which proposes three pre-training tasks to align representations of graph and text. DRAGON [44] presents a self-supervised method to pre-train a joint language-knowledge foundation model from text and KG. It takes text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. Then, DRAGON utilizes two self-supervised reasoning tasks, i.e., masked language modeling and KG link prediction to optimize the model parameters. HKLM [238] introduces a unified LLM which incorporates KGs to learn representations of domain-specific knowledge.</p>
<h3>6.2 Synergized Reasoning</h3>
<p>To better utilize the knowledge from text corpus and knowledge graph reasoning, Synergized Reasoning aims to design a synergized model that can effectively conduct reasoning with both LLMs and KGs.</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Fig. 24. The framework of LLM-KG Fusion Reasoning.</p>
<p><strong>LLM-KG Fusion Reasoning.</strong> LLM-KG Fusion Reasoning leverages two separated LLM and KG encoders to process the text and relevant KG inputs [244]. These two encoders are equally important and jointly fusing the knowledge from two sources for reasoning. To improve the interaction between text and knowledge, KagNet [38] proposes to first encode the input KG, and then augment the input textual representation. In contrast, MHGRN [234] uses the final LLM outputs of the input text to guide the reasoning process on the KGs. Yet, both of them only design a single-direction interaction between the text and KGs. To tackle this issue, QA-GNN [131] proposes to use a GNN-based model to jointly reason over input context and KG information via message passing. Specifically, QA-GNN represents the input textual information as a special node via a pooling operation and connects this node with other entities in KG. However, the textual inputs are only pooled into a single dense vector, limiting the information fusion performance. JointLK [245] then proposes a framework with fine-grained interaction between any tokens in the textual inputs and any KG entities through LM-to-KG and KG-to-LM bi-directional attention mechanism. As shown in Fig. 24, pairwise dot-product scores are calculated over all textual tokens and KG entities, the bi-directional attentive scores are computed separately. In addition, at each jointLK layer, the KGs are also dynamically pruned based on the attention score to allow later layers to focus on more important sub-KG structures. Despite being effective, in JointLK, the fusion process between the input text and KG still uses the final LLM outputs as the input text representations. GreaseLM [178] designs deep and rich interaction between the input text tokens and KG entities at each layer of the LLMs. The architecture and fusion approach is mostly similar to ERNIE [35] discussed in Section 6.1, except that GreaseLM does not use the text-only <em>T-encoder</em> to handle input text.</p>
<p><strong>LLMs as Agents Reasoning.</strong> Instead using two encoders to fuse the knowledge, LLMs can also be treated as agents to interact with the KGs to conduct reasoning [246], as illustrated in Fig. 25. KD-CoT [247] iteratively retrieves facts from KGs and produces faithful reasoning traces, which guide LLMs to generate answers. KSL [239] teaches LLMs to search on KGs to retrieve relevant facts and then generate answers. StructGPT [237] designs several API interfaces to allow LLMs to access the structural data and perform reasoning by traversing on KGs. Think-on-graph [240] provides a flexible plug-and-play framework where LLM agents iteratively execute beam searches on KGs to discover the</p>
<p><img alt="img-24.jpeg" src="img-24.jpeg" /></p>
<p>Fig. 25. Using LLMs as agents for reasoning on KGs.
reasoning paths and generate answers. To enhance the agent abilities, AgentTuning [248] presents several instructiontuning datasets to guide LLM agents to perform reasoning on KGs.
Comparison and Discussion. LLM-KG Fusion Reasoning combines the LLM encoder and KG encoder to represent knowledge in a unified manner. It then employs a synergized reasoning module to jointly reason the results. This framework allows for different encoders and reasoning modules, which are trained end-to-end to effectively utilize the knowledge and reasoning capabilities of LLMs and KGs. However, these additional modules may introduce extra parameters and computational costs while lacking interpretability. LLMs as Agents for KG reasoning provides a flexible framework for reasoning on KGs without additional training cost, which can be generalized to different LLMs and KGs. Meanwhile, the reasoning process is interpretable, which can be used to explain the results. Nevertheless, defining the actions and policies for LLM agents is also challenging. The synergy of LLMs and KGs is still an ongoing research topic, with the potential to have more powerful frameworks in the future.</p>
<h2>7 Future Directions and Milestones</h2>
<p>In this section, we discuss the future directions and several milestones in the research area of unifying KGs and LLMs.</p>
<h3>7.1 KGs for Hallucination Detection in LLMs</h3>
<p>The hallucination problem in LLMs, which generates factually incorrect content, significantly hinders the reliability of LLMs. As discussed in Section 4, existing studies try to utilize KGs to obtain more reliable LLMs through pretraining or KG-enhanced inference. Despite the efforts, the issue of hallucination may continue to persist in the realm of LLMs for the foreseeable future. Consequently, in order to gain the public's trust and border applications, it is imperative to detect and assess instances of hallucination within LLMs and other forms of AI-generated content (AIGC). Existing methods strive to detect hallucination by training a neural classifier on a small set of documents [249], which are neither robust nor powerful to handle ever-growing LLMs. Recently, researchers try to use KGs as an external source to validate LLMs [250]. Further studies combine LLMs and KGs to achieve a generalized fact-checking model that can detect hallucinations across domains [251]. Therefore,
it opens a new door to utilizing KGs for hallucination detection.</p>
<h3>7.2 KGs for Editing Knowledge in LLMs</h3>
<p>Although LLMs are capable of storing massive real-world knowledge, they cannot quickly update their internal knowledge updated as real-world situations change. There are some research efforts proposed for editing knowledge in LLMs [252] without re-training the whole LLMs. Yet, such solutions still suffer from poor performance or computational overhead [253]. Existing studies [254] also reveal that edit a single fact would cause a ripple effect for other related knowledge. Therefore, it is necessary to develop a more efficient and effective method to edit knowledge in LLMs. Recently, researchers try to leverage KGs to edit knowledge in LLMs efficiently.</p>
<h3>7.3 KGs for Black-box LLMs Knowledge Injection</h3>
<p>Although pre-training and knowledge editing could update LLMs to catch up with the latest knowledge, they still need to access the internal structures and parameters of LLMs. However, many state-of-the-art large LLMs (e.g., ChatGPT) only provide APIs for users and developers to access, making themselves black-box to the public. Consequently, it is impossible to follow conventional KG injection approaches described [38], [244] that change LLM structure by adding additional knowledge fusion modules. Converting various types of knowledge into different text prompts seems to be a feasible solution. However, it is unclear whether these prompts can generalize well to new LLMs. Moreover, the prompt-based approach is limited to the length of input tokens of LLMs. Therefore, how to enable effective knowledge injection for black-box LLMs is still an open question for us to explore [255], [256].</p>
<h3>7.4 Multi-Modal LLMs for KGs</h3>
<p>Current knowledge graphs typically rely on textual and graph structure to handle KG-related applications. However, real-world knowledge graphs are often constructed by data from diverse modalities [99], [257], [258]. Therefore, effectively leveraging representations from multiple modalities would be a significant challenge for future research in KGs [259]. One potential solution is to develop methods that can accurately encode and align entities across different modalities. Recently, with the development of multi-modal LLMs [98], [260], leveraging LLMs for modality alignment holds promise in this regard. But, bridging the gap between multi-modal LLMs and KG structure remains a crucial challenge in this field, demanding further investigation and advancements.</p>
<h3>7.5 LLMs for Understanding KG Structure</h3>
<p>Conventional LLMs trained on plain text data are not designed to understand structured data like knowledge graphs. Thus, LLMs might not fully grasp or understand the information conveyed by the KG structure. A straightforward way is to linearize the structured data into a sentence that LLMs can understand. However, the scale of the KGs</p>
<p><img alt="img-25.jpeg" src="img-25.jpeg" /></p>
<p>Fig. 26. The milestones of unifying KGs and LLMs.
makes it impossible to linearize the whole KGs as input. Moreover, the linearization process may lose some underlying information in KGs. Therefore, it is necessary to develop LLMs that can directly understand the KG structure and reason over it [237].</p>
<h3>7.6 Synergized LLMs and KGs for Birectional Reasoning</h3>
<p>KGs and LLMs are two complementary technologies that can synergize each other. However, the synergy of LLMs and KGs is less explored by existing researchers. A desired synergy of LLMs and KGs would involve leveraging the strengths of both technologies to overcome their individual limitations. LLMs, such as ChatGPT, excel in generating human-like text and understanding natural language, while KGs are structured databases that capture and represent knowledge in a structured manner. By combining their capabilities, we can create a powerful system that benefits from the contextual understanding of LLMs and the structured knowledge representation of KGs. To better unify LLMs and KGs, many advanced techniques need to be incorporated, such as multi-modal learning [261], graph neural network [262], and continuous learning [263]. Last, the synergy of LLMs and KGs can be applied to many real-world applications, such as search engines [100], recommender systems [10], [89], and drug discovery.</p>
<p>With a given application problem, we can apply a KG to perform a knowledge-driven search for potential goals and unseen data, and simultaneously start with LLMs to perform a data/text-driven inference to see what new data/goal items can be derived. When the knowledge-based search is combined with data/text-driven inference, they can mutually validate each other, resulting in efficient and effective solutions powered by dual-driving wheels. Therefore, we can anticipate increasing attention to unlock the potential of integrating KGs and LLMs for diverse downstream applications with both generative and reasoning capabilities in the near future.</p>
<h2>8 CONCLUSION</h2>
<p>Unifying large language models (LLMs) and knowledge graphs (KGs) is an active research direction that has attracted increasing attention from both academia and industry. In this article, we provide a thorough overview of the recent research in this field. We first introduce different manners that integrate KGs to enhance LLMs. Then, we introduce existing methods that apply LLMs for KGs and establish taxonomy based on varieties of KG tasks. Finally, we discuss the challenges and future directions in this field.</p>
<p>We envision that there will be multiple stages (milestones) in the roadmap of unifying KGs and LLMs, as shown in Fig. 26. In particular, we will anticipate increasing research on three stages: Stage 1: KG-enhanced LLMs, LLM-augmented KGs, Stage 2: Synergized LLMs + KGs, and Stage 3: Graph Structure Understanding, Multi-modality, Knowledge Updating. We hope that this article will provide a guideline to advance future research.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research was supported by the Australian Research Council (ARC) under grants FT210100097 and DP240101547 and the National Natural Science Foundation of China (NSFC) under grant 62120106008.</p>
<h2>REFERENCES</h2>
<p>[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pretraining of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[2] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.
[3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020.
[4] D. Su, Y. Xu, G. I. Winata, P. Xu, H. Kim, Z. Liu, and P. Fung, "Generalizing question answering system with pre-trained language model fine-tuning," in Proceedings of the 2nd Workshop on Machine Reading for Question Answering, 2019, pp. 203-211.
[5] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in ACL, 2020, pp. 78717880 .
[6] J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, "Pretrained language models for text generation: A survey," arXiv preprint arXiv:2105.10311, 2021.
[7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., "Emergent abilities of large language models," Transactions on Machine Learning Research.
[8] K. Malinka, M. Pereini, A. Firc, O. Hujk, and F. Janu, "On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?" arXiv preprint arXiv:2303.11146, 2023.
[9] Z. Li, C. Wang, Z. Liu, H. Wang, S. Wang, and C. Gao, "Cctest: Testing and repairing code completion systems," ICSE, 2023.
[10] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, "Is chatgpt a good recommender? a preliminary study," arXiv preprint arXiv:2304.10149, 2023.
[11] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., "A survey of large language models," arXiv preprint arXiv:2303.18223, 2023.
[12] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, "Pre-trained models for natural language processing: A survey," Science China Technological Sciences, vol. 63, no. 10, pp. 1872-1897, 2020.
[13] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu, "Harnessing the power of llms in practice: A survey on chatgpt and beyond," arXiv preprint arXiv:2304.13712, 2023.
[14] F. Petroni, T. Rocktschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller, "Language models as knowledge bases?" in EMNLP-IICNLP, 2019, pp. 2463-2473.
[15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, "Survey of hallucination in natural language generation," ACM Computing Surveys, vol. 55, no. 12, pp. 1-38, 2023.
[16] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, "A survey of controllable text generation using transformer-based pre-trained language models," arXiv preprint arXiv:2201.05337, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
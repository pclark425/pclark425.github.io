<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2994 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2994</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2994</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-269983112</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.14860v3.pdf" target="_blank">Not All Language Model Features Are One-Dimensionally Linear</a></p>
                <p><strong>Paper Abstract:</strong> Recent work has proposed that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Next, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we examine the continuity of the days of the week feature in Mistral 7B. Overall, our work argues that understanding multi-dimensional features is necessary to mechanistically decompose some model behaviors.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2994.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2994.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter transformer language model studied in this paper; shown to use multi-dimensional circular internal representations to solve natural-language modular-arithmetic tasks (days/months) via computations performed in MLP layers and copy-to-previous-token behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer language model, ~7B parameters (cited as Mistral 7B). SAEs were trained on layers 8, 16, 24; activations normalized to model norm √d_model = 64; SAE dictionary size 65536 for each trained SAE (training on Pile + Alpaca subset).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Natural-language modular addition tasks: Weekdays (7-way modular arithmetic over day tokens) and Months (12-way modular arithmetic over month tokens); evaluation also attempted on plain numeric modular prompts (e.g. '5 + 3 (mod 7)').</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Computation via multi-dimensional circular representations ('circle' manifolds) that encode input tokens (α) in angle on a 2D subspace; models copy pertinent information to the token before the predicted result and perform the modular computation in later MLP layers using a trigonometry-like algorithm (consistent with 'clock'/'pizza' descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) Sparse autoencoder (SAE) clustering recovered a clear 2D circular plane for days/months in layer 8 (PCA visualizations). 2) PCA plots across layers show circular encoding of α. 3) Explanation via Regression (EVR) on later-layer hidden states (γ representation) reveals a circle after removing α/β components. 4) Activation-patching interventions on the circular subspace using circular probes and SAE probes causally change output logits toward the target token (average logit-difference metrics reported). 5) Off-distribution grid interventions (sweeping radius r and angle θ) show the model treats angle as encoding α (angle changes affect predicted γ). 6) Fine-grained layer/token patching shows copying to token before γ and MLPs doing the computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>1) Mistral gets trivial accuracy on plain numeric modular addition prompts (e.g. '5 + 3 (mod 7) ≡') despite performing well on natural-language weekday/month tasks, indicating reliance on distributional/lexical cues. 2) Representations for α are often clustered discretely (heptagon/dodecagon), showing discontinuity in many contexts; continuity exists under some prompt distributions but is not universal. 3) Backup circuits and other subspaces can interfere with isolated interventions, complicating causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching / subspace patching: trained linear 'circular probe' on PCA directions and SAE-derived 2D plane, then replace the model's projection on that subspace with the clean target circle point; also baseline interventions replacing first k PCA dims, whole layer replacement, average-ablation of non-intervened dimensions; off-distribution interventions sweeping r,θ; layer-wise MLP/attention patching.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Intervening on the circular subspace toward the target circle point increases the target token's logit and shifts model outputs toward the desired γ; quantitative average-logit-difference effects reported across many patch experiments. Using the SAE-derived probe slightly reduced intervention performance compared to per-layer-trained circular probes on layer 8 (reported change from -2.58 to -2.01 average logit difference on layer 8), but SAE probes were substantially more robust to layer shifts (e.g., per-layer circular probe on layer 6 gave average logit difference 0.029 vs SAE probe -2.32). Off-distribution (r,θ) sweeps show smooth dependence on θ in many cases, supporting angle encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Weekdays accuracy: 31 / 49 prompts correct (per-highest-logit-token metric). Months accuracy: 125 / 144 prompts correct. Plain numeric modular addition prompts: trivial accuracy (near 0). Intervention metrics: average logit-difference and Average Intervention Effect reported in figures; example numbers cited include average-logit-difference baseline comparisons (e.g., -2.58 vs -2.01) and layer-shift probe results (0.029 vs -2.32).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails on equivalent plain numeric modular prompts (shows task sensitivity to lexical framing); many circular representations are discretized (clustered at vertices) so generalization between adjacent tokens is not guaranteed; presence of backup/parallel subspaces limits intervention isolation; some layers do not show clear circular structure; copying to token before γ means computation distributed across positions and layers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors interpret results as consistent with an internal trigonometry-like algorithm (e.g., 'clock' or 'pizza') for modular addition rather than symbolic arithmetic; computation resembles algorithmic/transformation-based processing (angle add then decode) rather than explicit symbolic arithmetic or lookup-table memorization. No direct human-vs-model behavioral comparison or speed/complexity analysis against symbolic calculators is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Language Model Features Are One-Dimensionally Linear', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2994.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2994.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter transformer model (AI@Meta) evaluated in this paper; shows circular α representations for month/day tasks and achieves high accuracy on Months and moderate on Weekdays, with similar copy-and-compute circuitry as Mistral 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer language model, ~8B parameters (Llama 3 8B cited). Evaluated on the Weekdays and Months natural-language modular arithmetic tasks; PCA plots and activation-patching performed across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Natural-language modular addition tasks: Weekdays (mod 7 with day tokens) and Months (mod 12 with month tokens); also tested on plain numeric modular prompts (fails).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Uses circular multi-dimensional internal representations of α (angle on 2D subspace/PCA plane) and computes γ using copy-to-previous-token plus MLP computation (consistent with trigonometry-based 'clock'/'pizza' style mechanisms), with causal role demonstrated via activation patching.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) PCA projections across layers show circular structure for α in many layers. 2) Activation patching/subspace interventions on circular probes causally change logits toward patched target γ (reported average logit-difference interventions in figures). 3) Layer-wise patching indicates writing/copying behavior to the token before γ and later MLP computation. 4) EVR and residual analyses (primarily shown for Mistral but the causal chain is similar) support a circular γ representation developing in late layers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>1) Like Mistral, Llama 3 8B performs poorly on plain numeric modular prompts despite strong natural-language performance, suggesting dependence on lexical/context cues. 2) Not all layers show circular structure; backup circuits and other subspaces can dilute intervention effects.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation/subspace patching using circular probes trained on PCA components; layer-wise MLP/attention patching; various baselines (replace first k PCA dims, replace full layer, average ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Patching the circular subspace toward the target raises the target token's logit and often shifts model choice to the desired γ; intervention effects vary by layer and probe method (plots show per-layer average logit-difference and 96% error bars). Specific numerical intervention magnitudes are reported in figures (see main text Fig. 6 and Fig. 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Weekdays accuracy: 29 / 49 prompts correct. Months accuracy: 143 / 144 prompts correct. Plain numeric modular addition prompts: trivial accuracy (near 0). Intervention effect sizes visualized in figures (average logit differences and patching recovery percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Same as Mistral: fails on plain numeric modular prompts; representations sometimes discontinuous; presence of backup circuits; not all layers contain clean circular encodings; computation involves copying so localized editing is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Interpreted as implementing geometric/trigonometric internal algorithms (angle addition and decoding) analogous to the 'clock'/'pizza' mechanisms proposed in prior work rather than explicit symbolic computation; no direct human or calculator comparison given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Language Model Features Are One-Dimensionally Linear', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2994.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2994.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (small)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (small)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained GPT-2 model (Radford et al.) in which SAE analysis found circular representations (days/months/years) but which nonetheless performs poorly on the Weekdays and Months tasks in this paper's evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (small)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained GPT-2 small (Radford et al., 2019) activations were analyzed using public SAEs (Bloom, 2024). Sparse autoencoder clusters revealed circular arrangements for days/months/years in some layers, but behavioral accuracy on modular calendar tasks is low.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Natural-language modular addition tasks: Weekdays and Months (same setup as for other models); also analysis of learned positional/temporal structures.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Contains circular multi-dimensional representations (PCA visualizations of SAE-cluster reconstructions show circles), but the model does not successfully use them to produce correct answers on the tested Weekdays/Months tasks; prior related work on small models indicates use of circular encodings for modular arithmetic in toy settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) SAE clustering of GPT-2 dictionary elements produced clusters whose reconstructed activations projected into interpretable circles for days/months/years (PCA plots). 2) Separability and ϵ-mixture indices rank these clusters highly as candidate irreducible multidimensional features. 3) Prior literature (cited in paper) shows small models trained on modular addition can learn circular encodings and use them algorithmically.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite exhibiting circular representations, GPT-2 achieves poor task accuracy on Weekdays/Months in the paper's evaluations (Weekdays 8/49, Months 0/144), indicating that existence of a circular representation alone does not guarantee use of that representation in the relevant computation or sufficient downstream wiring to decode results.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>SAE-based discovery and fixed-cluster projection probes were used for analysis; the paper reports SAE reconstruction and irreducibility tests but does not report successful causal patching experiments for GPT-2 on these tasks (GPT-2 behavioral accuracy is low).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>No reported successful intervention that recovers correct arithmetic behavior for GPT-2 on the Weekdays/Months natural-language tasks; SAE analysis mainly descriptive (recovery of circular feature manifolds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Weekdays accuracy: 8 / 49 prompts correct. Months accuracy: 0 / 144 prompts correct. SAE cluster ranking: days/months/year clusters rank among top-scoring clusters by irreducibility metrics (reported ranks 9, 28, 15 in one ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Although circular representations exist, GPT-2 lacks downstream computation or decoding circuits to use them for the evaluated tasks; representations may be present but not causally connected to the output; discontinuity and clustering of tokens on discrete vertices limit generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No explicit comparison; existence of circular encodings aligns qualitatively with toy-model algorithmic solutions (clock/pizza), but GPT-2's failure to use them highlights that representation alone does not imply algorithmic arithmetic competence comparable to symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Language Model Features Are One-Dimensionally Linear', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>The clock and the pizza: Two stories in mechanistic explanation of neural networks <em>(Rating: 2)</em></li>
                <li>Language models use trigonometry to do addition <em>(Rating: 1)</em></li>
                <li>Language models represent space and time <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2994",
    "paper_id": "paper-269983112",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B",
            "brief_description": "A 7-billion-parameter transformer language model studied in this paper; shown to use multi-dimensional circular internal representations to solve natural-language modular-arithmetic tasks (days/months) via computations performed in MLP layers and copy-to-previous-token behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 7B",
            "model_description": "Transformer language model, ~7B parameters (cited as Mistral 7B). SAEs were trained on layers 8, 16, 24; activations normalized to model norm √d_model = 64; SAE dictionary size 65536 for each trained SAE (training on Pile + Alpaca subset).",
            "arithmetic_task_type": "Natural-language modular addition tasks: Weekdays (7-way modular arithmetic over day tokens) and Months (12-way modular arithmetic over month tokens); evaluation also attempted on plain numeric modular prompts (e.g. '5 + 3 (mod 7)').",
            "reported_mechanism": "Computation via multi-dimensional circular representations ('circle' manifolds) that encode input tokens (α) in angle on a 2D subspace; models copy pertinent information to the token before the predicted result and perform the modular computation in later MLP layers using a trigonometry-like algorithm (consistent with 'clock'/'pizza' descriptions).",
            "evidence_for_mechanism": "1) Sparse autoencoder (SAE) clustering recovered a clear 2D circular plane for days/months in layer 8 (PCA visualizations). 2) PCA plots across layers show circular encoding of α. 3) Explanation via Regression (EVR) on later-layer hidden states (γ representation) reveals a circle after removing α/β components. 4) Activation-patching interventions on the circular subspace using circular probes and SAE probes causally change output logits toward the target token (average logit-difference metrics reported). 5) Off-distribution grid interventions (sweeping radius r and angle θ) show the model treats angle as encoding α (angle changes affect predicted γ). 6) Fine-grained layer/token patching shows copying to token before γ and MLPs doing the computation.",
            "evidence_against_mechanism": "1) Mistral gets trivial accuracy on plain numeric modular addition prompts (e.g. '5 + 3 (mod 7) ≡') despite performing well on natural-language weekday/month tasks, indicating reliance on distributional/lexical cues. 2) Representations for α are often clustered discretely (heptagon/dodecagon), showing discontinuity in many contexts; continuity exists under some prompt distributions but is not universal. 3) Backup circuits and other subspaces can interfere with isolated interventions, complicating causal claims.",
            "intervention_type": "Activation patching / subspace patching: trained linear 'circular probe' on PCA directions and SAE-derived 2D plane, then replace the model's projection on that subspace with the clean target circle point; also baseline interventions replacing first k PCA dims, whole layer replacement, average-ablation of non-intervened dimensions; off-distribution interventions sweeping r,θ; layer-wise MLP/attention patching.",
            "effect_of_intervention": "Intervening on the circular subspace toward the target circle point increases the target token's logit and shifts model outputs toward the desired γ; quantitative average-logit-difference effects reported across many patch experiments. Using the SAE-derived probe slightly reduced intervention performance compared to per-layer-trained circular probes on layer 8 (reported change from -2.58 to -2.01 average logit difference on layer 8), but SAE probes were substantially more robust to layer shifts (e.g., per-layer circular probe on layer 6 gave average logit difference 0.029 vs SAE probe -2.32). Off-distribution (r,θ) sweeps show smooth dependence on θ in many cases, supporting angle encoding.",
            "performance_metrics": "Weekdays accuracy: 31 / 49 prompts correct (per-highest-logit-token metric). Months accuracy: 125 / 144 prompts correct. Plain numeric modular addition prompts: trivial accuracy (near 0). Intervention metrics: average logit-difference and Average Intervention Effect reported in figures; example numbers cited include average-logit-difference baseline comparisons (e.g., -2.58 vs -2.01) and layer-shift probe results (0.029 vs -2.32).",
            "notable_failure_modes": "Fails on equivalent plain numeric modular prompts (shows task sensitivity to lexical framing); many circular representations are discretized (clustered at vertices) so generalization between adjacent tokens is not guaranteed; presence of backup/parallel subspaces limits intervention isolation; some layers do not show clear circular structure; copying to token before γ means computation distributed across positions and layers.",
            "comparison_to_humans_or_symbolic": "Authors interpret results as consistent with an internal trigonometry-like algorithm (e.g., 'clock' or 'pizza') for modular addition rather than symbolic arithmetic; computation resembles algorithmic/transformation-based processing (angle add then decode) rather than explicit symbolic arithmetic or lookup-table memorization. No direct human-vs-model behavioral comparison or speed/complexity analysis against symbolic calculators is provided.",
            "uuid": "e2994.0",
            "source_info": {
                "paper_title": "Not All Language Model Features Are One-Dimensionally Linear",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Llama-3-8B",
            "name_full": "Llama 3 8B",
            "brief_description": "An 8-billion-parameter transformer model (AI@Meta) evaluated in this paper; shows circular α representations for month/day tasks and achieves high accuracy on Months and moderate on Weekdays, with similar copy-and-compute circuitry as Mistral 7B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3 8B",
            "model_description": "Transformer language model, ~8B parameters (Llama 3 8B cited). Evaluated on the Weekdays and Months natural-language modular arithmetic tasks; PCA plots and activation-patching performed across layers.",
            "arithmetic_task_type": "Natural-language modular addition tasks: Weekdays (mod 7 with day tokens) and Months (mod 12 with month tokens); also tested on plain numeric modular prompts (fails).",
            "reported_mechanism": "Uses circular multi-dimensional internal representations of α (angle on 2D subspace/PCA plane) and computes γ using copy-to-previous-token plus MLP computation (consistent with trigonometry-based 'clock'/'pizza' style mechanisms), with causal role demonstrated via activation patching.",
            "evidence_for_mechanism": "1) PCA projections across layers show circular structure for α in many layers. 2) Activation patching/subspace interventions on circular probes causally change logits toward patched target γ (reported average logit-difference interventions in figures). 3) Layer-wise patching indicates writing/copying behavior to the token before γ and later MLP computation. 4) EVR and residual analyses (primarily shown for Mistral but the causal chain is similar) support a circular γ representation developing in late layers.",
            "evidence_against_mechanism": "1) Like Mistral, Llama 3 8B performs poorly on plain numeric modular prompts despite strong natural-language performance, suggesting dependence on lexical/context cues. 2) Not all layers show circular structure; backup circuits and other subspaces can dilute intervention effects.",
            "intervention_type": "Activation/subspace patching using circular probes trained on PCA components; layer-wise MLP/attention patching; various baselines (replace first k PCA dims, replace full layer, average ablation).",
            "effect_of_intervention": "Patching the circular subspace toward the target raises the target token's logit and often shifts model choice to the desired γ; intervention effects vary by layer and probe method (plots show per-layer average logit-difference and 96% error bars). Specific numerical intervention magnitudes are reported in figures (see main text Fig. 6 and Fig. 8).",
            "performance_metrics": "Weekdays accuracy: 29 / 49 prompts correct. Months accuracy: 143 / 144 prompts correct. Plain numeric modular addition prompts: trivial accuracy (near 0). Intervention effect sizes visualized in figures (average logit differences and patching recovery percentages).",
            "notable_failure_modes": "Same as Mistral: fails on plain numeric modular prompts; representations sometimes discontinuous; presence of backup circuits; not all layers contain clean circular encodings; computation involves copying so localized editing is nontrivial.",
            "comparison_to_humans_or_symbolic": "Interpreted as implementing geometric/trigonometric internal algorithms (angle addition and decoding) analogous to the 'clock'/'pizza' mechanisms proposed in prior work rather than explicit symbolic computation; no direct human or calculator comparison given.",
            "uuid": "e2994.1",
            "source_info": {
                "paper_title": "Not All Language Model Features Are One-Dimensionally Linear",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-2 (small)",
            "name_full": "GPT-2 (small)",
            "brief_description": "A pretrained GPT-2 model (Radford et al.) in which SAE analysis found circular representations (days/months/years) but which nonetheless performs poorly on the Weekdays and Months tasks in this paper's evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (small)",
            "model_description": "Pretrained GPT-2 small (Radford et al., 2019) activations were analyzed using public SAEs (Bloom, 2024). Sparse autoencoder clusters revealed circular arrangements for days/months/years in some layers, but behavioral accuracy on modular calendar tasks is low.",
            "arithmetic_task_type": "Natural-language modular addition tasks: Weekdays and Months (same setup as for other models); also analysis of learned positional/temporal structures.",
            "reported_mechanism": "Contains circular multi-dimensional representations (PCA visualizations of SAE-cluster reconstructions show circles), but the model does not successfully use them to produce correct answers on the tested Weekdays/Months tasks; prior related work on small models indicates use of circular encodings for modular arithmetic in toy settings.",
            "evidence_for_mechanism": "1) SAE clustering of GPT-2 dictionary elements produced clusters whose reconstructed activations projected into interpretable circles for days/months/years (PCA plots). 2) Separability and ϵ-mixture indices rank these clusters highly as candidate irreducible multidimensional features. 3) Prior literature (cited in paper) shows small models trained on modular addition can learn circular encodings and use them algorithmically.",
            "evidence_against_mechanism": "Despite exhibiting circular representations, GPT-2 achieves poor task accuracy on Weekdays/Months in the paper's evaluations (Weekdays 8/49, Months 0/144), indicating that existence of a circular representation alone does not guarantee use of that representation in the relevant computation or sufficient downstream wiring to decode results.",
            "intervention_type": "SAE-based discovery and fixed-cluster projection probes were used for analysis; the paper reports SAE reconstruction and irreducibility tests but does not report successful causal patching experiments for GPT-2 on these tasks (GPT-2 behavioral accuracy is low).",
            "effect_of_intervention": "No reported successful intervention that recovers correct arithmetic behavior for GPT-2 on the Weekdays/Months natural-language tasks; SAE analysis mainly descriptive (recovery of circular feature manifolds).",
            "performance_metrics": "Weekdays accuracy: 8 / 49 prompts correct. Months accuracy: 0 / 144 prompts correct. SAE cluster ranking: days/months/year clusters rank among top-scoring clusters by irreducibility metrics (reported ranks 9, 28, 15 in one ranking).",
            "notable_failure_modes": "Although circular representations exist, GPT-2 lacks downstream computation or decoding circuits to use them for the evaluated tasks; representations may be present but not causally connected to the output; discontinuity and clustering of tokens on discrete vertices limit generalization.",
            "comparison_to_humans_or_symbolic": "No explicit comparison; existence of circular encodings aligns qualitatively with toy-model algorithmic solutions (clock/pizza), but GPT-2's failure to use them highlights that representation alone does not imply algorithmic arithmetic competence comparable to symbolic methods.",
            "uuid": "e2994.2",
            "source_info": {
                "paper_title": "Not All Language Model Features Are One-Dimensionally Linear",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 2,
            "sanitized_title": "how_does_gpt2_compute_greaterthan_interpreting_mathematical_abilities_in_a_pretrained_language_model"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "The clock and the pizza: Two stories in mechanistic explanation of neural networks",
            "rating": 2,
            "sanitized_title": "the_clock_and_the_pizza_two_stories_in_mechanistic_explanation_of_neural_networks"
        },
        {
            "paper_title": "Language models use trigonometry to do addition",
            "rating": 1,
            "sanitized_title": "language_models_use_trigonometry_to_do_addition"
        },
        {
            "paper_title": "Language models represent space and time",
            "rating": 1,
            "sanitized_title": "language_models_represent_space_and_time"
        }
    ],
    "cost": 0.015626249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>January February March April May June July August September</p>
<p>Joshua Engels jengels@mit.edu 
Eric J Michaud ericjm@mit.edu 
Isaac Liao iliao@mit.edu 
Wes Gurnee wesg@mit.edu 
Max Tegmark tegmark@mit.edu 
Monday Tuesday 
Wednesday Thursday 
Friday Saturday 
Sunday Other 
January February March 
April May 
June July January February March April May June July August September7D7401C0802F86D0E83B1F8D462782F0arXiv:2405.14860v3[cs.LG]
Recent work has proposed that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space.In contrast, we explore whether some language model representations may be inherently multi-dimensional.We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features.Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B.These auto-discovered features include strikingly interpretable examples, e.g.circular features representing days of the week and months of the year.We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year.Next, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we examine the continuity of the days of the week feature in Mistral 7B.Overall, our work argues that understanding multi-dimensional features is necessary to mechanistically decompose some model behaviors.</p>
<p>INTRODUCTION</p>
<p>Language models trained for next-token prediction on large text corpora have demonstrated remarkable capabilities, including coding, reasoning, and in-context learning (Bubeck et al., 2023;Achiam et al., 2023;Anthropic, 2024;Team et al., 2023).However, the specific algorithms models learn to achieve these capabilities remain largely a mystery to researchers; we do not understand how language models write poetry.Mechanistic interpretability is a field that seeks to address this gap by reverse-engineering trained models from the ground up into variables (features) and the programs (circuits) that process these variables (Olah et al., 2020).</p>
<p>One mechanistic interpretability research direction has focused on understanding toy models in detail.This work has found multi-dimensional representations of inputs such as lattices (Michaud et al., 2024) and circles (Liu et al., 2022;Nanda et al., 2023a), and has successfully reverse-engineered the algorithms that models use to manipulate these representations.A separate direction has identified onedimensional representations of high level concepts and quantities in large language models (Gurnee &amp; Tegmark, 2023;Marks &amp; Tegmark, 2023;Heinzerling &amp; Inui, 2024;Bricken et al., 2023).These findings have led to the linear representation hypothesis (LRH): a hypothesis which has historically claimed both that 1. all representations in pretrained large language models lie along one-dimensional lines, and 2. model states are a simple sparse sum of these representations (Park et al., 2023;Bricken et al., 2023).In this work, we specifically call into question the first part of the LRH. 1 defined above.Jiang et al. (2024) provide theoretical evidence for this hypothesis, assuming a latent (binary) variable-based model of language.Empirically, Bricken et al. (2023) and Cunningham et al. (2023) successfully use sparse autoencoders to break down a model's feature space into an over-complete basis of linear features.These works assume that the number of linear features stored in superposition exceeds the model dimensionality (Elhage et al., 2022).</p>
<p>Multi-Dimensional</p>
<p>Representations: There has been comparatively little research on multidimensional features in language models.Shai et al. (2024) predict and verify that a transformer trained on a hidden Markov model uses a fractal structure to represent the probability of each next token, a clear example of a necessary multi-dimensional feature, but the analysis is restricted to a toy setting.Yedidia (2023a;b) finds that GPT-2 learned position vectors form a helix, which implies a circle when "viewed" from below.Thus, we are not the first to find a circular feature in a language model.However, our work finds circular features that represent latent concepts from text, while the GPT-2 learned position vectors are specific to tokenization, separate from the rest of the model parameters, and causally implicated only due to positional attention masking.Another suggestive result, due to Hanna et al. (2024), is the presence of a U-shape in the representation of numbers between 0 and 100; however, Hanna et al. (2024) find that this representation is not causal, and they only show it exists within a specific prompt distribution.Recent work on dictionary learning (Bricken et al., 2023) has speculated about multi-dimensional feature manifolds; our work is similar to this direction and develops the idea of feature manifolds theoretically and empirically.Finally, in a separate direction, Black et al. (2022) argue for interpreting neural networks through the polytopes they split the input space into, and identifies regions of low polytope density as "valid" regions for a potential linear representation.</p>
<p>Circuits: Circuits research seeks to identify and understand circuits, subsets of a model (usually represented as a directed acyclic graph) that explain specific behaviors (Olah et al., 2020).The base units that form a circuit can be layers, neurons (Olah et al., 2020), or sparse autoencoder features (Marks et al., 2024).In the first circuits-style work, Olah et al. (2020) found line features that were combined into curve detection features in the InceptionV1 image model.More recent work has examined language models, for example the indirect object identification circuit in GPT-2 (Wang et al., 2022).Given the difficulty of designing bespoke experiments, there has been increased research in automated circuit discovery methods (Marks et al., 2024;Conmy et al., 2023;Syed et al., 2023).</p>
<p>Interpretability for Arithmetic Problems: Liu et al. (2022) study models trained on modular arithmetic problems a + b = c (mod m) and find that models that generalize well have circular representations for a and b.Further work by Nanda et al. (2023a) and Zhong et al. (2024) shows that models use these circular representations to compute c via a "clock" algorithm and a separate "pizza" algorithm.These papers are limited to the case of a small model trained only on modular arithmetic.Another direction has studied how large language models perform basic arithmetic, including a circuits level description of the greater-than operation in GPT-2 (Hanna et al., 2024) and addition in GPT-J (Stolfo et al., 2023).These works find that to perform a computation, models copy pertinent information to the token before the computed result and perform the computation in the subsequent MLP layers.Finally, recent work by Gould et al. (2023) investigates language models' ability to increment numbers and finds linear features that fire on tokens equivalent modulo 10.</p>
<p>DEFINITIONS</p>
<p>This section focuses on hypotheses for how hidden states of language models can be decomposed into sums of functions of the input (features).We focus on L layer transformer models M that take in token input t = (t 1 , . . ., t n ) from input token distribution T , have hidden states x 1,l , . . ., x n,l for layers l, and output logit vectors y 1 , . . ., y n .Given a set of inputs T , we let X i,l be the set of all corresponding x i,l .We write matrices in capital bold, vectors and vector valued functions in lowercase bold, and sets in capital non-bold.</p>
<p>MULTI-DIMENSIONAL FEATURES</p>
<p>Definition 1 (Feature).We define a d f -dimensional feature as a function f that maps a subset of the input space into R d f .We say that a feature is active on the aforementioned subset.</p>
<p>The input token distribution T induces a d f -dimensional probability distribution over feature vectors f (t).As an example, let n = 1 (so inputs are single tokens) and consider a feature f that maps integer tokens to their integer values in R 1 .Then f is a 1-dimensional feature that is active on integer tokens, and f (t) is the marginal integer occurrence distribution from the token distribution.</p>
<p>How can we differentiate "true" multi-dimensional features from sums of lower dimensional features?We make this distinction by examining the reducibility of a potential multi-dimensional feature.That is, f is a "true" multi-dimensional feature if it cannot be written as the sum of two statistically independent features and it cannot be written as the sum of two non-co-occurring features.Formally, we have the following definition: Definition 2. A feature f is reducible into features a and b if there exists an affine transformation
f → Rf + c ≡ a b(1)
for some orthonormal d f × d f matrix R and additive constant c, such that the transformed feature probability distribution p(a, b) satisfies at least one of these conditions:</p>
<ol>
<li>p is separable, i.e., factorizable as a product of its marginal distributions:
p(a, b) = p(a)p(b).</li>
<li>p is a mixture, i.e., a sum of disjoint distributions, one of which is lower dimensional:
p(a, b) = wp(a)δ(b) + (1 − w)p(a, b)
Here, p is a probability density function that is conditional on the subset of T that f is active on, δ is the Dirac delta function, and 0 &lt; w &lt; 1.By two probability distributions being disjoint, we mean that they have disjoint support (there is no set where both have positive probability measure, or equivalently the two features a and b cannot be active at the same time).In Eq. ( 1), a is the first k components of the vector Rf + c and b is the remaining d f − k components.When p is separable or a mixture, we also say that f is separable or a mixture.We term a feature irreducible if it is not reducible, i.e., if no rotation and translation makes it separable or a mixture.</li>
</ol>
<p>An example of a feature that is a mixture is a one hot encoding along a simplex; an example of a feature that is separable is a normal distribution2 .In natural language, a mixture might be a one hot encoding of "breed of dog", while a separable distribution might be the "latitude" and "longitude" of location tokens.</p>
<p>In practice, the mixture and separability definitions may not be precisely satisfied.Thus, we soften our definitions to permit degrees of reducibility: Definition 3 (Separability Index and ϵ-Mixture Index).Consider a feature f .The separability index S(f ) measures the minimal mutual information between all possible a and b defined in Eq. (1):
S(f ) ≡ min I(a; b)(2)
where I denotes the mutual information.Smaller values of S(f ) mean that f is more separable.</p>
<p>The ϵ-mixture index M ϵ (f ) tests how often f can be projected near zero while it is active:
M ϵ (f ) = max v∈R d f , c∈R P t∈T |v • f (t) + c| &lt; ϵ E<a href="3">(v • f (t) + c) 2 </a>
Larger values of M ϵ (f ) mean that f is more of a mixture.</p>
<p>In Appendix B, we expand on the intuition behind why the separability and ϵ-mixture indices as defined here correspond to weakened versions of Definition 2.</p>
<p>We develop optimization procedures to empirically solve for the separability and ϵ-mixture indices of two dimensional feature distributions.At a high level, the separability procedure iterates over a sweep of rotations and estimates the mutual information between the axes for each angle, while the ϵ-mixture index procedure performs gradient descent to find the ϵ band that contains the largest possible fraction of the feature distribution.For more details on the implementation of the tests, see Appendix B.2.In Section 4, we apply these empirical tests to real language model feature distributions to find irreducible multi-dimensional features; we show the detailed test results on the "days of the week" cluster in Fig. 2 PCA Figure 2: Empirical ϵ-mixture index and separability index for the "days of the week" cluster along PCA components 2 and 3. Left: The ϵ band parameterized by v and c that the optimization procedure found contained the highest fraction of points.Mid: Dot products of points in the feature distribution with the ϵ band; M ϵ (f ) is the percent of dot products within ϵ = 0.1 of 0. Right: Estimated mutual information for different rotations of the space; S(f ) is the minimum over all rotations.This point cloud has a lower ϵ-mixture index and higher separability index than PCA projections within typical clusters (see Fig. 3), indicating that it is more likely to be an irreducible multi-dimensional feature.</p>
<p>SUPERPOSITION</p>
<p>In this section, we propose an updated superposition hypothesis (Elhage et al., 2022) that takes into account multi-dimensional features.First, we restate the original superposition hypothesis: Definition 4 (δ-orthogonal matrices).Two matrices
A 1 ∈ R d×d1 and A 2 ∈ R d×d2 are δ-orthogonal if |x 1 • x 2 | ≤ δ for all unit vectors x 1 ∈ colspace(A 1 ) and x 2 ∈ colspace(A 2 ).
Hypothesis 1 (One-Dimensional Superposition Hypothesis, paraphrased from (Elhage et al., 2022)).Hidden states x i,l are the sum of many (≫ d) sparse one-dimensional features f i and pairwise δ-orthogonal vectors v i such that x i,l (t) = i v i f i (t).We set f i (t) to zero when t is outside the domain of f i .</p>
<p>In contrast, our new superposition hypothesis posits independence between irreducible multidimensional features instead of unknown levels of independence between one-dimensional features: Hypothesis 2 (Multi-Dimensional Superposition Hypothesis, changes underlined).Hidden states x i,l are the sum of many (≫ d) sparse low-dimensional irreducible features f i and pairwise δ-orthogonal matrices V i ∈ R d×d f i such that x i,l (t) = i V i f i (t).We set f i (t) to zero when t is outside the domain of f i .</p>
<p>Note that since multi-dimensional features can be written as the sums of projections of lowerdimensional features, our new superposition hypothesis is a stricter version of Hypothesis 1.In the next section, we will explore empirical evidence for our hypothesis, while in Appendix A, we prove upper and lower bounds on the number of δ-almost orthogonal matrices V i that can be packed into d dimensional space.</p>
<p>SPARSE AUTOENCODERS FIND MULTI-DIMENSIONAL FEATURES</p>
<p>In this section, we describe a method to identify multi-dimensional features in language model hidden states using sparse autoencoders (SAEs).Sparse autoencoders (SAEs) deconstruct model hidden states into sparse vector sums from an over-complete basis (Bricken et al., 2023;Cunningham et al., 2023).For hidden states X i,l , a one-layer SAE of size m with sparsity penalty λ minimizes the following dictionary learning loss (Bricken et al., 2023;Cunningham et al., 2023):
DL(X i,l ) = arg min E∈R m×d ,D∈R d×m x i,l ∈X i,l ∥x i,l − D • ReLU(E • x i,l )∥
In practice, the L 0 loss on the last term is relaxed to L p for 0 &lt; p ≤ 1 to make the loss differentiable.We call the m columns of D (vectors in R d ) dictionary elements.</p>
<p>We now argue that SAEs can discover irreducible multi-dimensional features by clustering D. We will consider a simple form of clustering: build a complete graph on D with edge weights equal to the cosine similarity between dictionary elements, prune all edges below a threshold T , and then set the clusters equal to the connected components of the graph.If we now consider the spaces spanned by each cluster, they will be approximately T -orthogonal by construction, since their basis vectors are all T -orthogonal.Now, consider some irreducible two-dimensional feature f ; we claim that if the SAE is large enough and f is active enough such that the SAE can reconstruct f when f is active, one of the clusters is likely to be exactly equal to f .If D includes just two dictionary elements spanning f , then these elements both must have nonzero activations post-ReLU to reconstruct f (otherwise f is a mixture).Because of the sparsity penalty in Eq. ( 4), this two-vector solution to reconstruct f is disincentivized, so instead the dictionary is likely to learn many elements that span f .These dictionary elements will then have a high cosine similarity, and so the edges between them will not be pruned away during the clustering process; hence, they will be in a cluster.</p>
<p>Thus, we have a way to operationalize Hypothesis 2: clustering D finds T -orthogonal subspaces, and if irreducible multi-dimensional features exist, they are likely to be equal to some of these subspaces.This suggests a natural approach to using sparse autoencoders to search for irreducible multi-dimensional features:</p>
<ol>
<li>
<p>Cluster dictionary elements by their pairwise cosine similarity.We use both the simple similaritybased pruning technique described above, as well as spectral clustering; see Appendix F for details, including comments on scalability.</p>
</li>
<li>
<p>For each cluster, run the SAEs on all x i,l ∈ X i,l and ablate all dictionary elements not in the cluster.This will give the reconstruction of each x i,l restricted to the cluster found in step 1 (if no cluster dictionary elements are non-zero for a given point, we ignore the point).</p>
</li>
<li>
<p>Examine the resulting reconstructed activation vectors for irreducible multi-dimensional features.This step can be done manually by visually inspecting the PCA projections for known irreducible multi-dimensional structures (e.g.circles, see Fig. 10) or automatically by passing the PCA projections to the tests for Definition 3.  Pseudocode for this method is in the appendix in Alg. 1.This method succeeds on toy datasets of synthetic irreducible multi-dimensional features; see Appendix D. 3 We apply this method to language models using GPT-2 (Radford et al., 2019) SAEs trained by Bloom (2024) for every layer and Mistral 7B (Jiang et al., 2023) SAEs that we train on layers 8, 16, and 24 (training details in Appendix E).</p>
</li>
</ol>
<p>Strikingly, we reconstruct irreducible multi-dimensional features that are interpretable circles: in GPT-2, days, months, and years are arranged circularly in order (see Fig. 1); in Mistral 7B, days and months are arranged circularly in order (see Fig. 15).These plots contain the PCA dimensions that most clearly show circular structure; these best dimensions are usually the second and third because the first PCA dim is an "intensity" direction that manifests as the radius of the circle in Fig. 1 (thus the overall structure for these multi-d features is perhaps best thought of as a cone).See Fig. 14 for all PCA dimensions visualized).</p>
<p>For each cluster of GPT-2 SAE features, we take the reconstructed activations and project them onto PCA components 1-2, 2-3, 3-4, and 4-5 (or fewer if there are fewer features in the cluster) and measure the separability index and ϵ-mixture index of each 2D point cloud as described in Appendix B.2.The mean scores across these planes are a computationally tractable approximation of Definition 3. We plot these mean scores in Fig. 3, and find that the features which we had manually identified in Fig. 1 are among the top scoring features along  both measures of irreducibility.Thus, our theoretical tests can indeed be used to find interpretable irreducible features.We show the top 20 feature clusters, measured by the product of (1 − ϵ-mixture index ) and separability index, in Appendix G. Out of all 1000 clusters, the Fig. 1 clusters rank 9, 28, and 15 by this metric, respectively.4</p>
<p>CIRCULAR REPRESENTATIONS IN LARGE LANGUAGE MODELS</p>
<p>In this section, we examine tasks in which models use the multi-dimensional features we discovered in Section 4, thereby providing evidence that these representations are indeed the fundamental unit of computation for some problems.Inspired by prior work studying circular representations in modular arithmetic (Liu et al., 2022), we define two prompts that represent "natural" modular arithmetic tasks:</p>
<p>Weekdays task: "Let's do some day of the week math.Two days from Monday is" Months task: "Let's do some calendar math.Four months from January is"</p>
<p>For Weekdays, we range over the 7 days of the week and durations between 1 and 7 days to get 49 prompts.For Months, we range over the 12 months of the year and durations between 1 and 12 months to get 144 prompts.Mistral 7B and Llama 3 8B (AI@Meta, 2024) achieve reasonable performance on the Weekdays task and excellent performance on the Months task (measured by comparing the highest logit valid token against the ground truth answer), as summarized in Table 1.Interestingly, although these problems are equivalent to modular arithmetic problems α + β ≡ ?(mod m) for m = 7, 12, both models get trivial accuracy on plain modular addition prompts, e.g."5 + 3 (mod 7) ≡".Finally, although GPT-2 has circular representations, it gets trivial accuracy on Weekdays and Months.</p>
<p>To simplify discussion, let α be the day of the week or month of the year token (e.g."Monday" or "April"), β be the duration token (e.g."four" or "eleven"), and γ be the target ground truth token the model should predict, such that (abusing notation) we have α + β = γ.Let the prompts of the task be parameterized by j, such that the jth prompt asks about α j , β j , and γ j .</p>
<p>We confirm that Llama 3 8B and Mistral 7B have circular representations of α on this task by examining the PCA projections of hidden states across prompts at various layers on the α token.We plot two of these in Fig. 4 and show all layers in Fig. 18.These plots show circular representations as the highest varying two components in the model's representation of α at many layers.</p>
<p>INTERVENING ON CIRCULAR DAY AND MONTH REPRESENTATIONS</p>
<p>We now experiment with intervening on these circular representations.We base our experiments on the common interpretability technique of activation patching, which replaces activations from a "dirty" run of the model with the corresponding activations from a "clean" run (Zhang &amp; Nanda, 2023).Activation patching empirically tests whether a specific model component, position, and/or representation has a causal influence on the model's output.We employ a custom subspace patching Published as a conference paper at ICLR 2025</p>
<p>Step 2: Intervene with</p>
<p>Step 1: Learn Circular Probe method to allow testing for whether a specific circular subspace of a hidden state is sufficient to causally explain model output.Specifically, our patching technique relies on the following steps (visualized in Fig. 5):</p>
<ol>
<li>
<p>Find a subspace with a circle to intervene on: Using a PCA reduced activation subspace to avoid overfitting, we train a "circular probe" to identify representations which exhibit strong circular patterns.More formally, let x j i,l be the hidden state at layer l token position i for prompt j.Let W i,l ∈ R k×d be the matrix consisting of the top k principal component directions of x j i,l .In our experiments, we set k = 5.We learn a linear probe P ∈ R 2,k from W i,l • X i,l to a unit circle in α.In other words, if circle(α) = [cos(2πα/7), sin(2πα/7)] for Weekdays and circle(α) = [cos(2πα/12), sin(2πα/12)] for Months, P is defined as follows:
P = arg min P ′ ∈R 2,k x j i,l P ′ • W i,l • x j i,l − circle(α) 2 2
(5)</p>
</li>
<li>
<p>Intervene on the subspace: Say our initial prompt had α = α j and we are intervening with α = α j ′ .In this step, we replace the model's projection on the subspace P • W i,l , which will be close to circle(α j ), with the "clean" point circle(α j ′ ).Note that we do not use the hidden state x j ′ i,l from the "clean" run, only the "clean" label α j ′ .In practice, other subspaces of x j i,l may be used concurrently by the model in "backup" circuits (see e.g.Wang et al. (2022)) to compute the answer, so if we just intervene on the circular subspace the remaining components of the activation may interfere in downstream computations.Thus, to isolate the effect of our intervention, we set the average ablate the portion of the activation not in the intervened subspace.Letting x i,l be the average of x j i,l across all prompts indexed by j and P + be the pseudoinverse of P, we intervene via the formula
x j * i,l = x i,l + W i,l T P + (circle(α j ′ ) − x i,l )(6)
We run our patching on all 49 Weekday problems and 144 Month problems and use as "clean" runs the 6 or 11 other possible values for β, resulting in a total of 49 * 6 patching experiments for Weekdays and 144 * 11 patching experiments for Months.We also run baselines where we (1) replace the entire subspace corresponding to the first 5 PCA dimensions with the corresponding subspace from the clean run, (2) replace the entire layer with the corresponding layer from the clean run, and (3) replace the entire layer with the average across the task.The metric we use is average logit difference across all patching experiments between the original correct token (α j ) and the target token (α j ′ ).See Fig. 6 for these interventions on all layers of Mistral 7B and Llama 3 8B on Weekdays and Months.Patching experiments in Appendix J show α is copied to the final token on layers 15 to 17, which is why interventions drop off there.Additionally, while in this section we train a probe on a dataset of prompts, in Section 5.2, we show that intervening on the circle discovered via SAE clustering in Section 4 also works.</p>
</li>
</ol>
<p>To investigate exactly how models use the circular subspace, we perform off distribution interventions.We modify Eq. ( 6) so that instead of intervening on the circumference circle(α), we sweep over a grid of positions (r, θ) within the circle:
x j * i,l = x i,l + W i,l T P + [r cos(θ), r sin(θ)] T − x i,l ) (7)
We intervene with r ∈ [0, 0.1, . . ., 2], θ ∈ [0, 2π/100, . . ., 198π/100] and record the highest logit γ after the forward pass.</p>
<p>INTERVENING WITH THE SAE PLANE</p>
<p>In the last section, we train probes manually on the PCA of the activations to fit a circle.A perhaps more natural approach is intervening with the precise circle we found in Section 4. To determine if this approach is feasible, we first project layer 8 Mistral 7B Weekdays activations into the weekdays plane that was discovered by clustering (see Fig. 15; the plane is defined by PCA dimensions 2 and 3 of the cluster).In the rest of this section, we call this plane the SAE plane.In Fig. 19, we find that indeed, the Weekdays representations projected into the SAE plane form a circle (see Fig. 19).We thus can fit a circular probe to this 2D plane as in Eq. ( 5).Similarly, we call this probe the SAE probe.Because we only have layer 8 clustering results for Mistral, we train an SAE probe only on layer 8.We then evaluate interventions using this SAE probe on layer 8 of Mistral, but also at neighboring layers, since nearby layers should have similar representations (see e.g.Belrose et al. ( 2023)).We compare to two baselines: 1) training a normal circular probe on the PCA projections of each layer as described in Section 5.1, and 2) training a circular probe only on layer 8 and then evaluating on adjacent layers (in the same way as for the layer 8 SAE probe).We show the results of these methods in Fig. 8.</p>
<p>We find that on all layers, using the SAE probe only slightly decreases intervention performance as compared to training a circular probe (from -2.58 to -2.01 average logit difference on layer 8).Even more interestingly, the layer 8 SAE probe is much more robust to layer shifts than the layer 8 circular probe; for example, using the layer 8 circular probe on layer 6 results in an average logit difference of 0.029, whereas using the layer 8 SAE probe results in an average logit difference of -2.32.This is intriguing evidence that the SAE is perhaps finding more "true" (or at least more robust) features than our circular probing technique.In past sections, the representations of the interpretable numeric quantities we have discovered have been mostly discontinuous; that is, the days of the week and months of the year in Fig. 1 and Fig. 15 are clustered at the vertices of a heptagon and dodecagon, and there is nothing "between" adjacent weekdays or months along the circle.In this section, we will examine the "continuity" of the circular features we have discovered.Although continuity of the representation is not a requirement of Definition 3, it would further decrease the ϵ-mixture index, and would also increase our subjective perception of the circular feature as an intrinsic model feature representing a continuous quantity (time).Thus, we create a synthetic dataset containing the text "[very early/very late] on [Monday/Tuesday/.../Sunday]" and simply plot the projections of the layer 30 activations into the top two PCA components of the activations of [Monday/Tuesday/.../Sunday].The results, shown in Fig. 9, show that Mistral 7B indeed can map intermediate quantities to their expected place in the circle: the very early and very late version of each weekday are more towards the last and the next weekday along the circle, respectively.We show similar results for "[morning/evening] on [Monday/Tuesday/.../Sunday]" in Appendix Fig. 22.</p>
<p>CONTINUITY OF CIRCULAR REPRESENTATIONS</p>
<p>DISCUSSION</p>
<p>Our work proposes a significant refinement to the simple one-dimensional linear representation hypothesis.While previous work has convincingly shown the existence of one-dimensional features, we find evidence for irreducible multi-dimensional representations, requiring us to generalize the notion of a feature to higher dimensions.Fortunately, we find that existing unsupervised feature extraction methodologies like sparse autoencoders can readily be applied to discover multi-dimensional representations.However, we think our work raises interesting questions about whether individual SAE features are appropriate "mediators" (Mueller et al., 2024) for understanding model computation, if some features are in fact multi-dimensional.Although taking a multi-dimensional representation perspective may be more complicated, we believe that uncovering the true (perhaps multi-dimensional) nature of model representations is necessary for discovering the underlying algorithms that use these representations.Ultimately, our field aims to turn complex circuits in future more-capable models into formally verifiable programs (Tegmark &amp; Omohundro, 2023;Dalrymple et al., 2024), which requires the ground truth "variables" of language models; we believe this work takes an important step towards discovering these variables.</p>
<p>Limitations: It is unclear why we did not find more interpretable multi-dimensional features.We are unsure if we are failing to interpret some of the high-scoring multi-dimensional features, if most multi-dimensional features lie in dimensions higher than two, if our clustering technique is not powerful enough to find some features, or if there are truly not that many.Additionally, our definitions for irreducible features (Definition 2) are purely statistical and not intervention based, and also had to be relaxed to hold in practice, resulting in measures that return a possibly subjective "degree" of reducibility (Definition 3).Thus, although this work provides preliminary evidence for the multi-dimensional superposition hypothesis (Hypothesis 2), it is still unclear if this theory provides the best description for the representations models use.Future work might make progress on this question by investigating new techniques for decomposing model representations, exploring higher dimensional representations, or determining conclusively whether models use representations in ways that necessitate the representations are non-linear.</p>
<p>A MULTI-DIMENSIONAL FEATURE CAPACITY</p>
<p>The Johnson-Lindenstrauss (JL) Lemma (Johnson &amp; Lindenstrauss, 1984) implies that we can choose e Cdδ 2 pairwise one-dimensional δ-orthogonal vectors to satisfy Hypothesis 1 for some constant C, thus allowing us to build the model's feature space with a number of one-dimensional δ-orthogonal features exponential in d.We now prove a similar result for low-dimensional projections (the main idea of the proof is to combine δ-orthogonal vectors as guaranteed from the JL lemma):</p>
<p>Theorem 1.For any d ′ and δ, it is possible to choose 1 dmax e C1(d/d ′2 )δ 2 pairwise δ-orthogonal matrices A i ∈ R ni×d ′ for some constant C 1 .Furthermore, it is not possible to choose more than e C2(d−dmaxδ log( 1 δ )) for some constant C 2 .</p>
<p>We will first prove a lemma that will help us prove Theorem 1. Lemma 1. Pick n pairwise δ-orthogonal unit vectors in v 1 , . . ., v n ∈ R d .Let y ∈ R d be a unit norm vector that is a linear combination of unit norm vectors v 1 , . . ., v n with coefficients z 1 . . ., z n ∈ R.</p>
<p>We can write A = [v 1 , . . ., v n ] and z = [z 1 , . . ., z n ] T , so that we have y
= n k=1 z k v k = Az T with ∥y∥ 2 = 1. Then, n k=1 z k = ∥z∥ 1 ≤ n 1 − δn
Proof.We will first bound the L 2 norm of z.If σ n is the minimum singular value of A, then we have via standard singular value inequalities (Higham, 2021)
σ n ≤ ∥y∥ 2 ∥z∥ 2 =⇒ ∥z∥ 2 ≤ ∥y∥ 2 σ n = 1 σ n
Thus we now lower bound σ n .The singular values are the square roots of the eigenvalues of the matrix A T A, so we now examine A T A. Since all elements of A are unit vectors, the diagonal of A T A is all ones.The off diagonal elements are dot products of pairs of δ-orthogonal vectors, and so are within the range [−δ, δ].Then by the Gershgorin circle theorem (Gershgorin, 1931), all eigenvalues λ i of A T A are in the range
(1 − δ(n − 1), 1 + δ(n − 1))
In particular, σ 2 n = λ n ≥ 1 − δ(n − 1), and thus σ n ≥ 1 − δ(n − 1).Plugging into our upper bound for ∥z∥ 2 , we have that ∥z∥ 2 ≤ 1/ 1 − δ(n − 1).Finally, the largest L 1 for a point on an n-hypersphere of radius r is when all dimensions are equal and such a point has magnitude √ nr, so
∥z∥ 1 ≤ n 1 − δ(n − 1) ≤ n 1 − δn Theorem 1. For any d ′ and δ, it is possible to choose 1 dmax e C1(d/d ′2 )δ 2 pairwise δ-orthogonal matrices A i ∈ R ni×d ′ for some constant C 1 . Furthermore, it is not possible to choose more than e C2(d−dmaxδ log( 1 δ )) for some constant C 2 .
Proof.By the JL lemma (Johnson &amp; Lindenstrauss, 1984; , https://mathoverflow.net/users/2554/bill johnson), for any d and δ, we can choose e Cdδ 2 δ-orthogonal unit vectors in R d indexed as v i , for some constant C.
Let A i = [v dmax * i , . . . , v dmax * i+ni−1 ]
where each element in the brackets is a column.Then by construction all A i are matrices composed of unique δ-orthogonal vectors and there are 1 dmax e Cdδ 2 matrices A i .Now, consider two of these matrices A i = [v 1 , . . ., v ni ] and A j = [u 1 , . . ., u nj ], i ̸ = j; we will prove that they are f (δ)-orthogonal for some function f .Let y i = ni k=1 z i,k v k be a vector in the colspace of A i and y j = nj k=1 z j,k u k be a vector in the colspace of A j , such that y i and y j are unit vectors.To prove f (δ)-orthogonality, we must bound the absolute dot product between y i and y j :
|y i • y j | = ni k=1 z i,k v k • nj k=1 z j,k u k = ni k1=1 nj k2=1 (z i,k1 v k1 ) • (z j,k2 u k2 ) ≤ ni k1=1 nj k2=1 |z i,k1 z j,k2 | |v k1 • u k2 | Triangle Inequality ≤ ni k1=1 nj k2=1 |z i,k1 z j,k2 | δ All v i , u j are δ orthogonal = δ ni k1=1 nj k2=1 |z i,k1 z j,k2 | = δ ni k=1 z i,k nj k=1 z j,k Factoring the product ≤ δ n i 1 − δn i n j 1 − δn j By Lemma 1 ≤ δd max 1 − δd max n i , n j ≤ d max by assumption
Thus A i and A j are f (δ)-orthogonal for f (δ) = δd max /(1 − δd max ), and so it is possible to choose 1 dmax e Cdδ 2 pairwise f (δ)-orthogonal projection matrices.Remapping the variable δ with δ → f −1 (δ) = δ/(d max (1 + δ)), we find that it is possible to choose 1 dmax e Cdδ 2 /((1+δ) 2 d 2 max )</p>
<p>pairwise δ-orthogonal projection matrices.Because 1 + δ is at most 2 with δ ∈ (0, 1), we can further simplify the exponent and find that it is possible to choose 1 dmax e C(d/d 2 max )δ 2 /4 pairwise δ-orthogonal projection matrices.Absorbing the 4 into the constant C finishes the proof of the lower bound.</p>
<p>For the upper bound, we can proceed much more simply.Consider k pairwise δ-orthogonal matrices
A i ∈ R d ′
. Since these matrices are full rank, their column spaces each parameterize a subspace of dimension d ′ , and so by a result from (Alon, 2003) it is possible to choose e Cd ′ δ 2 log( 1 δ ) almost orthogonal vectors in this subspace.Furthermore, by our definition of δ-orthogonal matrices, all pairs of these vectors between subspaces will be δ-orthogonal.Finally, again by (Alon, 2003) we cannot have more than e Cdδ 2 log( 1 δ ) δ-orthogonal vectors overall, so we have that
ke Cdmaxδ 2 log( 1 δ ) &lt; e Cdδ 2 log( 1 δ )
and simplfying,
k &lt; e C(d−dmax)δ 2 log( 1 δ )
These results imply that models can still represent an exponential number of higher dimensional features.However, there is a large exponential gap between the lower and upper bound we have shown.</p>
<p>If the lower bound is reasonably tight, then this would mean that models would be highly incentivized to fit features within the smallest dimensional space possible, suggesting a reason for recent work showing interesting encodings of multi-dimensional features in toy problems (Morwani et al., 2023).</p>
<p>Note that the proof assumes the "worst case" scenario that all of the features are dimension d max , while in practice many of the features may be 1 or low dimensional, so the effect on the capacity of a real model that represents multi-dimensional features is unlikely to be this extreme.</p>
<p>Finally, we note that the dictionary learning literature may have discovered similar results in the past (which we were unaware of), see Foucart &amp; Rauhut (2013).</p>
<p>B MORE ON REDUCIBILITY B.1 ADDITIONAL INTUITION FOR DEFINITIONS</p>
<p>Here, we present some extra intuition and high level ideas for understanding our definitions and the motivation behind them.Roughly, we intend for our definitions in the main text to identify representations in the model that describe an object or concept in a way that fundamentally takes multiple dimensions.We operationalize this as finding a subspace of representations that 1. has basis vectors that "always co-occur" no matter the orientation 2. is not made up of combinations of independent lower-dimensional features.</p>
<p>1.The first condition is met by the mixture part of our definition.The feature in question should be part of an irreducible manifold, and so should "fill" a plane or hyperplane.There shouldn't be any part of the plane where the probability distribution of the feature is concentrated, because this region is then likely part of a lower dimensional feature.The idea of this part of the definition is to capture multi-dimensional objects; if the entire multi-dimensional space is truly being used to represent a high-dimensional object, then the representations for the object should be "spread out" entirely through the space.</p>
<ol>
<li>The second condition is met by the separability part of our definition.This part of the definition is intended to rule out features that co-occur frequently but are fundamentally not describing the same object or concept.For example, latitude and longitude are not a mixture in that they frequently cooccur, but we do not think it is necessarily correct to say they are part of the same multi-dimensional feature because they are independent.</li>
</ol>
<p>B.2 EMPIRICAL IRREDUCIBLE FEATURE TEST DETAILS</p>
<p>Our tests for reducibility require the computation of two quantities S(f ) for the separability index and M ϵ (f ) for the ϵ-mixture index.We describe how we compute each index in the following two subsections.</p>
<p>B.2.1 SEPARABILITY INDEX</p>
<p>We define the separability index in Equation 2as
S(f ) = min I(a; b)
where the min is over rotations R used to split f ′ = Rf + c into a and b.In two dimensions, the rotation is defined by a single angle, so we can iterate over a grid of 1000 angles and estimate the mutual information between a and b for each angle.We first normalize f by subtracting off the mean and then dividing by the root mean squared norm of f (and multiplying by √ 2 since the toy datasets are in two dimensions).To estimate the mutual information, we first clip the data f to a 6 by 6 square centered on the origin.We then bin the points into a 40 by 40 grid, to produce a discrete distribution p(a, b).After computing the marginals p(a) and p(b) by summing the distribution over each axis, we obtain the mutual information via the formula
I(a; b) = a,b p(a, b) log p(a, b) p(a)p(b)(8)</p>
<p>B.2.2 ϵ-MIXTURE INDEX</p>
<p>We define the ϵ-mixture index in Equation 3as For feature a, 63.96% lies within the narrow dotted lines, indicating the feature is likely a mixture.For feature b, 17.84% lies within the wide lines, indicating the feature is unlikely to be a mixture.The green cross indicates the angle θ that minimizes mutual information.Middle in each subfigure: Histograms of the distribution of v • x with red lines indicating a 2ϵ-wide region.Right in each subfigure: Mutual information between a and b as a function of the rotation angle θ of matrix R. Feature b has a large minimum mutual information so is unlikely to be separable; feature a has a medium value of minimum mutual information of about 0.37 bits.
M ϵ (f ) = max v∈R d f , c∈R P |v • f + c| &lt; ϵ E[(v • f + c) 2 ] representation dim 1 representation dim 2 −5 0 5 normalized v • f + c
The challenge with computing M ϵ (f ) is to compute the maximum.We opted to maximize via gradient descent; and we guaranteed differentiability by softening the inequality &lt; with a sigmoid,
M ϵ,T (f , v, c) =E σ 1 T ϵ − |v • f + c| E[(v • f + c) 2 ] (9)
where T is a temperature, which we linearly decay from 1 to 0 throughout training.We optimize for v and c using this loss M ϵ,T (f , v, c) using full batch gradient descent over 10000 steps with learning rate 0.1.With the solution (v * , c * ), the final value of M ϵ,T =0 (f , v * , c * ) is then our estimate of M ϵ (f ).</p>
<p>We also run the irreducibility tests on additional synthetic feature distributions in Fig. 11a and Fig. 11b.</p>
<p>C ALTERNATIVE DEFINITIONS</p>
<p>In this section, we present an alternative definition of a reducible feature that we considered during our work.This chiefly deals with multi-dimensional features from the angle of computational reducibility as opposed to statistical reducibility.In other words, this definition considers whether representations of features on a specific set of tasks can be split up without changing the accuracy of the task.This captures an interesting (and important) aspect of feature reducibility, but because it requires a specific set of prompts (as opposed to allowing unsupervised discovery) we chose not to use it as our main definition.</p>
<p>Our alternative definitions consider representation spaces that are possibly multi-dimensional, and defines these spaces through whether they can completely explain a function h on the output logits.We consider a group theoretic approach to irreducible representations, via whether computation involving multiple group elements can be decomposed.For feature c, 7.94% lies within the narrow dotted lines, indicating the feature is unlikely to be a mixture.For feature d, 25.90% lies within the wide lines, indicating the feature is likely a mixture.The green cross indicates the angle θ that minimizes mutual information.Middle in each subfigure: Histograms of the distribution of v • x with red lines indicating a 2ϵ-wide region.Right in each subfigure: Mutual information between a and b as a function of the rotation angle θ of matrix R. Both features have a small (&lt; 0.5 bits) minimum mutual information and so are likely separable.</p>
<p>C.1 ALTERNATIVE DEFINITION: INTERVENTIONS AND REPRESENTATION SPACES</p>
<p>Assume that we restrict the input set of prompts T = {t j } to some subset of prompts and that we have some evaluation function h that maps from the output logit distribution of M to a real number.For example, for the Weekdays problems, T is the set of 49 prompts and h could be the arg max over the days of week logits.Abusing notation, we let M also be the function from the layer we are intervening on; this is always clear from context.Then we can define a representation space of x j i,l as a subspace in which interventions always work: Definition 5 (Representation Space).Given a prompt set T = {t j }, a rank-r dimensional representation space of intermediate value x j i,l is a rank r projection matrix P such that for all j, j ′ , h(M ((
I − P )x j i,l + P x j ′ i,l )) = h(M (x j ′ i,l )).
Note that it immediately follows that the rank d dimensional matrix I d is trivially a rank d representation space for all prompt sets T .Definition 6 (Minimality).A representation space P of rank r is minimal if there does not exist a lower rank representation space.</p>
<p>A minimal representation with rank &gt; 1 is a multi-dimensional representation.Definition 7 (Alternative Reducibility).A representation space P of rank r is reducible if there are orthonormal representation spaces P 1 and P 2 (such that P 1 + P 2 = P , P 1 P 2 = 0) where h(M (P 1 x j i,l ) + M (P 2 x j i,l )) = h(M (P 1 x j i,l + P 2 x j i,l )) for all j, j ′ .Suppose T , h and M define the multiplication of two elements in a finite group G of order n.Then if we interpret the embedding vectors as the group representations, our definition of reducibility implies to the standard group-theoretical definition of irreducibility --specifically, reducibility into a tensor product representation.</p>
<p>D TOY CASE OF TRAINING SAES ON CIRCLES</p>
<p>To explore how SAEs behave when reconstructing irreducible features of dimension d f &gt; 1, we perform experiments with the following toy setup.Inspired by the circular representations of integers that networks learn when trained on modular addition (Nanda et al., 2023a;Liu et al., 2022), we create synthetic datasets of activations containing multiple features which are each 2d irreducible circles.</p>
<p>First however, consider activations for a single circle -points uniformly distributed on the unit circle in R 2 .We train SAEs on this data with encoder Enc(x) = ReLU(W e (x − b d ) + b e ) and decoder Dec(f ) = W d f + b d .We train SAEs with m = 2 and m = 10 with the Adam optimizer and a learning rate of 10 −3 , sparsity penalty λ = 0.1, for 20,000 steps, and a warmup of 1000 steps.In Fig. 12 we show the dictionary elements of these SAEs.When m = 2, the SAE must use both SAE features on each input point, and uses d b to shift the reconstructed circle so it is centered at the origin.When m = 10, the SAE learns d b ≈ 0 and the features spread out across the circle, arranged close together, and only a subset are active on each input.When there are several SAE features, there is no natural/canonical choice of feature directions, and the dictionary elements spread out across the circle.</p>
<p>We now consider synthetic activations with multiple circular features.Our data consists of points in R 10 , where we choose two orthogonal planes spanned by (e 1 , e 2 ) and (e 3 , e 4 ), respectively.With probability one half a points is sampled uniformly on the unit circle in the e 1 -e 2 plane, otherwise the point will be sampled uniformly on the unit circle in the e 3 -e 4 plane.We train SAEs with m = 64 on this data with the same hyperparameters as the single-circle case.</p>
<p>We now apply the procedure described in Section 4 to see if we can automatically rediscover these circles.Encouragingly, we first find that the alive SAE features align almost exactly with either the e 1 -e 2 or the e 3 -e 4 plane.When we apply spectral clustering with n_clusters = 2 to the features with the pairwise angular similarities between dictionary elements as the similarity matrix (Fig. 13, left), the two clusters correspond exactly to the features which span each plane.As described in Section 4, given a cluster of dictionary elements S ⊂ {1, . . ., m}, we run a large set of activations through the SAE, then filter out samples which don't activate any element in S. For samples which do activate an element of S, reconstruct the activation while setting all SAE features not in S to have a hidden activation of zero.If some collection of SAE features together represent some irreducible feature, we want to remove all other features from the activation vector, and so we only allow SAE features in the collection to participate in reconstructing the input activation.We find that this procedure almost exactly recovers the original two circles, which encouraged us to apply this method for discovering the features shown in Fig. 1 and Fig. 15.</p>
<p>E TRAINING MISTRAL SAES</p>
<p>Our Mistral 7B (Jiang et al., 2023) sparse autoencoders (SAEs) are trained on over one billion tokens from a subset of the Pile (Gao et al., 2020) and Alpaca (Peng et al., 2023) datasets.We train our SAEs on layers 8, 16, and 24 out of 32 total layers to maximize coverage of the model's representations.We use a 16× expansion factor, yielding a total of 65536 dictionary elements for each SAE.</p>
<p>To train our SAEs, we use an L p sparsity penalty for p = 1/2 with sparsity coefficient λ = 0.012.Before an SAE forward pass, we normalize our activation vectors to have norm √ d model = 64 in the case of Mistral.We do not apply a pre-encoder bias.We use an AdamW optimizer with weight decay 10 −3 and learning rate 0.0002 with a linear warm up.We apply dead feature resampling (Bricken et al., 2023) five times over the course of training to converge on SAEs with around 1000 dead features.</p>
<p>F GPT-2 AND MISTRAL 7B DICTIONARY ELEMENT CLUSTERING</p>
<p>In this section, we first present pseudocode in Alg. 1 for the overall high level technique that finds multi-dimensional features and that uses clustering as a subroutine.We then provide the specific clustering algorithm implementations we use for GPT-2 and Mistral.</p>
<p>Algorithm 1: High Level Clustering Approach For Finding Multi-D Features Input: Dictionary elements D, activation vectors X i,l , SAE Output: Irreducible multi-dimensional features S i,j ← CosineSim(D i , D j ); clusters ← Cluster(S); reconstructions ← {}; for cluster in clusters do R cluster ← ids of dictionary elements in cluster; For GPT-2-small, we perform spectral clustering on the roughly 25k layer 7 SAE features from (Bloom, 2024), using pairwise angular similarities between dictionary elements as the similarity matrix.We use n_clusters = 1000 and manually looked at roughly 500 of these clusters.For each cluster, we looked at projections onto principal components 1-4 of the reconstructed activations for these clusters.In Fig. 14, we show projections for the most interesting clusters we identified, which appear to be circular representations of days of the week, months of the year, and years of the 20th century.
for x i,l in X i,l do encoding ← ReLU(E • x i,l ); if max(encoding[R cluster ]) &gt; 0 then r ← D[:, R cluster</p>
<p>F.2 MISTRAL 7B METHODS AND RESULTS</p>
<p>For Mistral 7B, our SAEs have 65536 dictionary elements and we found it difficult to run spectral clustering on all of these at once.We therefore develop a simple graph based clustering algorithm that we run on Mistral 7B SAEs:</p>
<ol>
<li>
<p>Create a graph G out of the dictionary elements by adding directed edges from each dictionary element to its k closest dictionary elements by cosine similarity.We use k = 2.</p>
</li>
<li>
<p>Make the graph undirected by turning every directed edge into an undirected edge.</p>
</li>
<li>
<p>Prune edges with cosine similarity less than a threshold value τ .We use τ = 0.5.</p>
</li>
</ol>
<p>Return the connected components as clusters.</p>
<p>We run this algorithm on the Mistral 7B layer 8 SAE (2 16 dictionary elements) and find roughly 2700 clusters containing between 2 and 1000 elements.We manually inspected roughly 2000 of these.From these, we re-discover circular representations of days of the week and months of the year, shown in Fig. 15.However, we did not find other obviously interesting and clearly irreducible features.</p>
<p>We also investigate the sensitivity of this method to τ and k by varying τ and k and showing the max Jaccard similarity between any of the resulting clusters and the days of the week cluster we show in Fig. 15.We show the results in Fig. 16, where we find that varying k has minimal effect, while varying τ shows 3 regimes: small τ causes all features to group in one cluster, so the days of the week cluster is not found; medium τ causes the days of the week cluster to become identifiable; large τ causes all features to be divided into their own clusters.</p>
<p>Figure 15: Circular representations of days of the week and months of the year which we discover with our unsupervised SAE clustering method in Mistral 7B.Unlike similar features in GPT-2, we also find an additional "weekend" representation in between Saturday and Sunday representations (left) and additional representations of seasons among the months (right).For instance, "winter" tokens activate a region of the circle in between the representation of January and December.</p>
<p>k=2 k=3 k=4</p>
<p>Top-k for Graph (k=2,3,4)  Mean Cluster Size (log scale)</p>
<p>Figure 16: Hyperparameter regimes where the days of the week cluster exists.The cluster exists in the regime between all features clumping together and all features being in their own cluster; this regime seems reasonably stable.</p>
<p>As future work, we think it would be exciting to develop better clustering techniques for SAE features.Our graph based clustering technique could likely be improved by more recent efficient and high-quality graph based clustering techniques, e.g.hierarchical agglomerate clustering with single-linkage (Lattanzi et al., 2020).Additionally, we believe we would see a large improvement by setting edge weights to be a combination of both the cosine and Jaccard similarity of the dictionary elements, e.g.max(cosine, Jaccard).</p>
<p>G OTHER DISCOVERED CLUSTERS</p>
<p>In Fig. 17, we plot the top 11 ranked clusters by the product of a) the measured separability index and b) one minus the measured ϵ-mixture index with ϵ = 0.1 (this is just one of many possible ways to get an ordered ranking from a two-parameter score).We color by both the current token (which results in clear patterns for all tokens) and the next token (to see if we find belief states as found by Shai et al. (2024) in toy transformers).We note that weekdays are ranked 9 and so are shown in the plot.Additionally, the next token patterns of the 'such' cluster and the 'B' cluster do seem to display some clustering independently of the the current token pattern, which might lend the belief state hypothesis some support.</p>
<p>H FURTHER EXPERIMENT DETAILS</p>
<p>H.3 ERROR BAR CALCULATION</p>
<p>In Fig. 6 we report 96% error bars for all intervention methods.To compute these error bars, we loop over all intervention methods and all layers and compute a confidence interval for each (method, layer) pair across all prompts.Assuming normally distributed errors, we compute error bars with the following standard formula:
EB = µ ± z * SE
where µ is the sample mean, z is the z score (slightly larger than 2 for 96% error bars), and SE is the standard error (the standard deviation divided by the square root of the number of samples).We use standard Python functions to compute this value.We show the results of Mistral 7B and Llama 3 8B on all individual instances of Weekdays that at least one of the models get wrong in Table 2 and present a similar table for Months in Table 3.</p>
<p>We show projections onto the top two PCA directions for both Mistral 7B and Llama 3 8B in Fig. 18 on the hidden layers on top of the α token, colored by α.These are similar plots to Fig. 4, except they are on all layers.The circular structure in α is visible on many-but not all-layers.Much of the linear structure visible is due to β.</p>
<p>I.0.2 INTERVENING WITH THE SAE PROBE</p>
<p>We show the results of projecting Mistral Weekdays representations into the plane discovered by clustering SAE features in Fig. 19.The result is clearly circular.We first perform a patching experiment with the same setup Fig. 21 and Fig. 20 on individual attention heads on the final token.From the patching results we identify the top 10 attention heads by average intervention effect.</p>
<p>For each attention head, we compute one EVR run with explanatory functions equal to one-hot functions of α and β (resulting in 14 functions g i for Weekdays and 24 for Months) and one with explanatory functions equal to onehot functions of α, β, and γ.We find that for all layers before 25, adding γ to the explanatory functions adds almost no explanatory power.Since we established above that the model has already computed γ at this point, we know that attention heads do not participate in computing γ.</p>
<p>To isolate the rough circuit for Weekdays and Months, we perform layer-wise activation patching on 40 random pairs of prompts.The results, displayed in Fig. 21 show that the circuit to compute γ consists of MLPs on top of the α and β tokens, a copy to the token before γ, and further MLPs there (roughly similar to prior work studying arithmetic circuits (Stolfo et al., 2023)).Moreover, fine-grained patching in Appendix K shows that there are just a few responsible attention heads for the writes to the token before γ.However, patching alone cannot tell use how or where γ is represented.For that, we need a new technique, which we expand on in the next section.</p>
<p>K EXPLANATION VIA REGRESSION (EVR)</p>
<p>So far, we have focused on examining and intervening on the representation for α, which we present as a circle in the top PCA components on top of the α token.In this section, we examine how the generated output, γ, is represented.</p>
<p>First, to isolate the rough circuit for Weekdays and Months, we perform layer-wise activation patching on 40 random pairs of prompts.The results, displayed in Fig. 21 and Fig. 20, show that the circuit to compute γ consists of MLPs on top of the α and β tokens, a copy to the token before This information tells us what can and cannot be extracted via a linear probe, without having to train any probes.Furthermore, if we treat each g i as a feature (see Definition 1), then the linear regression coefficients tell us which directions in X i,l these features are represented in, connecting back to Hypothesis 2.</p>
<p>Since X i,l consists of modular addition problems with two inputs α and β, we can visualize the errors as we iteratively construct g 1 , . . ., g k by making a heatmap with α and β on the two axes, where the color shows what kind of error is made.More specifically, we take the top 3 PCA components of the error distribution and assign them to the colors red, green, and blue.We call the resulting heatmap a residual RGB plot.Errors that depend primarily on α, β, or γ show up as horizontal, vertical, or diagonal stripes on the residual RGB plot.</p>
<p>In Fig. 24, we perform EVR on the layer 17-29 hidden states of Mistral 7B on the Weekdays task; additional deconstructions are in Appendix K. We find that a circle in γ develops and grows in explanatory power; we plot the layer 25 residuals after explaining with one hot functions in α and β (i.e.g 1 = [α = 0], g 2 = [β = 1], g 3 = [α = 1], . ..) in Fig. 23 to show this incredibly clear circle in γ.This suggests that the models may be generating γ by using a trigonometry based algorithm like the "clock" (Nanda et al., 2023a) or "pizza" (Zhong et al., 2024) algorithm in late MLP layers.</p>
<p>Figure 3 :
3
Figure 3: Mixture index and separability index of GPT-2 features.Features from Fig. 1, which we had manually identified, score highly as candidate multidimensional features with these metrics.</p>
<p>Figure 4 :
4
Figure 4: Top two PCA components on the α token.Colors show α.Left: Layer 30 of Mistral on Weekdays.Right: Layer 5 of Llama on Months.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Visual representation of the circular intervention process.Top: We learn a circular probe on the PCA projection of a training set.Bot: To intervene, we change the circular representation to α ′ j and average ablate other dimensions.</p>
<p>Figure 7 :
7
Figure 7: Off distribution interventions on Mistral layer 5 on the Weekdays task.The color corresponds to the highest logit γ after performing the circular subspace intervention on that point.</p>
<p>Fig. 7 displays these results on Mistral layer 5 for β ∈ [2, 3, 45].They imply that Mistral treats the circle as a multi-dimensional representation with α encoded in the angle.</p>
<p>after intervention Intervene w/ Layer 8 Probe Intervene w/ SAE Probe Intervene w/ Per-Layer Probe</p>
<p>Figure 8 :
8
Figure 8: Interventions on the Mistral 7B Weekdays task with different methods of determining the probe.</p>
<p>Figure 9 :
9
Figure 9: Layer 30 Mistral 7B activations for [morning/evening] on [Monday/Tuesday/.../Sunday], plotted projected into the PCA plane for [Monday/Tuesday/.../Sunday].</p>
<p>Figure 10: Testing irreducibility of synthetic features.Left in each subfigure: Distributions of x.For feature a, 63.96% lies within the narrow dotted lines, indicating the feature is likely a mixture.For feature b, 17.84% lies within the wide lines, indicating the feature is unlikely to be a mixture.The green cross indicates the angle θ that minimizes mutual information.Middle in each subfigure: Histograms of the distribution of v • x with red lines indicating a 2ϵ-wide region.Right in each subfigure: Mutual information between a and b as a function of the rotation angle θ of matrix R. Feature b has a large minimum mutual information so is unlikely to be separable; feature a has a medium value of minimum mutual information of about 0.37 bits.</p>
<p>Figure 11: Testing irreducibility of synthetic features.Left in each subfigure: Distributions of x.For feature c, 7.94% lies within the narrow dotted lines, indicating the feature is unlikely to be a mixture.For feature d, 25.90% lies within the wide lines, indicating the feature is likely a mixture.The green cross indicates the angle θ that minimizes mutual information.Middle in each subfigure: Histograms of the distribution of v • x with red lines indicating a 2ϵ-wide region.Right in each subfigure: Mutual information between a and b as a function of the rotation angle θ of matrix R. Both features have a small (&lt; 0.5 bits) minimum mutual information and so are likely separable.</p>
<p>Figure 12 :
12
Figure12: SAEs trained to reconstruct a single 2d circle with m = 2 (left) and m = 10 (middle and right) dictionary elements.When there are several SAE features, there is no natural/canonical choice of feature directions, and the dictionary elements spread out across the circle.</p>
<p>Figure 13 :
13
Figure 13: Automatic discovery of synthetic circular features by clustering SAE dictionary elements.</p>
<p>Figure 14 :
14
Figure 14: Projections of days of week, months of year, and years of the 20th century representations onto top four principal components, showing additional dimensions of the representations than Fig. 1.</p>
<p>Figure 17 :
17
Figure 17: Top 10 GPT-2 clusters by Mixture and Separability Index.</p>
<p>Figure 18 :Figure 19 :
1819
Figure 18: Projections onto the top two PCA dimensions of model hidden states on the α token show that circular representations of α are present in various layers.</p>
<p>Figure 22 :
22
Figure 22: Layer 30 Mistral 7B activations for [very early/very late] on [Monday/Tuesday/.../Sunday], plotted projected into the PCA plane for [Monday/Tuesday/.../Sunday].</p>
<p>Table 1 :
1
Aggregate model accuracy on days of the week and months of the year modular arithmetic tasks.Performance broken down by problem instance in Appendix I.
ModelWeekdays MonthsMon Tue Wed Thu Fri Sat SunJan Feb Mar May Jun Jul Aug Sep Oct Nov Apr DecLlama 3 8B29 / 49143 / 144Mistral 7B31 / 49125 / 144GPT-28 / 4910 / 144</p>
<p>Table 4 :
4
Highest intervention effect attention heads from fine-grained attention head patching, as well as EVR results with one hot α, β and one hot α, β, γ.
(a) Mistral 7B, Weekdays.(b) Llama 3 8B, Weekdays.L H AverageEVR R 2EVR R 2L H AverageEVR R 2EVR R 2Inter-One HotOne HotInter-One HotOne Hotventionα, βα, β, γventionα, βα, β, γEffectEffect28 180.220.390.731700.180.980.9918 300.170.950.961710.080.980.9815 130.170.940.9519 100.080.950.9622 150.110.770.8230 170.070.850.9016 210.090.920.931730.070.930.9528 160.080.420.6917 270.061.001.0015 140.060.980.9931 220.050.370.7830 240.050.430.792190.040.730.7821 260.040.530.6320 280.041.001.001420.040.930.9530 160.040.730.85(c) Mistral 7B, Months.(d) Llama 3 8B, Months.L H AverageEVR R 2EVR R 2L H AverageEVR R 2EVR R 2Inter-One HotOne HotInter-One HotOne Hotventionα, βα, β, γventionα, βα, β, γEffectEffect20 280.150.760.7615 130.260.620.621700.100.770.7716 210.170.760.7625 140.080.190.6118 300.130.770.771710.070.800.8228 180.110.130.521730.060.710.7128 160.070.130.5231 220.060.120.6721 250.050.650.7017 270.050.580.5815 140.030.720.721940.050.400.6617 260.020.770.7719 100.040.620.623110.020.110.5730 260.040.510.6221 240.020.300.45
An earlier version of this manuscript sparked discussion in the mechanistic interpretability community on the distinction between non-linear features and multi-dimensional features, and in fact this discussion directly led
. In Section 4, we build on the definitions proposed in Section
to develop a theoretically grounded and empirically practical test that uses sparse autoencoders to find irreducible features. Using this test, we identify multi-dimensional representations automatically in GPT-2 and Mistral 7B, including circular representations for the day of the week and month of the year.3. In Section 5, we show that Mistral 7B and Llama 3 8B use these circular representations when performing modular addition in days of the week and in months of the year. To the best of our knowledge, we are the first to find causal circular representations of concepts in a language model. We additionally find that the model's circular representations respect a continuous notion of time.
since any multidimensional Gaussian can be rotated to have a diagonal covariance matrix
+ λ∥ReLU(E • x i,l )∥ 0 (4)
Code: https://github.com/JoshEngels/MultiDimensionalFeatures
We also tried an alternative ranking scheme: we sorted the clusters by separability and irreducibility and set the cluster score equal to the minimum sorted position between the two sorted lists. The Fig.1clusters rank 8, 105, and 12 by this metric.
ACKNOWLEDGMENTSWe thank (in alphabetical order) Dowon Baek, Kaivu Hariharan, Vedang Lad, Ziming Liu, and Tony Wang for helpful discussions and suggestions.This work is supported by Erik Otto, Jaan Tallinn, the Rothberg Family Fund for Cognitive Science, the NSF Graduate Research Fellowship (Grant No. 2141064), and IAIFI through NSF grant PHY-2019786.of problems, we patch the MLP/attention outputs from the "clean" to the "dirty" problem for each layer and token, and then complete the forward pass.Defining the logit difference as the logit of the clean γ minus the logit of the dirty γ, we record what percent of the difference between the original logit difference of the dirty problem and the logit difference of the clean problem is recovered upon intervening, and average across these 40 percentages for each layer and token.This gives us a score we call the Average Intervention Effect.For simplicity of presentation, we clip all of the (few) negative intervention averages to 0 (prior work(Zhang &amp; Nanda, 2023) has also found negative-effect attention heads during patching experiments).I.0.4 CIRCLE CONTINUITYFinally, in Fig.22, we show another example of the continuity of the circular days of the week representation in Mistral 7B.J PATCHINGIn this section, we present results to support a claim that MLPs (and not attention blocks) are responsible for computing γ.In Fig.25, we deconstruct states on top of the final token (before predicting γ) on Llama 3 8B Months (we show a similar plot for the states on the final token of Mistral 7B on Weekdays in the main text in Fig.24.These plots show that the value of γ is computed on the final token around layers 20 to 25.To show that this computation of occurs in the MLPs, we must show that no attention head is copying γ from a prior token or directly computing γ.Published as a conference paper at ICLR 2025  Figure24: EVR residual RGB plots on Mistral hidden states on the Weekdays final token, layers 17 to 29.From top to bottom, we show each residual RGB plot after adding the function(s) g i labelled just underneath, as well as the resulting r 2 value.We write "tmr" meaning "tomorrow" for β = 1.We also write "circle for x" meaning the inclusion of two functions g i (x) = {cos, sin}(2πx/7).γ, and further MLPs there (roughly similar to whatStolfo et al. (2023)find in prior work studying arithmetic circuits).Thus, we know where to look for a representation of γ: in the second half of the layers on the token before γ.However, patching alone cannot tell use how γ is represented.Unlike α, γ has no obvious circular (or linear) pattern in the top PCA components on these layers.To determine the representation for γ, we introduce a more powerful technique we call Explanation via Regression (EVR): given a set of token sequences with a corresponding set of hidden states X i,l , we choose a set of interpretable explanation functions of the input tokens {g j (t)}.The r 2 value of a linear regression from {g j (t)} to X i,l tells us how much of the variance in the activations the {g j (t)} explain, and conversely the residuals show the exact components of the representation we have yet to explain.K.1 USING EVR TO UNCOVER A CIRCULAR REPRESENTATION FOR γWe first use EVR to determine the representation for γ by plotting the top two PCA components of the layer 25 Mistral 7B activations after subtracting the components that can be explained using a regression with one hot functions in α and β (i.e.The result, shown in Fig.23, is an incredibly clear circle in γ, which suggests that the model's generated representation of γ lies along a circle.A simple PCA projection was not enough to find this result because the representation for γ has interference from α and β, which the EVR removes.This suggests that the models may be generating γ by using a trigonometry based algorithm like the "clock"Nanda et al. (2023a)or "pizza"Zhong et al. (2024)algorithm in late MLP layers.K.2 MORE EXPERIMENTS WITH EVRWe now apply EVR to Months and Weekdays to break down X i,l completely into interpretable functions.We build a list of g i iteratively and greedily.At each iteration, we perform a linear regression with the current list g 1 . . .g k , visualize and interpret the residual prediction errors, and build a new function g k+1 representing these errors to add to the list.Once most variance is explained, we can conclude that g 1 , . . ., g k constitutes the entirety of what is represented in the hidden states.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>A I , Meta , Llama 3 model card. 2024</p>
<p>Noga Alon. Problems and results in extremal combinatorics-i. Discrete Mathematics. 2003273</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Anthropic, Zach Belrose, Logan Furman, Danny Smith, Igor Halawi, Lev Ostrovsky, Stella Mckinney, Jacob Biderman, Steinhardt, arXiv:2303.081122024. 2023AnthropicarXiv preprintThe claude 3 model family: Opus, sonnet, haiku</p>
<p>Sid Black, Lee Sharkey, Leo Grinsztajn, Eric Winsor, Dan Braun, Jacob Merizian, Kip Parker, Carlos Ramón Guevara, Beren Millidge, Gabriel Alfour, arXiv:2211.12312Interpreting neural networks through the polytope lens. 2022arXiv preprint</p>
<p>Open source sparse autoencoders for all residual stream layers of gpt2 small. Joseph Bloom, 2024</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden Mclean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Christopher Olah, 2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Towards automated circuit discovery for mechanistic interpretability. Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso, Advances in Neural Information Processing Systems. 202336</p>
<p>Recurrent neural networks learn to store and generate sequences using non-linear representations. Róbert Csordás, Christopher Potts, Christopher D Manning, Atticus Geiger, arXiv:2408.109202024arXiv preprint</p>
<p>Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey, arXiv:2309.08600Sparse autoencoders find highly interpretable features in language models. 2023arXiv preprint</p>
<p>Towards guaranteed safe ai: A framework for ensuring robust and reliable ai systems. David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, arXiv:2405.066242024arXiv preprint</p>
<p>Toy models of superposition. Transformer Circuits Thread. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam Mccandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, Christopher Olah, 2022</p>
<p>A Mathematical Introduction to Compressive Sensing. Simon Foucart, Holger Rauhut, 2013</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.000272020arXiv preprint</p>
<p>über die abgrenzung der eigenwerte einer matrix. Izvestiya Rossiȋskoi akademii nauk. Semyon Aranovich, Gershgorin , Seriya matematicheskaya. 61931</p>
<p>Successor heads: Recurring, interpretable attention heads in the wild. Rhys Gould, Euan Ong, George Ogden, Arthur Conmy, arXiv:2312.092302023arXiv preprint</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark, arXiv:2310.022072023arXiv preprint</p>
<p>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, Advances in Neural Information Processing Systems. 202436</p>
<p>Monotonic representation of numeric properties in language models. Benjamin Heinzerling, Kentaro Inui, arXiv:2403.103812024arXiv preprint</p>
<p>J Nicholas, Higham, Singular value inequalities. May 2021</p>
<p>Almost orthogonal vectors. Bill Johnson, version: 2010-05-16</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, Victor Veitch, arXiv:2403.03867On the origins of linear representations in large language models. 2024arXiv preprint</p>
<p>Extensions of lipschitz mappings into a hilbert space. William B Johnson, Joram Lindenstrauss, 10.1090/conm/026/737400Conference in modern analysis and probability. New Haven, Conn; Providence, RIAmerican Mathematical Society1982. 198426</p>
<p>Language models use trigonometry to do addition. Subhash Kantamneni, Max Tegmark, arXiv:2502.008732025arXiv preprint</p>
<p>A framework for parallelizing hierarchical clustering methods. Silvio Lattanzi, Thomas Lavastida, Kefu Lu, Benjamin Moseley, Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019. Würzburg, GermanySpringerSeptember 16-20, 2019. 2020Proceedings, Part I</p>
<p>Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, arXiv:2210.13382Emergent world representations: Exploring a sequence model trained on a synthetic task. 2022arXiv preprint</p>
<p>Towards understanding grokking: An effective theory of representation learning. Ziming Liu, Ouail Kitouni, Eric Niklas S Nolte, Max Michaud, Mike Tegmark, Williams, Advances in Neural Information Processing Systems. 202235</p>
<p>Samuel Marks, Max Tegmark, arXiv:2310.06824The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. 2023arXiv preprint</p>
<p>Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, Aaron Mueller, arXiv:2403.19647Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. 2024arXiv preprint</p>
<p>Sae feature geometry is outside the superposition hypothesis. Jake Mendel, AI Alignment Forum. 2024</p>
<p>Opening the ai black box: program synthesis via mechanistic interpretability. Eric J Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide, Chloe Loughridge, Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukelić, Max Tegmark, arXiv:2402.051102024arXiv preprint</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. 262013a</p>
<p>Linguistic regularities in continuous space word representations. Tomáš Mikolov, Wen-Tau Yih, Geoffrey Zweig, Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies. the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies2013b</p>
<p>Feature emergence via margin maximization: case studies in algebraic tasks. Depen Morwani, Costin-Andrei Benjamin L Edelman, Rosie Oncescu, Sham Zhao, Kakade, arXiv:2311.075682023arXiv preprint</p>
<p>The quest for the right mediator: A history, survey, and theoretical grounding of causal interpretability. Aaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can Rager, Aruna Sankaranarayanan, Sen Arnab, Jiuding Sharma, Sun, arXiv:2408.014162024arXiv preprint</p>
<p>. Neel Nanda, Joseph Bloom, Transformerlens, </p>
<p>. Transformerlensorg/Transformerlens, 2022</p>
<p>Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, arXiv:2301.05217Progress measures for grokking via mechanistic interpretability. 2023aarXiv preprint</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, arXiv:2309.009412023barXiv preprint</p>
<p>What is a linear representation? what is a multidimensional feature? Transformer Circuits Thread. Chris Olah, 2024</p>
<p>Zoom in: An introduction to circuits. Distill. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter, 10.23915/distill.00024.0012020</p>
<p>The linear representation hypothesis and the geometry of large language models. Kiho Park, Yo Joong Choe, Victor Veitch, arXiv:2311.036582023arXiv preprint</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing. the 2014 conference on empirical methods in natural language processing2014</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 2019</p>
<p>Adam Shai, Paul Riechers, Lucas Teixeira, Alexander Oldenziel, Sarah Marzen, Transformers represent belief state geometry in their residual stream. 2024</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Attribution patching outperforms automated circuit discovery. Aaquib Syed, Can Rager, Arthur Conmy, arXiv:2310.103482023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Provably safe systems: the only path to controllable agi. Max Tegmark, Steve Omohundro, arXiv:2309.019332023arXiv preprint</p>
<p>Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, arXiv:2211.005932022arXiv preprint</p>
<p>Gpt-2's positional embedding matrix is a helix. Adam Yedidia, 2023agpt-2-s-positional-embedding-matrix-is-a-helix</p>
<p>The positional embedding matrix and previous-token heads: how do they actually work?. Adam Yedidia, 2023b</p>
<p>Fred Zhang, Neel Nanda, arXiv:2309.16042Towards best practices of activation patching in language models: Metrics and methods. 2023arXiv preprint</p>
<p>The clock and the pizza: Two stories in mechanistic explanation of neural networks. Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
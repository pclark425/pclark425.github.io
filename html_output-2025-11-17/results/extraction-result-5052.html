<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5052 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5052</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5052</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-a5378175d31d3dd8fa004037df663aa00f236a0b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a5378175d31d3dd8fa004037df663aa00f236a0b" target="_blank">Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that two-layer transformers learn reliable solutions to multi-level problems, develop signs of task decomposition, and encode input items in a way that encourages the exploitation of shared computation across related tasks.</p>
                <p><strong>Paper Abstract:</strong> Transformer networks have seen great success in natural language processing and machine vision, where task objectives such as next word prediction and image classification benefit from nuanced context sensitivity across high-dimensional inputs. However, there is an ongoing debate about how and when transformers can acquire highly structured behavior and achieve systematic generalization. Here, we explore how well a causal transformer can perform a set of algorithmic tasks, including copying, sorting, and hierarchical compositions of these operations. We demonstrate strong generalization to sequences longer than those used in training by replacing the standard positional encoding typically used in transformers with labels arbitrarily paired with items in the sequence. We search for the layer and head configuration sufficient to solve these tasks, then probe for signs of systematic processing in latent representations and attention patterns. We show that two-layer transformers learn reliable solutions to multi-level problems, develop signs of task decomposition, and encode input items in a way that encourages the exploitation of shared computation across related tasks. These results provide key insights into how attention layers support structured computation both within a task and across multiple tasks.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5052.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5052.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-encoded causal transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-layer causal transformer with label-based order encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small, task-conditioned causal transformer (1–2 layers, variable attention heads) trained on synthetic sequence rearrangement tasks (COPY, REVERSE, GROUP, SORT) using a label-based random integer order encoding to enable length extrapolation and to induce multi-stage attention computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom causal transformer (two-layer variants; multi-headed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal (autoregressive) transformer variants studied with 1 or 2 alternating attention and MLP sublayers, residuals and layernorm; embedding dims ~128–192 depending on setting; MLP hidden size 64; architectures varied number of attention heads per layer while controlling total parameters (examples: [1,1], [1,4], [1,6], [4,4], etc.); trained with full teacher forcing using Adam (LR 1e-4 single-task, 5e-4 six-task) on a synthetic dataset of 100k sequences of items (each item encoded as concatenated one-hot features for shape, color, texture) over training lengths 5–25 and evaluated for length generalization on lengths 26–50.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Structured sequence rearrangement tasks: COPY, REVERSE, GROUP[shape/color], SORT[shape,color,texture]</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Synthetic puzzles consist of sequences of items drawn from a pool defined by three discrete features (shape, color, texture). Tasks require (a) copying the sequence, (b) reversing it, (c) grouping items by a specified feature while preserving input order within groups, or (d) hierarchical sorting by multiple features (e.g., shape then color then texture). These are hierarchical/order-based spatial reasoning tasks over sequence positions (ordering, grouping, and hierarchical spatial arrangement) rather than 2D-grid puzzles like Sudoku.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Key mechanisms: (1) label-based order encoding — items paired with random integer labels (embedded) instead of fixed positional encodings to expose model to a wide label range and support length extrapolation; (2) architectural induction — two-layer models with a distinct pattern of attention across layers (first layer cross-contextualizes items within first-level groups, second layer attends to next output token to prepare readout); (3) multi-task conditioning via an explicit task token that the model outputs/predicts and uses to contextualize behavior; (4) multi-headed attention with different head allocations (frontload/backload) to enable reusable computations across tasks; (5) training with teacher forcing and evaluation under top-1 rollout (greedy decoding). Attention ablations and preserving attention to certain tokens were used as mechanistic probes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Multiple analyses indicate the model uses structured, spatial/relational processing: (a) attention maps: first-layer heads attend preferentially to items that share the same first-level feature (e.g., same shape group) showing intra-group cross-contextualization; second-layer heads focus on the next output token, indicating a readout stage; (b) task-conditioned item representations: PCA of item embeddings shows clustering by the relevant first-level grouping feature and ring-like structures for secondary sort features, implying learned representations that encode hierarchical order; (c) input embedding analyses: learned feature embedding weights reflect within-feature order similarities (dot-product similarity and Gaussian-similarity fits); (d) ablation studies: masking attention to non-task tokens but preserving task token and next-output-token attention preserves GROUP and SORT performance, implying task-conditioned, structured readout; (e) comparisons across positional encodings: label-based encoding produced substantially better length generalization than sinusoidal or learnable positional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generalization evaluated on sequences length 26–50. Representative numbers (token-level accuracies, mean ± std across seeds): Single-task SORT[shape,color,texture] with two-layer [1,1] and label encoding: item prediction 99.60% ± 0.07, label prediction 98.41% ± 0.23 (Table S2). In six-task (multi-task) attention-backload models, top-performing architectures (e.g., [1,4]) achieved across-all-tasks token-level item accuracy ~99.40% ± 0.29 and label accuracy ~99.21% ± 0.43 (Table S3(c)); attention-balanced [6,6] also achieved item ~99.11% ± 0.32 and label ~99.17% ± 0.32. Sequence-level accuracy degrades with increasing length but most long extrapolation sequences have <5% token errors; allowing up to 5% prediction errors yields very high sequence-level success rates (Fig 2D, Fig 6C). EOS prediction near-perfect under teacher forcing, but can deteriorate under top1 rollout in multi-task models (Fig S6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) Single-layer models (even with multiple heads) often failed to reliably sort items in middle positions and lacked consistent inter-group structure; (2) positional encodings other than label-based (sinusoidal, learnable) performed markedly worse for length generalization and training; (3) sequence-level accuracy degrades with increased length (though not catastrophically); (4) top-1 rollout (greedy decoding) performance can be worse than teacher-forced evaluation, with EOS predictions especially susceptible in multi-task setups; (5) some attention heads show redundancy and ablating certain heads had small effects under teacher forcing but larger effects under rollout; (6) texture feature was less strongly represented (third-level sort) likely due to task data distribution; (7) these tasks are synthetic sequence-order problems (not true spatial-grid puzzles like Sudoku), and label-based encoding does not represent true metric item distances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons include different positional encodings (label-based vs sinusoidal vs learnable): label-based encoding substantially outperformed sinusoidal and learnable encodings for length generalization and training speed. Architectural comparisons: two-layer models outperformed single-layer models for multi-level tasks; attention-backload (more heads in second layer) outperformed attention-frontload in multi-task accuracy; multi-headed models outperformed single-headed for rollout performance. The paper references other algorithmic/graph baselines (e.g., graph neural networks in CLRS benchmarks) as generally struggling on long extrapolation, but no human baseline is provided; comparisons vs large pretrained LLMs are discussed only in related work (length-generalization challenges) and not empirically evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring length generalization in large language models <em>(Rating: 2)</em></li>
                <li>Randomized positional encodings boost length generalization of transformers <em>(Rating: 2)</em></li>
                <li>The CLRS algorithmic reasoning benchmark <em>(Rating: 2)</em></li>
                <li>Neural algorithmic reasoning <em>(Rating: 2)</em></li>
                <li>Grokking: Generalization beyond overfitting on small algorithmic datasets <em>(Rating: 1)</em></li>
                <li>Neural turing machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5052",
    "paper_id": "paper-a5378175d31d3dd8fa004037df663aa00f236a0b",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "Label-encoded causal transformer",
            "name_full": "Two-layer causal transformer with label-based order encoding",
            "brief_description": "A small, task-conditioned causal transformer (1–2 layers, variable attention heads) trained on synthetic sequence rearrangement tasks (COPY, REVERSE, GROUP, SORT) using a label-based random integer order encoding to enable length extrapolation and to induce multi-stage attention computations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom causal transformer (two-layer variants; multi-headed)",
            "model_description": "Causal (autoregressive) transformer variants studied with 1 or 2 alternating attention and MLP sublayers, residuals and layernorm; embedding dims ~128–192 depending on setting; MLP hidden size 64; architectures varied number of attention heads per layer while controlling total parameters (examples: [1,1], [1,4], [1,6], [4,4], etc.); trained with full teacher forcing using Adam (LR 1e-4 single-task, 5e-4 six-task) on a synthetic dataset of 100k sequences of items (each item encoded as concatenated one-hot features for shape, color, texture) over training lengths 5–25 and evaluated for length generalization on lengths 26–50.",
            "puzzle_name": "Structured sequence rearrangement tasks: COPY, REVERSE, GROUP[shape/color], SORT[shape,color,texture]",
            "puzzle_description": "Synthetic puzzles consist of sequences of items drawn from a pool defined by three discrete features (shape, color, texture). Tasks require (a) copying the sequence, (b) reversing it, (c) grouping items by a specified feature while preserving input order within groups, or (d) hierarchical sorting by multiple features (e.g., shape then color then texture). These are hierarchical/order-based spatial reasoning tasks over sequence positions (ordering, grouping, and hierarchical spatial arrangement) rather than 2D-grid puzzles like Sudoku.",
            "mechanism_or_strategy": "Key mechanisms: (1) label-based order encoding — items paired with random integer labels (embedded) instead of fixed positional encodings to expose model to a wide label range and support length extrapolation; (2) architectural induction — two-layer models with a distinct pattern of attention across layers (first layer cross-contextualizes items within first-level groups, second layer attends to next output token to prepare readout); (3) multi-task conditioning via an explicit task token that the model outputs/predicts and uses to contextualize behavior; (4) multi-headed attention with different head allocations (frontload/backload) to enable reusable computations across tasks; (5) training with teacher forcing and evaluation under top-1 rollout (greedy decoding). Attention ablations and preserving attention to certain tokens were used as mechanistic probes.",
            "evidence_of_spatial_reasoning": "Multiple analyses indicate the model uses structured, spatial/relational processing: (a) attention maps: first-layer heads attend preferentially to items that share the same first-level feature (e.g., same shape group) showing intra-group cross-contextualization; second-layer heads focus on the next output token, indicating a readout stage; (b) task-conditioned item representations: PCA of item embeddings shows clustering by the relevant first-level grouping feature and ring-like structures for secondary sort features, implying learned representations that encode hierarchical order; (c) input embedding analyses: learned feature embedding weights reflect within-feature order similarities (dot-product similarity and Gaussian-similarity fits); (d) ablation studies: masking attention to non-task tokens but preserving task token and next-output-token attention preserves GROUP and SORT performance, implying task-conditioned, structured readout; (e) comparisons across positional encodings: label-based encoding produced substantially better length generalization than sinusoidal or learnable positional encodings.",
            "performance_metrics": "Generalization evaluated on sequences length 26–50. Representative numbers (token-level accuracies, mean ± std across seeds): Single-task SORT[shape,color,texture] with two-layer [1,1] and label encoding: item prediction 99.60% ± 0.07, label prediction 98.41% ± 0.23 (Table S2). In six-task (multi-task) attention-backload models, top-performing architectures (e.g., [1,4]) achieved across-all-tasks token-level item accuracy ~99.40% ± 0.29 and label accuracy ~99.21% ± 0.43 (Table S3(c)); attention-balanced [6,6] also achieved item ~99.11% ± 0.32 and label ~99.17% ± 0.32. Sequence-level accuracy degrades with increasing length but most long extrapolation sequences have &lt;5% token errors; allowing up to 5% prediction errors yields very high sequence-level success rates (Fig 2D, Fig 6C). EOS prediction near-perfect under teacher forcing, but can deteriorate under top1 rollout in multi-task models (Fig S6).",
            "limitations_or_failure_cases": "Reported limitations include: (1) Single-layer models (even with multiple heads) often failed to reliably sort items in middle positions and lacked consistent inter-group structure; (2) positional encodings other than label-based (sinusoidal, learnable) performed markedly worse for length generalization and training; (3) sequence-level accuracy degrades with increased length (though not catastrophically); (4) top-1 rollout (greedy decoding) performance can be worse than teacher-forced evaluation, with EOS predictions especially susceptible in multi-task setups; (5) some attention heads show redundancy and ablating certain heads had small effects under teacher forcing but larger effects under rollout; (6) texture feature was less strongly represented (third-level sort) likely due to task data distribution; (7) these tasks are synthetic sequence-order problems (not true spatial-grid puzzles like Sudoku), and label-based encoding does not represent true metric item distances.",
            "comparison_baseline": "Comparisons include different positional encodings (label-based vs sinusoidal vs learnable): label-based encoding substantially outperformed sinusoidal and learnable encodings for length generalization and training speed. Architectural comparisons: two-layer models outperformed single-layer models for multi-level tasks; attention-backload (more heads in second layer) outperformed attention-frontload in multi-task accuracy; multi-headed models outperformed single-headed for rollout performance. The paper references other algorithmic/graph baselines (e.g., graph neural networks in CLRS benchmarks) as generally struggling on long extrapolation, but no human baseline is provided; comparisons vs large pretrained LLMs are discussed only in related work (length-generalization challenges) and not empirically evaluated here.",
            "uuid": "e5052.0",
            "source_info": {
                "paper_title": "Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring length generalization in large language models",
            "rating": 2
        },
        {
            "paper_title": "Randomized positional encodings boost length generalization of transformers",
            "rating": 2
        },
        {
            "paper_title": "The CLRS algorithmic reasoning benchmark",
            "rating": 2
        },
        {
            "paper_title": "Neural algorithmic reasoning",
            "rating": 2
        },
        {
            "paper_title": "Grokking: Generalization beyond overfitting on small algorithmic datasets",
            "rating": 1
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 1
        }
    ],
    "cost": 0.00935325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks</h1>
<p>Yuxuan Li<br>Department of Psychology<br>Stanford University<br>Stanford, CA 94305<br>liyuxuan@stanford.edu</p>
<p>James L. McClelland<br>Department of Psychology<br>Stanford University<br>Stanford, CA 94305<br>jlmcc@stanford.edu</p>
<h4>Abstract</h4>
<p>Transformer networks have seen great success in natural language processing and machine vision, where task objectives such as next word prediction and image classification benefit from nuanced context sensitivity across high-dimensional inputs. However, there is an ongoing debate about how and when transformers can acquire highly structured behavior and achieve systematic generalization. Here, we explore how well a causal transformer can perform a set of algorithmic tasks, including copying, sorting, and hierarchical compositions of these operations. We demonstrate strong generalization to sequences longer than those used in training by replacing the standard positional encoding typically used in transformers with labels arbitrarily paired with items in the sequence. We search for the layer and head configuration sufficient to solve these tasks, then probe for signs of systematic processing in latent representations and attention patterns. We show that two-layer transformers learn reliable solutions to multi-level problems, develop signs of task decomposition, and encode input items in a way that encourages the exploitation of shared computation across related tasks. These results provide key insights into how attention layers support structured computation both within a task and across multiple tasks.</p>
<h2>1 Introduction</h2>
<p>Since their introduction (Vaswani et al., 2017), transformer-based models have become the new norm of natural language modeling (Brown et al., 2020; Devlin et al., 2018) and are being leveraged for machine vision tasks as well as in reinforcement learning contexts (Chen et al., 2021; Dosovitskiy et al., 2020; Janner et al., 2021; Ramesh et al., 2021). Transformers trained on large amounts of data under simple self-supervised, sequence modeling objectives are capable of subsequent generalization to a wide variety of tasks, making them an appealing option for building multi-modal, multi-task, generalist agents (Bommasani et al., 2021; Reed et al., 2022).
Central to this success is the ability to represent each part of the input in the context of other parts through the self-attention mechanism. This may be especially important for task objectives such as next word prediction and image classification at scale with naturalistic data, which benefit from nuanced context sensitivity across high-dimensional inputs. Interestingly, transformer-based language models seem to also acquire some knowledge of syntactic structures without being explicitly trained to do so and display few-shot learning capabilities (Brown et al., 2020; Linzen and Baroni, 2021; Manning et al., 2020). These insights have led to ongoing work assessing broader reasoning capabilities in these models (Binz and Schulz, 2022; Dasgupta et al., 2022).</p>
<p>Despite success in learning large-scale, naturalistic data and signs of generalizable behavior or sensitivity to structures, how transformer models support systematic generalization remains to</p>
<p>be better understood. Recent work demonstrated that large language models struggle at longer problems and fail to robustly reason beyond the training data (Anil et al., 2022; Razeghi et al., 2022). Different architectural variations have been proposed to improve length generalization in transformers, highlighting the role of variants of position-based encodings (Csordás et al., 2021a,b; Ontanón et al., 2021; Press et al., 2021). Indeed, whether neural networks will ever be capable of systematic generalization without building in explicit symbolic components remains an open question (Fodor and Pylyshyn, 1988; Smolensky et al., 2022).</p>
<p>Here, we approach this question by training a causal transformer to perform a set of algorithmic operations, including copy, reverse, and hierarchical group or sort tasks. We explicitly sought the minimal transformer that would reliably solve these simple tasks and thoroughly analyze such minimal solution through attention ablation and representation analysis to understand the internal computational dynamics. Exploring how a transformer with no predefined task-aligned structure could adapt to structures in these algorithmic tasks provides a starting point for understanding how self-attention can tune to structures in more complex problems, e.g., those with the kinds of exceptions and partial regularities of natural datasets, where the exploitation of task structures may occur in a more approximate and graded manner. Our main contributions are:</p>
<ol>
<li>We highlight a simple label-based order encoding method in place of the positional encoding methods typically used in transformers, and show that it helps our models achieve strong length generalization performance across the set of algorithmic tasks we examine.</li>
<li>We thoroughly analyze simple, two-layer causal transformers that learn these algorithmic tasks, and show that the attention layers develop signs of systematic decomposition within tasks and exploitation of shared structures across tasks.</li>
</ol>
<h1>2 Method</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Task and model design.
Dataset. We created an item pool covering all combinations of 5 shapes, 5 colors, and 5 textures, and generated a sequence dataset by sampling 100k sequences of 5-50 items randomly selected from the item pool. The tasks we used to train the models are shown in Fig 1A. Each task corresponds to one of the following rules, which relies on item feature and/or item order information to rearrange an input sequence (grouping or sorting items by a particular feature is with respect to a pre-defined feature sort order, e.g., circles $&lt;$ squares $&lt;$ pentagons, or red $&lt;$ purple $&lt;$ blue):</p>
<p>COPY (C): copy the input sequence.
REVERSE (R): reverse the input sequence.
GROUP[SHAPE] (G[S]): group the items by shape, preserve the input order within each shape group.
GROUP[COLOR] (G[C]): group the items by color, preserve the input order within each color group.
SORT[SHAPE, COLOR, TEXTURE] (S[S]): sort the items first by shape, then by color, then by texture.
SORT[COLOR, SHAPE, TEXTURE] (S[C]): sort the items first by color, then by shape, then by texture.
We instantiated the token vocabularies as onehot or multihot vectors. The task tokens were onehot vectors with the corresponding task category set to one, with one additional task dimension corresponding to the end-of-sequence (EOS) token. The item tokens were multihot vectors whose units</p>
<p>indicated its value in each feature dimension (equivalent to concatenated onehot feature vectors). As such, the model receives disentangled feature information in the input, though in principle it can learn to disentangle feature information given onehot encodings for each unique item.</p>
<p>Label-based order encoding. Using position-based order encodings, models trained with sequences up to length $L$ encounter an out-of-distribution problem when tested on longer sequences, as position encodings beyond $L$ are unfamiliar to the model. We introduce label-based encoding, which instead pairs items in each sequence with ascending random integer labels to communicate order information (Fig 1B). This allows models to encode longer sequences of tokens with familiar labels seen during training. In our model, these labels were embedded with learnable weights, and we contrast the random label encoding method with sinusoidal and learnable encodings based on item positions. A concurrent work also explored the random position method and tested with other types of encodings (Anonymous, 2022). In all reported results, we pre-generated item labels sampled from a range up to the maximum generalization length (50) for all sequences in the dataset, and these labels were shared across training steps and model seeds. In practice, the labels for each sequence can be sampled online and from a larger range to accommodate generalization to even longer sequences.</p>
<p>Model. The main model architecture is shown in Fig 1B. Each input sequence consisted of a task token and the paired item and label tokens, with the EOS token serving as the first query for tokens in the output sequence. The input tokens were first embedded to the model's latent representational space through a set of embedding layers depending on the token type (task, item, or label). The item and label embeddings were then added to form a composite item embedding. These embedded tokens were fed into a causal transformer, which contained one or two layers of alternating future-masked attention sublayers and MLP sublayers. Residual connections and layer normalization were applied after each sublayer as in Vaswani et al. (2017). We tested architectural variations in the number of attention heads in different layers of the model while controlling for the total number of learnable parameters (see detailed hyperparameters in Appendix B). The state of the query token at the output of the causal transformer was passed through two linear heads to predict the next output token (the task token, or an item and its associated label).</p>
<p>Training and evaluation. The models were trained using full teacher forcing (where we always feed the model the correct tokens) on all sequences of lengths 5 to 25 in the dataset ( $\sim 46 \mathrm{k}$ ) and evaluated for length generalization on sequences of lengths 26 to $50(\sim 54 \mathrm{k})$. We trained models in both single-task and multi-task settings. In both cases, the output sequence consisted of the correctly ordered items and their labels given the task being trained, followed by an EOS token. In single-task learning, we did not include the task token in training or evaluation. In multi-task learning, the task token was used and the models were trained to first output the task token before predicting the output sequence. The training sequences used in multi-task learning remained the same ones between lengths 5-25, but each sequence corresponded to a different output sequence under different tasks.
The models were trained using softmax cross-entropy loss on the prediction of feature classes, labels, and task/EOS categories for tokens in the output sequence. Item predictions were treated as average feature prediction accuracy, i.e., if the model predicted $2 / 3$ features correct, its token-level item accuracy is $2 / 3$. Training stopped at 32 k gradient updates for single-task models and 38 k gradient updates for multi-task models. Below, we report both token-level and sequence-level accuracy, under both teacher forcing and top1 rollout (i.e., greedy decoding). Results were aggregated over four random seeds for each task type $\times$ architecture pair. Unless otherwise specified, results were taken from the checkpoint with the highest generalization accuracy within each seed. Error shades and error bars indicate standard error of the mean across models.</p>
<h1>3 Results</h1>
<h3>3.1 Single-task learning</h3>
<p>Two-layer models with label encoding learn the SORT task and generalize to longer sequences. We first trained the model with the SORT[SHAPE, COLOR,TEXTURE] task. Using our label encoding method, models with two single-headed layers (indicated as [1,1]) were able to achieve near-ceiling accuracy on training sequences and generalize to longer sequences (Fig 2; also see quantitative results in Appendix C). The predictions of the EOS token were also highly accurate in these models (see Fig S1A in Appendix A.1). Item prediction was more accurate than label prediction in this task, reflecting that the models represented item feature information more accurately in order to sort</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Token- and sequence-level accuracy for single-task models. A. Token-level accuracy on training and generalization sequences over learning. B. Token-level accuracy over sequence length. C. Token-level accuracy over sequence positions. D. Proportion of sequences that the model predicted 100\% tokens correct (upper) or predicted greater than 95\% tokens correct (lower). In B, C, and D, results were taken from 5 k novel sequences in the training length range (in B and D) and 5 k generalization sequences (B, C, and D). Legends indicate the number of attention heads in each layer and the order encoding used (in A). Gray shades indicate the range of lengths used in training.
the input tokens. The two-layer models showed some degradation in sequence-level accuracy as a function of sequence length, but the failures on longer sequences were not catastrophic, as these models scored very well on longer sequences when up to 5\% prediction errors were allowed (Fig 2D; also see Fig S1B, and Fig S2 for accuracy under rollout in Appendix A.1). In contrast, two-layer models trained with sinusoidal or learnable position encodings performed worse across both training and generalization sequences (Fig 2A).
The two-layer models were also much better than single-layer models with either one or two attention heads. While these single-layer models were able to exploit some correlations between items and output positions (e.g., item $[0,0,0]$ always came first, and item $[4,4,4]$ always came last), they failed to sort items in the middle positions (Fig 2C). In contrast, a single-layer, single-headed model was sufficient to learn the COPY or the REVERSE task (see Fig S3A in Appendix A.1), suggesting that multiple layers strongly benefit successful learning of multi-level problems.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Attention patterns in two-layer models. A. Attention maps for an example generalization sequence. Items in the input sequence were reordered to match their output order for visualization purposes. Numbers 1-5 mark the beginning of each shape group. Label e indicates the EOS token. B. First-layer attention from query items to source items within shape groups. C. Attention to EOS as a function of item index within each shape group (indicated by labels s1-s5). Results in B and C were aggregated across 1 k generalization sequences and across seeds.</p>
<p>Distinct two-stage processing across attention layers. The attention weights in the two-layer models revealed signs of task decomposition (Fig 3A). The attention head in the first layer tended to distribute attention to the unsorted items that share the same shape as the current query item. The attention head in the second layer then almost exclusively attended to the next output token in preparation for feature and label readout. This pattern appeared robustly across sequences and across different seeds (Fig 3B). Interestingly, there was an increase in the attention weights to the EOS token as the model received query items towards the end of each shape group. This attention to EOS increased to similar degrees in early or late shape groups (Fig 3C), again suggesting that the model learned to systematically process items within each shape group, even though generating the EOS</p>
<p>token was only relevant after sorting all items. We also found similar attention patterns in two-layer, single-headed models learning the GROUP[SHAPE] task (see Fig S3B in Appendix A.1).</p>
<p>The single-layer models displayed some attention to subsequent items in the output sequence but lacked consistent structures across different shape groups (see Fig S4 in Appendix A.1). This could be due to the burden for the attention head(s) within a single layer to implement a mixture of item contextualization and readout of the correct item or label, and reflects an advantage of the two-layer models in injecting an inductive bias for a multi-stage solution.</p>
<p>Acquisition of within-feature order information. To solve the sort task accurately, the models needed to learn the invariant sort order within each feature. We tested if the learned order information can be parsed out from the input embeddings. We found that the embedding weights associated with shape and color feature units reflected within-feature order similarities, with weights associated with different features appearing mostly orthogonal (Fig 4A). The weaker structure and smaller magnitude associated with texture weights may be due to lower demand in sorting multiple texture values, as texture was the third-level sort feature and thus had fewer consecutive values in a single output sequence under limited sequence length.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Input item embeddings capture information about feature sort order. A. Dot product similarity measure of item embedding weights. s1-s5/c1-c5/t1-t5: shape/color/texture embedding weights. B. Fitting Gaussian representations to track learned feature sort order in the input embeddings. Left, fit objective (mean L2 loss) over pairwise similarities of feature embeddings. Right, best-fitting Gaussian spread parameter for each feature.</p>
<p>We quantitatively tracked how information about feature sort order was acquired in the input embeddings over learning, using an L2 loss over the difference in the pairwise similarities from synthetic Gaussian feature representations and that from the models' feature embeddings. This analysis suggested that the two-layer models initially began to acquire feature sort order information concurrently in all three features (Fig 4B, left panel). In later stages during learning, the sort order information in color embeddings more quickly and better corresponded to similarities between Gaussian representations, with shape embeddings deviating from Gaussian-like, monotonic order similarities.</p>
<h1>3.2 Multi-task learning</h1>
<p>Multi-task learning and length generalization in two-layer, multi-headed models. To explore the ability for the causal transformer to simultaneously learn multiple algorithmic tasks, we trained models to predict different output sequences on the same input sequence conditioned on a task token. The two-layer, single-headed model used in single-task learning was unable to learn all six tasks, while two-layer, multi-headed models achieved good training and generalization performance (Fig 5A, also see quantitative performance in Appendix C). Increasing the number of attention heads in the model did not lead to drastically different learning curves under teacher forcing, but more attention heads supported much better performance under top1 rollout (Fig 5B).</p>
<p>We tested whether multi-headed attention served for better learning when it occurred in the first layer (attention-frontload) or the second layer (attention-backload). Attention-backload models generally achieved higher accuracy across all tasks compared to their attention-frontload counterparts (Fig 5C and 5D). Performance of the attention-backload models was even comparable with their attention-balanced counterparts, which signals that a bottlenecked architecture may be particularly suited for multi-task learning in our setting, considering that some tasks in the task suite share the first-level grouping feature.</p>
<p>Consistent with the single-task model, six-task models demonstrated strong generalization to longer sequences (Fig 6A). Even though sequence-level accuracy degraded as sequence length increased, the models only made less than 5\% prediction errors for long extrapolation sequences (Fig 6C). Token-level accuracy among generalization sequences was also relatively stable until the last few output positions (Fig 6B). We also observed that accurately predicting item features in long sequences was easier in the SORT tasks but harder in the COPY, REVERSE, and GROUP tasks (Fig 6A and 6C). This echos results from the single-task models and again suggests that the models more accurately represented information directly used for sorting the items in each task. See additional results on EOS prediction and accuracy under rollout in Appendix A.2.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Token-level item and label accuracy in multi-task models. Legends indicate the number of attention heads in the two layers. A. Accuracy across all tasks over learning. B, C, and D. Top1 rollout accuracy for models with attention-balanced, attention-frontload, and attention-backload architectures. In A, results were taken from 75 k generalization sequences ( 12.5 k per task). In B, C, and D, results were taken from 1.2 k generalization sequences ( 200 per task).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Length generalization across tasks. A and B. Token-level item and label accuracy over sequence length and sequence position. C. Proportion of sequences that the model predicted 100\% tokens correct (upper) or predicted greater than $95 \%$ tokens correct (lower). Results were taken from 1 k novel sequences in the training length range (in A and C) and 1 k generalization sequences (in A, B, and C) for each task. Performances were aggregated across the five top-performing runs. Gray shades indicate the range of lengths used in training.</p>
<p>Learned task embeddings recover task similarity. We examined the input task embeddings to understand the basis of task-shared and task-specific computation in the model. Fig 7A shows that similarities among the learned task embeddings reflected similarities across tasks. For example, the COPY task was recognized to be similar to the REVERSE task and the two GROUP tasks, potentially reflecting the shared need for stronger representation of label information in order to accurately sort the input items. Representations for the two SORT tasks were also highly similar, as they both rely</p>
<p>on item features to sort the input items. The models also recovered similarities between the pairs of GROUP and SORT tasks that share the same first-level grouping feature.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: A. Pairwise similarities of task embeddings. B. Token-level accuracy from ablating single attention heads. C. Token-level accuracy from preserving attention to certain tokens across attention heads. D. Attention maps from selected attention heads in an example generalization sequence (see Fig S7 for all attention maps). Items corresponding to the input sequence were reordered to match their output order for visualization purposes. Numbers 1-5 mark the beginning of each first-level feature group (shape or color). Label t indicates the task token. Label e indicates the EOS token. In B, C, and D, results were taken from the top-performing model (architecture: [1,4]).</p>
<p>Task-shared and task-dependent computation across attention layers. Structures in the task embedding similarities and strong performance from models with fewer attention heads than there are tasks already hinted that the model could be exploiting shared processing across tasks. Because task-conditioned computation can only occur across the attention layers, we next sought to understand the role of multi-headed attention in implementing such shared computation. We performed two ablation experiments: ablating a single attention head entirely and preserving attention in all other heads, or preserving attention to certain tokens across all attention heads. Attention weights were ablated by masking the attention weights as zero after softmax.</p>
<p>Fig 7B and C show the ablation results for the top-performing model with one attention head in the first layer and four attention heads in the second layer. The attention heads did not exhibit strong selectivity for single tasks as they usually contributed to multiple tasks, and they also showed equal contribution to item and label prediction (Fig 7B, also see results from other models in Fig S8 in Appendix A.2). One attention head appeared redundant as ablating it resulted in little impact on the performance of any task under teacher forcing, but it significantly improved accuracy under rollout.</p>
<p>Ablating attention to certain items in the sequence further indicated that the models were heavily relying on the learnable task embedding to contextualize items under different tasks (Fig 7C, also see Fig S8). When the models were only allowed to attend to the task token in the first layer and the next output token in the second layer, performance was largely preserved in the GROUP tasks and the SORT tasks (ablation type "taskL0+nextL1"). This is different from the two-layer single-task models, whose first layer relied on attention to other items in the first-level feature group to cross-contextualize items. However, there were still some signs of task decomposition in the multi-task model. For example, one attention head in the second layer consistently attended to the first item in the next feature group across all multi-level tasks (Fig 7D; see full attention maps in Fig S7 in Appendix A.2), which proved useful for accurate predictions across multi-level tasks under both teacher forcing and rollout (Fig 7B).</p>
<p>Task-conditioned item encoding. To better understand task-conditioned item representations, we analyzed how the items were encoded under different tasks after each attention layer. We computed average item representations of all 25 shape $\times$ color feature pairs in each task, aggregating across texture and label values in 200 generalization sequences. Projecting these item representations to lower dimensions revealed that the model reorganizes representations of item features depending</p>
<p>on the task structure (Fig 8). For the two GROUP tasks, representations in the first layer encoded all items by their relevant first-level grouping feature (shape or color). The item representations did not consistently encode other feature information across both layers, presumably because label information became more relevant for sorting items within each feature group.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Task-conditioned encoding of the same sequence of items from the top-performing model. PCA was performed for average item representations across 200 generalization sequences within each layer $\times$ task pair. The legend indicates the predefined sort order of the shape and color features.</p>
<p>Interestingly, for the two SORT tasks, the encoded representations after the first layer also showed a high degree of clustering in the corresponding first-level grouping feature, with the secondary sort feature represented in a ring structure. The subsequent layer then expanded the item representations along the secondary feature in a way that was consistent across all first-level feature groups. Notably, the clustering effect and the ring structure in the first-layer representations in the SORT tasks were more strongly observed in the attention-backload models (see Fig S8 in Appendix A.2). This could suggest that room for flexible multi-headed transformation downstream can relax first-layer encoding, enabling a more consistent solution to emerge across the GROUP and SORT tasks.</p>
<h1>4 Related Work</h1>
<p>There is a growing interest in analyzing small models in more controlled task settings to better understand the capabilities and detailed computations in transformers. For example, Power et al. (2022) explored learning and generalization dynamics in two-layer causal transformers learning binary operations, and Elhage et al. (2021) explored mechanistic interpretability in one- and two-layer transformers without MLP sublayers. Our work contributes to these efforts in beginning to shape some understanding of the computation and representation dynamics in transformers using detailed analyses on small-scale models.</p>
<p>Recent work examining systematic generalization in transformers or pre-trained language models highlighted that length generalization remains a challenge and observed that positional encoding can have a significant impact on the extent to which models can systematically generalize (Anil et al., 2022; Csordás et al., 2021a; Delétang et al., 2022; Ontanón et al., 2021). Many types of architectural modifications have also been proposed to help transformers achieve better length generalization, including different ways to represent positional information (e.g. Csordás et al., 2021b; Dehghani et al., 2018; Press et al., 2021; Su et al., 2021). Our label-based encoding method adds to this effort by demonstrating the potential in formulating sequence modeling tasks with a more general item-label binding approach rather than item-position binding. Concurrent to our work, Anonymous (2022) developed randomized position encoding (equivalent to our label-based encoding method) and showed its advantage over a variety of position-based encodings.</p>
<p>Outside of the context of transformers and language models, the use of synthetic, algorithmic tasks has enabled much understanding of the core capabilities of many models (e.g. Graves et al., 2014). There has also been some interest in performing algorithmic reasoning with neural networks for its own sake (Veličković and Blundell, 2021). Correspondingly, Veličković et al. (2022) recently proposed a benchmark for algorithmic reasoning, and evaluated a variety of graph neural network architectures, all of which struggled to extrapolate algorithms to longer or larger inputs. Our work shows that self-attention has the potential to adapt to structures in the sequence and find reliable solutions to algorithmic tasks.</p>
<p>Ablation experiments and a variety of representation analyses have been applied to understand the role of the attention mechanism in NLP tasks as well as in transformer-based vision models (Chefer et al., 2021; Manning et al., 2020; Michel et al., 2019; Voita et al., 2019). However, it is often debated</p>
<p>to what extent attention weights afford model interpretability in these settings (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Vashishth et al., 2019), especially considering head redundancy and the difficulty in correctly attributing relevance over high-dimensional inputs. We show that at least in simple settings, the attention heads can exhibit some level of interpretability consistent with known task structures. Similar methods have also been applied to understand unit-level and layer-level dynamics that support multi-task computation in small-scale RNNs (Driscoll et al., 2022; Yang et al., 2019), which revealed some patterns that are consistent with our findings here, as we discuss below.</p>
<h1>5 Discussion</h1>
<p>We sought to understand how transformers can solve a set of highly-structured algorithmic tasks and systematically generalize. We presented two-layer causal transformers that can learn copying, reversing, and hierarchical sorting operations that generalize to sequences longer than seen during training. We found that these models learned to exploit structures within tasks or shared across related tasks and exhibited interesting signatures of task decomposition. Specifically, the attention layers learned to represent item features in a way that helps subsequent individual attention heads multitask or implement similar computations across item groups.
We highlight that the label-based order encoding method was key to enabling our models to generalize the learned tasks to longer sequences. The key insight is to sample random labels to communicate sequence order information instead of relying on sequence positions. This simple extension exposes models to a large range of possible labels evenly during training so that longer sequences can be encoded with familiar labels, and is shown to be effective both in our tasks and in a range of different algorithmic tasks (Anonymous, 2022). Compared to this approach, learnable position encoding not only resulted in poor length generalization, but also slower learning on the training sequences in our tasks - potentially because the models had fewer sequences to learn to encode later positions compared to early ones. Sinusoidal position encoding has been noted to have limited length generalization capabilities (Ontanón et al., 2021; Csordás et al., 2021a). In our setting, it may have additionally suffered from the demand to read out the corresponding item positions, which is not common across NLP tasks. It is worth noting that label-based encoding alone would not represent true item distance information. However, in light of recent work showing that transformers can learn positional information without explicit positional encodings (Haviv et al., 2022), it is possible that the benefits of label-based order encoding may transfer to natural language inputs.
Our findings on the contributions of individual attention heads in a multi-headed layer in solving these algorithmic tasks echo results from analyses of language models. For example, single attention heads are rarely responsible for a particular task or syntactic relationship and can often appear redundant (Manning et al., 2020; Michel et al., 2019; Voita et al., 2019). We do see some degree of selectivity, with conceptually distinct task components shared across a subset of attention heads. Recent work studying multi-task computation in RNNs has similarly found that multi-task learning led to the exploitation of reusable computation across related tasks (Driscoll et al., 2022; Yang et al., 2019). Interestingly, these recurrent models also exhibited high degrees of task-selectivity at the level of individual units in the hidden layer. The degree of task-selectivity in different architectural components may vary depending on the inductive biases of different model families and the tasks being learned. Future work is needed to fully characterize the degree of possible computational modularity in multi-headed attention in relation to task structures.
The flexibility to learn and perform multiple tasks is a key desired capability for machine learning. Our work here provides insights into the dynamics of within-task and cross-task computations that stacks of attention layers develop when learning highly-structured sequences. Recent work has explored multi-task learning in transformers at scale and achieved impressive results (Lee et al., 2022; Reed et al., 2022). As transformers are increasingly being leveraged for multi-task and multi-modal learning in domains with richer task structures, it is possible that these models may implicitly learn to decompose complex decisions into reusable, multi-level policies. In future work, we hope to explore these learning and generalization dynamics in transformer-based agents to understand the acquisition of task-conditioned, multi-level behavioral policies in structured environments.</p>
<h1>Acknowledgments</h1>
<p>We would like to thank Andrew Nam, Mengye Ren, and members of the Stanford PDP lab for useful discussions, and Andrew Lampinen for comments on the manuscript draft.</p>
<h2>References</h2>
<p>Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. arXiv preprint arXiv:2207.04901, 2022.</p>
<p>Anonymous. Randomized positional encodings boost length generalization of transformers. https://openreview.net/forum?id=nMYj4argap, 2022.</p>
<p>Marcel Binz and Eric Schulz. Using cognitive psychology to understand gpt-3. arXiv preprint arXiv:2206.14576, 2022.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 782-791, 2021.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097, 2021.</p>
<p>Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. arXiv preprint arXiv:2108.12284, 2021a.</p>
<p>Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The neural data router: Adaptive control flow in transformers improves systematic generalization. arXiv preprint arXiv:2110.07732, 2021b.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051, 2022.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Marcus Hutter, Shane Legg, and Pedro A Ortega. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Laura Driscoll, Krishna Shenoy, and David Sussillo. Flexible multitask computation in recurrent networks utilizes shared dynamical motifs. bioRxiv, 2022.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html.</p>
<p>Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022.</p>
<p>Sarthak Jain and Byron C Wallace. Attention is not explanation. arXiv preprint arXiv:1902.10186, 2019 .</p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling problem. In ICML 2021 Workshop on Unsupervised Reinforcement Learning, 2021.</p>
<p>Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. arXiv preprint arXiv:2205.15241, 2022.</p>
<p>Tal Linzen and Marco Baroni. Syntactic structure from deep learning. Annual Review of Linguistics, 7:195-212, 2021.</p>
<p>Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the National Academy of Sciences, 117(48):30046-30054, 2020.</p>
<p>Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019.</p>
<p>Santiago Ontanón, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks. arXiv preprint arXiv:2108.04378, 2021.</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.</p>
<p>Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.</p>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR, 2021.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.</p>
<p>Paul Smolensky, Richard Thomas McCoy, Roland Fernandez, Matthew Goldrick, and Jianfeng Gao. Neurocompositional computing: From the central paradox of cognition to a new generation of ai systems. AI Magazine, 2022.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.</p>
<p>Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. Attention interpretability across NLP tasks. arXiv preprint arXiv:1909.11218, 2019.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Petar Veličković and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, 2021.
Petar Veličković, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, and Charles Blundell. The CLRS algorithmic reasoning benchmark. arXiv preprint arXiv:2205.15659, 2022.</p>
<p>Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.</p>
<p>Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019.</p>
<p>Guangyu Robert Yang, Madhura R Joglekar, H Francis Song, William T Newsome, and Xiao-Jing Wang. Task representations in neural networks trained to perform many cognitive tasks. Nature neuroscience, 22(2):297-306, 2019.</p>
<h1>A Additional Results</h1>
<h2>A. 1 Single-task learning</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure S1: Additional teacher forcing results. A. Single-task models predict EOS token near perfectly. B. Sequence-level accuracy when up to $10 \%$ errors were allowed. Results were taken from 5 k novel sequences in the training length range (in B) and 5 k generalization sequences (in A and B ).
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure S2: Top1 rollout accuracy for two-layer models. A. Token-level (upper) and sequencelevel (lower) accuracy across sequence length. B. Token-level accuracy across sequence positions. Results were taken from 5 k novel sequences in the training length range (in A) and 5 k generalization sequences (in A and B).
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure S3: Learning other tasks in the single-task setting. A. Single-layer, single-headed models can learn the COPY task and the REVERSE task, while two-layer, single-headed models learn the GROUP[SHAPE] task. B. The attention maps from the GROUP[SHAPE] model resemble that from the SORT[SHAPE] model (visualization as in Fig 3A).</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure S4: Attention maps from single-layer models in an example generalization sequence. A. Single-layer, single-headed model. B. Single-layer, two-headed model. Visualization as in Fig 3A.</p>
<h1>A. 2 Multi-task learning</h1>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure S5: Visualization as in Fig 6, but item and label predictions were obtained using top1 rollout.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure S6: A. Six-task models predict the EOS token perfectly under teacher forcing. B. Prediction of the EOS token deteriorates under top1 rollout in six-task models. Results were taken from 1 k training sequences and 1 k length generalization sequences across the five top-performing runs.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure S7: Attention maps for an example generalization sequence (for the top-performing multi-task model with [1,4] architecture). Each row shows the attention maps for one task, and each column corresponds to a single attention head denoted by the layer and head index. Tokens corresponding to the input sequence were reorderd to match their order in the output sequence for visualization purposes. Label t indicates the task token. Label e indicates the EOS token. In the bottom four rows, number labels ranging from 1-5 mark the beginning of the items in each first-level feature group (shape or color).</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure S8: Attention ablation and task-conditioned item representations in other top-performing models. Visualization as in Fig 7B and Fig 8.</p>
<h1>B Hyperparameters</h1>
<p>Table S1: Model and experiment hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Single-task learning</th>
<th style="text-align: center;">Six-task learning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Number of layers</td>
<td style="text-align: center;">1 or 2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Number of attention heads</td>
<td style="text-align: center;">(see paper)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Embedding dimension</td>
<td style="text-align: center;">128 (two-layer) or 184 (single-layer)</td>
<td style="text-align: center;">192</td>
</tr>
<tr>
<td style="text-align: center;">MLP hidden layer size</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Activation function</td>
<td style="text-align: center;">ReLU</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Batch size</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Training teacher forcing rate</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">$10^{-4}$</td>
<td style="text-align: center;">$5 \cdot 10^{-4}$</td>
</tr>
</tbody>
</table>
<h2>C Quantitative Performance</h2>
<p>Table S2: Token-level item and label accuracy across 54 k generalization sequences in single-task models. Mean and standard deviation across four random seeds are shown for each architecture.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">task</th>
<th style="text-align: center;">architecture</th>
<th style="text-align: center;">position encoding</th>
<th style="text-align: center;">item prediction</th>
<th style="text-align: center;">label prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">$[1]$</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$99.03 \pm 0.41$</td>
<td style="text-align: center;">$100.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">$[1]$</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$99.75 \pm 0.21$</td>
<td style="text-align: center;">$100.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">$[1]$</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$76.49 \pm 2.99$</td>
<td style="text-align: center;">$51.75 \pm 5.35$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">$[2]$</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$91.01 \pm 2.76$</td>
<td style="text-align: center;">$78.02 \pm 5.89$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">$[1,1]$</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$99.60 \pm 0.07$</td>
<td style="text-align: center;">$98.41 \pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;">G[s]</td>
<td style="text-align: center;">$[1,1]$</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$98.41 \pm 0.32$</td>
<td style="text-align: center;">$99.31 \pm 0.16$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">$[1,1]$</td>
<td style="text-align: center;">sinusoidal</td>
<td style="text-align: center;">$57.04 \pm 10.75$</td>
<td style="text-align: center;">$16.50 \pm 6.52$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">$[1,1]$</td>
<td style="text-align: center;">learnable</td>
<td style="text-align: center;">$73.15 \pm 11.65$</td>
<td style="text-align: center;">$40.51 \pm 14.28$</td>
</tr>
</tbody>
</table>
<p>Table S3: Token-level prediction accuracy across 75 k generalization sequences in six-task models (including 500 unique sequences for each length evaluated within each task). Mean and standard deviation across four random seeds are shown for each architecture. Model architectures are denoted by [# heads in the first layer, # heads in the second layer].
(a) Attention-balanced models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">$[1,1]$</th>
<th style="text-align: center;">$[2,2]$</th>
<th style="text-align: center;">$[3,3]$</th>
<th style="text-align: center;">$[4,4]$</th>
<th style="text-align: center;">$[6,6]$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">all</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$83.38 \pm 13.05$</td>
<td style="text-align: center;">$97.04 \pm 0.88$</td>
<td style="text-align: center;">$98.55 \pm 0.46$</td>
<td style="text-align: center;">$98.79 \pm 0.94$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$87.21 \pm 7.86$</td>
<td style="text-align: center;">$96.46 \pm 1.24$</td>
<td style="text-align: center;">$97.98 \pm 0.42$</td>
<td style="text-align: center;">$98.68 \pm 0.84$</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$89.28 \pm 14.44$</td>
<td style="text-align: center;">$97.57 \pm 1.19$</td>
<td style="text-align: center;">$98.81 \pm 0.77$</td>
<td style="text-align: center;">$98.98 \pm 0.95$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$97.39 \pm 0.84$</td>
<td style="text-align: center;">$98.59 \pm 0.99$</td>
<td style="text-align: center;">$99.46 \pm 0.39$</td>
<td style="text-align: center;">$99.64 \pm 0.34$</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$64.30 \pm 24.46$</td>
<td style="text-align: center;">$96.95 \pm 1.21$</td>
<td style="text-align: center;">$98.95 \pm 0.45$</td>
<td style="text-align: center;">$98.69 \pm 0.94$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$94.56 \pm 8.26$</td>
<td style="text-align: center;">$98.53 \pm 0.80$</td>
<td style="text-align: center;">$99.64 \pm 0.16$</td>
<td style="text-align: center;">$99.39 \pm 0.29$</td>
</tr>
<tr>
<td style="text-align: center;">G[s]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$86.47 \pm 13.51$</td>
<td style="text-align: center;">$96.51 \pm 0.93$</td>
<td style="text-align: center;">$98.42 \pm 0.66$</td>
<td style="text-align: center;">$98.52 \pm 1.15$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$91.54 \pm 8.30$</td>
<td style="text-align: center;">$97.83 \pm 1.07$</td>
<td style="text-align: center;">$99.07 \pm 0.33$</td>
<td style="text-align: center;">$\mathbf{9 9 . 2 3} \pm \mathbf{0 . 5 2}$</td>
</tr>
<tr>
<td style="text-align: center;">G[c]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$86.30 \pm 13.39$</td>
<td style="text-align: center;">$96.72 \pm 0.80$</td>
<td style="text-align: center;">$98.28 \pm 0.65$</td>
<td style="text-align: center;">$98.45 \pm 1.13$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$91.69 \pm 7.79$</td>
<td style="text-align: center;">$97.93 \pm 1.06$</td>
<td style="text-align: center;">$98.98 \pm 0.19$</td>
<td style="text-align: center;">$99.21 \pm 0.46$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$86.97 \pm 8.80$</td>
<td style="text-align: center;">$97.28 \pm 1.00$</td>
<td style="text-align: center;">$98.48 \pm 0.29$</td>
<td style="text-align: center;">$99.01 \pm 0.82$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$74.75 \pm 11.47$</td>
<td style="text-align: center;">$93.21 \pm 2.23$</td>
<td style="text-align: center;">$95.53 \pm 1.01$</td>
<td style="text-align: center;">$97.09 \pm 1.99$</td>
</tr>
<tr>
<td style="text-align: center;">S[c]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$86.77 \pm 8.63$</td>
<td style="text-align: center;">$97.20 \pm 0.95$</td>
<td style="text-align: center;">$98.37 \pm 0.33$</td>
<td style="text-align: center;">$99.08 \pm 0.70$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$73.57 \pm 10.59$</td>
<td style="text-align: center;">$92.73 \pm 2.05$</td>
<td style="text-align: center;">$95.23 \pm 0.90$</td>
<td style="text-align: center;">$97.53 \pm 1.71$</td>
</tr>
</tbody>
</table>
<p>(b) Attention-frontload models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">$[2,1]$</th>
<th style="text-align: center;">$[3,1]$</th>
<th style="text-align: center;">$[4,1]$</th>
<th style="text-align: center;">$[6,1]$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">all</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$69.45 \pm 17.65$</td>
<td style="text-align: center;">$68.05 \pm 19.27$</td>
<td style="text-align: center;">$77.86 \pm 22.15$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$79.95 \pm 8.53$</td>
<td style="text-align: center;">$78.35 \pm 11.60$</td>
<td style="text-align: center;">$86.53 \pm 11.67$</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$57.93 \pm 27.09$</td>
<td style="text-align: center;">$54.07 \pm 30.36$</td>
<td style="text-align: center;">$72.01 \pm 30.48$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$97.66 \pm 1.64$</td>
<td style="text-align: center;">$95.18 \pm 3.28$</td>
<td style="text-align: center;">$98.48 \pm 1.10$</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$57.97 \pm 26.94$</td>
<td style="text-align: center;">$55.33 \pm 29.52$</td>
<td style="text-align: center;">$70.49 \pm 32.68$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$95.74 \pm 3.34$</td>
<td style="text-align: center;">$94.90 \pm 3.50$</td>
<td style="text-align: center;">$97.97 \pm 2.17$</td>
</tr>
<tr>
<td style="text-align: center;">G[s]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$70.16 \pm 16.79$</td>
<td style="text-align: center;">$71.29 \pm 16.08$</td>
<td style="text-align: center;">$79.20 \pm 19.34$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$80.56 \pm 11.79$</td>
<td style="text-align: center;">$79.37 \pm 12.51$</td>
<td style="text-align: center;">$87.82 \pm 12.52$</td>
</tr>
<tr>
<td style="text-align: center;">G[c]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$70.33 \pm 16.54$</td>
<td style="text-align: center;">$70.99 \pm 16.21$</td>
<td style="text-align: center;">$78.86 \pm 19.18$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$79.18 \pm 12.71$</td>
<td style="text-align: center;">$78.71 \pm 12.82$</td>
<td style="text-align: center;">$87.28 \pm 12.69$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$79.94 \pm 11.69$</td>
<td style="text-align: center;">$78.00 \pm 12.49$</td>
<td style="text-align: center;">$83.99 \pm 15.22$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$62.76 \pm 13.85$</td>
<td style="text-align: center;">$61.17 \pm 19.50$</td>
<td style="text-align: center;">$76.69 \pm 18.13$</td>
</tr>
<tr>
<td style="text-align: center;">S[c]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$80.03 \pm 11.15$</td>
<td style="text-align: center;">$78.22 \pm 12.21$</td>
<td style="text-align: center;">$82.46 \pm 16.50$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$64.28 \pm 12.87$</td>
<td style="text-align: center;">$61.27 \pm 18.82$</td>
<td style="text-align: center;">$71.27 \pm 24.19$</td>
</tr>
</tbody>
</table>
<p>(c) Attention-backload models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">$[1,2]$</th>
<th style="text-align: center;">$[1,3]$</th>
<th style="text-align: center;">$[1,4]$</th>
<th style="text-align: center;">$[1,6]$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">all</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$96.06 \pm 0.24$</td>
<td style="text-align: center;">$98.65 \pm 0.80$</td>
<td style="text-align: center;">$\mathbf{9 9 . 4 0} \pm \mathbf{0 . 2 9}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$95.48 \pm 0.30$</td>
<td style="text-align: center;">$97.37 \pm 0.89$</td>
<td style="text-align: center;">$\mathbf{9 9 . 2 1} \pm \mathbf{0 . 4 3}$</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$96.86 \pm 0.57$</td>
<td style="text-align: center;">$99.10 \pm 0.74$</td>
<td style="text-align: center;">$\mathbf{9 9 . 4 6} \pm \mathbf{0 . 3 4}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$97.78 \pm 0.71$</td>
<td style="text-align: center;">$99.62 \pm 0.24$</td>
<td style="text-align: center;">$\mathbf{9 9 . 8 5} \pm \mathbf{0 . 0 8}$</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$95.99 \pm 0.59$</td>
<td style="text-align: center;">$98.82 \pm 1.08$</td>
<td style="text-align: center;">$99.47 \pm 0.37$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$98.09 \pm 0.52$</td>
<td style="text-align: center;">$99.55 \pm 0.39$</td>
<td style="text-align: center;">$\mathbf{9 9 . 8 3} \pm \mathbf{0 . 0 4}$</td>
</tr>
<tr>
<td style="text-align: center;">G[s]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$96.12 \pm 0.38$</td>
<td style="text-align: center;">$98.28 \pm 1.00$</td>
<td style="text-align: center;">$\mathbf{9 8 . 9 8} \pm \mathbf{0 . 4 3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$97.70 \pm 0.42$</td>
<td style="text-align: center;">$98.77 \pm 0.66$</td>
<td style="text-align: center;">$\mathbf{9 9 . 2 9} \pm \mathbf{0 . 2 7}$</td>
</tr>
<tr>
<td style="text-align: center;">G[c]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$95.82 \pm 0.72$</td>
<td style="text-align: center;">$98.53 \pm 0.76$</td>
<td style="text-align: center;">$\mathbf{9 9 . 1 7} \pm \mathbf{0 . 3 4}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$97.51 \pm 0.51$</td>
<td style="text-align: center;">$99.06 \pm 0.42$</td>
<td style="text-align: center;">$\mathbf{9 9 . 5 1} \pm \mathbf{0 . 1 8}$</td>
</tr>
<tr>
<td style="text-align: center;">S[s]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$95.86 \pm 0.77$</td>
<td style="text-align: center;">$98.51 \pm 0.72$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6 8} \pm \mathbf{0 . 2 6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$91.12 \pm 0.83$</td>
<td style="text-align: center;">$93.54 \pm 2.44$</td>
<td style="text-align: center;">$98.40 \pm 1.16$</td>
</tr>
<tr>
<td style="text-align: center;">S[c]</td>
<td style="text-align: center;">item</td>
<td style="text-align: center;">$95.72 \pm 0.54$</td>
<td style="text-align: center;">$98.70 \pm 0.71$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6 4} \pm \mathbf{0 . 2 8}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">$90.74 \pm 0.66$</td>
<td style="text-align: center;">$93.76 \pm 2.15$</td>
<td style="text-align: center;">$98.36 \pm 1.05$</td>
</tr>
</tbody>
</table>            </div>
        </div>

    </div>
</body>
</html>
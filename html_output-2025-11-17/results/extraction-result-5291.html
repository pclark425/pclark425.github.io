<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5291 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5291</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5291</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-c3e029b4a44c5784f296303f8cab2d7feb2d4ffa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c3e029b4a44c5784f296303f8cab2d7feb2d4ffa" target="_blank">Fine-Tuned Language Models Generate Stable Inorganic Materials as Text</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that language models'ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.</p>
                <p><strong>Paper Abstract:</strong> We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models'ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5291.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5291.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned LLaMA-2 (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned LLaMA-2 models for crystal generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained LLaMA-2 decoder-only transformer models (7B, 13B, 70B) were parameter-efficiently fine-tuned on string-encoded inorganic crystal data to directly generate three-dimensional crystal structures as text (CIF-parsable strings), enabling unconditional generation, text-conditioned generation, and structural infilling for materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (autoregressive language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B (reported variants)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on Materials Project-derived datasets: MP-20 (45,231 stable materials) for unconditional generation and an extended Materials Project set (~127,609 crystals after filtering) for conditional tasks; inputs were string-formatted crystal tuples (lattice, angles, element identities, fractional coordinates) with translation augmentations and stochastic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — generation/design of inorganic crystalline materials (stable/metastable crystals)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Parameter-efficient fine-tuning using LoRA (low-rank adapters) on pretrained LLaMA-2, multitask curriculum (two-thirds unconditional/text-conditional generation, one-third infilling), prompt engineering (task-specific instructions), and numeric tokenization (digit-level) with sampling controlled by temperature and nucleus (top-p).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Text-formatted crystal strings (fixed-precision numeric fields) parsed into CIF; fractional coordinates, lattice lengths/angles and element tokens encoded as text tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Structural validity (no atomic overlaps), compositional validity (net neutral charge), coverage/recall/precision vs test set, diversity and novelty (Matminer featurizations), Increase in Perplexity under Transformation (IPT) for translation invariance, predicted stability via M3GNet ML potential (energy above hull ĤE_hull < 0.1 eV/atom for metastability) and gold-standard DFT (VASP) relaxations (used as final check), sampling speed (time per 10k samples).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>MP-20 (Materials Project subset), extended Materials Project dataset (April 2023 snapshot, filtered to ≤30 atoms/unit cell); compared against CDVAE (Crystal Diffusion VAE) baseline and language-model-from-scratch baselines (LM-CH, LM-AC).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned LLaMA-2 models produced very high physical-validity rates (structural validity up to ~0.996 and compositional validity ~0.954 for best sampling hyperparameters). Best model (LLaMA-2 70B, nucleus r=0.7 sampling setting) produced ~49.8% of generated samples predicted metastable by M3GNet (ĤE_hull < 0.1 eV/atom) and ~10.6% confirmed stable by DFT (VASP) among M3GNet-predicted metastable samples. By comparison, CDVAE reported ~28.8% metastable by M3GNet and ~5.4% by DFT. Larger LLaMA models learned better translation invariance (lower IPT) and yielded more diverse stable samples; smaller LLaMA variants sampled substantially faster.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to CDVAE (diffusion + VAE hybrid specifically designed for crystals), fine-tuned LLaMA-2 70B generated metastable materials at about twice the rate (49.8% vs 28.8% by M3GNet) and more DFT-confirmed stable materials (10.6% vs 5.4%). LLaMA models matched or exceeded diversity and novelty of CDVAE in many settings and were competitive in sampling throughput (especially for 13B). Unlike diffusion models that impose architectural invariances, the LLaMA approach uses text augmentations to encourage invariance and leverages generic pretrained biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Hallucination of nonstandard/unphysical element tokens (e.g., 'Ln') unless constrained; sensitivity to prompt phrasing and numeric tokenization choices; trade-offs between stability/validity and coverage controlled by sampling hyperparameters (temperature, top-p); reliance on M3GNet surrogate for large-scale stability screening (surrogate imperfect; DFT used only for M3GNet-predicted metastable subset); expensive to train the largest model (70B) despite LoRA; conditional generation (e.g., precise space group or stability conditioning) remains challenging (space group conditioning ~24% success noted), and generating 'unstable' samples is hard because training data are mostly stable compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuned Language Models Generate Stable Inorganic Materials as Text', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5291.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5291.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flam-Shepherd & Aspuru-Guzik 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (referenced) used language models trained from scratch on discrete token sequences (including digit-level tokens for coordinates) to directly generate 3D molecular, materials, and protein structures as text files (xyz, CIF, PDB), demonstrating that standard language-model architectures can model both discrete identities and continuous coordinates when everything is tokenized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Language models trained from scratch (unspecified architectures in this paper; autoregressive LMs implied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive language model (trained from scratch on domain tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (paper references training LMs from scratch; specific sizes not detailed in this citing paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular and materials datasets encoded as token sequences where every component including numeric digits of coordinates are tokenized; prior work used standard molecular datasets (not exhaustively enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation, materials, and protein binding site design (3D structure generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Train language models from scratch on tokenized representations of coordinates and identities; unconditional/conditional sampling via autoregressive next-token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Discrete token sequences representing 3D coordinates and element/atom identities, output as xyz, CIF, or PDB formatted text files.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity and high-level dataset statistics (elemental composition validity, structural plausibility); comparisons to domain-specific models (e.g., CDVAE) for validity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Common molecular and materials datasets used in generative modeling (not exhaustively listed in this paper), comparisons made to CDVAE in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced work reported that language models trained from scratch can outperform domain-specific generative models (including CDVAE) on metrics like valid element compositions and certain high-level statistics of the training data, demonstrating the feasibility of tokenizing coordinates and using standard LMs for 3D structure generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Found to outperform CDVAE on validity and some distributional statistics in the cited work; approach differs by treating all data as discrete tokens instead of specialized continuous/diffusion models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training from scratch requires careful tokenization (e.g., of numbers) and potentially large datasets; paper notes differences in tokenization choices impact performance; such from-scratch training can be data- and compute-intensive compared to fine-tuning pretrained LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuned Language Models Generate Stable Inorganic Materials as Text', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5291.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5291.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Antunes et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crystal structure generation with autoregressive large language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that trains autoregressive language models from scratch on large corpora of CIF strings to generate crystal structures as discrete sequences, demonstrating an alternative LLM-based strategy for materials generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Crystal structure generation with autoregressive large language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive language models trained from scratch (specific model family not detailed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer / language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this citing paper (trained on 'millions of CIF strings' in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Millions of CIF-format crystal strings (domain-specific large corpus of crystal structures used to train from scratch).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — crystal structure generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Train autoregressive LMs from scratch on CIF textual representations; sample sequences autoregressively to produce new CIFs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>CIF text strings encoding lattice parameters, atomic positions, and element identities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity and distributional statistics relative to training data (as typical in generative modeling); specifics in the referenced paper but summarized here as showing feasibility of autoregressive LMs for CIF generation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Large-scale CIF corpus (millions of examples) assembled by the referenced authors; compared conceptually against domain-specific generative methods.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced work demonstrated successful generation of crystal CIF strings with autoregressive LMs trained from scratch; highlighted that language-model approaches can be effective for crystal generation when large amounts of domain-specific text-formatted examples are available.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Approach contrasts with diffusion- or graph-based specialized generative models; requires large-scale domain-specific training data to match performance of pretrained LLM fine-tuning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Needs millions of CIFs and training from scratch (data- and compute-intensive); saltation of numeric tokenization and handling of coordinate precision are important design choices; likely less sample-efficient than fine-tuning pretrained LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-Tuned Language Models Generate Stable Inorganic Materials as Text', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files <em>(Rating: 2)</em></li>
                <li>Atom-by-atom protein generation and beyond with language models <em>(Rating: 2)</em></li>
                <li>Crystal structure generation with autoregressive large language modeling <em>(Rating: 2)</em></li>
                <li>Crystal diffusion variational autoencoder for periodic material generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5291",
    "paper_id": "paper-c3e029b4a44c5784f296303f8cab2d7feb2d4ffa",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "Fine-tuned LLaMA-2 (this work)",
            "name_full": "Fine-tuned LLaMA-2 models for crystal generation",
            "brief_description": "Pretrained LLaMA-2 decoder-only transformer models (7B, 13B, 70B) were parameter-efficiently fine-tuned on string-encoded inorganic crystal data to directly generate three-dimensional crystal structures as text (CIF-parsable strings), enabling unconditional generation, text-conditioned generation, and structural infilling for materials discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_type": "decoder-only transformer (autoregressive language model)",
            "model_size": "7B, 13B, 70B (reported variants)",
            "training_data": "Fine-tuned on Materials Project-derived datasets: MP-20 (45,231 stable materials) for unconditional generation and an extended Materials Project set (~127,609 crystals after filtering) for conditional tasks; inputs were string-formatted crystal tuples (lattice, angles, element identities, fractional coordinates) with translation augmentations and stochastic prompts.",
            "application_domain": "Materials science — generation/design of inorganic crystalline materials (stable/metastable crystals)",
            "generation_method": "Parameter-efficient fine-tuning using LoRA (low-rank adapters) on pretrained LLaMA-2, multitask curriculum (two-thirds unconditional/text-conditional generation, one-third infilling), prompt engineering (task-specific instructions), and numeric tokenization (digit-level) with sampling controlled by temperature and nucleus (top-p).",
            "output_representation": "Text-formatted crystal strings (fixed-precision numeric fields) parsed into CIF; fractional coordinates, lattice lengths/angles and element tokens encoded as text tokens.",
            "evaluation_metrics": "Structural validity (no atomic overlaps), compositional validity (net neutral charge), coverage/recall/precision vs test set, diversity and novelty (Matminer featurizations), Increase in Perplexity under Transformation (IPT) for translation invariance, predicted stability via M3GNet ML potential (energy above hull ĤE_hull &lt; 0.1 eV/atom for metastability) and gold-standard DFT (VASP) relaxations (used as final check), sampling speed (time per 10k samples).",
            "benchmarks_or_datasets": "MP-20 (Materials Project subset), extended Materials Project dataset (April 2023 snapshot, filtered to ≤30 atoms/unit cell); compared against CDVAE (Crystal Diffusion VAE) baseline and language-model-from-scratch baselines (LM-CH, LM-AC).",
            "results_summary": "Fine-tuned LLaMA-2 models produced very high physical-validity rates (structural validity up to ~0.996 and compositional validity ~0.954 for best sampling hyperparameters). Best model (LLaMA-2 70B, nucleus r=0.7 sampling setting) produced ~49.8% of generated samples predicted metastable by M3GNet (ĤE_hull &lt; 0.1 eV/atom) and ~10.6% confirmed stable by DFT (VASP) among M3GNet-predicted metastable samples. By comparison, CDVAE reported ~28.8% metastable by M3GNet and ~5.4% by DFT. Larger LLaMA models learned better translation invariance (lower IPT) and yielded more diverse stable samples; smaller LLaMA variants sampled substantially faster.",
            "comparison_to_other_methods": "Compared to CDVAE (diffusion + VAE hybrid specifically designed for crystals), fine-tuned LLaMA-2 70B generated metastable materials at about twice the rate (49.8% vs 28.8% by M3GNet) and more DFT-confirmed stable materials (10.6% vs 5.4%). LLaMA models matched or exceeded diversity and novelty of CDVAE in many settings and were competitive in sampling throughput (especially for 13B). Unlike diffusion models that impose architectural invariances, the LLaMA approach uses text augmentations to encourage invariance and leverages generic pretrained biases.",
            "limitations_or_challenges": "Hallucination of nonstandard/unphysical element tokens (e.g., 'Ln') unless constrained; sensitivity to prompt phrasing and numeric tokenization choices; trade-offs between stability/validity and coverage controlled by sampling hyperparameters (temperature, top-p); reliance on M3GNet surrogate for large-scale stability screening (surrogate imperfect; DFT used only for M3GNet-predicted metastable subset); expensive to train the largest model (70B) despite LoRA; conditional generation (e.g., precise space group or stability conditioning) remains challenging (space group conditioning ~24% success noted), and generating 'unstable' samples is hard because training data are mostly stable compounds.",
            "uuid": "e5291.0",
            "source_info": {
                "paper_title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Flam-Shepherd & Aspuru-Guzik 2023",
            "name_full": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files",
            "brief_description": "Prior work (referenced) used language models trained from scratch on discrete token sequences (including digit-level tokens for coordinates) to directly generate 3D molecular, materials, and protein structures as text files (xyz, CIF, PDB), demonstrating that standard language-model architectures can model both discrete identities and continuous coordinates when everything is tokenized.",
            "citation_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files",
            "mention_or_use": "mention",
            "model_name": "Language models trained from scratch (unspecified architectures in this paper; autoregressive LMs implied)",
            "model_type": "autoregressive language model (trained from scratch on domain tokenization)",
            "model_size": "various (paper references training LMs from scratch; specific sizes not detailed in this citing paper)",
            "training_data": "Molecular and materials datasets encoded as token sequences where every component including numeric digits of coordinates are tokenized; prior work used standard molecular datasets (not exhaustively enumerated here).",
            "application_domain": "Molecular generation, materials, and protein binding site design (3D structure generation)",
            "generation_method": "Train language models from scratch on tokenized representations of coordinates and identities; unconditional/conditional sampling via autoregressive next-token prediction.",
            "output_representation": "Discrete token sequences representing 3D coordinates and element/atom identities, output as xyz, CIF, or PDB formatted text files.",
            "evaluation_metrics": "Validity and high-level dataset statistics (elemental composition validity, structural plausibility); comparisons to domain-specific models (e.g., CDVAE) for validity metrics.",
            "benchmarks_or_datasets": "Common molecular and materials datasets used in generative modeling (not exhaustively listed in this paper), comparisons made to CDVAE in prior work.",
            "results_summary": "Referenced work reported that language models trained from scratch can outperform domain-specific generative models (including CDVAE) on metrics like valid element compositions and certain high-level statistics of the training data, demonstrating the feasibility of tokenizing coordinates and using standard LMs for 3D structure generation.",
            "comparison_to_other_methods": "Found to outperform CDVAE on validity and some distributional statistics in the cited work; approach differs by treating all data as discrete tokens instead of specialized continuous/diffusion models.",
            "limitations_or_challenges": "Training from scratch requires careful tokenization (e.g., of numbers) and potentially large datasets; paper notes differences in tokenization choices impact performance; such from-scratch training can be data- and compute-intensive compared to fine-tuning pretrained LLMs.",
            "uuid": "e5291.1",
            "source_info": {
                "paper_title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Antunes et al. 2023",
            "name_full": "Crystal structure generation with autoregressive large language modeling",
            "brief_description": "Referenced prior work that trains autoregressive language models from scratch on large corpora of CIF strings to generate crystal structures as discrete sequences, demonstrating an alternative LLM-based strategy for materials generation.",
            "citation_title": "Crystal structure generation with autoregressive large language modeling",
            "mention_or_use": "mention",
            "model_name": "Autoregressive language models trained from scratch (specific model family not detailed in this paper)",
            "model_type": "autoregressive transformer / language model",
            "model_size": "not specified in this citing paper (trained on 'millions of CIF strings' in the referenced work)",
            "training_data": "Millions of CIF-format crystal strings (domain-specific large corpus of crystal structures used to train from scratch).",
            "application_domain": "Materials science — crystal structure generation",
            "generation_method": "Train autoregressive LMs from scratch on CIF textual representations; sample sequences autoregressively to produce new CIFs.",
            "output_representation": "CIF text strings encoding lattice parameters, atomic positions, and element identities.",
            "evaluation_metrics": "Validity and distributional statistics relative to training data (as typical in generative modeling); specifics in the referenced paper but summarized here as showing feasibility of autoregressive LMs for CIF generation.",
            "benchmarks_or_datasets": "Large-scale CIF corpus (millions of examples) assembled by the referenced authors; compared conceptually against domain-specific generative methods.",
            "results_summary": "Referenced work demonstrated successful generation of crystal CIF strings with autoregressive LMs trained from scratch; highlighted that language-model approaches can be effective for crystal generation when large amounts of domain-specific text-formatted examples are available.",
            "comparison_to_other_methods": "Approach contrasts with diffusion- or graph-based specialized generative models; requires large-scale domain-specific training data to match performance of pretrained LLM fine-tuning approaches.",
            "limitations_or_challenges": "Needs millions of CIFs and training from scratch (data- and compute-intensive); saltation of numeric tokenization and handling of coordinate precision are important design choices; likely less sample-efficient than fine-tuning pretrained LLMs.",
            "uuid": "e5291.2",
            "source_info": {
                "paper_title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files",
            "rating": 2
        },
        {
            "paper_title": "Atom-by-atom protein generation and beyond with language models",
            "rating": 2
        },
        {
            "paper_title": "Crystal structure generation with autoregressive large language modeling",
            "rating": 2
        },
        {
            "paper_title": "Crystal diffusion variational autoencoder for periodic material generation",
            "rating": 1
        }
    ],
    "cost": 0.014095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Fine-Tuned Language Models Generate Stable InORGANIC Materials as Text</h1>
<p>Nate Gruver ${ }^{1}$ Anuroop Sriram ${ }^{2}$ Andrea Madotto ${ }^{2}$<br>Andrew Gordon Wilson ${ }^{1}$ C. Lawrence Zitnick ${ }^{2}$ Zachary Ulissi ${ }^{2}$<br>${ }^{1}$ NYU ${ }^{2}$ Meta FAIR</p>
<h4>Abstract</h4>
<p>We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around $90 \%$ of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate ( $49 \%$ vs $28 \%$ ) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) are trained to compress large text datasets, but can also act as strong foundations for non-text data (Delétang et al., 2023). As compressors, LLMs extract common patterns and find simple programs that can produce them (Goldblum et al., 2023; Sutskever, 2023), regardless of the data's origin. From text pretraining alone, LLMs can compress or extrapolate data as diverse as images (Delétang et al., 2023), tabular data (Goldblum et al., 2023), time series (Gruver et al., 2023a), or robotic trajectories (Mirchandani et al., 2023). Alongside generality, LLM pre-training also gives rise to sample efficiency, as in-context learning and fine-tuning require far fewer training examples to identify salient patterns than training a model from scratch (Brown et al., 2020).</p>
<p>The generality and sample efficiency of LLMs make them particular promising for scientific problems, where data are often limited, collected from diverse sources, or challenging for non-experts to interpret. In materials science, for example, the number of known stable materials is relatively small, and the data describing each material are diverse, including composition, structure, and complex properties. LLMs can learn generalizable rules from a small number of examples (Zhu et al., 2023), combine modalities into a single model (Moon et al., 2023), and provide users with a text-based interface. A text interface, in particular, has the potential to improve access to scientific discovery (White, 2023); LLMs can use text to describe new observations, or, in design applications (e.g. materials design, drug discovery), LLMs can ingest text that specifies desired properties or constraints (Bran et al., 2023).</p>
<p>In this work, we show that fine-tuned LLMs can generate the three-dimensional structure of stable crystals as text (Figure 1). Our method is simple: first, encode crystals as new-line separated strings and combine with text instructions, then perform parameter efficient fine tuning (PEFT) on a base LLM (LLaMA-2) with a multitask curriculum and translation augmentations (Section 4). We evaluate our method with Materials Project data (Jain et al., 2013), comparing against an invariant diffusion model and a sequence model trained from scratch. Using both learned ML potentials and goldstandard DFT calculations, we show that our method can generate materials predicted to be stable at higher rates than baseline methods. To understand the success of our fine-tuning approach, we probe the learned symmetry properties of our model, proposing a new metric for language models trained on atomistic data and examining the effect of model scale on learned invariance. Going beyond</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our approach to materials generation with large language models. Using string formatted crystals and task-specific prompting, we enable unconditional stable materials generation, text-condition materials generation, and structural infilling. Base LLaMA-2 models are fine-tuned on a database of known inorganic materials (Liu et al., 2020) using low-rank adapters.
unconditional generation, we also show that our LLMs have other useful abilities within materials design, such as text-conditional generation and infilling, which can be used to optimize the properties of existing materials. ${ }^{1}$</p>
<h1>2 Related Work</h1>
<p>There are two central challenges in applying generative models to crystals and related atomistic data. The first challenge is that atoms are intrinsically both discrete and continuous objects, as each atom has both an element identity and a position in three dimensional space. Approaches to generative modeling often differ between for discrete and continuous data, and modeling both simultaneously can be significantly more complex than modeling either individually. The second key challenge is the prevalence of symmetries in atomistic data. The unit cell, a repeated pattern tiled infinitely in every direction, is the common representation for crystals because it easily captures translation invariance, the fact that atoms can be shifted and wrapped around the unit cell while still representing the same underlying structure. Symmetries can pose challenges to deep learning models because they entail constraints on the functions that neural networks can learn.</p>
<p>Diffusion models Xie et al. (2021) introduced crystal diffusion variational autoencoder (CDVAE) to directly deal with both of these challenges. CDVAE uses several individual generative models for discrete and continuous components that share a continuous (VAE) latent space. The chemical composition is reconstructed from this latent space using a language modeling head, while atom positions are generated with a denoising diffusion model (Ho et al., 2020). Since CDVAE, several works have extended diffusion processes to capture all parameters of the crystal, not just the atomic coordinates. Both Jiao et al. (2023) and Zeni et al. (2023) accomplish this by creating diffusions for the lattice parameters and atom identities, while Yang et al. (2023) design a new continuous representation that unifies atom identities and positions in a single high-dimensional tensor. In most cases, these diffusion models were designed with a careful eye towards symmetries and are built on top of graph neural networks with strict invariance/equivariance properties (Xie et al., 2021; Jiao et al., 2023; Zeni et al., 2023). The approach of Yang et al. (2023) is more similar to ours, as they apply a general-purpose architecture (3D U-net) and modeling approach (Gaussian diffusion) to a new representation, without guaranteeing symmetries. Discrete atom identities and variable length (number of atoms), however, require special considerations in diffusion models, unlike standard language models, which were originally designed for modeling discrete sequences.</p>
<p>Language models Flam-Shepherd \&amp; Aspuru-Guzik (2023) demonstrate an alternative to continuous denoising models and architectural invariances. Instead of treating discrete and continuous modalities separately, as in CDVAE, Flam-Shepherd \&amp; Aspuru-Guzik (2023) uses sequences of discrete tokens to represent everything, including the digits of atomic coordinates. With all data encoded as tokens, standard language modeling methods designed for text can be applied with little to no modification. The simplicity of this method also makes it simple to adapt to many different kinds of molecular</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>structures, including small molecules, protein binding pockets, and, of course, crystals. In lieu of architectural symmetries, augmentations of the training data are used to encourage learning known invariances. Flam-Shepherd \&amp; Aspuru-Guzik (2023) demonstrates that language models trained from scratch on many common molecular datasets actually outperform popular domain-specific models, including CDVAE, in their ability to capture valid element compositions and high-level statistics of the training data. Similarly, Antunes et al. (2023) also use language models to generate crystal structures as discrete sequences by training from scratch on millions of CIF strings.</p>
<p>Our work In this work, we show that pretrained LLMs are also useful for understanding and generating 3-dimensional atomic structures. By using a pre-trained LLM, we can achieve high rates of validity without crystal-specific tokenization (Flam-Shepherd \&amp; Aspuru-Guzik, 2023) or millions of auxiliary structures (Antunes et al., 2023). Unlike many methods designed specifically for crystal structures and symmetries, our method can also be easily extended to multiple crystal generation tasks and, in the future, to other atomistic modalities without any changes to the underlying model or training procedure. Building on the basic observations made by Flam-Shepherd \&amp; Aspuru-Guzik (2023), we show that larger models, which are often more effective compressors of data, demonstrate improved ability to learn symmetries from the training data and augmentation.</p>
<h1>3 BACKGROUND</h1>
<p>Language Modeling LLMs perform next-token prediction over sequences. The model is a categorical distribution, $p\left(w_{t+1} \mid w_{0: t}\right)$, where $w_{0: t}$ is the prompt, a sequence of input tokens, and $w_{t+1}$ is the predicted next token. To generate sequences from the model, the conditional distribution is sampled sequentially, but samples are rarely drawn from the original, unmodified categorical distributions. Instead the sampling procedure is typically modulated with temperature ( $\tau$ ) and nucleus size $(p)$ hyperparameters. Temperature serves to flatten the conditional distributions to uniform (high temperature) or collapse them around their maximal probabilities (low temperature). Nucleus size limits which tokens can be sampled based on the cumulative distribution function, clipping out values that contribute very little mass. A nucleus of $p(0&lt;p \leq 1)$ corresponds to keeping tokens to cumulatively contribute $p \%$ of the total probability, and discarding the rest.</p>
<p>Tokenization To train language models on text datasets, strings are converted into sequences of tokens. Most modern LLMs rely on byte pair encoding (BPE) (Gage, 1994), a compression method that assigns tokens to common substrings, making overall sequence lengths shorter. One downside of BPE tokenization is the default tokenization of numbers. BPE typically breaks numbers into irregular substrings instead of individual digits. While breaking numbers into multi-digit tokens creates shorter sequences, it also complicates learning basic arithmetic operations, which typically operate at the level of individual digits. Luckily, Touvron et al. (2023b) introduce tokenizers for LLaMA-2 models that break numbers into a sequence of digits, which has been shown to dramatically improve performance on arithmetic tasks (Liu \&amp; Low, 2023). We use LLaMA models in our work because they have a natural representation of 3D coordinates and can therefore learn simple functions over those coordinates that obey domain-specific symmetries (Section 5).</p>
<p>Crystal structures and energy prediction Periodic materials are defined by a unit cell repeated infinitely along all three dimensions (Figure 2). The unit cell comprises a lattice (parallelepiped) with side lengths $\left(l_{1}, l_{2}, l_{3}\right)$ and angles $\left(\theta_{1}, \theta_{2}, \theta_{3}\right)$. Within the lattice, there are $N$ atoms, each specified by an element identity, $e_{i}$, and set of 3 d coordinates $\left(x_{i}, y_{i}, z_{i}\right)$ which can be absolute or fractional (specified as a percentage of the unit cell side lengths). Therefore a bulk material can be fully described by the tuple</p>
<p>$$
C=\left(l_{1}, l_{2}, l_{3}, \theta_{1}, \theta_{2}, \theta_{3}, e_{1}, x_{1}, y_{1}, z_{1}, \ldots, e_{N}, x_{N}, y_{N}, z_{N}\right)
$$</p>
<p>For a given set of environmental conditions, every crystal has a corresponding energy that describes how likely it will occur in a particular configuration. Configuration with unfavorable electrostatic interactions from unlike atomic positions, such as highly overlapping atoms, are typically high energy. The gold standard for energy prediction is density functional theory (DFT), which provides tractable approximations to governing quantum mechanical equations that describe the energy and time evolution of a system. DFT, however, can be prohibitively expensive, often scaling $O\left(n^{2}\right)$ with the system size, which has motivated development of deep learning potentials to approximate DFT solutions (Lan et al., 2022).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (left) We convert the crystal lattice, atom identities, and atom positions into strings. The model is trained to generate a structure conditioned on the text prompt, which might contain additional information about the composition, properties, or a starting structure to modify. (right) Energy above hull ($E_{\text{hull}}$) quantifies the stability of a material. A crystal with $E_{\text{hull}}&lt;0.1$ will be energetically favorable both in its structure and composition.</p>
<h3>Stability of hypothetical materials ($E_{hull}$)</h3>
<p>The composition of a crystal also impacts its energy, as different elements have different geometries and charge properties. Certain stoichiometries, or ratios of elements, are naturally favored, and a composition of elements $A$ and $B$ with constituent parts $A_{x}B_{y}$ can dissociate into the composition $A_{c}B_{d}$ if it is energetically favorable. Because of the effect of composition, the energy of a crystal is typically a two-dimensional concept captured by the energy hull, which is the minimum observed configuration energy for a given composition. For a crystal to be low-energy and stable, and therefore give rise to a practically useful material, it must have a small <em>energy above hull</em> ($E_{hull}$), the distance from the energy hull for the crystal's elemental composition (Figure 2). Crystals with $E_{\text{hull}}&lt;0$ are considered stable and by definition have lower energy than the known minimum (which has $E_{\text{hull}}=0$). Crystals with $E_{\text{hull}}&lt;0.1$ eV/atom are often <em>metastable</em> and likely to be practical useful (Sun et al., 2016).</p>
<h1>4 METHOD</h1>
<p>Our approach to generating stable materials is pleasingly simple. We take a pre-trained LLM, which has useful biases towards generalizable patterns, and fine-tune it on crystal string representations. Because language models can also ingest text, we can condition the model's generations on text descriptions. The flexibility of language models also allows us to solve other tasks, such as infilling, through small modifications to the input formatting. Though we focus solely on crystal structures in this work, our method itself is general purpose and could be easily extended to proteins, nucleic acids, or small molecules. We include a more detailed discussion of how general text-pretraining impacts our method in Appendix A.5.</p>
<h3>String formatting and tokenization</h3>
<p>We convert the crystal tuple $C$ (Equation 1) using fixed precision numbers. An example of crystal string formatting is shown in Figure 2. We represent lattice lengths with one decimal place (2-3 digits) and lattice angles as integers (1-3 digits). Fractional coordinates are always represented with two digits. 3D coordinates are combined with spaces and all other crystal components are combined with newlines. We deliberately chose LLaMA-2 models because they are both state-of-the-art in overall performance among open-source models and because they tokenize numbers as individual digits by default. Notably, it is therefore impossible to create one token per full number, as <em>Flam-Shepherd &amp; Aspuru-Guzik (2023)</em> do in their best-performing model (further discussion in Appendix A.1). Instead, we rely on the extensive pretraining of LLaMA-2 models to instill useful biases over numerical operations (Liu &amp; Low, 2023).</p>
<h3>Prompt design</h3>
<p>To train a model that can be used for many tasks, including unconditional generation, text-conditional generation, and infilling, we use task-specific prompts. The input to the model is a prompt followed by the string-formatted crystal (Figure 2). In the most basic case, the prompt indicates that the model should generate bulk materials represented as a lattice and atoms. The prompt can also be expanded to include a desired composition or material properties, or to include a starting</p>
<p>structure, in the case of infilling. For infilling, the prompt includes the string-formatted crystal with every instance of a randomly chosen element replaced with [MASK], and the model is trained to generate the identity of the masked element at the end of the sequence. During training all three tasks are included through random sampling, with two thirds generation and one third infilling (details in Appendix A.2). As in instruction tuning, the prompt is given as input to the model but does not contribute to the generative loss function. The model is only penalized for its predictions on the crystal string or masked element.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Generation Prompt</th>
<th style="text-align: left;">Infill Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$&lt;\mathrm{s}&gt;$ Below is a description of a bulk <br> material. [The chemical formula is <br> Pm2ZnRh]. Generate a description of <br> the lengths and angles of the lattice <br> vectors and then the element type and <br> coordinates for each atom within the <br> lattice:</td>
<td style="text-align: left;">$&lt;\mathrm{s}&gt;$ Below is a partial description of a bulk material where <br> one element has been replaced with the string "[MASK]":</td>
</tr>
<tr>
<td style="text-align: left;">[ Crystal string ]</s></td>
<td style="text-align: left;">[ Crystal string with [MASK]s ]</td>
</tr>
</tbody>
</table>
<p>Generate an element that could replace [MASK] in the bulk material:
[ Masked element ]</s></p>
<p>Blue text is optional and included to enable conditional generation. Purple text stands in for string encodings of atoms.</p>
<p>Augmentations Crystals structures are symmetric under translational. All atomic coordinates can be shifted modulo the lattice boundaries without changing the resulting material structure. Similarly, the ordering of atoms within the lattice is irrelevant to the underlying material (permutation invariance). Prior work on diffusion generative models guarantee these symmetries as invariance or equivariance constraints on the model architecture (Xie et al., 2021; Jiao et al., 2023). To encourage translation invariance in our language models, we apply random uniform translations to the fractional coordinates. We chose not to augment the ordering of atoms because these variables often contained valuable information, for example grouping set of elements together for placement in the lattice (discussion in Appendix A.1).</p>
<h1>5 EXPERIMENTS</h1>
<p>We explore several uses of language models in crystal generative modeling. First, in order to compare with prior work, we show that fine-tuned LLMs can be used for unconditional generation of novel materials and that the resulting materials correspond to stable relaxed structures under the predictions of an ML potential and DFT. We then show that LLMs can also be used for text-conditional generation and to propose small changes to existing materials.</p>
<p>Datasets and models For consistency with prior work (Xie et al., 2021; Flam-Shepherd et al., 2023) we used MP-20 (Jain et al., 2013), a dataset of 45231 materials, when training for unconditional generation. All structures in MP-20 are stable, and therefore an effective generative model trained on MP-20 should tend to propose new crystals that are at least metastable. For text-conditioned generation, we train with all forms of prompting (Section 4) on a collection of 120,000 crystals from Materials Project (Appendix A.3). The collection includes basic property information, such as the space group number, band gap, $E_{\text {ball }}$ and the chemical formula. All of our experiments were conducted with LLaMA-2 models (7B 13B, and 70B) (Touvron et al., 2023a;b) through the Transformers library (Wolf et al., 2020) and PyTorch (Paszke et al., 2019). In order to train on small number of GPUs we use 4-bit quantization (Dettmers et al., 2022) and Low-Rank Adapters (LoRA) (Hu et al., 2021). We provide the full hyperparameters and training details in Appendix A.4.</p>
<p>Evaluation For basic evaluation of the LLM samples, we use the validity and diversity metrics introduced by Xie et al. (2021). Structural validity is determined by non-overlapping atomic radii (overlapping taken to be both atoms within half a radius of each other), while compositional validity captures the net charge of the structure (only structures with net neutral total charge are valid). Diversity is computed as pairwise distance between samples under featurizations of the structure and composition from Matminer (Ward et al., 2018; Xie et al., 2021).</p>
<p>Table 1: Following prior work <em>(Xie et al., 2021)</em>, we evaluate fine-tuned LLaMA-2 models using validity, which captures physical constraints, as well as coverage and property metrics, which capture alignment between the ground truth and sampling distribution. We add stability checks, which count the percentage of samples estimated to be stable by M3GNet <em>(Chen &amp; Ong, 2022)</em> and DFT <em>(Hafner, 2008)</em> (details in Appendix B.2). LLaMA models generate a high percentage of both valid and stable materials.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Validity Check</th>
<th></th>
<th>Coverage</th>
<th></th>
<th>Property Distribution</th>
<th></th>
<th>Metastable</th>
<th>Stable</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Structural$\uparrow$</td>
<td>Composition$\uparrow$</td>
<td>Recall$\uparrow$</td>
<td>Precision$\uparrow$</td>
<td>$\operatorname{wdist}(\rho)_{\downarrow}$</td>
<td>$\operatorname{wdist}(N_{cl})_{\downarrow}$</td>
<td>M3GNet $\uparrow$</td>
<td>DFT $\uparrow$</td>
</tr>
<tr>
<td>CDVAE</td>
<td>1.00</td>
<td>0.867</td>
<td>0.991</td>
<td>0.995</td>
<td>0.688</td>
<td>1.43</td>
<td>28.8%</td>
<td>5.4%</td>
</tr>
<tr>
<td>LM-CH</td>
<td>0.848</td>
<td>0.835</td>
<td>0.9925</td>
<td>0.9789</td>
<td>0.864</td>
<td>0.13</td>
<td>n/a</td>
<td>n/a</td>
</tr>
<tr>
<td>LM-AC</td>
<td>0.958</td>
<td>0.889</td>
<td>0.996</td>
<td>0.9855</td>
<td>0.696</td>
<td>0.09</td>
<td>n/a</td>
<td>n/a</td>
</tr>
<tr>
<td>LLaMA-2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>7B $(r=1.0)$</td>
<td>0.918</td>
<td>0.879</td>
<td>0.969</td>
<td>0.960</td>
<td>3.85</td>
<td>0.96</td>
<td>35.1%</td>
<td>6.7%</td>
</tr>
<tr>
<td>7B $(r=0.7)$</td>
<td>0.964</td>
<td>0.933</td>
<td>0.911</td>
<td>0.949</td>
<td>3.61</td>
<td>1.06</td>
<td>35.0%</td>
<td>6.2%</td>
</tr>
<tr>
<td>13B $(r=1.0)$</td>
<td>0.933</td>
<td>0.900</td>
<td>0.946</td>
<td>0.988</td>
<td>2.20</td>
<td>0.05</td>
<td>33.4%</td>
<td>8.7%</td>
</tr>
<tr>
<td>13B $(r=0.7)$</td>
<td>0.955</td>
<td>0.924</td>
<td>0.889</td>
<td>0.979</td>
<td>2.13</td>
<td>0.10</td>
<td>38.0%</td>
<td>14.4%</td>
</tr>
<tr>
<td>70B $(r=1.0)$</td>
<td>0.965</td>
<td>0.863</td>
<td>0.968</td>
<td>0.983</td>
<td>1.72</td>
<td>0.55</td>
<td>35.4%</td>
<td>10.0%</td>
</tr>
<tr>
<td>70B $(r=0.7)$</td>
<td>0.996</td>
<td>0.954</td>
<td>0.858</td>
<td>0.989</td>
<td>0.81</td>
<td>0.44</td>
<td>49.8%</td>
<td>10.6%</td>
</tr>
</tbody>
</table>
<p>While useful for sanity checking models, simple validity metrics only reflect a subset of our real-world priorities in generating novel materials. Arguably the most important property that we hope to assess in samples is their predicted stability, which we can approximate by predicting the energy of relaxed structures. Using known materials and energy calculations from Materials Project we construct the ground truth energy convex hull and then calculate the approximate energy above hull, $\hat{E}_{\text{hull}}$. We chose two methods to estimate material stability:</p>
<ul>
<li>ML potential: M3GNet <em>(Chen &amp; Ong, 2022)</em> provides energy, force, and stress approximations for crystal unit cells. For each sample we first run a relaxation using force and stress approximations then use the energy of the final structure.</li>
<li>DFT: We run a relaxation using the Density Functional Theory code VASP <em>(Hafner, 2008)</em> with INCAR settings chosen by Pymatgen <em>(Ong et al., 2013)</em>. DFT is the more accurate, but also much more computationally intense, of the two options.</li>
</ul>
<p>In both cases, results are compatible with Materials Project values <em>(Jain et al., 2013)</em> (Appendix B.1). Because DFT is prohibitively expensive for many use cases (often hours per calculation), we only use it to double-check results obtained with ML potentials, and we only run VASP calculations on materials that have already been predicted as metastable by M3GNet ( $&lt;0.1$ eV/atom $\hat{E}_{\text{hull}}$ ). The use of a M3GNet surrogate model is not perfect as many structures in Figure 4 (right) have energies above the expected 0.1 eV/atom threshold, but the structures are largely close to the hull compared to the broader distribution of materials generated.</p>
<p>Unconditional generation We sample 10,000 structures from each finetuned LLaMA model, parsing a CIF from the generated string. We reject the sample and draw another if a CIF cannot be parsed from the sampled string, which guarantees all samples can be interpreted as crystals but does not guarantee validity of the resulting crystal. We show the validity and predicted stability <em>(Xie et al., 2021)</em> of the resulting structures in Table 1, which shows that LLMs can achieve near-perfect rates of structural and compositional validity. Hyper-parameters like temperature and nucleus size can be used to trade-off validity and stability of samples with their coverage (Appendix B.3). LLaMA-2 70B strikes an effective balance, generating high rates of stable materials with good coverage and diversity (Figure 4). By default, generation is completely unconstrained and therefore the model can hallucinates imaginary elements, for example "Ln," a common abbreviation for Lanthanide (Figure 3), but the problem can be easily avoided by constraining the tokens for element identities <em>(Wolf et al., 2020)</em>.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A sample with "hallucinated" element identities (Ln).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Stability of LLaMA samples compared to CDVAE (Xie et al., 2021). Fine-tuned LLaMA-2 70B generates a higher rate of metastable ( $\hat{E}<em _hull="{hull" _text="\text">{\text {hull }}&lt;0.1$ ) and stable materials than CDVAE, using estimates of $\hat{E}</em>$ from both M3GNet (Chen \&amp; Ong, 2022) and VASP (Hafner, 2008). Because of computational cost, we only run VASP on structures predicted to be stable by M3GNet. Stable materials generated by LLaMA are also more diverse (as quantified by Matminer featurization (Ward et al., 2018)) than stable samples from CDVAE. We include sampled stable structures, shown as $(2,2,2)$ supercells, which display a high-degree of regularity and understanding of three-dimensional space.}</p>
<p>Symmetry learning As crystal structures have translational symmetry, ideally our model's likelihood should be invariant to translations. We propose Increase in Perplexity under Transformation (IPT) as metric for assessing the invariance of language models to continuous group transformations. For a transformation group $G$ with group elements $g$ and group action $t$, we define IPT for an input $s$,</p>
<p>$$
\operatorname{IPT}(s)=\mathbb{E}<em g="g">{g \in G}\left[\operatorname{PPL}\left(t</em>(s)\right)\right]
$$}(s)\right)-\operatorname{PPL}\left(t_{g^{*}</p>
<p>where</p>
<p>$$
g^{<em>}=\arg \min \operatorname{PPL}\left(t_{g^{</em>}}(s)\right)
$$</p>
<p>and PPL is the perplexity of the sequence, the exponent of the length-normalized cross entropy loss, $\operatorname{PPL}(s)=2^{\operatorname{CE}(s) / n}$. In our case $G$ is the group of translation, where each $g$ is a distance to translate by, and $t_{g}$ is the mapping that decode the string, translates the coordinates (wrapping them around the boundary), and re-encodes the string. IPT captures the degree to which transformations change a language model's compression ability. Good understanding of group transformations and invariance in the data should lead to minimal change in the perplexity of a transformed sequence. We can approximate IPT by sampling many values of $g$ (e.g. 20), picking $g^{*}$ as the minimum among those values, and computing a sample mean. Figure 5 shows the mean IPT of 500 random crystals from the test set, for each of the three LLaMA model sizes. We include additional details about our IPT calculation in Appendix B.5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Translation invariance on test data and ability to generate stable materials increase in proportion. Larger models learn invariances from augmentations more effectively during training, likely as a result of their preference for abstract and compressible patterns.</p>
<p>Diversity, novelty, and sampling speed When using generative models to discover new stable materials, there are several properties beyond the rate of stability that are practically significant. Novel and diverse samples encourage sufficient exploration of unknown material space, and sampling speed dictates how expensive it is to search within that space. We compare these properties for LLaMA-2 models and CDVAE in Figure 6. To calculate diversity and novelty, we use the same featurizations as in Table 1, calculating pairwise distances for diversity and distance to the closest neighbor in the training set for novelty (details in Appendix B.6). All metrics are computed over crystals judged metastable by M3GNet, so that all novelty and diversity are relevant and not an artifact of invalid generations. LLaMA-2 samples match or exceed the diversity of CDVAE samples and also obtain high rates of novelty when we consider both composition and structure. Interestingly, larger LLaMA models display less novel structures but more novel compositions. It's worth noting, however, that both CDVAE and LLaMA-2 7B far exceed the structural novelty of a held out test set, while 13B and 70B are just slightly lower. To judge sampling speed, we calculate the time required for 10,000</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: We compare LLaMA-2 models with CDVAE in their ability to generate novel and diverse samples as well as their overall speed. (left) We calculate diversity and novelty using a featurization of structure and composition (as in Table 1). Diversity is calculated as pairwise distance in feature space, while novelty quantifies the percentage of inputs that are far from the training set (Appendix B.6). All metrics are calculated only for samples that were already judged to be metastable. LLaMA-2 models often generate more diverse samples than CDVAE, and achieve similar overall rates of novelty. Interestingly, structural novelty is lower in larger models, while compositional novelty is higher. (right) We compare the time required to generate 10,000 samples from each model. We run LLaMA-2 models with the largest feasible batch size on one A100 GPU (Appendix B.7). While the largest LLaMA model is computationally expensive, smaller language models are very fast, especially when we consider both sampling speed and rate of stability.
samples, using the largest possible batch size on one A100 GPU (Appendix B.7). In Figure 6, we compare the sampling speed with CDVAE and find that smaller models are often significantly faster when generating metastable samples.</p>
<p>Text-conditioned generation Extending our method to text-conditional generation is as simple as including additional information in the prompt, with a small amount of additional text (Figure 4). We explore conditioning on spacegroup number, composition, and $E_{\text {ball }}$, as these properties are easy to verify (at least approximately) in silico. We assess the model's ability to perform conditional generation by comparing the intended condition with labels obtained from an in-silico oracle for the constraint. For the chemical formula, we simply parse the composition from the generated CIF. For space group determination, we use pymatgen's SpacegroupAnalyzer with a precision of 0.2 angstroms (Ong et al., 2013). For stability, we use M3GNet to estimate $E_{\text {ball }}$ as before. Using the oracle's labels, we then compute the percentage of cases in which the condition was properly met (Figure 7). The model is able to generate a material with the correct composition the majority of the time but becomes less reliable as the number of atoms in the chemical formula increases. Space group conditioning is more challenging, as it requires precise control and understanding of 3D structure, but the observed $24 \%$ is impressive when considering the 230 possible space groups. Generating stable/unstable structures as a binary task is the most challenging, likely because the training dataset is predominantly stable compounds and stability is defined only in reference to existing compounds. Stability is most easily controlled by modulating sampling hyperparameters.</p>
<p>Infilling Existing Materials In many practical settings, sampling and filtering materials from scratch is unnecessary. Good starting materials are often known, and manufacturing processes are easier to adapt to related compositions than develop completely from scratch by making small edits to their composition-often referred to as template methods (Kirklin et al., 2015; Saal et al., 2013). To emulate a typical template method, we construct a lookup table that maps each element to elements that have a similar atom radius when in the same oxidation state (code in Appendix C). We choose an element uniformly at random and swap it with a random element chosen from the table. The resulting structure is then relaxed using M3GNet. To improve this strategy using our fine-tuned LLM, we used the infilling prompt (Section 4) to obtain a distribution over elements (modulated with temperature $\tau$ ) which we use instead of a uniform distribution over swaps. To evaluate our mutation procedure, we sample 3000 structures randomly from the test set and generate perform one mutation-relaxation step for each, using both uniform and language model-guided sampling. In Figure, 7 we show the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Text-conditional generation and infilling of existing structures with fine-tuned LLMs. (left) Including composition or property information (sampled from a hold-out set) in the text prompt leads to a high rate of samples with the desired composition/property (space group or stability). We bin stability as $\hat{E}<em _bull="{bull" _text="\text">{\text {bull }}&lt;0.1$ (metastable) and $\hat{E}</em>&gt;0.1$ (unstable) for simplicity. Complex formulas and space groups challenge the model, but the samples are correct at a rate that facilitates practical use. We also show the rate of samples that both satisfy the condition and are predicted to be metastable by M3GNet. (right) Using the infilling prompt we can select mutations to existing materials. LLaMA-2 70B proposes a distribution over elements, which we constrain using knowledge of atom radii and charge interactions. We sample mutations with temperature $\tau$ and relax the results structure with M3GNet. When we apply this mutation procedure, we obtain more stable materials per mutation, with negligible changes to the overall diversity of the stable materials.
percentage of stable compounds and diversity in the stable compounds for the uniform baseline and LLaMA-2 70B with different temperature values. LLaMA-2 70B proposes elements that lead to stable structures at a higher rate than the baseline template method without sacrificing diversity.}</p>
<h1>6 DISCUSSION</h1>
<p>By generating a high rate of plausible stable materials (verified by DFT), we have demonstrated LLMs can be state-of-the-art generative models for atomistic domains with direct application of parameter-efficient instruction tuning and minimal task-specific modeling choices. This approach to generative modeling opens the door to multitask capabilities within a single sampling paradigm and multimodal training on atoms and text (e.g. to extract knowledge from a large corpus of scientific papers). We also advocate for the use of evaluation metrics (e.g. $\hat{E}_{\text {bull }}$ ) for generative models that are more closely tied to the downstream task of generating stable or metastable materials. The space of all hypothetical materials is combinatorially large (consider all the ways to pack 20 arbitrary elements into a box), but only a small subset of materials will actually be stable or metastable. Models that can directly generate near-stable structures make all downstream tasks far easier, and increases the likelihood the generative models may be useful for day-to-day tasks in materials discovery.</p>
<p>Limitations Our method shares the limitations of the underlying generative models. LLMs can be sensitive to precise details of the chosen prompt and the tokenization strategies, particularly in how tokenization effects processing of numbers. Hallucination of unphysical chemical elements or structures has been observed, though fortunately is easy to check and filter. Text-conditioning has the potential to tap latent conceptual understanding in the underlying LLM, but training LLMs that successfully leverage scientific and chemistry literature is a major outstanding challenge. Lastly, training the largest of our LLMs can be prohibitively expensive for some computational budgets. Despite this, inference from all LLMs is often highly tractable when compared to baseline methods (Appendix B.7).</p>
<p>Future directions There is substantial room for improvement in conditional generation, which could be used to directly generate materials with desired properties. While we did not pursue alternative sampling strategies in depth, approaches like classifier-free guidance (Sanchez et al., 2023) or variants of PPLM (Dathathri et al., 2019) might be useful in combination with fine-tuned LLMs to improve conditional generation. These methods could also be combined with primitives from Bayesian optimization for sample-efficient and uncertainty-aware design (Stanton et al., 2022; Gruver et al., 2023b).</p>
<p>Acknowledgements. We thank NSF CAREER IIS-2145492, NSF HDR-2118310, and NSF CDS\&amp;EMSS 2134216 for support.</p>
<h1>REFERENCES</h1>
<p>Luis M Antunes, Keith T Butler, and Ricardo Grau-Crespo. Crystal structure generation with autoregressive large language modeling. arXiv preprint arXiv:2307.04340, 2023.</p>
<p>Stephen Balaban. NVIDIA A100 GPU Benchmarks for Deep Learning. Lambda Labs Blog, 2020. URL https://lambdalabs.com/blog/ nvidia-a100-gpu-deep-learning-benchmarks-and-architectural-overview.</p>
<p>Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Chi Chen and Shyue Ping Ong. A universal graph deep learning interatomic potential for the periodic table. Nature Computational Science, 2(11):718-728, 2022.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164, 2019.</p>
<p>Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023.</p>
<p>Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022.</p>
<p>Daniel Flam-Shepherd and Alán Aspuru-Guzik. Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. arXiv preprint arXiv:2305.05708, 2023.</p>
<p>Daniel Flam-Shepherd, Kevin Zhu, and Alán Aspuru-Guzik. Atom-by-atom protein generation and beyond with language models. arXiv preprint arXiv:2308.09482, 2023.</p>
<p>Philip Gage. A new algorithm for data compression. The C Users Journal archive, 12:23-38, 1994.
Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon Wilson. The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning. arXiv preprint arXiv:2304.05366, 2023.</p>
<p>Nate Gruver, Marc Finzi, Micah Goldblum, and Andrew Gordon Wilson. The lie derivative for measuring learned equivariance. arXiv preprint arXiv:2210.02984, 2022.</p>
<p>Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.</p>
<p>Nate Gruver, Samuel Stanton, Nathan C Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson. Protein design with guided discrete diffusion. arXiv preprint arXiv:2305.20009, 2023b.</p>
<p>Jürgen Hafner. Ab-initio simulations of materials using vasp: Density-functional theory and beyond. Journal of computational chemistry, 29(13):2044-2078, 2008.</p>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.</p>
<p>J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021.</p>
<p>Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: The materials project: A materials genome approach to accelerating materials innovation. APL materials, 1(1), 2013.</p>
<p>Rui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu, and Yang Liu. Crystal structure prediction by joint equivariant diffusion on lattices and fractional coordinates. In Workshop on"Machine Learning for Materials"ICLR 2023, 2023.</p>
<p>Scott Kirklin, James E Saal, Bryce Meredig, Alex Thompson, Jeff W Doak, Muratahan Aykol, Stephan Rühl, and Chris Wolverton. The open quantum materials database (oqmd): assessing the accuracy of dft formation energies. npj Computational Materials, 1(1):1-15, 2015.</p>
<p>Janice Lan, Aini Palizhati, Muhammed Shuaibi, Brandon M Wood, Brook Wander, Abhishek Das, Matt Uyttendaele, C Lawrence Zitnick, and Zachary W Ulissi. Adsorbml: Accelerating adsorption energy calculations with machine learning. arXiv preprint arXiv:2211.16486, 2022.</p>
<p>Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. arXiv preprint arXiv:2305.14201, 2023.</p>
<p>Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742, 2020.</p>
<p>Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023.</p>
<p>Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and scalable any-modality augmented language model. arXiv preprint arXiv:2309.16058, 2023.</p>
<p>Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L Chevrier, Kristin A Persson, and Gerbrand Ceder. Python materials genomics (pymatgen): A robust, open-source python library for materials analysis. Computational Materials Science, 68:314-319, 2013.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, 2019.</p>
<p>James E Saal, Scott Kirklin, Muratahan Aykol, Bryce Meredig, and Christopher Wolverton. Materials design and discovery with high-throughput density functional theory: the open quantum materials database (oqmd). Jom, 65:1501-1509, 2013.</p>
<p>Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman. Stay on topic with classifier-free guidance. arXiv preprint arXiv:2306.17806, 2023.</p>
<p>Philipp Schmid. LLaMA 2 on Amazon Sagemaker, a Benchmark. https://huggingface.co/ blog/llama-sagemaker-benchmark, 2023.</p>
<p>Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew Gordon Wilson. Accelerating bayesian optimization for biological sequence design with denoising autoencoders. In International Conference on Machine Learning, pp. 20459-20478. PMLR, 2022.</p>
<p>Wenhao Sun, Stephen T Dacek, Shyue Ping Ong, Geoffroy Hautier, Anubhav Jain, William D Richards, Anthony C Gamst, Kristin A Persson, and Gerbrand Ceder. The thermodynamic scale of inorganic crystalline metastability. Science advances, 2(11):e1600225, 2016.</p>
<p>Ilya Sutskever. An observation on generalization. Workshop on Large Language Models and Transformers, 2023. URL https://www.youtube.com/watch?v=AKMuA_TVz3A\&amp;ab_ channel=SimonsInstitute.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023b.</p>
<p>Logan Ward, Alexander Dunn, Alireza Faghaninia, Nils ER Zimmermann, Saurabh Bajaj, Qi Wang, Joseph Montoya, Jiming Chen, Kyle Bystrom, Maxwell Dylla, et al. Matminer: An open source toolkit for materials data mining. Computational Materials Science, 152:60-69, 2018.</p>
<p>Andrew D White. The future of chemistry is language. Nature Reviews Chemistry, pp. 1-2, 2023.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.</p>
<p>Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi Jaakkola. Crystal diffusion variational autoencoder for periodic material generation. arXiv preprint arXiv:2110.06197, 2021.</p>
<p>Mengjiao Yang, KwangHwan Cho, Amil Merchant, Pieter Abbeel, Dale Schuurmans, Igor Mordatch, and Ekin Dogus Cubuk. Scalable diffusion for materials generation. arXiv preprint arXiv:2311.09235, 2023.</p>
<p>Claudio Zeni, Robert Pinsler, Daniel Zügner, Andrew Fowler, Matthew Horton, Xiang Fu, Sasha Shysheya, Jonathan Crabbé, Lixin Sun, Jake Smith, et al. Mattergen: a generative model for inorganic materials design. arXiv preprint arXiv:2312.03687, 2023.</p>
<p>Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules. arXiv preprint arXiv:2310.07064, 2023.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A Training Details ..... 13
A. 1 Numerical Formatting ..... 13
A. 2 Training with Stochastic Prompts ..... 13
A. 3 Extended Materials Project Dataset ..... 14
A. 4 Training Hyperparameters and Details ..... 14
A. 5 Role of Text Pretraining ..... 14
B Model Evaluation ..... 15
B. 1 Evaluation with ML potentials and DFT ..... 15
B. 2 Stability Checks and Percentages ..... 15
B. 3 Trade-Offs in Sampling ..... 15
B. 4 "Hallucination" Examples ..... 16
B. 5 Increase in Perplexity under Transformation (IPT) ..... 18
B. 6 Diversity and Novelty Calculation ..... 18
B. 7 Sampling Speed ..... 18
C Template Method Baseline ..... 19</p>
<h2>A Training Details</h2>
<h2>A. 1 NUMERICAL FORMATTING</h2>
<p>Notably, our approach to tokenization is distinctly different from prior work on modeling atomic structures with language models. Instead of using a special vocabulary and training models from scratch, we use LLaMA-2's existing tokenizer. This choice allows us to easily process both encoded crystals and text data. In early experiments, we tried out many other approaches, including finetuning LLaMA-2 models with additional tokens specific to crystal data. These methods were more challenging to train and didn't lead to any improvements over using a shared tokenizer. We include a set of example training losses below:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Epoch 1</th>
<th style="text-align: center;">Epoch 2</th>
<th style="text-align: center;">Epoch 3</th>
<th style="text-align: center;">Epoch 4</th>
<th style="text-align: center;">Epoch 5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Special Crystal Tokens</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.588</td>
</tr>
<tr>
<td style="text-align: center;">Shared Tokenization</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.385</td>
</tr>
</tbody>
</table>
<p>There are many important decisions involved both in text formatting (e.g the choice of fractional or absolute coordinates) and augmentation of the input data (e.g. translation or permutation augmentations on coordinates). As a simple example, we provide average validity numbers (using low temperature sampling) from earlier experiments on LLaMA-2 7B models trained with different formatting styles</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Structural Validity</th>
<th style="text-align: center;">Compositional Validity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fractional coords</td>
<td style="text-align: center;">$91.4 \%$</td>
<td style="text-align: center;">$83.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Absolute coords</td>
<td style="text-align: center;">$90.8 \%$</td>
<td style="text-align: center;">$80.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">No permutations</td>
<td style="text-align: center;">$92.5 \%$</td>
<td style="text-align: center;">$82.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">With permutations</td>
<td style="text-align: center;">$89.2 \%$</td>
<td style="text-align: center;">$81.7 \%$</td>
</tr>
</tbody>
</table>
<h2>A. 2 Training with Stochastic Prompts</h2>
<p>In order to enable multi-task use of the fine-tuned LLMs, we train on a stochastically generated prompt. Two thirds of the time we provide the model with a generation task, in which the prompt</p>
<p>consists of a basic instruction to generate a bulk material as a lattice and atom positions. We randomly sample a set of properties from the available descriptors of a given crystal and add any chosen ones (if any) to the prompt, using a small amount of wrapper text. The remaining one third of the time, we provide use the sampled crystal to construct and infilling task. We choose on element randomly from the set of elements in the composition and we construct a prompt that contain the string encoding of the crystal with this element replaced with [MASK]. The model then generates the replaced element as text following the prompt.</p>
<h1>A. 3 Extended Materials Project DataSet</h1>
<p>To facilitate text-conditional generation, we extend the original CDVAE training dataset with materials from Materials Project (Jain et al., 2013) as of April 2023. We filter out crystal with more than 30 atoms in the unit cell, which slow down training with minimal benefit to model performance, leaving a training set that contains 127609 crystal structures. The original validation and test splits are left unchanged and all test/validation points are removed from the new training set.</p>
<h2>A. 4 Training Hyperparameters and Details</h2>
<p>We provide the training details per model:</p>
<ul>
<li>LLaMA-2 7B: Batch size of 256 for 65 epochs with a cosine annealed learning rate of 0.0005 . LoRA rank 8 and alpha 32 .</li>
<li>LLaMA-2 13B: Batch size of 256 for 44 epochs with a cosine annealed learning rate of 0.0005 . LoRA rank 8 and alpha 32 .</li>
<li>LLaMA-2 70B: Batch size of 32 for 21 epochs with a cosine annealed learning rate of 0.0005 . LoRA rank 8 and alpha 32 .</li>
</ul>
<p>Limitations around available compute lead to our use of differing batch sizes and total number of epochs for each model. Ideally, we would train all models with the largest batch sized used among all models and would train all models for the same number of epochs (the maximum used by any model). At the same time, we wanted to properly demonstrate the full potential of all model sizes and therefore chose to present results for the best model we were able to train at each model size.</p>
<h2>A. 5 Role of Text Pretraining</h2>
<p>Text pretraining is essential to our method for two reasons.</p>
<ol>
<li>It would be impractically expensive or computationally infeasible to train models with up to 70B parameters from scratch on our data. Using a pretrained model with LoRA (Hu et al., 2021) offers the benefits of model scale while maintaining tractability and limiting overfitting, as the actual number of trainable parameters can be relatively small.</li>
<li>Pretraining on text data yields a model that can be conditioned on text for free, and text conditioning opens up a huge new realm of exciting possibilities, like conditioning samples on desired properties. It would be challenging to achieve a similar result from scratch without significantly expanding the size of the dataset (to improve general text understanding) and without essentially training a general-purpose language model in the process.</li>
</ol>
<p>To better understand the first point, let's quickly review the exact details of the finetuning procedure. We are using low-rank adapters (LoRA), as opposed to end-to-end finetuning, and this means we are adding a small number of additional parameters to an existing, frozen model. The easiest way to see the difference between this approach and training a model from scratch-as in (Flam-Shepherd \&amp; Aspuru-Guzik, 2023)-is to compare the training loss over the first few epochs of training.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Epoch 1</th>
<th style="text-align: center;">Epoch 2</th>
<th style="text-align: center;">Epoch 3</th>
<th style="text-align: center;">Epoch 4</th>
<th style="text-align: center;">Epoch 5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-2 (from scratch)</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.740</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B (LoRA)</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.385</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-70B (LoRA)</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.296</td>
</tr>
</tbody>
</table>
<p>If we attempt to run LoRA finetuning with randomly initialized parameters for the LLaMA-2 7B model we observe an immediate and significant difference in the training losses:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">1 Iter</th>
<th style="text-align: center;">0.33 Epochs</th>
<th style="text-align: center;">0.66 Epochs</th>
<th style="text-align: center;">1 Epoch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">13.46</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained</td>
<td style="text-align: center;">1.57</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.39</td>
</tr>
</tbody>
</table>
<p>While LoRA finetuning is tractable because $99.95 \%$ of the model is frozen, finetuning a LLaMA-2 model end-to-end in half-precision would require at least 4 times as many GPUs, making it infeasible for all but a handful of researchers. When using LoRA, even though the base models are large the number of trainable parameters is very small. In fact, the LLamA-2 7B model has less trainable parameters than one of the baseline methods we compared (CDVAE) (Xie et al., 2021). The number of trainable parameters for each of our models and the baseline models is shown below:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Trainable parameters (millions)</th>
<th style="text-align: center;">Percentage of total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CDVAE</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LM-CH/AC</td>
<td style="text-align: center;">$1-100$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 7B</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">$0.05 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 13B</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">$0.05 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 70B</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">$0.05 \%$</td>
</tr>
</tbody>
</table>
<h1>B Model Evaluation</h1>
<h2>B. 1 Evaluation with ML Potentials and DFT</h2>
<p>Approximating $E_{\text {hull }}$ from the energies of known materials in Materials Project requires a consistent correction scheme. We touch on some of the details here.</p>
<p>M3GNet Importantly, M3GNet was trained on the total energy of VASP calculations in the Materials Project dataset, so the results were expected to be consistent with the correction schemes and absolute energies in Section 5.</p>
<p>VASP To be consistent with the Materials Project settings (e.g. the PBE functional, DFT/DFT+U as appropriate, consistent pseudopotentials, etc). We did a single relaxation for every candidate structure using the default parameters in MPRelaxSet (Ong et al., 2013). VASP relaxations were run using the GPU-accelerated VASP6 code.</p>
<p>In both situations, the total energies were corrected using the MP2020 compatibility scheme, which was important to maintain consistency when calculating formation energies, and allow the use of varying functionals (DFT/DFT+U) for different materials.</p>
<h2>B. 2 Stability Checks and Percentages</h2>
<p>To calculate the percentage of metastable compounds, we take all samples and remove samples that are invalid under the basic structure and composition checks. We then run relaxations with M3GNet and obtain the final relaxation energies. The final percentage takes into account both the rate of validity (used to perform the initial filtering), and the rate of compounds with $\hat{E}<em _hull="{hull" _text="\text">{\text {hull }}&lt;0.1$, as determined by the convex hull calculation using the M3GNet relaxation energy. To calculate the VASP percentage, we select materials determined to be metastable M3GNet and run VASP with default setting. We then report the percentage of the materials with $\hat{E}</em>&lt;0.0$.}</p>
<h2>B. 3 Trade-Offs in Sampling</h2>
<p>We note that modulating stability with sampling parameters like temperature and nucleus size has a significant effect on the coverage properties of the resulting samples. We illustrate the trade-offs between stability and coverage in Figure 8. Coverage most likely decreases because nucleus size and temperature collapse the distribution around samples with high likelihood, which are also more likely to be valid or stable. Notably, LLaMA-2 70B appears to demonstrate the best trade-offs, possibly</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Validity and rate of stability depend on sampling hyper-parameters. Lowering the temperature or restricting the nucleus size leads to significant improvements in validity/stability but incurs a cost to coverage of a held-out test set (recall). Fine-tuned LLaMA-2 70B displays the best trade-off between coverage and stability, generating materials that are both stable and diverse.
indicating a likelihood model that corresponds better to both the underlying properties of stability and the full, diverse distribution of structures.</p>
<h1>B. 4 "Hallucination" Examples</h1>
<h2>LLaMA-2 7B:</h2>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nx">generated</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">pymatgen</span>
<span class="nx">data_Met8</span><span class="p">(</span><span class="nx">Cu2N</span><span class="p">)</span><span class="mi">5</span>
<span class="nx">_symmetry_space_group_name_H</span><span class="o">-</span><span class="nx">M</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">P</span><span class="w"> </span><span class="mi">1</span><span class="err">&#39;</span>
<span class="nx">_cell_length_a</span><span class="w"> </span><span class="m m-Double">5.0000</span>
<span class="nx">_cell_length_b</span><span class="w"> </span><span class="m m-Double">5.0000</span>
<span class="nx">_cell_length_c</span><span class="w"> </span><span class="m m-Double">5.0000</span>
<span class="nx">_cell_angle_alpha</span><span class="w"> </span><span class="m m-Double">90.0000</span>
<span class="nx">_cell_angle_beta</span><span class="w"> </span><span class="m m-Double">90.0000</span>
<span class="nx">_cell_angle_gamma</span><span class="w"> </span><span class="m m-Double">90.0000</span>
<span class="nx">_symmetry_Int_Tables_number</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">_chemical_formula_structural</span><span class="w"> </span><span class="nx">Met8</span><span class="p">(</span><span class="nx">Cu2N</span><span class="p">)</span><span class="mi">5</span>
<span class="nx">_chemical_formula_sum</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">Met8</span><span class="w"> </span><span class="nx">Cu10</span><span class="w"> </span><span class="nx">N5</span><span class="err">&#39;</span>
<span class="nx">_cell_volume</span><span class="w"> </span><span class="m m-Double">125.0000</span>
<span class="nx">_cell_formula_units_Z</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">loop_</span>
<span class="nx">_symmetry_equiv_pos_site_id</span>
<span class="nx">_symmetry_equiv_pos_as_xyz</span>
<span class="w">    </span><span class="mi">1</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="p">,</span><span class="w"> </span><span class="nx">z</span><span class="err">&#39;</span>
<span class="nx">loop_</span>
<span class="nx">_atom_site_type_symbol</span>
<span class="nx">_atom_site_label</span>
<span class="nx">_atom_site_symmetry_multiplicity</span>
<span class="nx">_atom_site_fract_x</span>
<span class="nx">_atom_site_fract_y</span>
<span class="nx">_atom_site_fract_z</span>
<span class="nx">_atom_site_occupancy</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu0</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.8300</span><span class="w"> </span><span class="m m-Double">0.3900</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.8300</span><span class="w"> </span><span class="m m-Double">0.4900</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu2</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.8300</span><span class="w"> </span><span class="m m-Double">0.9900</span><span class="w"> </span><span class="m m-Double">0.5000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu3</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.6300</span><span class="w"> </span><span class="m m-Double">0.1900</span><span class="w"> </span><span class="m m-Double">0.2000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu4</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.2300</span><span class="w"> </span><span class="m m-Double">0.7900</span><span class="w"> </span><span class="m m-Double">0.2000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu5</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.6300</span><span class="w"> </span><span class="m m-Double">0.7000</span><span class="w"> </span><span class="m m-Double">0.3100</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu6</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.2300</span><span class="w"> </span><span class="m m-Double">0.1900</span><span class="w"> </span><span class="m m-Double">0.3000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu7</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.8900</span><span class="w"> </span><span class="m m-Double">0.7000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu8</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.3900</span><span class="w"> </span><span class="m m-Double">0.2000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Cu</span><span class="w"> </span><span class="nx">Cu9</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4900</span><span class="w"> </span><span class="m m-Double">0.8900</span><span class="w"> </span><span class="m m-Double">0.7000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met10</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.6300</span><span class="w"> </span><span class="m m-Double">0.6000</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met11</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4000</span><span class="w"> </span><span class="m m-Double">0.4700</span><span class="w"> </span><span class="m m-Double">0.4700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met12</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4000</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.9900</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met13</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.2200</span><span class="w"> </span><span class="m m-Double">0.9700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met14</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.6300</span><span class="w"> </span><span class="m m-Double">0.5000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met15</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.2300</span><span class="w"> </span><span class="m m-Double">0.2200</span><span class="w"> </span><span class="m m-Double">0.6000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met16</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.0000</span><span class="w"> </span><span class="m m-Double">0.6100</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Met0</span><span class="o">+</span><span class="w"> </span><span class="nx">Met17</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.6300</span><span class="w"> </span><span class="m m-Double">0.1000</span><span class="w"> </span><span class="m m-Double">0.5000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">N</span><span class="w"> </span><span class="nx">N18</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.1200</span><span class="w"> </span><span class="m m-Double">0.7000</span><span class="w"> </span><span class="m m-Double">0.8000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">N</span><span class="w"> </span><span class="nx">N19</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.2300</span><span class="w"> </span><span class="m m-Double">0.5900</span><span class="w"> </span><span class="m m-Double">0.2000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">N</span><span class="w"> </span><span class="nx">N20</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.2300</span><span class="w"> </span><span class="m m-Double">0.1900</span><span class="w"> </span><span class="m m-Double">0.7000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">N</span><span class="w"> </span><span class="nx">N21</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4900</span><span class="w"> </span><span class="m m-Double">0.2100</span><span class="w"> </span><span class="m m-Double">0.1000</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">N</span><span class="w"> </span><span class="nx">N22</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4800</span><span class="w"> </span><span class="m m-Double">0.6100</span><span class="w"> </span><span class="m m-Double">0.6000</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nx">data_L3Li</span>
<span class="nx">_symmetry_space_group_name_H</span><span class="o">-</span><span class="nx">M</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">P</span><span class="w"> </span><span class="mi">1</span><span class="err">&#39;</span>
<span class="nx">_cell_length_a</span><span class="w"> </span><span class="m m-Double">5.1000</span>
<span class="nx">_cell_length_b</span><span class="w"> </span><span class="m m-Double">7.1000</span>
<span class="nx">_cell_length_c</span><span class="w"> </span><span class="m m-Double">7.4000</span>
<span class="nx">_cell_angle_alpha</span><span class="w"> </span><span class="m m-Double">84.0000</span>
<span class="nx">_cell_angle_beta</span><span class="w"> </span><span class="m m-Double">68.0000</span>
<span class="nx">_cell_angle_gamma</span><span class="w"> </span><span class="m m-Double">68.0000</span>
<span class="nx">_symmetry_Int_Tables_number</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">_chemical_formula_structural</span><span class="w"> </span><span class="nx">L3Li</span>
<span class="nx">_chemical_formula_sum</span><span class="w"> </span><span class="nx">L12</span><span class="w"> </span><span class="nx">Li4</span>
<span class="nx">_cell_volume</span><span class="w"> </span><span class="m m-Double">230.15214369</span>
<span class="nx">_cell_formula_units_Z</span><span class="w"> </span><span class="mi">4</span>
<span class="nx">loop_</span>
<span class="nx">_symmetry_equiv_pos_site_id</span>
<span class="nx">_symmetry_equiv_pos_as_xyz</span>
<span class="w">    </span><span class="mi">1</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="p">,</span><span class="w"> </span><span class="nx">z</span><span class="err">&#39;</span>
<span class="nx">loop_</span>
<span class="nx">_atom_site_type_symbol</span>
<span class="nx">_atom_site_label</span>
<span class="nx">_atom_site_symmetry_multiplicity</span>
<span class="nx">_atom_site_fract_x</span>
<span class="nx">_atom_site_fract_y</span>
<span class="nx">_atom_site_fract_z</span>
<span class="nx">_atom_site_occupancy</span>
<span class="nx">Li</span><span class="w"> </span><span class="nx">Li0</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.7100</span><span class="w"> </span><span class="m m-Double">0.4000</span><span class="w"> </span><span class="m m-Double">0.8300</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Li</span><span class="w"> </span><span class="nx">Li1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.2200</span><span class="w"> </span><span class="m m-Double">0.3700</span><span class="w"> </span><span class="m m-Double">0.3600</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Li</span><span class="w"> </span><span class="nx">Li2</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.7100</span><span class="w"> </span><span class="m m-Double">0.8900</span><span class="w"> </span><span class="m m-Double">0.3300</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">Li</span><span class="w"> </span><span class="nx">Li3</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.2100</span><span class="w"> </span><span class="m m-Double">0.8700</span><span class="w"> </span><span class="m m-Double">0.8600</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L4</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.6300</span><span class="w"> </span><span class="m m-Double">0.6900</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L5</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.5100</span><span class="w"> </span><span class="m m-Double">0.1400</span><span class="w"> </span><span class="m m-Double">0.6600</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L6</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.9600</span><span class="w"> </span><span class="m m-Double">0.5700</span><span class="w"> </span><span class="m m-Double">0.1700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L7</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4700</span><span class="w"> </span><span class="m m-Double">0.0700</span><span class="w"> </span><span class="m m-Double">0.1700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L8</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.9800</span><span class="w"> </span><span class="m m-Double">0.6100</span><span class="w"> </span><span class="m m-Double">0.1400</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L9</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4900</span><span class="w"> </span><span class="m m-Double">0.1100</span><span class="w"> </span><span class="m m-Double">0.1400</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L10</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.1000</span><span class="w"> </span><span class="m m-Double">0.6800</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L11</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.1100</span><span class="w"> </span><span class="m m-Double">0.1500</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L12</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4700</span><span class="w"> </span><span class="m m-Double">0.5500</span><span class="w"> </span><span class="m m-Double">0.1800</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L13</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.5800</span><span class="w"> </span><span class="m m-Double">0.6800</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L14</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4700</span><span class="w"> </span><span class="m m-Double">0.0600</span><span class="w"> </span><span class="m m-Double">0.6700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">L0</span><span class="o">+</span><span class="w"> </span><span class="nx">L15</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.1300</span><span class="w"> </span><span class="m m-Double">0.1700</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>

<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<h1>LLaMA-2 70B:</h1>
<div class="codehilite"><pre><span></span><code><span class="nx">data_Ln3BO4</span>
<span class="nx">_symmetry_space_group_name_H</span><span class="o">-</span><span class="nx">M</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">P</span><span class="w"> </span><span class="mi">1</span><span class="err">&#39;</span>
<span class="nx">_cell_length_a</span><span class="w"> </span><span class="m m-Double">5.9000</span>
<span class="nx">_cell_length_b</span><span class="w"> </span><span class="m m-Double">5.9000</span>
<span class="nx">_cell_length_c</span><span class="w"> </span><span class="m m-Double">5.9000</span>
<span class="nx">_cell_angle_alpha</span><span class="w"> </span><span class="m m-Double">59.0000</span>
<span class="nx">_cell_angle_beta</span><span class="w"> </span><span class="m m-Double">59.0000</span>
<span class="nx">_cell_angle_gamma</span><span class="w"> </span><span class="m m-Double">59.0000</span>
<span class="nx">_symmetry_lnt_Tables_number</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">_chemical_formula_structural</span><span class="w"> </span><span class="nx">LaB7</span><span class="p">(</span><span class="nx">NO3</span><span class="p">)</span><span class="mi">2</span>
<span class="nx">_chemical_formula_sum</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">Le1</span><span class="w"> </span><span class="nx">B7</span><span class="w"> </span><span class="nx">N2</span><span class="w"> </span><span class="nx">O6</span><span class="err">&#39;</span>
<span class="nx">_cell_volume</span><span class="w"> </span><span class="m m-Double">141.91223582</span>
<span class="nx">_cell_formula_units_Z</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">loop_</span>
<span class="nx">_symmetry_equiv_pos_site_id</span>
<span class="nx">_symmetry_equiv_pos_as_xyz</span>
<span class="w">    </span><span class="mi">1</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="p">,</span><span class="w"> </span><span class="nx">z</span><span class="err">&#39;</span>
<span class="nx">loop_</span>
<span class="nx">_atom_site_type_symbol</span>
<span class="nx">_atom_site_label</span>
<span class="nx">_atom_site_symmetry_multiplicity</span>
<span class="nx">_atom_site_fract_x</span>
<span class="nx">_atom_site_fract_y</span>
<span class="nx">_atom_site_fract_z</span>
<span class="nx">_atom_site_occupancy</span>
<span class="nx">Le0</span><span class="o">*</span><span class="w"> </span><span class="nx">Le0</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.7100</span><span class="w"> </span><span class="m m-Double">0.5000</span><span class="w"> </span><span class="m m-Double">0.1700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">B</span><span class="w"> </span><span class="nx">B1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.3800</span><span class="w"> </span><span class="m m-Double">0.1600</span><span class="w"> </span><span class="m m-Double">0.0200</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">B</span><span class="w"> </span><span class="nx">B2</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4600</span><span class="w"> </span><span class="m m-Double">0.1600</span><span class="w"> </span><span class="m m-Double">0.5700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">B</span><span class="w"> </span><span class="nx">B3</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4600</span><span class="w"> </span><span class="m m-Double">0.7200</span><span class="w"> </span><span class="m m-Double">0.5700</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">B</span><span class="w"> </span><span class="nx">B4</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.0400</span><span class="w"> </span><span class="m m-Double">0.7900</span><span class="w"> </span><span class="m m-Double">0.6500</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">B</span><span class="w"> </span><span class="nx">B5</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">1.0000</span><span class="w"> </span><span class="m m-Double">0.2500</span><span class="w"> </span><span class="m m-Double">0.6500</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">B</span><span class="w"> </span><span class="nx">B6</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.0000</span><span class="w"> </span><span class="m m-Double">0.7900</span><span class="w"> </span><span class="m m-Double">0.0900</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">B</span><span class="w"> </span><span class="nx">B7</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.0000</span><span class="w"> </span><span class="m m-Double">0.1600</span><span class="w"> </span><span class="m m-Double">0.6500</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">N</span><span class="w"> </span><span class="nx">N8</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.6200</span><span class="w"> </span><span class="m m-Double">0.5700</span><span class="w"> </span><span class="m m-Double">0.9800</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">N</span><span class="w"> </span><span class="nx">N9</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.0600</span><span class="w"> </span><span class="m m-Double">0.3300</span><span class="w"> </span><span class="m m-Double">0.2500</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">O</span><span class="w"> </span><span class="nx">O10</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.5500</span><span class="w"> </span><span class="m m-Double">0.7600</span><span class="w"> </span><span class="m m-Double">0.7100</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">O</span><span class="w"> </span><span class="nx">O11</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.1800</span><span class="w"> </span><span class="m m-Double">0.5400</span><span class="w"> </span><span class="m m-Double">0.6100</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">O</span><span class="w"> </span><span class="nx">O12</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.4300</span><span class="w"> </span><span class="m m-Double">0.9500</span><span class="w"> </span><span class="m m-Double">0.5400</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">O</span><span class="w"> </span><span class="nx">O13</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.9400</span><span class="w"> </span><span class="m m-Double">0.1100</span><span class="w"> </span><span class="m m-Double">0.9600</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">O</span><span class="w"> </span><span class="nx">O14</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.6400</span><span class="w"> </span><span class="m m-Double">0.7700</span><span class="w"> </span><span class="m m-Double">0.2900</span><span class="w"> </span><span class="mi">1</span>
<span class="nx">O</span><span class="w"> </span><span class="nx">O15</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="m m-Double">0.3000</span><span class="w"> </span><span class="m m-Double">0.3800</span><span class="w"> </span><span class="m m-Double">0.1300</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>

<p>data_MandeGd2O4
<em>symmetry_space_group_name_H-M 'P 1'
_cell_length_a 3.6000
_cell_length_b 3.6000
_cell_length_c 5.9000
_cell_angle_alpha 90.0000
_cell_angle_beta 90.0000
_cell_angle_gamma 90.0000
_symmetry_lnt_Tables_number 1
_chemical_formula_structural Ln3BO4
_chemical_formula_sum 'Ln3 B1 O4'
_cell_volume 146.33178751
_cell_formula_units_Z 1
loop</em>
<em>symmetry_equiv_pos_site_id
_symmetry_equiv_pos_as_xyz
1 'x, y, z'
loop</em>
_atom_site_type_symbol
_atom_site_label
_atom_site_symmetry_multiplicity
_atom_site_fract_x
_atom_site_fract_y
_atom_site_fract_z
_atom_site_occupancy
La0<em> La0 1 0.1800 0.0600 0.9900 1
La0</em> La1 1 0.6800 0.5600 0.9900 1
La0* La2 1 0.1800 0.5600 0.4900 1
B B3 1 0.6800 0.0600 0.4900 1
O O4 1 0.6800 0.3300 0.1500 1
O O5 1 0.1800 0.2800 0.1800 1
O O6 1 0.6800 0.7800 0.8000 1
O O7 1 0.1800 0.8300 0.8500 1</p>
<p>data_MandeGd2O4
symmetry_space_group_name_H-M 'P 1'
<em>cell_length_a 3.6000
_cell_length_b 3.6000
_cell_length_c 5.9000
_cell_angle_alpha 90.0000
_cell_angle_beta 90.0000
_cell_angle_gamma 90.0000
_symmetry_lnt_Tables_number 1
_chemical_formula_structural MandeGd2O4
_chemical_formula_sum 'Mandel Gd2 O4'
_cell_volume 76.46400000
_cell_formula_units_Z 1
loop</em>
<em>symmetry_equiv_pos_site_id
_symmetry_equiv_pos_as_xyz
1 'x, y, z'
loop</em>
<em>atom_site_type_symbol
_atom_site_label
_atom_site_symmetry_multiplicity
_atom_site_fract_x
_atom_site_fract_y
_atom_site_fract_z
_atom_site_occupancy
Gd Gd0 1 0.8200 0.2300 0.1500 1
Gd Gd1 1 0.8200 0.2300 0.6300 1
Mandelv Mande2 1 0.3200 0.7300 0.8900 1
O O3 1 0.8200 0.7300 0.4100 1
O O4 1 0.3200 0.7300 0.1000 1
O O5 1 0.3200 0.2300 0.3900 1
O O6 1 0.8200 0.7300 0.7900 1
data_Gro15Nd4
symmetry_space_group_name_H-M 'P 1'
_cell_length_a 7.0000
_cell_length_b 7.0000
_cell_length_c 6.9000
_cell_angle_alpha 71.0000
_cell_angle_beta 71.0000
_cell_angle_gamma 69.0000
symmetry_lnt_Tables_number 1
_chemical_formula_structural Gro15Nd4
_chemical_formula_sum 'Gro15 Nd4'
_cell_volume 289.96945358
_cell_formula_units_Z 1
loop</em>
<em>symmetry_equiv_pos_site_id
_symmetry_equiv_pos_as_xyz
1 'x, y, z'
loop</em>
_atom_site_type_symbol
_atom_site_label
_atom_site_symmetry_multiplicity
_atom_site_fract_x
_atom_site_fract_y
_atom_site_fract_z
_atom_site_occupancy
Nd Nd0 1 0.5600 0.5700 0.7800 1
Nd Nd1 1 0.7500 0.7500 0.5600 1
Nd Nd2 1 0.1700 0.1700 0.1400 1
Nd Nd3 1 0.9500 0.9500 0.3800 1
Gro0<em> Gro4 1 0.7600 0.2300 0.3000 1
Gro0</em> Gro5 1 0.1200 0.4800 1.0000 1
Gro0<em> Gro6 1 0.3800 0.8700 0.1000 1
Gro0</em> Gro7 1 0.0300 0.6600 0.8400 1
Gro0<em> Gro8 1 0.6500 0.1700 0.6400 1
Gro0</em> Gro9 1 0.5600 0.0600 0.7400 1
Gro0<em> Gro10 1 0.9200 0.5000 0.1600 1
Gro0</em> Gro11 1 0.4900 0.7400 0.2200 1
Gro0<em> Gro12 1 0.2400 0.1000 0.5800 1
Gro0</em> Gro13 1 0.9100 0.2700 0.6200 1
Gro0<em> Gro14 1 0.4000 0.6100 0.4600 1
Gro0</em> Gro15 1 0.2900 0.2900 0.4200 1
Gro0<em> Gro16 1 0.4500 0.9200 0.9400 1
Gro0</em> Gro17 1 0.9900 0.1300 0.0200 1
Gro0* Gro18 1 0.8400 0.5100 0.8200 1</p>
<h1>B. 5 Increase in Perplexity under Transformation (IPT)</h1>
<p>Although there are existing metrics for invariance and equivariance in neural networks, language models pose unique challenges because of their discrete tokens, which do not change smoothly under continuous transformations. Though it might be possible to compute a meaningful analogue of the Lie derivative (Gruver et al., 2022), or similar metrics, through interpolation of word embeddings, we decide to adopt a simpler metric (IPT), which still highlights significant differences between base models. We calculate IPT for each model using 500 test datapoints and 20 randomly translation sampled as fraction coordinates from a uniform distribution per dimension. The translations themselves are implemented in PyMatgen and respect periodic boundary conditions (Ong et al., 2013). In order to combine the IPT values in a meaningful way across different datapoints, we normalize their values by the mean perplexity over transformations. Thus datapoints which happen to have large perplexity, and therefore naturally large potential changes in perplexity, do not drown out points with small perplexity.</p>
<h2>B. 6 Diversity and Novelty Calculation</h2>
<p>Following (Xie et al., 2021), we calculate diversity as the pairwise distance between samples using a featurization of structure and composition. To calculate novelty, we also featurize the training dataset and calculate the distance to the nearest element of the training set for each sample. A sample is considered novel if the nearest element in the training set is above a threshold. We use a structural distance cutoff of 0.1 and composition distance cutoff of 2 . In addition to novelty of structure and composition individual, we also consider the overall novelty of a crystal, where overall novelty is determined by having either a new structure or a new composition. All metrics are calculated on filtered samples that M3GNet qualifies as metastable. We report metrics on metastable samples because these numbers are more practically relevant and because the samples are more likely to contribute meaningful variation, instead of being different from the training set and each other simply because they are wildly invalid. We normalize all diversity and novelty values by corresponding value for the test set to provide a sense for the underlying data distribution.</p>
<h2>B. 7 SAMPLING SPEED</h2>
<p>Although LLMs might seem like computational overkill at face value, batching for large-scale sampling allows LLaMA models to have comparable computational overhead to competing approaches. Making exact comparisons between LLaMA models and CDVAE are slightly challenging because of available hardware and differences in compatibility. We ran experiments primarily on A100 GPUs, while the publicly available code for CDVAE cannot be run on an A100 and reports results on a RTX2080 Ti.</p>
<p>We provide two analyses for the sampling rate of LLaMA models, one from experiments we ran on a single A100 and alternative using third-party numbers for LLaMA models deployed on AWS instances.</p>
<p>Local analysis We obtain benchmark LLaMA-2 sampling times by running 5 batched generations and computingn the average time to completion. We then use these numbers to calculate the equivalent time to sample 10,000 structures. In practice, we used distributed sampling on a cluster, so reporting our direct times to compute 10,000 samples would be less informative. We use the maximum batch size that we can fit on an A100 GPU with each model without causing out-of-memory (OOM) errors during sampling. The batch sizes were {7B: 512, 13B: 256, 70B: 128}. To compare CDVAE with our results we perform a rough, but generous, conversion of their results to an A100 GPU. We multiply their rate of sampling by 16 , to account for the 2 x faster rate of operations (Balaban, 2020) and approximately 8 times larger GPU memory (allowing for large batch sizes and utilization rates). We report the intermediate numbers and calculations below. The final rates for metastable samples are shown in Figure 6.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Batch size</th>
<th style="text-align: center;">Seconds / batch</th>
<th style="text-align: center;">Samples / hour</th>
<th style="text-align: center;">Hours / 10,000 crystals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CDVAE</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">1.260</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 7B</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">27.18</td>
<td style="text-align: center;">67814</td>
<td style="text-align: center;">0.147</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 13B</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">38.24</td>
<td style="text-align: center;">24100</td>
<td style="text-align: center;">0.414</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 70B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">52.52</td>
<td style="text-align: center;">8774</td>
<td style="text-align: center;">1.139</td>
</tr>
</tbody>
</table>
<p>AWS analysis Considering AWS as the deployment environment, we can build on a recent benchmark on a cloud instance with 8 A100 GPUs (ml.p4d.12xlarge) (Schmid, 2023), which found that LLaMA-2 13B achieved $0.416 \mathrm{hr} / 1 \mathrm{M}$ tokens and LLaMA-2 70B achieved $0.864 \mathrm{hr} / 1 \mathrm{M}$ tokens. One crystal is around 100 tokens on average, so the throughput for 10,000 crystals is the same as for 1 M tokens. For comparison, we use CDVAE and its recorded runtimes for generating 10,000 crystals on a single RTX2080 Ti GPU (Xie et al., 2021). To obtain the final numbers, we adjust for the number of GPUs (8) and a 2x improvement from RTX2080 Ti to A100 GPUs (Balaban, 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Hours / 10,000 crystals</th>
<th style="text-align: center;">Hours / 10,000 metastable (M3GNet) crystals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CDVAE</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">1.260</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 13B</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">1.094</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 70B</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">1.728</td>
</tr>
</tbody>
</table>
<p>We see that LLaMA-2 13B actually has a comparable computational overhead to prior work, and LLaMA-2 70B is only slightly higher. When considering the rate of stable materials generated by each method, we see that LLaMA-2 13B actually has a higher throughput than CDVAE.</p>
<h1>C Template Method Baseline</h1>
<p>We provide code in Listing 1 implementing construction of the physically-inspired element swap table. This table is used by both the template method and the LLM-guided sampling method to constrain search to elements that are physically plausible. Listing 2 shows our implementation of a basic template method with uniform sampling. The LLM-guided procedure is mostly identical, except with uniform sampling of the swap element changed for sampling from a distribution obtained from the LLM with an infilling prompt (and modulated with temperature parameter $\tau$ )</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pymatgen.core</span><span class="w"> </span><span class="kn">import</span> <span class="n">Element</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pymatgen.core.structure</span><span class="w"> </span><span class="kn">import</span> <span class="n">Structure</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">m3gnet.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Relaxer</span>
<span class="k">def</span><span class="w"> </span><span class="nf">find_similar_elements</span><span class="p">(</span><span class="n">target_element</span><span class="p">,</span> <span class="n">elements</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">similar_elements</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">radius</span> <span class="ow">in</span> <span class="n">target_element</span><span class="o">.</span><span class="n">ionic_radii</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">elements</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">el</span><span class="o">.</span><span class="n">ionic_radii</span><span class="p">:</span>
                <span class="n">radius_diff</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">radius</span> <span class="o">-</span> <span class="n">el</span><span class="o">.</span><span class="n">ionic_radii</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">radius_diff</span> <span class="o">&lt;</span> <span class="n">tolerance</span> <span class="ow">and</span> <span class="n">el</span><span class="o">.</span><span class="n">symbol</span> <span class="o">!=</span>
    <span class="n">target_element</span><span class="o">.</span><span class="n">symbol</span><span class="p">:</span>
                        <span class="n">similar_elements</span><span class="o">.</span><span class="kp">append</span><span class="p">((</span><span class="n">el</span><span class="o">.</span><span class="n">symbol</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span>
    <span class="n">radius_diff</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">similar_elements</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">make_swap_table</span><span class="p">():</span>
    <span class="n">elements</span> <span class="o">=</span> <span class="p">[</span><span class="n">Element</span><span class="p">(</span><span class="n">el</span><span class="p">)</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">Element</span><span class="p">]</span>
    <span class="n">swap_table</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">elements</span><span class="p">:</span>
        <span class="n">swap_table</span><span class="p">[</span><span class="n">el</span><span class="o">.</span><span class="n">symbol</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">find_similar_elements</span><span class="p">(</span><span class="n">el</span><span class="p">,</span> <span class="n">elements</span><span class="p">)</span>
        <span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    return swap_table
</code></pre></div>

<p>Listing 1: Self contained code to construct the template method table which can be used to proposed mutations for local optimization around an existing material. The same table can be used in tandem with a language model to provide sampling constraints (i.e. eliminate elements which are very physically unlikely).</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">propose_new_structures</span><span class="p">(</span><span class="n">cif_str</span><span class="p">,</span><span class="w"> </span><span class="n">swap_table</span><span class="p">,</span><span class="w"> </span><span class="n">max_swaps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">struct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">Structure</span><span class="p">.</span><span class="n">from_str</span><span class="p">(</span><span class="n">cif_str</span><span class="p">,</span><span class="w"> </span><span class="n">fmt</span><span class="o">=</span><span class="ss">&quot;cif&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">elements</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">el.symbol for el in struct.species</span><span class="o">]</span>
<span class="w">    </span><span class="n">swappable_elements</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span>
<span class="n">        el for el in elements if el in swap_table and len(swap_table[el</span><span class="o">]</span><span class="p">)</span>
<span class="w">        </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="err">]</span>
<span class="w">    </span><span class="n">num_possible_swaps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="o">[</span><span class="n">len(swap_table[el</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">el</span><span class="w"> </span><span class="ow">in</span>
<span class="w">    </span><span class="n">swappable_elements</span><span class="err">]</span><span class="p">)</span>
<span class="w">    </span><span class="n">num_swaps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">num_possible_swaps</span><span class="p">,</span><span class="w"> </span><span class="n">max_swaps</span><span class="p">)</span>
<span class="w">    </span><span class="n">relaxer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Relaxer</span><span class="p">()</span>
<span class="w">    </span><span class="n">new_bulks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">num_swaps</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">old_el</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">swappable_elements</span><span class="p">)</span>
<span class="w">        </span><span class="n">possible_new</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">swap_table</span><span class="o">[</span><span class="n">old_el</span><span class="o">]</span>
<span class="w">        </span><span class="n">new_el</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">possible_new</span><span class="p">)</span>
<span class="w">        </span><span class="n">new_bulk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">struct</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="w">        </span><span class="n">new_bulk</span><span class="p">.</span><span class="n">replace_species</span><span class="p">(</span><span class="err">{</span><span class="nl">old_el</span><span class="p">:</span><span class="w"> </span><span class="n">new_el</span><span class="err">}</span><span class="p">)</span>
<span class="w">        </span><span class="n">relax_results</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">relaxer</span><span class="p">.</span><span class="n">relax</span><span class="p">(</span><span class="n">new_bulk</span><span class="p">)</span>
<span class="w">        </span><span class="n">final_structure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">relax_results</span><span class="o">[</span><span class="n">&#39;final_structure&#39;</span><span class="o">]</span>
<span class="w">        </span><span class="n">final_relaxed_energy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">relax_results</span><span class="o">[</span><span class="n">&#39;trajectory&#39;</span><span class="o">]</span><span class="p">.</span><span class="n">energies</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span>
<span class="w">        </span><span class="n">new_bulks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">dict</span><span class="p">(</span>
<span class="w">            </span><span class="n">cif</span><span class="o">=</span><span class="n">final_structure</span><span class="p">.</span><span class="k">to</span><span class="p">(</span><span class="n">fmt</span><span class="o">=</span><span class="ss">&quot;cif&quot;</span><span class="p">),</span>
<span class="w">            </span><span class="n">energy</span><span class="o">=</span><span class="n">final_relaxed_energy</span>
<span class="w">        </span><span class="p">))</span>
<span class="w">    </span><span class="n">new_bulks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_bulks</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">new_bulks</span>
</code></pre></div>

<p>Listing 2: Self contained code implementing a template method with uniform sampling. Our language model procedure is essentially the same but replaces uniform sampling with logits from a prompted language model. This language model can use the context from the rest of the crystal structure to propose a mutation instead of choosing a mutation completely at random.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/facebookresearch/crystal-llm&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
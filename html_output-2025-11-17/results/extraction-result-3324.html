<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3324 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3324</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3324</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-fa3609e00f9f422a309c621a35394c4a38f88687</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fa3609e00f9f422a309c621a35394c4a38f88687" target="_blank">Using cognitive psychology to understand GPT-3</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the National Academy of Sciences of the United States of America</p>
                <p><strong>Paper TL;DR:</strong> The present article finds that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books.</p>
                <p><strong>Paper Abstract:</strong> Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3324.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3324.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 — "Davinci" variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The largest public GPT-3 API model (Davinci) evaluated across vignette- and task-based cognitive-psychology experiments in this paper; used with temperature=0 (deterministic) unless otherwise noted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An auto-regressive transformer language model trained by OpenAI on hundreds of billions of tokens of internet and book text; uses self-attention and in-context learning to perform tasks given textual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['in-context learning', 'intuitive (fast) responses', 'deliberative / model-based reasoning (in some tasks)', 'model-free style reward-updating (implicit)', 'random exploration (stochastic choice-like behavior)', 'no detected directed exploration', 'observational vs. interventional causal inference (attempted)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>In-context learning: the model is provided task descriptions and previous examples or trial histories as text and produces next responses without parameter updates. Intuitive (fast) responses: the model often outputs immediate, human-like heuristics (e.g., CRT intuitive answers). Deliberative/model-based indications: in sequential tasks (two-step) GPT-3 used information about transitions and outcomes in a way consistent with model-based reasoning. Model-free-like updates: in some tasks the model's choices depend on prior rewards in ways similar to reward-driven repetition. Random exploration: increased stochasticity of choices as a function of estimated reward differences (but not strategically modulated by horizon). Directed exploration: absent — GPT-3 did not prefer sampling more-informative options when longer horizons warranted it. Observational vs interventional causal inference: GPT-3 made some normative-looking interventional predictions in one condition but failed to incorporate causal-graph information consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>The paper finds that GPT-3 exhibits multiple distinct reasoning styles depending on task and prompt (intuitive heuristics, in-context exploitation of statistical regularities, model-based-like planning in the two-step task, and a rudimentary form of random exploration), but it does not deploy a robust diversity of purposeful strategies (notably it lacks directed exploration and fails to reliably perform causal interventions). This determination is empirical: the authors tested behavior across several controlled task paradigms and regressions to detect signatures of different strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Battery of cognitive-psychology tasks (vignette-based tasks; decisions-from-descriptions benchmark; horizon multi-armed bandit; two-step RL task; causal intervention task adapted from Waldmann & Hagmayer)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A set of canonical human cognitive tasks used to probe reasoning styles: vignette problems (Linda, cab, Wason, Blicket, CRT items), a 13k-problem decisions-from-descriptions benchmark (regret measured), Wilson's horizon two-armed bandit to probe directed vs random exploration, the two-step task to dissociate model-free vs model-based RL, and a 3-variable observation-vs-intervention causal inference task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported task-wise (selected highlights):
- Vignettes: 12 vignette problems — GPT-3 produced human-like responses in all 12 and answered 6 correctly under standard phrasing; small adversarial phrasing changes caused large failures. (No single scalar metric given beyond counts.)
- Decisions-from-descriptions (13,000 problems): Only Davinci performed above chance (t(29134) = -16.85, p < .001) but did not reach human-level performance (t(29134) = -11.50, p < .001); performance reported as mean regret (Davinci lower regret than smaller GPT-3 variants but higher than humans). Exact regret numerical values are presented in Figure 2B (paper). 
- Horizon bandit task (exploration): Short-horizon: GPT-3 regret indistinguishable from humans (t(5566) = -0.043, p = .97). Long-horizon: GPT-3 initial regret significantly lower than humans (t(5550) = -4.07, p < .001); overall across experiment GPT-3 had lower mean regret (M=2.72, SD=5.98) than human subjects (M=3.24, SD=10.26), t(38878) = -5.03, p < .001.
- Exploration regression effects: GPT-3 showed a significant effect of estimated reward difference consistent with random exploration (β = 0.18 ± 0.01, z = 14.48, p < .001); interaction with horizon (strategy modulation) was not significant (β = -0.02 ± 0.02, z = -1.47, p = .14); no evidence of directed exploration in unequal-information condition (β = -0.15 ± 0.27, z = -0.56, p = .58).
- Two-step RL: GPT-3 displayed signatures of model-based learning: probability of repeating first-stage action decreased after a rare transition followed by reward (t(1982) = -6.16, p < .001); probability increased after rare non-rewarded transitions (t(1814) = 2.33, p = .01), interpreted as model-based influences.
- CRT (deliberation): On three CRT items GPT-3 repeatedly gave intuitive but incorrect answers (consistent failure to override fast responses).
- Causal observation vs intervention task: In the common-cause condition GPT-3's interventional predictions matched the normative prescription in some respects and predicted increases in C after observing B=1; but GPT-3 failed to reduce predictions when observing B=0, and in the causal-chain condition GPT-3 made identical predictions to the common-cause condition (it did not incorporate causal-graph information).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper contrasts signatures of distinct strategies by (a) using task structure and statistical regressions and (b) comparing GPT-3's choices to human benchmarks and normative models. Key comparisons: 1) Random vs directed exploration — regression shows presence of random exploration-like behavior but absence of horizon-modulated (directed) exploration; 2) Model-free vs model-based RL — two-step task analysis shows GPT-3 displays model-based signatures (interaction of transition type and reward), suggesting use of an internal transition model; 3) Intuitive vs deliberative reasoning — CRT shows persistent intuitive (fast) errors despite model-based signatures elsewhere, indicating mixed styles; 4) Observational vs interventional causal inference — GPT-3 fails to reliably implement do()-style causal reasoning and does not consistently incorporate causal-graph information, contrasting with normative expectations and human patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 (Davinci) displays multiple, task-dependent reasoning styles: it can (1) perform strong in-context learning to produce human-like responses on many vignette tasks (though susceptible to adversarial rephrasings), (2) solve decisions-from-descriptions above chance but below human level, (3) exhibit random-exploration-like stochastic choice but not strategic/directed exploration, (4) show model-based RL signatures in the two-step task, and (5) fail to robustly perform causal reasoning that requires distinguishing observation from intervention (do-operations). The model thus uses similar surface-level strategies across many tasks (statistical pattern exploitation and in-context inference) but does not deploy a reliably diverse and causally-aware set of reasoning methods comparable to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Negative/limiting results include: (1) Vignette sensitivity — small adversarial changes to vignettes caused qualitatively different and non-human-like answers (e.g., Reverse Wason, Black Cab), indicating brittle surface-level pattern matching rather than robust reasoning. (2) No directed exploration — despite good overall bandit performance, GPT-3 did not increase informative-choice sampling when longer horizons would benefit exploration. (3) CRT failures — model repeatedly gave intuitive wrong answers, showing inability to override fast heuristics even when capable of model-based reasoning elsewhere. (4) Causal failures — in the Waldmann-style intervention task GPT-3 failed to use causal-graph information consistently; identical behavior across common-cause and causal-chain conditions suggests accidental alignment with normative answers rather than true causal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using cognitive psychology to understand GPT-3', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3324.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3324.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Directed vs Random Exploration (Horizon task)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directed versus Random Exploration as probed by Wilson's horizon task (two-armed bandit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A focused analysis in the paper that probes whether GPT-3 uses directed exploration (choosing more informative options when the horizon is long) versus random exploration (increasing stochasticity/noise) to solve explore–exploit trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Davinci variant of GPT-3, given sequential textual trial histories and asked to choose options on each trial; deterministic generation in main experiments (temperature=0) except where probabilistic choice probabilities were estimated (temperature=1 used for choice-probability estimation in the decisions-from-descriptions contrast analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['random exploration (stochastic-choice signature)', 'directed exploration (information-seeking signature) - tested and not found', 'perseveration bias (repetition tendency)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Random exploration: operationalized as choice variability correlated with estimated reward differences and increased stochasticity in long-horizon conditions; detected by positive effect of estimated reward difference in logistic regression. Directed exploration: operationalized as horizon-dependent preference for more informative options in unequal-information forced-trial condition; detected by positive effect of horizon in unequal-information regression (absent here). Perseveration: tendency to repeat options that have been recently observed in the prompt history.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>The model displays at least two surface behaviors relevant to exploration: a stochastic/random exploration signature plus a perseveration bias, but it lacks the strategic, directed exploration humans use — thus similar (non-adaptive) styles rather than a diverse, adaptive set.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Wilson's Horizon Task (two-armed bandit forced-choice + free-choice design)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A bandit experiment where each block contains forced sampling trials (equal or unequal information) followed by either one or six free-choice trials; the horizon length manipulates the benefit of exploration and allows the dissociation of directed vs random exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Key metrics reported:
- Regret: short-horizon GPT-3 vs humans: no difference (t(5566) = -0.043, p = .97).
- Long-horizon initial regret: GPT-3 lower than humans (t(5550) = -4.07, p < .001).
- Overall mean regret across whole experiment: GPT-3 M = 2.72 (SD = 5.98) vs humans M = 3.24 (SD = 10.26), t(38878) = -5.03, p < .001.
- Regression: estimated reward difference effect β = 0.18 ± 0.01, z = 14.48, p < .001 (evidence of random exploration/stochastic sensitivity); interaction with horizon β = -0.02 ± 0.02, z = -1.47, p = .14 (no strategic increase in randomness with horizon). Unequal-info condition horizon effect β = -0.15 ± 0.27, z = -0.56, p = .58 (no directed exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Regression contrasts reveal that GPT-3 uses reward-difference-sensitive stochasticity (random exploration signature) but does not modulate this stochasticity by horizon length (no strategic/random exploration interaction) and does not preferentially sample the less-observed/more-informative option in unequal-information long-horizon conditions (no directed exploration). Humans show both random and directed exploration signatures; GPT-3 shows only random.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 achieves bandit performance comparable or superior to humans overall (lower mean regret) but does so without evidence of directed exploration; it relies on a simpler set of strategies (stochastic sampling + perseveration) rather than human-like, horizon-sensitive directed exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite lower overall regret, GPT-3's lack of directed exploration is a negative result relative to humans; the model's perseveration bias (preference for recently observed option names) may artificially boost short-term performance in this textual formulation and is likely an artifact of next-token prediction training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using cognitive psychology to understand GPT-3', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3324.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3324.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deliberation vs Intuition (CRT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognitive Reflection Test performance: deliberative override vs intuitive errors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessment of GPT-3's tendency to provide intuitive (fast, wrong) answers versus deliberative (slow, correct) answers using the three-item Cognitive Reflection Test (CRT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Davinci prompted with standard CRT items in vignette-style prompts; responses recorded (temperature=0 deterministic).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['intuitive (fast) heuristics', 'deliberative reasoning (tested, not observed)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Intuitive heuristics: the model outputs immediate, common heuristic responses (e.g., ball costs $0.10), replicating human intuitive error answers. Deliberative reasoning: the process of overriding intuitive answers via reflective calculation (not observed in GPT-3 on CRT items in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>GPT-3 demonstrates the intuitive/fast style reliably on CRT items and does not show evidence of switching to a deliberative calculation style in these tasks (i.e., similar reasoning style repeated across CRT items).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Cognitive Reflection Test (3 items)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Three canonical items designed to elicit an intuitive but incorrect answer and require reflective reasoning to produce the correct answer (e.g., bat-and-ball, machines/widgets, lily pads doubling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GPT-3 provided the intuitive incorrect answer for all three CRT items (examples in SI Appendix):
- Ball & bat: answered $0.10 (intuitive wrong)
- Machines/widgets: answered 100 minutes (intuitive wrong)
- Lily pads doubling: answered 24 days (intuitive wrong)
No metric of accuracy other than these qualitative errors is reported for CRT; all three items elicited the intuitive error.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Although GPT-3 shows model-based reasoning signatures in other tasks (two-step task), it nonetheless fails to apply deliberative override on CRT items — indicating that the presence of model-based behavior in some tasks does not generalize to deliberate analytic reasoning across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 reliably outputs intuitive, wrong CRT answers, indicating a failure to engage deliberative reflective reasoning on these classic problems despite exhibiting more deliberation-like behavior in other sequential tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>This is a striking negative result: CRT failures show that GPT-3's apparent model-based reasoning in sequential decision tasks does not imply general ability to deliberately reason or to override heuristics in standard analytic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using cognitive psychology to understand GPT-3', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3324.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3324.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-based vs Model-free (Two-step task)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based versus model-free reinforcement learning signatures in the two-step task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper probes whether GPT-3 behaves like a model-free learner (reward-driven repetition) or a model-based learner (uses transition model to plan) using the canonical two-step task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Davinci given sequences of two-stage trials (textual descriptions of transitions and outcomes), asked to choose first- and second-stage actions; behavior analyzed across simulated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['model-based planning (uses transition knowledge to adjust first-stage choices)', 'model-free-like reward influence (present as part of combined behavior)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model-based: explicit use of transition probabilities (common vs rare) and outcome to update first-stage action preference in a way predicted by forward planning. Model-free: simple reinforcement where rewarded actions are repeated regardless of transition type. Authors analyze stay probabilities conditional on reward and transition type to dissociate the two.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>GPT-3 displays a mixture of behaviors indicative of both model-based and model-free processes, but crucially shows significant model-based signatures — indicating a richer repertoire than purely model-free repetition, i.e., diverse styles in this sequential domain.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Two-step reinforcement learning task</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A sequential decision task with two stages: first-stage choices probabilistically lead to second-stage states (common vs rare transitions); second-stage choices yield stochastic rewards whose probabilities drift over time. Comparing stay/shift patterns after combinations of reward and transition distinguishes model-free and model-based strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Key statistical findings:
- After a rare transition that produced reward (treasure), GPT-3 decreased probability of repeating the previous first-stage action (t(1982) = -6.16, p < .001) — a hallmark of model-based reasoning.
- After a rare non-rewarded outcome, GPT-3 increased probability of repeating the same first-stage action (t(1814) = 2.33, p = .01) — also interpreted in context of model-based patterns in the paper.
- These results indicate a measurable model-based influence in GPT-3's behavior across simulated runs of the two-step task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Empirically contrasted by stay-probability analysis conditioned on transition type (common vs rare) and reward (treasure vs junk). Model-free predicts main effect of reward only; model-based predicts an interaction of transition and reward. GPT-3's significant interaction-like patterns align with model-based predictions and contrast with pure model-free behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 shows signatures of model-based reinforcement learning in the two-step task, suggesting it can internalize and use transition structure presented in textual form to guide planning-like choices, despite other domains showing heuristic-dominated behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although model-based signatures are present, CRT and causal-intervention failures indicate that GPT-3's model-based-like behavior is domain- and prompt-dependent and does not imply general-purpose deliberative capabilities across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using cognitive psychology to understand GPT-3', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3324.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3324.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observational vs Interventional Causal Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difference between observational and interventional causal inference (Waldmann & Hagmayer adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal-reasoning experiment adapted from Waldmann & Hagmayer testing whether GPT-3 distinguishes seeing (observation) from doing (intervention/do()), and whether it incorporates specified causal graph structures (common-cause vs causal-chain).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Davinci provided with 20 observational samples of three-variable configurations and additional textual information about causal structure (A->B and A->C for common-cause, or B->A->C for causal-chain), then asked to predict outcomes under interventions (manually setting B) vs observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['observational statistical inference (pattern-matching from observed frequencies)', 'attempted interventional reasoning (inconsistent)', 'lack of explicit do-operator causal manipulation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Observational inference: using co-occurrence patterns in observed data to make predictions. Interventional reasoning: properly applying do()-style logic (cut incoming edges to intervened variable) to update predictions under intervention. The paper probes whether GPT-3 treats interventions differently from observations in line with causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>GPT-3 sometimes produced interventional predictions consistent with normative prescriptions in the common-cause condition, but it failed to incorporate causal-graph information consistently (it produced nearly identical answers across common-cause and causal-chain conditions), indicating shallow, similar styles (pattern completion) rather than true diverse causal reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Observation-vs-Intervention causal inference task (Waldmann & Hagmayer paradigm adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Subjects observe 20 cases of three variables A, B, C, are told a causal structure (common-cause A->B & A->C or causal-chain B->A->C), then asked how many of 20 new casks will have C when B is set to 1 by intervention (do(B=1)) or when B is observed to be 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported patterns:
- Common-cause condition: GPT-3 made interventional inferences that in some respects matched normative (do-based) prescriptions and predicted an increase in C after observing B=1 (consistent with both normative theory and humans for observation). However GPT-3 failed to reduce its prediction when observing B=0 (not normative and not human-like).
- Causal-chain condition: GPT-3 produced the same predictions as in the common-cause condition (normatively incorrect, and inconsistent with human data which differ by condition). No precise numeric error rates given; results are described qualitatively with plots in the paper's Figure 5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Comparison shows mismatch: humans and normative models make different predictions for interventions versus observations depending on the causal graph (common-cause vs chain). GPT-3 did not reliably reflect these distinctions — its behavior did not change appropriately with specified causal structure. Thus, diverse causal-reasoning methods (observational-statistical vs true interventional/do()-based inference) were not robustly implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 fails to consistently perform interventional causal reasoning that requires mechanistically deleting incoming edges (do-operator). Instances of correct interventional predictions in one condition appear accidental; GPT-3 does not systematically use causal-graph information to differentiate observation from intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although GPT-3 sometimes matched normative interventional answers in the common-cause condition, its failures when B=0 and identical behavior across distinct causal-graph conditions are strong negative results indicating inability to perform robust causal reasoning via interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using cognitive psychology to understand GPT-3', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Humans use directed and random exploration to solve the explore-exploit dilemma <em>(Rating: 2)</em></li>
                <li>Model-based influences on humans' choices and striatal prediction errors <em>(Rating: 2)</em></li>
                <li>Seeing versus doing: two modes of accessing causal knowledge <em>(Rating: 2)</em></li>
                <li>Causality <em>(Rating: 2)</em></li>
                <li>Using large-scale experiments and machine learning to discover theories of human decision-making <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3324",
    "paper_id": "paper-fa3609e00f9f422a309c621a35394c4a38f88687",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-3 (Davinci)",
            "name_full": "Generative Pre-trained Transformer 3 — \"Davinci\" variant",
            "brief_description": "The largest public GPT-3 API model (Davinci) evaluated across vignette- and task-based cognitive-psychology experiments in this paper; used with temperature=0 (deterministic) unless otherwise noted.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci)",
            "model_description": "An auto-regressive transformer language model trained by OpenAI on hundreds of billions of tokens of internet and book text; uses self-attention and in-context learning to perform tasks given textual prompts.",
            "model_size": "175B",
            "reasoning_methods": [
                "in-context learning",
                "intuitive (fast) responses",
                "deliberative / model-based reasoning (in some tasks)",
                "model-free style reward-updating (implicit)",
                "random exploration (stochastic choice-like behavior)",
                "no detected directed exploration",
                "observational vs. interventional causal inference (attempted)"
            ],
            "reasoning_methods_description": "In-context learning: the model is provided task descriptions and previous examples or trial histories as text and produces next responses without parameter updates. Intuitive (fast) responses: the model often outputs immediate, human-like heuristics (e.g., CRT intuitive answers). Deliberative/model-based indications: in sequential tasks (two-step) GPT-3 used information about transitions and outcomes in a way consistent with model-based reasoning. Model-free-like updates: in some tasks the model's choices depend on prior rewards in ways similar to reward-driven repetition. Random exploration: increased stochasticity of choices as a function of estimated reward differences (but not strategically modulated by horizon). Directed exploration: absent — GPT-3 did not prefer sampling more-informative options when longer horizons warranted it. Observational vs interventional causal inference: GPT-3 made some normative-looking interventional predictions in one condition but failed to incorporate causal-graph information consistently.",
            "diversity_of_methods": "The paper finds that GPT-3 exhibits multiple distinct reasoning styles depending on task and prompt (intuitive heuristics, in-context exploitation of statistical regularities, model-based-like planning in the two-step task, and a rudimentary form of random exploration), but it does not deploy a robust diversity of purposeful strategies (notably it lacks directed exploration and fails to reliably perform causal interventions). This determination is empirical: the authors tested behavior across several controlled task paradigms and regressions to detect signatures of different strategies.",
            "reasoning_task_name": "Battery of cognitive-psychology tasks (vignette-based tasks; decisions-from-descriptions benchmark; horizon multi-armed bandit; two-step RL task; causal intervention task adapted from Waldmann & Hagmayer)",
            "reasoning_task_description": "A set of canonical human cognitive tasks used to probe reasoning styles: vignette problems (Linda, cab, Wason, Blicket, CRT items), a 13k-problem decisions-from-descriptions benchmark (regret measured), Wilson's horizon two-armed bandit to probe directed vs random exploration, the two-step task to dissociate model-free vs model-based RL, and a 3-variable observation-vs-intervention causal inference task.",
            "performance_by_method": "Reported task-wise (selected highlights):\n- Vignettes: 12 vignette problems — GPT-3 produced human-like responses in all 12 and answered 6 correctly under standard phrasing; small adversarial phrasing changes caused large failures. (No single scalar metric given beyond counts.)\n- Decisions-from-descriptions (13,000 problems): Only Davinci performed above chance (t(29134) = -16.85, p &lt; .001) but did not reach human-level performance (t(29134) = -11.50, p &lt; .001); performance reported as mean regret (Davinci lower regret than smaller GPT-3 variants but higher than humans). Exact regret numerical values are presented in Figure 2B (paper). \n- Horizon bandit task (exploration): Short-horizon: GPT-3 regret indistinguishable from humans (t(5566) = -0.043, p = .97). Long-horizon: GPT-3 initial regret significantly lower than humans (t(5550) = -4.07, p &lt; .001); overall across experiment GPT-3 had lower mean regret (M=2.72, SD=5.98) than human subjects (M=3.24, SD=10.26), t(38878) = -5.03, p &lt; .001.\n- Exploration regression effects: GPT-3 showed a significant effect of estimated reward difference consistent with random exploration (β = 0.18 ± 0.01, z = 14.48, p &lt; .001); interaction with horizon (strategy modulation) was not significant (β = -0.02 ± 0.02, z = -1.47, p = .14); no evidence of directed exploration in unequal-information condition (β = -0.15 ± 0.27, z = -0.56, p = .58).\n- Two-step RL: GPT-3 displayed signatures of model-based learning: probability of repeating first-stage action decreased after a rare transition followed by reward (t(1982) = -6.16, p &lt; .001); probability increased after rare non-rewarded transitions (t(1814) = 2.33, p = .01), interpreted as model-based influences.\n- CRT (deliberation): On three CRT items GPT-3 repeatedly gave intuitive but incorrect answers (consistent failure to override fast responses).\n- Causal observation vs intervention task: In the common-cause condition GPT-3's interventional predictions matched the normative prescription in some respects and predicted increases in C after observing B=1; but GPT-3 failed to reduce predictions when observing B=0, and in the causal-chain condition GPT-3 made identical predictions to the common-cause condition (it did not incorporate causal-graph information).",
            "comparison_of_methods": "The paper contrasts signatures of distinct strategies by (a) using task structure and statistical regressions and (b) comparing GPT-3's choices to human benchmarks and normative models. Key comparisons: 1) Random vs directed exploration — regression shows presence of random exploration-like behavior but absence of horizon-modulated (directed) exploration; 2) Model-free vs model-based RL — two-step task analysis shows GPT-3 displays model-based signatures (interaction of transition type and reward), suggesting use of an internal transition model; 3) Intuitive vs deliberative reasoning — CRT shows persistent intuitive (fast) errors despite model-based signatures elsewhere, indicating mixed styles; 4) Observational vs interventional causal inference — GPT-3 fails to reliably implement do()-style causal reasoning and does not consistently incorporate causal-graph information, contrasting with normative expectations and human patterns.",
            "key_findings": "GPT-3 (Davinci) displays multiple, task-dependent reasoning styles: it can (1) perform strong in-context learning to produce human-like responses on many vignette tasks (though susceptible to adversarial rephrasings), (2) solve decisions-from-descriptions above chance but below human level, (3) exhibit random-exploration-like stochastic choice but not strategic/directed exploration, (4) show model-based RL signatures in the two-step task, and (5) fail to robustly perform causal reasoning that requires distinguishing observation from intervention (do-operations). The model thus uses similar surface-level strategies across many tasks (statistical pattern exploitation and in-context inference) but does not deploy a reliably diverse and causally-aware set of reasoning methods comparable to humans.",
            "counter_examples_or_negative_results": "Negative/limiting results include: (1) Vignette sensitivity — small adversarial changes to vignettes caused qualitatively different and non-human-like answers (e.g., Reverse Wason, Black Cab), indicating brittle surface-level pattern matching rather than robust reasoning. (2) No directed exploration — despite good overall bandit performance, GPT-3 did not increase informative-choice sampling when longer horizons would benefit exploration. (3) CRT failures — model repeatedly gave intuitive wrong answers, showing inability to override fast heuristics even when capable of model-based reasoning elsewhere. (4) Causal failures — in the Waldmann-style intervention task GPT-3 failed to use causal-graph information consistently; identical behavior across common-cause and causal-chain conditions suggests accidental alignment with normative answers rather than true causal reasoning.",
            "uuid": "e3324.0",
            "source_info": {
                "paper_title": "Using cognitive psychology to understand GPT-3",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Directed vs Random Exploration (Horizon task)",
            "name_full": "Directed versus Random Exploration as probed by Wilson's horizon task (two-armed bandit)",
            "brief_description": "A focused analysis in the paper that probes whether GPT-3 uses directed exploration (choosing more informative options when the horizon is long) versus random exploration (increasing stochasticity/noise) to solve explore–exploit trade-offs.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci)",
            "model_description": "Davinci variant of GPT-3, given sequential textual trial histories and asked to choose options on each trial; deterministic generation in main experiments (temperature=0) except where probabilistic choice probabilities were estimated (temperature=1 used for choice-probability estimation in the decisions-from-descriptions contrast analysis).",
            "model_size": "175B",
            "reasoning_methods": [
                "random exploration (stochastic-choice signature)",
                "directed exploration (information-seeking signature) - tested and not found",
                "perseveration bias (repetition tendency)"
            ],
            "reasoning_methods_description": "Random exploration: operationalized as choice variability correlated with estimated reward differences and increased stochasticity in long-horizon conditions; detected by positive effect of estimated reward difference in logistic regression. Directed exploration: operationalized as horizon-dependent preference for more informative options in unequal-information forced-trial condition; detected by positive effect of horizon in unequal-information regression (absent here). Perseveration: tendency to repeat options that have been recently observed in the prompt history.",
            "diversity_of_methods": "The model displays at least two surface behaviors relevant to exploration: a stochastic/random exploration signature plus a perseveration bias, but it lacks the strategic, directed exploration humans use — thus similar (non-adaptive) styles rather than a diverse, adaptive set.",
            "reasoning_task_name": "Wilson's Horizon Task (two-armed bandit forced-choice + free-choice design)",
            "reasoning_task_description": "A bandit experiment where each block contains forced sampling trials (equal or unequal information) followed by either one or six free-choice trials; the horizon length manipulates the benefit of exploration and allows the dissociation of directed vs random exploration strategies.",
            "performance_by_method": "Key metrics reported:\n- Regret: short-horizon GPT-3 vs humans: no difference (t(5566) = -0.043, p = .97).\n- Long-horizon initial regret: GPT-3 lower than humans (t(5550) = -4.07, p &lt; .001).\n- Overall mean regret across whole experiment: GPT-3 M = 2.72 (SD = 5.98) vs humans M = 3.24 (SD = 10.26), t(38878) = -5.03, p &lt; .001.\n- Regression: estimated reward difference effect β = 0.18 ± 0.01, z = 14.48, p &lt; .001 (evidence of random exploration/stochastic sensitivity); interaction with horizon β = -0.02 ± 0.02, z = -1.47, p = .14 (no strategic increase in randomness with horizon). Unequal-info condition horizon effect β = -0.15 ± 0.27, z = -0.56, p = .58 (no directed exploration).",
            "comparison_of_methods": "Regression contrasts reveal that GPT-3 uses reward-difference-sensitive stochasticity (random exploration signature) but does not modulate this stochasticity by horizon length (no strategic/random exploration interaction) and does not preferentially sample the less-observed/more-informative option in unequal-information long-horizon conditions (no directed exploration). Humans show both random and directed exploration signatures; GPT-3 shows only random.",
            "key_findings": "GPT-3 achieves bandit performance comparable or superior to humans overall (lower mean regret) but does so without evidence of directed exploration; it relies on a simpler set of strategies (stochastic sampling + perseveration) rather than human-like, horizon-sensitive directed exploration.",
            "counter_examples_or_negative_results": "Despite lower overall regret, GPT-3's lack of directed exploration is a negative result relative to humans; the model's perseveration bias (preference for recently observed option names) may artificially boost short-term performance in this textual formulation and is likely an artifact of next-token prediction training.",
            "uuid": "e3324.1",
            "source_info": {
                "paper_title": "Using cognitive psychology to understand GPT-3",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Deliberation vs Intuition (CRT)",
            "name_full": "Cognitive Reflection Test performance: deliberative override vs intuitive errors",
            "brief_description": "Assessment of GPT-3's tendency to provide intuitive (fast, wrong) answers versus deliberative (slow, correct) answers using the three-item Cognitive Reflection Test (CRT).",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci)",
            "model_description": "Davinci prompted with standard CRT items in vignette-style prompts; responses recorded (temperature=0 deterministic).",
            "model_size": "175B",
            "reasoning_methods": [
                "intuitive (fast) heuristics",
                "deliberative reasoning (tested, not observed)"
            ],
            "reasoning_methods_description": "Intuitive heuristics: the model outputs immediate, common heuristic responses (e.g., ball costs $0.10), replicating human intuitive error answers. Deliberative reasoning: the process of overriding intuitive answers via reflective calculation (not observed in GPT-3 on CRT items in the paper).",
            "diversity_of_methods": "GPT-3 demonstrates the intuitive/fast style reliably on CRT items and does not show evidence of switching to a deliberative calculation style in these tasks (i.e., similar reasoning style repeated across CRT items).",
            "reasoning_task_name": "Cognitive Reflection Test (3 items)",
            "reasoning_task_description": "Three canonical items designed to elicit an intuitive but incorrect answer and require reflective reasoning to produce the correct answer (e.g., bat-and-ball, machines/widgets, lily pads doubling).",
            "performance_by_method": "GPT-3 provided the intuitive incorrect answer for all three CRT items (examples in SI Appendix):\n- Ball & bat: answered $0.10 (intuitive wrong)\n- Machines/widgets: answered 100 minutes (intuitive wrong)\n- Lily pads doubling: answered 24 days (intuitive wrong)\nNo metric of accuracy other than these qualitative errors is reported for CRT; all three items elicited the intuitive error.",
            "comparison_of_methods": "Although GPT-3 shows model-based reasoning signatures in other tasks (two-step task), it nonetheless fails to apply deliberative override on CRT items — indicating that the presence of model-based behavior in some tasks does not generalize to deliberate analytic reasoning across domains.",
            "key_findings": "GPT-3 reliably outputs intuitive, wrong CRT answers, indicating a failure to engage deliberative reflective reasoning on these classic problems despite exhibiting more deliberation-like behavior in other sequential tasks.",
            "counter_examples_or_negative_results": "This is a striking negative result: CRT failures show that GPT-3's apparent model-based reasoning in sequential decision tasks does not imply general ability to deliberately reason or to override heuristics in standard analytic benchmarks.",
            "uuid": "e3324.2",
            "source_info": {
                "paper_title": "Using cognitive psychology to understand GPT-3",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Model-based vs Model-free (Two-step task)",
            "name_full": "Model-based versus model-free reinforcement learning signatures in the two-step task",
            "brief_description": "The paper probes whether GPT-3 behaves like a model-free learner (reward-driven repetition) or a model-based learner (uses transition model to plan) using the canonical two-step task.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci)",
            "model_description": "Davinci given sequences of two-stage trials (textual descriptions of transitions and outcomes), asked to choose first- and second-stage actions; behavior analyzed across simulated runs.",
            "model_size": "175B",
            "reasoning_methods": [
                "model-based planning (uses transition knowledge to adjust first-stage choices)",
                "model-free-like reward influence (present as part of combined behavior)"
            ],
            "reasoning_methods_description": "Model-based: explicit use of transition probabilities (common vs rare) and outcome to update first-stage action preference in a way predicted by forward planning. Model-free: simple reinforcement where rewarded actions are repeated regardless of transition type. Authors analyze stay probabilities conditional on reward and transition type to dissociate the two.",
            "diversity_of_methods": "GPT-3 displays a mixture of behaviors indicative of both model-based and model-free processes, but crucially shows significant model-based signatures — indicating a richer repertoire than purely model-free repetition, i.e., diverse styles in this sequential domain.",
            "reasoning_task_name": "Two-step reinforcement learning task",
            "reasoning_task_description": "A sequential decision task with two stages: first-stage choices probabilistically lead to second-stage states (common vs rare transitions); second-stage choices yield stochastic rewards whose probabilities drift over time. Comparing stay/shift patterns after combinations of reward and transition distinguishes model-free and model-based strategies.",
            "performance_by_method": "Key statistical findings:\n- After a rare transition that produced reward (treasure), GPT-3 decreased probability of repeating the previous first-stage action (t(1982) = -6.16, p &lt; .001) — a hallmark of model-based reasoning.\n- After a rare non-rewarded outcome, GPT-3 increased probability of repeating the same first-stage action (t(1814) = 2.33, p = .01) — also interpreted in context of model-based patterns in the paper.\n- These results indicate a measurable model-based influence in GPT-3's behavior across simulated runs of the two-step task.",
            "comparison_of_methods": "Empirically contrasted by stay-probability analysis conditioned on transition type (common vs rare) and reward (treasure vs junk). Model-free predicts main effect of reward only; model-based predicts an interaction of transition and reward. GPT-3's significant interaction-like patterns align with model-based predictions and contrast with pure model-free behavior.",
            "key_findings": "GPT-3 shows signatures of model-based reinforcement learning in the two-step task, suggesting it can internalize and use transition structure presented in textual form to guide planning-like choices, despite other domains showing heuristic-dominated behavior.",
            "counter_examples_or_negative_results": "Although model-based signatures are present, CRT and causal-intervention failures indicate that GPT-3's model-based-like behavior is domain- and prompt-dependent and does not imply general-purpose deliberative capabilities across tasks.",
            "uuid": "e3324.3",
            "source_info": {
                "paper_title": "Using cognitive psychology to understand GPT-3",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Observational vs Interventional Causal Inference",
            "name_full": "Difference between observational and interventional causal inference (Waldmann & Hagmayer adaptation)",
            "brief_description": "A causal-reasoning experiment adapted from Waldmann & Hagmayer testing whether GPT-3 distinguishes seeing (observation) from doing (intervention/do()), and whether it incorporates specified causal graph structures (common-cause vs causal-chain).",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci)",
            "model_description": "Davinci provided with 20 observational samples of three-variable configurations and additional textual information about causal structure (A-&gt;B and A-&gt;C for common-cause, or B-&gt;A-&gt;C for causal-chain), then asked to predict outcomes under interventions (manually setting B) vs observations.",
            "model_size": "175B",
            "reasoning_methods": [
                "observational statistical inference (pattern-matching from observed frequencies)",
                "attempted interventional reasoning (inconsistent)",
                "lack of explicit do-operator causal manipulation"
            ],
            "reasoning_methods_description": "Observational inference: using co-occurrence patterns in observed data to make predictions. Interventional reasoning: properly applying do()-style logic (cut incoming edges to intervened variable) to update predictions under intervention. The paper probes whether GPT-3 treats interventions differently from observations in line with causal graphs.",
            "diversity_of_methods": "GPT-3 sometimes produced interventional predictions consistent with normative prescriptions in the common-cause condition, but it failed to incorporate causal-graph information consistently (it produced nearly identical answers across common-cause and causal-chain conditions), indicating shallow, similar styles (pattern completion) rather than true diverse causal reasoning methods.",
            "reasoning_task_name": "Observation-vs-Intervention causal inference task (Waldmann & Hagmayer paradigm adaptation)",
            "reasoning_task_description": "Subjects observe 20 cases of three variables A, B, C, are told a causal structure (common-cause A-&gt;B & A-&gt;C or causal-chain B-&gt;A-&gt;C), then asked how many of 20 new casks will have C when B is set to 1 by intervention (do(B=1)) or when B is observed to be 1.",
            "performance_by_method": "Reported patterns:\n- Common-cause condition: GPT-3 made interventional inferences that in some respects matched normative (do-based) prescriptions and predicted an increase in C after observing B=1 (consistent with both normative theory and humans for observation). However GPT-3 failed to reduce its prediction when observing B=0 (not normative and not human-like).\n- Causal-chain condition: GPT-3 produced the same predictions as in the common-cause condition (normatively incorrect, and inconsistent with human data which differ by condition). No precise numeric error rates given; results are described qualitatively with plots in the paper's Figure 5.",
            "comparison_of_methods": "Comparison shows mismatch: humans and normative models make different predictions for interventions versus observations depending on the causal graph (common-cause vs chain). GPT-3 did not reliably reflect these distinctions — its behavior did not change appropriately with specified causal structure. Thus, diverse causal-reasoning methods (observational-statistical vs true interventional/do()-based inference) were not robustly implemented.",
            "key_findings": "GPT-3 fails to consistently perform interventional causal reasoning that requires mechanistically deleting incoming edges (do-operator). Instances of correct interventional predictions in one condition appear accidental; GPT-3 does not systematically use causal-graph information to differentiate observation from intervention.",
            "counter_examples_or_negative_results": "Although GPT-3 sometimes matched normative interventional answers in the common-cause condition, its failures when B=0 and identical behavior across distinct causal-graph conditions are strong negative results indicating inability to perform robust causal reasoning via interventions.",
            "uuid": "e3324.4",
            "source_info": {
                "paper_title": "Using cognitive psychology to understand GPT-3",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Humans use directed and random exploration to solve the explore-exploit dilemma",
            "rating": 2
        },
        {
            "paper_title": "Model-based influences on humans' choices and striatal prediction errors",
            "rating": 2
        },
        {
            "paper_title": "Seeing versus doing: two modes of accessing causal knowledge",
            "rating": 2
        },
        {
            "paper_title": "Causality",
            "rating": 2
        },
        {
            "paper_title": "Using large-scale experiments and machine learning to discover theories of human decision-making",
            "rating": 2
        }
    ],
    "cost": 0.018460499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Using cognitive psychology to understand GPT-3</h1>
<p>Marcel Binz ${ }^{1, <em>}$ and Eric Schulz ${ }^{1}$<br>${ }^{1}$ MPRG Computational Principles of Intelligence, Max Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany<br></em>marcel.binz@tue.mpg.de</p>
<h4>Abstract</h4>
<p>We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.</p>
<h2>Introduction</h2>
<p>With the advent of increasingly capable artificial agents, comes the urgency to improve our understanding of how they learn and make decisions ${ }^{1}$. Take as an example large language models ${ }^{2}$. These models' abilities are, by many standards, impressive. They can generate text that human evaluators have difficulty distinguishing from text written by other humans ${ }^{2}$, generate computer code ${ }^{3}$, or converse with humans about a range of different topics ${ }^{4}$. What is perhaps even more surprising, is that these models' abilities go beyond mere language generation: they can, for instance, also play chess at a reasonable level ${ }^{5}$ and solve university-level math problems ${ }^{6}$. These observations have prompted some to argue that this new class of foundation models, which are models trained on broad data at scale and adapted to a wide range of downstream tasks, shows some form of general intelligence ${ }^{7}$. Yet others have been more skeptical, pointing out that these models are still a far cry away from a human-level understanding of language and semantics ${ }^{8}$. But how can we evaluate whether or not these models -at least in some situationslearn and think like people? One approach towards evaluating a model's human-likeness comes from cognitive psychology. Psychologists, after all, are experienced in trying to formally understand another notoriously impenetrable algorithm: the human mind.</p>
<p>In the present article, we investigate the Generative Pre-trained Transformer 3 model (or short: GPT-3) ${ }^{2}$ on several experiments taken from the cognitive psychology literature. Our analyses cover two types of experiments: vignette-based and task-based experiments. While vignette-based experiments involve a short and predefined description of a hypothetical scenario, task-based experiments are programmatically generated on a trail-by-trial basis. The selected tasks for both of these settings cover well-known areas of cognitive psychology: decision-making, information search, deliberation, and causal reasoning. We are primarily interested in whether GPT-3 can solve these tasks appropriately as well as how its behavior compares to human subjects. Our results show that GPT-3 can solve challenging vignette-based problems. Yet, we also highlight that these vignettes or similar texts might have been part of its training set. Moreover, we find that GPT-3's behavior strongly depends on how the vignettes are presented. Thus, we also subject GPT-3 to a battery of task-based problems. The results from these task-based assessments show that GPT-3 can make human-level decisions in both description-based and experience-based decision-making experiments, yet does not learn and explore in a human-like fashion. Furthermore, even though GPT-3 shows signatures of model-based reinforcement learning, it fails altogether in a causal reasoning task. Taken together, our results improve our understanding of current large language models, suggest ways in which they can be improved, and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.</p>
<h2>GPT-3</h2>
<p>GPT-3 is an auto-regressive language model ${ }^{2}$. It utilizes the transformer architecture ${ }^{9}$-a deep learning model that heavily relies on the mechanism of self-attention- to produce human-like text. Just like recurrent neural networks, transformers are designed to process sequential data, such as natural language. However, unlike recurrent neural networks, transformers process the entire data all at once, with the attention mechanism providing context for any position in the input sequence. The model</p>
<p>Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.</p>
<p>Q: Which option is the most probable?</p>
<ul>
<li>Option 1: Linda is a bank teller.</li>
<li>Option 2: Linda is a bank teller and is active in the feminist movement.</li>
<li>Option 3: Linda is a member of the NRA.</li>
</ul>
<p>A: Option</p>
<p>B
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Vignette-based tasks. A: Example prompt of hypothetical scenario, in this case the famous Linda problem, as submitted to GPT-3. B: Results. While in 12 out 12 standard vignettes, GPT-3 either answers correctly or makes human-like mistakes, it makes mistakes that are not human-like when given the adversarial vignettes.
itself is large, it has 175 billion parameters, and it was trained on a vast amount of text: hundreds of billions of words from the internet and books. GPT-3's architecture is similar to that of its predecessor, GPT-2 ${ }^{10}$, but contains many more trainable parameters. Thus, GPT-3 can be thought of as an experiment in massively scaling up known algorithms ${ }^{11}$. Larger models can capture more of the complexities of the data they were trained on and can transfer this knowledge to tasks that they have not been specifically trained for. Rather than being fine-tuned on a problem, these large language models can be given an instruction together with some examples of the task and identify what to do based on this alone. This is called "in-context learning" because the model picks up on patterns in its "context", for example, the string of words that the model is asked to complete. GPT-3 does incredibly well at in-context learning across a range of settings ${ }^{12}$, sometimes even performing at a level comparable to the best fine-tuned models ${ }^{2,13}$. Since GPT-3 is one of the biggest and most versatile large language models, it is a good candidate to be scrutinized using cognitive psychology.</p>
<h1>A cognitive psychology view on GPT-3</h1>
<p>We will subject GPT-3 to several tasks taken from the cognitive psychology literature. These tasks fall into four categories: 1. decision-making, 2. information search, 3. deliberation, and 4. causal reasoning. We will begin our investigations with several, classical vignette-based problems. For these vignette-based investigations, we confronted GPT-3 with text-based descriptions of hypothetical situations while collecting its responses. However, as we will point out, these vignettes have the problem that GPT-3 has likely experienced identical or similar such tasks in its training data. Moreover, we found that GPT-3's response can be tampered with just by marginally changing the vignettes and thereby creating adversarial situations. Thus, we also evaluated GPT-3's abilities in various task-based experiments. In these task-based investigations, we take canonical tasks from the literature and emulate their experimental structure as programmatically generated text to which GPT-3 responds on every experimental trial. We then use GPT-3's responses to analyze its behavior similar to how cognitive psychologists would analyze human behavior in the same tasks.</p>
<h2>Results</h2>
<p>We used the public OpenAI API to run all our simulations ${ }^{14}$. There are four GPT-3 models accessible through this API: "Ada", "Babbage", "Curie" and "Davinci" (sorted from the least to the most complex model). We focused our investigation on the most powerful of these models ("Davinci") unless otherwise noted. We furthermore set the temperature parameter to 0 , leading to deterministic answers, and kept the default values for all other parameters.</p>
<h1>Vignette-based investigations</h1>
<p>For the vignette-based investigations, we took canonical scenarios from the cognitive psychology literature, entered them as prompts into GPT-3, and recorded its answer. For each scenario, we report if GPT-3 responded correctly or not. Moreover, we classified each response as something a human could have said because it was either the correct response or a mistake commonly observed in human data. For cases where there were only two options, one correct and one that is normally chosen by human subjects, we added a third option that was neither correct nor plausibly chosen by people. The following subsections briefly summarize our main findings. We refer the reader to SI Appendix for a detailed description of the submitted prompts and GPT-3's corresponding answers.</p>
<h2>Decision-making: Heuristics and biases</h2>
<p>We began our investigations of GPT-3's decision-making by prompting the canonical "Linda problem" 15 (Linda, see Figure 1A). This problem has been known to assess the conjunction fallacy, a formal fallacy that occurs when it is assumed that specific conditions are more probable than a single general one. In the standard vignette, a hypothetical woman named Linda is described as "outspoken, bright, and politically active". Participants are then asked if it was more likely that Linda is a bank teller or that she is a bank teller and an active feminist. GPT-3, just like people, chose the second option, thereby falling for the conjunction fallacy.</p>
<p>Next, we prompted the so-called "cab problem" 16 (Cab, see SI Appendix), in which participants commonly fail to take the base rate of different colors of taxis in a city into account when judging the probability of the color of a cab that was involved in an accident. Unlike people, GPT-3 did not fall for the base-rate fallacy, i.e. to ignore the base rates of different colors, but instead provided the (approximately) correct answer.</p>
<p>Finally, we asked GPT-3 to provide an answer to the "hospital problem" 17 (Hospital, see SI Appendix), in which participants are asked which of two hospitals, a smaller or a larger one, is more likely to report more days on which more than $60 \%$ of all born children were boys. While the correct answer would be the smaller hospital (due to the larger variance of smaller samples), GPT-3, just like people, thought that the probability was about equal.</p>
<h2>Information search: Questions and hypothesis tests</h2>
<p>First, we assessed if GPT-3 can adaptively change between constraint-seeking vs. hypothesis-scanning questions. Constraintseeking questions target a feature shared by multiple objects, such as "Is the person female?", whereas hypothesis-scanning questions target a single object, such as "Is the person Linda?". Crucially, which type of question is more informative depends on past observations. Ruggeri et al. ${ }^{18}$ manipulated the particular reasons for why a fictitious character named Toma was repeatedly late to school (Toma, see SI Appendix). While for one group he was frequently late because his bicycle had broken, for the other group he was late for various reasons with half of them being that he could not find various objects. While trying to find out why Toma is late to school again, the first group should ask the hypothesis-scanning question "Was he late because his bicycle broke?", whereas the second group should ask the constraint-seeking question "Was he late because he could not find something?". GPT-3 picked the appropriate question in each scenario.</p>
<p>Secondly, we confronted GPT-3 with a scenario originally presented by Baron et al. ${ }^{19}$ in which subjects need to choose an appropriate test to discriminate between two illnesses (Test, see SI Appendix). Empirically, participants tend to choose the wrong test, likely because they overvalue questions that have a high probability of a positive result given the most likely hypothesis. GPT-3, just like people, fell for the same congruence bias.</p>
<p>Finally, we presented Wason's well-known "Card Selection Task"20 to GPT-3, explaining that the visible faces of four cards showed A, K, 4 and 7, and that the truth of the proposition "If a card shows a vowel on one face, then its opposite face shows an even number" needed to be tested (Wason, see SI Appendix). GPT-3 suggested to turn around A and 7, which is commonly accepted as the correct answer, even though most people turn around A and 4.</p>
<h2>Deliberation: The Cognitive Reflection Test</h2>
<p>We also tried to estimate GPT-3's tendency to override an incorrect fast response with answers derived by further deliberation. For this, we prompted the three items of the Cognitive Reflection Test ${ }^{21}$ (CRT1-CRT3, see SI Appendix). One example item of this task is: "If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?". While the initial response might be to say " 100 ", 100 machines would just be as fast as 5 machines and thus also take 5 minutes. For all three items of the CRT, GPT-3 responded with the intuitive but incorrect answer, as has been observed in earlier work ${ }^{22}$.</p>
<h2>Causal reasoning: Blickets, interventions, and counterfactuals</h2>
<p>We lastly assessed GPT-3's causal reasoning abilities. In a first test, we prompted GPT-3 with a version of the well-known "Blicket" experiment ${ }^{23}$ (Blicket, see SI Appendix). For this, blickets are introduced as objects that turn on a machine. Afterward, two objects are introduced. The first object turns on the machine on its own. The second machine does not turn on the machine on its own. Finally, both objects together turn on the machine. GPT-3, just like people, managed to correctly identify that the first but not the second object is a blicket.</p>
<p>In a second test, we asked GPT-3 to intervene in a scenario by removing the correct object to prevent an effect after having read about three different objects, one causing and two not causing the effect (in this case, an allergic reaction; Intervene, see SI Appendix). GPT-3 identified the correct object to be removed.</p>
<p>In the final test, we probed GPT-3's ability of mature causal reasoning ${ }^{24}$ (Mature, see SI Appendix). In this task, GPT-3 was told that there were four pills: A, B, C and D. While A and B individually could kill someone, C and D could not. GPT-3 successfully answered multiple questions about counterfactuals correctly, such as: "A man took pill B and pill C and he died. If he had not taken pill B , could he still have died?".</p>
<h1>Problems with vignette-based investigations</h1>
<p>Of the 12 vignette-based problems presented to GPT-3, it answered six correctly and all 12 in a way that could be described as human-like (Figure 1B). Does this mean that GPT-3 could pass as a human in a cognitive psychology experiment? We believe that the answer, based on the vignette-based tasks alone, has to be "No.". Since many of the prompted scenarios were taken from famous psychological experiments, there is a chance that GPT-3 has encountered these scenarios or similar ones in its training set. Moreover, in additional investigations, we found that many of the vignettes could be slightly modified, i.e., made into adversarial vignettes, such that GPT-3 would give vastly different responses. In the cab problem, for example, it is clearly stated that $15 \%$ of the cabs are blue and $85 \%$ are green. Yet asking GPT-3 about the probability that a cab involved in an accident was black, it responded with " $20 \%$ " (Black Cab, see SI Appendix). Simply changing the order of the options in Wason's card selection task from "A, K, 4, and 7" to "4, 7, A, and K" caused GPT-3 to suggest turning around "A" and "K" (Reverse Wason, see SI Appendix). Giving GPT-3 the first item of the CRT and stating that "The bat costs $\$ 1.00$ more than the bat.", it still thought that the ball was $\$ 0.10$ (Wrong CRT, see SI Appendix). Finally, when phrasing the mature causal reasoning problem as a "Blicket" problem in which machines could be turned on or off, GPT-3 answered some questions incorrectly while contradicting itself in its explanations (Immature Blicket, see SI Appendix). There have recently been other, much larger investigations using similar vignettes, whose results agree largely with our assessment ${ }^{25}$.</p>
<h2>Task-based investigations</h2>
<p>The results from the previous section indicate that GPT-3 can produce passable responses in some vignette-based tasks. It is, however, not possible to decide whether it is merely behaving like a parrot, repeating what it has seen in the training data, or whether it is reasoning successfully. We, therefore, next turned our lens of investigation to a more challenging setting and tested GPT-3 on actual, task-based experiments. In order to do so, we selected a set of four classical experiments that we believe to be representative of the cognitive psychology literature. For each of these, we programmatically generated a description that was entered as a prompt and -if there were multiple trials- updated the text with GPT-3's response and the received feedback.</p>
<h2>Decision-making: Decisions from descriptions</h2>
<p>How people make decisions from descriptions is one of the most well-studied areas of cognitive psychology, ranging from the early, seminal work of Kahneman \&amp; Tversky ${ }^{28}$ to modern, large-scale investigations ${ }^{26,27}$. In the decisions from descriptions paradigm, a decision-maker is asked to choose between one of two hypothetical gambles like the ones shown in Figure 2A. To test whether GPT-3 can reliably solve such problems, we presented the model with over 13,000 problems taken from a recent benchmark data-set ${ }^{26}$. Figure 2B shows the regret, which is defined as the difference between the expected outcome of the optimal option and that of the actually chosen option, obtained by different models in the GPT-3 family and compares their performance to human decisions. We found that only the largest of the GPT-3 models ("Davinci") was able to solve these problems above chance-level $(t(29134)=-16.85, p=&lt;.001)$, whereas the three smaller models did not (all $p&gt;0.05$ ). While the "Davinci" model did reasonably well, it did not reach human-level performance $(t(29134)=-11.50, p&lt;.001)$.</p>
<p>However, given that GPT-3 was not too far away from human performance, it is reasonable to ask whether the model also exhibited human-like, cognitive biases. In their original work on prospect theory, Kahneman \&amp; Tversky ${ }^{17}$ identified several biases of human decision-making by contrasting answers to multiple carefully selected problems pairs. We replicated the original analysis of Kahneman \&amp; Tversky using choice probabilities of GPT-3 and found that GPT-3 showed three of the six biases identified by Kahneman \&amp; Tversky. First, it displayed a framing effect, meaning that its preferences changed depending on whether a choice was presented in terms of gains or losses. GPT-3 was also subject to a certainty effect, meaning that it preferred guaranteed outcomes to risky ones even when they had slightly lower expected values. Finally, GPT-3 showed an overweighting bias and assigned higher importance to a difference between two small probabilities (e.g., $1 \%$ and $2 \%$ ) than to the same differences between two larger probabilities (e.g., $41 \%$ and $42 \%$ ). Figure 2C contains an analysis of these three biases and the three additional ones we did not find in GPT-3. For a detailed description of the conducted analysis, see SI Appendix.</p>
<h2>Information search: Directed and Random Exploration</h2>
<p>GPT-3 did well in the vignette-based information search tasks, so we were curious how it would fare in a more complex setting. The multi-armed bandit paradigm provides a suitable test-bed for this purpose. It extends the decisions from descriptions paradigm from the last section by adding two layers of complexity. First, the decision-maker is not provided with descriptions</p>
<p>Q: Which option do you prefer?</p>
<ul>
<li>Option F: 69.0 dollars with $1.0 \%$ chance, 26.0 dollars with $99.0 \%$ chance.</li>
<li>Option J: 2.0 dollars with $75.0 \%$ chance, 94.0 dollars with $25.0 \%$ chance.</li>
</ul>
<p>A: Option</p>
<p>B
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>C
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2. Decisions from descriptions. A: Example prompt of a problem provided to GPT-3. B: Mean regret averaged over all 13,000 problems taken from Peterson et al. ${ }^{26}$. Lower regret means better performance. Error bars indicate the standard error of the mean. C: Log-odds ratios of different contrasts used to test for cognitive biases. Positive values indicate that the given bias is present in humans (circle) or GPT-3 (triangle). Human data adapted from Ruggeri et al. ${ }^{27}$. For a detailed description of this analysis, see SI Appendix.
for each option anymore but has to learn their value from noisy samples, i.e. from experience ${ }^{29}$. Second, the interaction is not confined to a single choice but potentially involves repeated decisions about which option to sample. Together, these two modifications call for an important change in how a decision-maker must approach such problems. It is not enough to merely exploit currently available knowledge anymore, but also crucial to explore options that are unfamiliar and thereby gain information about their value. Previous research suggests that people solve this exploration-exploitation trade-off by applying a combination of two distinct strategies: directed and random exploration ${ }^{30}$. Whereas directed exploration encourages the decision-maker to collect samples from previously unexplored options, random exploration strategies inject some form of stochasticity into the decision process ${ }^{31,32}$.</p>
<p>Wilson's horizon task is the canonical experiment to test whether a decision-maker applies the two aforementioned forms of exploration ${ }^{30}$. It involves a series of two-armed bandit tasks, in each of which the decision-maker is provided with data from four forced-choice trials, followed by either one or six free-choice trials (referred to as the horizon). Forced-choice trials are used to control the amount of information available to the decision-maker. They either provide two observations for each option (equal information condition) or a single observation from one option and three from the other (unequal information condition). These two conditions make it possible to tease apart directed and random exploration by looking at the decision in the first free-choice trial. In the equal information condition, a choice is classified as random exploration if it corresponds to the option with the lower estimated mean. In the unequal information condition, a choice is classified as directed exploration if it corresponds to the option that was observed fewer times during the forced-choice trials. Note that short-horizon tasks do not benefit from making exploratory choices and, hence, we should expect the decision-maker to make fewer such choices in them.</p>
<p>You are going to a casino that owns two slot machines. You earn money each time you play on one of these machines.</p>
<p>You have received the following amount of dollars when playing in the past:</p>
<ul>
<li>Machine F delivered 51 dollars.</li>
<li>Machine J delivered 39 dollars.</li>
<li>Machine J delivered 40 dollars.</li>
<li>Machine J delivered 26 dollars.</li>
</ul>
<p>Your goal is to maximize the sum of received dollars within six additional rounds.</p>
<p>Q: Which machine do you choose?
A: Machine</p>
<p>C
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>B
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>D
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 3. Horizon task. A: Example prompt for one trial as submitted to GPT-3. B: Mean regret for GPT-3 and human subjects by horizon condition. Lower regret means better performance. Error bars indicate the standard error of the mean. Human data taken from Zaller et al. ${ }^{33}$. C: Probability of selecting option "J" in the equal information condition for both GPT-3 and human subjects by horizon condition. D: Probability of selecting the more informative option in the unequal information condition for both GPT-3 and human subjects by horizon condition.</p>
<p>We presented a text-based version of the horizon task as illustrated in Figure 3A to GPT-3. Figure 3B compares the model's regret to the regret of human subjects. For short-horizon tasks, GPT-3's performance was indistinguishable from human performance $(t(5566)=-0.043, p=.97)$. This result highlights that GPT-3 can not only make sensible decisions when presented with descriptions of options but is also able to integrate this information from noisy samples. The initial regret of GPT-3 in long-horizon tasks was significantly lower than the corresponding human regret $(t(5550)=-4.07, p&lt;.001)$ and was</p>
<p>only slightly above the one from short-horizon tasks. However, within each task people improved more than GPT-3 and reached a final regret that was slightly but not significantly lower than that of GPT-3 $(t(5550)=-0.75, p=.23)$. Looking at the entire experiment, GPT-3 $(M=2.72, S D=5.98)$ achieved a significantly lower regret than human subjects $(M=3.24, S D=10.26)$, $t(38878)=-5.03, p&lt;.001$.</p>
<p>To investigate how GPT-3 managed the trade-off between exploration and exploitation, we fitted a separate logistic regression model for each information condition. We used the estimated reward difference, horizon, their interaction, and a bias term as independent variables for both models. The model for the equal information condition used an indicator for selecting option J in the first free-choice trial as the dependent variable, whereas the model for the unequal condition used an indicator for selecting the more informative option (i.e., the one that has been observed fewer times during the forced-choice trials). The results of this regression analysis are summarized visually in Figure 3C and D. If GPT-3 applied random exploration, we should observe a positive effect of estimated reward difference. If its random exploration was furthermore strategic, we should find more noisy decisions in long-horizon tasks of the equal information condition (reflected in a negative interaction effect of estimated reward difference and horizon). People show both of these effects ${ }^{30}$. GPT-3 also displayed a significant effect of estimated reward difference ( $\beta=0.18 \pm 0.01, z=14.48, p&lt;.001$ ), suggesting that it used at least a rudimentary form of random exploration. However, we did not find a significant interaction effect between estimated reward difference and horizon ( $\beta=-0.02 \pm 0.02, z=-1.47, p=.14$ ), indicating that GPT-3 did not apply random exploration in a strategic way and simply ignored the information about the task horizon. If GPT-3 applied directed exploration, we should find a positive effect of horizon in the unequal information condition, indicating that more informative actions were sampled more frequently when the horizon was longer. While humans show such an effect ${ }^{30}$, we did not find it in GPT-3 ( $\beta=-0.15 \pm 0.27, z=-0.56, p=.58$ ), which indicates that the model also did not employ directed exploration.</p>
<p>Lastly, we found that GPT-3 had a tendency to repeat previously observed options. For example, in the unequal information condition (Figure 3D), GPT-3 showed a strong bias to select the option from which it had seen more samples, even when there was not reason to do so. We believe that this bias partially arose from how GPT-3 was trained: if the goal is to predict future words, and the agent has recently observed a certain phrase (in this case either "Option F" or "Option J"), it is likely that the same phrase will appear again in the near future. Interestingly, humans also show such a perseveration bias in many situations ${ }^{34}$, but, in the case of the horizon task, it seems to be overruled by other processes.</p>
<h1>Deliberation: Model-based and model-free reinforcement learning</h1>
<p>Many realistic sequential decision-making problems do not only require the decision-maker to keep track of reward probabilities, but also to learn how to navigate from state to state within an environment. Two modes of learning are plausible in such scenarios: model-free and model-based learning. Model-free learning -the more habitual mode of the two- stipulates that the decision-maker should adjust its strategy directly using the actually observed rewards. If something led to a good outcome, a model-free agent will do more of it; if it led to a bad outcome, a model-free agent will do less of it. Model-based learning -the more deliberate mode of the two- instead stipulates that the agent should explicitly learn the transition and reward probabilities of the environment and use them to update its strategy by reasoning about future outcomes.</p>
<p>These two modes of learning can be disentangled empirically in the two-step task paradigm ${ }^{35}$. The two-step task involves a series of two-stage decision problems. There are two actions available from the initial state: taking a spaceship to planet X or to planet Y. Taking a spaceship transfers the agent to a second stage. The spaceship arrives with a probability of 0.7 to the selected planet, and with a probability of 0.3 to the other planet. After arriving at one of these planets, the agent encounters two local aliens with which it can trade. Trading with an alien can lead to receiving treasures or junk. The probabilities of receiving treasures are initialized randomly from a uniform distribution with a minimum of 0.25 and a maximum value of 0.75 for each alien. While these probabilities drift slowly over time to encourage learning, the first-stage transition probabilities remain fixed throughout the entire experiment. Model-free learning predicts that the probability of the selected first-stage action should increase upon receiving treasures in the second stage, regardless of whether the decision-maker experienced a rare or a common first-stage transition. Model-based learning, on the other hand, predicts that, upon encountering a rare transition and receiving treasures, the probability of the selected first-stage action should decrease. SI Appendix contains plots of simulated behavior for the two learning strategies. People tend to solve this task using a combination of model-free and model-based learning ${ }^{35-37}$ as shown in Figure 4A.</p>
<p>We tested how GPT-3 learns in the two-step task by providing it with prompts like the one shown in Figure 4B. We ran 200 simulations in total and measured the stay probability of the first-stage action for each combination of transition (rare or common) and reward (treasures or junk). Each simulation involved 20 repetitions of the two stages. Figure 4C visualizes our results. We observed that the probability of repeating the previous first-stage action decreased after finding treasures through a rare transition $(t(1982)=-6.16, p&lt;.001)$. Meanwhile, the probability of repeating the same first-stage action increased after a rare and not rewarded action $(t(1814)=2.33, p=.01)$. These two findings suggest that GPT-3 relies on a deliberate model-based approach to solve the two-step task. Interestingly, this conclusion is at odds with our earlier simulations on the CRT, where GPT-3 consistently chose the intuitive but wrong over the more deliberate but correct answer. The contrast between</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>C
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>B</p>
<p>You will travel to foreign planets in search of treasures. When you visit a planet, you can choose an alien to trade with. The chance of getting treasures from these aliens changes over time. Your goal is to maximize the number of received treasures.</p>
<p>Your previous space travels went as follows:</p>
<ul>
<li>3 days ago, you boarded the spaceship to planet X , arrived at planet X , traded with alien D , and received treasures.</li>
<li>2 days ago, you boarded the spaceship to planet Y , arrived at planet X , traded with alien D , and received junk.</li>
<li>1 day ago, you boarded the spaceship to planet Y , arrived at planet Y , traded with alien K , and received junk.</li>
</ul>
<p>Q: Do you want to take the spaceship to planet X or planet Y ?
A: Planet X .</p>
<p>You arrive at planet X .</p>
<p>Q: Do you want to trade with alien D or F?
A: Alien</p>
<p>Figure 4. Two-step task. A: Human behavior in dependency of rewarded and unrewarded as well as common and rare transitions. Human data adapted from Daw et al. ${ }^{35}$. B: Example prompt of one trial in the canonical two-step task as submitted to GPT-3. C: GPT-3's behavior in dependency of rewarded and unrewarded as well as common and rare transitions. Error bars indicate the standard error of the mean.
those two analyses suggests that the answer to whether GPT-3 engages in deliberate reasoning might be more nuanced than initially thought.</p>
<h1>Causal reasoning: Interventions after passive observations</h1>
<p>The analysis of the two-step task indicated that GPT-3 can learn a model of the environment and use this learned model to update its strategy. In our final test, we wanted to analyze whether GPT-3 can also use such a model to make more complex inferences, such as reasoning about cause and effect. From our earlier vignette-based investigations, we have already learned that GPT-3 can solve some causal reasoning problems, although these results depended heavily on how the problems were presented.</p>
<p>Perhaps the most crucial insight of theories of causal reasoning is that there is a difference between merely observing variables and actively manipulating them. Take, for instance, the classical example of a barometer. Under normal circumstances, barometer measurements provide insights into the upcoming weather. However, if someone would manually set the scale of the barometer to a particular value, then it would become totally uninformative about the weather - a clear difference from the observational inference. Waldman \&amp; Hagmayer ${ }^{38}$ devised an experiment to highlight that people are sensitive to the difference between seeing and doing. They first presented subjects with 20 observations of a three-variable system, and then provided additional information about the causal structure of the system. In the common-cause condition, they told participants that $A$ causes both $B$ and $C(B \leftarrow A \rightarrow C)$. In the causal-chain condition, they inverted the causal direction of $A$ and $B$, such that $B$ now causes $A$, which, as before, causes $C(B \rightarrow A \rightarrow C)$. Finally, they asked their subjects to imagine 20 new observations</p>
<p>You have previously observed the following chemical substances in different wine casks:</p>
<ul>
<li>Cask 1: substance $A$ was present, substance $B$ was present, substance $C$ was present.</li>
<li>Cask 2: substance $A$ was present, substance $B$ was present, substance $C$ was present.
$[\ldots]$</li>
<li>Cask 20: substance $A$ was absent, substance $B$ was absent, substance $C$ was absent.</li>
</ul>
<p>You have the following additional information from previous research:</p>
<ul>
<li>Substance $A$ likely causes the production of substance $B$.</li>
<li>Substance $A$ likely causes the production of substance $C$.</li>
</ul>
<p>Imagine that you test 20 new casks in which you have manually added substance $B$.</p>
<p>Q: How many of these new casks will contain substance $C$ on average?
A: [insert] casks.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 5. Causal reasoning. A: Example prompt for the causal reasoning task adapted from Waldman et al. ${ }^{38}$. B: Simulation results comparing GPT-3's responses with people as well as the ideal agent in the common-cause condition. C: Simulation results comparing GPT-3's responses with people as well as the ideal agent in the causal-chain condition.
for which they either had actively intervened on the values of $B$ or for which they merely had observed a particular value of $B$. Participants had to report for how many of these 20 new observations variable $C$ would be active. Like in the barometer example, observing an active value of $B$ in the common-cause condition enabled participants to make the inference that $A$ was likely to be active as well, which, in turn, made it more likely that $C$ was also active. In contrast, activating $B$ by means of interventions did not allow for such an inference. Mathematically, the act of intervening can be formalized by Pearl's $d o()$ operator ${ }^{39}$, which sets a variable to a particular value but deletes all arrows going into that variable from the causal graph. For the causal-chain condition one therefore would expect to find no differences between intervening and observing, as there was no arrow going into $B$ that had to be deleted, and hence both inferences were identical.</p>
<p>We probed GPT-3's ability to make causal inferences in this task using a cover story about substances found in different wine casks ${ }^{40}$ (see Figure 5A). When provided with the additional information about the common-cause structure, GPT-3 made interventional inferences that matched the normative prescription of causal inference as illustrated in Figure 5B. GPT-3 furthermore predicted an increase in the number of observations with $C=1$ after observing $B=1$, which was in line with both the normative theory and human judgments. However, when observing $B=0$, GPT-3 did not reduce its prediction, which was neither the correct inference nor human-like. The causal-chain condition does not lead to a difference between observational and interventional inferences from a normative perspective. While human subjects show exactly this pattern ${ }^{38}$, GPT-3 made identical predictions compared to the common-cause condition as illustrated in Figure 5C. This observation suggests that the model was not able to incorporate the additional information about the underlying causal structure into its inference process and therefore makes it likely that the results from the common-cause condition were purely accidental. Taken together, these results suggest that GPT-3 has difficulties with causal reasoning in tasks that go beyond a vignette-based characterization.</p>
<h1>Discussion</h1>
<p>In 1904, sixteen leading academics of the Prussian Academy of Sciences signed a statement indicating that a horse, named "Clever Hans", could solve mathematical problems at a human-like level. Back then, it took another scientist, Oskar Pfungst, years of systematic investigations to prove that the horse was merely reacting to the people who were watching him ${ }^{41}$. With the advent of large-scale machine learning models, the risks of over-interpreting simple behaviors as intelligent runs rampant. The abilities of large language models, in particular the ability to solve tasks beyond language generation, are impressive at first glance. These models have, therefore, been called many things; some think they are sentient ${ }^{42}$ and that they show a form of general intelligence ${ }^{7}$. Yet others believe that they are merely stochastic parrots ${ }^{43}$ or a linguistic one-trick pony ${ }^{8}$. But how can we realistically gauge these models' abilities?</p>
<p>We have argued to approach this problem similar to how Oskar Pfungst approached his object of study: via systematic investigations and psychological experimentation. Using tools from cognitive psychology, we have subjected one particular large language model, GPT-3, to a series of investigations, probing its decision-making, information search, deliberation, and causal reasoning abilities. Our results have shown that GPT-3 can solve some vignette-based experiments similarly or better than human subjects. However, interpreting these results is difficult because many of these vignettes might have been part of its training set, and GPT-3's performance suffered greatly given only minor changes to the original vignettes. We, therefore, turned the lens of our investigations to task-based assessments of GPT-3's abilities. Therein, we found that GPT-3 made reasonable decisions for gambles provided as descriptions while also mirroring some human behavioral biases. GPT-3 also managed to solve a multi-armed bandit task well, where it performed better than human subjects; yet it only showed traces of random but not of directed exploration. In the canonical two-step decision-making task, GPT-3 showed signatures of model-based reinforcement learning. However, GPT-3 failed spectacularly in using an underlying causal structure for its inference, leading to responses that were neither correct nor human-like.</p>
<p>What do we make of GPT-3's performance in our tasks? We believe that GPT-3's performance contained both surprising and expected elements. We found it surprising that GPT-3 could solve many of the provided tasks reasonably well, that it performed well in gambles, a simple bandit task, and even showed signatures of model-based reinforcement learning. These findings could indicate that -at least in some instances-GPT-3 is not just a stochastic parrot and could pass as a valid subject for some of the experiments we have administered. Yet what was not surprising were some of GPT-3's failure cases. GPT-3 did not show any signatures of directed exploration. We believe that this is intuitive and can be explained by the differences in how humans and GPT-3 learn about the world. Whereas humans learn by connecting with other people, asking them questions, and actively engaging with their environments, large language models learn by being passively fed a lot of text and predicting what word comes next. GPT-3 also failed to learn about and use causal knowledge in a simple reasoning task. Causal reasoning is frequently seen as a pillar of intelligent behavior ${ }^{44}$ and has been difficult to master for artificial agents ${ }^{45}$. We believe it makes sense that GTP-3 struggles to reason causally because acquiring knowledge about interventions from passive streams of data is hard to impossible ${ }^{46}$. The upside of our findings is the recommendation that to create more intelligent agents researchers should not only scale up algorithms that are passively fed with data but instead let agents directly interact and engage with the world ${ }^{47}$.</p>
<p>We are not the first to probe large-scale machine learning models' abilities. Indeed, recently there has been a push towards creating large benchmarks to assess the capability of foundation models ${ }^{48-50}$. Large language models have also been studied using other methods from cognitive psychology, such as property induction ${ }^{51}$, thinking-out-loud protocols ${ }^{52}$, or learning causal over-hypotheses ${ }^{53}$, where researchers have come to similar conclusions. Methods from cognitive psychology have also previously been applied to understand other deep learning models' behavior ${ }^{54}$. Therefore, our current work can be seen as part of a larger scientific movement where methods from psychology are becoming increasingly more important to understand capable black-box algorithms' learning and decision-making processes ${ }^{55-58}$.</p>
<p>Although we consider the present work as a step towards a psychological understanding of foundation models, several shortcomings remain. First of all, as we have seen in our vignette-based assessment, GPT-3's responses often times crucially depend on how a prompt is presented. The same might hold for our task-based assessments, where it is conceivable that GPT-3's behavior could change if the generating program of the tasks was modified. Yet we have simply tried to show that -in principle- GPT-3 could solve some of these tasks and believe that our current results emphasize the differences between GPT-3 and humans well. Secondly, we have only focused on a rather small subset of cognitive tasks, where we have tried to cover informative ground about GPT-3's abilities. Futures investigations could focus on additional psychological domains such as category learning, problem-solving, or economic games, to name but a few. Finally, our current results run the risk of portraying GPT-3 as more intelligent than it actually is, simply because canonical tasks taken from the psychological literature might be too easy to solve. In that sense, showing that large language models can perform well in such tasks might tell us more about how solvable and perhaps overly simplistic some tasks are than about GPT-3 itself and point to the importance of using more complex paradigms to study both natural and artificial agents ${ }^{59,60}$.</p>
<p>To summarize, we studied GPT-3, a recent large-scale language model, using tools from cognitive psychology. We assessed GPT-3's decision-making, information search, deliberation, and causal reasoning abilities, and found that it was able to solve</p>
<p>most of the presented tasks at a decent level. Less than two years ago, the sheer fact that a general-purpose language model could give reasonable responses to our problems would have been a large surprise. From this perspective, our analysis highlights how far these models have come. Nevertheless, we also found that small perturbations to the provided prompts easily led GPT-3 astray and that it lacks important features of human cognition, such as directed exploration and causal reasoning. While it does not seems so far-fetched that even larger models could acquire more robust and sophisticated reasoning abilities, we ultimately believe that actively interacting with the world will be crucial for matching the full complexity of human cognition. Fortunately, many user already interact with GPT-3-like models, and this number is only increasing with new applications on the horizon. Future language models will likely be trained on this data, leading to a natural interaction loop between artificial and natural agents.</p>
<h1>References</h1>
<ol>
<li>Gunning, D. et al. Xai—explainable artificial intelligence. Sci. Robotics 4, eaay7120 (2019).</li>
<li>Brown, T. et al. Language models are few-shot learners. Adv. neural information processing systems 33, 1877-1901 (2020).</li>
<li>Chen, M. et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).</li>
<li>Lin, Z. et al. Caire: An end-to-end empathetic chatbot. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 13622-13623 (2020).</li>
<li>Noever, D., Ciolino, M. \&amp; Kalin, J. The chess transformer: Mastering play using generative language models. arXiv preprint arXiv:2008.04057 (2020).</li>
<li>Drori, I. et al. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. arXiv preprint arXiv:2112.15594 (2021).</li>
<li>Chalmers, D. Gpt-3 and general intelligence. Dly. Nous, July 30 (2020).</li>
<li>Marcus, G. \&amp; Davis, E. Gpt-3, bloviator: Openai's language generator has no idea what it's talking about. Technol. Rev. (2020).</li>
<li>Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).</li>
<li>Radford, A., Narasimhan, K., Salimans, T. \&amp; Sutskever, I. Improving language understanding by generative pre-training. None (2018).</li>
<li>Sutton, R. The bitter lesson. Incomplete Ideas (blog) 13, 12 (2019).</li>
<li>Liu, J. et al. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804 (2021).</li>
<li>Lampinen, A. K. et al. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329 (2022).</li>
<li>OpenAI API. https://beta.openai.com/overview. Accessed: 2022-06-20.</li>
<li>Tversky, A. \&amp; Kahneman, D. Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Psychol. review 90, 293 (1983).</li>
<li>Tversky, A. \&amp; Kahneman, D. Causal schemas in judgments under uncertainty. Prog. social psychology 1, 49-72 (2015).</li>
<li>Kahneman, D. \&amp; Tversky, A. Subjective probability: A judgment of representativeness. Cogn. psychology 3, 430-454 (1972).</li>
<li>Ruggeri, A., Sim, Z. L. \&amp; Xu, F. "why is toma late to school again?" preschoolers identify the most informative questions. Dev. psychology 53, 1620 (2017).</li>
<li>Baron, J., Beattie, J. \&amp; Hershey, J. C. Heuristics and biases in diagnostic reasoning: Ii. congruence, information, and certainty. Organ. Behav. Hum. Decis. Process. 42, 88-110 (1988).</li>
<li>Wason, P. C. Reasoning about a rule. Q. journal experimental psychology 20, 273-281 (1968).</li>
<li>Frederick, S. Cognitive reflection and decision making. J. Econ. perspectives 19, 25-42 (2005).</li>
<li>Nye, M., Tessler, M., Tenenbaum, J. \&amp; Lake, B. M. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Adv. Neural Inf. Process. Syst. 34, 25192-25204 (2021).</li>
<li>Sobel, D. M., Yoachim, C. M., Gopnik, A., Meltzoff, A. N. \&amp; Blumenthal, E. J. The blicket within: Preschoolers' inferences about insides and causes. J. Cogn. Dev. 8, 159-182 (2007).</li>
<li>
<p>Nyhout, A. \&amp; Ganea, P. A. Mature counterfactual reasoning in 4-and 5-year-olds. Cognition 183, 57-66 (2019).</p>
</li>
<li>
<p>Srivastava, A. et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, DOI: 10.48550/ARXIV.2206.04615 (2022).</p>
</li>
<li>Peterson, J. C., Bourgin, D. D., Agrawal, M., Reichman, D. \&amp; Griffiths, T. L. Using large-scale experiments and machine learning to discover theories of human decision-making. Science 372, 1209-1214 (2021).</li>
<li>Ruggeri, K. et al. Replicating patterns of prospect theory for decision under risk. Nat. human behaviour 4, 622-633 (2020).</li>
<li>Kahneman, D. Prospect theory: An analysis of decisions under risk. Econometrica 47, 278 (1979).</li>
<li>Hertwig, R., Barron, G., Weber, E. U. \&amp; Erev, I. Decisions from experience and the effect of rare events in risky choice. Psychol. science 15, 534-539 (2004).</li>
<li>Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A. \&amp; Cohen, J. D. Humans use directed and random exploration to solve the explore-exploit dilemma. J. Exp. Psychol. Gen. 143, 2074 (2014).</li>
<li>Gershman, S. J. Deconstructing the human algorithms for exploration. Cognition 173, 34-42 (2018).</li>
<li>Schulz, E. \&amp; Gershman, S. J. The algorithmic architecture of exploration in the human brain. Curr. opinion neurobiology 55, 7-14 (2019).</li>
<li>Zaller, I., Zorowitz, S. \&amp; Niv, Y. Information seeking on the horizons task does not predict anxious symptomatology. Biol. Psychiatry 89, S217-S218 (2021).</li>
<li>Gershman, S. J. Origin of perseveration in the trade-off between reward and complexity. Cognition 204, 104394 (2020).</li>
<li>Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P. \&amp; Dolan, R. J. Model-based influences on humans' choices and striatal prediction errors. Neuron 69, 1204-1215 (2011).</li>
<li>Gläscher, J., Daw, N., Dayan, P. \&amp; O’Doherty, J. P. States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning. Neuron 66, 585-595 (2010).</li>
<li>Kool, W., Cushman, F. A. \&amp; Gershman, S. J. Competition and cooperation between multiple reinforcement learning systems. Goal-directed decision making 153-178 (2018).</li>
<li>Waldmann, M. R. \&amp; Hagmayer, Y. Seeing versus doing: two modes of accessing causal knowledge. J. Exp. Psychol. Learn. Mem. Cogn. 31, 216 (2005).</li>
<li>Pearl, J. Causality (Cambridge university press, 2009).</li>
<li>Meder, B., Hagmayer, Y. \&amp; Waldmann, M. R. Inferring interventional predictions from observational learning data. Psychon. Bull. \&amp; Rev. 15, 75-80 (2008).</li>
<li>Pfungst, O. Das Pferd des Herrn von Osten: der kluge Hans. Ein Beitrag zur experimentellen Tier-und MenschenPsychologie (Barth, 1907).</li>
<li>Luscombe, R. Google engineer put on leave after saying ai chatbot has become sentient. The Guard. .</li>
<li>Bender, E. M., Gebru, T., McMillan-Major, A. \&amp; Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610-623 (2021).</li>
<li>Schölkopf, B. Causality for machine learning. In Probabilistic and Causal Inference: The Works of Judea Pearl, 765-804 (2022).</li>
<li>Lakretz, Y., Desbordes, T., Hupkes, D. \&amp; Dehaene, S. Causal transformers perform below chance on recursive nested constructions, unlike humans. arXiv preprint arXiv:2110.07240 (2021).</li>
<li>Dasgupta, I. et al. Causal reasoning from meta-reinforcement learning. arXiv preprint arXiv:1901.08162 (2019).</li>
<li>Hill, F. et al. Environmental drivers of systematicity and generalization in a situated agent. In International Conference on Learning Representations (2020).</li>
<li>Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).</li>
<li>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. \&amp; Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).</li>
<li>Collins, K. M., Wong, C., Feng, J., Wei, M. \&amp; Tenenbaum, J. B. Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. arXiv preprint arXiv:2205.05718 (2022).</li>
<li>
<p>Han, S. J., Ransom, K., Perfors, A. \&amp; Kemp, C. Human-like property induction is a challenge for large language models. PsyArXiv (2022).</p>
</li>
<li>
<p>Betz, G., Richardson, K. \&amp; Voigt, C. Thinking aloud: Dynamic context generation improves zero-shot reasoning performance of gpt-2. arXiv preprint arXiv:2103.13033 (2021).</p>
</li>
<li>Kosoy, E. et al. Towards understanding how machines can learn causal overhypotheses, DOI: 10.48550/ARXIV.2206.08353 (2022).</li>
<li>Ritter, S., Barrett, D. G., Santoro, A. \&amp; Botvinick, M. M. Cognitive psychology for deep neural networks: A shape bias case study. In International conference on machine learning, 2940-2949 (PMLR, 2017).</li>
<li>Rich, A. S. \&amp; Gureckis, T. M. Lessons for artificial intelligence from the study of natural stupidity. Nat. Mach. Intell. 1, 174-180 (2019).</li>
<li>Rahwan, I. et al. Machine behaviour. Nature 568, 477-486 (2019).</li>
<li>Schulz, E. \&amp; Dayan, P. Computational psychiatry for computers. Iscience 23, 101772 (2020).</li>
<li>Schramowski, P., Turan, C., Andersen, N., Rothkopf, C. A. \&amp; Kersting, K. Large pre-trained language models contain human-like biases of what is right and wrong to do. Nat. Mach. Intell. 4, 258-268 (2022).</li>
<li>Ma, W. J. \&amp; Peters, B. A neural network walks into a lab: towards using deep nets as models for human behavior. arXiv preprint arXiv:2005.02181 (2020).</li>
<li>Brändle, F., Binz, M. \&amp; Schulz, E. Exploration beyond bandits (2021).</li>
</ol>
<h1>SI Appendix</h1>
<h2>Vigettes</h2>
<p>Tables 1, 2, 3, 4 and 5 contain a detailed description of submitted prompts and GPT-3's corresponding answers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Linda</td>
<td style="text-align: center;">Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. <br> Q: Which option is the most probable? <br> - Option 1: Linda is a bank teller. <br> - Option 2: Linda is a bank teller and is active in the feminist movement. <br> - Option 3: Linda is a member of the NRA.</td>
<td style="text-align: center;">A: Option 2.</td>
</tr>
<tr>
<td style="text-align: center;">Cab</td>
<td style="text-align: center;">A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: $85 \%$ of the cabs in the city are Green and $15 \%$ are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors $80 \%$ of the time and failed $20 \%$ of the time. <br> Q: What is the probability that the cab involved in the accident was Blue rather than Green?</td>
<td style="text-align: center;">A: The probability that the cab involved in the accident was Blue rather than Green is 0.4 .</td>
</tr>
<tr>
<td style="text-align: center;">Hospital</td>
<td style="text-align: center;">A certain town is served by two hospitals. In the larger hospital, about 45 babies are born each day, and in the smaller hospital, about 15 babies are born each day. As you know, about $50 \%$ of all babies are boys. The exact percentage of baby boys, however, varies from day to day. Sometimes it may be higher than $50 \%$, sometimes lower. For a period of 1 year, each hospital recorded the days on which more than $60 \%$ of the babies born were boys. Q: Which hospital do you think recorded more such days? <br> - Option 1: The larger hospital. <br> - Option 2: The smaller hospital. <br> - Option 3: About the same (i.e., within 5\% of each other).</td>
<td style="text-align: center;">A: Option 3.</td>
</tr>
</tbody>
</table>
<p>Table 1. Prompts and answers for vignettes testing GPT-3's decision-making.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Toma <br> v1</td>
<td style="text-align: left;">In the last 6 days, Toma always arrived late to school. <br> On the first day, Toma was late because he could not find his jacket. <br> On the second day, Toma was late because his bicycle broke. <br> On the third day, Toma was late because his bicycle broke. <br> On the fourth day, Toma was late because he could not find his shoes. <br> On the fifth day, Toma was late because he was watching TV. <br> On the sixth day, Toma was late because he could not find his books. <br> Today, on the seventh day, Toma was again late to school.</td>
<td style="text-align: left;">A: Option 1.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Q: Which of the following question would be better if wanted to find out why Toma was again late <br> to school. <br> -Option 1: Were you late because you could not find something? <br> -Option 2: Were you late because your bicycle was broken?</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Toma <br> v2</th>
<th style="text-align: center;">In the last 8 days, Toma always arrived late to school. <br> On the first day, Toma was late because he could not find his jacket. <br> On the second day, Toma was late because his bicycle broke. <br> On the third day, Toma was late because his bicycle broke. <br> On the fourth day, Toma was late because he could not find his shoes. <br> On the fifth day, Toma was late because his bicycle broke. <br> On the sixth day, Toma was late because his bicycle broke. <br> On the seventh day, Toma was late because he was watching TV. <br> On the eighth day, Toma was late because his bicycle broke. <br> Today, on the ninth day, Marcel was again late to school. <br> Q: Which of the following question would be better if wanted to find out why Toma was again late to school. <br> -Option 1: Were you late because you could not find something? <br> -Option 2: Were you late because your bicycle was broken?</th>
<th style="text-align: center;">A: Option 2.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">A patient has a 0.8 probability of having Chamber-of-Commerce disease and a 0.2 probability of Elk's disease. (He surely has one or the other.) A tetherscopic examination yields a positive result in $90 \%$ of patients with Chamber-of-Commerce disease and in $20 \%$ of patients without it (including those with some other disease). An intraocular smear yields a positive result in $90 \%$ of patients with Elk's disease and in $10 \%$ of patients without it. <br> Q: Which test would you run on the patient? <br> - Option 1: A tetherscopic examination. <br> - Option 2: An intraocular smear. <br> - Option 3: Both are equally good.</td>
<td style="text-align: center;">A: Option 1.</td>
</tr>
<tr>
<td style="text-align: center;">Wason</td>
<td style="text-align: center;">You are shown a set of four cards placed on a table, each of which has a number on one side and a colored patch on the other side. The visible faces of the cards show A, K, 4, and 7. <br> Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face, then its opposite face shows an even number?</td>
<td style="text-align: center;">A: The cards that must be turned over are the A and the 7 .</td>
</tr>
</tbody>
</table>
<p>Table 2. Prompts and answers for vignettes testing GPT-3's information search abilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CRT1</td>
<td style="text-align: left;">A bat and a ball cost 1.10intotal. Thebatcosts1.00 more than the ball. <br> Q: How much does the ball cost?</td>
<td style="text-align: left;">A: The ball costs $\$ 0.10$.</td>
</tr>
<tr>
<td style="text-align: left;">CRT2</td>
<td style="text-align: left;">Q: If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to <br> make 100 widgets?</td>
<td style="text-align: left;">A: 100 machines would <br> make 100 widgets in 100 <br> minutes.</td>
</tr>
<tr>
<td style="text-align: left;">CRT3</td>
<td style="text-align: left;">In a lake, there is a patch of lily pads. Every day, the patch doubles in size. <br> Q: If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to <br> cover half of the lake?</td>
<td style="text-align: left;">A: It would take 24 days <br> for the patch to cover half <br> of the lake.</td>
</tr>
</tbody>
</table>
<p>Table 3. Prompts and answers for vignettes testing GPT-3's intuitive vs. reflective answers. Items are based on the Cognitive Reflection Test (CRT).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Blickets</th>
<th style="text-align: center;">Blickets are objects that turn on a machine. I have two objects, A and B, and a machine. My goal is to find out if any or both of these objects are a blicket. When I put object A on the machine, the machine turned on. When I put object A and object B on the machine, the machine turned on. When I put object B on the machine, the machine did not turn on. <br> Q: Do you think object A is a blicket? <br> Q: Do you think object B is a blicket? <br> Q: Why is object B not a blicket?</th>
<th style="text-align: center;">A: Yes, I think object A is a blicket. <br> A: No, I think object B is not a blicket. <br> A: I think object B is not a blicket because it did not turn on the machine when it was placed on the machine by itself.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Intervene</td>
<td style="text-align: center;">I am allergic to flowers. When my friend gave me red flowers, I did not sneeze. When my friend gave me yellow flowers, I did not sneeze. When my friend gave me blue flowers, I sneezed. <br> Q: Imagine a bouquet containing red, yellow, and blue flowers. Which flowers would need to be removed such that I would not show an allergic reaction?</td>
<td style="text-align: center;">A: The blue flowers would need to be removed.</td>
</tr>
<tr>
<td style="text-align: center;">Mature</td>
<td style="text-align: center;">Imagine there are four different pills: pill A, pill B, pill C, and pill D. Pill A can kill people. Pill B can kill people. Pill C cannot kill people. Pill D cannot kill people. <br> Q: A man took pill A and pill B and he died. If he had not taken pill A, could he still have died, yes or no? <br> Q: A man took pill B and pill C and he died. If he had not taken pill B, could he still have died, yes or no? <br> Q: A man took pill A and pill D and he died. If he had not taken pill D, could he still have died, yes or no? <br> Q: A man took pill C and pill D and he did not die. If he had not taken pill D, could he have died, yes or no?</td>
<td style="text-align: center;">A: Yes <br> A: No <br> A: Yes <br> A: No</td>
</tr>
</tbody>
</table>
<p>Table 4. Prompts and answers for vignettes testing GPT-3's causal reasoning abilities. In cases for which there are more than one Q , questions were asked in succession.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Black <br> Cab</td>
<td style="text-align: center;">A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: $85 \%$ of the cabs in the city are Green and $15 \%$ are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors $80 \%$ of the time and failed $20 \%$ of the time. <br> Q: What is the probability that the cab involved in the accident was black?</td>
<td style="text-align: center;">A: The probability that the cab involved in the accident was black is 0.2 .</td>
</tr>
<tr>
<td style="text-align: center;">Reverse <br> Wason</td>
<td style="text-align: center;">You are shown a set of four cards placed on a table, each of which has a number on one side and a colored patch on the other side. The visible faces of the cards show $4,7, \mathrm{~A}$, and K . <br> Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face, then its opposite face shows an even number?</td>
<td style="text-align: center;">A: The A and the K.</td>
</tr>
<tr>
<td style="text-align: center;">Wrong <br> CRT</td>
<td style="text-align: center;">A bat and a ball cost 1.10 intotal. Thebatcosts 1.00 more than the bat. <br> Q: How much does the ball cost?</td>
<td style="text-align: center;">A: The ball costs $\$ 0.10$.</td>
</tr>
<tr>
<td style="text-align: center;">Immature <br> Blicket</td>
<td style="text-align: center;">Imagine a box that makes a sound if certain objects are placed on it. When I put a red object on it, the box made a sound. When I put a green object on it, the box made a sound. When I put a blue object on it, the box did not make a sound. When I put a yellow object on it, the box did not make a sound. <br> Q: I put a green and a yellow object on the box and it made a sound. Would it still have made a sound if I had not put the yellow object on it? <br> Q: Would the box make a sound if I only put a green object on it? <br> Q: But if there are two objects on the box, a green and a yellow one, and I remove the yellow one, then there would only be a green object on it. Why does that not turn on the machine?</td>
<td style="text-align: center;">A: No, the box would not have made a sound if you had not put the yellow object on it. <br> A: Yes, the box would make a sound if you only put a green object on it. <br> A: The box would not make a sound if there is only a green object on it.</td>
</tr>
</tbody>
</table>
<p>Table 5. Prompts and answers for vignettes deliberately designed to confuse GPT-3. In cases for which there are more than one Q , questions were asked in succession.</p>
<h1>Decisions from descriptions</h1>
<p>Following Kahneman \&amp; Tversky ${ }^{28}$, we tested whether a cognitive bias is present by contrasting answers to two decisions from description problems. The full list of these problems can be found in Table 6. Table 7 shows a list of used contrasts. For each contrast, we obtained the probability of selecting option F (setting GPT-3's temperature parameter to 1) and then computed the log-odds ratio between the choice probabilities of both questions. The order of presented options was counterbalanced. Positive log-odds ratios indicate that the given bias is present.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">$p(\mathrm{~F})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $33 \%$ chance at 2,500 dollars, a $66 \%$ chance at 2,400 dollars, and a $1 \%$ chance of 0 dollars. <br> - Option J: Guaranteed 2,400 dollars. <br> A: Option</td>
<td style="text-align: center;">0.20455745</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $33 \%$ chance of 2,500 dollars ( $67 \%$ chance of 0 dollars). <br> - Option J: $34 \%$ chance of 2,400 dollars ( $66 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.38613685</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $80 \%$ chance of 4,000 dollars ( $20 \%$ chance of 0 dollars). <br> - Option J: $100 \%$ guarantee of 3,000 dollars. <br> A: Option</td>
<td style="text-align: center;">0.27955511</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $20 \%$ chance of 4,000 dollars ( $80 \%$ chance of 0 dollars). <br> - Option J: $25 \%$ chance of 3,000 dollars ( $75 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.22312672</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $45 \%$ chance of 6,000 dollars ( $55 \%$ chance of 0 dollars). <br> - Option J: $90 \%$ chance of 3,000 dollars ( $10 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.05952152</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance of 6,000 dollars ( $99.9 \%$ chance of 0 dollars). <br> - Option J: $0.2 \%$ chance of 3,000 dollars ( $99.8 \%$ chance of 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.4925257</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $80 \%$ chance of losing 4,000 dollars ( $20 \%$ chance of losing 0 dollars). <br> - Option J: $100 \%$ guarantee of losing 3,000 dollars. <br> A: Option</td>
<td style="text-align: center;">0.69021772</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $20 \%$ chance of losing 4,000 dollars ( $80 \%$ chance of losing 0 dollars). <br> - Option J: $25 \%$ chance of losing 3,000 dollars ( $75 \%$ chance of losing 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.37691269</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $45 \%$ chance of losing 6,000 dollars ( $55 \%$ chance of losing 0 dollars). <br> - Option J: $90 \%$ chance of losing 3,000 dollars ( $10 \%$ chance of losing 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.49701181</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance of losing 6,000 dollars ( $99.9 \%$ chance of losing 0 dollars). <br> - Option J: $0.2 \%$ chance of losing 3,000 dollars ( $99.8 \%$ chance of losing 0 dollars). <br> A: Option</td>
<td style="text-align: center;">0.34726597</td>
</tr>
</tbody>
</table>
<p>| 11 | Imagine you are playing a game with two levels, but you have to make a choice about the second level before you know the outcome of the first. At the first level, there is a $75 \%$ chance that the game will end without you winning anything, and a $25 \%$ chance that you will advance to the second level.
Q: What would you choose in the second level? <br> - Option F: $80 \%$ chance of 4,000 dollars ( $20 \%$ chance of 0 dollars). <br> - Option J: $100 \%$ guarantee of 3,000 dollars. <br> A: Option | 0.66757223 |
| :--: | :--: | :--: |
| 12 | Imagine we gave you 1,000 dollars right now to play a game. <br> Q: Which option do you prefer? <br> - Option F: $50 \%$ chance to gain an additional 1,000 dollars ( $50 \%$ chance of gaining 0 dollars beyond what you already have). <br> - Option J: $100 \%$ guarantee of gaining an additional 500 dollars. <br> A: Option | 0.51510189 |
| 13 | Imagine we gave you 2,000 dollars right now to play a game. <br> Q: Which option do you prefer? <br> - Option F: $50 \%$ chance you will lose 1,000 dollars ( $50 \%$ chance of losing 0 dollars). <br> - Option J: $100 \%$ chance you will lose 500 dollars. <br> A: Option | 0.61653453 |
| 14 | Q: Which option do you prefer? <br> - Option F: $25 \%$ chance of 6,000 dollars ( $75 \%$ chance of 0 dollars). <br> - Option J: $25 \%$ chance of 4,000 dollars ( $25 \%$ chance of 2,000 dollars, $50 \%$ chance of 0 dollars). <br> A: Option | 0.21011495 |
| 15 | Q: Which option do you prefer? <br> - Option F: $25 \%$ chance of losing 6,000 dollars ( $75 \%$ chance of losing nothing). <br> - Option J: $25 \%$ chance of losing 4,000 dollars ( $25 \%$ chance of 2,000 dollars, $50 \%$ chance of 0 dollars). <br> A: Option | 0.20015262 |
| 16 | Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance at 5,000 dollars ( $99.9 \%$ chance of 0 dollars). <br> - Option J: $100 \%$ guarantee of 5 dollars. <br> A: Option | 0.09527163 |
| 17 | Q: Which option do you prefer? <br> - Option F: $0.1 \%$ chance of losing 5,000 dollars ( $99.9 \%$ chance of losing nothing). <br> - Option J: $100 \%$ guarantee of losing 5 dollars. <br> A: Option | 0.82562455 |</p>
<p>Table 6. Decision from description problems used for the contrast analysis from Figure 2C. The rightmost column displays GPT-3's choice probabilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Contrast ID</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
<th style="text-align: center;">9</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">11</th>
<th style="text-align: center;">12</th>
<th style="text-align: center;">13</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Problem 1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">Problem 2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">16</td>
</tr>
</tbody>
</table>
<p>Table 7. List of contrasts used for the analysis of Figure 2C.</p>
<p>Two-step task</p>
<p>Figure 6 contains simulated behavior of a model-free and model-based reinforcement learning algorithm. For a detailed description of these algorithms, see Daw et al. ${ }^{35}$.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 6. Model simulations on the two-step task. A: Model-free reinforcement learning algorithm. B: Model-based reinforcement learning algorithm. Figure adapted from Daw et al. ${ }^{35}$.</p>            </div>
        </div>

    </div>
</body>
</html>
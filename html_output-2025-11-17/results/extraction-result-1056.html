<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1056 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1056</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1056</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-265094964</p>
                <p><strong>Paper Title:</strong> Deep Learning for Embodied Visual Navigation Research: A Survey</p>
                <p><strong>Paper Abstract:</strong> —“Embodied visual navigation” problem requires an agent to navigate in a 3D environment mainly rely on its ﬁrst-person observation. This problem has attracted rising attention in recent years due to its wide application in vacuum cleaner, and rescue robot, etc. A navigation agent is supposed to have various intelligent skills, such as visual perceiving, mapping, planning, exploring and reasoning, etc. Building such an agent that observes, thinks, and acts is a key to real intelligence. The remarkable learning ability of deep learning methods empowered the agents to accomplish embodied visual navigation tasks. Despite this, embodied visual navigation is still in its infancy since a lot of advanced skills are required, including perceiving partially observed visual input, exploring unseen areas, memorizing and modeling seen scenarios, understanding cross-modal instructions, and adapting to a new environment, etc. Recently, embodied visual navigation has attracted rising attention of the community, and numerous works has been proposed to learn these skills. This paper attempts to establish an outline of the current works in the ﬁeld of embodied visual navigation by providing a comprehensive literature survey. We summarize the benchmarks and metrics, review different methods, analysis the challenges, and highlight the state-of-the-art methods. Finally, we discuss unresolved challenges in the ﬁeld of embodied visual navigation and give promising directions in pursuing future research.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1056.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1056.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DD-PPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DD-PPO (Distributed Data-Parallel Proximal Policy Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale distributed RL training method for PointGoal navigation that uses massive parallel data collection to learn high-performance navigation policies in photo-realistic simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DD-PPO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A distributed reinforcement learning agent trained with a data-parallel implementation of PPO; learns end-to-end policies mapping egocentric visual inputs (RGB, depth) to discrete navigation actions via large-scale experience collection.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat / PointGoal navigation (photorealistic indoor simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor photo-realistic 3D environments (e.g., Matterport3D, Gibson) with navigable meshes and high-resolution RGB-D observations; complexity arises from diverse room layouts, obstacles and long trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>environmental complexity characterized by number of scenes/houses, room layouts, object clutter, trajectory length (steps) and presence of obstacles; high-resolution sensory observations and continuous/discrete state-space used (no single numeric measure in survey except dataset sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (benchmarked on large indoor scenes; survey reports PointGoal remains challenging)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation characterized by number of environment instances (e.g., hundreds of houses in Gibson/MP3D), seen vs unseen train/test splits, and richness/diversity of scene assets; DD-PPO trained on billions of frames across many scene instances</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (trained across many scenes; survey notes Habitat and large datasets enable high variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Success weighted by Path Length (SPL)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey reports state-of-the-art PointGoal: SR = 64.5%, SPL = 37.7% (survey attribution to state-of-the-art PointGoal models such as DD-PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey emphasizes trade-off: increased environment complexity (larger scenes, more obstacles, longer trajectories) reduces success rates and increases failure proportion; high variation across training environments (more houses/scenes) improves learned robustness/generalization but requires larger sample complexity (DD-PPO used extremely large-scale data).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Reported high overall training scale (billions of frames) is used to achieve strong PointGoal performance; specific numeric split-level results not given in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Large-scale distributed RL (PPO) with heavy parallel data collection; effectively multi-environment training/data-parallelism</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey notes models trained at massive scale generalize better to unseen scenes in PointGoal benchmarks, but overall performance remains far from perfect; generalization evaluated via unseen splits.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very low sample efficiency (survey references training on extremely large datasets / billions of frames as in DD-PPO paper)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scaling up data collection and distributed RL (DD-PPO) achieves strong PointGoal performance in simulators, but requires huge sample counts; environment complexity (longer trajectories, clutter) reduces success and demands more training data; high variation in training environments improves robustness but increases required data and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1056.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ANS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Neural SLAM (ANS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical neural SLAM approach that learns exploration and mapping jointly to improve PointGoal/ObjectGoal navigation by combining learned mapping with hierarchical planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Neural SLAM (Active Neural SLAM referenced in survey; original: Active Neural SLAM for Exploration?)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active Neural SLAM (ANS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A neural SLAM-based agent that uses learned mapping (2D occupancy/exploration channels) and a hierarchical planner: a learned high-level planner (RL) and a low-level controller (often imitation-learned) for navigation and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Indoor exploration/navigation benchmarks (Habitat PointGoal / ObjectGoal)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photo-realistic indoor environments with partial observations; complexity stems from unknown geometry, occluded areas, and requirement to explore to find targets (long-horizon trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>complexity captured by need for exploration (percentage of unexplored area), trajectory length (long-term), number of distinct rooms to visit; degree of partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high for long-horizon and multi-room exploration tasks</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation via different scenes/houses and object placements; evaluated on unseen environments to measure robustness to scene variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (trained and evaluated across multiple scenes; survey notes improved exploration generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), SPL, exploration/coverage metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper (survey) highlights that modeling explicit spatial memory (maps) helps cope with complexity (long trajectories) and variation (new houses) — mapping improves sample efficiency and generalization versus pure model-free approaches, but requires additional architectural components and inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Hierarchical RL (high-level planner via RL, low-level via imitation), auxiliary map-prediction tasks as supervision</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey reports ANS achieved state-of-the-art on Habitat PointGoal (at time) and improved exploration/coverage in unseen environments, indicating better generalization for long-horizon tasks with mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved relative to purely model-free RL for exploration because of map-based inductive bias; exact training counts not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit neural SLAM/memory modules mitigate the difficulty imposed by environment complexity (long trajectories, partial observability) and improve generalization across varied scenes, at the cost of architectural complexity and reliance on pose/odometry signals or learned localization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1056.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SemExp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal-Oriented Semantic Exploration (SemExp)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that constructs episodic semantic maps and uses target-category-driven exploration policies to improve ObjectGoal navigation in realistic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Object goal navigation using goal-oriented semantic exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SemExp</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A planner that builds and updates episodic semantic maps (category-specific belief maps) and uses them to guide exploration policy conditioned on the target object category; trained using RL/imitation components.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat / ObjectGoal navigation (realistic indoor scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor photorealistic houses with multiple rooms and objects; complexity arises from distribution and occlusion of target objects and long search trajectories across rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>complexity expressed via number of rooms to search, object sparsity/clutter, required exploration coverage and trajectory length; ObjectGoal requires object labels and object placements.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (ObjectGoal is reported as challenging in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation measured by number of houses/scenes (e.g., Gibson, Matterport), object placement randomness and test-on-unseen splits</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (benchmarking on many diverse indoor scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), SPL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey reports overall best ObjectGoal performance in benchmarks: SR ≈ 21.08%, SPL ≈ 8.38% (survey attribute: best ObjectGoal model performance; SemExp noted as achieving SOTA in Habitat ObjectNav Challenge 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes ObjectGoal performance is much lower than PointGoal, showing that semantic complexity (need for object recognition + search) greatly reduces success; semantic exploration using category maps helps but absolute performance remains low under high complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Low absolute performance even for state-of-the-art (SR ~21%), indicating combined high complexity and high variation produce poor results.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Episodic semantic mapping with RL-guided exploration; uses semantic priors (category maps) to focus search.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>SemExp demonstrated improved ObjectNav results on Habitat ObjectNav Challenge (unseen environments), indicating mapping semantics improve cross-scene generalization but absolute performance remains low.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not explicitly quantified in survey; method relies on episodic map updates and RL training over varied scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating semantic episodic maps oriented by target category substantially improves ObjectGoal navigation compared to naive exploration, but ObjectGoal remains substantially harder than geometric tasks (PointGoal) due to semantic complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1056.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>6-Act Tether</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>6-Action Tethered Policy (6-Act Tether)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A navigation policy variant using a compact six-action discrete action space and tethering strategies to improve learning and robustness in embodied navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>6-Act Tether</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model-free navigation policy using a reduced discrete six-action action set (high-level actions) trained with RL and/or auxiliary losses to improve stability and sample efficiency in visual navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat / ObjectGoal and PointGoal navigation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor photorealistic scenes; complexity includes partial observability, obstacle avoidance and variable trajectory lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>complexity via length of trajectories, obstacle density, and required turning/heading changes; smaller action space reduces control complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (reduced action space trades off fine-grained control for easier learning)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation across scenes and object placements; tested on seen/unseen splits</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey includes 6-Act Tether among methods compared on ObjectGoal, noting that action-space design is a trade-off: smaller discrete action sets simplify learning in varied environments but can limit fine-grained maneuvering in complex cluttered scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Model-free RL with action-space design to increase stability; often combined with auxiliary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey lists as baseline in ObjectGoal comparisons; no detailed numeric results given in text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency relative to very large action spaces due to reduced decision complexity; no numeric counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reducing action-space (e.g., high-level actions) reduces learning complexity and stabilizes training across varied environments, but may reduce fine-grained navigation performance in highly cluttered/complex scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1056.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLN-BERT (Recurrent Vision-and-Language BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining-based vision-and-language navigation agent that uses transformer/BERT-style pretraining to better align language instructions with visual observations and improve generalization to unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vln bert: A recurrent vision-and-language bert for navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A recurrent transformer-based agent that maintains temporal context and is pretrained on image-text(-action) data (masked language modeling / action prediction), then fine-tuned for VLN tasks; combines language and visual features for step-by-step action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>VLN / R2R and related vision-and-language navigation datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic indoor trajectories annotated with human natural language instructions; complexity stems from language variability, long instructions, indirect paths and diverse scene layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>complexity measured by instruction length (R2R average 29 words), path complexity (direct vs indirect), number of unique environments/houses and diversity of language; also path length and number of sub-instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (VLN combines semantic/language complexity with visual complexity of scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation via dataset size (R2R ~21,567 instructions), augmented datasets (R4R, RxR), multilingual variations and seen/unseen splits; pretraining on large image-text corpora adds variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (survey describes pretraining and augmentation to address variation and bias)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), SPL, Navigation Error (NE), nDTW, CLS for instruction fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey reports state-of-the-art VLN model [152] achieves SR ≈ 63% and SPL ≈ 57% (compared to human SR = 86%, SPL = 76%)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey discusses that language variability + complex paths (indirect routes) increase difficulty; pretraining on large cross-modal data and using recurrent transformers reduces sensitivity to dataset bias and improves generalization to unseen scenes/instructions, but there remains a notable gap to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Pretraining on large image-text-action triplets (self-supervised objectives like MLM and action prediction) then fine-tuning on VLN; data augmentation (speaker-follower) and stochastic sampling used to mitigate train-test mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>VLN-BERT and other pretraining-based methods improve generalization to unseen scenes and instruction variants relative to purely supervised seq2seq models; survey reports significant improvements but still below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Pretraining yields sample-efficiency gains when fine-tuning on smaller VLN datasets; exact sample counts not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining transformers on large cross-modal corpora and maintaining temporal context substantially improves VLN generalization under high language and environment variation, reducing overfitting to limited instruction/trajectory pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1056.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PREVALENT (Pretraining with Reinforcement and Vision-Language Action Prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised pretraining framework that learns joint image-text-action representations from embodied environment trajectories to improve downstream VLN and dialog navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PREVALENT (as referenced in survey; original: Towards learning a generic agent for vision-and-language navigation via pre-training?)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A pretraining-based agent trained with masked language modeling and action prediction on image-text-action triplets sampled from embodied environments; weights are transferred to downstream VLN models.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (pretraining on simulated environment data)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Embodied vision-language navigation datasets (R2R, CVDN, HANNA etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Trajectories sampled from simulated embodied environments paired with language; complexity arises from language diversity, path non-optimality, and varied visual contexts across houses.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>instruction length, number of trajectories/instruction pairs, and environment diversity (dataset sizes); pretraining uses large quantities of triplets from embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (addresses VLN complexity via large-scale pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation via large-scale sampling of image-text-action triplets from many episodes/environments and subsequent fine-tuning on target datasets; aims to reduce dataset bias.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (designed to capture varied multimodal contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL and downstream VLN metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey highlights PREVALENT as addressing the training-data variation problem in VLN: pretraining on varied embodied triplets reduces overfitting to small VLN datasets and improves generalization under high variation, especially for complex (indirect) instruction trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Self-supervised pretraining (MLM, action prediction) on image-text-action triplets sampled from embodied environments; followed by task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey reports PREVALENT shows effectiveness on R2R, CVDN and HANNA; improves transfer/generalization compared to models trained from scratch on small VLN datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Pretraining increases sample-efficiency of downstream fine-tuning (survey qualitative statement); no numeric sample counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on varied embodied triplets addresses data bias and improves performance and generalization in cross-modal navigation under high environment and language variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1056.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Speaker-Follower</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Speaker-Follower (Data Augmentation for VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data augmentation approach that uses a learned 'speaker' to synthesize instructions for sampled trajectories and a 'follower' to learn from these augmented instruction-trajectory pairs to reduce overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Speakerfollower models for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Speaker-Follower</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A two-model augmentation pipeline: a speaker model generates natural-language instructions for arbitrary trajectories, and a follower model is trained on both human and speaker-generated instructions to increase training data diversity; training combines imitation learning and RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (augmentation method used in training VLN agents)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>VLN / R2R and extended VLN datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic navigation trajectories where human instructions are limited; augmentation increases variation in language-trajectory pairs to cover indirect and suboptimal paths.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>instruction-trajectory diversity, number of augmented trajectories, path directness/indirectness and trajectory length</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>increases effective complexity of training data by including indirect/suboptimal paths</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation measured by amount of synthetic instruction data generated and cross-connection of trajectories (R4R creates more varied trajectories from R2R)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (speaker-follower explicitly increases variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL, NE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey emphasizes that augmenting with speaker-generated instructions increases variation in training data and helps generalization to unseen instruction variants and paths, at the cost of potential noise from synthetic instructions; effectiveness saturates beyond a point (diminishing returns reported).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Data augmentation via learned speaker; followed by imitation learning and RL fine-tuning on follower.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey notes speaker-follower improves generalization for VLN (R2R), but later analyses show only a fraction of augmented paths are useful and improvements diminish after using most useful subset.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Augmentation increases effective sample diversity without requiring extra human annotation; sample-efficiency improvements depend on how many high-quality augmentations are used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Synthetic instruction augmentation increases training variation and improves generalization in VLN, but noisy or low-quality augmentations provide limited benefit; there are diminishing returns with excessive augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1056.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RPA (Regretful Path Agent / Lookahead Module models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLN approach that augments sequence-to-sequence navigation with a learned lookahead module to predict future states and rewards, improving generalization and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RPA (in survey referencing 'RPA')</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RPA (lookahead-enhanced VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A sequence-to-sequence navigation agent augmented with a lookahead module that predicts future observations/rewards to guide current action selection; trained with imitation and reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Instruction-following trajectories in indoor photorealistic scenes; complexity from long instructions, multi-step decisions and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>instruction length, path complexity, and prediction horizon for lookahead</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation across trajectories and instructions; tested on unseen splits</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL, NE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes lookahead/prediction modules can partially mitigate failures caused by complex or ambiguous instructions by anticipating future states, improving robustness to variation in trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Sequence-to-sequence with auxiliary lookahead prediction module; combined imitation learning and RL.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Improved generalization and stability over vanilla seq2seq baselines on R2R seen/unseen splits as reported in survey; exact numeric gains not reproduced in survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lookahead auxiliary supervision improves data efficiency modestly; precise counts not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Predictive lookahead modules help mitigate instability of seq2seq VLN agents under complex instructions and partial observability, improving generalization to unseen trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1056.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1056.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural SLAM / MapNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural SLAM / MapNet (neural mapping & planning families)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that embed SLAM-like mapping modules or allocentric spatial memory into end-to-end learning pipelines to improve long-horizon navigation under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural SLAM / MapNet family</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that incorporate differentiable mapping modules (egocentric-to-world projection, allocentric memory, topological graph memory) combined with planners/value functions; training mixes imitation, RL and auxiliary mapping objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Long-horizon navigation benchmarks (PointGoal, ObjectGoal, MultiON)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable indoor environments where agents must accumulate spatial memory over long trajectories; complexity arises from mapping and localization uncertainty, dynamic obstacles and semantic variability.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>complexity measured by trajectory length, partial observability, number of distinct places to remember, and semantic diversity of scenes and objects; Map size / node count in topological memory is another proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (designed for long-horizon, partially observable tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>variation across scenes, semantic object distributions, and dynamic changes; topological memory aims to capture observed variation in a compressed form.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (memory-based methods target generalization across varied scene geometries)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, SPL, coverage-based exploration rewards, mapping accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey emphasizes that explicit mapping and topological memory reduce the effective complexity induced by long-horizon tasks and variation across scenes by encoding previously seen observations; such inductive biases improve performance in complex/high-variation settings relative to pure model-free policies.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Hybrid: auxiliary map prediction tasks, imitation learning for low-level control, RL for high-level planning; episodic/topological memory updates during episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Neural SLAM and MapNet variants improve navigation in unseen environments and long-horizon object search by leveraging spatial memory; survey cites Active Neural SLAM, Neural Topological SLAM, and MapNet as effective.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved relative to unconstrained model-free approaches due to structured memory; exact sample counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding mapping/topological memory into policies is an effective way to trade model complexity for improved generalization and sample efficiency in complex, highly variable environments; however, mapping modules require localization signals and careful design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Embodied Visual Navigation Research: A Survey', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames <em>(Rating: 2)</em></li>
                <li>Active Neural SLAM <em>(Rating: 2)</em></li>
                <li>Object goal navigation using goal-oriented semantic exploration <em>(Rating: 2)</em></li>
                <li>Vln bert: A recurrent vision-and-language bert for navigation <em>(Rating: 2)</em></li>
                <li>PREVALENT <em>(Rating: 2)</em></li>
                <li>Speakerfollower models for vision-and-language navigation <em>(Rating: 1)</em></li>
                <li>Neural Topological SLAM for visual navigation <em>(Rating: 2)</em></li>
                <li>MapNet: An allocentric spatial memory for mapping environments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1056",
    "paper_id": "paper-265094964",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "DD-PPO",
            "name_full": "DD-PPO (Distributed Data-Parallel Proximal Policy Optimization)",
            "brief_description": "A large-scale distributed RL training method for PointGoal navigation that uses massive parallel data collection to learn high-performance navigation policies in photo-realistic simulators.",
            "citation_title": "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "mention_or_use": "mention",
            "agent_name": "DD-PPO",
            "agent_description": "A distributed reinforcement learning agent trained with a data-parallel implementation of PPO; learns end-to-end policies mapping egocentric visual inputs (RGB, depth) to discrete navigation actions via large-scale experience collection.",
            "agent_type": "simulated agent",
            "environment_name": "Habitat / PointGoal navigation (photorealistic indoor simulators)",
            "environment_description": "Indoor photo-realistic 3D environments (e.g., Matterport3D, Gibson) with navigable meshes and high-resolution RGB-D observations; complexity arises from diverse room layouts, obstacles and long trajectories.",
            "complexity_measure": "environmental complexity characterized by number of scenes/houses, room layouts, object clutter, trajectory length (steps) and presence of obstacles; high-resolution sensory observations and continuous/discrete state-space used (no single numeric measure in survey except dataset sizes)",
            "complexity_level": "high (benchmarked on large indoor scenes; survey reports PointGoal remains challenging)",
            "variation_measure": "variation characterized by number of environment instances (e.g., hundreds of houses in Gibson/MP3D), seen vs unseen train/test splits, and richness/diversity of scene assets; DD-PPO trained on billions of frames across many scene instances",
            "variation_level": "high (trained across many scenes; survey notes Habitat and large datasets enable high variation)",
            "performance_metric": "Success Rate (SR), Success weighted by Path Length (SPL)",
            "performance_value": "Survey reports state-of-the-art PointGoal: SR = 64.5%, SPL = 37.7% (survey attribution to state-of-the-art PointGoal models such as DD-PPO)",
            "complexity_variation_relationship": "Survey emphasizes trade-off: increased environment complexity (larger scenes, more obstacles, longer trajectories) reduces success rates and increases failure proportion; high variation across training environments (more houses/scenes) improves learned robustness/generalization but requires larger sample complexity (DD-PPO used extremely large-scale data).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Reported high overall training scale (billions of frames) is used to achieve strong PointGoal performance; specific numeric split-level results not given in survey.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Large-scale distributed RL (PPO) with heavy parallel data collection; effectively multi-environment training/data-parallelism",
            "generalization_tested": true,
            "generalization_results": "Survey notes models trained at massive scale generalize better to unseen scenes in PointGoal benchmarks, but overall performance remains far from perfect; generalization evaluated via unseen splits.",
            "sample_efficiency": "Very low sample efficiency (survey references training on extremely large datasets / billions of frames as in DD-PPO paper)",
            "key_findings": "Scaling up data collection and distributed RL (DD-PPO) achieves strong PointGoal performance in simulators, but requires huge sample counts; environment complexity (longer trajectories, clutter) reduces success and demands more training data; high variation in training environments improves robustness but increases required data and compute.",
            "uuid": "e1056.0",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "ANS",
            "name_full": "Active Neural SLAM (ANS)",
            "brief_description": "A hierarchical neural SLAM approach that learns exploration and mapping jointly to improve PointGoal/ObjectGoal navigation by combining learned mapping with hierarchical planning.",
            "citation_title": "Active Neural SLAM (Active Neural SLAM referenced in survey; original: Active Neural SLAM for Exploration?)",
            "mention_or_use": "mention",
            "agent_name": "Active Neural SLAM (ANS)",
            "agent_description": "A neural SLAM-based agent that uses learned mapping (2D occupancy/exploration channels) and a hierarchical planner: a learned high-level planner (RL) and a low-level controller (often imitation-learned) for navigation and exploration.",
            "agent_type": "simulated agent",
            "environment_name": "Indoor exploration/navigation benchmarks (Habitat PointGoal / ObjectGoal)",
            "environment_description": "Photo-realistic indoor environments with partial observations; complexity stems from unknown geometry, occluded areas, and requirement to explore to find targets (long-horizon trajectories).",
            "complexity_measure": "complexity captured by need for exploration (percentage of unexplored area), trajectory length (long-term), number of distinct rooms to visit; degree of partial observability.",
            "complexity_level": "high for long-horizon and multi-room exploration tasks",
            "variation_measure": "variation via different scenes/houses and object placements; evaluated on unseen environments to measure robustness to scene variation.",
            "variation_level": "medium-high (trained and evaluated across multiple scenes; survey notes improved exploration generalization)",
            "performance_metric": "Success Rate (SR), SPL, exploration/coverage metrics",
            "performance_value": null,
            "complexity_variation_relationship": "Paper (survey) highlights that modeling explicit spatial memory (maps) helps cope with complexity (long trajectories) and variation (new houses) — mapping improves sample efficiency and generalization versus pure model-free approaches, but requires additional architectural components and inductive bias.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Hierarchical RL (high-level planner via RL, low-level via imitation), auxiliary map-prediction tasks as supervision",
            "generalization_tested": true,
            "generalization_results": "Survey reports ANS achieved state-of-the-art on Habitat PointGoal (at time) and improved exploration/coverage in unseen environments, indicating better generalization for long-horizon tasks with mapping.",
            "sample_efficiency": "Improved relative to purely model-free RL for exploration because of map-based inductive bias; exact training counts not provided in survey.",
            "key_findings": "Explicit neural SLAM/memory modules mitigate the difficulty imposed by environment complexity (long trajectories, partial observability) and improve generalization across varied scenes, at the cost of architectural complexity and reliance on pose/odometry signals or learned localization.",
            "uuid": "e1056.1",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "SemExp",
            "name_full": "Goal-Oriented Semantic Exploration (SemExp)",
            "brief_description": "A method that constructs episodic semantic maps and uses target-category-driven exploration policies to improve ObjectGoal navigation in realistic environments.",
            "citation_title": "Object goal navigation using goal-oriented semantic exploration",
            "mention_or_use": "mention",
            "agent_name": "SemExp",
            "agent_description": "A planner that builds and updates episodic semantic maps (category-specific belief maps) and uses them to guide exploration policy conditioned on the target object category; trained using RL/imitation components.",
            "agent_type": "simulated agent",
            "environment_name": "Habitat / ObjectGoal navigation (realistic indoor scenes)",
            "environment_description": "Indoor photorealistic houses with multiple rooms and objects; complexity arises from distribution and occlusion of target objects and long search trajectories across rooms.",
            "complexity_measure": "complexity expressed via number of rooms to search, object sparsity/clutter, required exploration coverage and trajectory length; ObjectGoal requires object labels and object placements.",
            "complexity_level": "high (ObjectGoal is reported as challenging in survey)",
            "variation_measure": "variation measured by number of houses/scenes (e.g., Gibson, Matterport), object placement randomness and test-on-unseen splits",
            "variation_level": "high (benchmarking on many diverse indoor scenes)",
            "performance_metric": "Success Rate (SR), SPL",
            "performance_value": "Survey reports overall best ObjectGoal performance in benchmarks: SR ≈ 21.08%, SPL ≈ 8.38% (survey attribute: best ObjectGoal model performance; SemExp noted as achieving SOTA in Habitat ObjectNav Challenge 2020).",
            "complexity_variation_relationship": "Survey notes ObjectGoal performance is much lower than PointGoal, showing that semantic complexity (need for object recognition + search) greatly reduces success; semantic exploration using category maps helps but absolute performance remains low under high complexity and variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Low absolute performance even for state-of-the-art (SR ~21%), indicating combined high complexity and high variation produce poor results.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Episodic semantic mapping with RL-guided exploration; uses semantic priors (category maps) to focus search.",
            "generalization_tested": true,
            "generalization_results": "SemExp demonstrated improved ObjectNav results on Habitat ObjectNav Challenge (unseen environments), indicating mapping semantics improve cross-scene generalization but absolute performance remains low.",
            "sample_efficiency": "Not explicitly quantified in survey; method relies on episodic map updates and RL training over varied scenes.",
            "key_findings": "Incorporating semantic episodic maps oriented by target category substantially improves ObjectGoal navigation compared to naive exploration, but ObjectGoal remains substantially harder than geometric tasks (PointGoal) due to semantic complexity and variation.",
            "uuid": "e1056.2",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "6-Act Tether",
            "name_full": "6-Action Tethered Policy (6-Act Tether)",
            "brief_description": "A navigation policy variant using a compact six-action discrete action space and tethering strategies to improve learning and robustness in embodied navigation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "6-Act Tether",
            "agent_description": "A model-free navigation policy using a reduced discrete six-action action set (high-level actions) trained with RL and/or auxiliary losses to improve stability and sample efficiency in visual navigation.",
            "agent_type": "simulated agent",
            "environment_name": "Habitat / ObjectGoal and PointGoal navigation benchmarks",
            "environment_description": "Indoor photorealistic scenes; complexity includes partial observability, obstacle avoidance and variable trajectory lengths.",
            "complexity_measure": "complexity via length of trajectories, obstacle density, and required turning/heading changes; smaller action space reduces control complexity.",
            "complexity_level": "medium (reduced action space trades off fine-grained control for easier learning)",
            "variation_measure": "variation across scenes and object placements; tested on seen/unseen splits",
            "variation_level": "medium",
            "performance_metric": "SR, SPL",
            "performance_value": null,
            "complexity_variation_relationship": "Survey includes 6-Act Tether among methods compared on ObjectGoal, noting that action-space design is a trade-off: smaller discrete action sets simplify learning in varied environments but can limit fine-grained maneuvering in complex cluttered scenes.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Model-free RL with action-space design to increase stability; often combined with auxiliary tasks.",
            "generalization_tested": true,
            "generalization_results": "Survey lists as baseline in ObjectGoal comparisons; no detailed numeric results given in text.",
            "sample_efficiency": "Improved sample efficiency relative to very large action spaces due to reduced decision complexity; no numeric counts provided.",
            "key_findings": "Reducing action-space (e.g., high-level actions) reduces learning complexity and stabilizes training across varied environments, but may reduce fine-grained navigation performance in highly cluttered/complex scenes.",
            "uuid": "e1056.3",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "VLN-BERT",
            "name_full": "VLN-BERT (Recurrent Vision-and-Language BERT)",
            "brief_description": "A pretraining-based vision-and-language navigation agent that uses transformer/BERT-style pretraining to better align language instructions with visual observations and improve generalization to unseen environments.",
            "citation_title": "Vln bert: A recurrent vision-and-language bert for navigation",
            "mention_or_use": "mention",
            "agent_name": "VLN-BERT",
            "agent_description": "A recurrent transformer-based agent that maintains temporal context and is pretrained on image-text(-action) data (masked language modeling / action prediction), then fine-tuned for VLN tasks; combines language and visual features for step-by-step action prediction.",
            "agent_type": "simulated agent",
            "environment_name": "VLN / R2R and related vision-and-language navigation datasets",
            "environment_description": "Photorealistic indoor trajectories annotated with human natural language instructions; complexity stems from language variability, long instructions, indirect paths and diverse scene layouts.",
            "complexity_measure": "complexity measured by instruction length (R2R average 29 words), path complexity (direct vs indirect), number of unique environments/houses and diversity of language; also path length and number of sub-instructions.",
            "complexity_level": "high (VLN combines semantic/language complexity with visual complexity of scenes)",
            "variation_measure": "variation via dataset size (R2R ~21,567 instructions), augmented datasets (R4R, RxR), multilingual variations and seen/unseen splits; pretraining on large image-text corpora adds variation.",
            "variation_level": "high (survey describes pretraining and augmentation to address variation and bias)",
            "performance_metric": "Success Rate (SR), SPL, Navigation Error (NE), nDTW, CLS for instruction fidelity",
            "performance_value": "Survey reports state-of-the-art VLN model [152] achieves SR ≈ 63% and SPL ≈ 57% (compared to human SR = 86%, SPL = 76%)",
            "complexity_variation_relationship": "Survey discusses that language variability + complex paths (indirect routes) increase difficulty; pretraining on large cross-modal data and using recurrent transformers reduces sensitivity to dataset bias and improves generalization to unseen scenes/instructions, but there remains a notable gap to human performance.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Pretraining on large image-text-action triplets (self-supervised objectives like MLM and action prediction) then fine-tuning on VLN; data augmentation (speaker-follower) and stochastic sampling used to mitigate train-test mismatch.",
            "generalization_tested": true,
            "generalization_results": "VLN-BERT and other pretraining-based methods improve generalization to unseen scenes and instruction variants relative to purely supervised seq2seq models; survey reports significant improvements but still below human performance.",
            "sample_efficiency": "Pretraining yields sample-efficiency gains when fine-tuning on smaller VLN datasets; exact sample counts not provided in survey.",
            "key_findings": "Pretraining transformers on large cross-modal corpora and maintaining temporal context substantially improves VLN generalization under high language and environment variation, reducing overfitting to limited instruction/trajectory pairs.",
            "uuid": "e1056.4",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "PREVALENT",
            "name_full": "PREVALENT (Pretraining with Reinforcement and Vision-Language Action Prediction)",
            "brief_description": "A self-supervised pretraining framework that learns joint image-text-action representations from embodied environment trajectories to improve downstream VLN and dialog navigation tasks.",
            "citation_title": "PREVALENT (as referenced in survey; original: Towards learning a generic agent for vision-and-language navigation via pre-training?)",
            "mention_or_use": "mention",
            "agent_name": "PREVALENT",
            "agent_description": "A pretraining-based agent trained with masked language modeling and action prediction on image-text-action triplets sampled from embodied environments; weights are transferred to downstream VLN models.",
            "agent_type": "simulated agent (pretraining on simulated environment data)",
            "environment_name": "Embodied vision-language navigation datasets (R2R, CVDN, HANNA etc.)",
            "environment_description": "Trajectories sampled from simulated embodied environments paired with language; complexity arises from language diversity, path non-optimality, and varied visual contexts across houses.",
            "complexity_measure": "instruction length, number of trajectories/instruction pairs, and environment diversity (dataset sizes); pretraining uses large quantities of triplets from embodied environments.",
            "complexity_level": "medium-high (addresses VLN complexity via large-scale pretraining)",
            "variation_measure": "variation via large-scale sampling of image-text-action triplets from many episodes/environments and subsequent fine-tuning on target datasets; aims to reduce dataset bias.",
            "variation_level": "high (designed to capture varied multimodal contexts)",
            "performance_metric": "SR, SPL and downstream VLN metrics",
            "performance_value": null,
            "complexity_variation_relationship": "Survey highlights PREVALENT as addressing the training-data variation problem in VLN: pretraining on varied embodied triplets reduces overfitting to small VLN datasets and improves generalization under high variation, especially for complex (indirect) instruction trajectories.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Self-supervised pretraining (MLM, action prediction) on image-text-action triplets sampled from embodied environments; followed by task-specific fine-tuning.",
            "generalization_tested": true,
            "generalization_results": "Survey reports PREVALENT shows effectiveness on R2R, CVDN and HANNA; improves transfer/generalization compared to models trained from scratch on small VLN datasets.",
            "sample_efficiency": "Pretraining increases sample-efficiency of downstream fine-tuning (survey qualitative statement); no numeric sample counts provided.",
            "key_findings": "Pretraining on varied embodied triplets addresses data bias and improves performance and generalization in cross-modal navigation under high environment and language variation.",
            "uuid": "e1056.5",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Speaker-Follower",
            "name_full": "Speaker-Follower (Data Augmentation for VLN)",
            "brief_description": "A data augmentation approach that uses a learned 'speaker' to synthesize instructions for sampled trajectories and a 'follower' to learn from these augmented instruction-trajectory pairs to reduce overfitting.",
            "citation_title": "Speakerfollower models for vision-and-language navigation",
            "mention_or_use": "mention",
            "agent_name": "Speaker-Follower",
            "agent_description": "A two-model augmentation pipeline: a speaker model generates natural-language instructions for arbitrary trajectories, and a follower model is trained on both human and speaker-generated instructions to increase training data diversity; training combines imitation learning and RL fine-tuning.",
            "agent_type": "simulated agent (augmentation method used in training VLN agents)",
            "environment_name": "VLN / R2R and extended VLN datasets",
            "environment_description": "Photorealistic navigation trajectories where human instructions are limited; augmentation increases variation in language-trajectory pairs to cover indirect and suboptimal paths.",
            "complexity_measure": "instruction-trajectory diversity, number of augmented trajectories, path directness/indirectness and trajectory length",
            "complexity_level": "increases effective complexity of training data by including indirect/suboptimal paths",
            "variation_measure": "variation measured by amount of synthetic instruction data generated and cross-connection of trajectories (R4R creates more varied trajectories from R2R)",
            "variation_level": "high (speaker-follower explicitly increases variation)",
            "performance_metric": "SR, SPL, NE",
            "performance_value": null,
            "complexity_variation_relationship": "Survey emphasizes that augmenting with speaker-generated instructions increases variation in training data and helps generalization to unseen instruction variants and paths, at the cost of potential noise from synthetic instructions; effectiveness saturates beyond a point (diminishing returns reported).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Data augmentation via learned speaker; followed by imitation learning and RL fine-tuning on follower.",
            "generalization_tested": true,
            "generalization_results": "Survey notes speaker-follower improves generalization for VLN (R2R), but later analyses show only a fraction of augmented paths are useful and improvements diminish after using most useful subset.",
            "sample_efficiency": "Augmentation increases effective sample diversity without requiring extra human annotation; sample-efficiency improvements depend on how many high-quality augmentations are used.",
            "key_findings": "Synthetic instruction augmentation increases training variation and improves generalization in VLN, but noisy or low-quality augmentations provide limited benefit; there are diminishing returns with excessive augmentation.",
            "uuid": "e1056.6",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "RPA",
            "name_full": "RPA (Regretful Path Agent / Lookahead Module models)",
            "brief_description": "A VLN approach that augments sequence-to-sequence navigation with a learned lookahead module to predict future states and rewards, improving generalization and stability.",
            "citation_title": "RPA (in survey referencing 'RPA')",
            "mention_or_use": "mention",
            "agent_name": "RPA (lookahead-enhanced VLN)",
            "agent_description": "A sequence-to-sequence navigation agent augmented with a lookahead module that predicts future observations/rewards to guide current action selection; trained with imitation and reinforcement learning.",
            "agent_type": "simulated agent",
            "environment_name": "Vision-and-Language Navigation (R2R)",
            "environment_description": "Instruction-following trajectories in indoor photorealistic scenes; complexity from long instructions, multi-step decisions and partial observability.",
            "complexity_measure": "instruction length, path complexity, and prediction horizon for lookahead",
            "complexity_level": "medium-high",
            "variation_measure": "variation across trajectories and instructions; tested on unseen splits",
            "variation_level": "medium-high",
            "performance_metric": "SR, SPL, NE",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes lookahead/prediction modules can partially mitigate failures caused by complex or ambiguous instructions by anticipating future states, improving robustness to variation in trajectories.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Sequence-to-sequence with auxiliary lookahead prediction module; combined imitation learning and RL.",
            "generalization_tested": true,
            "generalization_results": "Improved generalization and stability over vanilla seq2seq baselines on R2R seen/unseen splits as reported in survey; exact numeric gains not reproduced in survey text.",
            "sample_efficiency": "Lookahead auxiliary supervision improves data efficiency modestly; precise counts not provided in survey.",
            "key_findings": "Predictive lookahead modules help mitigate instability of seq2seq VLN agents under complex instructions and partial observability, improving generalization to unseen trajectories.",
            "uuid": "e1056.7",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Neural SLAM / MapNet",
            "name_full": "Neural SLAM / MapNet (neural mapping & planning families)",
            "brief_description": "A class of methods that embed SLAM-like mapping modules or allocentric spatial memory into end-to-end learning pipelines to improve long-horizon navigation under partial observability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Neural SLAM / MapNet family",
            "agent_description": "Agents that incorporate differentiable mapping modules (egocentric-to-world projection, allocentric memory, topological graph memory) combined with planners/value functions; training mixes imitation, RL and auxiliary mapping objectives.",
            "agent_type": "simulated agent",
            "environment_name": "Long-horizon navigation benchmarks (PointGoal, ObjectGoal, MultiON)",
            "environment_description": "Partially observable indoor environments where agents must accumulate spatial memory over long trajectories; complexity arises from mapping and localization uncertainty, dynamic obstacles and semantic variability.",
            "complexity_measure": "complexity measured by trajectory length, partial observability, number of distinct places to remember, and semantic diversity of scenes and objects; Map size / node count in topological memory is another proxy.",
            "complexity_level": "high (designed for long-horizon, partially observable tasks)",
            "variation_measure": "variation across scenes, semantic object distributions, and dynamic changes; topological memory aims to capture observed variation in a compressed form.",
            "variation_level": "high (memory-based methods target generalization across varied scene geometries)",
            "performance_metric": "SR, SPL, coverage-based exploration rewards, mapping accuracy",
            "performance_value": null,
            "complexity_variation_relationship": "Survey emphasizes that explicit mapping and topological memory reduce the effective complexity induced by long-horizon tasks and variation across scenes by encoding previously seen observations; such inductive biases improve performance in complex/high-variation settings relative to pure model-free policies.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Hybrid: auxiliary map prediction tasks, imitation learning for low-level control, RL for high-level planning; episodic/topological memory updates during episodes.",
            "generalization_tested": true,
            "generalization_results": "Neural SLAM and MapNet variants improve navigation in unseen environments and long-horizon object search by leveraging spatial memory; survey cites Active Neural SLAM, Neural Topological SLAM, and MapNet as effective.",
            "sample_efficiency": "Improved relative to unconstrained model-free approaches due to structured memory; exact sample counts not provided.",
            "key_findings": "Embedding mapping/topological memory into policies is an effective way to trade model complexity for improved generalization and sample efficiency in complex, highly variable environments; however, mapping modules require localization signals and careful design.",
            "uuid": "e1056.8",
            "source_info": {
                "paper_title": "Deep Learning for Embodied Visual Navigation Research: A Survey",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "rating": 2,
            "sanitized_title": "ddppo_learning_nearperfect_pointgoal_navigators_from_25_billion_frames"
        },
        {
            "paper_title": "Active Neural SLAM",
            "rating": 2,
            "sanitized_title": "active_neural_slam"
        },
        {
            "paper_title": "Object goal navigation using goal-oriented semantic exploration",
            "rating": 2,
            "sanitized_title": "object_goal_navigation_using_goaloriented_semantic_exploration"
        },
        {
            "paper_title": "Vln bert: A recurrent vision-and-language bert for navigation",
            "rating": 2,
            "sanitized_title": "vln_bert_a_recurrent_visionandlanguage_bert_for_navigation"
        },
        {
            "paper_title": "PREVALENT",
            "rating": 2
        },
        {
            "paper_title": "Speakerfollower models for vision-and-language navigation",
            "rating": 1,
            "sanitized_title": "speakerfollower_models_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "Neural Topological SLAM for visual navigation",
            "rating": 2,
            "sanitized_title": "neural_topological_slam_for_visual_navigation"
        },
        {
            "paper_title": "MapNet: An allocentric spatial memory for mapping environments",
            "rating": 1,
            "sanitized_title": "mapnet_an_allocentric_spatial_memory_for_mapping_environments"
        }
    ],
    "cost": 0.023805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep Learning for Embodied Visual Navigation Research: A Survey
11 Oct 2021</p>
<p>Fengda Zhu 
Yi Zhu 
Vincent Cs Lee 
Xiaodan Liang 
Xiaojun Chang 
Deep Learning for Embodied Visual Navigation Research: A Survey
11 Oct 20219E761A8F9EC60F0C6EBB5EA1F394170CarXiv:2108.04097v4[cs.RO]deep learningembodied environmentsembodied visual navigationcross-modal navigationnavigation robotics Navigation in Simulated Environments Embodied Environments (Section 2.1) Evaluations Metrics (Section 2.2) Target-driven Navigation (Section 3.1) Cross-modal Navigation (Section 3.2) Navigation in the Realworld Environment Embodied Datasets Embodied Simulators Embodied Navigation Tasks Trajectoryinsensitive Metrics Trajectory-sensitive Metrics
Embodied visual navigation" problem requires an agent to navigate in a 3D environment mainly rely on its first-person observation.This problem has attracted rising attention in recent years due to its wide application in vacuum cleaner, and rescue robot, etc.A navigation agent is supposed to have various intelligent skills, such as visual perceiving, mapping, planning, exploring and reasoning, etc. Building such an agent that observes, thinks, and acts is a key to real intelligence.The remarkable learning ability of deep learning methods empowered the agents to accomplish embodied visual navigation tasks.Despite this, embodied visual navigation is still in its infancy since a lot of advanced skills are required, including perceiving partially observed visual input, exploring unseen areas, memorizing and modeling seen scenarios, understanding cross-modal instructions, and adapting to a new environment, etc.Recently, embodied visual navigation has attracted rising attention of the community, and numerous works has been proposed to learn these skills.This paper attempts to establish an outline of the current works in the field of embodied visual navigation by providing a comprehensive literature survey.We summarize the benchmarks and metrics, review different methods, analysis the challenges, and highlight the state-of-the-art methods.Finally, we discuss unresolved challenges in the field of embodied visual navigation and give promising directions in pursuing future research.</p>
<p>INTRODUCTION</p>
<p>B UILDING a robot to accomplish tasks autonomously in place of humans has been a topic researched for a long time [1], [2], [3].Some complex applications, such as vacuum cleaning, disabled helping and rescuing, require an agent to navigate to finish several sub-tasks in different places in a 3D embodied environment.Therefore, navigation is one of the key capabilities in building the intelligent navigable robots in the real world.During the process of navigation, a robot needs to move around to find the target location by perceiving embodied visual inputs, which is named as "embodied visual navigation".The agent that interacts with the environment through its physical entity within that environment is named "embodied agent" [4].Fig. 2 demonstrates a navigation process.An agent firstly receives an instruction "Put the chair in the living room into the second balcony".Then it navigates to find the target chair.The agent picks up the chair and navigate to the balcony and put it down.</p>
<p>Early works on robotics navigation [5], [6] mainly rely on hand-crafted features like optical flow and traditional algorithms like Markov localization [7], incremental localization [8], or landmark tracking [9].These methods involve lots of hyperparameters and cannot generalize well in unseen environments.Recent developments of deep learning reveal its ability to learn a robust model from large-scale data.Vision robots trained by end-to-end deep learning methods is more robust, have less hyper-parameters, and have better generalization ability in unseen environments.</p>
<p>• Fengda Zhu and Vincent CS Lee are with Department of Data Science and AI, Faculty of Information Technology, Monash University.However, some challenges are going to be tackled in achieving deep learning for embodied visual navigation: 1) collecting data from the real world is expensive; 2) the model learned from partial observation is unstable; 3) it is difficult to learn the skills for long-term navigation such as exploration and memorization; 4) perceiving natural language instructions is challenging because natural language is diverse with flexible formats; 5) the large domain gap between the simulated environment and the real-world environment impedes the adaptation of navigation policy, etc.</p>
<p>This paper discusses the related works in robot navigation and give a promising direction in building real-world navigation robots.The structure of this paper is shown in Fig. 1.Training and testing in the real-world has many disadvantages: 1) The data sampling efficiency in the real world is very low since a real robot can only sample a trajectory at once while a simulator can efficiently sample trajectories in multi-processing; 2) the complexity and the dynamic of the real-world environment hinges the reproductivity; 3) there is large domain bias between different environments, etc.The development of 3D simulation technology enables researchers to construct a simulated environment [10], [11], [12], [13], [14] to study in building a robust navigation agent within it.Simulators render these 3D assets to generate RGB-D images and provide sensors like physical sensors, GPS sensors to simulate a realistic embodied robotic environment.Learning to navigation in a simulated environment is a broad field with lots of challenges to solve.In solving target-driven navigation problem, researchers propose model-free methods [10], [15], [16], selfsupervised methods [17], [18], [19], planning-based methods [20], [21], [22].Perceiving natural language is a challenging task due to its diversity and complexity.It requires the agents not only can following a sentence instruction step-by-step [12], [23], [24], but also understand dialogues [20], [21], [22] or navigate to answer questions [20], [21], [22].In building a real-world navigation robots, some works [25], [26], [27] proposed to train an agent in real-world environments directly while other works [28], [29], [30] propose to introduce transfer learning to transfer the learned navigation policy from simulated environments to the real-world environment.</p>
<p>Compared with previous surveys of robotic navigation [5], [6], our paper focuses on deep learning methods that solve the embodied navigation problems:</p>
<p>1) To the best of our knowledge, our paper is the first comprehensive study on the advances of deep learning methods on embodied navigation tasks.2) This paper summarizes and compares their unique insights of recently proposed embodied navigation datasets, simulators and tasks.3) This paper introduce deep learning methods for embodied visual navigation, including their motivations and contributions.4) This paper classifies the research results in recent years, and gives some promising embodied navigation directions.</p>
<p>The paper is organized as follows.Sec. 2 discusses the current embodied datasets and embodied simulators.Sec. 3 introduces the embodied navigation benchmarks including the navigation tasks and navigation metrics.Sec. 4 lists the methods to train an agent navigation in a simulated embodied navigation environment, where Sec.4.1 lists the methods for target-driven tasks and Sec.4.2 introduces the methods for cross-modal tasks.Sec. 5 summarizes the works that builds a navigation robot.Sec.6 illustrates the domain gap betweeen simulated environments and the real-world environment, and introduce the methods that solves these challenges.In Sec. 7, we highlight the recent state-of-the-art works, discuss about the limitations of current works, and propose promising directions in building a real-world navigation robot.</p>
<p>EMBODIED NAVIGATION ENVIRONMENTS</p>
<p>Here, we discuss the environments used for embodied navigation.We summarize the dataset that provides 3D assets and the simulators that render assets and provide interactive interfaces for navigation agents.</p>
<p>Embodied Datasets</p>
<p>An embodied dataset contains 3D assets like textures and meshes for rendering and other configuration data like object location, object category and camera pose for high-level tasks.A comparison of the proposed datasets is shown in Tab. 1.</p>
<p>Early works focused on rendering composite RGB views [31].It trains a probabilistic model to generate synthetic data based on hand-created scenes.Later, SceneNet [32] introduces a generator model to annotate 2D semantics.As depth channel is proved to be helpful for navigation agents [19], [38], 2D-3D-S [33] provides assets with depth information.Different from these works that render a single room at once, later works [34], [39], [40] provides a large number of scenes consist of bedrooms, living  rooms, bathrooms, kitchens, etc.However, the synthetic view used by the aforementioned datasets is quite different from the real world scene, which limits the application of the datasets.To this end, Matterport3D [36] provides photo-realistic panoramic views by 3D reconstruction and 2D and 3D semantics of these views.</p>
<p>Gibson [14] provides a more diverse dataset with 572 houses.Replica [37] proposes a dataset with 18 indoor scene consist of dense meshes and high-resolution textures.Some work such as AI2-THOR [40], RoboTHOR [39] and CHALET [35] rely on the datasets that not currently released.The rendering scenes of some datasets are shown in Fig. 3.</p>
<p>Embodied Simulators</p>
<p>An embodied simulator provides an interface for an agent to interact with the environment.We compare different features of the existing simulators in Tab. 2. A simulators is equipped with many sensors, such as a RGB sensor, a depth sensor, a physical sensor and a position sensor.Early simulators provide low RGB resolution and unrealistic imagery due to the limit of 3D rendering technology.The lack of visual detail, limits the navigation performance of the agent.Afterwards, to address this, simulations such as Matterport3D simulator [12], Gibson simulator [14] and Habitat [13] propose high-resolution photo-realistic panoramic view to simulate a more realistic environment.Rendering frame rate is also important to embodied simulators since it is critical to training efficiency.MINOS [11] runs over 100 frames per second (FPS), which is 10 times faster than its previous works.Habitat [13] over than 1000 FPS on 512 × 512 RGB-D image, making it become the fastest simulator among existing simulators.Discrete state space in [12] simplifies the navigation problem and makes the agent easy to learn complex vision-language navigation tasks.However, continuous state space is more welcome since it facilitates transferring a learned agent to a real-world robot.A customizable simulator is able to generate more diverse data by moving the objects, changing the textures of objects and reconfiguring the lights.Diverse data has little bias and therefore, enables the deep learning to learn a robust navigation policy.Despite of navigating to find the target object in a static room, interacting is another key skill for real-world robots.Some complex tasks may require a robot to interact with objects, such as picking up a cup, moving a chair, or opening a door.AI2-THOR [40], iGibson [41] and RoboTHOR [39] provide interactive environments to train such a skill.Multi-agent reinforcement learning [42], [43] is an emerging problem of cooperation and competition among agents.AI2-THOR and iGibson also support multi-agent training in studying cooperative tasks.</p>
<p>EMBODIED NAVIGATION BENCHMARKS</p>
<p>Here, we introduce several tasks to study the embodied visual navigation problem.These tasks can be divided into three cate-gories: target-driven navigation task, cross-modal navigation task, and interactive navigation task.</p>
<p>Target-driven Navigation Tasks</p>
<p>PointGoal Navigation, firstly defined by Anderson et al. [44], is a task where an agent is initialized to a random starting position and orientation then asked to navigate to a target position.The target position is indicated by its relative coordinates to the starting position.This task requires an agent to estimate the cumulative distance from the starting position so that the agent knows how far away from the goal.Theoretically, this task is able to be applied to all embodied environments.ObjectGoal Navigation is proposed by Zhu et al. [15].In this task, an agent is initialized to a random starting position and is required to find a specific object, such as a desk or a bed.Once the navigation agent find the object, it stops.The navigation process is regarded as a success if the agent is located within a distance to the target object.In addition to the room structure, the ObjectGoal navigation task needs the object labels and locations.Object recognizing and exploring are key skills to the ObjectGoal navigation.RoomGoal Navigation is proposed by Wu et al. [10].In this task, an agent initialized at a random position is asked to navigate to a room (e.g.bedroom or kitchen).The navigation process is regarded as a success if the agent is stopped within the target room.</p>
<p>RoomGoal navigation requires the room annotations.The concept of the room is a high-level semantic.Therefore, a RoomGoal navigation agent needs to understand the scene based on the visual details, such as furniture type and room layout.</p>
<p>Multi-Object Navigation (MultiON) Recently, more and more researchers are paying attention to long-term navigation where an agent memorize all the visited scenes.Motivated by this, Wani et al. [45] propose MultiON, a benchmark for Multi-Object Navigation.In MultiON, an agent is asked to navigate to multiple target objects one-by-one, which makes the navigation trajectory quite long.The agent raise a FOUND action when it reaches the instructed target.Perception and effective planning under partial observation would be the key to solve this task.</p>
<p>Cross-modal Navigation Tasks</p>
<p>Vision-and-Language Navigation (VLN) VLN is a task where an agent navigates step-by-step following natural language instructions [12].Previous tasks such as ObjectGoal and RoomGoal hard-code the object and room semantics as a one-hot vector.On the contrary, VLN introduce natural language sentences to instruct the navigation process like "Head upstairs and walk past the piano through an archway directly in front.Turn right when the hallway ends at pictures and table.Wait by the moose antlers hanging on Simulator Year Use Dataset(s) Resolution Physics FPS Customizable Interactive Multi-agent MINOS [11] 2017 SUNCG, Matterport3D 84 × 84 100 AI2-THOR<em> [40] 2017 -300 × 300 120 House3D [10] 2018 SUNCG 120 × 90 600 CHALET [35] 2018 CHALET 800 × 600 10 Matterport3D [12] 2018 Matterport3D 512 × 512 1,000 Gibson [14] 2018 Gibson, Matterport3D, 2D-3D-S 512 × 512 400 iGibson [41] 2018 Gibson 512 × 512 400 Habitat [13] 2019 Matterport3D, Gibson, Replica 512 × 512 10,000 RoboTHOR</em> [39] 2020 -300 × 300 1200 TABLE 2: Comparison of existing embodied simulators (*: the dataset that the simulator uses is not currently released).</p>
<p>the wall".The VLN task is successfully completed if the agent stops close to the intended goal following the instruction.There are several datasets have been proposed for visionlanguage navigation: R2R [12], R4R [46], and RxR [47].The room-to-room (R2R) dataset is proposed in [12] to study visionlanguage navigation.The R2R dataset contains 21,567 navigation instructions with an average length of 29 words.However, the R2R dataset has several shortcomings: 1) the referenced paths are direct-to-goal so that R2R instructions lack the capability of describing complex paths; 2) the instruction consists of several sentences and not fine-grained; 3) the training data is small, and the model is easily overfitting; 4) the language of instruction is English only, and no other languages are included.To address these problems, more advances datasets have been proposed.Jain et al. [46] cross-connects the trajectories and instructions in R2R and generate a new dataset named R4R.FGR2R [48] enriches R2R with sub-instructions and their corresponding trajectories.RxR [47] is a time-aligned dataset and it relieves the known biases in trajectories and elicits more references to visible entities in R2R.Navigation from Dialog History (NDH) When navigating in an unfamiliar environment, a human usually asks for assistance and continued navigation according to the responses of other humans.However, building an agent that is able to autonomously ask natural language questions and react to the answer is still a long-term goal in robotic navigation.In NDH [49], an agent is required to navigate according to a dialog history, which consists of several questionanswering pairs.Studying NDH is fundamental for building a real-world dialog navigation robot.Embodied Questioning and Answering (EQA) Visual Question Answering (VQA) [50] is a cross-modal task, in which a system answers a text-based question with a given image.VQA soon became one of the most popular computer vision tasks, because it revealed the possibility of interaction between human beings and artificial intelligence agents in natural language [51], [52], [53].Compared with VQA, a more advanced activity is to answer questions by self-exploration in an unseen environment.Embodied Questioning and Answering (EQA) [54] is a task where an agent is spawned at a random location in a 3D environment and asked a question.EQA is a challenging task since it requires a wide range of AI skills: visual perception, language understanding, target-driven navigation, commonsense reasoning, etc.In addition to navigation accuracy in other tasks, EQA propose EQA accuracy to measure whether the agent correctly answers the question or not.REVERIE Recently, Qi et al. [55] proposes Remote Embodied Visual referring Expression in Real Indoor Environments, named REVERIE in short, to research associating natural language instructions and the visual semantics.Different from VLN that gives an instruction that describes the trajectory step-by-step toward the target, the natural language instruction in REVERIE refers to a remote target object.Compared with ObjectGoal navigation, REVERIE offers rich language descriptions to enable the agent to find a unique target in the house.Audio-visual Navigation, proposed by Chen et al. [56] introduces audio modality for embodied navigation environment.This task requires the agent to navigate to a sound object by seeing and hearing.It encourages researchers to study the role of audio plays in navigation.This work also offer the SoundSpaces [56] dataset for the Audio-visual Navigation task.The SoundSpaces dataset is built upon two simulators, Replica and Matterport3D.It contains 102 natural sounds across a wide variety of categories: bell, door opening, music, people speaking, telephone, etc. Multi-Target Embodied Questioning and Answering (MT-EQA) The natural language questions in EQA are simple since each of them describes one object and lacks attributes and relationships between multiple targets.In MT-EQA [57], the instructions are like "Is the dresser in the bedroom bigger than the oven in the kitchen", where the dresser and the oven locate in different places with different attributes.Thus the agent has to navigate to multiple places, find all targets, analyze the relationships between them, and answer the question.</p>
<p>Interactive Navigation Tasks</p>
<p>Interactive Questioning and Answering (IQA) Building an agent which is able to interact with a dynamic environment is a long-standing goal of the AI community.Recently proposed interactive simulators [39], [40], [41] provide basic functions like opening a door or moving a chair, which enables researchers to build an interactive navigation agent.Interactive Questioning and Answering (IQA) [58] asks an agent to answer questions by interacting with objects in an environment.IQA contains 76,800 training questions that include existence questions, counting questions, spatial relationship questions."Help, Anna!" (HANNA) HANNA [59] is an object-finding task that allows an agent to request help from Automatic Natural Navigation Assistants (ANNA) when it gets lost.Different from NDH that provides a global dialog history as the instruction, the HANNA offers an environment where the instructions dynamically change by the situation.The environment creates an interface that enables a human to help the agent when it gets lost in testing time.</p>
<p>Evaluation Metrics</p>
<p>Many evaluation metrics have been proposed to evaluate how well a navigation agent performs.We divide them into two categories: trajectory-insensitive metrics and the trajectory-sensitive metrics.Trajectory-insensitive Metrics Zhu et al. [15]
PC(P,R)•PL(R) | O(|R| • |P |)
Coverage weighted by LS (CLS) number of steps (i.e., average trajectory length) it takes to reach a target from a random starting point.However, there is a large proportion of trajectories fail when the navigation environment becomes more complex and the navigation task becomes more challenging.Later works [10], [11], [60] introduce propose the Success Rate (SR) to measure frequency of the agent successfully reach the goal and other works [54], [60] report Navigation Error (NE), the mean distance toward the goal when the agent finally stops.Oracle Success Rate (OSR) is proposed to evaluate if the agent correctly stops following the oracle stopping rule [12], [61].These metrics measure the probability of whether the agent completes the task or not, however, fail to measure how much proportion it completes the task.
↑ PC(P, R) • LS(P, R) O(|R| • |P |) Normalized Dynamic Time Warping (nDTW) ↑ exp   − min W ∈W (i k ,j k )∈W d(r i k , p j k ) |R| • d th   O(|R| • |P |)
Trajectory-sensitive Metrics Success weighted by Path Length (SPL) is the first metric that evaluates both the efficiency and efficacy of a navigation agent, and it is regarded as the primary metric in VLN.The SPL ignores the turning actions and the agent heading.Success weighted by edit distance (SED) [62] takes turning actions into consideration and fix this problem.SED is designed for instruction compliance in a graph-based environment, where there exists a certain correct path.However, in some tasks like R4R [46] and R6R [63], the instructed paths are not direct-to-goal.Therefore, it is not appropriate to evaluate the navigation performance of the SPL.Therefore, Coverage weighted by Length Score (CLS) [46] is proposed to measure the fidelity of the agent's behavior to the described path.CLS is the product of two variables: path coverage and length fraction.Ilharco et al. absorb the idea of Dynamic Time Warpping [64], an approach widely used in various areas [65], [66], [67], and propose normalized Dynamic Time Warping (nDTW) metric [68] to evaluate the navigation performance.Similar to CLS, nDTW evaluates the distance between the predicted path with the ground-truth path.Moreover, nDTW is sensitive to the order of the navigation path while CLS is order-invariant.nDTW can be implemented in an efficient dynamic programming algorithm.The path-sensitive metrics, like CLS and nDTW, perform better when they are used as reward functions than target-oriented reward functions in reinforcement learning to navigate [46], [68].</p>
<p>Measurements of Metrics Each metric has its unique characteristics according to their formulation.We compare the formulation and characteristics of existing metrics in Tab. 3. In this part, we introduce measurements to evaluate the functions of a metric: 1) Path Similarity (PS) characterizes a notion of similarity between the P and the R.This implies that metrics should depend on all nodes in P and all nodes in R. PS penalizes deviations from the ground truth path, even if they lead to the same goal.This is not only prudent, as agents might wander around undesired terrain if this is not enforced, but also explicitly gauges the fidelity of the predictions with respect to the provided language instructions.</p>
<p>2) Soft Penalties (SP) penalizes differences from the ground truth path according to a soft notion of dissimilarity that depends on distances in the graph.This ensures that larger discrepancies are penalized more severely than smaller ones and that SP should not rely only on dichotomous views of intersection.</p>
<p>3) Unique Optimum (UO) yields a perfect score if and only if the reference and predicted paths are an exact match.This ensures that the perfect score is unambiguous: the reference path R is therefore treated as a golden standard.No other path should have the same or higher score as the reference path itself.4) Scale Invariance (SI) measures if a metric is independent over different datasets.If a metric variants over datasets, such as navigation error, its scores across different datasets cannot be directly compared.5) Order Sensitive (OS) indicates if a metric is sensitive to the navigation order with the same trajectory length, success rate, etc.</p>
<p>The navigation order reveals some sorts of navigation policy even though it is usually hard to be evaluated.6) Computational Complexity (CC) measures the cost of computing a pair of (P, R).It is important to design a fast algorithm to calculate the score for automatic validation and testing.</p>
<p>Summary</p>
<p>Embodied navigation benchmarks define the tasks and metrics for different settings.The target-oriented tasks like PointGoal, ObjectGoal, and RoomGoal Navigation can provide label by the 3D assets and do not require extra human annotation.cross-modal navigation tasks like R2R [12], Visual Dialogue Navigation [49] or REVERIE [55] require human to label the trajectory and the corresponding language description.The interactive interactive tasks [41], [59] require the agent to learn to manipulate objects, which attract rising attention due to their wide application in realworld scenarios.</p>
<p>METHODS IN SIMULATED ENVIRONMENTS</p>
<p>In this section, we mainly discuss two problems in the simulated environments: target-driven navigation and cross-modal navigation.</p>
<p>And we introduce the methods to solve these problems.</p>
<p>Target-driven Navigation</p>
<p>Methods for this problem focus on navigating from a random starting position to a target.The target may be specified by an RGB image, a vector, or a word.The agent predict actions like turn left, turn right, move forward to navigate in the embodied environment and predict stop indicate the stop action.There are diverse methods that try to solve this problem, including: 1) model-free methods; 2) planning-based methods; and 3) self-supervised methods.</p>
<p>Model-free Methods</p>
<p>The model-free methods learn to navigate end-to-end without modeling the environment, as illustrated in the Fig. 4. The learning objective includes imitation learning or reinforcement learning.The formulation of the learning object is:
L = t −a * t log (p t ) − t a t log (p t ) A t ,(1)
where a * is the ground truth action, p t is the action probability, and A t is the advantage in A3C [69].Though extensive reinforcement learning works [61], [70], [71] have long studied 2D navigation problem where an agent receives global state for each step, the embodied navigation problem with partial observation remains challenging.Many robot control works [72], [73], [74], [75] focus on obstacle avoidance rather than trajectory planning.Zhu et al. [15] firstly propose to use deep learning for feature matching and deep reinforcement learning for policy prediction, which allows the agent to better generalize to unseen environments.Afterwards, Successor Representation (SR) [76] is proposed to enable the agent to interact with objects.This framework takes the states of objects and a discrete description of the scene into consideration.Successor Representation encodes semantic information and concatenate it with the visual representation as in [15].Different from [15] that only uses reinforcement learning to learn a policy predictor, Successor Representation model that bootstraps reinforcement learning with imitation learning.Previous models lack of the ability of encoding temporal information.By This model learned from imitation learning and reinforcement learning.r t is the reward and f (s t ) stands for the labels calculated from the state s t .And a is the label stands for the optimal action.introducing an LSTM layer to encode historical information, Wu et al. [10] are able to build an agent that is able to generalize to unseen scenarios.In the ablation study, this work proves that A3C [69] outperforms DDPG [77] in visual navigation task, and the model learned from semantic mask outperforms which learned from RGB inputs.Inspite of solving visual navigation problem via on-policy deep reinforcement learning algorithms, some works adopt other algorithms.Li et al. [78] propose an end-to-end model based on Q-learning that learns viewpoint invariant and target on invariant visual servoing for local mobile robot navigation.</p>
<p>There are lots of works use segmentation masks of objects to augment visual inputs.Mousavian et al. [16] exploit the instance features in the vision inputs by introducing Faster-RCNN detector trained on MSCOCO dataset [79] and a segmenter defined by [80] to detect and segment objects.Shen et al. [81] improve zero-shot generalization of a navigation agent by fusing diverse visual representations, including RGB features, depth features, segmentation features, detection features, etc.The different visual representations are adaptively weighted for fusing.To further improve the robustness, they propose a inter-task affinity regularization that encourages the agent to select more complementary and less redundant representations to fuse.Despite the well-performed detector and segmenter, learning a robust navigation policy is still challenging.For example, to search for mugs, a human would search cabinets near the coffee machine and for fruits a human may try the fridge first.To address this, Lv et al. [82] integrate 3D knowledge graph and sub-targets into deep reinforcement learning framework.To enhance the cross-scene generalization, Wu et al. [83] introduce an information theoretic regularization term into the RL objective and models the action-observation dynamics by learning a variational generative model.</p>
<p>Some works investigate problem settings other than indoor navigation, such as street view navigation or combining other modalities.Khosla et al. [84] firstly attempt to solve outdoor street navigation task by embodied visual navigation method ,where the agent navigate purely based on panoramic street views.DeepNav [85] is build upon a Convolutional Neural Network (CNN) for navigating in large cities using locally visible streetview images.These works rely on supervised training with the ground truth compass input , however, the compass can sometimes be unavailable in real-world.Another work [86] propose an endto-end deep reinforcement learning framework that uses the street scenes from Google Street View as visual input but without the ground truth compass.Recognizing the importance of localespecific knowledge to navigation, they propose a dual pathway architecture that allows locale-specific features to be encapsulated.AV-WaN [87] is proposed to tackle the challenges in Audio-visual Navigation.This model learns the audio-visual waypoints and</p>
<p>Self-Supervised Methods</p>
<p>Self-supervised learning is a long studied topic of exploiting extra training signals via various pretext tasks.It enables an agent to learn more knowledge without additional human annotations.Various self-supervised tasks have been proposed in the field of deep learning, such as context prediction [88], solving jigsaw puzzles [89], colorization [90], rotation [91].There is also some auxiliary tasks proposed to improve data efficiency and generalization in reinforcement learning.Xie et al. [17] combines self-supervised learning with model-based reinforcement learning to solve robotic tasks.Motivated by traditional UVFA architecture [92] which learns a value function by means of feature learning, Jaderberg et al. [18] invent auxiliary control and reward prediction tasks that dramatically improve both data efficiency and robustness.</p>
<p>In embodied navigation, the environment contains unstructured semantic information that is hard to learn in end-to-end manner.In spite of explicitly modeling the environment using SLAM or memory mechanism, self-supervised learning provide another feasible way of learning the unstructured knowledge.Mirowski et al. [19] propose an online navigation model with two selfsupervised auxiliary objectives, predicting the current depth view by RGB view and detecting the loop closure.Similiar idea [93] has been applied in game applications [94] for rapid exploration.Auxiliary tasks also can speed up learning.Ye et al. [95], [96] achieve great success in PointGoal and ObjectGoal navigation by assembling reinforcement learning with various kinds of auxiliary tasks, formulated as:
L total = L RL + n i β i L Aux,i .(2)
Visual perception is critical for visual navigation.But the training signal provided by reinforcement learning contain too much noise to train a robust feature perception network.An encoderdecoder architecture is proved to be beneficial [97] in visual encoding and segmentation predicting.In addition, an auxiliary task is used to penalize the segmentation error, which benefits the learning of feature perception.However, this self-supervised auxiliary task only learns the low-level dynamic function between two adjacent states and fails to learn the high-level semantic information.To guarantee the semantic consistency of actions in a trajectory, Liu et al. [98] propose an auxiliary regularization task to penalizes the inconsistency of representations.This regularization task encourages the policy network to extract salient features from each sensor.Real-world robot locomotion is far from deterministic due to the presence of actuation noise, which might be caused by wheels slipping, motion sensor error, rebound, etc.To reduce the noise, Datta et al. [99] introduce an auxiliary task of localization estimation by means of temporal difference.The auxiliary task is used to train a CNN network and use the estimated locomotion as an input of the policy network.A curiosity-driven self-supervised objective [100] is applied to encourage exploration while penalizing the repeating actions.A stable curiosity-driven policy without repeating actions could improve the exploration efficiency.Self-supervised auxiliary tasks are also helpful in crossmodal understanding for navigation.Dean et al. [101] use audio as an additional modality for self-supervised exploration.It includes an curiosity driven intrinsic reward, which encourages the agent to explore novel associations between different sensory modalities (audio and visual).An overview of the pipeline of self-supervised navigation methods is shown in Fig. 5.The agent firstly embeds a visual image and an instruction as features.Then the visual feature and the instruction feature are fused to predict the action.The auxiliary tasks use the fused feature to make a prediction, such as predicting the reward, or reconstructing the input visual image.</p>
<p>Planning-based Methods</p>
<p>The map building problem for an unknown environment while solving the localization problem at the same time is known as Simultaneous Localization and Mapping (SLAM) [102], [103].The earlier investigations on visual navigation were carried out with a stereo camera [104], [105] and a monocular camera, such as MonoSLAM [106].Over the past decade, traditional geometricbased approaches [107], [108], [109] remains dominating the field.With the development of deep learning, some methods like CNN-SLAM [110], DVO [111] and D3VO [112] are proposed.Some indoor tasks are proposed to study SLAM, such as KITTI [113] and EuRoC [114].However, these tasks are different from embodied navigation task.The odometry benchmark is to estimate the location given a sequence of visual inputs while the navigation task is to align the instruction with the environment semantics.</p>
<p>Recently, researchers find that the ability of localization is important to navigation, especially for long-term path planning.Thus, some works introduce SLAM methods to model the house and improve the localization ability of the agent.Neural Map [115] generalize this idea for all deep reinforcement learning agents rather than navigation only.However, this work assumes the location of the agent is always known and does not utilize the 2D structure of this memory.Neural SLAM [116] fixes this problem by embedding SLAM-like procedures into the soft-attention [117].To avoid spatial blurring associated with repeated warping, MapNet [118] proposes to use a world-centric rather than an egocentric map.Different from previous works, MapNet maintains a 2.5D representation by a deep neural network module that learns to distill visual representations from 3D embodied visual inputs.Gordon et al. [58] proposes Hierarchical Interactive Memory Network (HIMN), a framework with hierarchical controller for IQA task.The highlevel controller is a planner that decides the long-term navigation target, and the low-level controller predicts the action, interacts with the environment, and answers the question.Gupta et al. [20] introduce the Neural-SLAM method in embodied navigation.This work consists of two parts: mapping and planning.The mapping mechanism maintains a 2D memory map.For each step, it transforms the embodied scene into a 2D feature and update the map with the feature.The planning mechanism uses a value function to output a policy.</p>
<p>Efficient exploration is widely regarded as one of the main challenges in reinforcement learning (RL) [119], [120], [121].Similarly, it is important in navigation since the target does not always visible from the starting position and the agent is required to explore the unseen scene and search for the target.Recently, exploration based on explicitly modeled semantic memory is proven to be efficient.To learn a policy with spatial memory, Chen et al. [122] bootstrap the model with imitation learning and finetune it with coverage rewards derived purely from on-board sensors.Active Neural SLAM (ANS) [22] is a successful neural SLAM method which achieves the state-of-the-art on the CVPR 2019 Habitat Pointgoal Navigation Challenge.ANS proposes a hierarchical structure for planning.Inspired by the idea of hierarchical RL [123], [124], ANS learns the high-level planner by reinforcement learning and learns the low-level planner by imitation learning.The mapper is implemented by an auxiliary task of predicting a 2D map.The first channel of the map stands for if there is an obstacle and the second map stands for whether the position has been explored or not.However, the predefined 2D map cannot help longterm navigation since the semantic information of the scenes in different viewpoints is not encoded in the map.Neural Topological SLAM [125] propose a more advanced way which stores the observed feature representations.This method introduce a graph update module to leverage semantics.The graph update module maintains a topological feature memory.For each step, the module localize current observation into memory nodes.If an observation is not localized in any node of the memory, the graph update module will add a new node into the topological feature memory.Goal-Oriented Semantic Exploration (SemExp) [126] tackles the object goal navigation task in realistic environments.This method first builds a episodic semantic map and uses it to explore the environment based on the category of the target object.This approach achieves state-of-the-art in Habitat ObjectNav Challenge 2020.An overview of the common practice of the 'Neural SLAM'based model is shown in Fig. 6.In addition to the visual encoder and the instruction encoder as in Fig. 5, 'Neural SLAM'-based model have a unique module to project a embodied visual view to feature representation and store it in a 2D top-down map:
m t , xt = f SLAM (s t , x t−1:t , m t−1 |θ S )(3)
where x t−1:t stands for previous poses, m t−1 is previous maps, and θ S stands for parameters.This map models the room structure and visual representation of scenes.The projected feature representations are fused with the visual feature and the instruction feature to jointly predict an action.</p>
<p>Summary</p>
<p>Compared with traditional robotics methods, the model-free methods are able to obtain robust navigation models by sampling large scale data with the embodied simulator.Some works adopt detection and segmentation approaches to get better visual views.In spite of indoor scenarios, model-free methods achieve great success in street scene and multi-modal environments.Self-supervised methods are proposed to exploit the extra knowledge by auxiliary tasks to improve the learning efficiency and generalization ability.</p>
<p>Planning-based methods utilize a 2D map or a topological memory to model the environment during navigation.</p>
<p>Cross-modal Navigation</p>
<p>A navigation robot who understands natural language can accomplish more complex tasks, such as "pick up the cup in the kitchen" or "help me find my glass upstairs".In this section, we introduce three kinds of works that solves cross-modal navigation tasks: 1) step-by-step methods; 2) pretraining-based methods; 3) planning based methods.</p>
<p>Sequence-to-sequence Navigation</p>
<p>Anderson et al. [12] firstly propose a sequence-to-sequence model similar to [127] to address the vision language navigation problem.This model sequentially encodes a language instruction word-byword, concatenates the sentence feature with the visual image feature and decodes the action sequence.However, a sequence-tosequence model is lack of stability and generalization since it fails to consider the dynamics in the real-world environments.RPA [23] is proposed to tackle the generalization issue by equipping a 'lookahead' module, which learns to predict the future state and reward.</p>
<p>To improve the generalization ability in instruction-trajectory alignment, Fried et al. [24] propose a data augmentation approach named "speaker-follower" to improve the model generalization.</p>
<p>To generate augmentation data, the speaker firstly translates a randomly trajectory into an instruction, and the follower secondly translates an instruction into a trajectory:
argmax r∈R(d) P S (d|r) • P F (r|d) (1−γ) ,(4)
where P S is the speaker, P F is the follower, d stands for an instruction, r stands for a trajectory, and γ is a weighting factor.Another contribution of this paper [24] is the definition of a highlevel action that move forward toward an orientation in a panoramic space in stead of low-level actions like turn left, turn right and go forward.Compared with the definition of low-level actions, this approach largely reduce the length of the action sequence that describes the same trajectory.Navigating by the high-level action space requires less prediction times, which makes the model easier to train and more robust to test.Howeverr, previous methods learn to navigate by imitation learning only with the instructiontrajectory data pairs, which supervises the shortest path while ignore the sub-optimal trajectories so that leds to overfitting.</p>
<p>To tackle this problem, Wang et al. [128] propose to jointly learn a navigation agent by imitation learning and reinforcement learning.In addition, this method introduce an LSTM to encode the temporal information of visual features and introduce a crossmodal mechanism to achieve better vision-language navigation ability.Ma et al. [129] propose a self-monitoring agent with a visual-textual co-grounding module and progress monitor.The progress monitor use the cross-modal feature from the co-grounding  regard the step-by-step navigation process as a visual tracking task.This approach implements the navigation agent within the framework of Bayesian state tracking [3] and formulates an end-toend differentiable histogram filter [133] with learnable observation and motion models.One commonly used method that relieve the visual overfitting is to apply an dropout [134] layer on the visual feature, which is extracted by a pretrained network like VGG [135] or ResNet [136].Tan et al. [137] argue that simply applying a dropout layer on the visual feature leads to inconsistency, e.g. a chair in this frame could be dropped in the next frame.To solve the problem, They propose a environmental dropout layer that randomly dropout some fixed channels during a trajectory.Zhu et al. [138] propose AuxRN, a framework that introduce selfsupervised auxiliary tasks to exploit environmental knowledge from several aspects.In addition to introducing the temporal difference auxiliary task that is widely use in other embodied visual navigation methods [19], [96], AuxRN introduces a trajectory retelling task and instruction-trajectory matching task that learn the temporal semantics of a trajectory.Instead of generating the low-quality augmented data, Fu et al. [139] introduce the concept of counterfactual thinking to sample challenging paths to augment training dataset.They present a model-agnostic adversarial path sampler (APS) to pick the difficult trajectories and only consider useful counterfactual conditions.Different from the earlier works that based on data augmentation and other classical navigation methods, some works discover the importance of natural language to VLN.Thomason et al. [140] find the unimodal baseline outperforms random baselines and even some of their multimodal counterparts.Thus the work advocates that ablating unimodal to evaluate the bias is important to proposing a dataset.A study of Huang et al. [141] shows that only a limited number of those augmented paths in [24] are useful and after using 60% of the augmented data, the improvement diminishes with additional augmented data.To avoid the extensive work in reward engineering, Wang et al. [142] propose a Soft Expert Reward Learning model that includes two parts: 1) soft expert distillation, which encourages agents to behave like an expert in soft fashion; 2) self perceiving, which pushes the agent towards the final destination as fast as possible.Xia et al. [143] leverages multiple instructions as different descriptions for the same trajectory to resolve language ambiguity and improve generalization ability.This work indicates that the human annotations in VLN are largely biased according to the specific scene and the trajectory.The quality of visual features is critical for improving the performance of embodied navigation.Previous works extract global visual features from panoramic views by a pretrained CNN network like ResNet-101 [136].Hong et al. [144] introduce Faster-RCNN to detect objects in navigation and build a relationship graph between visual and language entities for vision-language alignment.In spite of the visual inputs, the structure information also helps navigation.Hu et al. [145] discover that the language instructions contain high-level semantic information while visual representations are a lower-level modality, which makes the vision-language alignment difficult.Motivated by this, they decomposes the grounding procedure into a set of expert models with access to different modalities and ensemble them at prediction time.To better research what role does language understanding play in VLN task, Hong et al. [48] argue that the intermediate supervision is important in vision-language alignment.Thus, they propose FGR2R, a method which enables navigation processes to be traceable and encourage the agent to move at the level of sub-instructions.</p>
<p>Pretraining-based Methods</p>
<p>Several challenges are discovered during the research on the visionlanguage navigation: 1) low training efficiency; 2) large data bias (include both vision and language); 3) lack of generalization from seen to unseen scenes.To address these challenges, pretrainingbased models are proposed to learn from large-scale data sets from other sources and fast adapt to unseen scenarios.</p>
<p>Low training efficiency</p>
<p>The traditional encoder-decoder framework first samples the total trajectory by teacher-forcing or studentforcing and then back-propagate the gradients.In other deep learning tasks like image classification [1] or text recognition [146], the model predicts a result directly.However, in the vision-language navigation task, the agent predicts a trajectory by interacting with the environment in a step-by-step manner, which is so timeconsuming that reduce the training efficiency.Large data bias The vision-language navigation scenarios are so diverse that 61 houses in R2R cannot cover all of them.From the aspect of natural language, in the R2R task, only 69% of bigrams are shared between training and evaluation.Lack of generalization Lacking of diverse training data still largely limits the generalization in spite of the proposed augmentation methods like trajectory augmentation, visual feature augmentation and natural language augmentation.Thus, introducing extra knowledge from other tasks and datasets becomes a promising topic.</p>
<p>Pretraining-based methods largely improve the generalization ability of a model by learning in large scale of data [51],</p>
<p>VDN: Find a Table</p>
<p>Where to go?Is the table next to the refrigerator?</p>
<p>3</p>
<p>No, it is behind the chairs Find the table! 4 Fig. 9: A comparison of three embodied vision-language navigation tasks: EQA-v1 [54], MT-EQA [57] and VDN [49].[136].Furthermore, bert-based methods [147], [148] pretrain a transformer network with proxy tasks and achieve great success in vision, language and cross-modal tasks.Many researchers consider to solve the vision-language navigation problem by pretainingbased methods.Li et al. [149] propose PRESS first introduce a pretrained language models to learn instruction representations.And they propose a stochastic sampling scheme to reduce the gap between the expert actions in training and the sampled actions in testing.Majumdar et al. [150] advocate to improve model by leveraging large-scale of web data.However, it is hard to transfer the static image data to VLN task.Therefore, they propose VLN-bert, a transformer-based model which is pretrained by static images and their captions.PREVALENT [151] is self-supervisedly pretrained on large amount of image-text-action triplets sampled from an embodied environment with two pretrianing objectives, masked language modeling (MLM) and action prediction (AP):
L M LM = −E s∼p(τ ),(τ,x)∼D E log p(x i |x, s), L AP = −E (a,s)∼p(τ ),(τ,x)∼D E log p(x|x [CLS] , s),(5)
where (s, a) a state-action pair.PREVALENT is proven to be effective on several vision-language navigation datasets, including R2R, CVDN and HANNA.The embodied navigation agent receives partial observation rather than global observation, which is better to be modeled as a partially observable Markov Decision Process.Different from the encoder-decoder model, previous pretraining-based models do not memorize previously seen scenes during navigation and utilize temporal knowledge, which causes information loss in action prediction.Motivated by this, Hong et al. [152] propose a recurrent multi-layer transformer network that is time-aware for use in VLN.This method introduce a Transformer which maintains a feature vector to represent temporal context.</p>
<p>Navigation with Questioning and Answering</p>
<p>Instead of passively perceive natural language instructions from a human commander, Das et al. [57] suggest that an intelligent agent should be able to answer a question via navigation.[153] to decide how many times to repeatly execute an action [57].The PACMAN is bootstrapped by shortest path demonstrations and then fine-tuned with RL.However, this method is lack of the ability of high-level representation.In a later work [154], Das et al. propose a hierarchical policy named Neural Modular Controller (NMC) that operates at multiple timescales, where the higher-level master policy proposes sub-goals to be executed by low-level sub-policies.Anand et al. [155] find that a blindfold (question-only) baseline on EQA and find that the baseline perform previous state-of-the-art models.They suggest that previous EQA models are ineffective at leveraging the context from the environment and the EQAv1 dataset has lots of noise.</p>
<p>Wu et al. [156] propose a simple supervised learning baseline which is competitive to the state-of-the-art EQA methods.To improve EQA performance in unseen environment, in this paper, they propose a setting in which allows the agent to answer questions for adaptation.Yu et al. [57] argues that the EQA task assumes that each question has exactly one target, which limits its application.Therefore, Yu et al. present Multi-Target EQA (MT-EQA), a generalized version of EQA.The question of this task contains multiple targets.And it require the agent to perform comparative reasoning over multiple targets rather than simply perceive the attributes of one target.Wijmans et al. [157] extend the EQA problem to photorealstic environment.In this environment, they discover that point cloud representations are more effective for navigation.Luo et al. [158] suggest that the visual perception ability limits the performance of the EQA.They introduce Flownet2 [159], a high-speed video segmentation framework as a backbone to assist navigation and question answering.Li et al. [160] propose a MIND module that model the environment imagery and generate mental images that are treated as short-term sub-goals.Tan et al. [161] investigate the questioning and answering problems between multiple targets.In this task, the agent has to navigate to multiple places, find all targets, analysis the relationships between them, and answer the question.Motivated by recent progress in Visual Question Answering (VQA) [50] and Video Question Answering (VideoQA) [162], Cangea et al. [163] propose VideoNavQA, a dataset that contains pairs of questions and videos generated in the House3D environment.This dataset fills the gap between the VQA and the EQA.The VideoNavQA task represents an alternative view of the EQA paradigm: By providing nearlyoptimal trajectories to the agent, the navigation problem is easier to solve compared with the reasoning problem.Deng et al. [164] propose Manipulation Question Answering (MQA) where the robot is required to find the answer to the question by actively exploring the environment via manipulation.To suggest a promising direction of solving MQA, they provide a framework which consists of a QA module (VQA framework) and a manipulation model (Q learning framework).Nilsson et al. [165] build an agent which explores in a 3D environment and occasionally requests annotation during navigation.Similarly, Roman et al. [166] suggest a two-agent paradigm for cooperative vision-and-dialogue navigation.Their model learns multiple-skills, including navigation, question asking, and questioning-answering components.</p>
<p>Navigation with Dialogue</p>
<p>There is a long history that human use a dialog to guide a robot [167], [168].In the field of embodied navigation, Banerjee et al. [59] propose "Help, Anna!" (HANNA), an interactive photorealistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural language and vision assistance.Nguyen et al. [169] propose a task named VLNA, where an agent is guided via language to find objects.However, the language instruction in these two tasks far from real-world problem: the responses of HANNA are automatic generated from a trained model while the guidance of VLNA are in the form of templated language that encodes gold-standard planner action.Vries et al. [170] propose "Talk The Walk" (TtW), where two humans communicate to reach a goal location in an outdoor environment.However, in TtW, the human uses an abstracted semantic map rather than an egocentric view of the environment.</p>
<p>Thomason et al. [49] propose vision-and-dialog navigation (VDN), a scaffold for navigation-centered question asking and question answering tasks where an agent navigates following a multi-round dialog history rather than an instruction.Compared with the single-round instructions in R2R dataset, VDN provides multi-round annotation, in which each round of dialogue describes a sub-trajectory.The more fine-grained dialogue annotation facilitate researchers to study the problem of navigation with natural language.Zhu et al. [138] propose a framework with a cross-modal memory mechanism to capture the hierarchical correlation between the dialogue rounds and the sub-trajectories.More generally, several methods, such as PREVALENT [151] and BABYWALK [63], validate their navigation ability using both sentence instructions and dialog instructions.Unfortunately, these works heavily rely on dialogue annotations which is labor-intensive.To alleviate this, Roman et al. [166] exploit to generate dialogue questions answers based on visual views.This work addresses four challenges in modeling turn-based dialogues, which includes: 1) deciding when to ask a question; 2) generating navigator questions; 3) generating questionanswer pairs for guidance; 4) generating navigator actions.To achieve this, Roman et al. [166] introduce a two-agent paradigm, where one agent navigates and asks questions while the other guides agent answers.Different from previous works that guide navigator with template language, this work initialize the oracle model via pretraining on CVDN dialogues to generate natural language.</p>
<p>A dialog does not always describe a step-by-step navigation process.Rather, the oracle describes the target scene and let the navigator to find it, which commonly occurs when someone get lost in a new building.Hahn et al. [171] propose a LED task (localizing the observer from dialog history) to realize when it get lost.Motivated by this, they present a dataset named WHERE ARE YOU [171] that consists of 6k dialogues of two humans.Due to the wide application of multi-agent communication systems [172], [173] in real-world, researchers become interested in implementing dialog navigating in physical environments.Marge et al. [174] present MRDwH, a platform that implements autonomous dialogue management and navigation of two simulated robots in a large outdoor simulated environment.Banerjee et al. [175] propose RobotSlang benchmark, a dataset which is gathered by pairing a human "driver" controlling a physical robot and asking questions of a human "commander"</p>
<p>We compare the difference of Embodied Question Answering (EQA) [54], Multi-Target Embodied Question Answering (MT-EQA) [57] and Vision-and-dialog navigation (VDN) [49] in Fig. 9.We demonstrate three different dialogues for the same navigation trajectory as an example.Compared with EQA, the question in MT-EQA are more complex since it should describe multiple targets.The agent have to acquire high-level skills, such as reasoning, comparison and multi-object localization, to accomplish MT-EQA.In the EQA and MT-EQA tasks, the agent is required to answer question from a human via navigation.However, in the VDN task, the agent is the navigator and the questioner which asks a human for hints to find the target.The difference of the task setting led to the different designs of the navigation model.</p>
<p>Summary</p>
<p>Natural language provides an interface for a human to interact with a robot.A robot with cross-modal understanding is able to accomplish complex tasks such as navigating following a natural language instruction or a dialogue, asking the oracle for more details, etc. Lots of works have been proposed to research on vision-language navigation problem from diverse aspects.</p>
<p>METHODS IN REAL-WORLD ENVIRONMENTS</p>
<p>Embodied navigation methods in simulated environments give a promising direction of solving real-world navigation problems.In this section, we are going to 1) introduce methods for real-world applications; 2) compare them with the methods in simulators; 3) discuss the possibility of sim-to-real transferring.</p>
<p>Real-world Navigation Methods</p>
<p>Indoor Robotic Navigation</p>
<p>Deep learning plays an important role in indoor navigation for realworld applications.LeCun et al. [176] firstly adopt convolutional network for obstacle avoidance.Hadsell et al. [177] propose a selfsupervised learning process that accurately classifies long-range vision semantics via a hierarchical deep model.The method is validated on a Learning applied to ground robots (LAGR) [25].Later, more and more real-world robots adopt deep learning to perceive and extract distinctive visual features [178].Zhang et al. [179] research on the problem where a real robot navigates in simple maze-like environments.Based on the success of RL algorithms for solving challenging control tasks [77], [180], Zhang et al. employ successor representation in learning to achieve quick adaptation.Morad et al. [26] present an indoor object-driven navigation method named NavACL that uses automatic curriculum learning and is easily generalized to new environments and targets.Kahn et al. [181] adopt multitask learning and off-policy RL learning to learn directly from real-world events.This method enables a robot to learn autonomously and be easily deployed on multiple real-world tasks without any human provided labels.</p>
<p>Outdoor Robotic Navigation</p>
<p>There has been a long history that human study outdoor navigation robot.Thorpe et al. [27] present two algorithms, a RGB-based method for road following and a 3D-based method for obstacle detection, for a robot to learn to navigate in a campus.Ross et al. [182] combine deep learning and reinforcement learning to learn obstacle avoidance for UAVs.Morad et al. evaluate the performance of NavACL on two simulated environments, Gibson and Habitat.And we transfer the navigation to a Turtlebot3 wheeled robot (AGV) and a DJI Tello quadrotor (UAV).Both quantitative and qualitative results reveal that the policy of NavACL trained in the simualted environment is surprisingly effective in AGV and UAV.Manderson et al. [183] use conditional imitation learning to train an underwater vehicle to navigate close to sparse geographic waypoints without any prior map.</p>
<p>Long-range Navigation</p>
<p>Their model achieves the best performance and shows competitive generalization ability on a real robot platform.Borenstein et al. [184] propose to maintain a world model [185] that updated continuously and in real-time to avoid obstacles.The world model learns and simulates the real-world environment and reduce the cost of data sampling [185].Liu et al. propose Lifelong Federated Reinforcement Learning (LFRL), a learning architecture for navigation in cloud robotic systems to address this problem.</p>
<p>Long-range navigation is challenging for real-world robots.To address this proble, Francis et al. [186] present PRM-RL, a hierarchical robot navigation method.The PRM-RL model consists of a reinforcement learning agent that learns short-range obstacle avoidance from noisy sensors, and a sampling-based planner to map the navigation space.Shah et al. [187] propose ViNG, a learning-based navigation system for reaching visually indicated goals and demonstrating this system on a real mobile robot platform.Unlike prior work, ViNG uses purely offline experience and does not require a simulator or online data collection, which significantly improves the training efficiency.Mapping [188] and path planning [189] has also been widely adopted by many realworld applications.Davison et al. [190] builds an automatic system, which is able to detect, store and track suitable landmark features during goal-directed navigation.They show how a robot can use active vision to provide continuous and accurate global positioning, thus achieving efficient navigation.Sim et al. [191] enable a robot to accurately localize its location by employing a hybrid map representation of 3D point landmarks.</p>
<p>NAVIGATION FROM SIMULATOR TO REAL-WORLD</p>
<p>In this section, we first demonstrate the challenges in the realworld navigation by comparing the difference between simulated environments and the real-world environment.Then we introduce the methods that focus on solving these challenges.</p>
<p>Comparison of Simulated and Real Navigation</p>
<p>Today, current achievements in the simulated navigation are still far of building a real-world navigation robot.Compared with the simulated environments, the real-world navigation environment is much more complex and ever changing.An comparison of inputs between a simulated environment (Habitat [13]) and the real-world environment is shown in Fig. 10.</p>
<p>Reasons of Domain Gap</p>
<p>We summarize three aspects cause the sim-real domain gap: 1) observation space; 2) action space; 3) environmental dynamics.Observation Difference.An observation of the simulated environment can be an RGB image, a depth image, or a ground truth map.The quality of the RGB image and depth image inputs are high.The environment contains all static object information and enable it to provide ground truth information, like room structure, segmentation or object labels.The simulated environments provide unreal synthetic images with fewer objects where the real-world environments are far more complex with many.The sensors in the real-world environment, including RGB, GPS, and the velocity sensor, are usually noisy while the sensors in the simulated environment have no noise.Although some simulators [11], [13], [14] provide physical sensors and simulate some physical interactions (such as collision and acceleration), the performance of their physics engine is still far from real.Action Difference.Different from the simple action space consists of 'turn left', 'turn right', and 'go forward' in the simulated environment, the action space in the real world is more challenging, depending on the structure of the robot.Lots of obstacles exist during real-world navigation, which blocks the robot from turning or moving forward.Real-world environments are often dynamic since the environment is so complex that many factors are changing in the long term or short term, such as temperature, moisture, friction, obstacles, and pedestrians.Another challenge which is also  Fig. 11: The performances of methods on the Habitat ObjectGoal navigation, including DD-PPO [195], Active Exploration [22], SemExp [126] and 6-Act Tether [96].</p>
<p>widely ignored in the simulated environments is the complexity and instability of the action space.For example, the results of executing the same action are uncertain since the physical condition is evolving, such as the wheels are skidding or get stuck.Environmental Dynamics.The evolving of environmental conditions, such as temperature, humidity or parts wear cause the environmental dynamics.A policy without online adaptation ability cannot handle this problem well.Recently, more and more attention has been paid to the adaptive policy of learning dynamic environment.Some works [192], [193], [194] propose simulated robot environments to accomplish this, however, the simulation is far simpler than the real-world.</p>
<p>Solutions for the Domain Gap</p>
<p>The domain gap brings critical challenges and researchers put forward methods to fill the gap between these two settings.Mobile robot navigation is considered a geometric problem, which requires the robot to sense the geometric shape of the environment in order to plan collision-free paths to reach the target.Obstacle avoidance is one of the most important challenges, and many methods [27], [176], [186] have been put forward in previous work to achieve this.However, robot navigation in simulated tasks are regarded as a policy learning problem that learns a robust navigation policy from a starting position to the target in a complex environment with many possible routes.SLAM-based methods as in [60], [116] contribute a lot to mapping and path planning, which is general for both simulated and real navigation.Deep learning shows its ability in processing images and learning policies for robotic control, which is widely applied in both settings.However, the usages of deep learning are different between simulated navigation and real navigation.In real-world navigation, the deep neural network is used to perceive RGB inputs [177], predict the future [197] and learn the navigation policy [186].However, due to the sampling inefficiency and the complex dynamic factors of the real-world environment, the policy is not robust enough.Some works [184], [185] propose to model the environment and other works [186] adopt handcrafted rules to improve the robustness of the navigation policy.Data sampling is much more efficient in simulated environments.Most of the simulators render RGB and depth images in more than hundreds of frames per second (FPS), in which the fastest simulator, Habitat [13], achieves 100,000 FPS.Fast data sampling enables learning with large batch size.Many works prove that a large training batch size leads to robustness in representations [45], [198].In spite of the rendered RGB and depth Fig. 12: The performances of methods on the Habitat PointGoal Challenge, including DD-PPO [195], ego-localization [99], Occupancy Anticipation [199] and SLAM-net.</p>
<p>images, some simulated environments are able to provide semantic segmentation masks [13], [14], [157].A more accurate simulator with few noise to facilitate training.With richer, noise-free data, researchers can apply a deeper neural network on navigation agents without worrying about overfitting.For example, Transformer [117] is widely applied in navigation works in simulated environments due to its capability of feature representation while it is easily overfitting if it is trained on noisy data.</p>
<p>Learning Efficiency</p>
<p>Many researchers focus on learning efficiency since data sampling in the real world is slow and expensive.Lobos-Tsunekawa et al. [200] propose a map-less visual navigation method for biped humanoid robots.In this method, DDPG algorithm [77] is used to extract information from color images, so as to derive motion commands.This method runs 20 ms on a physical robot, allowing its use in real-time applications.Bruce et al. [201] present a method for learning to navigate to a fixed goal on a mobile robot.By using an interactive replay of a single traversal of the environment and stochastic environmental augmentation, Bruce et al. demonstrates zero-shot transfer under real-world environmental variations without fine-tuning.To further improve the sampling efficiency, Pfeiffer et al. [202] leverage prior expert demonstrations for pre-training so that the training cost could be largely reduced in the fine-tuning process.</p>
<p>Navigation Transferring</p>
<p>Transfer learning is attracting rising attention in embodied navigation.The researchers are motivated from two aspects: 1) learn a navigation agent that is able to perform accurate and efficient navigation in diverse domains and tasks; 2) deploy an agent trained in a simulated environment in a real-world navigation robot.It is challenging to train a model to learn skills for navigating in different domains.Moreover, due to the large domain gap between simulated environments and the real-world environment, a wellperformed navigation policy trained on a simulated environment cannot be easily transferred to the real-world environment.A lot of navigation tasks have been proposed to investigate different capabilities for navigation in diverse scenarios.</p>
<p>In this section, we discuss the transfer learning in navigation from two different levels:  transferring requires the model to be invariant to different dynamics and transition functions.DisCoRL [28] introduce a policy distillation method [203] to transfer a 2D navigation policy.In addition to the navigation policy, the vision and language embedding layer could also be transferred [29].Motivated by the success of meta-learning [204], Dimension-variable skill transfer (DVST) [205] obtains a metaagent with deep reinforcement learning and then transfers the meta-skill to a robot with a different dimensional configuration using a method named dimension-variable skill transfer.Similarly, Li et al. [206] propose an unsupervised reinforcement learning method to learn transferable meta-skills.Zhu et al. [63] decompose long navigation instructions into shorter ones, and thus enables the model to be easily transferred to navigation tasks with longer trajectories.Chaplot et al. [207] propose a multi-task model that jointly learns multi-modal tasks, and transfers vision-language knowledge across the tasks.The model adopts a Dual-Attention unit to disentangle the vision knowledge and language knowledge and align them with each other.</p>
<p>Wang et al. [208] propose to learn environment-agnostic representations for the navigation policy enables the model to perform on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks.Yan et al. [30] propose MVV-IN, a method that acquires transferable meta-skills with multi-modal inputs to cope with new tasks.Liu et al. [209] investigate on how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments.Gordon et al. [210] propose to decouple the visual perception and policy to facilitates transfer to new environments and tasks.</p>
<p>Sim-real transferring have been well studied in the field of robotic control [211], [212].Sadeghi et al. [213] firstly propose a learning-based method, which trains a navigation agent entirely in a simulator and then transfers it into real-world environments without finetuning on any real images.Consequently, Yuan et al. [214] adopt a sim-real transfer strategy for learning navigation controllers using an end-to-end policy that maps raw pixels as visual input to control actions without any form of engineered feature extraction.Tai et al. [215] train a robot in simulation with Asynchronous DDPG [77] algorithm and directly deployed the learned controller to a real robot for navigation transferring.Rusu et al. [212] introduce a progressive network to transfer the learned policies from simulation to the real world.Similarly, adversarial feature adaptation methods [216] is also applicable in sim-to-real policy transferring [217].Sim-to-real transfer for deep reinforcement learning policies can be applied to complex navigation tasks [218], including six-legged robots [219], robots for soccer competitions [220], etc.</p>
<p>Summary</p>
<p>In this section, We firstly compare the difference between simulated environments and real-world environments.Then, we reason about the domain gaps that cause the domain gaps.Finally, we introduce some transfer learning works in the navigation to give a promising direction to solve this problem.</p>
<p>FUTURE DIRECTIONS</p>
<p>Although extensive works have addressed the navigation problem from diverse aspects, current research progress is still far from real artificial intelligence.Also, current work cannot build a robust robot for real-world navigation.We summarize the challenges in solving embodied AI into these aspects: 1) the functions and the performance are limited by the embodied environment; 2) the navigation problem is not well defined; 3) the performances of embodied AI agents in complex environments are still poor; 4) perceiving natural language is difficult to learn; 5) hard to deploy a trained navigation policy to the real-world application.Future Embodied Environments.The advanced functions in the environment help the navigation model to obtain high-level abilities.For instance, compared to the early embodied environments, the large scene in the Matterport3D [36] firstly requires the navigation model to explore and memorize the complex room structure.The vision-language navigation benchmark [12] enables the agents to perceive natural language.The interactive embodied environment like AI2-THOR [40] and iGibson [41] enable the agent to perform  interactive actions.The agent learned in an interactive environment is able to move an object, put an object and open a door.An environment with more functions is the basis of learning a smart agent.An agent must be able to handle a dynamic environment when the objects in the rooms with evolving conditions.In stead of navigating within the navigable areas like [12], [13], we expect an agent to find possible roads within a room that has many obstacles.In addition, we need an interactive agent, which can pick up and put down objects, move chairs, and interact with human beings.Other modes such as walking, running, and climbing also need to be considered if we want to build a robust navigator within a complex indoor environment.Define Advanced Navigation Tasks.Even though many embodied navigation tasks and navigation metrics have been proposed, what is a good navigation policy remains unclear.This problem has two folds: 1) what factors have to be considered; 2) how to balance these factors.As we analysed in Sec.3.4, the accuracy and efficiency are two main factors to evaluate the performance of navigation.However, the importance of accuracy and efficiency are different among metrics.Optimal navigation policy varies according to different metrics.In the interactive navigation task proposed by [221], the performance of the agent is evaluated by a path efficiency score and a effort efficiency score.The interactive navigation task varies the score weights in evaluation to test if an agent performs well in different settings.However, it is still unclear how the weight of the factors affects the test results and what setting it is in real-world application.</p>
<p>Moreover, in the questioning-answering settings like Help Anna [59], or RMM [166], the frequency of questioning or requesting from the agent are take into consideration.Current methods regard that the performance is lower if the agent ask for more information from human.Balancing the 'cost' of asking questions and the 'cost' of navigation is still a challenging problem.We hope that the community could publish more works discussing on how to evaluate the advanced navigation behavior or comparing the differences of navigation policies between an agent and a human.Improve Navigation Performance.Current navigation agents still perform poorly even in easy navigation tasks such as PointGoal task and ObjectGoal task, as shown in Fig. 12 and Fig. 11.The state-of-the art model of the PointGoal task performs 64.5% on success rate (SR) and 37.7% on SPL, while on the ObjectGoal task, the best model performs 21.08% on success rate (SR) and 8.38% on SPL.As shown in Fig. 4, the current state-of-the-art model [152] in the vision-language navigation task performs 63% in SR and 57% in SPL while the human performance is 86% in SR and 76% in SPL.The navigation performances of current models are still far from human performance.Besides, existing models usually perform poorly on the challenging task of vision-and-language navigation.As shown in Tab. 4, there is about 19% performance gap between the state-of-the-art model and the human baseline.In the interactive dialog tasks [59], [166], [222], the natural language used by agents to interact with oracle has many errors and is not fluent.The baselines in the recently proposed tasks [221] can hardly complete the task.Moreover, the navigation robot in the real world cannot perform as well as in the simulated environment.</p>
<p>Several promising directions, which are motivated by referring recent works, could tackle these problems.Transformer [117] shows its capability in feature extraction and cross-modal fusion.Some works [151], [152] build navigation models based on Transformer and achieve great success in vision-language navigation task, which reveals that Transformer structure is beneficial for crossmodal navigation policy.Chaplot et al. [125] build a neural SLAM module into a navigation model and train the model via hierarchical reinforcement learning.This model is able to learn the structure of the room and perform a robust low-level navigation policy in an environment with continuous state space.We suggest that model-based methods [96], [138] and hierarchical reinforcement learning [223] are the key to build a robust navigation model.Smartly Perceiving Natural Language.Natural language is a complex modality for a robot to understand due to its diversity and complexity.At this moment, however, teach a navigation robot to learn to understand language requires a large amount of natural language annotations and each of them describes the semantics of a trajectory, a scene or a kind of behavior.The language annotations can be a word, a sentence, a question-answer pair or a dialogue, which are pretty expensive and labor-intensive.</p>
<p>Even if we have sufficient language annotations for training a navigation robot, it is still challenging for the robot to correctly understand language instructions.For example, because there are many natural language variants for describing the same trajectory or scene, supervising an agent with trajectory-instruction pairs may led to severe overfitting.In addition, the skill of perceiving natural language needs prior knowledge.For example, "find the forth chair in the living room" requires the agent be able to count and "navigate to bathroom safely" requires the agent to turn smoothly and do not touch any objects.Some works [151], [152] adopt pre-training methods to obtain a better language understanding skill with prior knowledge.Based on the success of these works, we believe that learning from other large-scale language datasets [147], [224], [225] and transfer the prior knowledge might be a promising direction in solving the challenges in understanding natural language instructions.Deploy Robust Policies on Real World.Even though we have obtained a robust navigation policy in a simulated environment, how to deploy this policy to real-world still remains challenging.As demonstrated in Sec.6.1, three major differences cause the large sim-real domain gap: 1) observation; 2) action space; 3) environmental dynamics.Large sim-real domain gap hinders the direct deployment of the learned navigation policy to the real world.There are two directions in tackling this problem.One way is building a realistic simulator, including a realistic visual image rendering mechanism, advanced physical sensors, obstacle objects, dynamic simulation, simulation of robot components like wheels and gears, etc.However, such a realistic visual simulator is computation costly.Another way of solving sim-real deployment problem is achieving online adaptation by transfer learning or meta-reinforcement learning [204], [226].These methods enable an agent to change its policy to adapt the environment.This method not only has high computational efficiency, but also has stronger adaptability when accidents happen.</p>
<p>CONCLUSION</p>
<p>This paper presented a comprehensive survey on the embodied navigation scenario by summarizing hundreds of works.We thoroughly investigate the environments, tasks, and metrics to introduce the problem that the researchers are trying to solve.And we introduce hundreds of methods that solve these tasks in the embodied environments and compare their differences.Then we introduce the methods in the real-world environment and demonstrate how the large domain gap led to the drop in navigation performance.At last, we analyze the current problems that exist in the embodied navigation and give out four future directions to improve our community.</p>
<p>Fig. 1 :Fig. 2 :
12
Fig. 1: A taxonomy of deep learning methods for embodied navigation.</p>
<p>Fig. 3 :
3
Fig. 3: The render scenes of each dataset.</p>
<p>Fig. 4 :
4
Fig.4: An illustration of an model-free visual navigation model.This model learned from imitation learning and reinforcement learning.r t is the reward and f (s t ) stands for the labels calculated from the state s t .And a is the label stands for the optimal action.</p>
<p>Fig. 5 :
5
Fig. 5: An illustration of an end-to-end visual navigation model with self-supervised objectives.r t is the reward and f (s t ) stands for the labels calculated from the state s t .dynamically sets intermediate goal locations based on its audiovisual observations and partial maps in an end-to-end manner.</p>
<p>Fig. 6 :
6
Fig. 6: An overview of the common practice of the "Neural SLAM"based model."ST" is the spatial transformation function.</p>
<p>Fig. 7 :
7
Fig. 7: A comparison of seq-to-seq models in VLN and VQA.</p>
<dl>
<dt>Fig. 8 :</dt>
<dt>8</dt>
<dt>Fig. 8: An example of pretraining-based framework.</dt>
<dd>
<p>what color is the car?Orange MT-EQA: Does the bed in the bedroom has the same color as the chairs in the kitchen?</p>
</dd>
</dl>
<p>Fig. 10 :
10
Fig. 10: A comparison of input space and action space of a simulated environment and the real-world environment.</p>
<p>Fig. 13 :
13
Fig.13: A summary of future directions for building an advanced robotic for real-world navigation.</p>
<p>TABLE 1 :
1
Comparison of existing embodied datasets (*: the datasets render only a room as scene).</p>
<p>TABLE 3 :
3
We compare the existing metrics from several aspects, including performance (↑ indicates the higher the better, ↓ indicates the lower the better), Formulation, Path similarity (PS), Soft Penalties (SP), Unique Optimum (UO), Scale Invariance (SI), Order Sensitivity (OS), and Computational Complexity (CC).Suppose we have a predicted trajectory P and a ground truth trajectory R. p i and r i are the ith node on trajectory P and R. |P | and |R| stand for the length of P and R respectively.The Dijkstra distance of the house has been preprocessed and the computation complexity of any d(p i , r j ) is O(1).</p>
<p>1) task-level transferring; 2) environmentlevel transferring, including sim-to-real transferring.The tasklevel transferring requires the agent to learn a policy that adapts to different input modalities or targets; the environment-level
MethodsR2R Validation Seen TL NE↓ SR↑ SPL↑R2R Validation Unseen TL NE↓ SR↑ SPL↑TLR2R Test Unseen NE↓ SR↑ SPL↑Random9.589.4516-9.779.2316-9.899.791312Human--------11.85 1.618676Seq2Seq [12]11.33 6.0139-8.397.8122-8.137.852018Speaker-Follower [24]-3.3666--6.6235-14.82 6.623528RPA [23]8.465.5643-7.227.6525-9.157.5325-SMNA [129]-3.226758-5.52453218.04 5.674835RCM+SIL [128]10.65 3.5367-11.46 6.0943-11.97 6.124338Regretful [130]-3.236963-5.32504113.69 5.694840PRESS<em> [149]10.57 4.39585510.36 5.28494510.77 5.494945FAST-Short [131]----21.17 4.97564322.08 5.145441EnvDrop [137]11.00 3.99625910.70 5.22524811.66 5.235147AuxRN [138]-3.337067-5.285550-5.155551PREVALENT</em> [151]10.32 3.67696510.19 4.71585310.51 5.305451Active Exploration [196] 19.70 3.20705220.60 4.36584021.64.336041RelGraph [144]10.13 3.4767659.994.73575310.29 4.755552VLN BERT* [152]11.13 2.90726812.01 3.93635712.35 4.096357</p>
<p>TABLE 4 :
4
Comparison of agent performance on R2R in single-run setting.*pretraining-based methods.</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Communications of The ACM. 6062017</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 52175532015</p>
<p>. S Thrun, Probabilistic Robotics. 2005</p>
<p>Where the Action Is: The Foundations of Embodied Interaction. P Dourish, 2001</p>
<p>Human-aware robot navigation: A survey. T Kruse, A K Pandey, R Alami, A Kirsch, Robotics and Autonomous Systems. 61122013</p>
<p>Vision for mobile robot navigation: A survey. G N Desouza, A C Kak, TPAMI. 2422002</p>
<p>Probabilistic algorithms in robotics. S Thrun, Ai Magazine. 2142000</p>
<p>Fast vision-guided mobile robot navigation using model-based reasoning and prediction of uncertainties. A Kosaka, A Kak, IROS. 31992</p>
<p>Position verification of a mobile robot using standard pattern. M Kabuka, A Arenas, ICRA. 361987</p>
<p>Building generalizable agents with a realistic and rich 3d environment. Y Wu, Y Wu, G Gkioxari, Y Tian, ICLR (Workshop). 2018</p>
<p>Minos: Multimodal indoor simulator for navigation in complex environments. M Savva, A X Chang, A Dosovitskiy, T A Funkhouser, V Koltun, arXiv:1712.039312017arXiv preprint</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sunderhauf, I Reid, S Gould, A Van Den, Hengel, CVPR. 2018</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, ICCV. 2019</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, CVPR. 2018</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, ICRA. 2017</p>
<p>Visual representations for semantic target driven navigation. A Mousavian, A Toshev, M Fiser, J Kosecka, A Wahid, J Davidson, ICRA. 2019</p>
<p>Model-based reinforcement learning with parametrized physical models and optimismdriven exploration. C Xie, S Patil, T Moldovan, S Levine, P Abbeel, ICRA. 2016</p>
<p>Reinforcement learning with unsupervised auxiliary tasks. M Jaderberg, V Mnih, W M Czarnecki, T Schaul, J Z Leibo, D Silver, K Kavukcuoglu, ICLR 2017 : ICLR 20172017</p>
<p>Learning to navigate in complex environments. P Mirowski, R Pascanu, F Viola, H Soyer, A Ballard, A Banino, M Denil, R Goroshin, L Sifre, K Kavukcuoglu, D Kumaran, R Hadsell, ICLR. 2017</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, V Tolani, J Davidson, S Levine, R Sukthankar, J Malik, IJCV. 12852020</p>
<p>Learning to move with affordance maps. W Qi, R T Mullapudi, S Gupta, D Ramanan, ICLR2020</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, ICLR2020</p>
<p>Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. X Wang, W Xiong, H Wang, W Yang Wang, ECCV. 2018</p>
<p>Speakerfollower models for vision-and-language navigation. D Fried, R Hu, V Cirik, A Rohrbach, J Andreas, L.-P Morency, T Berg-Kirkpatrick, K Saenko, D Klein, T Darrell, NeurIPS. 2018</p>
<p>The darpa lagr program: Goals, challenges, methodology, and phase i results. L D Jackel, E Krotkov, M Perschbacher, J Pippine, C Sullivan, Journal of Field robotics. 2311-122006</p>
<p>Embodied visual navigation with automatic curriculum learning in real environments. S D Morad, R Mecca, R P K Poudel, S Liwicki, R Cipolla, IEEE Robotics and Automation Letters. 622021</p>
<p>Vision and navigation for the carnegie-mellon navlab. C Thorpe, M H Hebert, T Kanade, S A Shafer, TPAMI. 1031988</p>
<p>Discorl: Continual reinforcement learning via policy distillation. R Traoré, H Caselles-Dupré, T Lesort, T Sun, G Cai, N Díaz-Rodríguez, D Filliat, NeurIPS workshop on Deep Reinforcement Learning. 2019</p>
<p>Transferable representation learning in vision-and-language navigation. H Huang, V Jain, H Mehta, A Ku, G Magalhaes, J Baldridge, E Ie, ICCV. 2019</p>
<p>Multimodal aggregation approach for memory vision-voice indoor navigation with meta-learning. L Yan, D Liu, Y Song, C Yu, IROS. 2020</p>
<p>Example-based synthesis of 3d object arrangements. M Fisher, D Ritchie, M Savva, T Funkhouser, P Hanrahan, international conference on computer graphics and interactive techniques. 201231135</p>
<p>Scenenet: An annotated model generator for indoor scene understanding. A Handa, V Patraucean, S Stent, R Cipolla, ICRA. 2016</p>
<p>Joint 2d-3d-semantic data for indoor scene understanding. I Armeni, S Sax, A R Zamir, S Savarese, arXiv:1702.011052017arXiv preprint</p>
<p>Semantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T Funkhouser, CVPR. 2017</p>
<p>Chalet: Cornell house agent learning environment. C Yan, D K Misra, A Bennett, A Walsman, Y Bisk, Y Artzi, arXiv:1801.073572018arXiv preprint</p>
<p>Matterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niebner, M Savva, S Song, A Zeng, Y Zhang, 20173</p>
<p>. J Straub, T Whelan, L Ma, Y Chen, E Wijmans, S Green, J J Engel, R Mur-Artal, C Ren, S Verma, A Clarkson, M Yan, B Budge, Y Yan, X Pan, J Yon, Y Zou, K Leon, N Carter, J Briales, T Gillingham, E Mueggler, L Pesqueira, M Savva, D Batra, H , </p>
<p>R D Strasdat, M Nardi, S Goesele, R A Lovegrove, Newcombe, arXiv:1906.05797The replica dataset: A digital replica of indoor spaces. 2019arXiv preprint</p>
<p>Towards cognitive exploration through deep reinforcement learning for mobile robots. L Tai, M Liu, arXiv:1610.017332016arXiv preprint</p>
<p>Robothor: An open simulation-to-real embodied ai platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, CVPR. 2020</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, D Gordon, Y Zhu, A Gupta, A Farhadi, arXiv:1712.054742017arXiv preprint</p>
<p>Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments. F Xia, W B Shen, C Li, P Kasimbeg, M E Tchapmi, A Toshev, R Martin-Martin, S Savarese, IEEE Robotics and Automation Letters. 522020</p>
<p>Markov games as a framework for multi-agent reinforcement learning. M L Littman, ICML. 1994</p>
<p>Multi-agent reinforcement learning: independent vs. cooperative agents. M Tan, ICML. 1997</p>
<p>On evaluation of embodied navigation agents. P Anderson, A X Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, A R Zamir, arXiv:1807.067572018arXiv preprint</p>
<p>Multion: Benchmarking semantic map memory using multi-object navigation. S Wani, S Patel, U Jain, A X Chang, M Savva, NeurIPS. 332020</p>
<p>Stay on the path: Instruction fidelity in vision-and-language navigation. V Jain, G Magalhaes, A Ku, A Vaswani, E Ie, J Baldridge, ACL. 2019</p>
<p>Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. A Ku, P Anderson, R Patel, E Ie, J Baldridge, EMNLP. 2020</p>
<p>Sub-instruction aware vision-and-language navigation. Y Hong, C Rodriguez, Q Wu, S Gould, EMNLP. 2020</p>
<p>Vision-anddialog navigation. J Thomason, M Murray, M Cakmak, L Zettlemoyer, CoRL. 2019</p>
<p>Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, ICCV. 2015</p>
<p>Bottom-up and top-down attention for image captioning and visual question answering. P Anderson, X He, C Buehler, D Teney, M Johnson, S Gould, L Zhang, CVPR. 2018</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, M S Bernstein, L Fei-Fei, IJCV. 12312017</p>
<p>Stacked attention networks for image question answering. Z Yang, X He, J Gao, L Deng, A Smola, CVPR. 2016</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, CVPR Workshops (CVPRW). 2018</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Y Qi, Q Wu, P Anderson, X Wang, W Y Wang, C Shen, A Van Den, Hengel, CVPR. 2020</p>
<p>Soundspaces: Audio-visual navigation in 3d environments. C Chen, U Jain, C Schissler, S V A Gari, Z Al-Halah, V K Ithapu, P Robinson, K Grauman, ECCV. 2020</p>
<p>Multitarget embodied question answering. L Yu, X Chen, G Gkioxari, M Bansal, T L Berg, D Batra, CVPR. 2019</p>
<p>Iqa: Visual question answering in interactive environments. D Gordon, A Kembhavi, M Rastegari, J Redmon, D Fox, A Farhadi, CVPR. 2018</p>
<p>Help, anna! vision-based navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. K Nguyen, H Daumé, EMNLP. 2019</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, CVPR. 2017</p>
<p>Mapping instructions and visual observations to actions with reinforcement learning. D K Misra, J Langford, Y Artzi, EMNLP. 2017</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. H Chen, A Suhr, D Misra, N Snavely, Y Artzi, CVPR. 2019547</p>
<p>Babywalk: Going farther in vision-and-language navigation by taking baby steps. W Zhu, H Hu, J Chen, Z Deng, V Jain, E Ie, F Sha, ACL. 2020</p>
<p>Using dynamic time warping to find patterns in time series. D J Berndt, J Clifford, AAAIWS'94 Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining. 1994</p>
<p>Dynamic programming algorithm optimization for spoken word recognition. H Sakoe, S Chiba, IEEE transactions on acoustics, speech, and signal processing. 197826</p>
<p>Trajectory learning for robot programming by demonstration using hidden markov model and dynamic time warping. A Vakanski, I Mantegh, A Irish, F Janabi-Sharifi, IEEE Transactions on Systems, Man, and Cybernetics. 4242012</p>
<p>Scaling up dynamic time warping for datamining applications. E J Keogh, M J Pazzani, KDD. 2000</p>
<p>General evaluation for instruction conditioned navigation using dynamic time warping. G Ilharco, V Jain, A Ku, E Ie, J Baldridge, 2019ViGIL@NeurIPS</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Harley, T P Lillicrap, D Silver, K Kavukcuoglu, ICML. 2016</p>
<p>Value iteration networks. A Tamar, Y Wu, G Thomas, S Levine, P Abbeel, IJCAI. 2017</p>
<p>Gated path planning networks. L Lee, E Parisotto, D S Chaplot, E P Xing, R Salakhutdinov, ICML. 2018</p>
<p>Visual sonar: fast obstacle avoidance using monocular vision. S Lenser, M Veloso, IROS. 20031</p>
<p>Outdoor autonomous navigation using monocular vision. E Royer, J Bom, M Dhome, B Thuilot, M Lhuillier, F Marmoiton, IROS. 2005</p>
<p>Reactive navigation in outdoor environments using potential fields. H Haddad, M Khatib, S Lacroix, R Chatila, IROS. 21998</p>
<p>Robot motion control from a visual memory. A Remazeilles, F Chaumette, P Gros, ICRA. 52004</p>
<p>Visual semantic planning using deep successor representations. Y Zhu, D Gordon, E Kolve, D Fox, L Fei-Fei, A Gupta, R Mottaghi, A Farhadi, ICCV. 2017</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, ICLR2016</p>
<p>Learning view and target invariant visual servoing for navigation. Y Li, J Kosecka, ICRA. 2020</p>
<p>. T.-Y Lin, M Maire, S J Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, 2014Microsoft coco: Common objects in context," in ECCV</p>
<p>Joint semantic segmentation and depth estimation with deep convolutional networks. A Mousavian, H Pirsiavash, J Kosecka, 20163</p>
<p>Situational fusion of visual representation for visual navigation. W Shen, D Xu, Y Zhu, L Fei-Fei, L Guibas, S Savarese, ICCV. 2019</p>
<p>Improving targetdriven visual navigation with attention on 3d spatial relationships. Y Lv, N Xie, Y Shi, Z Wang, H T Shen, arXiv:2005.021532020arXiv preprint</p>
<p>Reinforcement learning-based visual navigation with information-theoretic regularization. Q Wu, K Xu, J Wang, M Xu, X Gong, D Manocha, IEEE Robotics and Automation Letters. 622021</p>
<p>Looking beyond the visible scene. A Khosla, B An, J J Lim, A Torralba, CVPR. 2014</p>
<p>Deepnav: Learning to navigate large cities. S Brahmbhatt, J Hays, CVPR. 2017</p>
<p>Learning to navigate in cities without a map. P Mirowski, M K Grimes, M Malinowski, K M Hermann, K Anderson, D Teplyashin, K Simonyan, K Kavukcuoglu, A Zisserman, R Hadsell, NeurIPS. 312018</p>
<p>Learning to set waypoints for audio-visual navigation. C Chen, S Majumder, Z Al-Halah, R Gao, S K Ramakrishnan, K Grauman, ICLR2021</p>
<p>Unsupervised visual representation learning by context prediction. C Doersch, A Gupta, A A Efros, ICCV. 2015</p>
<p>Unsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, ECCV. 2016</p>
<p>Colorful image colorization. R Zhang, P Isola, A A Efros, ECCV. 2016</p>
<p>Test-time training with self-supervision for generalization under distribution shifts. Y Sun, X Wang, Z Liu, J Miller, A A Efros, M Hardt, ICML. 2020</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, ICML. 2015</p>
<p>Learning to act by predicting the future. A Dosovitskiy, V Koltun, ICLR. 2016</p>
<p>Vizdoom: A doom-based ai research platform for visual reinforcement learning. M Kempka, M Wydmuch, G Runc, J Toczek, W Jaśkowski, 2016CIG. IEEE</p>
<p>Auxiliary tasks speed up learning pointgoal navigation. J Ye, D Batra, E Wijmans, A Das, arXiv:2007.045612020arXiv preprint</p>
<p>Auxiliary tasks and exploration enable objectnav. J Ye, D Batra, A Das, E Wijmans, arXiv:2104.041122021arXiv preprint</p>
<p>Gaple: Generalizable approaching policy learning for robotic object searching in indoor environment. X Ye, Z Lin, J.-Y Lee, J Zhang, S Zheng, Y Yang, IEEE Robotics and Automation Letters. 442019</p>
<p>Learning end-to-end multimodal sensor policies for autonomous navigation. G.-H Liu, A Siravuru, S P Selvaraj, M M Veloso, G Kantor, CoRL. 2017</p>
<p>Integrating egocentric localization for more realistic point-goal navigation agents. S Datta, O Maksymets, J Hoffman, S Lee, D Batra, D Parikh, arXiv:2009.032312020arXiv preprint</p>
<p>Explore and explain: Self-supervised navigation and recounting. R Bigazzi, F Landi, M Cornia, S Cascianelli, L Baraldi, R Cucchiara, ICPR. 2021</p>
<p>See, hear, explore: Curiosity via audio-visual association. V Dean, S Tulsiani, A Gupta, NeurIPS. 332020</p>
<p>Simultaneous localization and mapping: part i. H Durrant-Whyte, T Bailey, IEEE robotics &amp; automation magazine. 1322006</p>
<p>Comparison of various slam systems for mobile robot in an indoor environment. M Filipenko, I Afanasyev, in IS. 2018</p>
<p>Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks. S Se, D G Lowe, J J Little, The International Journal of Robotics Research. 2182002</p>
<p>Rover navigation using stereo ego-motion. C F Olson, L H Matthies, M Schoppers, M W Maimone, Robotics and Autonomous Systems. 4342003</p>
<p>Real-time simultaneous localisation and mapping with a single camera. Davison, ICCV. 22003</p>
<p>Direct sparse odometry. J Engel, V Koltun, D Cremers, TPAMI. 4032017</p>
<p>Lsd-slam: Large-scale direct monocular slam. J Engel, T Schöps, D Cremers, ECCV. Springer2014</p>
<p>Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. R Mur-Artal, J D Tardós, IEEE Transactions on Robotics. 3352017</p>
<p>Cnn-slam: Real-time dense monocular slam with learned depth prediction. K Tateno, F Tombari, I Laina, N Navab, CVPR. 2017</p>
<p>Learning depth from monocular videos using direct methods. C Wang, J Miguel, R Buenaposada, S Zhu, Lucey, CVPR. 2018</p>
<p>D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. N Yang, L V Stumberg, R Wang, D Cremers, CVPR. 2020</p>
<p>Are we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, CVPR. IEEE2012</p>
<p>The euroc micro aerial vehicle datasets. M Burri, J Nikolic, P Gohl, T Schneider, J Rehder, S Omari, M W Achtelik, R Siegwart, The International Journal of Robotics Research. 35102016</p>
<p>Neural map: Structured memory for deep reinforcement learning. E Parisotto, R Salakhutdinov, ICLR. 2017</p>
<p>Neural slam: Learning to explore with external memory. J Zhang, L Tai, J Boedecker, W Burgard, M Liu, arXiv:1706.095202017arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, NeurIPS. 302017</p>
<p>Mapnet: An allocentric spatial memory for mapping environments. J F Henriques, A Vedaldi, CVPR. 2018</p>
<p>Near-optimal regret bounds for reinforcement learning. T Jaksch, R Ortner, P Auer, JMLR. 11512010</p>
<p>Is q-learning provably efficient. C Jin, Z Allen-Zhu, S Bubeck, M I Jordan, NeurIPS. 312018</p>
<p>Minimax regret bounds for reinforcement learning. M G Azar, I Osband, R Munos, ICML. 2017</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, ICLR. 2019</p>
<p>Recent advances in hierarchical reinforcement learning. A G Barto, S Mahadevan, Discrete Event Dynamic Systems. 1312003</p>
<p>Feudal reinforcement learning. P Dayan, G E Hinton, NeurIPS. 51992</p>
<p>Neural topological slam for visual navigation. D S Chaplot, R Salakhutdinov, A Gupta, S Gupta, CVPR. 202012884</p>
<p>Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D P Gandhi, A Gupta, R R Salakhutdinov, NeurIPS. 332020</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, NeurIPS. 272014</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. X Wang, Q Huang, A Celikyilmaz, J Gao, D Shen, Y.-F Wang, W Y Wang, L Zhang, CVPR. 2018</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. C.-Y Ma, J Lu, Z Wu, G Alregib, Z Kira, R Socher, C Xiong, ICLR2019</p>
<p>The regretful agent: Heuristic-aided navigation through progress estimation. C.-Y Ma, Z Wu, G Alregib, C Xiong, Z Kira, CVPR. 2019</p>
<p>Tactical rewind: Self-correction via backtracking in vision-and-language navigation. L Ke, X Li, Y Bisk, A Holtzman, Z Gan, J Liu, J Gao, Y Choi, S Srinivasa, CVPR. 2019</p>
<p>Chasing ghosts: Instruction following as bayesian state tracking. P Anderson, A Shrivastava, D Parikh, D Batra, S Lee, NeurIPS. 2019</p>
<p>End-to-end learnable histogram filters. R Jonschkowski, O Brock, 2016</p>
<p>Dropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The JMLR. 1512014</p>
<p>Very deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR2015</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. 2016</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. H Tan, L Yu, M Bansal, NAACL. 2019</p>
<p>Visiondialog navigation by exploring cross-modal memory. Y Zhu, F Zhu, Z Zhan, B Lin, J Jiao, X Chang, X Liang, CVPR. 202010739</p>
<p>Counterfactual vision-and-language navigation via adversarial path sampling. T.-J Fu, X E Wang, M F Peterson, S T Grafton, M P Eckstein, W Y Wang, ECCV. 2019</p>
<p>Shifting the baseline: Single modality performance on visual navigation &amp; qa. J Thomason, D Gordon, Y Bisk, NAACL. 2019</p>
<p>Multi-modal discriminative model for vision-and-language navigation. H Huang, V Jain, H Mehta, J Baldridge, E Ie, RoboNLP. 2019</p>
<p>Soft expert reward learning for visionand-language navigation. H Wang, Q Wu, C Shen, ECCV. 2020</p>
<p>Multi-view learning for vision-and-language navigation. Q Xia, X Li, C Li, Y Bisk, Z Sui, J Gao, Y Choi, N A Smith, arXiv:2003.008572020arXiv preprint</p>
<p>Language and visual entity relationship graph for agent navigation. Y Hong, C Rodriguez, Y Qi, Q Wu, S Gould, NeurIPS. 332020</p>
<p>Are you looking? grounding to multiple modalities in vision-and-language navigation. R Hu, D Fried, A Rohrbach, D Klein, T Darrell, K Saenko, ACL. 2019</p>
<p>Robust scene text recognition with automatic rectification. B Shi, X Wang, P Lyu, C Yao, X Bai, CVPR. 2016</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K N Toutanova, NAACL. 2018</p>
<p>Lxmert: Learning cross-modality encoder representations from transformers. H Tan, M Bansal, EMNLP. 2019</p>
<p>Robust navigation with language pretraining and stochastic sampling. X Li, C Li, Q Xia, Y Bisk, A Celikyilmaz, J Gao, N A Smith, Y Choi, EMNLP. 2019</p>
<p>Improving vision-and-language navigation with image-text pairs from the web. A Majumdar, A Shrivastava, S Lee, P Anderson, D Parikh, D Batra, ECCV. 2020</p>
<p>Towards learning a generic agent for vision-and-language navigation via pre-training. W Hao, C Li, X Li, L Carin, J Gao, CVPR. 2020</p>
<p>Vln bert: A recurrent vision-and-language bert for navigation. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, CVPR. 2021</p>
<p>Adaptive computation time for recurrent neural networks. A Graves, arXiv:1603.089832016arXiv preprint</p>
<p>Neural Modular Control for Embodied Question Answering. A Das, G Gkioxari, S Lee, D Parikh, D Batra, CoRL2018</p>
<p>Blindfold baselines for embodied qa. A Anand, E Belilovsky, K Kastner, H Larochelle, A C Courville, arXiv: Computer Vision and Pattern Recognition. 2018</p>
<p>Revisiting embodiedqa: A simple baseline and beyond. Y Wu, L Jiang, Y Yang, IEEE Transactions on Image Processing. 292020</p>
<p>Embodied Question Answering in Photorealistic Environments with Point Cloud Perception. E Wijmans, S Datta, O Maksymets, A Das, G Gkioxari, S Lee, I Essa, D Parikh, D Batra, CVPR. 2019</p>
<p>Segeqa: Video segmentation based visual attention for embodied question answering. H Luo, G Lin, Z Liu, F Liu, Z Tang, Y Yao, ICCV. 2019</p>
<p>Flownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, CVPR. 2017</p>
<p>Walking with mind: Mental imagery enhanced embodied qa. J Li, S Tang, F Wu, Y Zhuang, ACM Multimedia. 2019</p>
<p>Multi-agent embodied question answering in interactive environments. S Tan, W Xiang, H Liu, D Guo, F Sun, ECCV. 2020</p>
<p>Video question answering via attribute-augmented attention network learning. Y Ye, Z Zhao, Y Li, L Chen, J Xiao, Y Zhuang, 2017ACM</p>
<p>Videonavqa: Bridging the gap between visual and embodied question answering. C Cangea, E Belilovsky, P Liò, A C Courville, ViGIL@NeurIPS. 2019280</p>
<p>Mqa: Answering the question via robotic manipulation. Y Deng, D Guo, X Guo, N Zhang, H Liu, F Sun, RSS. 172021</p>
<p>Embodied visual active learning for semantic segmentation. D Nilsson, A Pirinen, E Gärtner, C Sminchisescu, AAAI. 2020</p>
<p>Rmm: A recursive mental model for dialog navigation. H R Roman, Y Bisk, J Thomason, A Celikyilmaz, J Gao, EMNLP. 2020</p>
<p>Improving grounded natural language understanding through human-robot dialog. J Thomason, A Padmakumar, J Sinapov, N Walker, Y Jiang, H Yedidsion, J Hart, P Stone, R J Mooney, ICRA. IEEE2019</p>
<p>Asking for help using inverse semantics. S Tellex, R A Knepper, A Li, D Rus, N Roy, Robotics: Science and Systems. 2014. 201410</p>
<p>Vision-based navigation with language-based assistance via imitation learning with indirect intervention. K Nguyen, D Dey, C Brockett, B Dolan, CVPR. 2019</p>
<p>H Vries, K Shuster, D Batra, D Parikh, J Weston, D Kiela, Talk the walk: Navigating new york city through grounded dialogue. 2018</p>
<p>Where are you? localization from embodied dialog. M Hahn, J Krantz, D Batra, D Parikh, J M Rehg, S Lee, P Anderson, EMNLP. 2020</p>
<p>Two body problem: Collaborative visual task completion. U Jain, L Weihs, E Kolve, M Rastegari, S Lazebnik, A Farhadi, A G Schwing, A Kembhavi, CVPR. 2019</p>
<p>Learning when to communicate at scale in multiagent cooperative and competitive tasks. A Singh, T Jain, S Sukhbaatar, ICLR2018</p>
<p>A research platform for multi-robot dialogue with humans. M Marge, S Nogar, C J Hayes, S M Lukin, J Bloecker, E Holder, C R Voss, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American Chapterthe Association for Computational Linguistics2019</p>
<p>The robotslang benchmark: Dialog-guided robot localization and navigation. S Banerjee, J Thomason, J J Corso, arXiv: Robotics2020</p>
<p>Off-road obstacle avoidance through end-to-end learning. U Muller, J Ben, E Cosatto, B Flepp, Y L Cun, NeurIPS. Citeseer. 2006</p>
<p>Learning long-range vision for autonomous off-road driving. R Hadsell, P Sermanet, J Ben, A Erkan, M Scoffier, K Kavukcuoglu, U Muller, Y Lecun, Journal of Field Robotics. 2622009</p>
<p>W Zhang, K Liu, W Zhang, Y Zhang, J Gu, Deep neural networks for wireless localization in indoor and outdoor environments. 2016194</p>
<p>Deep reinforcement learning with successor features for navigation across similar environments. J Zhang, J T Springenberg, J Boedecker, W Burgard, IROS. 2017</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 51875402015</p>
<p>Composable actionconditioned predictors: Flexible off-policy learning for robot navigation. G Kahn, A Villaflor, P Abbeel, S Levine, CoRL. PMLR2018</p>
<p>Learning monocular reactive uav control in cluttered natural environments. S Ross, N Melik-Barkhudarov, K S Shankar, A Wendel, D Dey, J A Bagnell, M Hebert, ICRA. IEEE2013</p>
<p>Vision-based goal-conditioned policies for underwater navigation in the presence of obstacles. T Manderson, J C G Higuera, S Wapnick, J.-F Tremblay, F Shkurti, D Meger, G Dudek, RSS. 162020</p>
<p>Real-time obstacle avoidance for fast mobile robots in cluttered environments. J Borenstein, Y Koren, ICRA. IEEE1990</p>
<p>World models. D Ha, J Schmidhuber, arXiv:1803.101222018arXiv preprint</p>
<p>Long-range indoor navigation with prm-rl. A Francis, A Faust, H.-T L Chiang, J Hsu, J C Kew, M Fiser, T.-W E Lee, IEEE Transactions on Robotics. 3642020</p>
<p>Ving: Learning open-world navigation with visual goals. D Shah, B Eysenbach, G Kahn, N Rhinehart, S Levine, arXiv:2012.098122020arXiv preprint</p>
<p>Probabilistic robotics. S Thrun, Communications of the ACM. 4532002</p>
<p>Planning algorithms. S M Lavalle, 2006Cambridge university press</p>
<p>Mobile robot localisation using active vision. A J Davison, D W Murray, ECCV. Springer1998</p>
<p>Autonomous vision-based exploration and mapping using hybrid maps and rao-blackwellised particle filters. R Sim, J J Little, IROS. IEEE2006</p>
<p>Smirl: Surprise minimizing reinforcement learning in unstable environments. G Berseth, D Geng, C M Devin, N Rhinehart, C Finn, D Jayaraman, S Levine, ICLR2021</p>
<p>Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. A Nagabandi, I Clavera, S Liu, R S Fearing, P Abbeel, S Levine, C Finn, ICLR. 2018</p>
<p>Context-aware dynamics model for generalization in model-based reinforcement learning. K Lee, Y Seo, S Lee, H Lee, J Shin, ICML. 20201</p>
<p>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, 2020in Eighth ICLR</p>
<p>Active visual information gathering for vision-language navigation. H Wang, W Wang, T Shu, W Liang, J Shen, ECCV. 2020</p>
<p>Deep visual foresight for planning robot motion. C Finn, S Levine, ICRA. IEEE2017</p>
<p>Dd t07 -designing a situational judgment test to evaluate lll-structured problem solving in a virtual learning environment. N Malone, V Kypraios, 2012</p>
<p>Occupancy anticipation for efficient exploration and navigation. S K Ramakrishnan, Z Al-Halah, K Grauman, ECCV. 2020</p>
<p>Visual navigation for biped humanoid robots using deep reinforcement learning. K Lobos-Tsunekawa, F Leiva, J Ruiz-Del-Solar, IEEE Robotics and Automation Letters. 342018</p>
<p>Oneshot reinforcement learning for robot navigation with interactive replay. J Bruce, N Sünderhauf, P Mirowski, R Hadsell, M Milford, arXiv:1711.101372017arXiv preprint</p>
<p>Reinforced imitation: Sample efficient deep reinforcement learning for mapless navigation by leveraging prior demonstrations. M Pfeiffer, S Shukla, M Turchetta, C Cadena, A Krause, R Siegwart, J I Nieto, IEEE Robotics and Automation Letters. 342018</p>
<p>Policy distillation. A A Rusu, S G Colmenarejo, C Gulcehre, G Desjardins, J Kirkpatrick, R Pascanu, V Mnih, K Kavukcuoglu, R Hadsell, ICLR2016</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, ICML. 2017</p>
<p>Map-less navigation: a single drlbased controller for robots with varied dimensions. W Zhang, Y Zhang, N Liu, arXiv:2002.063202020arXiv preprint</p>
<p>Unsupervised reinforcement learning of transferable meta-skills for embodied navigation. J Li, X Wang, S Tang, H Shi, F Wu, Y Zhuang, W Y Wang, CVPR. 202012132</p>
<p>Embodied multimodal multitask learning. D S Chaplot, L Lee, R Salakhutdinov, D Parikh, D Batra, IJCAI. 32020</p>
<p>Environment-agnostic multitask learning for natural language grounded navigation. X E Wang, V Jain, E Ie, W Y Wang, Z Kozareva, S Ravi, ECCV. 2020</p>
<p>Lifelong federated reinforcement learning: A learning architecture for navigation in cloud robotic systems. B Liu, L Wang, M Liu, IEEE Robotics and Automation Letters. 442019</p>
<p>Splitnet: Sim2sim and task2task transfer for embodied visual navigation. D Gordon, A Kadian, D Parikh, J Hoffman, D Batra, ICCV. 2019</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IROS. 2017IEEE</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Večerík, T Rothörl, N Heess, R Pascanu, R Hadsell, 2017</p>
<p>Cad2rl: Real single-image flight without a single real image. F Sadeghi, S Levine, RSS. 132017</p>
<p>End-toend nonprehensile rearrangement with deep reinforcement learning and simulation-to-reality transfer. W Yuan, K Hang, D Kragic, M Y Wang, J A Stork, Robotics and Autonomous Systems. 1192019</p>
<p>Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation. L Tai, G Paolo, M Liu, IROS. IEEE2017</p>
<p>Adversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, CVPR. 2017</p>
<p>Sim-real joint reinforcement transfer for 3d indoor navigation. F Zhu, L Zhu, Y Yang, CVPR. 201911397</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, SSCI. 2020</p>
<p>Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning. B Qin, Y Gao, Y Bai, ICRAE2019</p>
<p>Learning to play soccer by reinforcement and applying sim-to-real to compete in the real world. H F Bassani, R A Delgado, J N De, O Lima Junior, H R Medeiros, P H M Braga, A Tapp, 2020CoRR</p>
<p>igibson, a simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Martín-Martín, L Fan, G Wang, S Buch, C D'arpino, S Srivastava, L P Tchapmi, M E Tchapmi, K Vainio, L Fei-Fei, S Savarese, arXiv:2012.029242020arXiv preprint</p>
<p>Selfmotivated communication agent for real-world vision-dialog navigation. Y Zhu, Y Weng, F Zhu, X Liang, Q Ye, Y Lu, J Jiao, ICCV. 2021</p>
<p>Hierarchical reinforcement learning for robot navigation using the intelligent space concept. L Jeni, Z Istenes, P Korondi, H Hashimoto, 2007 11th International Conference on Intelligent Engineering Systems. IEEE2007</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J G Carbonell, R Salakhutdinov, Q V Le, NeurIPS. 322019</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinformatics. 3642019</p>
<p>Learning neural network policies with guided policy search under unknown dynamics. S Levine, P Abbeel, NeurIPS. 272014</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4486 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4486</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4486</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-270213027</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.01391v2.pdf" target="_blank">Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Identifying and predicting the factors that contribute to the success of interdisciplinary research is crucial for advancing scientific discovery. However, there is a lack of methods to quantify the integration of new ideas and technological advancements in astronomical research and how these new technologies drive further scientific breakthroughs. Large language models, with their ability to extract key concepts from vast literature beyond keyword searches, provide a new tool to quantify such processes. In this study, we extracted concepts in astronomical research from 297,807 publications between 1993 and 2024 using large language models, resulting in a set of 24,939 concepts. These concepts were then used to form a knowledge graph, where the link strength between any two concepts was determined by their relevance through the citation-reference relationships. By calculating this relevance across different time periods, we quantified the impact of numerical simulations and machine learning on astronomical research. The knowledge graph demonstrates two phases of development: a phase where the technology was integrated and another where the technology was explored in scientific discovery. The knowledge graph reveals that despite machine learning has made much inroad in astronomy, there is currently a lack of new concept development at the intersection of AI and Astronomy, which may be the current bottleneck preventing machine learning from further transforming the field of astronomy.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4486.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4486.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AstroKG-LLM-pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Knowledge Graph Construction Pipeline for Astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM pipeline that distills scientific concepts from titles/abstracts, validates them with a clarifying agent using retrieval-augmented generation, vectorizes and clusters concepts with embeddings, merges semantically similar concepts, and constructs a knowledge graph whose link strengths are quantified via a citation-reference transition probability metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based Knowledge Graph Pipeline (concept extraction + citation-reference relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three-agent pipeline: (1) an extraction agent reads titles and abstracts (OCRed with Nougat) and proposes candidate scientific concepts; (2) a clarifying/validation agent uses retrieval-augmented generation (RAG) with the full document as context to explain/validate and filter out hallucinations; (3) vectorization and nearest-neighbor clustering followed by a merging agent that coarsens clusters into unified concepts. Concepts are embedded using JINA-EMBEDDINGS-V2-BASE-EN; semantic grouping uses cosine-similarity clustering with a threshold of 0.85; the neighbor-finding + merging loop is iterated three times to reduce ~1.06M initial concepts to 24,797 final concepts. Concept-concept link strength is computed from a citation-reference transition probability metric computed over the corpus citation network (see equations in law_examples). Visualization uses force-directed layout with link strengths set to these relevance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MISTRAL-7B-INSTRUCT-V0.2 (inference); GPT-4 (categorization); JINA-EMBEDDINGS-V2-BASE-EN (embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Mistral); GPT-4 size not specified; embedding model size not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy and Astrophysics (arXiv astro-ph corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>297,807</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Quantified conceptual relationships / empirical relational patterns: transition-probability-based relevance between concepts derived from citation-reference networks (probabilistic proximity/correlation metric between concepts over the literature), plus temporal co-evolution patterns of concept adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>The paper defines transition probabilities and a symmetric relevance: (1) p_{α→β|n_k} = N_β / |S(n_k, L, β)| where S(n_k, L, β) is the citation-expansion set starting at paper n_k until a paper containing concept c_β is found and N_β is the count of c_β within that set; (2) p_{α→β} = (1/|S_α|) Σ_{n_k∈S_α} p_{α→β|n_k} averaging over papers containing c_α; (3) p_{α,β} = (p_{α→β} * p_{β→α})^{1/2} as the bidirectional citation-reference relevance used as link strength in the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text-mining with LLMs: (a) OCR (Nougat) of PDFs, (b) LLM-based extraction from titles/abstracts to propose candidate concepts, (c) clarifying agent with RAG over full text to filter/validate concepts, (d) text embedding (JINA embeddings) for vectorization, (e) cosine-similarity nearest-neighbor clustering and iterative merging to control granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Human domain-expert evaluation of final granularity (qualitative); internal validation via reproducibility/consistency with citation-reference relationships (the relevance metric itself uses citation network statistics). No formal held-out labeled dataset metrics reported; clarifying agent uses RAG to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Reported challenges include LLM hallucinations producing spurious concepts, variable granularity (too fine or too coarse), synonym redundancy, reliance on titles/abstracts (some information lost), crude merging via embedding cosine thresholds, dependency on agent alignment and prompt design, absence of quantitative precision/recall metrics for concept extraction, and potential OCR errors despite Nougat performing well.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Authors contrast their approach qualitatively with classical keyword/field classification (insufficient granularity) and with vectorized semantic representations (hard to parse at human level), but do not provide quantitative benchmarking or side-by-side performance comparisons with human annotation or other automatic pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4486.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4486.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MISTRAL-7B-INSTRUCT-V0.2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned large language model used here as the primary inference model for extracting candidate scientific concepts from paper titles and abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MISTRAL-7B-INSTRUCT-V0.2 (inference agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the inference LLM in the extraction agent to parse titles and abstracts and output candidate scientific concepts. Its outputs are subsequently passed to a clarifying LLM agent and then embedded/clustered for merging. The pipeline uses prompting to extract lists of concepts and to produce explanations; hallucination filtering is performed by a secondary agent with RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MISTRAL-7B-INSTRUCT-V0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy and Astrophysics (concept extraction from arXiv astro-ph papers)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>297,807 (model applied across the full corpus for extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not directly used to extract physical/quantitative laws; used to distill conceptual entities and labels that are then used to quantify inter-concept relationships via citation statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No explicit physical equations extracted by Mistral are reported; its role is to output candidate concept strings that feed into the relevance computations (see AstroKG-LLM-pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted LLM extraction from titles/abstracts; outputs validated via a separate clarifying agent and RAG against the full document; candidate concepts embedded and clustered.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Outputs were filtered by a clarifying agent (RAG) and final granularity evaluated by domain experts; no numeric evaluation metrics reported for Mistral outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Hallucinations (irrelevant concepts), occasional ambiguous or synonymous outputs requiring clustering and merging, dependence on prompt quality and the secondary validation agent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not quantitatively compared to other LLMs or to manual extraction in the paper; the authors opted for open-source LLMs for cost-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4486.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4486.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JINA-Embeddings-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JINA-EMBEDDINGS-V2-BASE-EN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text embedding model used to vectorize extracted concept strings for nearest-neighbor semantic clustering and cosine-similarity based merging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>JINA-EMBEDDINGS-V2-BASE-EN (embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Transforms validated concept strings into vector representations for semantic similarity computations. Cosine similarity between these embeddings is used to cluster redundant or synonymous concepts; a threshold of 0.85 was applied to determine cluster membership during iterative merging.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JINA-EMBEDDINGS-V2-BASE-EN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy and Astrophysics (concept vectorization)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>297,807 (used on the entire extracted concept set derived from this corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not a law-extraction model per se; enables grouping of semantically similar textual concepts which are used to build a concept-level relational graph (patterns of concept co-occurrence and citation-based relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>N/A (embedding model used to cluster textual concepts; the quantitative relevance laws come from citation-transition probabilities computed after concept extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Vectorization of concept labels and cosine-similarity nearest-neighbor search; iterative clustering and merging based on cosine threshold (0.85).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Cluster size and granularity validated via domain expert inspection; no classification metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Choice of cosine threshold and embedding quality determine cluster granularity; potential semantic conflation or separation depending on embedding behavior; no quantitative evaluation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared quantitatively to other embedding models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4486.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4486.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (categorization)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4) used for taxonomy categorization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used to classify extracted concepts into higher-level domain categories (e.g., astrophysics subdomains and technical domains such as ML, simulation, instrumentation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (categorization/labeling agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applied to map finalized concepts to arXiv-style domain categories (Astrophysics of Galaxies, Cosmology, Solar & Stellar, etc.) and to three technological domains (Statistics & ML, Numerical Simulation, Instrumental Design). The classification step organizes nodes for downstream analysis of cross-domain linkages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy and Astrophysics (concept categorization)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>297,807 (concepts derived from this corpus were categorized)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not directly extracting quantitative laws; enables grouping that allows computation of average cross-domain linkages (aggregated relevance statistics between technical and scientific domain concept sets).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Enables computation of Average Linkage = (1/(|A|*|B|)) Σ_{α∈A,β∈B} p_{α,β} where sets A/B are domain-labeled concepts (machine-learning vs scientific domains).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompt-based classification of concepts into predefined taxonomic buckets using GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not formally quantified; classification used to produce aggregated analyses (e.g., Figures 2-4) and descriptions; no per-class accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Potential classification errors from GPT-4 not quantified; domain boundary ambiguity; reliance on GPT-4 outputs for aggregated statistics could propagate mislabeling into domain-level linkage metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No quantitative baseline comparison reported against manual labeling or other classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards Specialized Foundation Models in Astronomy <em>(Rating: 2)</em></li>
                <li>LLMs for Knowledge Graph Construction and Reasoning. Recent Capabilities and Future Opportunities <em>(Rating: 2)</em></li>
                <li>Sparks of Artificial General Intelligence: Early experiments with GPT-4 <em>(Rating: 1)</em></li>
                <li>Machine Learning in Astronomy: a practical overview <em>(Rating: 1)</em></li>
                <li>The frontier of simulation-based inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4486",
    "paper_id": "paper-270213027",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "AstroKG-LLM-pipeline",
            "name_full": "LLM-based Knowledge Graph Construction Pipeline for Astronomy",
            "brief_description": "A multi-agent LLM pipeline that distills scientific concepts from titles/abstracts, validates them with a clarifying agent using retrieval-augmented generation, vectorizes and clusters concepts with embeddings, merges semantically similar concepts, and constructs a knowledge graph whose link strengths are quantified via a citation-reference transition probability metric.",
            "citation_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
            "mention_or_use": "use",
            "system_name": "LLM-based Knowledge Graph Pipeline (concept extraction + citation-reference relevance)",
            "system_description": "Three-agent pipeline: (1) an extraction agent reads titles and abstracts (OCRed with Nougat) and proposes candidate scientific concepts; (2) a clarifying/validation agent uses retrieval-augmented generation (RAG) with the full document as context to explain/validate and filter out hallucinations; (3) vectorization and nearest-neighbor clustering followed by a merging agent that coarsens clusters into unified concepts. Concepts are embedded using JINA-EMBEDDINGS-V2-BASE-EN; semantic grouping uses cosine-similarity clustering with a threshold of 0.85; the neighbor-finding + merging loop is iterated three times to reduce ~1.06M initial concepts to 24,797 final concepts. Concept-concept link strength is computed from a citation-reference transition probability metric computed over the corpus citation network (see equations in law_examples). Visualization uses force-directed layout with link strengths set to these relevance scores.",
            "model_name": "MISTRAL-7B-INSTRUCT-V0.2 (inference); GPT-4 (categorization); JINA-EMBEDDINGS-V2-BASE-EN (embeddings)",
            "model_size": "7B (Mistral); GPT-4 size not specified; embedding model size not specified",
            "scientific_domain": "Astronomy and Astrophysics (arXiv astro-ph corpus)",
            "number_of_papers": "297,807",
            "law_type": "Quantified conceptual relationships / empirical relational patterns: transition-probability-based relevance between concepts derived from citation-reference networks (probabilistic proximity/correlation metric between concepts over the literature), plus temporal co-evolution patterns of concept adoption.",
            "law_examples": "The paper defines transition probabilities and a symmetric relevance: (1) p_{α→β|n_k} = N_β / |S(n_k, L, β)| where S(n_k, L, β) is the citation-expansion set starting at paper n_k until a paper containing concept c_β is found and N_β is the count of c_β within that set; (2) p_{α→β} = (1/|S_α|) Σ_{n_k∈S_α} p_{α→β|n_k} averaging over papers containing c_α; (3) p_{α,β} = (p_{α→β} * p_{β→α})^{1/2} as the bidirectional citation-reference relevance used as link strength in the knowledge graph.",
            "extraction_method": "Text-mining with LLMs: (a) OCR (Nougat) of PDFs, (b) LLM-based extraction from titles/abstracts to propose candidate concepts, (c) clarifying agent with RAG over full text to filter/validate concepts, (d) text embedding (JINA embeddings) for vectorization, (e) cosine-similarity nearest-neighbor clustering and iterative merging to control granularity.",
            "validation_approach": "Human domain-expert evaluation of final granularity (qualitative); internal validation via reproducibility/consistency with citation-reference relationships (the relevance metric itself uses citation network statistics). No formal held-out labeled dataset metrics reported; clarifying agent uses RAG to reduce hallucinations.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Reported challenges include LLM hallucinations producing spurious concepts, variable granularity (too fine or too coarse), synonym redundancy, reliance on titles/abstracts (some information lost), crude merging via embedding cosine thresholds, dependency on agent alignment and prompt design, absence of quantitative precision/recall metrics for concept extraction, and potential OCR errors despite Nougat performing well.",
            "comparison_baseline": "Authors contrast their approach qualitatively with classical keyword/field classification (insufficient granularity) and with vectorized semantic representations (hard to parse at human level), but do not provide quantitative benchmarking or side-by-side performance comparisons with human annotation or other automatic pipelines.",
            "uuid": "e4486.0",
            "source_info": {
                "paper_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mistral-7B-Instruct",
            "name_full": "MISTRAL-7B-INSTRUCT-V0.2",
            "brief_description": "An open-source instruction-tuned large language model used here as the primary inference model for extracting candidate scientific concepts from paper titles and abstracts.",
            "citation_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
            "mention_or_use": "use",
            "system_name": "MISTRAL-7B-INSTRUCT-V0.2 (inference agent)",
            "system_description": "Used as the inference LLM in the extraction agent to parse titles and abstracts and output candidate scientific concepts. Its outputs are subsequently passed to a clarifying LLM agent and then embedded/clustered for merging. The pipeline uses prompting to extract lists of concepts and to produce explanations; hallucination filtering is performed by a secondary agent with RAG.",
            "model_name": "MISTRAL-7B-INSTRUCT-V0.2",
            "model_size": "7B",
            "scientific_domain": "Astronomy and Astrophysics (concept extraction from arXiv astro-ph papers)",
            "number_of_papers": "297,807 (model applied across the full corpus for extraction)",
            "law_type": "Not directly used to extract physical/quantitative laws; used to distill conceptual entities and labels that are then used to quantify inter-concept relationships via citation statistics.",
            "law_examples": "No explicit physical equations extracted by Mistral are reported; its role is to output candidate concept strings that feed into the relevance computations (see AstroKG-LLM-pipeline).",
            "extraction_method": "Prompted LLM extraction from titles/abstracts; outputs validated via a separate clarifying agent and RAG against the full document; candidate concepts embedded and clustered.",
            "validation_approach": "Outputs were filtered by a clarifying agent (RAG) and final granularity evaluated by domain experts; no numeric evaluation metrics reported for Mistral outputs.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Hallucinations (irrelevant concepts), occasional ambiguous or synonymous outputs requiring clustering and merging, dependence on prompt quality and the secondary validation agent.",
            "comparison_baseline": "Not quantitatively compared to other LLMs or to manual extraction in the paper; the authors opted for open-source LLMs for cost-efficiency.",
            "uuid": "e4486.1",
            "source_info": {
                "paper_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "JINA-Embeddings-v2",
            "name_full": "JINA-EMBEDDINGS-V2-BASE-EN",
            "brief_description": "A text embedding model used to vectorize extracted concept strings for nearest-neighbor semantic clustering and cosine-similarity based merging.",
            "citation_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
            "mention_or_use": "use",
            "system_name": "JINA-EMBEDDINGS-V2-BASE-EN (embedding model)",
            "system_description": "Transforms validated concept strings into vector representations for semantic similarity computations. Cosine similarity between these embeddings is used to cluster redundant or synonymous concepts; a threshold of 0.85 was applied to determine cluster membership during iterative merging.",
            "model_name": "JINA-EMBEDDINGS-V2-BASE-EN",
            "model_size": null,
            "scientific_domain": "Astronomy and Astrophysics (concept vectorization)",
            "number_of_papers": "297,807 (used on the entire extracted concept set derived from this corpus)",
            "law_type": "Not a law-extraction model per se; enables grouping of semantically similar textual concepts which are used to build a concept-level relational graph (patterns of concept co-occurrence and citation-based relevance).",
            "law_examples": "N/A (embedding model used to cluster textual concepts; the quantitative relevance laws come from citation-transition probabilities computed after concept extraction).",
            "extraction_method": "Vectorization of concept labels and cosine-similarity nearest-neighbor search; iterative clustering and merging based on cosine threshold (0.85).",
            "validation_approach": "Cluster size and granularity validated via domain expert inspection; no classification metrics provided.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Choice of cosine threshold and embedding quality determine cluster granularity; potential semantic conflation or separation depending on embedding behavior; no quantitative evaluation reported.",
            "comparison_baseline": "Not compared quantitatively to other embedding models in the paper.",
            "uuid": "e4486.2",
            "source_info": {
                "paper_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 (categorization)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4) used for taxonomy categorization",
            "brief_description": "GPT-4 was used to classify extracted concepts into higher-level domain categories (e.g., astrophysics subdomains and technical domains such as ML, simulation, instrumentation).",
            "citation_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
            "mention_or_use": "use",
            "system_name": "GPT-4 (categorization/labeling agent)",
            "system_description": "Applied to map finalized concepts to arXiv-style domain categories (Astrophysics of Galaxies, Cosmology, Solar & Stellar, etc.) and to three technological domains (Statistics & ML, Numerical Simulation, Instrumental Design). The classification step organizes nodes for downstream analysis of cross-domain linkages.",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Astronomy and Astrophysics (concept categorization)",
            "number_of_papers": "297,807 (concepts derived from this corpus were categorized)",
            "law_type": "Not directly extracting quantitative laws; enables grouping that allows computation of average cross-domain linkages (aggregated relevance statistics between technical and scientific domain concept sets).",
            "law_examples": "Enables computation of Average Linkage = (1/(|A|*|B|)) Σ_{α∈A,β∈B} p_{α,β} where sets A/B are domain-labeled concepts (machine-learning vs scientific domains).",
            "extraction_method": "Prompt-based classification of concepts into predefined taxonomic buckets using GPT-4.",
            "validation_approach": "Not formally quantified; classification used to produce aggregated analyses (e.g., Figures 2-4) and descriptions; no per-class accuracy reported.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Potential classification errors from GPT-4 not quantified; domain boundary ambiguity; reliance on GPT-4 outputs for aggregated statistics could propagate mislabeling into domain-level linkage metrics.",
            "comparison_baseline": "No quantitative baseline comparison reported against manual labeling or other classifiers.",
            "uuid": "e4486.3",
            "source_info": {
                "paper_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards Specialized Foundation Models in Astronomy",
            "rating": 2
        },
        {
            "paper_title": "LLMs for Knowledge Graph Construction and Reasoning. Recent Capabilities and Future Opportunities",
            "rating": 2
        },
        {
            "paper_title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "rating": 1
        },
        {
            "paper_title": "Machine Learning in Astronomy: a practical overview",
            "rating": 1
        },
        {
            "paper_title": "The frontier of simulation-based inference",
            "rating": 1
        }
    ],
    "cost": 0.0139945,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery
15 Jun 2024</p>
<p>Zechang Sun 
Department of Astronomy
Tsinghua University
BeijingChina</p>
<p>Yuan-Sen Ting yuan-sen.ting@anu.edu.au 
Research School of Astronomy and Astrophysics
Australian National University
CanberraAustralia</p>
<p>School of Computing
Australian National University
CanberraAustralia</p>
<p>Yaobo Liang 
Microsoft Research Asia
BeijingChina</p>
<p>Nan Duan 
Microsoft Research Asia
BeijingChina</p>
<p>Song Huang shuang@mail.tsinghua.edu 
Department of Astronomy
Tsinghua University
BeijingChina</p>
<p>Zheng Cai zcai@mail.tsinghua.edu 
Department of Astronomy
Tsinghua University
BeijingChina</p>
<p>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery
15 Jun 20244FB2512F6FC890B8AB6320027CADC67FarXiv:2406.01391v2[astro-ph.IM]
Identifying and predicting the factors that contribute to the success of interdisciplinary research is crucial for advancing scientific discovery.However, there is a lack of methods to quantify the integration of new ideas and technological advancements in astronomical research and how these new technologies drive further scientific breakthroughs.Large language models, with their ability to extract key concepts from vast literature beyond keyword searches, provide a new tool to quantify such processes.In this study, we extracted concepts in astronomical research from 297,807 publications between 1993 and 2024 using large language models, resulting in a set of 24,939 concepts.These concepts were then used to form a knowledge graph, where the link strength between any two concepts was determined by their relevance through the citation-reference relationships.By calculating this relevance across different time periods, we quantified the impact of numerical simulations and machine learning on astronomical research.The knowledge graph demonstrates two phases of development: a phase where the technology was integrated and another where the technology was explored in scientific discovery.The knowledge graph reveals that despite machine learning has made much inroad in astronomy, there is currently a lack of new concept development at the intersection of AI and Astronomy, which may be the current bottleneck preventing machine learning from further transforming the field of astronomy.</p>
<p>Introduction</p>
<p>Interdisciplinary collaborations can drive innovation in research by introducing new theoretical, analytical, or computational tools to specific scientific domains.These new tools can innovate and transform the fields.For instance, * Work done during internship in Microsoft Research Asia the theoretical understanding of quantum physics and general relativity has driven much of modern cosmology [Weinberg, 2008], and each subsequent engineering breakthrough leads to new windows of observation.A prime example is the detection of gravitational waves with LIGO [Abbott et al., 2016], which was made possible by the convergence of cutting-edge technologies in interferometry.Simultaneously, high-performance computing has paved the way for understanding complex systems in the cosmos, such as the evolution of galaxies [McAlpine et al., 2016;Pillepich et al., 2018] and the inner workings of stars and stellar atmospheres [Gudiksen et al., 2011], through N-body or hydrodynamical simulations.</p>
<p>The advancement of astronomy also relies heavily on the revolution of statistical and analytical methods, which allow for proper inferences based on observations.The introduction of even well-known statistical techniques to astrophysics often leads to key turning points in the field.For example, a cornerstone of our understanding of cosmology comes from analyzing the power spectrum of the cosmic microwave background [Hu and Dodelson, 2002], while the detection of planetary systems outside the solar system has benefited from Gaussian Processes [Hara and Ford, 2023].More recently, the advent of deep learning, with numerous successes in sciences such as AlphaFold [Jumper et al., 2021], has propelled much of the field to rethink statistical inference in astronomy.This includes using generative models as surrogates for the likelihood or posterior [Cranmer et al., 2020;Sun et al., 2023a] and employing flow-based generative models to capture higher-order moment information in stochastic fields [Diaz Rivero and Dvorkin, 2020].</p>
<p>However, the underpinnings of these successful interdisciplinary results often stem from a rigorous process of debate and adaptation within the community.New thought processes are initially treated as disruptors, but a subset of these promising methods subsequently becomes integrated into the field's knowledge base.Over time, such integration gains significant traction and further creates branching of knowledge in the field, fostering its growth.Consider the example of numerical simulation, which was initially viewed as a "distraction" from pure mathematical interest in solving N-body problems and Navier-Stokes equa-tions [Bertschinger, 1998].However, astrophysics has gradually acknowledged that some aspects of the field are nonlinear and beyond analytical understanding.The integration of numerical simulations has subsequently led to the thriving study of galaxy evolution [McAlpine et al., 2016], a widely researched topic, and has also gradually permeated into more specialized domains like solving the accretion physics of black holes and protoplanetary disks [Jiang et al., 2014;Bai, 2016].</p>
<p>However, while such integration and branching off are intuitively clear, studying and quantifying them remains a challenge.Questions such as how long it might take for a field to adopt a new concept and the quantitative impact it has on the field still evade rigorous study.A key bottleneck is the difficulty in defining and extracting the various concepts described in a paper.The classical approach of classification using only keywords or the field [Xu et al., 2018] of research lacks granularity.Other implicit methods that aim to extract vectorized semantic representations from papers [Meijer et al., 2021] are hard to parse at the human level.</p>
<p>Recent advancements in large language models (LLMs), particularly generalized pre-trained transformer techniques [Brown et al., 2020;OpenAI et al., 2023], have demonstrated exceptional zero-shot/few-shot capabilities across various downstream tasks and have shown broad domain knowledge coverage [Bubeck et al., 2023].The synergy between LLMs and knowledge graphs constitutes an active area of research.On the one hand, LLMs have shown robust capability for knowledge graph construction, while on the other hand, the constructed knowledge graph can help LLMs further enhance the accuracy of their responses through retrieval augmented generation [Pan et al., 2023;Zhu et al., 2023].</p>
<p>In this study, we explore the possibility of using LLMs as a bridging tool by distilling concepts from research papers in astronomy and astrophysics and constructing knowledge graphs to study their relationships and co-evolution over time.To the best of our knowledge, this is the first time an LLM-based knowledge graph has been constructed for astrophysics.The combination of the LLM-extracted concepts with our proposed citation-reference-based relevance allows us to quantitatively analyze cross-domain interactions over time and the co-evolution of subfields in astronomy.</p>
<p>This paper is organized as follows: In Section 2, we outline the dataset used for this study.Section 3 details the methodologies employed, including knowledge graph construction with large language model agents and the citation-referencebased relevance to quantify the interconnection between different concepts.We present our findings in Section 4, including a case study focusing on how numerical simulations were gradually adopted by the astronomical community, and by extension, quantifying the current impact of machine learning in astronomy.We discuss and conclude in Section 5.</p>
<p>Literature in Astronomical Research</p>
<p>This study employs a dataset of 297,807 arXiv papers in the fields of astronomy and astrophysics, collected from 1993 to 2024 and sourced from the NASA Astrophysics Data System (NASA/ADS) [Accomazzi, 2024].Astrophysics is known to be a field where the vast majority of publications are on arXiv and easily searchable on ADS.Therefore, the number of arXiv papers here comprises a nearly complete collection of literature published in the field.We downloaded all PDFs from arXiv and performed OCR with Nougat [Blecher et al., 2023].Through human inspection, we found that Nougat did a great job transcribing the data with minimal failures.Auxiliary minor mistakes were identified and cleaned up during those iterations.</p>
<p>A key component of this paper is understanding the relationship between concepts, as viewed by the research community, through the citation relationships within the existing literature.The fact that NASA/ADS oversees a nearly complete literature makes astronomy one of the well-curated fields to explore in this study.We further extract the citation-reference relationships for the entire corpus using the NASA/ADS API 1 to quantify the interaction among various scientific concepts during their co-evolution.</p>
<p>Constructing a Knowledge Graph for Astronomy</p>
<p>Constructing a knowledge graph between concepts in astrophysics requires two essential components: extracting the concepts in astronomical literature through large language model agents, and determining the strength of interconnectivity between concepts through the underlying relationships between paper citations.In this section, we explore these components in more detail.</p>
<p>Concept Extraction with Large Language Models</p>
<p>The key challenges in distilling concepts from publications using large language models are twofold.Firstly, LLM agents may generate hallucinations, producing lists of concepts that deviate from the expectations of human experts.Secondly, even when the concepts are accurately distilled, the models may yield concepts that are either too detailed, overly broad, or merely synonymous with each other, thereby diminishing the practical relevance of understanding their interrelationships.To address these challenges, we employ a multi-agent system in this study, as shown in Figure 1.This system consists of three parts: (a) extraction of concepts from astronomical publications; (b) nearest neighbor search of the concepts; and (c) merging of the concepts.This iterative approach enables control over the granularity of the knowledge graph, tailoring it to our purpose.</p>
<p>In this study, we focus on extracting key concepts from the titles and abstracts of astronomical publications to minimize computational cost.In astronomy, the abstract often encapsulates the essential information, including scientific motivation, methods, and data sources.The whole text processing process, which will be detailed in Section 3, involved about 2 billion tokens with additional prompt and RAG source.To efficiently handle this large-scale data while maintaining cost-effectiveness, we leverage open-source large language models for concept extraction.Specifically, we em- Final Knowledge Graph ploy MISTRAL-7B-INSTRUCT-V0.2 2 [Jiang et al., 2023] as our inference model and JINA-EMBEDDINGS-V2-BASE-EN 3 [Günther et al., 2023] for text embedding.</p>
<p>Concept Extraction:</p>
<p>The first agent is prompted to extract a preliminary set of scientific concepts from the abstracts and titles.While most of these concepts appear to be valid, some of them seem to be hallucinations that are not pertinent to astronomy, such as "misleading result" and "maternal entity in astronomy".To address this issue, a secondary LLM agent is deployed to explain and clarify each term, ensuring the removal of ambiguities and allowing only scientifically valid concepts to proceed.In this clarifying step, we utilize the entire document as an additional source enhanced by retrieval augmented generation to assist our agent in accurately understanding the meanings of various scientific terminologies.The validated scientific concepts are denoted as {c 1 , c 2 , . . ., c N }.</p>
<p>Vectorize and Nearest Neighbor Finding: Once the concepts are extracted and validated, they are transformed into vector representations using the text-embedding models, enabling the accurate computation of similarity measures.We group the concepts based on the cosine similarity of their corresponding vector representations into M clusters, represented as {{c i j , j = 1, . . ., k i }, i = 1, . . ., M }.The number of elements in each cluster, k i , is adaptively determined based on a predefined cosine similarity threshold among the 2 https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 3 https://huggingface.co/jinaai/jina-embeddings-v2-base-enelements within the cluster.In this study, we set the threshold at 0.85, striking a balance between the granularity of the concepts and the computational feasibility of the subsequent steps.</p>
<p>Concept Merging: Finally, the final agent merges these grouped concepts by analyzing clusters of semantically similar concepts and distilling them into more general, unified entities.For example, the concepts "X-Shooter spectra", "DEIMOS spectrograph", and "Keck LRIS spectrograph" were combined into the broader concept of "spectrograph".This merging simplifies the structure of the knowledge graph, reducing redundancy.Furthermore, a coarser knowledge graph improves the readability of the visualization.</p>
<p>We iterate the neighbour finding and merging steps three times, gradually coarsening the collection of concepts from 1,057,280, 164,352, and finally 24,797 concepts, respectively.We found, through domain expert evaluation that, the granularity of the concepts after three iterations is appropriate, with sufficient concepts covering the broad range of topics explored and methods employed in the literature, but with enough fine-grained level to understand the subtle evolution of the field in astrophysics.Some of the final concepts include the commonly known concepts such as "dark matter" and "inflation."On average, each paper consists of ∼ 10 concepts.</p>
<p>Determining Concept Relevance</p>
<p>Upon defining the concepts, perhaps more critical is to determine, quantitatively, how strongly two concepts are rel-evant.The relevancy of two concepts is certainly subjective-concepts that were deemed irrelevant at a certain point in time by the domain expert community might gradually become relevant over time.However, such temporal evolution is exactly what we are after to understand the shift of knowledge over time.</p>
<p>To gauge how two concepts are perceived as relevant by the community at a fixed point in time, the citation-reference relationships between articles become a natural annotated link between the concepts.In the following, we will define based on the probability with which a pair of concepts appears simultaneously in a certain article and its neighboring documents that have a citation-reference relationship, the proximity of the two concepts.This metric between concepts is inspired by the process by which researchers randomly sample through the network of articles from one concept to another.If the researcher can find another new concept from the parent concept that they were originally interested in by searching through the direct citation relation from the paper which contains the parent concept, and this leads the researcher to another paper with a new concept, the two concepts are deemed close.However, if the two concepts can only be found through a small subset of papers of the parent concepts and their citations or references, then the two concepts are deemed further apart at that point in time.We emphasize that while the linkage (and here, the hypothetical "search") is done through the domain of the published literature, the knowledge graph is constructed at the level of the extracted concepts.</p>
<p>More formally, let the final set of concepts be denoted as C : {c 1 , c 2 , . . ., c n }, identified using large-language modelbased agents as outlined in Section 3.1.Let these concepts be associated with a corpus of academic papers, N : {n 1 , n 2 , . . ., n k }, and a set of citation-reference relationships L : {(n a , n b )|n a , n b ∈ N, ∃n a → n b }, where n a → n b signifies that paper n a cites paper n b .To explore the propagation of a concept c α within this network, we define the probability of encountering another concept c β starting from a specific paper n k that discusses c α .This probability, denoted as p α→β|n k , is formulated as:
p α→β|n k = N β |S(n k , L, β)| .(1)
The set S(n k , L, β) is defined through an iterative process starting with the initial paper set n k (denoted as S 0 ).In each iteration, we expand the set by including papers that are directly cited by any paper in the current set and have not been included in previous sets.Formally, if
S n−1 is the set of pa- pers at iteration n − 1, then S n = S n−1 ∪ {n e |(n s , n e ) ∈ L, n s ∈ S n−1 , n e / ∈ S n−1 }.
The iteration continues until at least one paper in the current set contains concept c β , at which point we denote the final set as S T and set S T = S(n k , L, β).The number of papers containing c β within S(n k , L, β) is set to be N β .</p>
<p>Typically, the growth of the sets follows a pattern where |S 0 | = 1, |S 1 | ∼ 10 2 , and |S 2 | ∼ 10 4 in our experiments.This means that if the concepts cannot be found directly from a direct citation from the original paper that contains the parent concept, the number of papers "needed to be read", i.e., |S|, will drastically reduce the relevance of the two concepts.Nonetheless, if the concepts are very prevalent, after a certain level of search, the numerator N β would then offset the volume of search.</p>
<p>As this probability pertains to just a specific paper containing concept c α , the probability of transitioning from concept c α to c β , for all the papers S α that contain c α , would then be the expectation averaging over all papers in S α , or,
p α→β = 1 |S α | n k ∈Sα p α→β|n k (2)
The above equation computes the average probability of moving from c α to c β across all papers that contain c α .To assess the bidirectional relevance of concepts c α and c β , and we will assume that the order of transition between two concepts is not relevant, we define the citation-reference relevance between them as the geometric average of the probabilities of transitioning in both directions:
p α,β = (p α→β • p β→α ) 1/2
(3)</p>
<p>Finally, the transition probability attains the following trivial properties: (1)
p α,β ≤ 1, ∀c α , c β ∈ C; (2) p α,α ≡ 1, ∀c α ∈ C; and (3) p α,β = p β,α , ∀c α , c β ∈ C.
These properties ensure that the relevance metric is well-defined and consistent, providing a foundation for analyzing the relationships between concepts in the knowledge graph.</p>
<p>From Concept Relevance to Knowledge Graph</p>
<p>From the relevance defined as p α,β above, which serves as a robust metric for the link strength between two nodes, we can visualize the knowledge as a force-directed graph.A forcedirected graph [Kobourov, 2012;Bannister et al., 2012], also known as a spring-embedder or force-based layout, is a visual tool designed to illustrate relational data within network graphs.This method leverages simulation techniques inspired by physical systems, arranging nodes-which symbolize entities or concepts-and links-which depict the relationships or connections between these nodes-in a coherent and insightful layout.These graphs utilize the concept of attraction and repulsion forces to strategically distribute nodes.By iteratively updating the positions of nodes based on these attraction and repulsion forces, the force-directed graph algorithm converges to a layout that minimizes the overall energy of the system.This results in an informative 3D representation of the knowledge graph, where closely related concepts are automatically positioned near each other, enhancing the visibility of the density and connectivity within the graph.The capacity of force-directed graphs to dynamically represent complex relational data makes them particularly suitable for visualizing knowledge graphs.</p>
<p>In our context, the link strength between two nodes (concepts) is set to their citation-reference relevance, p α,β .Concepts with higher relevance will attract each other more strongly [Cheong et al., 2021], causing them to be positioned closer together in the visualized graph.Conversely, the repulsion force is applied between all pairs of nodes, ensuring that they remain adequately spaced to prevent overlap and  In the upper panels, we show connections between galaxy physics and other scientific domains.In the lower panel, we highlight the concepts from simulation, statistics, and observational instruments and their respective locations with respect to galaxy physics.Unsurprisingly, the technological concepts are generally more globally spread, as the same techniques can have wide implications for a broad range of topics in astronomy.Machine learning techniques are still at the periphery of the knowledge graph, suggesting that their integration in astronomy is still in its early stages.The interactive version of the knowledge graph is made publicly available at https://astrokg.github.io/.</p>
<p>maintain clear visual separation.By leveraging the citationreference relevance as the link strength between concepts, we can create a graph that intuitively conveys the relationships and clustering of ideas within the astronomical literature.</p>
<p>Intersection between Technological Advancement and Scientific Discovery</p>
<p>Our knowledge graph consists of 24,939 concepts, extracted from 297,807 astronomical research papers, with 339,983,272 interconnections.The visualization of the knowledge graph as a force-directed graph is shown in Figure 2. The filamentous structure shown in the knowledge graph demonstrates the close interconnections across various subdomains within astronomical research.For clarity, we only display concepts that appear in at least 20 papers and consider only those links with a citation-reference relevance p α,β &gt; 0.001.This leads to 9,367 nodes and 32,494 links for the visualization.We set the size of the nodes to be proportional to the logarithm of their frequency of occurrence in the papers.</p>
<p>In the visualization, we further categorize all the concepts into scientific concepts, following the categorization of astrophysics on arXiv4 , namely Astrophysics of Galaxies,5 Cosmology and Nongalactic Astrophysics,6 Earth and Planetary Astrophysics,7 High Energy Astrophysics,8 and Solar and Stellar Astrophysics,9 .As we aim to understand how concepts in technological advancement propel scientific discoveries, we further define another three classes of "technological" domains, which we identify as Statistics and Machine Learning, Numerical Simulation, and Instrumental Design.The classifications below are conducted using GPT-410 .</p>
<p>Figure 2 illustrates how relevant concepts cluster within the same domain and how different domains interconnect.The upper panels demonstrate how the different scientific clusters interact with each other.For instance, galaxy physics, as anticipated, connects with both the largest scales in astronomical research, such as cosmology and general relativity, and the smaller scales, including stellar physics and planetary physics.The lower panel shows how the technological concepts are embedded within the scientific concepts, including numerical simulations, statistics, machine learning, and instrumental design.The technological concepts are generally distributed more globally in the knowledge graph, demonstrating their omnipresence in different subfields.</p>
<p>Interestingly, as shown in the figure, despite the booming interest and popularity, machine learning techniques, particularly deep learning, are situated only at the peripheral region of the knowledge graph.This suggests that machine learning techniques are not yet fully integrated into the astronomical research community, at least from the citation-reference point of view.We will provide a more quantitative comparison of this observation in the following section.</p>
<p>Numerical Simulations in Astronomy</p>
<p>To demonstrate how technological advancement drives scientific discovery, we will study the impact of numerical simulations on astronomy in more depth.In modern-day astronomical research, numerical simulation has become an indispensable tool.However, this was not always the case.The scientific community experienced a gradual transition from focusing primarily on theoretical deduction and analytical formulas to modeling complex phenomena through numerical simulations.To understand this transition, we assess the average relevance between numerical simulations and scientific concepts across various time periods.We divided the dataset into five time periods from 1993 to 2020.In each time period, we recalculate the citation-reference relevance using the papers published within that specific timeframe.</p>
<p>As shown in the bottom panel of Figure 3, unsurprisingly, the number of "scientific concepts" has surged over time.Complementary to these scientific concepts, we also see that the number of technical concepts has surged alongside, especially in terms of numerical simulations and statistical methods, which are shown as red and blue lines in the middle panel.On the other hand, despite the interest in the field, the number of concepts in machine learning in the astronomical literature, as shown by the green line, is still lagging behind these other well-developed technological concepts by an order of magnitude.</p>
<p>Perhaps more interesting is showing the weighted "intersection" between the scientific concepts and the technical concepts, which is shown in the top panels.The top panel shows the weighted "linkage" among all the scientific concepts with the specific technical domain.We define the average linkage as follows:
Average Linkage = 1 |A| • |B| α∈A,β∈B p α,β(4)
where A and B represent the sets of concepts related to specific subfields, such as machine learning and numerical simulation in our study.Here, p α,β denotes the citation-reference relevance as defined in Equation 3. If the new methods are well-adopted in the astronomical community and advance scientific discovery, we should see an improvement in the average citation-reference linkage (large values in the top panel).Viewed this way, there is a clear two-phase evolution with the gradient of the integration oscillating positively (blue arrow) and negatively (red arrow).This is perhaps not surprising.For any technological advancement, it might once be proposed with many technically</p>
<p>Technological Advancement</p>
<p>Technology to Science Discovery</p>
<p>Figure 3: The average linkage for five distinct time periods is used to investigate the temporal integration of technological techniques into scientific research.The middle and lower panels illustrate a consistent increase in the count of concepts, both in terms of scientific concepts (bottom panel) and technical concepts (middle panel).The upper panel shows the total cross-linkage between individual technical domains and scientific concepts, with higher values indicating stronger adoption.The upper panel reveals a two-phase evolution, with an observed latency of approximately five years.The two phases signify the period of development and introduction of new techniques in astronomy and their subsequent adoption by the community (see text for details).Machine learning has begun to reach integration levels comparable to those of numerical simulations seen two decades earlier.However, the number of concepts in machine learning within astronomical research has only increased rather marginally, rising from 152 between 1993 and 2000, to 215 from 2005 to 2010, and reaching 230 between 2015 and 2020.</p>
<p>focused papers written; however, the citation-reference relation is mostly limited to the "technologists," leading to a dilution of the cross-correlation, which is shown by the red arrow.For example, during the period of 1993-2000, there have been many works focusing on the development of N-body simulation techniques [Bode et al., 2000;Romeo et al., 2004;Springel, 2005].Yet, the integration remains marginal.However, from 2000 onward, the astronomical community began to embrace N-body simulations to resolve scientific questions [Paz et al., 2006;Peñarrubia et al., 2006;Zhou and Lin, 2007], resulting in an increase in citation-reference relevance during this time.A similar two-phase pattern is observed from [2010,2015) to [2015,2020), during which time hydrodynamical simulations developed [Genel et al., 2014;Carlesi et al., 2014b;Carlesi et al., 2014a] and gradually gained acceptance [McAlpine et al., 2016;Pillepich et al., 2018] within the community.The delay between the development of new technologies and their impact on scientific discovery spans approximately five years.</p>
<p>Machine Learning in Astrophysics</p>
<p>The revelation of the two-phase adoption in numerical simulations leads to the possibility of better quantifying the integration of machine learning in astronomy.In recent years, we have seen a booming interest in AI and its applications in science.As modern-day astronomy is driven by big data, with billions of sources routinely being surveyed, it is not surprising that astronomy has also seen a drastic integration of AI to advance data processing and analysis [Baron, 2019].</p>
<p>Figure 4 shows the average cross-domain linkage, as defined in Equation 3, but between the concepts in machine learning and the five scientific domains.In terms of the application of machine learning in astronomy, Cosmology &amp; Nongalactic Astrophysics takes the lead, as it benefits from machine learning's capacity to manage complex, large data sets from simulations and surveys [Villaescusa-Navarro et al., 2021b;Villaescusa-Navarro et al., 2021a;Sun et al., 2023b].This is followed by Galaxy Physics, which leverages ML for tasks like photometric redshift prediction [Sun et al., 2023a] and galactic morphology classification [Robertson et al., 2023].Solar and Stellar Physics have also shown promise in emulating and analyzing stellar spectra [Ting et al., 2019].High Energy Astrophysics and Earth &amp; Planetary Astrophysics have been slower to adopt ML.</p>
<p>But is machine learning now well-adopted in astronomical research?Figures 2 and 3 paint an interesting picture.On the one hand, the top panel of Figure 3 shows that there has been a rapid increase in the cross-science-and-AI citation-reference linkage, demonstrating a huge interest among the community.For instance, the scientific-technology score remains flat and low before 2015, signifying that despite a history of AI in astronomy such as the use of neural networks for galaxy morphology classification can trace back to as early as 1990s [Storrie-Lombardi et al., 1992]-its impact remained minimal until the surge in popularity of deep learning post-2015.</p>
<p>Yet, at the same time, even currently, Figure 2 shows that most of these concepts still occupy a peripheral position in the knowledge graph.This suggests that, from a citationreference relevance perspective, such concepts are still con- sidered niche within the broader scientific community.This is perhaps not too surprising because, compared to the deep integration of numerical simulations, quantitatively, the crosslinkage score of machine learning with astronomy remains only at the level that numerical simulations and classical statistics were twenty years ago.</p>
<p>Perhaps what is strikingly lacking is that the number of machine learning concepts (∼ 300) in the astronomical literature remains an order of magnitude smaller than that of numerical simulations (∼ 2, 000), as shown in the middle panel of Figure 3.This might imply that the machine learning techniques widely adopted in astronomy, even at present, remain some of the more classical techniques, such as linear regression and random forests.The rapid adoption of "existing" techniques, while encouraging, might also signify a bigger underlying problem of lack of innovation in applying AI to astronomy.However, if the two-phase evolution applies, we should expect that in the coming years, there will be more novel deep learning techniques introduced before they are gradually adopted by the community.</p>
<p>Discussions and Conclusions</p>
<p>A quantitative study of the evolution of concepts and their interconnections would not be possible without modern-day LLMs, partly due to the large amount of arduous work required to manually label, extract concepts, and classify topics, which can be easily done with minimal computing resources in our case.Even when manual extraction is possible, the taxonomy of a scientific field is often limited-tailored to provide vague contours of the domain, e.g., for publication purposes, rather than a deep and more fine-grained differentiation of the knowledge embedded in the field.</p>
<p>In this study, we construct, to the best of our knowledge, the first large-language-model-based knowledge graph in the domain of astronomy and astrophysics.The knowledge graph comprises 24,939 concepts extracted through a careful iterative process with LLMs from 297,807 papers.We design a relevance metric defined through the citation-reference relations in the astronomical literature to understand the relations as well as the temporal evolution between different concepts.The relevance metric follows the intuition of how humans search for new concepts by quantifying the degree of separation in the citation network as well as the prevalence of the concepts in the field.The relevance is then applied as the linkage strength of the force-directed graph to construct the knowledge graph, allowing us to visualize the knowledge in the field in detail.</p>
<p>Based on this knowledge graph, we evaluate the temporal evolution of the relevance of numerical simulations and machine learning in astronomical research.We showed that while numerical simulations are routinely adopted in modern-day astronomy, the concepts related to them have gone through a long process of gradually being integrated into and accepted by the community.We also found that the integration of numerical simulation into scientific discovery shows a two-phase process, in which a five-year latency can be observed between the development of techniques, where the relevance of the techniques and the science might temporarily diminish, followed by the flourishing period, where the methods mature and are widely applied to astronomical research.We also found that the same trend can be found in classical statistical analysis.</p>
<p>By the same metric, we found that, despite much of the interest and the booming field of machine learning, the impact of machine learning in astronomy remains marginal.While there is a drastic increase in the technique-science cross-referencing, quantitatively, the referencing remains at a level that we observed for numerical simulations about two decades ago.Furthermore, the number of machine learning concepts introduced in astronomy remains an order of magnitude smaller than that of numerical simulations and classical statistical methods, which might imply that the current rapid increase in relevance is driven mainly by the adoption of established machine learning techniques from decades ago.Nonetheless, if the two-phase transition applies, we expect more innovative techniques will be gradually introduced.In fact, in recent years, we have seen many more modern-day techniques, both in terms of flow-based and score-based generative models [De Santi et al., 2024;Zhao et al., 2023], being introduced, as well as, like this study, the application of LLMs in astronomical research [Dung Nguyen et al., 2023;Perkowski et al., 2024].The metric introduced here will be able to continue monitoring this process.</p>
<p>This study primarily aims to show a proof of concept, using LLM-based Knowledge Graph to quantifiably understand the evolution of astronomical research.As such our study certainly has much room for improvement.For instance, proper robust extraction of scientific concepts from literature heavily relies on the alignment between the agents and the researchers' perception.In our study, the concepts are autonomously extracted through the LLM agent, with the granularity of the concepts optimized through merging and pruning.Such an LLM agent can certainly benefit from a subset of high-quality annotated data and comparison with existing hierarchical taxonomies.The process of concept pruning and merging is also somewhat crude, involving vectorizing the concepts and performing a cosine similarity search.A better method would involve further comparing these concepts, utilizing the capabilities of large language models for more detailed concept differentiation and pruning.</p>
<p>In a nutshell, our study demonstrates the potential of LLMbased knowledge graphs in uncovering the intricate relationships and evolution of astronomical research.By providing a quantitative framework for analyzing the integration of new technologies and methodologies, this approach opens up new avenues for understanding the dynamics of interdisciplinary research and the factors that drive scientific progress, in astronomy and beyond.</p>
<p>Ethical Statement</p>
<p>In this study, we construct a knowledge graph by extracting concepts from the astronomical literature available on the arXiv preprint server.Our work aims to advance the understanding of the evolution and interconnections of scientific concepts within the field of astronomy.We emphasize that our study does not involve the direct reproduction or distribution of the original literature itself.Instead, we focus on distilling and analyzing the key concepts present in the existing body of work.</p>
<p>To ensure ethical compliance and respect for intellectual property rights, we will only release the extracted concepts and their relationships, without sharing or reproducing the original text or any substantial portions of the literature.This approach minimizes the risk of copyright infringement and maintains the integrity of the original authors' works.</p>
<p>Furthermore, the field of astronomical research generally operates under an open-sky policy, which promotes collaboration, transparency, and the free exchange of scientific knowledge.This policy aligns with our research objectives and mitigates potential ethical or monetary disputes arising from our work.Our goal is to provide insights that benefit the astronomical community and contribute to the advancement of scientific understanding.</p>
<p>Figure 1 :
1
Figure 1: Schematic plot outlining the knowledge graph construction using large language model agents.The extraction of concepts comprises three main phases: (1) Concept Extraction, where agents construct scientific concepts from documents; (2) Vectorization and Nearest Neighbor Finding, in which concepts are vectorized and grouped by semantic similarity;(3) Concept Merging, where similar concepts are combined to form a more coarse-grained structures.The connections between concepts are then defined by citation-reference relevance as detailed in Section 3.2, with concepts involved in more citation-reference pairs assigned a higher relevance.</p>
<p>Figure 2 :
2
Figure 2: Visualization of a knowledge graph of 24,939 concepts, constructed from 297,807 astronomical research papers.Only concepts appearing in more than 20 papers and links with a link strength greater than 0.001 are displayed.Each concept is categorized into one of the following domains: (A) Galaxy Physics, (B) Cosmology &amp; Nongalactic Physics, (C) Earth &amp; Planetary Science, (D) High Energy Astrophysics, (E) Solar &amp; Stellar Physics, (F) Statistics &amp; AI, (G) Numerical Simulation, or (H) Instrumental Design.In the upper panels, we show connections between galaxy physics and other scientific domains.In the lower panel, we highlight the concepts from simulation, statistics, and observational instruments and their respective locations with respect to galaxy physics.Unsurprisingly, the technological concepts are generally more globally spread, as the same techniques can have wide implications for a broad range of topics in astronomy.Machine learning techniques are still at the periphery of the knowledge graph, suggesting that their integration in astronomy is still in its early stages.The interactive version of the knowledge graph is made publicly available at https://astrokg.github.io/.</p>
<p>Figure 4 :
4
Figure 4: Integration of machine learning in different subfields of astronomy.The integration is defined as the average crossdomain linkage similar to the top panel of Figure 3. Cosmology and Nongalactic Astrophysics currently lead the application of machine learning in astronomy, followed by Galaxy Physics and Solar &amp; Stellar Physics.The adoption of machine learning concepts in Earth &amp; Planetary Physics and High Energy Astrophysics still lags behind.</p>
<p>{ &amp; ,  ' , … ,  ( } { &amp; ,  ' , … ,  ( }
(𝑐 ! ! , 𝑐 " ! , … , 𝑐 # ! )Set of Scientific Concepts(𝑐 ! " , 𝑐 " " , … , 𝑐 $ " ) (𝑐 ! % , 𝑐 " % , … , 𝑐 &amp; % )Vectorize and Nearest Neighbor Finding… ' , … , 𝑐 ( ' , 𝑐 " (𝑐 ! ' )Groups of Concepts Based onSemantic Similarity… Concepts Extraction &amp; FilteringI t e r a t e k t i m e sConcept MergingB a s e d R e f e r e n c e R e l e v a n c e C o m p u t a t i o n o n C i t a t i o n R e l a t i o nDocumentsCoarse-Grained Concepts1 https://ui.adsabs.harvard.edu/help/api/
https://arxiv.org/archive/astro-ph
Astrophysics of Galaxies focuses on phenomena related to galaxies and the Milky Way, including star clusters, interstellar medium, galactic structure, formation, dynamics, and active galactic nuclei.
Cosmology and Nongalactic Astrophysics covers the early universe's phenomenology, cosmic microwave background, dark matter, cosmic strings, and the large-scale structure of the
universe.7  Earth and Planetary Astrophysics studies deal with the interplanetary medium, planetary physics, extrasolar planets, and the formation of the
solar system. 8 High Energy Astrophysics explores cosmic ray production, gamma ray astronomy, supernovae, neutron stars,
and black holes. 9 Solar and Stellar Astrophysics pertains to the investigation of white dwarfs, star formation, stellar evolution, and
helioseismology. 10 https://openai.com/index/gpt-4/
AcknowledgmentsThe authors acknowledge the initial discussions with Kangning Diao and Jing Tao from the Department of Astronomy in Tsinghua University.The authors are grateful to Dr. Peng Cheng of the School of Social Science at Tsinghua University for his expert advice on the philosophy of science.This research has made use of NASA's Astrophysics Data System Bibliographic Services.Y.S.T. acknowledges financial support received from the Australian Research Council via the DECRA Fellowship, grant number DE220101520 and support received from Microsoft's Accelerating Foundation Models Research (AFMR).S.H. is supported by the National Science Foundation of China (grant no.12273015).
. Abbott , Virgo CollaborationarXiv:2401.09685arXiv:1904.07248Physics Review Letter. Thomas, K. A. Thorne, K. S. Thorne, E. Thrane, S. Tiwari, V. Tiwari, K. V. Tokmakov, C. Tomlinson, M. Tonelli, C. V. Torres, C. I. Torrie, D. Töyrä, F. Travasso, G. Traylor, D. Trifirò, M. C. Tringali, L. Trozzo, M. Tse, M. Turconi, D. Tuyenbayev, D. Ugolini, C. S. Unnikrishnan, A. L. Urban, S. A. Usman, H. Vahlbruch, G. Vajente, G. Valdes, M. Vallisneri, N. van Bakel, M. van Beuzekom, J. F. J. van den Brand, C. Van Den Broeck, D. C. Vander-Hyde, L. van der Schaaf, J. V. van Heijningen, A. A. van Veggel, M. Vardaro, S. Vass, M. Vasúth, R. Vaulin, A. Vecchio, G. Vedovato, J. Veitch, P. J. Veitch, K. Venkateswara, D. Verkindt, F. Vetrano, A. Viceré, S. Vinciguerra, D. J. Vine, J. Y. Vinet, S. Vitale, T. Vo, H. Vocca, C. Vorvick, D. Voss, W. D. Vousden, S. P. Vyatchanin, A. R. Wade, L. E. Wade, M. Wade,1166802016. February 2016. January 2024. April 2016. September 2012. April 2019LIGO Scientific CollaborationJ. Zweizig. Accomazzi, 2024] Alberto Accomazzi. Decades of Transformation: Evolution of the NASA Astrophysics Data System's Infrastructure. arXiv e-prints. Bai, 2016] Xue-Ning Bai. Towards a Global Evolutionary Model of Protoplanetary Disks. The Astrophysical Journal. Bannister et al., 2012] Michael J. Bannister, David Eppstein, Michael T. Goodrich, and Lowell Trott. Force-Directed Graph Drawing Using Social Gravity and Scaling. arXiv e-prints. Baron, 2019] Dalya Baron. Machine Learning in Astronomy: a practical overview. arXiv e-prints</p>
<p>The Tree Particle-Mesh N-Body Gravity Solver. Edmund Bertschinger, ; Bertschinger, Blecher, arXiv:2308.13418arXiv:2005.14165Nougat: Neural Optical Understanding for Academic Documents. arXiv e-prints. Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal; Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford1998. January 1998. 2023. August 2023. 2000. June 2000. 2020. May 202036The Astrophysical Journal Supplement Series. Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv e-prints</p>
<p>Hydrodynamical simulations of coupled and uncoupled quintessence models -I. Halo properties and the cosmic web. Bubeck, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv e-prints. Se-Hang Cheong2023. March 2023. 2014a. April 2014. 2014b. April 2014. 2021. 2021439Information Sciences</p>
<p>The frontier of simulation-based inference. Cranmer, Proceedings of the National Academy of Science. the National Academy of Science2020. December 2020117</p>
<p>Deep learning to detect gravitational waves from binary close encounters: Fast parameter estimation using normalizing flows. De Santi, Physical Review D. 109101020042024. May 2024</p>
<p>Ana Diaz Rivero and Cora Dvorkin. Flow-based likelihoods for non-Gaussian inference. Diaz Rivero, Dvorkin , Physical Review D. 102101035072020. November 2020</p>
<p>Introducing the Illustris project: the evolution of galaxy populations across cosmic time. Dung Nguyen, arXiv:2309.06126Towards Specialized Foundation Models in Astronomy. 2023. September 2023. 2014. November 2014. 2011. July 2011445A154arXiv eprintsAstronomy &amp; Astrophysics</p>
<p>A Global Three-dimensional Radiation Magneto-hydrodynamic Simulation of Super-Eddington Accretion Disks. Günther, arXiv:2310.19923Cosmic Microwave Background Anisotropies. Annual Review of Astronomy and Astrophysics. 1011062023. October 2023. March 2023. 2002. January 2002. 2014. December 2014Wayne Hu and Scott DodelsonarXiv e-printsThe Astrophysical Journal</p>
<p>Willmer, and Joris Witstok. Morpheus Reveals Distant Disk Galaxy Morphologies with JWST: The First AI/ML Analysis of JWST Images. Jiang, arXiv:2310.06825arXiv:2310.20125Cathy Horellou, and Jöran Bergh. A wavelet add-on code for new-generation N-body simulations and data de-noising (JOFILUREN). Felipe Petroski Such. Brant E Robertson, Sandro Tacchella, Benjamin D Johnson, Ryan Hausen, Adebusola B Alabi, Kristan Boyett, Andrew J Bunker, Stefano Carniani, Eiichi Egami, Daniel J Eisenstein, Kevin N Hainline, Jakob M Helton, Zhiyuan Ji, Nimisha Kumari, Jianwei Lyu, Roberto Maiolino, Erica J Nelson, Marcia J Rieke, Irene Shivaei, Fengwu Sun, Hannah Übler, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder,; Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek; Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason; Christina C. Williams, Christopher N. A.; Alessandro B. RomeoVolker Springel2023. October 2023. 2021. August 2021. 2012. January 2012. 2016. April 2016. 2021. July 2021. 2023. March 2023. June 2023. March 2006. 2006. July 2006. January 2024. 2018. January 2018. 2023. January 2023. 2004. November 2004. 2005. December 2005. 1992. November 1992. October 2023. November 20235964Juan Felipe Cerón Uribe, Andrea VallonearXiv e-printsThe Astrophysical Journal Supplement Series</p>
<p>The Payne: Selfconsistent ab initio Fitting of Stellar Spectra. The Astrophysical. Ting, Journal. 8792692019. July 2019</p>
<p>. Villaescusa-Navarro, arXiv:2109.09747Multifield Cosmology with Artificial Intelligence. 2021a. September 2021arXiv e-prints</p>
<p>Villaescusa-Navarro, The CAMELS Project: Cosmology and Astrophysics with Machine-learning Simulations. 2021b. July 202191571</p>
<p>Understanding the formation of interdisciplinary research from the perspective of keyword evolution: a case study on joint attention. Steven Weinberg Weinberg, Cosmology, Xu, arXiv:2305.13168Monthly Notices of the Royal Astronomical Society. 11722008. 2008. 2018. 2018. 2023. December 2023. 2007. September 2007. May 2023Zhou and LinLLMs for Knowledge Graph Construction and Reasoning. Recent Capabilities and Future Opportunities. arXiv e-prints</p>            </div>
        </div>

    </div>
</body>
</html>
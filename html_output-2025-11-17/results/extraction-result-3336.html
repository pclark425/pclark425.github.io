<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3336 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3336</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3336</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-2ed471dbf2eac410b344de1b00e0ad9671bc1967</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2ed471dbf2eac410b344de1b00e0ad9671bc1967" target="_blank">The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs</a></p>
                <p><strong>Paper TL;DR:</strong> This work designs a simple task and evaluates four categories of widely used state-of-the-art models and finds that, despite only evaluating on utterances that require a binary inference (yes or no), models in three of these categories perform close to random.</p>
                <p><strong>Paper Abstract:</strong> Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context -- incorporating its pragmatics. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response"I wore gloves"to the question"Did you leave fingerprints?"as meaning"No". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate four categories of widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), models in three of these categories perform close to random. However, LLMs instruction-tuned at the example-level perform significantly better. These results suggest that certain fine-tuning strategies are far better at inducing pragmatic understanding in models. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3336.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3336.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's state-of-the-art instruction-tuned large language model (API model evaluated in this work) that achieved the highest implicature resolution accuracy among evaluated models and benefitted from chain-of-thought prompting to reach average human-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned, multi-task large transformer-based language model provided by OpenAI via API (example-level instruction-tuned family in this paper). Exact architecture and parameter count unknown in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot likelihood ranking / zero-shot prompting', 'few-shot in-context prompting (k-shot)', 'chain-of-thought (CoT) prompting (few-shot CoT)', 'greedy decoding (for API models without likelihood access)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot: prompt a single transformed example and use greedy decoding / likelihood ranking to choose coherent continuation. Few-shot: include k examples from dev set (k in {1,5,10,15,30}) in prompt to prime format and task. Chain-of-thought: manual five-shot CoT prompts where model is guided to produce step-by-step reasoning before the yes/no answer; evaluated in 5-shot CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar set of prompting-based reasoning styles were applied (zero-shot, few-shot, CoT); GPT-4 was evaluated across all and showed sensitivity to adding explicit reasoning (CoT) — the paper uses prompting to probe different reasoning styles rather than altering architecture or training.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A binary implicature task where each example is transformed into a coherent and an incoherent sentence differing only in a 'yes'/'no' implicature; a model 'resolves' the implicature if it assigns higher likelihood (or outputs the correct label in greedy decoding) to the coherent continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 81.8% ± 1.8; 1-shot: 82.3% ± 1.4; 5-shot: 82.0% ± 1.7; 5-shot CoT: 86.5% ± 1.0 (percent accuracy, std over prompt templates as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT prompting produced a +4.5 percentage point improvement for GPT-4 relative to 5-shot (82.0% → 86.5%), bringing it to average human-level (human avg. 86.2%). Few-shot provided small improvements; likelihood vs greedy decoding tradeoff is noted for API models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 benefits from explicit step-by-step (CoT) prompting for implicature resolution; CoT raised GPT-4 to approximate human average performance, indicating explicit reasoning prompts can materially improve pragmatic inference for strong instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>None for GPT-4 specifically; small increases from few-shot only (+0.2 pp from 0→1 or 1→5), indicating that short few-shot examples alone are less effective than CoT for this model on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3336.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI instruction-tuned model (part of the text-davinci series) in the example-level instruction-tuned family; showed substantial improvement from few-shot prompting and benefitted from chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI API model from the text-davinci series; example-level instruction-tuned on diverse tasks (exact training details proprietary). Evaluated with likelihood/greedy approaches via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot in-context prompting', 'chain-of-thought (CoT) prompting (few-shot CoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same evaluation protocol: compare coherent vs incoherent 'yes/no' continuations; CoT implemented as manual five-shot chain-of-thought prompts written for each template.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles applied; benefited from both few-shot and CoT, indicating responsiveness to different prompting-based reasoning styles rather than architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 71.2% ± 2.8; 1-shot: 74.3% ± 1.4; 5-shot: 79.7% ± 0.6; 5-shot CoT: 83.6% ± 0.6 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>text-davinci-003 shows +8.5 pp improvement going from 0-shot to 5-shot, and an additional +4.0 pp improvement with CoT over 5-shot, indicating both few-shot priming and explicit chain-of-thought help.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Example-level instruction-tuned OpenAI engines like text-davinci-003 gain substantial accuracy from few-shot and further from CoT prompting, demonstrating that explicit stepwise reasoning prompts can improve pragmatic implicature performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No negative effect of CoT observed for text-davinci-003 in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3336.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002 (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI text-davinci engine (example-level instruction-tuned) that benefits strongly from few-shot prompting and modestly from chain-of-thought prompting on implicature resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI API model in the text-davinci series; example-level instruction-tuned. Exact sizes/training proprietary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot in-context prompting', 'chain-of-thought prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluation as with other API models: models given templates in zero- and few-shot modes; CoT implemented via manual five-shot chain-of-thought prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Same family of prompting methods applied; few-shot had large effect while CoT provided a small additional improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 70.6% ± 2.3; 1-shot: 75.6% ± 2.8; 5-shot: 79.6% ± 2.0; 5-shot CoT: 80.1% ± 0.8 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>text-davinci-002 improved substantially with few-shot (+9.0 pp 0→5-shot) and marginally with CoT (+0.5 pp 5-shot→5-shot CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Few-shot priming is highly beneficial for this model; CoT gives small extra gains though not as large as for some other OpenAI engines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT impact is small (+0.5 pp), indicating diminishing returns of CoT for this engine compared to others.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3336.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-001 (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI API engine in the text-davinci series that shows moderate baseline performance and puzzling negative reaction to chain-of-thought prompting on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Earlier OpenAI engine in the text-davinci family; example-level instruction-tuned (proprietary details not disclosed). Evaluated via API prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot in-context prompting', 'chain-of-thought prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same evaluation regime; CoT implemented as manual five-shot CoT prompts. For this model CoT decreased measured accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Prompting styles applied uniformly; however CoT had a negative effect here — an example where adding explicit stepwise reasoning harmed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 72.3% ± 2.8; 1-shot: 72.7% ± 1.3; 5-shot: 74.5% ± 1.0; 5-shot CoT: 67.3% ± 2.6 (accuracy %; CoT caused -7.2 pp vs 5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Unlike other text-davinci variants, CoT prompting substantially reduced accuracy, indicating that explicit chain-of-thought can sometimes harm performance depending on model internals or prompt-match.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-thought prompts are not uniformly beneficial; they can degrade performance in some models (text-davinci-001) even when few-shot helps modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>5-shot CoT produced a large negative effect (-7.2 pp) compared to 5-shot, showing a counter-example to the assumption that CoT is always helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3336.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI chat-completion family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's chat-optimized instruction-tuned model (chat-completion API); part of the example-level instruction-tuned family evaluated and shown to benefit modestly from chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-optimized instruction-tuned model provided by OpenAI via chat API (exact version/size not disclosed). Evaluated via greedy decoding outputs for yes/no answers and via CoT prompts where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot in-context prompting', 'chain-of-thought prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompted in natural and structured templates in zero/few-shot modes; CoT implemented as five-shot chain-of-thought prompts that guide step-wise reasoning before answer.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Same suite of prompting styles tested; ChatGPT shows modest gains from CoT (≈+3.3 pp 5-shot→5-shot CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 72.1% ± 6.0; 1-shot: 75.1% ± 1.5; 5-shot: 73.9% ± 6.3; 5-shot CoT: 77.2% ± 1.0 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Few-shot had small/variable effects; CoT produced consistent improvements over 5-shot for ChatGPT in these tests.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chat-optimized models show modest benefit from explicit reasoning prompts; variance across prompt templates is higher relative to some API engines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative results</strong></td>
                            <td>None large; 5-shot without CoT showed small fluctuation versus 1-shot and 0-shot, indicating sensitivity to in-context examples and prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3336.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohere-command-52B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohere-command-52B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cohere's example-level instruction-tuned command model (52B parameters) that exhibits large few-shot gains but little or no benefit from chain-of-thought prompting on implicature resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cohere-command-52B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cohere's command-family model fine-tuned from Cohere-base with example-level instruction tuning (paper treats it as example-level IT). Known parameterization: 52B non-embedding parameters as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>52B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot likelihood ranking', 'few-shot in-context prompting', 'chain-of-thought prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated with likelihood ranking where available; few-shot examples prime yes/no-format; CoT prompts applied but did not materially help.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Prompting-based methods were applied; model shows responsiveness to few-shot priming but not to explicit CoT reasoning prompts, indicating that different prompting styles produce different efficacy across models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 60.2% ± 5.2; 1-shot: 72.8% ± 1.3; 5-shot: 75.4% ± 1.8; 5-shot CoT: 75.3% ± 0.5 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Large jump from 0→1-shot (≈+12.6 pp) and further improvement to 5-shot; CoT provided essentially no improvement (−0.1 pp), supporting authors' hypothesis that few-shot primarily calibrates output format rather than inducing pragmatics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Few-shot in-context examples dramatically improve calibration and measured accuracy for this model; CoT did not provide additional gains, so explicit stepwise prompting isn't universally effective.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT produced no benefit (5-shot 75.4% vs 5-shot CoT 75.3%), a negative result relative to models like GPT-4 and text-davinci-003.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3336.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3-175B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (davinci) 175B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's 175B-parameter base autoregressive model (pre-instruction-tuning baseline in this paper) that shows modest few-shot gains but markedly lower performance than example-level instruction-tuned models on implicature resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer (175B parameters) pre-trained with next-token prediction; evaluated here as a base model (not example-level instruction-tuned in the study).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot likelihood ranking', 'few-shot in-context prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same implicit-likelihood evaluation where possible, few-shot in-context examples sampled from development set; no CoT experiments reported for this base model in the main CoT table.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Evaluated with zero- and few-shot prompting styles; fewer reasoning styles tested compared to example-level IT models for CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 57.7% ± 4.4; 1-shot: 65.7% ± 1.4; 5-shot: 68.7% ± 1.5 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Few-shot improves performance (≈+11 pp from 0→5-shot), but base GPT-3 remains substantially lower than example-level IT models (e.g., GPT-4, text-davinci variants).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Base large-scale pretraining alone yields near-random or modest implicature resolution ability; few-shot helps but does not close the gap to example-level instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No CoT gains reported for GPT-3 in this study; extrapolated scaling suggests much larger parameter counts would be needed to reach human-level performance (authors estimate ~642B for GPT-3 scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3336.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark-level instruction-tuned model (T5 family fine-tuned on datasets with a single instruction per dataset) that shows small-to-no improvement with few-shot examples and overall poor performance relative to example-level IT models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling instruction-finetuned language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-based transformer fine-tuned on many tasks with a single instruction per dataset (benchmark-level instruction tuning); used as a benchmark IT model in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot in-context prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated in same zero/few-shot templates; benchmark-level IT models are designed for strong zero-shot generalization and sometimes do not benefit from few-shot formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles applied, but model class (benchmark IT) is trained with a single instruction per dataset rather than diverse per-example instructions; authors interpret this as less diverse instruction exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 60.8% ± 2.4; 1-shot: 57.4% ± 5.0; 5-shot: 61.7% ± 4.8 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Flan-T5 does not reliably improve with few-shot examples and shows small fluctuations; performance remains far below example-level IT models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Benchmark-level instruction tuning (single instruction per dataset) does not produce the pragmatic understanding needed for implicature resolution that example-level IT produces; few-shot often does not help and can sometimes decrease performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Observed decrease in performance for some k-shot settings (1-shot performance lower than 0-shot), demonstrating that few-shot can hurt benchmark IT models on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3336.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T0-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T0 (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multitask benchmark-level instruction-tuned model (T0) that was designed for improved zero-shot generalization but performed poorly on few-shot implicature resolution in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multitask prompted training enables zero-shot task generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T0 is a model fine-tuned via multitask prompted training (benchmark-level instruction tuning) to improve zero-shot task generalization; 11B parameter variant evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot in-context prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated with the study's set of six prompt templates across zero/few-shot; T0 is expected to generalize zero-shot but may not respond to few-shot priming.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Trained with a single instruction per dataset (benchmark-level), thus less per-example instruction diversity; tested with similar prompting styles as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 55.6% ± 7.0; 1-shot: 47.8% ± 0.5; 5-shot: 47.0% ± 0.2 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>T0 shows a decrease in performance when few-shot examples are added (0-shot >> 1-shot/5-shot), consistent with authors' expectation that benchmark-level IT optimizes zero-shot generalization and can be destabilized by in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Benchmark-level instruction-tuned models like T0 can be worse at few-shot in-context usage for pragmatic tasks and may even degrade when adding examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Significant negative effect of few-shot prompting (≈ −8.6 pp from 0-shot to 1-shot, and further to 5-shot), a clear counter-example to the idea that few-shot always helps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3336.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BlenderBot-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BlenderBot-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dialogue-fine-tuned model (BlenderBot) tuned on conversational data that performs near-random on implicature resolution across zero- and few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BlenderBot-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Facebook/Meta's BlenderBot variant fine-tuned on conversational data (dialogue FT group); 2.7B parameter model evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot in-context prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated under the same prompt templates; dialogue fine-tuning did not confer pragmatic implicature resolution ability in these tests.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Only standard prompting styles tested; dialogue fine-tuning represents a different training style but not a diversity of in-prompt reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>0-shot: 53.4% ± 0.3; 1-shot: 53.3% ± 0.1; 5-shot: 53.3% ± 0.1 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>No meaningful improvement with few-shot prompting; performance close to random (50%), showing dialogue fine-tuning alone is insufficient for implicature resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning on conversational data (dialogue FT) did not produce pragmatic implicature understanding in BlenderBot; it remained near-random across prompt styles.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No improvement from few-shot or CoT (CoT not meaningfully evaluated for BlenderBot in main CoT experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3336.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3336.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Base models (group)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale pre-trained base models (GPT-2, BERT, RoBERTa, BLOOM, OPT, EleutherAI, Cohere base, GPT-3 pre-instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of large-scale pre-trained models (no example-level instruction-tuning) that generally perform near-random on binary implicature resolution; some scale improves few-shot performance modestly but gap with humans remains large.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Base pre-trained models (e.g., GPT-2-xl, BLOOM-176B, OPT-13B, EleutherAI-20B, Cohere-base, GPT-3-175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Family of models pre-trained on next-token prediction without example-level instruction-tuning; sizes vary and several specific instances were evaluated (see Table 1 of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., GPT-3 175B, BLOOM 176B, OPT 13B, EleutherAI 20B, GPT-2-xl ~1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot likelihood ranking', 'few-shot in-context prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated via the coherent vs incoherent likelihood ranking and few-shot in-context examples; no CoT experiments reported for most base models in the main CoT table.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles applied across models; lack of instruction-tuning diversity correlated with low performance on the pragmatic task.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Binary implicature resolution (George & Mamidi 2020 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Representative values from Table 1: GPT-2-xl 0-shot 51.3% ± 2.9, 5-shot 57.7% ± 1.1; BLOOM-176B 0-shot 54.2% ± 1.2, 5-shot 65.4% ± 3.4; OPT-13B 0-shot 61.0% ± 5.5, 5-shot 67.4% ± 2.1; EleutherAI-20B 0-shot 57.5% ± 3.3, 5-shot 61.1% ± 4.9; GPT-3-175B 0-shot 57.7% ± 4.4, 5-shot 68.7% ± 1.5 (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Some base models showed positive correlation with scale for k-shot (k≥1), but overall base models stayed much closer to random performance than example-level IT models; few-shot helps but not to human levels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large-scale pretraining without example-level instruction-tuning does not produce strong pragmatic implicature understanding; scaling helps some but would require much larger sizes to match example-level IT models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>While some base models saw improvements with scale and few-shot, they still lagged behind example-level IT models; no CoT gains reported for base models in main experiments (two base models tried for CoT did not improve).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 2)</em></li>
                <li>Scaling instruction-finetuned language models <em>(Rating: 2)</em></li>
                <li>Conversational implicatures in english dialogue: Annotated dataset <em>(Rating: 2)</em></li>
                <li>GRICE: A grammar-based dataset for recovering implicature and conversational rEasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3336",
    "paper_id": "paper-2ed471dbf2eac410b344de1b00e0ad9671bc1967",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's state-of-the-art instruction-tuned large language model (API model evaluated in this work) that achieved the highest implicature resolution accuracy among evaluated models and benefitted from chain-of-thought prompting to reach average human-level performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "An instruction-tuned, multi-task large transformer-based language model provided by OpenAI via API (example-level instruction-tuned family in this paper). Exact architecture and parameter count unknown in this paper.",
            "model_size": "unknown",
            "reasoning_methods": [
                "zero-shot likelihood ranking / zero-shot prompting",
                "few-shot in-context prompting (k-shot)",
                "chain-of-thought (CoT) prompting (few-shot CoT)",
                "greedy decoding (for API models without likelihood access)"
            ],
            "reasoning_methods_description": "Zero-shot: prompt a single transformed example and use greedy decoding / likelihood ranking to choose coherent continuation. Few-shot: include k examples from dev set (k in {1,5,10,15,30}) in prompt to prime format and task. Chain-of-thought: manual five-shot CoT prompts where model is guided to produce step-by-step reasoning before the yes/no answer; evaluated in 5-shot CoT.",
            "diversity_of_methods": "Similar set of prompting-based reasoning styles were applied (zero-shot, few-shot, CoT); GPT-4 was evaluated across all and showed sensitivity to adding explicit reasoning (CoT) — the paper uses prompting to probe different reasoning styles rather than altering architecture or training.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "A binary implicature task where each example is transformed into a coherent and an incoherent sentence differing only in a 'yes'/'no' implicature; a model 'resolves' the implicature if it assigns higher likelihood (or outputs the correct label in greedy decoding) to the coherent continuation.",
            "performance_by_method": "0-shot: 81.8% ± 1.8; 1-shot: 82.3% ± 1.4; 5-shot: 82.0% ± 1.7; 5-shot CoT: 86.5% ± 1.0 (percent accuracy, std over prompt templates as reported).",
            "comparison_of_methods": "CoT prompting produced a +4.5 percentage point improvement for GPT-4 relative to 5-shot (82.0% → 86.5%), bringing it to average human-level (human avg. 86.2%). Few-shot provided small improvements; likelihood vs greedy decoding tradeoff is noted for API models.",
            "key_findings": "GPT-4 benefits from explicit step-by-step (CoT) prompting for implicature resolution; CoT raised GPT-4 to approximate human average performance, indicating explicit reasoning prompts can materially improve pragmatic inference for strong instruction-tuned models.",
            "counter_examples_or_negative_results": "None for GPT-4 specifically; small increases from few-shot only (+0.2 pp from 0→1 or 1→5), indicating that short few-shot examples alone are less effective than CoT for this model on this task.",
            "uuid": "e3336.0",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (OpenAI API)",
            "brief_description": "An OpenAI instruction-tuned model (part of the text-davinci series) in the example-level instruction-tuned family; showed substantial improvement from few-shot prompting and benefitted from chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "OpenAI API model from the text-davinci series; example-level instruction-tuned on diverse tasks (exact training details proprietary). Evaluated with likelihood/greedy approaches via API.",
            "model_size": "unknown",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot in-context prompting",
                "chain-of-thought (CoT) prompting (few-shot CoT)"
            ],
            "reasoning_methods_description": "Same evaluation protocol: compare coherent vs incoherent 'yes/no' continuations; CoT implemented as manual five-shot chain-of-thought prompts written for each template.",
            "diversity_of_methods": "Similar prompting styles applied; benefited from both few-shot and CoT, indicating responsiveness to different prompting-based reasoning styles rather than architectural changes.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 71.2% ± 2.8; 1-shot: 74.3% ± 1.4; 5-shot: 79.7% ± 0.6; 5-shot CoT: 83.6% ± 0.6 (accuracy %).",
            "comparison_of_methods": "text-davinci-003 shows +8.5 pp improvement going from 0-shot to 5-shot, and an additional +4.0 pp improvement with CoT over 5-shot, indicating both few-shot priming and explicit chain-of-thought help.",
            "key_findings": "Example-level instruction-tuned OpenAI engines like text-davinci-003 gain substantial accuracy from few-shot and further from CoT prompting, demonstrating that explicit stepwise reasoning prompts can improve pragmatic implicature performance.",
            "counter_examples_or_negative_results": "No negative effect of CoT observed for text-davinci-003 in this task.",
            "uuid": "e3336.1",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002 (OpenAI API)",
            "brief_description": "An earlier OpenAI text-davinci engine (example-level instruction-tuned) that benefits strongly from few-shot prompting and modestly from chain-of-thought prompting on implicature resolution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "OpenAI API model in the text-davinci series; example-level instruction-tuned. Exact sizes/training proprietary.",
            "model_size": "unknown",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot in-context prompting",
                "chain-of-thought prompting"
            ],
            "reasoning_methods_description": "Evaluation as with other API models: models given templates in zero- and few-shot modes; CoT implemented via manual five-shot chain-of-thought prompts.",
            "diversity_of_methods": "Same family of prompting methods applied; few-shot had large effect while CoT provided a small additional improvement.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 70.6% ± 2.3; 1-shot: 75.6% ± 2.8; 5-shot: 79.6% ± 2.0; 5-shot CoT: 80.1% ± 0.8 (accuracy %).",
            "comparison_of_methods": "text-davinci-002 improved substantially with few-shot (+9.0 pp 0→5-shot) and marginally with CoT (+0.5 pp 5-shot→5-shot CoT).",
            "key_findings": "Few-shot priming is highly beneficial for this model; CoT gives small extra gains though not as large as for some other OpenAI engines.",
            "counter_examples_or_negative_results": "CoT impact is small (+0.5 pp), indicating diminishing returns of CoT for this engine compared to others.",
            "uuid": "e3336.2",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "text-davinci-001",
            "name_full": "text-davinci-001 (OpenAI API)",
            "brief_description": "An OpenAI API engine in the text-davinci series that shows moderate baseline performance and puzzling negative reaction to chain-of-thought prompting on this task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-001",
            "model_description": "Earlier OpenAI engine in the text-davinci family; example-level instruction-tuned (proprietary details not disclosed). Evaluated via API prompts.",
            "model_size": "unknown",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot in-context prompting",
                "chain-of-thought prompting"
            ],
            "reasoning_methods_description": "Same evaluation regime; CoT implemented as manual five-shot CoT prompts. For this model CoT decreased measured accuracy.",
            "diversity_of_methods": "Prompting styles applied uniformly; however CoT had a negative effect here — an example where adding explicit stepwise reasoning harmed performance.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 72.3% ± 2.8; 1-shot: 72.7% ± 1.3; 5-shot: 74.5% ± 1.0; 5-shot CoT: 67.3% ± 2.6 (accuracy %; CoT caused -7.2 pp vs 5-shot).",
            "comparison_of_methods": "Unlike other text-davinci variants, CoT prompting substantially reduced accuracy, indicating that explicit chain-of-thought can sometimes harm performance depending on model internals or prompt-match.",
            "key_findings": "Chain-of-thought prompts are not uniformly beneficial; they can degrade performance in some models (text-davinci-001) even when few-shot helps modestly.",
            "counter_examples_or_negative_results": "5-shot CoT produced a large negative effect (-7.2 pp) compared to 5-shot, showing a counter-example to the assumption that CoT is always helpful.",
            "uuid": "e3336.3",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI chat-completion family)",
            "brief_description": "OpenAI's chat-optimized instruction-tuned model (chat-completion API); part of the example-level instruction-tuned family evaluated and shown to benefit modestly from chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Chat-optimized instruction-tuned model provided by OpenAI via chat API (exact version/size not disclosed). Evaluated via greedy decoding outputs for yes/no answers and via CoT prompts where applicable.",
            "model_size": "unknown",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot in-context prompting",
                "chain-of-thought prompting"
            ],
            "reasoning_methods_description": "Prompted in natural and structured templates in zero/few-shot modes; CoT implemented as five-shot chain-of-thought prompts that guide step-wise reasoning before answer.",
            "diversity_of_methods": "Same suite of prompting styles tested; ChatGPT shows modest gains from CoT (≈+3.3 pp 5-shot→5-shot CoT).",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 72.1% ± 6.0; 1-shot: 75.1% ± 1.5; 5-shot: 73.9% ± 6.3; 5-shot CoT: 77.2% ± 1.0 (accuracy %).",
            "comparison_of_methods": "Few-shot had small/variable effects; CoT produced consistent improvements over 5-shot for ChatGPT in these tests.",
            "key_findings": "Chat-optimized models show modest benefit from explicit reasoning prompts; variance across prompt templates is higher relative to some API engines.",
            "counter_examples_or_negative results": "None large; 5-shot without CoT showed small fluctuation versus 1-shot and 0-shot, indicating sensitivity to in-context examples and prompt wording.",
            "uuid": "e3336.4",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Cohere-command-52B",
            "name_full": "Cohere-command-52B",
            "brief_description": "Cohere's example-level instruction-tuned command model (52B parameters) that exhibits large few-shot gains but little or no benefit from chain-of-thought prompting on implicature resolution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Cohere-command-52B",
            "model_description": "Cohere's command-family model fine-tuned from Cohere-base with example-level instruction tuning (paper treats it as example-level IT). Known parameterization: 52B non-embedding parameters as reported.",
            "model_size": "52B",
            "reasoning_methods": [
                "zero-shot likelihood ranking",
                "few-shot in-context prompting",
                "chain-of-thought prompting"
            ],
            "reasoning_methods_description": "Evaluated with likelihood ranking where available; few-shot examples prime yes/no-format; CoT prompts applied but did not materially help.",
            "diversity_of_methods": "Prompting-based methods were applied; model shows responsiveness to few-shot priming but not to explicit CoT reasoning prompts, indicating that different prompting styles produce different efficacy across models.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 60.2% ± 5.2; 1-shot: 72.8% ± 1.3; 5-shot: 75.4% ± 1.8; 5-shot CoT: 75.3% ± 0.5 (accuracy %).",
            "comparison_of_methods": "Large jump from 0→1-shot (≈+12.6 pp) and further improvement to 5-shot; CoT provided essentially no improvement (−0.1 pp), supporting authors' hypothesis that few-shot primarily calibrates output format rather than inducing pragmatics.",
            "key_findings": "Few-shot in-context examples dramatically improve calibration and measured accuracy for this model; CoT did not provide additional gains, so explicit stepwise prompting isn't universally effective.",
            "counter_examples_or_negative_results": "CoT produced no benefit (5-shot 75.4% vs 5-shot CoT 75.3%), a negative result relative to models like GPT-4 and text-davinci-003.",
            "uuid": "e3336.5",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-3-175B",
            "name_full": "GPT-3 (davinci) 175B",
            "brief_description": "OpenAI's 175B-parameter base autoregressive model (pre-instruction-tuning baseline in this paper) that shows modest few-shot gains but markedly lower performance than example-level instruction-tuned models on implicature resolution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci)",
            "model_description": "Large autoregressive transformer (175B parameters) pre-trained with next-token prediction; evaluated here as a base model (not example-level instruction-tuned in the study).",
            "model_size": "175B",
            "reasoning_methods": [
                "zero-shot likelihood ranking",
                "few-shot in-context prompting"
            ],
            "reasoning_methods_description": "Same implicit-likelihood evaluation where possible, few-shot in-context examples sampled from development set; no CoT experiments reported for this base model in the main CoT table.",
            "diversity_of_methods": "Evaluated with zero- and few-shot prompting styles; fewer reasoning styles tested compared to example-level IT models for CoT.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 57.7% ± 4.4; 1-shot: 65.7% ± 1.4; 5-shot: 68.7% ± 1.5 (accuracy %).",
            "comparison_of_methods": "Few-shot improves performance (≈+11 pp from 0→5-shot), but base GPT-3 remains substantially lower than example-level IT models (e.g., GPT-4, text-davinci variants).",
            "key_findings": "Base large-scale pretraining alone yields near-random or modest implicature resolution ability; few-shot helps but does not close the gap to example-level instruction-tuned models.",
            "counter_examples_or_negative_results": "No CoT gains reported for GPT-3 in this study; extrapolated scaling suggests much larger parameter counts would be needed to reach human-level performance (authors estimate ~642B for GPT-3 scaling).",
            "uuid": "e3336.6",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Flan-T5-11B",
            "name_full": "Flan-T5 (11B)",
            "brief_description": "A benchmark-level instruction-tuned model (T5 family fine-tuned on datasets with a single instruction per dataset) that shows small-to-no improvement with few-shot examples and overall poor performance relative to example-level IT models.",
            "citation_title": "Scaling instruction-finetuned language models",
            "mention_or_use": "use",
            "model_name": "Flan-T5-11B",
            "model_description": "T5-based transformer fine-tuned on many tasks with a single instruction per dataset (benchmark-level instruction tuning); used as a benchmark IT model in this paper.",
            "model_size": "11B",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot in-context prompting"
            ],
            "reasoning_methods_description": "Evaluated in same zero/few-shot templates; benchmark-level IT models are designed for strong zero-shot generalization and sometimes do not benefit from few-shot formatting.",
            "diversity_of_methods": "Similar prompting styles applied, but model class (benchmark IT) is trained with a single instruction per dataset rather than diverse per-example instructions; authors interpret this as less diverse instruction exposure.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 60.8% ± 2.4; 1-shot: 57.4% ± 5.0; 5-shot: 61.7% ± 4.8 (accuracy %).",
            "comparison_of_methods": "Flan-T5 does not reliably improve with few-shot examples and shows small fluctuations; performance remains far below example-level IT models.",
            "key_findings": "Benchmark-level instruction tuning (single instruction per dataset) does not produce the pragmatic understanding needed for implicature resolution that example-level IT produces; few-shot often does not help and can sometimes decrease performance.",
            "counter_examples_or_negative_results": "Observed decrease in performance for some k-shot settings (1-shot performance lower than 0-shot), demonstrating that few-shot can hurt benchmark IT models on this task.",
            "uuid": "e3336.7",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "T0-11B",
            "name_full": "T0 (11B)",
            "brief_description": "A multitask benchmark-level instruction-tuned model (T0) that was designed for improved zero-shot generalization but performed poorly on few-shot implicature resolution in this study.",
            "citation_title": "Multitask prompted training enables zero-shot task generalization",
            "mention_or_use": "use",
            "model_name": "T0-11B",
            "model_description": "T0 is a model fine-tuned via multitask prompted training (benchmark-level instruction tuning) to improve zero-shot task generalization; 11B parameter variant evaluated.",
            "model_size": "11B",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot in-context prompting"
            ],
            "reasoning_methods_description": "Evaluated with the study's set of six prompt templates across zero/few-shot; T0 is expected to generalize zero-shot but may not respond to few-shot priming.",
            "diversity_of_methods": "Trained with a single instruction per dataset (benchmark-level), thus less per-example instruction diversity; tested with similar prompting styles as other models.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 55.6% ± 7.0; 1-shot: 47.8% ± 0.5; 5-shot: 47.0% ± 0.2 (accuracy %).",
            "comparison_of_methods": "T0 shows a decrease in performance when few-shot examples are added (0-shot &gt;&gt; 1-shot/5-shot), consistent with authors' expectation that benchmark-level IT optimizes zero-shot generalization and can be destabilized by in-context examples.",
            "key_findings": "Benchmark-level instruction-tuned models like T0 can be worse at few-shot in-context usage for pragmatic tasks and may even degrade when adding examples.",
            "counter_examples_or_negative_results": "Significant negative effect of few-shot prompting (≈ −8.6 pp from 0-shot to 1-shot, and further to 5-shot), a clear counter-example to the idea that few-shot always helps.",
            "uuid": "e3336.8",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "BlenderBot-2.7B",
            "name_full": "BlenderBot-2.7B",
            "brief_description": "A dialogue-fine-tuned model (BlenderBot) tuned on conversational data that performs near-random on implicature resolution across zero- and few-shot prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BlenderBot-2.7B",
            "model_description": "Facebook/Meta's BlenderBot variant fine-tuned on conversational data (dialogue FT group); 2.7B parameter model evaluated.",
            "model_size": "2.7B",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot in-context prompting"
            ],
            "reasoning_methods_description": "Evaluated under the same prompt templates; dialogue fine-tuning did not confer pragmatic implicature resolution ability in these tests.",
            "diversity_of_methods": "Only standard prompting styles tested; dialogue fine-tuning represents a different training style but not a diversity of in-prompt reasoning methods.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "0-shot: 53.4% ± 0.3; 1-shot: 53.3% ± 0.1; 5-shot: 53.3% ± 0.1 (accuracy %).",
            "comparison_of_methods": "No meaningful improvement with few-shot prompting; performance close to random (50%), showing dialogue fine-tuning alone is insufficient for implicature resolution.",
            "key_findings": "Fine-tuning on conversational data (dialogue FT) did not produce pragmatic implicature understanding in BlenderBot; it remained near-random across prompt styles.",
            "counter_examples_or_negative_results": "No improvement from few-shot or CoT (CoT not meaningfully evaluated for BlenderBot in main CoT experiments).",
            "uuid": "e3336.9",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Base models (group)",
            "name_full": "Large-scale pre-trained base models (GPT-2, BERT, RoBERTa, BLOOM, OPT, EleutherAI, Cohere base, GPT-3 pre-instruction)",
            "brief_description": "A class of large-scale pre-trained models (no example-level instruction-tuning) that generally perform near-random on binary implicature resolution; some scale improves few-shot performance modestly but gap with humans remains large.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Base pre-trained models (e.g., GPT-2-xl, BLOOM-176B, OPT-13B, EleutherAI-20B, Cohere-base, GPT-3-175B)",
            "model_description": "Family of models pre-trained on next-token prediction without example-level instruction-tuning; sizes vary and several specific instances were evaluated (see Table 1 of paper).",
            "model_size": "various (e.g., GPT-3 175B, BLOOM 176B, OPT 13B, EleutherAI 20B, GPT-2-xl ~1.5B)",
            "reasoning_methods": [
                "zero-shot likelihood ranking",
                "few-shot in-context prompting"
            ],
            "reasoning_methods_description": "Evaluated via the coherent vs incoherent likelihood ranking and few-shot in-context examples; no CoT experiments reported for most base models in the main CoT table.",
            "diversity_of_methods": "Similar prompting styles applied across models; lack of instruction-tuning diversity correlated with low performance on the pragmatic task.",
            "reasoning_task_name": "Binary implicature resolution (George & Mamidi 2020 dataset)",
            "reasoning_task_description": "See GPT-4 entry.",
            "performance_by_method": "Representative values from Table 1: GPT-2-xl 0-shot 51.3% ± 2.9, 5-shot 57.7% ± 1.1; BLOOM-176B 0-shot 54.2% ± 1.2, 5-shot 65.4% ± 3.4; OPT-13B 0-shot 61.0% ± 5.5, 5-shot 67.4% ± 2.1; EleutherAI-20B 0-shot 57.5% ± 3.3, 5-shot 61.1% ± 4.9; GPT-3-175B 0-shot 57.7% ± 4.4, 5-shot 68.7% ± 1.5 (accuracy %).",
            "comparison_of_methods": "Some base models showed positive correlation with scale for k-shot (k≥1), but overall base models stayed much closer to random performance than example-level IT models; few-shot helps but not to human levels.",
            "key_findings": "Large-scale pretraining without example-level instruction-tuning does not produce strong pragmatic implicature understanding; scaling helps some but would require much larger sizes to match example-level IT models.",
            "counter_examples_or_negative_results": "While some base models saw improvements with scale and few-shot, they still lagged behind example-level IT models; no CoT gains reported for base models in main experiments (two base models tried for CoT did not improve).",
            "uuid": "e3336.10",
            "source_info": {
                "paper_title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 2
        },
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 2
        },
        {
            "paper_title": "Conversational implicatures in english dialogue: Annotated dataset",
            "rating": 2
        },
        {
            "paper_title": "GRICE: A grammar-based dataset for recovering implicature and conversational rEasoning",
            "rating": 1
        }
    ],
    "cost": 0.022154499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs</h1>
<p>Laura Ruis*<br>University College London<br>Akbir Khan<br>University College London<br>Stella Biderman<br>EleutherAI, Booz Allen Hamilton<br>Sara Hooker<br>Cohere for AI<br>Tim Rocktäschel<br>University College London<br>Edward Grefenstette<br>University College London</p>
<h4>Abstract</h4>
<p>Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context-incorporating its pragmatics. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response "I wore gloves" to the question "Did you leave fingerprints?" as meaning "No". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate four categories of widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), models in three of these categories perform close to random. However, LLMs instruction-tuned at the example-level perform significantly better. These results suggest that certain fine-tuning strategies are far better at inducing pragmatic understanding in models. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.</p>
<h2>1 Introduction</h2>
<p>User: "Have you seen my phone?"
GPT-3: "Yes, I have seen your phone."
GPT-3's response ${ }^{2}$ is a perfectly fine answer to the question, but a human might answer differently. They might respond "it's in your bag," bypassing the obvious follow-up question ("where is it?"). Giving such a helpful and efficient answer is an example of pragmatic language use that goes beyond the mere production of semantically plausible and consistent utterances. Meaning is not only determined by a combination of words, but also context, beliefs, and social institutions (Wittgenstein, 1953; Grice, 1975; Huang, 2017). Consider another exchange where Esther asks her friend Juan "Can you come to my party on Friday?" and Juan responds "I have to work". We resolve Juan's response as him declining the invitation by using the contextual commonsense knowledge that having to work on a Friday night precludes attendance. Both these exchanges contain an implicature-utterances that convey something other than their literal meaning. ${ }^{3}$ Implicatures illustrate how context contributes to meaning; distinguishing writing and speaking from communicating (Green, 1996). We cannot fully</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A schematic depiction of the protocol we propose to evaluate whether language models can resolve implicatures. Each example in the test set gets wrapped in templates and transformed into an incoherent example by swapping "yes" and "no". The model is said to resolve the implicature if it assigns a higher likelihood to the coherent text than the incoherent text.</p>
<p>Understand utterances without understanding their implications. Indeed, the term "communication" presupposes the speaker's implications are understood by the addressee. Being able to resolve completely novel implicatures and, more broadly, engage in pragmatic understanding constitutes an essential and ubiquitous aspect of our everyday use of language.</p>
<p>Large language models (LLMs) have demonstrated remarkable ability on a variety of downstream tasks such as planning (Huang et al., 2022), commonsense reasoning (Kojima et al., 2022), information retrieval (Lewis et al., 2020; Kim et al., 2022) and code completion (Austin et al., 2021; Biderman and Raff, 2022), to name a few. When fine-tuned with human feedback, LLMs obtain higher ratings on desiderata like helpfulness (Ouyang et al., 2022; Bai et al., 2022), and are proposed as conversational agents (Thoppilan et al., 2022). Despite the widespread use of LLMs as conversational agents, there has been limited evaluation of their ability to navigate contextual commonsense knowledge.</p>
<p>This raises an important question: to what extent can large language models resolve conversational implicature? To answer this question we use a public dataset of conversational implicatures and propose an evaluation protocol on top of it (Figure 1). We evaluate a range of state-of-the-art models that can be categorised into four groups: large-scale pre-trained models, like OPT (Zhang et al., 2022), LLMs fine-tuned on conversational data, like BlenderBot (Ng et al., 2019), LLMs fine-tuned on common NLP benchmarks with natural instructions for each benchmark, like Flan-T5 (Chung et al., 2022), and LLMs fine-tuned on tasks with natural instructions for each example, e.g., versions of OpenAI's InstructGPT-3 series<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">2</a></sup>. Our results show that implicature resolution is a challenging task for LLMs. All pre-trained models obtain close to random zero-shot accuracy (around 60%), whereas humans obtain 86%. However, our results suggest that instruction-tuning at the example level is important for pragmatic understanding. Models fine-tuned with this method perform much better than others, and analysis of different model sizes shows that they have the best scaling properties. We further push performance for these models with chain-of-thought prompting, and find that one model in the group (GPT-4) reaches human-level performance. In summary, we conclude that pragmatic understanding has not yet arisen from large-scale pre-training on its own, but scaling analysis shows that it might for much larger scale. Fine-tuning on conversational data or benchmark-level instructions does not produce models with pragmatic understanding. However, fine-tuning on instructions at the example-level is a fruitful path towards more useful models of human discourse.</p>
<p>The main contributions of this work are: i) we motivate implicature understanding as a crucial aspect of communication that is currently mostly missing from evaluations of LLMs, ii) we design an implicature resolution task and propose a comprehensive evaluation protocol on which we evaluate both humans and LLMs to find that it poses a significant challenge for SotA LLMs, and iii) we provide a thorough analysis of the results and identify one fine-tuning strategy (instruction-tuning at the example-level) as a promising method that produces models with more pragmatic understanding.</p>
<h1>2 Related Work</h1>
<p>LLMs have demonstrated remarkable performance on tasks for which they were not explicitly trained (Brown et al., 2020). Building on the hypothesis that these abilities arise due to implicit multitask learning (Radford et al., 2019), the recent works of Sanh et al. (2022) and Wei et al. (2022) explicitly train LLMs in a supervised multitask fashion, leading to models that are better zero-shot learners with fewer parameters. Besides rapidly saturating language understanding benchmarks (Kiela et al., 2021), these advancements make LLMs beneficial foundations for agents performing a plethora of tasks (Adolphs et al., 2022; Reed et al., 2022). The trend towards using these models as agents brings along with it increased urgency for alignment with human values (Kenton et al., 2021). However, larger models trained with next-word prediction are generally more toxic and unhelpful (Gehman et al., 2020; Bender et al., 2021; Lin et al., 2022). Recent work mitigates this with methods like prompting and finetuning on human-annotated outputs (Askell et al., 2021; Ouyang et al., 2022; Thoppilan et al., 2022). The produced models are more aligned on desiderata such as informativeness when evaluated by dedicated benchmarks and humans. We argue, however, that there is still something missing in these benchmarks. What is helpful and informative, as Kasirzadeh and Gabriel (2022) also point out, depends on the context in which a conversation is held. Consequently, any application that requires communicating with humans will rely on pragmatic communication skills-something that is not explicitly captured by the benchmarks used to evaluate the alignment of LLMs.
There is a large body of work that investigates the interplay between pragmatics and computational modeling (Cianflone et al., 2018; Schuster et al., 2020; Louis et al., 2020; Kim et al., 2021; Li et al., 2021; Jeretic et al., 2020; Parrish et al., 2021; Hosseini et al., 2023). Cianflone et al. (2018) introduce the task of predicting adverbial presupposition triggers, which are words like 'again' that trigger the unspoken presupposition that an event has happened before. Schuster et al. (2020) study the ability of computational models to do scalar inferences, finding that models use linguistic features to make pragmatic inferences. Kim et al. (2021) find that a substantial part of question-answering datasets contain questions that are unanswerable due to false presuppositions (i.e. "which linguist invented the lightbulb"). Hosseini et al. (2023) present a dataset for selecting entities with indirect answers, and find that language models adapted for this task get reasonable accuracy, but that there is room for improvement. The difference with this body of work and ours is that we look at the emergence of pragmatic understanding from large-scale language modeling. Jeretic et al. (2020); Parrish et al. (2021) are early works investigating the emergence of pragmatic understanding in pretrained language models, but they only look at scalar implicatures and presuppositions. Zheng et al. (2021) are the first to evaluate pretrained language models on conversational implicatures. This is important pioneering work highlighting the difficulty of implicature for language models, but their evaluations require task-specific training and the models they evaluate are relatively small. In contrast, our evaluation protocol is applicable out-of-the-box and is much more comprehensive, evaluating models up to 176 billion parameters and using in-context prompting. Additionally, Zheng et al. (2021) benchmark synthetic data whereas this work evaluates performance on naturally occurring implicatures (George and Mamidi, 2020). We believe this to be a better representation of the true distribution of implicatures in natural dialogue.
The standard set of benchmarks LLMs are evaluated on covers many tasks, but even though implicature is one of the most important aspects of language pragmatics (Levinson, 1983), it is only evaluated as part of BIG-bench (Srivastava et al., 2022). Unfortunately, the methodology used by the BIG-bench implicature task contributors has limitations, which call into question the validity of their claims. Firstly, the task contributors discard a subset of the data that is ambiguous according to them. In our view this defeats the point of the benchmark. Implicatures are a type of non-literal, ambiguous language the intended meaning of which humans often easily interpret; comparing the way humans and models do this is precisely what we are interested in. In turn, we expect performance on the BIG-bench task to overestimate the ability of LLMs to resolve naturally occurring implicatures. We keep this challenging subset of the data and instead use human evaluation to deal with examples that are too ambiguous to understand. Secondly, the difference in performance between their average and best rater is $18 \%$, whereas for our evaluations this difference is $6 \%$. This indicates their human evaluation is of low quality, but it is impossible to verify because there are no details available on how the annotation is done. Finally, BIG-bench uses only base LLMs and no SotA fine-tuning methods. In summary, we use a more challenging dataset, and in turn at least six times more evaluations per model, we provide higher-quality human annotations, and evaluate four different categories of LLMs to investigate which aspects of LLMs contribute to their performance on implicature understanding.</p>
<h1>3 Evaluation Protocol</h1>
<p>Here we outline the evaluation protocol we use to answer the research question "To what extent can LLMs resolve conversational implicature?". We focus on binary implicatures that imply "yes" or "no" (see Figure 1). We say a model resolves an implicature correctly if it assigns higher likelihood to a coherent utterance than a similar but incoherent one, detailed below.</p>
<p>Zero-shot evaluation. Consider the example from the introduction packed into a single utterance:
Esther asked "Can you come to my party on Friday?" and Juan responded "I have to work", which means no.</p>
<p>We can transform this example to be pragmatically incoherent (in the sense that it will become pragmatically inconsistent with expected use) by replacing the word "no" with "yes":</p>
<p>Esther asked "Can you come to my party on Friday?" and Juan responded "I have to work", which means yes.</p>
<p>To resolve the implicature, the model should assign higher likelihood to the first of the two sentences above, namely the most coherent one. Importantly, both sentences have exactly the same words except for the binary implicature "yes" or "no", making the assigned likelihood scores directly comparable. Formally, let the coherent prompt be $\mathbf{x}$ and the augmented, incoherent prompt be $\tilde{\mathbf{x}}$. A model outputs a likelihood $p$ parameterized by weights $\theta$. We say a model correctly resolves an example $\mathbf{x}$ when it assigns $p_{\theta}(\mathbf{x})&gt;p_{\theta}(\tilde{\mathbf{x}})$. This is equivalent to evaluating whether the model assigns a higher likelihood to the correct continuation of the two options. Note that this is a more lenient evaluation protocol than sometimes used for language models, where models are evaluated on on their ability to generate the correct continuation, in this case "no". The greedy decoding approach (evaluating whether "yes" or "no" is generated) is also captured by our approach, but we additionally label an example correct if "no" is not the highest assigned likelihood, but still higher than "yes". We did not opt for greedy decoding because "no" is not the only coherent continuation here, and marginalising over all possible correct continuations is intractable. The more lenient evaluation does capture implicature resolution, because the choice of "no" versus "yes" is only determined by the resolution of the implicature. We guide the models to output "yes" or "no" explicitly in three of the six prompt templates with instructions, such that we can estimate the effect of this guidance on performance. For two model classes (i.e. GPT-3.5-turbo and GPT-4) we do not have access to likelihoods, and for these models we take the greedy decoding approach, guiding the model to output "yes" or "no" explicitly in all prompts (see Table 6 in Appendix F).
We use a dataset of conversational implicatures curated by George and Mamidi (2020) ${ }^{5}$. It contains implicatures that, like in Figure 1, are presented in utterance-response-implicature tuples. Of these, 718 are binary implicatures that we can convert into an incoherent sentence. We randomly sample 600 examples for the test set and keep the remaining 118 as a development set to improve implicature resolution after pre-training through in-context prompting or fine-tuning.
Few-shot in-context evaluation. We add $k$ examples of the task to the prompt, e.g. with $k=2$ :
Esther asked "Have you found him yet?" and Juan responded "They're still looking", which means no.
Esther asked "Are you having fun?" and Juan responded "Is the pope Catholic?", which means yes.
Finish the following sentence:
Esther asked "Can you come to my party on Friday?" and Juan responded "I have to work", which means no.</p>
<p>We evaluate the models' $k$-shot capabilities for $k \in{1,5,10,15,30}$ by randomly sampling $k$ examples from the development set for each test example. We opt for a random sampling approach to control for two sources of randomness. Firstly, examples have different levels of informativeness. Secondly, recent work found that the order in which examples are presented matters ( Lu et al., 2022). Ideally, to marginalise over these random factors, we would evaluate each test example with all</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>permutations of $k$ examples from the development set. This requires $\frac{118!}{118-k!!}$ evaluations for each test example, which is intractable. Instead, we estimate performance per test example by randomly sampling from the development set. In this way we control for some of the variance in performance, but avoid extra evaluations. We ensure each model sees the same few-shot examples per test example.</p>
<p>Controlling for prompt sensitivity. It has been shown language models are sensitive to prompt wording (Efrat and Levy, 2020; Tan et al., 2021; Reynolds and McDonell, 2021a; Webson and Pavlick, 2021). To control for this factor of randomness we manually curate six different template prompts and measure performance across these. One of the templates has been presented above, namely "Esther asked <utterance> and Juan responded <response>, which means <implicature>". Another template is: "Question: <utterance>, response: <response>, meaning: <implicature>". The former we call natural prompts and the latter structured prompts. Each group has three templates that only differ slightly in wording. This grouping allows us to look at the variance due to slight changes in wording as well as performance difference due to a completely different way of presenting the example. The full list of prompts can be found in Appendix F.</p>
<h1>4 Experiments</h1>
<p>The set of large language model classes we evaluate can be grouped into four distinct categories:</p>
<ol>
<li>Base models: large-scale pre-trained models; RoBERTa (Liu et al., 2019), BERT (Devlin et al., 2018), GPT-2 (Radford et al., 2019), EleutherAI (Wang and Komatsuzaki, 2021; Black et al., 2022), BLOOM (BigScience, 2022), OPT (Zhang et al., 2022), Cohere’s base models, and GPT-3 (Brown et al., 2020)</li>
<li>Dialogue FT: LLMs fine-tuned on dialogue, BlenderBot (Ng et al., 2019).</li>
<li>Benchmark IT: LLMs fine-tuned on tasks with natural instructions for each benchmark or "benchmark-level instruction-tuned models"; T0 (Sanh et al., 2022) and Flan-T5 (Chung et al., 2022).</li>
<li>Example IT: LLMs fine-tuned on tasks with natural instructions for each example or "example-level instruction-tuned models"; a subset of OpenAI's API models and Cohere's API models).</li>
</ol>
<p>For Benchmark IT models, annotators write a single instruction for an entire dataset. The models are then fine-tuned on each example from the dataset with the same instruction. We distinguish this from example-level IT; for that type of fine-tuning each example in a dataset gets a new instruction, resulting in a more diverse dataset. Each group contains model classes for which we evaluate a range of sizes. A detailed categorization of the models and their attributes can be found in appendix G. ${ }^{6}$ We make use of the OpenAI and Cohere APIs as well as the pretrained models in the transformers library (Wolf et al., 2020) and EleutherAI's framework to evaluate them (Gao et al., 2021). All code used for this paper can be found on GitHub ${ }^{7}$ and the dataset is made publicly available on HuggingFace ${ }^{8}$. Below, we present zero-shot and few-shot results, discussing patterns of performance of the models in the four different groups. We further look at the results for different model sizes of each model class and the variance over the prompt templates. We contrast the models' performance with human performance. To this end, each test example gets annotated by five humans. We split the test set in four and assign each annotator a subset, leaving us with twenty annotators in total. The average human performance is $86.2 \%$, and the best performance is $92 \%$. Some of the errors humans make uncover examples that have multiple interpretations, and others uncover annotation errors. The nature of the task of implicature resolution means we do not expect models to perform better than human best performance. Details on the human experiment can be found in the Appendix H (also containing an analysis of human errors), and detailed results per model and prompt template in Appendix K.10. We also test for spurious correlations present in the benchmark (like lexical cues the model can rely on), and find no indication (Appendix K.8).</p>
<p>Insight 1: Models instruction-tuned at the example level outperform all others. Table 1 shows the best 0 -, 1-, and 5 -shot accuracy each model class achieved on the implicature task. The best overall</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: The k-shot accuracy $(k\in{0,1,5})$ for the best performing model of each class. For each model, we select the model size to show by choosing the one that achieves the best 5-shot performance. The std is over prompt templates for the models and over annotators for humans. FT stands for fine-tuning and IT for instruction-tuning. We find that the models in the Example IT class obtain significantly higher performance than all others. $\star$ means size unknown.</p>
<table>
<thead>
<tr>
<th></th>
<th>Model</th>
<th>0-shot</th>
<th>1-shot</th>
<th>5-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baselines and Toplines</td>
<td>Random</td>
<td>$50\%$</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Human avg.</td>
<td>$86.2 \% \pm 2.3$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Base models</td>
<td>BERT-110M</td>
<td>$54.8 \% \pm 1.6$</td>
<td>$51.7 \% \pm 1.7$</td>
<td>$53.3 \% \pm 2.2$</td>
</tr>
<tr>
<td></td>
<td>RoBERTa-355M</td>
<td>$55.6 \% \pm 2.0$</td>
<td>$54.1 \% \pm 0.9$</td>
<td>$57.1 \% \pm 1.5$</td>
</tr>
<tr>
<td></td>
<td>GPT-2-xl</td>
<td>$51.3 \% \pm 2.9$</td>
<td>$57.4 \% \pm 3.3$</td>
<td>$57.7 \% \pm 1.1$</td>
</tr>
<tr>
<td></td>
<td>EleutherAI-20B</td>
<td>$57.5 \% \pm 3.3$</td>
<td>$55.9 \% \pm 2.3$</td>
<td>$61.1 \% \pm 4.9$</td>
</tr>
<tr>
<td></td>
<td>BLOOM-176B</td>
<td>$54.2 \% \pm 1.2$</td>
<td>$61.1 \% \pm 2.7$</td>
<td>$65.4 \% \pm 3.4$</td>
</tr>
<tr>
<td></td>
<td>OPT-13B</td>
<td>$61.0 \% \pm 5.5$</td>
<td>$60.6 \% \pm 2.7$</td>
<td>$67.4 \% \pm 2.1$</td>
</tr>
<tr>
<td></td>
<td>Cohere-52B</td>
<td>$58.5 \% \pm 4.0$</td>
<td>$63.0 \% \pm 3.8$</td>
<td>$65.1 \% \pm 2.9$</td>
</tr>
<tr>
<td></td>
<td>GPT-3-175B</td>
<td>$57.7 \% \pm 4.4$</td>
<td>$65.7 \% \pm 1.4$</td>
<td>$68.7 \% \pm 1.5$</td>
</tr>
<tr>
<td>Dialogue FT</td>
<td>BlenderBot-2.7B</td>
<td>$53.4 \% \pm 0.3$</td>
<td>$53.3 \% \pm 0.1$</td>
<td>$53.3 \% \pm 0.1$</td>
</tr>
<tr>
<td>Benchmark IT</td>
<td>T0-11B</td>
<td>$55.6 \% \pm 7.0$</td>
<td>$47.8 \% \pm 0.5$</td>
<td>$47.0 \% \pm 0.2$</td>
</tr>
<tr>
<td></td>
<td>Flan-T5-11B</td>
<td>$60.8 \% \pm 2.4$</td>
<td>$57.4 \% \pm 5.0$</td>
<td>$61.7 \% \pm 4.8$</td>
</tr>
<tr>
<td>Example IT</td>
<td>text-davinci-001-*</td>
<td>$72.3 \% \pm 2.8$</td>
<td>$72.7 \% \pm 1.3$</td>
<td>$74.5 \% \pm 1.0$</td>
</tr>
<tr>
<td></td>
<td>text-davinci-002-*</td>
<td>$70.6 \% \pm 2.3$</td>
<td>$75.6 \% \pm 2.8$</td>
<td>$79.6 \% \pm 2.0$</td>
</tr>
<tr>
<td></td>
<td>text-davinci-003-*</td>
<td>$71.2 \% \pm 2.8$</td>
<td>$74.3 \% \pm 1.4$</td>
<td>$79.7 \% \pm 0.6$</td>
</tr>
<tr>
<td></td>
<td>ChatGPT-*</td>
<td>$72.1 \% \pm 5.9$</td>
<td>$75.1 \% \pm 1.5$</td>
<td>$73.9 \% \pm 6.3$</td>
</tr>
<tr>
<td></td>
<td>GPT-4-*</td>
<td>$81.8 \% \pm 1.8$</td>
<td>$82.3 \% \pm 1.4$</td>
<td>$82.0 \% \pm 1.7$</td>
</tr>
<tr>
<td></td>
<td>Cohere-command-52B</td>
<td>$60.2 \% \pm 5.2$</td>
<td>$72.8 \% \pm 1.3$</td>
<td>$75.4 \% \pm 1.8$</td>
</tr>
</tbody>
</table>
<p>accuracy is achieved by GPT-4 (the size of this model is unknown) at $82.3 \% \pm 1.4$. This leaves a gap of $3.9 \%$ with human average performance. All models in the class Example IT perform significantly better than any of the other models for all $k$, except Cohere-command-52b at 0 -shot. This result is more clearly seen in Figure 2, where we present the average accuracy for each model group. The performance for the other model classes across $k$ ranges from $47.0 \%$ by BlenderBot-2.7b at $k=5$ and $68.7 \%$ by GPT-3-175b at $k=5$. Even though base models benefit from few-shot examples, their performance remains mostly closer to random than to humans for all $k$, showing a gap of at least $17.5 \%$ with the average human. We observe a decrease in performance for $k&gt;0$ for the group Benchmark IT. This is not surprising, as these kind of models are specifically fine-tuned to be better at zero-shot generalisation (Sanh et al., 2022; Chung et al., 2022). BlenderBot, in the group Dialogue $F T$, performs barely better than random for all $k$. We hypothesise that the lower performance which Cohere-command-52b achieves 0 -shot is not due to a lack of implicature understanding, but due to a failure to calibrate the yes/no likelihoods without examples. For this model, we observe a sharp rise in performance from $k=0$ to $k=1$ (see Table 1 or Figure 2). Since it is unlikely that one example of an implicature induces pragmatic understanding, we hypothesise that few-shot prompting mostly serves to clarify the task format. We test this hypothesis in Appendix K. 6 by repeating the 1- and 5-shot experiment with random labels for Cohere-command-52B and text-davinci-001. We find that the performance does not degrade, which confirms that the few-shot examples mainly serve to prime the model towards producing outputs following the yes/no structure.</p>
<p>Insight 2: The results are robust to different prompt templates. As detailed in Section 3, each example in the test set is wrapped in six different prompt templates. The standard deviation in Table 1 and in Figure 2 shows the sensitivity to different prompt wording. The standard deviation ranges from 0.3 for BlenderBot to 7.0 for T0-11B. All in all, the sensitivity to prompt wording does not seem to be a problem for this task; when taking into account the confidence intervals the result remains that models in the group Example IT perform significantly better than all other models, but worse than humans. In Appendix K. 4 another analysis is presented that shows how different prompt templates benefit from in-context examples. The takeaway from the analysis is that in-context prompting can mitigate the fact that some models are better at natural prompts and others better at structured prompts by improving performance on the type of prompt the model struggles with zero-shot. Again, when</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The few-shot accuracy for the best model of each class (e.g. the best performing model in the class Cohere-command is the 52b model, whereas the best model in the class OPT is the 13b model). The bars show the group means. Models fine-tuned on example-level instructions perform better than most other models, especially for $k&gt;0$. For all models there is a significant gap between best accuracy and human accuracy (which is $86.2 \%$ ). * means size unknown.</p>
<p>Table 2: Scaling results for OpenAI's text-<engine>-001-series, for which we do not know the number of non-embedding parameters but do know the ordering in terms of size. The colors indicate whether going up in size (from left-to-right) increases performance significantly or not.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Engine</th>
<th style="text-align: center;">Ada</th>
<th style="text-align: center;">Babbage</th>
<th style="text-align: center;">Curie</th>
<th style="text-align: center;">Davinci</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0-shot</td>
<td style="text-align: center;">$56.5 \% \pm 5.8$</td>
<td style="text-align: center;">$64.5 \% \pm 1.8(+8.0 \%)$</td>
<td style="text-align: center;">$69.0 \% \pm 2.9(+4.5 \%)$</td>
<td style="text-align: center;">$72.3 \% \pm 2.8(+3.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">5-shot</td>
<td style="text-align: center;">$57.6 \% \pm 2.8$</td>
<td style="text-align: center;">$66.1 \% \pm 0.3(+8.5 \%)$</td>
<td style="text-align: center;">$71.3 \% \pm 1.3(+5.2 \%)$</td>
<td style="text-align: center;">$74.5 \% \pm 1.0(+4.0 \%)$</td>
</tr>
</tbody>
</table>
<p>only looking at the best prompt type for each model class (i.e. structured or natural), the results remain that models in the group Example IT perform best.</p>
<p>Insight 3: Models instruction-tuned at the example-level have the most favourable scaling properties, but some base models also show positive correlation with scale. Figure 3 shows the scaling behaviour of the model classes for which we know the number of non-embedding parameters. We highlight 0 - and 5 -shot results, because for $k&gt;5$ the accuracy of most models plateaus (see Figure 2). However, detailed results for other $k$ can be found in Appendix K.10. Note that we do not know the number of parameters for OpenAI's 'text-<engine>-001'-series, but we do know the order of the engines in size, and we separately present its scaling results in Table 2. Except OpenAI's 'text-<engine>-001'-series, none of the models show significant performance increase with model size for the 0 -shot evaluations. However, for $k$-shot evaluations with $k \geq 1$ we observe significant positive correlation with size for the models in the Example IT class for which we have multiple sizes (Cohere-command and 'text-<engine>-001') as well as some models in the base model class. Not only do the models in the Example IT class exhibit higher performance for the same model size, these models also have a steeper performance increase with size than the base models. Comparing the scaling properties of the best base model (GPT-3) with Cohere-command, we see that the increase in performance from the second-largest to the largest model is $0.04 \%$ per billion parameters from GPT-3-6.7B to GPT-3-175B and $0.15 \%$ per billion parameters for Cohere-command-6B to Cohere-command-52b (exact numbers used to calculate the slope can be found in Appendix K.10). If performance is linearly extrapolated from this curve GPT-3 reaches human-level performance at 642b parameters where Cohere-command would need 125b parameters.
Insight 4: GPT-4 reaches average human-level performance with chain-of-thought prompting. For the model groups that benefit from in-context examples, we attempt to push performance further</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Scaling results for the model classes of which we know the number of non-embedding parameters. The error bars show standard deviation over prompt templates. Cohere's command models instruction-tuned at the example-level perform better than all other models. For all models there is still a significant gap between best accuracy and human accuracy.</p>
<p>Table 3: Results of the chain-of-thought (CoT) experiment for models in the group Example IT. The numbers between brackets show the difference in performance with the number on the same row one column to the left. Most models benefit from CoT-prompting, but not all. Additionally, GPT-4 reaches average human-level performance with CoT prompting. $\star$ means size unknown.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>$\mathbf{0}$-shot</th>
<th>$\mathbf{5}$-shot</th>
<th>$\mathbf{5}$-shot CoT</th>
</tr>
</thead>
<tbody>
<tr>
<td>text-davinci-001- $\star$</td>
<td>$72.3 \% \pm 2.8$</td>
<td>$74.5 \% \pm 1.0(+2.2 \%)$</td>
<td>$67.3 \% \pm 2.6(-7.2 \%)$</td>
</tr>
<tr>
<td>text-davinci-002- $\star$</td>
<td>$70.6 \% \pm 2.3$</td>
<td>$79.6 \% \pm 2.0(+9.0 \%)$</td>
<td>$80.1 \% \pm 0.8(+0.5 \%)$</td>
</tr>
<tr>
<td>text-davinci-003- $\star$</td>
<td>$71.2 \% \pm 2.8$</td>
<td>$79.7 \% \pm 0.6(+8.5 \%)$</td>
<td>$83.6 \% \pm 0.6(+4.0 \%)$</td>
</tr>
<tr>
<td>ChatGPT- $\star$</td>
<td>$72.1 \% \pm 6.0$</td>
<td>$73.9 \% \pm 6.3(+1.8 \%)$</td>
<td>$77.2 \% \pm 1.0(+3.3 \%)$</td>
</tr>
<tr>
<td>GPT-4- $\star$</td>
<td>$81.8 \% \pm 1.8$</td>
<td>$82.0 \% \pm 1.7(+0.2 \%)$</td>
<td>$\mathbf{8 6 . 5 \%} \pm \mathbf{1 . 0}(+4.5 \%)$</td>
</tr>
<tr>
<td>Cohere-command-52b</td>
<td>$60.2 \% \pm 5.2$</td>
<td>$75.4 \% \pm 1.8(+15.2 \%)$</td>
<td>$75.3 \% \pm 0.5(-0.1 \%)$</td>
</tr>
</tbody>
</table>
<p>with chain-of-thought prompting. We manually write a five-shot chain-of-thought prompt for all six prompt templates, and evaluate model performance using this prompt. One of the six chain-of-thought prompts can be found in Table 4 in Appendix F, and the other five are provided in the supplementary material. We only present the results for the group Example IT here, since CoT prompting did not improve performance for two of the base model classes we tried (see Appendix K.7). Consequently, we decided not to apply this experiment to the other models in the base group to save compute costs. The results of are shown in Table 3. We find that chain-of-thought prompting does not help for all models, but is nonetheless able to boost performance of GPT-4 to $86.5 \% \pm 1.0$. This is on-par with average human-level performance, and slightly below human best performance at $89.8 \%$. To illustrate how explicit reasoning helps implicature understanding, we highlight a CoT generated by GPT-4 for an example from the dataset that models persistently get wrong. "A: Is there a bus I can get to the station? B: You can't rely on it". The implicature is yes, there is a bus, you just cannot rely on it. GPT-4 five-shot gets this wrong for all six templates. With CoT it gets it right for five of six templates. The generated CoT for one template is the following:</p>
<p>Alice says 'You can't rely on it.' Alice must be implying that there is a bus, but it may not be dependable or timely. This means the response to Bob's question is yes, but with a caution about reliability. Answer: yes</p>
<p>More completions can be found in Appendix J.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The accuracy v. k for the generalised and particularised examples obtained by the Example IT models Cohere-command and GPT-4. Particularised (context-heavy) examples are often significantly more difficult than generalised (context-free) examples for both models and humans. For most models, in-context prompting can mitigate this, but for others (like GPT-4), a significant gap remains. We see that Cohere-command-52b achieves similar performance as GPT-4 on the particularised examples, but significantly lower on the generalised examples.</p>
<p>Table 4: An example from the dataset for two types of implicature found in the test set. The rightmost column shows the amount of that type we manually found in the test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Example Utterance</th>
<th style="text-align: left;">Example Response</th>
<th style="text-align: left;">Impl.</th>
<th style="text-align: center;">$#$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Generalised</td>
<td style="text-align: left;">You know all these people?</td>
<td style="text-align: left;">Some.</td>
<td style="text-align: left;">No.</td>
<td style="text-align: center;">47</td>
</tr>
<tr>
<td style="text-align: left;">Particularised</td>
<td style="text-align: left;">Want to stay for a nightcap?</td>
<td style="text-align: left;">I've gotta get up early.</td>
<td style="text-align: left;">No.</td>
<td style="text-align: center;">94</td>
</tr>
</tbody>
</table>
<p>Insight 5: Models often struggle with the same type of examples humans struggle with. We manually labeled 217 examples of the 600 examples in the test set according to a taxonomy. The remaining 383 examples do not fall as clearly within a category and are grouped together as type other. In Table 4 the two types of examples that occur frequently in the dataset are exemplified. Generalised implicatures require little or no context to be understood. They are the simplest type of example in the test set, and generally imply the same thing ("some" almost always implies "not all"). Particularised implicatures, by contrast, do require context to be resolved. For example, from Table 4 , we need the context that it is undesirable to stay up late drinking when one has to get up early (see in Appendix E for more on generalised vs. particularised). In these type of examples, the context needed to resolve it is different every time. We label three other types of implicatures in the dataset, but since the analysis of these examples does not show significant patterns, we present it in Appendix K.9. We show the accuracy broken down per example type for two models from the Example IT group, as these patterns hold more broadly for almost all models evaluated (see the detailed results broken down per example type in Appendix K.9). Figure 4 shows that for lower $k$, the models often have a significantly worse performance for particularised examples than for generalised examples, just like humans do. For some, like Cohere-command-52b, this is mitigated by few-shot prompting, which brings particularised and generalised performance closer together (sometimes at the cost of generalised performance). For others, like GPT-4, the gap between particularised and generalised performance remains large for all $k$. From the bottom row in Figure 4 we observe that the edge GPT-4 has over Cohere-command-52b seems mostly driven by a higher accuracy on generalised examples. The accuracy on the particularised examples is comparable between those two models.</p>
<h1>5 Discussion</h1>
<p>In this study we use prompting to evaluate whether different groups of LLMs can resolve implicatures. In designing our experimental protocol, we carefully considered various alternatives, and here we discuss limitations of the chosen approach. Firstly, evaluating LLM competencies is inherently uncertain and sensitive to prompt choice. Nonetheless, we are confident our evaluation is comprehensive enough to assess implicature understanding: we apply six different prompt templates per test example, each used in three different prompting techniques (zero-shot, few-shot, chain-of-thought). Addition-</p>
<p>ally, in the appendix we present alternative zero-shot prompts and task specifications (Appendix K. 3 and K. 1 respectively), but since these did not improve performance they were not further considered. Another limitation is the fact that a subset of the models we evaluate are behind APIs. This means models are subject to change (affecting reproducibility) and certain details about these models are unknown. This affects the group instruction-tuned at the example-level, which is the group we find outperforms all others and has the most favourable scaling properties. How do we know instructiontuning at the example-level is the main driver behind these findings without controlled A/B testing? Unfortunately, due to the secrecy surrounding the exact implementation of these models we cannot be certain, but we can be relatively confident. We evaluated ten models across six model classes and two APIs in the group example-level instruction tuned. Within this group, models probably vary significantly in other training and architecture details (especially Cohere-command models versus OpenAI models). The most salient commonality they share with each other and none of the other models is multi-task instruction-tuning at the example level, making it likely that this is the driving factor of their performance. A further datapoint in favour of this conclusion can be seen in Figure 3 (right); base models at similar scales as Example IT models perform significantly worse. We see that Cohere-command 52B significantly outperforms Cohere-base 52B, and the only difference between those models is instruction-tuning at the example level (Cohere-command is fine-tuned from Cohere-base). In fact, Cohere-command 52B outperforms other base models more than 3 times the size by a large margin (e.g. GPT-3 175B, BLOOM-176B, OPT-175B). We are therefore confident that instruction-tuning at the example-level is important for pragmatic understanding, an insight which can guide the development of open-source models capable of pragmatic understanding. Investigating the exact effect of this type of instruction-tuning on pragmatic understanding in a controlled setting is an interesting future work direction (e.g. by isolating the effect of data diversity from instructions). Another limitation is that some evaluations are subject to API stochasticity, which we address in Appendix K.5. After running the zero-shot experiment ten times through each API we conclude there is some stochasticity, but it is too small to impact our conclusions. We publish exact timestamps at which we queried APIs in Appendix L. Further, a downside of doing a comprehensive analysis on many models is compute costs. In Appendix M we publish a list of exact compute used (time and hardware), as well as estimated carbon emissions for each of the models that are not behind an API. Finally, the likelihood ranking approach we take limits our study to implicatures with clear alternative. However, implicatures in natural language can entail more complex propositions. For example, imagine Esther now asking "Can I use your stapler?" and Juan responding "Here's the key to my office.". Juan is implicating that (1) Esther can use the stapler, (2) the stapler is located in the office, and (3) the office is currently locked. This leaves ample room for the design of benchmarks with implicatures entailing multiple non-binary propositions.</p>
<h1>6 Conclusion</h1>
<p>LLMs have made remarkable progress on fluency and coherence in recent years. We argue however that a central aspect of language understanding is missing from evaluations. To understand language means to understand its pragmatics: its usage in a context that incorporates commonsense understanding, goals, objectives, and so on. We design a protocol that evaluates LLMs on binary implicature resolution and establish a significant gap with human understanding for SotA LLMs in three categories; large-scale pre-trained models, models fine-tuned on conversations, and models fine-tuned with benchmark-level instructions. By contrast, we find that models fine-tuned on example-level instructions perform significantly better. This group also exhibits the best correlation between accuracy and model size. Scaling analysis shows that for some large-scale pre-trained models accuracy also positively correlates with model size, but the best model in this group would need at least five times more parameters to reach similar performance. From these results, we conclude that instruction-tuning at the example level is important for pragmatic understanding. We hypothesise that there is something about the multi-task data diversity obtained from example-level instructions (i.e. each example a new task) that makes pragmatic understanding appear at smaller scale.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank members of the UCL DARK lab, members of the UCL NLP group, Pontus Stenetorp, Sebastian Borgeaud, Philipp Jettkant, Robert Kirk, and Max Bartolo for fruitful discussions and comments on an earlier draft of this paper. We would also like to thank the anonymous reviewers for engaging actively in discussion and providing feedback that has improved this work. This work was supported by the EPSRC Grant EP/S021566/1 and UCL International Scholar Award for Doctoral Training Centres.</p>
<h1>References</h1>
<p>Adolphs, L., Börschinger, B., Buck, C., Huebscher, M. C., Ciaramita, M., Espeholt, L., Hofmann, T., Kilcher, Y., Rothe, S., Sessa, P. G., and Sestorain, L. (2022). Boosting search engines with interactive agents. Transactions on Machine Learning Research.</p>
<p>American Psychiatric Association, A. P. A. (2013). Diagnostic and statistical manual of mental disorders : DSM-5. American Psychiatric Association Arlington, VA, 5th ed. edition.</p>
<p>Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. (2021). A general language assistant as a laboratory for alignment. CoRR, abs/2112.00861.</p>
<p>Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. (2021). Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>Bach, K. (1999). The myth of conventional implicature. Linguistics and Philosophy, 22(4):327-366.
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623.</p>
<p>Biderman, S. and Raff, E. (2022). Fooling moss detection with pretrained language models. arXiv preprint arXiv:2201.07406.</p>
<p>BigScience (2022). Bigscience language open-science open-access multilingual (bloom) language model.</p>
<p>Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L., Tow, J., Wang, B., and Weinbach, S. (2022). GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Workshop on Challenges \&amp; Perspectives in Creating Large Language Models.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. (2022). Scaling instruction-finetuned language models.</p>
<p>Cianflone, A., Feng, Y., Kabbara, J., and Cheung, J. C. K. (2018). Let's do it "again": A first computational approach to detecting adverbial presupposition triggers. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2747-2755, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Davis, W. (2019). Implicature. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, fall 2019 edition.</p>
<p>Davis, W. A. (1998). Implicature : intention, convention, and principle in the failure of Gricean theory / Wayne A. Davis. Cambridge studies in philosophy. Cambridge University Press, Cambridge England ; New York.</p>
<p>Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018). BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.</p>
<p>Efrat, A. and Levy, O. (2020). The turking test: Can language models understand instructions? CoRR, abs/2010.11982.</p>
<p>Frank, M. C. and Goodman, N. D. (2012). Predicting pragmatic reasoning in language games. Science, 336:998 - 998.</p>
<p>Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for few-shot language model evaluation.</p>
<p>Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. (2020). RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356-3369, Online. Association for Computational Linguistics.</p>
<p>George, E. J. and Mamidi, R. (2020). Conversational implicatures in english dialogue: Annotated dataset. Procedia Computer Science, 171:2316-2323. https://doi.org/10.1016/j.procs.2020.04.251.</p>
<p>Glaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., Campbell-Gillingham, L., Uesato, J., Huang, P.-S., Comanescu, R., Yang, F., See, A., Dathathri, S., Greig, R., Chen, C., Fritz, D., Sanchez Elias, J., Green, R., Mokrá, S., Fernando, N., Wu, B., Foley, R., Young, S., Gabriel, I., Isaac, W., Mellor, J., Hassabis, D., Kavukcuoglu, K., Hendricks, L. A., and Irving, G. (2022). Improving alignment of dialogue agents via targeted human judgements.</p>
<p>Goodman, N. and Frank, M. (2016). Pragmatic language interpretation as probabilistic inference. Trends in Cognitive Sciences, 20.</p>
<p>Green, G. (1996). Pragmatics and Natural Language Understanding. Tutorial essays in cognitive science. Erlbaum.</p>
<p>Greene, S. and Resnik, P. (2009). More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 503-511, Boulder, Colorado. Association for Computational Linguistics.</p>
<p>Grice, H. P. (1975). Logic and conversation. In Cole, P. and Morgan, J. L., editors, Syntax and Semantics: Vol. 3: Speech Acts, pages 41-58. Academic Press, New York.</p>
<p>Hosseini, M. J., Radlinski, F., Pareti, S., and Louis, A. (2023). Resolving indirect referring expressions for entity selection. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12313-12335, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. (2022). Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207.</p>
<p>Huang, Y. (2017). The Oxford Handbook of Pragmatics. Oxford handbooks in linguistics. Oxford University Press.</p>
<p>Jeretic, P., Warstadt, A., Bhooshan, S., and Williams, A. (2020). Are natural language inference models IMPPRESsive? Learning IMPlicature and PRESupposition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8690-8705, Online. Association for Computational Linguistics.</p>
<p>Kasirzadeh, A. and Gabriel, I. (2022). In conversation with artificial intelligence: aligning language models with human values.</p>
<p>Katsos, N., Roqueta, C. A., Estevan, R. A. C., and Cummins, C. (2011). Are children with specific language impairment competent with the pragmatics and logic of quantification? Cognition, $119(1): 43-57$.</p>
<p>Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., and Irving, G. (2021). Alignment of language agents. CoRR, abs/2103.14659.</p>
<p>Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., Ma, Z., Thrush, T., Riedel, S., Waseem, Z., Stenetorp, P., Jia, R., Bansal, M., Potts, C., and Williams, A. (2021). Dynabench: Rethinking benchmarking in nlp. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110-4124, Online. Association for Computational Linguistics.</p>
<p>Kim, N., Pavlick, E., Karagol Ayan, B., and Ramachandran, D. (2021). Which linguist invented the lightbulb? presupposition verification for question-answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3932-3945, Online. Association for Computational Linguistics.</p>
<p>Kim, S. Y., Park, H., Shin, K., and Kim, K.-M. (2022). Ask me what you need: Product retrieval using knowledge from gpt-3. arXiv preprint arXiv:2207.02516.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models are zero-shot reasoners.</p>
<p>Lepore, E. and Stone, M. (2014). Imagination and Convention: Distinguishing Grammar and Inference in Language. Oxford University Press.</p>
<p>Levinson, S. C. (1983). Pragmatics. Cambridge University Press, Cambridge, U.K.
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Li, E., Schuster, S., and Degen, J. (2021). Predicting scalar inferences from "or" to "not both" using neural sentence encoders. In Proceedings of the Society for Computation in Linguistics 2021, pages 446-450, Online. Association for Computational Linguistics.</p>
<p>Lin, S., Hilton, J., and Evans, O. (2022). TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Louis, A., Roth, D., and Radlinski, F. (2020). "I'd rather just go to bed": Understanding indirect answers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7411-7425, Online. Association for Computational Linguistics.</p>
<p>Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. (2022). Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In ACL.</p>
<p>Ng, N., Yee, K., Baevski, A., Ott, M., Auli, M., and Edunov, S. (2019). Facebook FAIR's WMT19 news translation task submission. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 314-319, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. J. (2022). Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Parrish, A., Schuster, S., Warstadt, A., Agha, O., Lee, S.-H., Zhao, Z., Bowman, S. R., and Linzen, T. (2021). NOPE: A corpus of naturally-occurring presuppositions in English. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 349-366, Online. Association for Computational Linguistics.</p>
<p>Patel, R. and Pavlick, E. (2021). "was it "stated" or was it "claimed"?: How linguistic bias affects generative language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10080-10095, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Potts, C. (2005). The Logic of Conventional Implicatures. Oxford University Press UK.
Potts, C. (2006). Conversational implicatures via general pragmatic pressures. In Washio, T., Satoh, K., Takeda, H., and Inokuchi, A., editors, New Frontiers in Artificial Intelligence, pages 205-218, Berlin, Heidelberg. Springer Berlin Heidelberg.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are unsupervised multitask learners.</p>
<p>Recasens, M., Danescu-Niculescu-Mizil, C., and Jurafsky, D. (2013). Linguistic models for analyzing and detecting biased language. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1650-1659, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., and de Freitas, N. (2022). A generalist agent.</p>
<p>Reynolds, L. and McDonell, K. (2021a). Prompt programming for large language models: Beyond the few-shot paradigm.</p>
<p>Reynolds, L. and McDonell, K. (2021b). Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA '21, New York, NY, USA. Association for Computing Machinery.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablam, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf, T., and Rush, A. M. (2022). Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Schaeken, W., Van Haeren, M., and Bambini, V. (2018). The understanding of scalar implicatures in children with autism spectrum disorder: Dichotomized responses to violations of informativeness. Frontiers in Psychology, 9.</p>
<p>Schuster, S., Chen, Y., and Degen, J. (2020). Harnessing the linguistic signal to predict scalar inferences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5387-5403, Online. Association for Computational Linguistics.</p>
<p>Sperber, D. and Wilson, D. (1986). Relevance: Communication and Cognition. Language and thought series. Harvard University Press.</p>
<p>Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., Xiang, A., Parrish, A., Nie, A., Hussain, A., Askell, A., Dsouza, A., Slone, A., Rahane, A., Iyer, A. S., Andreassen, A., Madotto, A., Santilli, A., Stuhlmüller, A., Dai, A., La, A., Lampinen, A., Zou, A., Jiang, A., Chen, A., Vuong, A., Gupta, A., Gottardi, A., Norelli, A., Venkatesh, A., Gholamidavoodi, A., Tabassum, A., Menezes, A., Kirubarajan, A., Mullokandov, A., Sabharwal, A., Herrick, A., Efrat, A., Erdem, A., Karakaş, A., Roberts, B. R., Loe, B. S., Zoph, B., Bojanowski, B., Özyurt, B., Hedayatnia, B., Neyshabur, B., Inden, B., Stein, B., Ekmekci, B., Lin, B. Y., Howald, B., Diao, C., Dour, C., Stinson, C., Argueta,</p>
<p>C., Ramírez, C. F., Singh, C., Rathkopf, C., Meng, C., Baral, C., Wu, C., Callison-Burch, C., Waites, C., Voigt, C., Manning, C. D., Potts, C., Ramirez, C., Rivera, C. E., Siro, C., Raffel, C., Ashcraft, C., Garbacea, C., Sileo, D., Garrette, D., Hendrycks, D., Kilman, D., Roth, D., Freeman, D., Khashabi, D., Levy, D., González, D. M., Perszyk, D., Hernandez, D., Chen, D., Ippolito, D., Gilboa, D., Dohan, D., Drakard, D., Jurgens, D., Datta, D., Ganguli, D., Emelin, D., Kleyko, D., Yuret, D., Chen, D., Tam, D., Hupkes, D., Misra, D., Buzan, D., Mollo, D. C., Yang, D., Lee, D.-H., Shutova, E., Cubuk, E. D., Segal, E., Hagerman, E., Barnes, E., Donoway, E., Pavlick, E., Rodola, E., Lam, E., Chu, E., Tang, E., Erdem, E., Chang, E., Chi, E. A., Dyer, E., Jerzak, E., Kim, E., Manyasi, E. E., Zheltonozhskii, E., Xia, F., Siar, F., Martínez-Plumed, F., Happé, F., Chollet, F., Rong, F., Mishra, G., Winata, G. I., de Melo, G., Kruszewski, G., Parascandolo, G., Mariani, G., Wang, G., Jaimovitch-López, G., Betz, G., Gur-Ari, G., Galijasevic, H., Kim, H., Rashkin, H., Hajishirzi, H., Mehta, H., Bogar, H., Shevlin, H., Schütze, H., Yakura, H., Zhang, H., Wong, H. M., Ng, I., Noble, I., Jumelet, J., Geissinger, J., Kernion, J., Hilton, J., Lee, J., Fisac, J. F., Simon, J. B., Koppel, J., Zheng, J., Zou, J., Kocoń, J., Thompson, J., Kaplan, J., Radom, J., Sohl-Dickstein, J., Phang, J., Wei, J., Yosinski, J., Novikova, J., Bosscher, J., Marsh, J., Kim, J., Taal, J., Engel, J., Alabi, J., Xu, J., Song, J., Tang, J., Waweru, J., Burden, J., Miller, J., Balis, J. U., Berant, J., Frohberg, J., Rozen, J., Hernandez-Orallo, J., Boudeman, J., Jones, J., Tenenbaum, J. B., Rule, J. S., Chua, J., Kanclerz, K., Livescu, K., Krauth, K., Gopalakrishnan, K., Ignatyeva, K., Markert, K., Dhole, K. D., Gimpel, K., Omondi, K., Mathewson, K., Chiafullo, K., Shkaruta, K., Shridhar, K., McDonell, K., Richardson, K., Reynolds, L., Gao, L., Zhang, L., Dugan, L., Qin, L., Contreras-Ochando, L., Morency, L.-P., Moschella, L., Lam, L., Noble, L., Schmidt, L., He, L., Colón, L. O., Metz, L., Şenel, L. K., Bosma, M., Sap, M., ter Hoeve, M., Farooqi, M., Faruqui, M., Mazeika, M., Baturan, M., Marelli, M., Maru, M., Quintana, M. J. R., Tolkiehn, M., Giulianelli, M., Lewis, M., Potthast, M., Leavitt, M. L., Hagen, M., Schubert, M., Baitemirova, M. O., Arnaud, M., McElrath, M., Yee, M. A., Cohen, M., Gu, M., Ivanitskiy, M., Starritt, M., Strube, M., Swędrowski, M., Bevilacqua, M., Yasunaga, M., Kale, M., Cain, M., Xu, M., Suzgun, M., Tiwari, M., Bansal, M., Aminnaseri, M., Geva, M., Gheini, M., T, M. V., Peng, N., Chi, N., Lee, N., Krakover, N. G.-A., Cameron, N., Roberts, N., Doiron, N., Nangia, N., Deckers, N., Muennighoff, N., Keskar, N. S., Iyer, N. S., Constant, N., Fiedel, N., Wen, N., Zhang, O., Agha, O., Elbaghdadi, O., Levy, O., Evans, O., Casares, P. A. M., Doshi, P., Fung, P., Liang, P. P., Vicol, P., Alipoormolabashi, P., Liao, P., Liang, P., Chang, P., Eckersley, P., Htut, P. M., Hwang, P., Miłkowski, P., Patil, P., Pezeshkpour, P., Oli, P., Mei, Q., Lyu, Q., Chen, Q., Banjade, R., Rudolph, R. E., Gabriel, R., Habacker, R., Delgado, R. R., Millière, R., Garg, R., Barnes, R., Saurous, R. A., Arakawa, R., Raymaekers, R., Frank, R., Sikand, R., Novak, R., Sitelew, R., LeBras, R., Liu, R., Jacobs, R., Zhang, R., Salakhutdinov, R., Chi, R., Lee, R., Stovall, R., Teehan, R., Yang, R., Singh, S., Mohammad, S. M., Anand, S., Dillavou, S., Shleifer, S., Wiseman, S., Gruetter, S., Bowman, S. R., Schoenholz, S. S., Han, S., Kwatra, S., Rous, S. A., Ghazarian, S., Ghosh, S., Casey, S., Bischoff, S., Gehrmann, S., Schuster, S., Sadeghi, S., Hamdan, S., Zhou, S., Srivastava, S., Shi, S., Singh, S., Asaadi, S., Gu, S. S., Pachchigar, S., Toshniwal, S., Upadhyay, S., Shyamolima, D., Shakeri, S., Thormeyer, S., Melzi, S., Reddy, S., Makini, S. P., Lee, S.-H., Torene, S., Hatwar, S., Dehaene, S., Divic, S., Ermon, S., Biderman, S., Lin, S., Prasad, S., Piantadosi, S. T., Shieber, S. M., Misherghi, S., Kiritchenko, S., Mishra, S., Linzen, T., Schuster, T., Li, T., Yu, T., Ali, T., Hashimoto, T., Wu, T.-L., Desbordes, T., Rothschild, T., Phan, T., Wang, T., Nkinyili, T., Schick, T., Kornev, T., Telleen-Lawton, T., Tunduny, T., Gerstenberg, T., Chang, T., Neeraj, T., Khot, T., Shultz, T., Shaham, U., Misra, V., Demberg, V., Nyamai, V., Raunak, V., Ramasesh, V., Prabhu, V. U., Padmakumar, V., Srikumar, V., Fedus, W., Saunders, W., Zhang, W., Vossen, W., Ren, X., Tong, X., Zhao, X., Wu, X., Shen, X., Yaghoobzadeh, Y., Lakretz, Y., Song, Y., Bahri, Y., Choi, Y., Yang, Y., Hao, Y., Chen, Y., Belinkov, Y., Hou, Y., Hou, Y., Bai, Y., Seid, Z., Zhao, Z., Wang, Z., Wang, Z. J., Wang, Z., and Wu, Z. (2022). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.</p>
<p>Tan, Z., Zhang, X., Wang, S., and Liu, Y. (2021). Msp: Multi-stage prompting for making pre-trained language models better translators. arXiv preprint arXiv:2110.06609.</p>
<p>Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhou, Y., Chang, C., Krivokon, I., Rusch, W., Pickett, M., Meier-Hellstern, K. S., Morris, M. R., Doshi, T., Santos, R. D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M.,</p>
<p>Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E. H., and Le, Q. (2022). Lamda: Language models for dialog applications. CoRR, abs/2201.08239.</p>
<p>Volden, J. (2017). Autism Spectrum Disorder, pages 59-83. Springer International Publishing, Cham.
Wang, B. and Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.</p>
<p>Webson, A. and Pavlick, E. (2021). Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247.</p>
<p>Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2022). Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Wittgenstein, L. (1921). Tractatus logico-philosophicus. London: Routledge, 1981.
Wittgenstein, L. (1953). Philosophical Investigations. Basil Blackwell, Oxford.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. (2022). Opt: Open pre-trained transformer language models.</p>
<p>Zheng, Z., Qiu, S., Fan, L., Zhu, Y., and Zhu, S.-C. (2021). GRICE: A grammar-based dataset for recovering implicature and conversational rEasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2074-2085, Online. Association for Computational Linguistics.</p>
<h1>Appendices</h1>
<p>Contents
A Contributions ..... 18
B Reproducibility Statement ..... 18
C Ethics Statement ..... 18
D Opener example with InstructGPT ..... 19
E Background on implicature ..... 20
F Detailed prompt templates ..... 22
G Model categorization ..... 26
H Human evaluation ..... 27
I Comparison with BIG-bench implicatures task ..... 29
I. 1 Discarding ambiguous examples ..... 29
I. 2 Overestimation of performance on implicature understanding ..... 30
I. 3 Other limitations ..... 30
J Chain-of-thought completions by GPT-4 ..... 31
K Additional results ..... 32
K. 1 Contrastive experiment ..... 32
K. 2 Variance over prompt ordering ..... 33
K. 3 Different zero-shot instruction prompts ..... 33
K. 4 The effect of in-context examples on sensitivity to prompt wording ..... 33
K. 5 Variance over API runs ..... 34
K. 6 Experiment with random in-context labels ..... 34
K. 7 Chain-of-thought on base models ..... 36
K. 8 Testing for spurious correlations ..... 36
K. 9 Detailed results type label analysis ..... 37
K. 10 Detailed results per model ..... 37
L Timestamps API calls ..... 69
M Compute and Emissions ..... 72</p>
<h1>A Contributions</h1>
<p>Laura Ruis: project proposal and leadership, dataset development, code writing, human experiment, manual error analysis, paper writing and editing.
Akbir Khan: code writing, model evaluations, human experiment, paper writing and editing.
Stella Biderman: model evaluations, compute usage, paper writing and editing, advisor.
Sara Hooker: compute usage, paper writing and editing, advisor.
Tim Rocktäschel: paper writing and editing, advisor.
Edward Grefenstette: initial idea, manual error analysis, paper writing and editing, advisor.</p>
<h2>B Reproducibility Statement</h2>
<p>We share all the data, human annotations, code used for the evaluations, and the raw results in the supplementary material. Additionally, in Appendix K. 5 we estimate the variance due to stochasticity in the API's of OpenAI and Cohere. Of course, if either OpenAI or Cohere decides to change the models behind the API, the results might look different. We publish the exact date and time each API was queried for the results in Appendix L. Finally, in Appendix K. 2 we estimate the variance over the prompt order of the in-context examples. The compute used for the experiments is detailed in Appendix M. The remaining experiments were done with API credits received as a research grant from OpenAI and Cohere.</p>
<h2>C Ethics Statement</h2>
<p>In this work, we conduct a study with human subjects (see Appendix H for details). To get matched with participants, we used the platform Prolific. Prolific complies with ethical standards according to UK law (e.g. complying with the GDPR). We compensated participants with a UK living wage at 15 GBP an hour, which is 6 GBP an hour more than Prolific recommends at 9 GBP per hour.
Implicature is an aspect of pragmatics, and pragmatic language impairments are universal in Autism Spectrum Disorder (ASD) (American Psychiatric Association, 2013). Difficulties in understanding scalar implicatures are claimed to be present in people with ASD (Volden, 2017), although the nature of the relation has proven hard to establish and has recently been debated (Katsos et al., 2011; Schaeken et al., 2018). For the purposes of this work, whether or not implicature understanding relates to ASD is not important. We took the following steps to make sure no sensitive data is collected or published. The human annotations we obtain are anonymous, related to a participant only by their Prolific ID for the purposes of compensation. In publishing the human annotations, we will not publish the Prolific ID of participants or anything else related to the participants. Additionally, we did not collect or request any personal or demographic characteristics of the participants apart from that they are all native English speakers.
Additionally, in this work we run a lot of compute-intensive experiments. We publish the estimated emissions per experiment in Appendix M. The total amount of GPU hours is estimated at maximally 966. How this is broken down per experiment can be seen in Appendix M.</p>
<h1>D Opener example with InstructGPT</h1>
<p>This quote was obtained through the OpenAI playground for text-davinci-002 on May 15th 2023. The model text-davinci-001 consistently generates better responses for the same prompt. GPT-3 itself (i.e. davinci) mainly gives nonsensical answers. In the following, the prompt is italic and the completion bold. The completion was generated with a maximum of 10 tokens and a temperature of 0 :</p>
<p>User: "Have you seen my phone?"
GPT-3: "Yes, I have seen your phone."
The opener example is used to introduce the problem we are looking at, and not to judge the model used to generate it. In fact, although text-davinci-002 sometimes completes conversations in a way that is unexpected according to pragmatic language usage, it is one of the better models when evaluated few-shot.</p>
<h1>E Background on implicature</h1>
<p>The first influential consideration of implicature is Grice (1975). In his work, Grice continues the trend of moving away from purely logical accounts of language started by Wittgenstein (1921) by hypothesising implicatures arise in conversation when some mutually agreed upon maxims seem to be violated. For example, if we agree on only making relevant contributions to conversation, Juan's response in the introduction seemingly violates this maxim-after all, he starts talking about work when Esther asks him about a party. However, because Juan agreed to be relevant he must be implying that having to work means he cannot come to the party. Grice contrasts conversational implicatures that arise through context with conventional implicatures. These are implicatures where the conventional meaning of the word determines what is implicated. An example given by Grice is the following sentence: "he is an Englishman; he is therefore brave.". Grice notes that this sentence does not literally state that an Englishman being brave is a direct consequence of him being English, but it's implied by the conventional meaning of the word 'therefore'.</p>
<p>Since then, issues with the Gricean cooperative principle have been pointed out by many (Levinson, 1983; Sperber and Wilson, 1986; Davis, 1998; Lepore and Stone, 2014). The most influential alternative theory is relevancy theory by Sperber and Wilson (1986). They do away with the cooperative principle and instead theorise implicatures arise because speakers try to produce utterances that are both as relevant as possible and require the least effort to process. Another point of contention is the incorporation of conventional implicatures on the pragmatics side. Bach (1999) argues that there is no such thing as conventional implicatures, and they are simply instances of something else. Based on a thorough treatment of what Grice calls conventional implicatures, Bach argues all examples of it can be filed under other concepts within semantics, like utterance modifiers (called "utterance modifiers" instead of "sentence modifiers" because they go against the semantic content of the rest of the sentence). Potts (2005) also argues that to explain conventional implicatures we can stay on semantic turf. Indeed, even Grice himself says conventional implicatures derive from the meaning of the words, not from conversational context. However, Potts does not claim conventional implicatures do not exist, but instead argues they arise by a combination of lexical meaning and novel ways of combining words-the latter being the well-known principle of compositionality, an important part of semantics, not of pragmatics. Potts provides us with an illuminating demarcation between conventional and conversational implicatures. Conventional implicatures are never negotiable by context, whereas conversational implicatures are context-dependent and can always be cancelled without causing incoherent discourse. Consider again the sentence "he is an Englishman; he is therefore brave." and the sentence "Eddie has three bicycles" (implicating that Eddie has exactly three bicycles and not more). The former sentence can not be cancelled by new context without contradiction, whereas for the latter, if we continue saying "In fact, Eddie has 10 bicycles, he is a bicycle junkie", we have cancelled the implicature. This demarcation clearly puts conventional implicatures on the semantic side, and conversational implicatures on the pragmatic side. Potts goes on by providing a formal theory for conventional implicatures.</p>
<p>In later work, Potts (2006) describes how pragmatic pressures interacting with context cause conversational implicature to arise. He shows how sensitive conversational implicatures are to small changes in the context. Novel information about a speaker's belief state might completely change what is implied. There are many more models of implicature that aim to explain how humans understand language in context. Most notably, Frank and Goodman (2012) formalise the view that speakers produce utterances that are helpful and not longer than necessary with a Bayesian model called the rational speech act (RSA). Many variants on the RSA framework have since been proposed. For example, Goodman and Frank (2016) extend it to handle nonliteral uses of language, like irony, and metaphor. In the context of computational models, prior work uses insights from pragmatics to show that the use of certain words can make a language model produce biased completions (Patel and Pavlick (2021), e.g. saying someone "claimed" something rather than "said" something), and inform bias and sentiment classifiers (Greene and Resnik, 2009; Recasens et al., 2013).</p>
<p>In this work, we focus on conversational implicatures and not on conventional implicatures. All conversational implicatures are negotiable by context, but the way they depend on context can be different. Grice (1975) identifies generalised conversational implicatures and particularised conversational implicatures. The former require little or no context to be resolved. For example, "some athletes smoke" can imply "not all athletes smoke", but might also imply "I do not know whether all athletes smoke" when it is a response to the question "do you know whether all athletes</p>
<p>smoke?" (Davis, 2019). The latter only arise in certain contexts. For example, the response "I have an early morning" to the question "do you want to stay for a drink?".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Note that there are several important aspects unknown for models behind APIs, like OpenAI's model sizes.
${ }^{7}$ https://github.com/LauraRuis/do-pigs-fly
${ }^{8}$ https://huggingface.co/datasets/UCL-DARK/ludwig&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>The precise method is unpublished and differs from the original instructGPT (Ouyang et al., 2022).&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5009 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5009</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5009</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-262465334</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.13734v2.pdf" target="_blank">Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification</a></p>
                <p><strong>Paper Abstract:</strong> Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations. We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance. We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily lead to better performance. In general, we discover that LLMs do not routinely outperform their smaller supervised machine learning models, and thus call for stance detection to be a benchmark for which LLMs also optimize for. The code used in this study is available at \url{https://github.com/ijcruic/LLM-Stance-Labeling}</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5009.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5009.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits intermediate step-by-step reasoning from LLMs by asking them to explain their chain of thought before giving a final answer; used to improve complex reasoning and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to various LLMs (GPT-3, GPT-3.5, ChatGPT, GPT-4, and open-source LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model — CoT is a prompting technique applied to decoder and encoder-decoder LLMs to induce multi-step internal reasoning; typically used with instruction-tuned variants of GPT-family and open-source models (Llama, Mistral, T5/UL2, Falcon, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Stance classification on benchmark Twitter datasets (e.g., SemEval2016, VAST, wtwt, P-Stance) used as a reasoning-eliciting task</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify stance (for/against/neutral/unrelated) of social media posts toward a specified target; while not a formal symbolic logic benchmark, the task benefits from multi-step reasoning to disambiguate implicature, sarcasm, and target-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot Chain-of-Thought prompting: ask model to 'think step-by-step' and produce an explanation/chain before a final stance label; two-stage prompting (reason then final answer) was used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that Chain-of-Thought style prompts generally improve stance-classification performance compared to simple instruction prompts; authors cite multiple prior works reporting superior results when reasoning is invoked. In this study, Zero-shot CoT and Few-Shot Prompting (FSP) were among the best prompting schemes; for 'good outputs' some models achieved very high F1 (examples from Table 6: T5 11B Flan-Alpaca and UL2 20B Flan show 1.00 for good-output F1 across several schemes), but overall performance is inconsistent across models/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT is a prompting technique, not a model change; its efficacy depends strongly on model, prompt construction, and example selection. The paper notes instability (LLMs sometimes fail to follow output format or produce invalid/multi-label outputs) and that CoT does not guarantee consistent gains across all models/datasets. Also, stance datasets are not strict formal logical-reasoning benchmarks, so gains reflect improved pragmatic/discourse reasoning rather than formal logical proof.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to simple task-only or task-definition prompts, CoT and FSP often outperform in stance classification. However, there is no single LLM that consistently benefits most from CoT; encoder-decoder models often produce more valid completions and higher accuracy when combined with CoT. The paper also references prior works that used CoT for both GPT-family and open-source models and reported improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The paper's experiments show that prompting scheme matters: FSP and Zero-shot CoT tend to produce more valid completions and higher accuracy. Encoder-decoder architectures tended to produce more valid single-word outputs (adhering to the instruction) than decoder-only models. They also find weak or negative correlations between longer output length and correctness (longer outputs weakly correlate with worse stance predictions). Fine-tuning (LoRA) did not reliably improve out-of-domain performance, indicating in-context CoT may be preferable to small-data fine-tuning for generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5009.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5009.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logically-Consistent CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Logically-Consistent Chain-of-Thought Approach for Stance Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of CoT prompting that enforces logical consistency in intermediate reasoning steps to improve stance detection accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Logically Consistent Chain-of-Thought Approach for Stance Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>reported applications to GPT-3 and ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method paper applying a logically-constrained chain-of-thought prompting design to large language models (GPT-3 family and ChatGPT) for stance detection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Stance detection (SemEval2016, VAST and related stance datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Elicit logically coherent intermediate reasoning that supports a final stance label; the task requires relating claims, context, and target-specific evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Design CoT prompts that explicitly require logical consistency checks in the chain and use the chain to derive a final label; operates as a prompt engineering approach rather than model retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>According to the paper's survey of related work, Logically-Consistent CoT prompts have been reported to yield superior performance on benchmark stance datasets in prior work (cited as [61] in the paper). The current paper does not re-run that method but references reported improvements in those prior studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As with CoT generally, gains depend on prompt design and model; potential data contamination concerns for closed models (ChatGPT) are noted in related literature. The approach is not validated on formal logical benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Reported as superior to simple instruction prompts and standard CoT in the referenced prior work for stance detection; no direct head-to-head numeric comparison is provided in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The present paper cites these prior results but does not provide a dedicated ablation; it does, however, find in its own experiments that reasoning-based prompts (CoT, FSP) outperform other prompt types on many model/dataset combos, supporting the prior observations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5009.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5009.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought / Tree of Thoughts prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative prompting paradigm that generates multiple candidate reasoning paths (a tree of thoughts), evaluates them, and selects or combines outputs to improve problem-solving and complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied in prior work to GPT-3.5 and GPT-family models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting/meta-reasoning approach that simulates multiple branches of chain-of-thought and performs internal evaluation to select promising branches; applied as a prompting protocol on top of existing autoregressive LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Used in prior work for stance detection (SemEval2016, VAST, wtwt) and other complex reasoning tasks (general problem solving); in this paper, it is mentioned as part of CoDA/embodied prompting hybrids.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generates and evaluates multiple intermediate reasoning 'thoughts' (branches) to improve final decision-making; for stance detection this helps explore alternative interpretations before yielding a label.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Embodied Tree-of-Thought or a multistage Tree-of-Thought-like prompt (multiple sub-prompts to generate arguments, evaluate them, and form a final decision); sometimes combined with role/embodiment prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper references Lan et al. reporting state-of-the-art performance on some benchmark stance datasets using an embodied Tree-of-Thought prompt with GPT-3.5 (exact numbers not reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Tree-of-Thought is computationally heavier (multiple forward passes per example) and more complex to implement; the paper notes CoDA/Tree-of-Thought variants were not always run for very large prompts in their own experiments due to resource limits. Success depends on evaluation heuristics and prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Reportedly outperforms simpler prompting methods on certain stance datasets in referenced work; in the present study, Tree-of-Thought-like CoDA is treated as one of the most informative prompting paradigms but the authors could not run CoDA on the largest prompts in every experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No direct ablation in this paper; referenced prior work indicates benefit for deliberative evaluation of multiple reasoning chains. The paper's own analysis supports that multi-stage and role-based prompting (CoDA) often produces more valid completions and high accuracy when valid outputs are produced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5009.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5009.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collaborative Role-Infused LLM-based Agents (CoDA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting architecture that spawns multiple 'embodied' agent prompts (e.g., linguist, subject matter expert, heavy social media user) to generate arguments, then evaluates and aggregates these arguments to produce a final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stance detection with collaborative role-infused llm-based agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated with GPT-3.5 and other LLMs in referenced work; mentioned as used in experiments in this paper's prompt set</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi-prompt pipeline that uses role-based sub-prompts to generate diverse perspectives, then uses evaluator prompts to synthesize a final stance label; implemented on top of existing instruction-tuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Stance classification (SemEval2016, VAST, etc.) treated as a complex reasoning task requiring evidence aggregation and perspective-taking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Create multiple expert-perspective explanations and arguments for/against and then evaluate them to reach a final stance label; this is intended to simulate multi-agent deliberation to improve logical coherence and evidence-based decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Six-stage prompting: generate several expert analyses, produce for/against arguments based on those analyses, then run an evaluator prompt to yield the final label; combines Tree-of-Thought and embodied prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Cited prior work (Lan et al / [25]) reports state-of-the-art performance on some benchmark stance datasets using CoDA/embodied multi-agent prompts; within this paper CoDA is one of the prompting schemes tested but the authors note resource constraints prevented running CoDA on the largest prompts in all experiments. Table 6 shows some high 'good-output' F1 values for CoDA for certain models (e.g., encoder-decoder models produce valid outputs more often). Exact numeric comparisons are left to the cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Resource intensive (multiple prompts per example). The present paper found CoDA sometimes could not be run on larger prompts due to computational limits. Success relies on well-crafted role prompts and reliable evaluator heuristics; LLMs still sometimes produce invalid/multilabel outputs even when instructed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>CoDA and Tree-of-Thought style prompting are generally reported to outperform single-step instruction prompts and can exceed performance of in-domain supervised baselines on selected datasets, but are not uniformly superior across all datasets and model families.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>This paper's analysis indicates FSP and CoT (and by extension structured multi-stage prompts like CoDA) tend to produce more valid completions and higher accuracy when valid outputs occur; however, no fine-grained ablation isolating CoDA components is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5009.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5009.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 CoT eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated with zero-shot and few-shot Chain-of-Thought for stance detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work evaluated GPT-4 as a stance annotator using zero-shot and few-shot CoT prompts on topical stance tasks (e.g., climate change), showing reasoning prompts can improve performance but with variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 as a Twitter Data Annotator: Unraveling Its Performance on a Stance Classification Task</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large instruction-tuned transformer (GPT-4) evaluated via prompting (zero/few-shot CoT) as a data annotator on stance classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Stance classification for climate-change related tweets (and other stance datasets in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Annotate tweets for stance (for/against/neutral) toward climate-change targets; requires pragmatic reasoning and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot Chain-of-Thought prompting to elicit intermediate reasoning before final label.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Referenced prior work (Liyanage et al.) reports that GPT-4 with CoT can improve annotation quality compared to vanilla prompting; exact numbers are in the cited work (not reproduced in detail here). The current paper lists this reference in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Potential instability and inconsistency across prompts and examples; concerns about data contamination for closed models are noted elsewhere in the literature. Also, gains are dataset-dependent and not guaranteed to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to simpler prompts, CoT with GPT-4 tends to produce better reasoning and sometimes better labels, but the paper notes mixed results across different prior studies and warns about possible contamination and evaluation reliability issues.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not reported in this paper; the present study's overall findings (that CoT and FSP tend to help and encoder-decoder families produce more valid outputs) are consistent with reported GPT-4 behavior in prior studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5009.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5009.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ThisPaper-OpenSource-CoTEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>This paper's evaluation of CoT / FSP prompting on open-source LLMs for stance (Cruickshank et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This study systematically tests seven prompting schemes (including Zero-shot CoT and Few-Shot Prompting) across 10 open-source LLMs on six Twitter-based stance datasets to evaluate whether reasoning-style prompts improve performance and output validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon 7B Instruct, Falcon 40B Instruct, T5 3B Flan-Alpaca, T5 11B Flan-Alpaca, Mistral v0.1 7B Instruct, UL2 20B Flan, Llama-2 13B Chat, Llama-2 7B Chat, Llama-2 7B, Phi-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mix of decoder-only and encoder-decoder open-source LLMs ranging from ~2.7B to 40B parameters; some models are instruction-tuned/chat-tuned (e.g., Falcon Instruct, Llama-2 Chat, Flan-tuned T5/UL2). Greedy decoding was used for consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>ranges from 2.7B (Phi-2) to 40B (Falcon 40B); specific sizes included: Phi-2 2.7B, T5 3B, Mistral 7B, Llama-2 7B/13B, Falcon 7B/40B, UL2 20B, T5 11B.</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Stance classification on six Twitter datasets (covid-lies, election2016, phemerumors, SemEval2016, srq, wtwt) used to probe reasoning capability via prompting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Targeted stance detection (agree/neutral/disagree or dataset-specific labels) requiring contextual and pragmatic reasoning about the relationship between statement and target.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Compared seven prompting schemes (Task-only, Task Definition, Context Analyze, Context Question, Few-shot Prompting (FSP), Zero-shot CoT, CoDA) and evaluated both zero-shot inference and LoRA-based parameter-efficient fine-tuning in leave-one-dataset-out (out-of-domain) experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Key findings: FSP and Zero-shot CoT generally performed better than simpler prompts; encoder-decoder models more reliably produced valid single-label outputs; some models (encoder-decoder family like T5/UL2) achieved very high 'good-output' F1 (examples: T5 11B Flan-Alpaca and UL2 20B Flan showing 1.00 for good-output F1 in the paper's Table 6). However, overall LLM performance was inconsistent across datasets and did not consistently beat in-domain supervised baselines. Fine-tuned models often did not improve out-of-domain performance and sometimes worsened it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Inconsistent model behavior and many invalid outputs (model returns extra text or ambiguous labels) reduce effectiveness; larger model size did not guarantee better performance (e.g., Falcon 40B did worse than Falcon 7B on a specific prompting scheme); fine-tuning with limited data reduced generalizability; stance datasets are not formal logical benchmarks, so improvements reflect pragmatic reasoning rather than formal logic capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Encoder-decoder architectures + FSP/CoT tended to outperform decoder-only families for valid completions. No single open-source LLM or prompt consistently dominated across all datasets. Fine-tuned LLMs (LoRA) frequently underperformed zero-shot prompting when tested out-of-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analyses include breakdown of 'good' vs 'bad' completions: when models produced a valid single-label output ('good results'), accuracy was substantially higher (good-output F1 often exceeds baseline); output length correlates weakly negatively with correctness; a simple decision-tree using output length/validity predicted correct predictions with 0.59 accuracy. The study reports CoT and FSP produce more valid completions and higher accuracy when valid outputs occur.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>A Logically Consistent Chain-of-Thought Approach for Stance Detection <em>(Rating: 2)</em></li>
                <li>Stance detection with collaborative role-infused llm-based agents <em>(Rating: 2)</em></li>
                <li>GPT-4 as a Twitter Data Annotator: Unraveling Its Performance on a Stance Classification Task <em>(Rating: 1)</em></li>
                <li>Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5009",
    "paper_id": "paper-262465334",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting method that elicits intermediate step-by-step reasoning from LLMs by asking them to explain their chain of thought before giving a final answer; used to improve complex reasoning and reduce hallucinations.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "applied to various LLMs (GPT-3, GPT-3.5, ChatGPT, GPT-4, and open-source LLMs)",
            "model_description": "Not a single model — CoT is a prompting technique applied to decoder and encoder-decoder LLMs to induce multi-step internal reasoning; typically used with instruction-tuned variants of GPT-family and open-source models (Llama, Mistral, T5/UL2, Falcon, etc.).",
            "model_size": null,
            "logical_reasoning_task": "Stance classification on benchmark Twitter datasets (e.g., SemEval2016, VAST, wtwt, P-Stance) used as a reasoning-eliciting task",
            "task_description": "Classify stance (for/against/neutral/unrelated) of social media posts toward a specified target; while not a formal symbolic logic benchmark, the task benefits from multi-step reasoning to disambiguate implicature, sarcasm, and target-dependence.",
            "method_or_approach": "Zero-shot and few-shot Chain-of-Thought prompting: ask model to 'think step-by-step' and produce an explanation/chain before a final stance label; two-stage prompting (reason then final answer) was used.",
            "performance": "Paper reports that Chain-of-Thought style prompts generally improve stance-classification performance compared to simple instruction prompts; authors cite multiple prior works reporting superior results when reasoning is invoked. In this study, Zero-shot CoT and Few-Shot Prompting (FSP) were among the best prompting schemes; for 'good outputs' some models achieved very high F1 (examples from Table 6: T5 11B Flan-Alpaca and UL2 20B Flan show 1.00 for good-output F1 across several schemes), but overall performance is inconsistent across models/datasets.",
            "limitations_or_failure_cases": "CoT is a prompting technique, not a model change; its efficacy depends strongly on model, prompt construction, and example selection. The paper notes instability (LLMs sometimes fail to follow output format or produce invalid/multi-label outputs) and that CoT does not guarantee consistent gains across all models/datasets. Also, stance datasets are not strict formal logical-reasoning benchmarks, so gains reflect improved pragmatic/discourse reasoning rather than formal logical proof.",
            "comparison": "Compared to simple task-only or task-definition prompts, CoT and FSP often outperform in stance classification. However, there is no single LLM that consistently benefits most from CoT; encoder-decoder models often produce more valid completions and higher accuracy when combined with CoT. The paper also references prior works that used CoT for both GPT-family and open-source models and reported improvements.",
            "ablation_or_analysis_results": "The paper's experiments show that prompting scheme matters: FSP and Zero-shot CoT tend to produce more valid completions and higher accuracy. Encoder-decoder architectures tended to produce more valid single-word outputs (adhering to the instruction) than decoder-only models. They also find weak or negative correlations between longer output length and correctness (longer outputs weakly correlate with worse stance predictions). Fine-tuning (LoRA) did not reliably improve out-of-domain performance, indicating in-context CoT may be preferable to small-data fine-tuning for generalizability.",
            "uuid": "e5009.0",
            "source_info": {
                "paper_title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Logically-Consistent CoT",
            "name_full": "A Logically-Consistent Chain-of-Thought Approach for Stance Detection",
            "brief_description": "A variant of CoT prompting that enforces logical consistency in intermediate reasoning steps to improve stance detection accuracy.",
            "citation_title": "A Logically Consistent Chain-of-Thought Approach for Stance Detection",
            "mention_or_use": "mention",
            "model_name": "reported applications to GPT-3 and ChatGPT",
            "model_description": "Method paper applying a logically-constrained chain-of-thought prompting design to large language models (GPT-3 family and ChatGPT) for stance detection tasks.",
            "model_size": null,
            "logical_reasoning_task": "Stance detection (SemEval2016, VAST and related stance datasets)",
            "task_description": "Elicit logically coherent intermediate reasoning that supports a final stance label; the task requires relating claims, context, and target-specific evidence.",
            "method_or_approach": "Design CoT prompts that explicitly require logical consistency checks in the chain and use the chain to derive a final label; operates as a prompt engineering approach rather than model retraining.",
            "performance": "According to the paper's survey of related work, Logically-Consistent CoT prompts have been reported to yield superior performance on benchmark stance datasets in prior work (cited as [61] in the paper). The current paper does not re-run that method but references reported improvements in those prior studies.",
            "limitations_or_failure_cases": "As with CoT generally, gains depend on prompt design and model; potential data contamination concerns for closed models (ChatGPT) are noted in related literature. The approach is not validated on formal logical benchmarks in this paper.",
            "comparison": "Reported as superior to simple instruction prompts and standard CoT in the referenced prior work for stance detection; no direct head-to-head numeric comparison is provided in this paper's experiments.",
            "ablation_or_analysis_results": "The present paper cites these prior results but does not provide a dedicated ablation; it does, however, find in its own experiments that reasoning-based prompts (CoT, FSP) outperform other prompt types on many model/dataset combos, supporting the prior observations.",
            "uuid": "e5009.1",
            "source_info": {
                "paper_title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Tree-of-Thought",
            "name_full": "Tree-of-Thought / Tree of Thoughts prompting",
            "brief_description": "A deliberative prompting paradigm that generates multiple candidate reasoning paths (a tree of thoughts), evaluates them, and selects or combines outputs to improve problem-solving and complex reasoning.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "mention",
            "model_name": "applied in prior work to GPT-3.5 and GPT-family models",
            "model_description": "Prompting/meta-reasoning approach that simulates multiple branches of chain-of-thought and performs internal evaluation to select promising branches; applied as a prompting protocol on top of existing autoregressive LLMs.",
            "model_size": null,
            "logical_reasoning_task": "Used in prior work for stance detection (SemEval2016, VAST, wtwt) and other complex reasoning tasks (general problem solving); in this paper, it is mentioned as part of CoDA/embodied prompting hybrids.",
            "task_description": "Generates and evaluates multiple intermediate reasoning 'thoughts' (branches) to improve final decision-making; for stance detection this helps explore alternative interpretations before yielding a label.",
            "method_or_approach": "Embodied Tree-of-Thought or a multistage Tree-of-Thought-like prompt (multiple sub-prompts to generate arguments, evaluate them, and form a final decision); sometimes combined with role/embodiment prompting.",
            "performance": "Paper references Lan et al. reporting state-of-the-art performance on some benchmark stance datasets using an embodied Tree-of-Thought prompt with GPT-3.5 (exact numbers not reproduced in this paper).",
            "limitations_or_failure_cases": "Tree-of-Thought is computationally heavier (multiple forward passes per example) and more complex to implement; the paper notes CoDA/Tree-of-Thought variants were not always run for very large prompts in their own experiments due to resource limits. Success depends on evaluation heuristics and prompt engineering.",
            "comparison": "Reportedly outperforms simpler prompting methods on certain stance datasets in referenced work; in the present study, Tree-of-Thought-like CoDA is treated as one of the most informative prompting paradigms but the authors could not run CoDA on the largest prompts in every experiment.",
            "ablation_or_analysis_results": "No direct ablation in this paper; referenced prior work indicates benefit for deliberative evaluation of multiple reasoning chains. The paper's own analysis supports that multi-stage and role-based prompting (CoDA) often produces more valid completions and high accuracy when valid outputs are produced.",
            "uuid": "e5009.2",
            "source_info": {
                "paper_title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoDA",
            "name_full": "Collaborative Role-Infused LLM-based Agents (CoDA)",
            "brief_description": "A prompting architecture that spawns multiple 'embodied' agent prompts (e.g., linguist, subject matter expert, heavy social media user) to generate arguments, then evaluates and aggregates these arguments to produce a final decision.",
            "citation_title": "Stance detection with collaborative role-infused llm-based agents",
            "mention_or_use": "mention",
            "model_name": "evaluated with GPT-3.5 and other LLMs in referenced work; mentioned as used in experiments in this paper's prompt set",
            "model_description": "A multi-prompt pipeline that uses role-based sub-prompts to generate diverse perspectives, then uses evaluator prompts to synthesize a final stance label; implemented on top of existing instruction-tuned LLMs.",
            "model_size": null,
            "logical_reasoning_task": "Stance classification (SemEval2016, VAST, etc.) treated as a complex reasoning task requiring evidence aggregation and perspective-taking.",
            "task_description": "Create multiple expert-perspective explanations and arguments for/against and then evaluate them to reach a final stance label; this is intended to simulate multi-agent deliberation to improve logical coherence and evidence-based decisions.",
            "method_or_approach": "Six-stage prompting: generate several expert analyses, produce for/against arguments based on those analyses, then run an evaluator prompt to yield the final label; combines Tree-of-Thought and embodied prompting.",
            "performance": "Cited prior work (Lan et al / [25]) reports state-of-the-art performance on some benchmark stance datasets using CoDA/embodied multi-agent prompts; within this paper CoDA is one of the prompting schemes tested but the authors note resource constraints prevented running CoDA on the largest prompts in all experiments. Table 6 shows some high 'good-output' F1 values for CoDA for certain models (e.g., encoder-decoder models produce valid outputs more often). Exact numeric comparisons are left to the cited works.",
            "limitations_or_failure_cases": "Resource intensive (multiple prompts per example). The present paper found CoDA sometimes could not be run on larger prompts due to computational limits. Success relies on well-crafted role prompts and reliable evaluator heuristics; LLMs still sometimes produce invalid/multilabel outputs even when instructed.",
            "comparison": "CoDA and Tree-of-Thought style prompting are generally reported to outperform single-step instruction prompts and can exceed performance of in-domain supervised baselines on selected datasets, but are not uniformly superior across all datasets and model families.",
            "ablation_or_analysis_results": "This paper's analysis indicates FSP and CoT (and by extension structured multi-stage prompts like CoDA) tend to produce more valid completions and higher accuracy when valid outputs occur; however, no fine-grained ablation isolating CoDA components is reported here.",
            "uuid": "e5009.3",
            "source_info": {
                "paper_title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4 CoT eval",
            "name_full": "GPT-4 evaluated with zero-shot and few-shot Chain-of-Thought for stance detection",
            "brief_description": "Prior work evaluated GPT-4 as a stance annotator using zero-shot and few-shot CoT prompts on topical stance tasks (e.g., climate change), showing reasoning prompts can improve performance but with variability.",
            "citation_title": "GPT-4 as a Twitter Data Annotator: Unraveling Its Performance on a Stance Classification Task",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Closed-source large instruction-tuned transformer (GPT-4) evaluated via prompting (zero/few-shot CoT) as a data annotator on stance classification tasks.",
            "model_size": null,
            "logical_reasoning_task": "Stance classification for climate-change related tweets (and other stance datasets in cited work)",
            "task_description": "Annotate tweets for stance (for/against/neutral) toward climate-change targets; requires pragmatic reasoning and inference.",
            "method_or_approach": "Zero-shot and few-shot Chain-of-Thought prompting to elicit intermediate reasoning before final label.",
            "performance": "Referenced prior work (Liyanage et al.) reports that GPT-4 with CoT can improve annotation quality compared to vanilla prompting; exact numbers are in the cited work (not reproduced in detail here). The current paper lists this reference in Table 1.",
            "limitations_or_failure_cases": "Potential instability and inconsistency across prompts and examples; concerns about data contamination for closed models are noted elsewhere in the literature. Also, gains are dataset-dependent and not guaranteed to generalize.",
            "comparison": "Compared to simpler prompts, CoT with GPT-4 tends to produce better reasoning and sometimes better labels, but the paper notes mixed results across different prior studies and warns about possible contamination and evaluation reliability issues.",
            "ablation_or_analysis_results": "Not reported in this paper; the present study's overall findings (that CoT and FSP tend to help and encoder-decoder families produce more valid outputs) are consistent with reported GPT-4 behavior in prior studies.",
            "uuid": "e5009.4",
            "source_info": {
                "paper_title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ThisPaper-OpenSource-CoTEval",
            "name_full": "This paper's evaluation of CoT / FSP prompting on open-source LLMs for stance (Cruickshank et al. 2024)",
            "brief_description": "This study systematically tests seven prompting schemes (including Zero-shot CoT and Few-Shot Prompting) across 10 open-source LLMs on six Twitter-based stance datasets to evaluate whether reasoning-style prompts improve performance and output validity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon 7B Instruct, Falcon 40B Instruct, T5 3B Flan-Alpaca, T5 11B Flan-Alpaca, Mistral v0.1 7B Instruct, UL2 20B Flan, Llama-2 13B Chat, Llama-2 7B Chat, Llama-2 7B, Phi-2",
            "model_description": "A mix of decoder-only and encoder-decoder open-source LLMs ranging from ~2.7B to 40B parameters; some models are instruction-tuned/chat-tuned (e.g., Falcon Instruct, Llama-2 Chat, Flan-tuned T5/UL2). Greedy decoding was used for consistency.",
            "model_size": "ranges from 2.7B (Phi-2) to 40B (Falcon 40B); specific sizes included: Phi-2 2.7B, T5 3B, Mistral 7B, Llama-2 7B/13B, Falcon 7B/40B, UL2 20B, T5 11B.",
            "logical_reasoning_task": "Stance classification on six Twitter datasets (covid-lies, election2016, phemerumors, SemEval2016, srq, wtwt) used to probe reasoning capability via prompting",
            "task_description": "Targeted stance detection (agree/neutral/disagree or dataset-specific labels) requiring contextual and pragmatic reasoning about the relationship between statement and target.",
            "method_or_approach": "Compared seven prompting schemes (Task-only, Task Definition, Context Analyze, Context Question, Few-shot Prompting (FSP), Zero-shot CoT, CoDA) and evaluated both zero-shot inference and LoRA-based parameter-efficient fine-tuning in leave-one-dataset-out (out-of-domain) experiments.",
            "performance": "Key findings: FSP and Zero-shot CoT generally performed better than simpler prompts; encoder-decoder models more reliably produced valid single-label outputs; some models (encoder-decoder family like T5/UL2) achieved very high 'good-output' F1 (examples: T5 11B Flan-Alpaca and UL2 20B Flan showing 1.00 for good-output F1 in the paper's Table 6). However, overall LLM performance was inconsistent across datasets and did not consistently beat in-domain supervised baselines. Fine-tuned models often did not improve out-of-domain performance and sometimes worsened it.",
            "limitations_or_failure_cases": "Inconsistent model behavior and many invalid outputs (model returns extra text or ambiguous labels) reduce effectiveness; larger model size did not guarantee better performance (e.g., Falcon 40B did worse than Falcon 7B on a specific prompting scheme); fine-tuning with limited data reduced generalizability; stance datasets are not formal logical benchmarks, so improvements reflect pragmatic reasoning rather than formal logic capability.",
            "comparison": "Encoder-decoder architectures + FSP/CoT tended to outperform decoder-only families for valid completions. No single open-source LLM or prompt consistently dominated across all datasets. Fine-tuned LLMs (LoRA) frequently underperformed zero-shot prompting when tested out-of-domain.",
            "ablation_or_analysis_results": "Analyses include breakdown of 'good' vs 'bad' completions: when models produced a valid single-label output ('good results'), accuracy was substantially higher (good-output F1 often exceeds baseline); output length correlates weakly negatively with correctness; a simple decision-tree using output length/validity predicted correct predictions with 0.59 accuracy. The study reports CoT and FSP produce more valid completions and higher accuracy when valid outputs occur.",
            "uuid": "e5009.5",
            "source_info": {
                "paper_title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "A Logically Consistent Chain-of-Thought Approach for Stance Detection",
            "rating": 2,
            "sanitized_title": "a_logically_consistent_chainofthought_approach_for_stance_detection"
        },
        {
            "paper_title": "Stance detection with collaborative role-infused llm-based agents",
            "rating": 2,
            "sanitized_title": "stance_detection_with_collaborative_roleinfused_llmbased_agents"
        },
        {
            "paper_title": "GPT-4 as a Twitter Data Annotator: Unraveling Its Performance on a Stance Classification Task",
            "rating": 1,
            "sanitized_title": "gpt4_as_a_twitter_data_annotator_unraveling_its_performance_on_a_stance_classification_task"
        },
        {
            "paper_title": "Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media",
            "rating": 1,
            "sanitized_title": "investigating_chainofthought_with_chatgpt_for_stance_detection_on_social_media"
        }
    ],
    "cost": 0.01872225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification
5 Mar 2024</p>
<p>Army Cyber Institute, USAIain J Cruickshank iain.cruickshank@westpoint.edu 
LYNNETTE HUI XIAN NG
Carnegie Mellon University
USA</p>
<p>Army Cyber Institute
2101 New South Post RoadHighland FallsNew YorkUSA</p>
<p>Lynnette Hui lynnetteng@cmu.edu 
Carnegie Mellon University
5000 Forbes AvePittsburghPennslyvaniaUSA</p>
<p>Xian Ng 
Carnegie Mellon University
5000 Forbes AvePittsburghPennslyvaniaUSA</p>
<p>Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification
5 Mar 2024D49C774B90245EA08D9333A57A524093arXiv:2309.13734v2[cs.CL]Manuscript submitted to ACM Manuscript submitted to ACMstance detectionstance classificationlarge language modelsprompting
Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning.Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model.However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts.In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations.We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance.We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily lead to better performance.In general, we discover that LLMs do not routinely outperform their smaller supervised machine learning models, and thus call for stance detection to be a benchmark for which LLMs also optimize for.The code used in this study is available at https://github.com/ijcruic/LLM-Stance-Labeling.CCS Concepts: • Computing methodologies → Natural language processing; Machine learning approaches.</p>
<p>INTRODUCTION</p>
<p>Stance classification, also known as stance detection, is a natural language processing task of eliciting the author's viewpoint towards a subject of interest.That is, understanding the stance towards a target.Approaches towards this task typically use a supervised machine learning model to learn stance labels (i.e., agree, disagree, neutral) that correspond to sentences, and classify unknown sentences by assigning them one of the learned stance labels.However, building supervised machine learning structures on top of manual stance annotation imposes limitations on the model's ability to fully generalize across different contexts.</p>
<p>The advent of Large Language Models (LLMs) opens up the possibility of classifying the stance of a huge number of sentences with an absolute minimum use of human labels.This allows for unsupervised stance detection, thus curtailing the need for manual stance annotation.With the lighting speed of LLM research, there are a plethora of LLMs released, and a variety of prompting schemes for formatting the input into the LLMs.In this paper, we study the use of LLMs for the stance detection task, essentially investigating the following questions.How suitable are LLMs for the stance detection task?Which model produces the best performance?How should one format the inputs to the model?To answer these questions, we perform the stance classification task across 10 LLMs and 7 prompting schemes.To ensure generalizability of our results, we perform our experiments across 6 benchmark, social media datasets that have been manually annotated for their stance.</p>
<p>Objectives and Contributions</p>
<p>This article studies the stance classification task with social media datasets, namely Twitter datasets.The stance classification task for social media datasets is particularly important in today's digital age.Understanding the stance of social media conversations is key towards societally critical tasks such as distilling the aggregate public opinion towards important or contentious topics [4].</p>
<p>Within this work, we test Large Language Models as a stance classification technique.LLMs provide an easy and intuitive way of extracting information from a sentence.Instead of having to code or train a particular model for a particular task, an analyst can simply type a prompt in natural language for the LLM to perform that task.This ease of analysis opens up text analysis to a variety of analyst types, from computational to social to policy analysts.A zero-shot inference scheme also aids in extracting stance from the volume of social media posts, for no prior training of a language model is required for many language tasks with the current state-of-the-art LLMs.However, for the best performance of LLMs, including accurately extracting a stance from a sentence, users and analysts must be able to formulate the appropriate prompts.Therefore, in this paper, we investigate different prompting schemes for LLMs for stance classification.Finally, since LLMs can be fine-tuned for a task to further potentially boost performance of the LLM for that task, we also investigate how fine-tuning affects LLM performance for stance classification.</p>
<p>The key objective of this article is to investigate the ability of LLMs for the task of stance detection, with a focus on different prompting schemes.We performed experiments on several social media datasets to provide insight into how the performance of LLMs in the stance detection task varies with the context of the data.</p>
<p>Our main contributions can be summarized as follows.</p>
<p>• We evaluated the stance detection task using Large Language Models (LLMs).Our experiments span 6 Twitter datasets, 10 models, and 7 prompting schemes.This large-scale evaluation reveals that LLMs are often competitive with in-domain supervised models but do not consistently outperform these supervised baselines.LLMs tend to outperform supervised models only with specific prompting schemes, indicating the importance of crafting an input prompt.</p>
<p>• To analyze whether specialized models perform better, we experimented on fine-tuned LLMs.Our results show that fine-tuned models do not necessarily perform better than zero-shot models.</p>
<p>• We analyzed the responses of LLMs when instructed to predict a stance from a sentence.We find that for those data points where the LLM produced a valid stance classification for the statement to be classified, about half of those entries (50.8%) were correctly predicted.</p>
<p>In the rest of this article, we review the relevant studies in section 2 in terms of the stance detection task, stance detection in social networks, and the use of Large Language Models for stance detection.We next detail the datasets prepared for the stance detection task in section 3. The stance detection methodology is detailed in section 4, where we describe the LLMs tested, the crafting of the prompting schemes, and the fine-tuning methodology.We present the results of the experimental evaluations quantifying the ability of LLMs to detect stance in section 5. section 6 discusses the results and outlines directions for future work before section 7 concludes the article.</p>
<p>Manuscript submitted to ACM 2 RELATED WORK</p>
<p>Stance Classification Task</p>
<p>Stance classification is a well-studied natural language processing task.At its core, stance classification is the task of classifying an entity's (e.g., person, text statement, etc.) opinion, either implied or expressed, about a target [34].As a result, this task has an inherent context-dependency in its definition that related tasks, unlike sentiment which do not depend on the context: stance is not defined without a target, and so classifying stance requires this context.Due to the importance of determining stance for many disciplines, this task has several benchmark datasets and methods.In previous work involving stance classification, much of the focus was on the construction of supervised machine learning models for the task.A commonly used machine learning classifier is the Support Vector Machine [14,24], which has performed well in the SemEval-2016 benchmark stance detection competition [32].Supervised models that use neural network architectures are also popular.These include the use of convolutional neural networks [53], recurrent neural networks [59], and the enhancement of these network architectures with textual entailment [65] and data augmentation [20].Other recent approaches focus on multi-task learning objectives and transfer learning from transformer-based neural networks [7,56,65].The resultant stance models typically boast strong in-domain performance, but they often struggle to generalize to new data or other unseen stance targets and thus are of little use to real world practitioners due to this generalizability shortcoming [7,34].</p>
<p>Although most of the work in stance classification has focused on supervised machine learning models built upon manual human annotations, there are also some unsupervised techniques.Unsupervised stance labeling methods make use of the idea of language homogeneity for label classification [64].One popular technique is classification using graph networks.Graph neural networks have been used to formulate homogeneous and heterogeneous information for Twitter users, thereafter inferring the stance of the user based on past tweets and the tweets of its interaction graph neighbors [64].Another technique is the use of label propagation on user interaction networks that represent relationships between users [50].The interaction network can also be divided into partitions, and the user stance can be interpreted as an aggregated stance of partitions [37].Darwish et al. [12] projected a large set of Twitter user data onto a low-dimensional space before clustering the interaction network into multiple partitions.These unsupervised methods do not rely on having a set of stance labels to train a model but often have very specific scenarios for their use, such as the explicit behavioral links between social media users to create a graph network.It also relies on strong assumptions about the language or behavior of users to infer stances, such as the association of a particular stance towards a word or hashtag.</p>
<p>A last family of techniques in the stance classification task is the use of zero-shot methods for stance detection.</p>
<p>Zero-shot classification is the idea that the model can classify items that it has not encountered previously without being trained on a particular use case.In particular, [6] discuss a variety of techniques for zero-shot stance detection and present an adaptation of the SemEval2016 dataset [59], along with their own VAST dataset, specifically designed for the purpose of using a zero-shot technique for stance classification.They highlight the multitude of ways that stance characterization can be achieved in a zero-shot manner.Essentially, there are three primary paradigms for zero-shot stance detection: topic, language, and genre [6].For each paradigm, a model is trained via an out-of-domain setup: a model is trained on all data except for one element of the paradigm, which is reserved for the zero-shot test.For instance, in a zero-shot topic stance detection, the model is trained on data from all topics except one, which is then used for evaluation.All models perform less optimally in the zero-shot setting compared to a fully supervised setting [6].</p>
<p>Manuscript submitted to ACM</p>
<p>The use of Large Language Models for Stance Classification</p>
<p>Large Language Models (LLMs) have the capability to process and evaluate sentences of natural languages, and have been used for tasks ranging from reading comprehension to mathematical problem solving [67].These models are trained on an enormous amount of data and have an uncanny ability to evaluate sentences and respond to input prompts.Some research has emerged on using these models for the stance classification task.Table 1 lists a table of current works involving stance detection using LLMs.</p>
<p>Although a suite of LLMs has been developed, much of the current work has focused solely on the GPT family (i.e., ChatGPT, GPT3) [36], producing mixed results.[60] found that with just an instruction-based prompt (although they hint at the use of reasoning in a prompt in their paper), ChatGPT could produce better results on the SemEval2016 benchmark data set than supervised models.On the other hand, [1] examined the stance detection task using the ChatGPT model and observed that while there is a performance boost, there could be potential data contamination through its massive training dataset, therefore making the evaluation unreliable.Furthermore, [30] investigated the usability of ChatGPT as a zero-shot stance detection classifier, using only a task-based prompt for stance detection on a custom dataset for the topic of immigration in news articles for various languages.In their experiments, they found that ChatGPT performed close to the best supervised model, but was ultimately inferior for the stance classification problem.</p>
<p>Most recently, a series of papers have proposed different Chain-of-Thought style prompts for stance classification, generally showing superior performance when the reasoning abilities of LLMs are deliberately invoked by these prompting schemes [61,62].Finally, and most recently, [25] used a multistage Tree-of-Thought-like prompt that also employs embodiements to deliver state-of-the-art performance on a couple of benchmark stance datasets for both GPT and open-source models.Also, given the similarities between stance and sentiment classification, it is worth mentioning that [21] investigated several OpenAI model offerings on the sentiment classification task and found that GPT models, especially if fine-tuned, can outperform other models.Overall, it is unclear if LLMs can perform the task of stance classification, especially with some prompt engineering and without the use of the additional step of model fine-tuning.</p>
<p>LLMs take in an input, which is called a prompt, which provides instructions to the model to provide a certain output.With the prompt being a free text format, a new discipline of prompt engineering has come into being.Prompt engineering is a discipline that tries to find the right ways to provide input to the LLMs -or "prompt" them -to get the best outputs [38,41,54].While this is a fast changing discipline, a couple of patterns have emerged.The first is zero-shot prompting, where the LLM is provided only the task description and asked for the desired output.Next we have few-shot prompting, where the LLM is given a few examples of the desired output as part of the prompt [8,52,54].This is different from fine-tuning the LLM or low-shot learning, as no training (i.e.adjusting of model weights) is performed when the examples are given; the examples are only given as part of the context to aid understanding of the task [8].While this prompting technique consistently produces improved outputs, there are still possible instabilities in the technique that can be caused by things such as the order of the provided examples [29,66].Another prompting technique that has consistently improved the output of LLMs is Chain-of-Thought Reasoning [9,51].In this prompting scheme, the LLM is asked to explain its reasoning and to work through answering a prompt step by step.Answering prompts in such a process usually improves outcomes and helps to prevent undesirable behaviors like hallucination, where the model produces a plausible-looking answer that is incorrect [9,51].This technique has been used in an iterative chat format to improve implicit sentiment classification in previous research [15].Although there are certain Manuscript submitted to ACM</p>
<p>Work</p>
<p>Dataset Base LLM Prompting Scheme Aiyappa et al. [1] SemEval2016 [32] ChatGPT Task-based prompt with target Kheiri and Karimi [21] SemEval2017 GPT-3, GPT-3, GPT-3.5 Embodied task-based prompt with definitions and target prompt Lan et al. [25] SemEval2016 [32], VAST [5], wtwt [11] GPT-3.5</p>
<p>Embodied Tree-of-Thought prompt Liyanage et al. [27] Climate change GPT-4 Zero-shot and few-shot Chainof-Thought prompts Li et al. [26] SemEval2016 [32], P-Stance, COVID19</p>
<p>ChatGPT</p>
<p>Discourse expanding prompt and few-shot prompt Mets et al. [30] Immigration in news articles ChatGPT Task-based prompt Šuppa et al. [44] CASE2024 Shared Task Climate Activism Stance and Hate Event Detection [46] GPT-4, LLaMA Few-shot prompt Zhang et al. [60] SemEval2016 [32] ChatGPT instruction-based prompt Zhang et al. [61] SemEval2016 [32], VAST [5] GPT-3 Logically-Consistent Chain-of-Thought prompt Zhang et al. [62] SemEval2016 [32], VAST [5], P-Stance ChatGPT One-shot Chain-of-Thought prompt</p>
<p>Stance Classification for Social Media</p>
<p>Stance classification is often used for social media analysis studies.Social media platforms are a major component of a person's social interaction and are tools for dissemination of information to express opinions [2].Stance detection on social media provides a conduit for measuring public opinion, especially on controversial political and social issues.</p>
<p>Such issues can be topics like abortion and climate change [32], or support and opposition to political debates on online forums [33].Other analyses involving stances on social media include observing the change in stance toward topics such as mandatory vaccination [35], or the role of automated and malicious actors in polarizing stances on a certain topic [3].</p>
<p>Stance classification for social media texts is a complicated problem.There are two key challenges in classifying stance for social media texts: a general lack of human-annotated data and the huge volume of text present in any given social media space.Stance classification via LLMs can provide respite to analysts for the labeling process and could potentially be a zero-shot classification process and not require further model training or additional annotations.</p>
<p>DATA SETS</p>
<p>In this work, we used a total of six publicly available benchmark datasets: covid-lies [17], election2016 [43], phemerumors [22], SemEval2016 [32], srq [49], and wtwt [11].These six data sets have similar properties: they are constructed from sentences that are Twitter posts, are written in the English language, and were manually annotated by human labelers.</p>
<p>The targets of these data sets range from misconceptions to elections to a root tweet in a reply chain, which means the Manuscript submitted to ACM definition of stance varies between them.For example, in covid-lies and phemerumors the stance is about whether the statement supports or denies a rumor, while in SemEval2016 and election2016 the stance is about an opinion of the politically-divisive target.Table 2 lists the datasets that are used, the targets that are present in the data sets, and the highest unweighted F1 score accuracy reported as found in the original articles.We urge the reader to refer to the original dataset papers for a deeper understanding of the dataset contents.</p>
<p>Data</p>
<p>METHODOLOGY</p>
<p>In this section, we detail the methodology we used to classify the stances by LLMs.We first elaborate on the architecture and construction of LLMs used, before detailing the series of prompting techniques we tested in this work.We then described how we fine-tuned the LLMs for stance classification.All of the experiments were run on a computer with Ubuntu 22.04 Linux with x64 CPU with 40 cores, 376 GB of RAM and four NVIDIA A6000 GPUs.</p>
<p>Large Language Models Used</p>
<p>In this work, we tested 10 Large Language Models (LLMs).These models varied in architecture and sizes.We also tested the same model architecture with different sizes (e.g., Llama-2 13B Chat vs Llama-2 7B Chat) or pretrainings (e.g., Llama-2 7B Chat vs Llama-2 7B).We used the HuggingFace version1 of the LLMs.We selected open-source LLMs that can be reliably run on a high-end computer, mimicking how a social media analyst would run stance detection for social media datasets.The following are the LLMs that we tested throughout this study:</p>
<p>(1) Falcon 7B Instruct: [42] Falcon is a series of decoder-only models.Relative to other decoder-only models the Falcon series uses different architectures, including Flash Attention.This variant has 7 billion parameters, fine-tuned on instruction datasets.</p>
<p>(2) Falcon 40B Instruct: [42] This variant of the Falcon architecture has 40 billion parameters and is fine-tuned on instruction datasets.</p>
<p>(3) T5 3B Flan-Alpaca: [10] Flan-Alpaca combines two elements: the alpaca direction of fine-tuning to approximate LLMs for cheaper and easier training and the encoder-decoder T5 model.This variant has 3 billion parameters.</p>
<p>(4) T5 11B Flan-Alpaca: [10] This Flan-Alpaca tuned T5 variant has 11 billion parameters.</p>
<p>( For each of the models, we use greedy decoding to in order to get consistency across models and replicability in results.We also experimented with other forms of decoding, such as beam search and top-p, with Mistral and Llama models on the election and SemEval2016 datasets, but did not find major performance differences between what these models produced with just greedy decoding.</p>
<p>Prompting Schemes for Stance Classification</p>
<p>In order to investigate the use of LLMs and prompting for the task of stance classification, we used seven different prompting schemes.The prompting schemes are generally hierarchical so that each prompting scheme incorporates more information from the previous scheme.Figure 1 displays the general, overarching prompting scheme and the elements available to the LLMs within each individual prompting scheme.Actual examples of each prompt scheme are provided in the Appendix A.</p>
<p>The following list details each of the prompting schemes we used to classify stance for each of the data sets.In a stance detection task, the input sentence comes with a target to which it is expressing opinion towards and stance options.Table 3 describes the different classification outcomes of the datasets used in this study.The core text of the prompts are tweaked towards the properties of each dataset.</p>
<p>(1) Task-only: In the task-only prompt, we adopt a zero-shot learning prompting method, providing only the task with no additional detail (e.g."Analyze the following statement and determine its stance.").For this prompt, we provide different classification outcomes depending on the data set Table 3), but they generally follow a 'agree', 'disagree', or 'neutral'// 'unrelated' format, with a final output instruction to only output the label.Thus, the remainder of the prompt is: Respond with a single word: "for", "against", "neutral", or "unrelated".Only return the stance as a single word, and no other text.</p>
<p>(2) Task Definition: In the task definition prompt, we adopt the zero-shot learning prompting method, but now provide the task and the definition of 'stance classification' (e.g."Stance classification is the task of determining the expressed or implied opinion, or stance, of a statement toward a certain, specified target.Analyze the following statement and determine its stance.")similar to what was done in [30].The outcome labels and output instructions remain the same as in the task-only prompt.</p>
<p>(  ...").We keep the definition of stance classification as previously given in the task definition prompt.This prompting scheme is what the previous works that investigated using ChatGPT for stance labeling used [1,30,60].</p>
<p>The outcome labels and output instructions remain the same as in the task-only prompt.</p>
<p>(4) Context Question: This prompting scheme is the same as the previous one, but with one variation in the task part of the prompt.Instead of phrasing the task as a statement with 'analyze', we instead phrase it as a question.</p>
<p>In our review of the literature, we noticed several ways of prompting the LLM for stance some of which included prompting by questioning [60].In this prompting scheme we change the task sentence to "... What is the stance of the following social media statement towards the following [target]?...".Other than this change, all other aspects of the prompt remain the exact same as those of the context analyze prompt.</p>
<p>(5) Zero-shot CoT: For this prompting scheme, we adopt a Chain of Thought prompt to elicit reasoning from the LLM as was done in [27,62].In this case, there are two prompts, one to have the LLM reason about the stance and for LLM to evaluate its reasoning and then make a final stance determination.The reasoning prompt is given by "Stance classification is the task of determining the expressed or implied opinion, or stance, of a statement toward a certain, specified target.Think step by step and explain the stance (for, against, neutral, or unrelated) of the social media statement towards the Manuscript submitted to ACM [target]."The results of this prompt are then fed into the final stance determination prompt as: "Therefore, based on your explanation, [reason prompt results], what is the final stance?... with the same outcome labels and output instructions as in the task-only prompt.</p>
<p>(6) CoDA: Finally, we also try the role-infused collaborative agents prompt (CoDA) presented in [25].For this prompting scheme, there are six total prompts: The first three prompts are for the analyses by the different embodiments (i.e., linguistics expert, subject matter expert, and heavy social media user).Then there are two prompts for evaluating the experts and forming arguments; one prompt for forming the positive or for argument, and one prompt for forming the against argument.Then, finally, there is one last evaluation prompt that evaluates the arguments and outputs the final stance label.As with the previous prompts, the output labels and output instructions remain the same.However, unlike the previous prompts, this prompt does not use the definition of stance as part of the prompt.In general, this prompting scheme combines elements of Tree-of-Thought prompting [57], by positing arguments or ideas and then evaluating those arguments or ideas, with embodied prompting where the LLM is given a role to play or embody.3. Summary of the prompt differences between each of the benchmark data sets.For each data set, we used slightly different target and stance label options in order to accommodate the different purposes of stance classification between the data sets.</p>
<p>Data Set</p>
<p>Fine-Tuning an LLM for Stance Classification</p>
<p>A popular way of adapting language models to tasks is through the fine-tuning process, which molds these models into specialized ones to analyze niche topics and achieve better performance [13,18].To investigate whether a specialized model can perform better for the stance classification task, we performed fine-tuning on some exemplar LLMs.Due to computational resource limitations, we selected representative LLMs of each type and of different sizes to fine-tune.For encoder-decoder models, we used: T5 11B Flan-Alpaca and UL2 20B Flan.For decoder-only models, we used: Mistral v0.1 7B Instruct, Llama-2 7B and Llama-2-7B-Chat.</p>
<p>To fine-tune the models, we opted to use the parameter-efficient method of LoRA [18].We used a parameter-efficient technique (versus full fine-tuning) as we only have a handful of benchmark datasets to train on for this task, and LLMs have a massive number of parameters.Furthermore, recent research has found LoRA to be more stable in fine-tuning and capable of providing good fine-tuning results in scenarios with limited training data [63].Specifically, we follow the guidance set out in [18] and use a LoRA adapter on all the LLM attention layers with default parameters of  = 16,  = 1, and a LoRA dropout of 0.1.We trained these models with a learning rate of 1 − 5 with a linear learning rate decay and 100 warmup steps, for a maximum of three epochs, using the AdamW optimizer [28] and mixed precision training [31].For the training task, we evaluated whether the model outputs the correct label, in a casual language modeling scenario.In other words, the model could output any tokens, but would only be correct if output the tokens for the correct stance classification and only those tokens.</p>
<p>Manuscript submitted to ACM</p>
<p>We fine-tuned the models using a leave-one-out approach across the datasets.This tests how well the model performs on out-of-domain datasets, which is critical for the stance classification task.This setup also mimics the zero-shot models, which are not specialized for the stance classification task nor its datasets, and performs classification based on pre-trained knowledge only.This idea is adopted from previous work that demonstrated that aggregating multiple stance datasets improves out-of-context stance detection, since the model has more data to work with creating a generalized stance classifier [34].Therefore, we iteratively hold out one dataset as the test dataset, and we fine-tuned the LLM on all the other datasets except the test dataset.We allowed the models to stop training when they had minimzed their loss on the held-out dataset (early stopping with 5 steps as patience).Then we perform stance detection on the test dataset and we report the F1 accuracy score on the held-out dataset.In this way, we can get a measure of how well fine-tuning works and make a comparison between fine-tuned models to simply prompting a pre-trained LLM.We encapsulated each datum in the Context Analyze prompt and fed the encapsulated datum into the model during the fine-tuning stage.The same prompt format was used to label the held-out test dataset after fine-tuning.</p>
<p>Evaluation Metrics</p>
<p>Each of the datasets have different sets of labels of stances.To make the evaluation comparable across the datasets, we followed the same data set handling procedures described in [34] and standardized the stance labels for evaluation.</p>
<p>This standardization step maps the set of stance labels for each dataset to a common set of {agree, neutral, disagree} labels.Note that the stance labels are not standardized in the prompts during the experimental stage, because each dataset is constructed to elicit the opinions towards different targets and we aim to preserve the definition of stance towards each of the varied targets.</p>
<p>For each response obtained from the LLMs, we further post-process the response.This is required because, despite instructing the models to only return a single stance label, the models sometimes return extra text, including explanations.</p>
<p>We construct lists of common labels of {agree, disagree, neutral} that the models return for each stance and match them within the returned response.If the returned response contains words from only one list, that data point is classified as that stance category.If more than one category word is detected from multiple categories or the model fails to output a stance label, the stance of the data point is labeled as "neutral".The data points that have a valid stance label as previously described (i.e., those with one and only one stance label present in the output) are termed as "good results", while those that have no valid stance are termed as "bad results".For example, 'agree' and 'the stance is agree' are valid labels and thus are good results, whereas 'unconfirmed' and 'agree, neutral, disagree' are not.We report the aggregated F1 scores of all data points as they are resolved, and also present the breakdown between good and bad results.</p>
<p>For evaluation of the stance detection task against the manually annotated gold labels, we report the unweighted, macro-F1 accuracy metric, in line with previous work [32].The macro-F1 score adjusts for the proportion of each class label type (i.e., stance label), for there is an imbalance of class labels in some of our datasets.We use the accuracy scores of the original dataset models as a baseline comparison.</p>
<p>RESULTS</p>
<p>In this section, we present the results of different prompting schemes and LLM combinations, the results of the finetuning models and observations on the quality of the LLM responses.We present a graphical visualization of all these results in Appendix B.</p>
<p>Manuscript submitted to ACM</p>
<p>Results of LLM &amp; Prompting Schemes Combinations</p>
<p>Table 4 presents the results of the combinations of LLMs, prompting schemes and datasets in terms of the unweighted F1-scores.From our results, we observe that while some combinations of LLMs and prompting schemes outperform the baselines, not all combinations consistently outperform the baselines.This speaks to the inconsistency and random nature of LLM inference, which can reduce the consistency and performance of using LLM as a stance classification technique [55].</p>
<p>In general, Few-Shot Prompting (FSP) and Zero-Shot CoT (CoT) perform better, as do the encoder-decoder family of models.Unfortunately, there is no one LLM that consistently outperforms across all the datasets, nor is there an LLM that consistently outperforms all other models regardless of prompting schemes in a single dataset.Similarly, there is no one prompting scheme that outperforms all other prompting schemes in this stance detection task.In addition, a larger model size does not necessarily mean that the model performs better.For example, Falcon 40B Instruct vs. Falcon 7B Instruct with the FSP scheme shows a performance of 0.25 against 0.35.</p>
<p>LLM/Prompting</p>
<p>Results of Fine-Tuning LLMs</p>
<p>Table 5 shows the results of fine-tuning selected LLMs on the datasets.In general, fine-tuning does not perform better than zero-shot LLMs for out-of-domain data.In fact, in most cases, LLMs performed worse compared to the zero-shot LLM setup and even the baselines.For example, the fine-tuned models tended to be much worse than zero-shot models on SemEval2016 and election2016, but better on covid-lies and phemerumours.This result may be due to how stance was operationalized differently between different datasets [34].It could also be due to the fact that fine-tuning could decrease the generalizability of the LLMs, as observed in past work, which includes fine-tuned LLMs for domains such as code completion [23,58,63].Cruickshank et al
Dataset</p>
<p>Evaluation of Valid Responses from Models</p>
<p>We find that there are a number of completions in which the LLM does not give a valid stance label in response to a prompt.With this, we find that once again the encoder-decoder architectures and FSP and CoT prompting schemes tend to produce more valid completions and are more accurate when they do produce a valid completion.Along with that, even though all prompts explicitly tell the LLM to output a single label, many completions have longer text than just a single label.</p>
<p>Table 6 shows the accuracy scores of the good results derived from the zero-shot LLMs.Table 7 shows the accuracy scores of the good results from the fine-tuned LLMs.We observe that the accuracy scores for the good results consistently and significantly outperform the baseline supervised approaches.However, when there is ambiguity in the response, the model performance drops drastically.This shows that the output response is an indicator of model performance in the task of stance detection on any given example.We observe that FSP and CoT have the best overall scores, suggesting the superiority of these prompting methods.</p>
<p>Furthermore, we analyzed the response lengths of the models.Despite being prompted to only return a single stance label, many models returned additional words.Table 8 presents the average response length of the models.</p>
<p>The encoder-decoder family of models faithfully follow this instruction by only outputting a single word for stance.</p>
<p>However, Llama-2 models tend to produce longer responses.Doing a correlation analysis, we find that having a longer text output weakly correlates to worse stance predictions.For example, we find that when we look at good labels, the correlation between the length of the output and whether the output is the correct stance label is −0.02 ( &lt; 0.001).</p>
<p>With these observations, we constructed a simple decision tree model using the Python sklearn package to test whether we can predict if the LLM had predicted the stance correctly.Using the raw output length, number of non-stance words in the output and whether the output contains a valid stance label as independent variables, we can predict with 0.59 accuracy whether the LLM predicted the stance correctly, regardless of what input it was fed.In other words, there is a distinct relation between the quality of the output in terms of producing a valid stance label and following the output instructions, and whether the LLM was correct in predicting the stance.</p>
<p>LLM/Prompting</p>
<p>DISCUSSION</p>
<p>In this section we discuss the findings of the various tests we used to investigate the use of LLMs for the task of stance classification.</p>
<p>LLMs are Competitive with In-Domain Supervised Models but have Inconsistent Performance.We evaluated open-source LLMs because these are the most cost-effective LLMs and thus are likely to have higher usage by analysts.</p>
<p>Furthermore, previous research has highlighted that there is likely contamination of data with GPT models that could skew the results [1].In our analysis, we observe that LLMs do not consistently outperform their baseline cousins.We do find that encoder-decoder LLM architectures tend to perform better as compared to the decoder-only architectures.We also find that a larger model size does not necessarily lead to an improvement in performance, hence we suggest that while using LLMs for stance detection, an appropriately chosen smaller-sized LLM could suffice, which can alleviate the computational burden for inference.Furthermore, given the inconsistent performance of LLMs in the task, we also recommend following something like a Data Programming paradigm [39] to use LLM generated labels in combination with other signals of stance to classify the stance of statements.</p>
<p>During our testing, we also found that there are a number of outputs where an LLM does not give a valid stance label in response to a prompt.We find that encoder-decoder based architectures together with FSP and CoT prompting schemes tend to produce more valid results.This reflects the instability of LLMs, in which they do not always produce the desired output even though we had specified the output format.This observation could also be a result of using a completion format as a prompt, in which the model takes it as a paragraph or sentence completion task after processing the information given prior in the prompt.Where there is a valid completion, the models perform generally well, and often better than in-domain supervised models.However, performance drops when there is ambiguity in the output; therefore, indicating the formatting of the output response is crucial to achieving performance with LLMs.Downstream</p>
<p>Manuscript submitted to ACM work can facilitate the crafting of prompt templates or the tweaking of LLMs architecture to produce consistent and valid outputs.</p>
<p>Importance of Prompting Scheme Development.The testing of an array of prompting schemes reveals the importance of the input prompt into the LLMs.The performance of any given LLM varies depending on the prompt format, suggesting that the prompting scheme and its formatting are crucial to eliciting good performance from the LLM.In general, the use of FSP and CoT prompting improves performance of the model, however, prompting schemes can also be model dependent.There are instances that the prompt does improve performance of the LLM over the baselines which use traditional machine learning methods.The baseline methods do not account for the target the sentence is directed towards, suggesting that the insertion of target and additional contextual information improves the performance.</p>
<p>Fine-Tuning Does not Necessarily Lead to Better Performance.Our results find that fine-tuning the LLMs to create a specialized model does not lead to better performance of the models.This is likely because the fine-tuning makes the model too specialized, and thus unable to generalize to out-of-domain data points and varying defintions of stance.Previous studies also discover that fine-tuning does not work well when the size of the model exceeds the size of the fine-tuning data, especially when working with out-of-domain data [23,63].In our case, we only have a few thousand data points against model sizes in billions.Therefore, a fine-tuned LLM is not as desirable as a zero-shot LLM, which can generalize better.</p>
<p>Stance detection as an LLM Benchmark.LLMs are built and assessed on a range of benchmarks, from common sense reasoning, reading comprehension, to mathematical reasoning [67].However, one language task not represented in current LLM benchmarking is the task of stance classification.This task is likely overlooked because it is not a task as part of the standard natural language processing evaluation playbook.The task of stance classification is, however, a crucial benchmark to investigate, as many policy formulations depend on understanding public opinion, for example [48].Misclassification of sentence stances can lead to incorrect interpretations of opinion slant.Stances, especially stances in social media posts towards a target, are used in downstream analyses that evaluate aggregated stances towards topics to understand public reaction and formulate policy.In those scenarios, incorrect stance classifications can result in erroneous interpretations and policy mismatches [7].</p>
<p>Given the importance of stance classification to the wider society and the results of this study, we would like to propose that stance classification be considered as a future benchmarking task for LLMs.</p>
<p>Limitations.</p>
<p>As in all studies, several limitations nuance our work.For our stance detection task, our data sets are premised on manual annotations, which could be subjected to inconsistent annotations and varied sentence interpretations [34].For example, there is a sentence about Michael Essien having Ebola, "@xx no he hasn't.The man himself confirmed not true @MichaelEssien" that was annotated as a neutral stance whereas it should be a stance against the claim that Michael Essien had contracted Ebola.</p>
<p>Next, in all research involving LLMs, a primary concern arises from the datasets used to pre-train LLMs, from which they acquire their language capabilities and knowledge base.These datasets may harbor inherent biases or offensive content [40].Although this study makes no attempt to exploit or introduce any form of bias, it is possible that these biases might inadvertently permeate the analysis performed using LLMs, especially when performing politicallycharged tasks like stance detection.This underlines the importance of diligently scrutinizing the data used to train Manuscript submitted to ACM LLMs, especially when they are employed in socially significant tasks such as stance classification, where bias can have profound implications.</p>
<p>Additionally, while we attempted to consider a wide range of open-source models and possible configurations for those models, we were constrained by computational resources and time from using every possible permutation of open-source models for the task of stance classification.We believe that we have tested a representative sample of offerings, however, it is possible that a certain LLM, perhaps due to pre-training data or even architectural differences, may actually perform uncharacteristically in regards to the models tested in this study.Additionally, we also only used greedy decoding for the LLMs and manual selection of examples for the FSP prompts.Its possible that different decoding techniques or a more optimal example selection for FSP (e.g., [29]) could have produced better results.</p>
<p>Ethical Considerations.Lastly, using LLMs for stance detection at scale presents an ethical consideration that pertains to the environmental impact of running these computationally intensive models.It is indisputable that LLMs consume more energy compared to their smaller counterparts.Thus, their use in tasks like stance classification is associated with a tangible energy cost.However, it is also crucial to balance this against the alternative scenario, which involves continuous human effort for labeling data, a process that is both labor-intensive and time-consuming due to the generalizability problem inherent in stance classification.</p>
<p>Looking ahead, as we strive to create more sustainable and efficient language models, one potential avenue could be leveraging LLMs to distill smaller, more energy-efficient models for production purposes.This could significantly decrease the energy demand, making stance classification tasks more environmentally sustainable, while still benefiting from the superior performance of LLMs.As our understanding of LLMs continues to evolve, it is paramount to remain vigilant about these ethical considerations and strive towards more responsible and sustainable practices.</p>
<p>Additionally, as with any classification effort of text, efforts towards stance detection could be used for text-based censorship.For example, it is possible that our research could be used to identify, at scale, comments and users that a presenting a certain stance toward a target and then moderate those users or comments, thus maintaining a harmonious digital society.While there can be misuse towards using stance detection models to identify and retain toxic comments rather than remove them, we believe that the benefits of being able to more correctly classify stances of text comments outweigh its potential misuse, and that the same precautions used to prevent misuse with the classification of texts more broadly can also be applied to this work.</p>
<p>CONCLUDING REMARKS</p>
<p>Stance classification is a tricky problem.It requires natural language understanding to sieve out the opinion of the author towards a target, and involves many language nuances and subtleties, and is context dependent.Through this paper, we shed light on the stance classification abilities of LLMs, contributing valuable insights that can guide future advancements in this domains.From a series of extensive experiments with stance detection using a large series of Large Language Models, we observed the behavior and performance of LLMs on classifying the stance of social media datasets.We summarize our observations as follows:</p>
<p>(1) Large Language Models do not consistently outperform supervised machine learning models for the stance classification task.They are sensitive to the input instructions, therefore the formulation of a prompting scheme for input is crucial.</p>
<p>Manuscript submitted to ACM</p>
<p>(2) Performance accuracy for stance classification varies considerably across models and prompts.At the current stage of LLM development, we suggest to use encoder-decoder based model architectures together with FSP or</p>
<p>CoT prompting schemes for best performance.</p>
<p>(3) Fine-tuning LLMs does not necessarily improve the out-of-domain performance of the model, and can in fact worsen it.Thus, a more generalized model is preferred over a specialized one.</p>
<p>The application of LLMs on the stance detection task not only streamlines the process but also paves the way for expanding stance detection capabilities across languages and domains.Given our observation that LLMs do sometimes perform better than the baseline supervised models, there is hope of improvement of stance classification capabilities of LLMs.As stance detection is a crucial task in many workstreams that analyze the opinion of the public, we urge the world of natural language processing to join forces and build a set of practices for the use of LLMs in analyzing social data like in this stance detection problem.Such sets of practices should reduce inconsistencies between and within models, improving the usage of the models in downstream tasks.</p>
<p>A EXAMPLES OF PROMPTING SCHEMES</p>
<p>Table 9 provides examples of the prompts constructed as inputs to the LLMs.</p>
<p>Task-only "'Analyze the following statement and determine its stance.</p>
<p>Respond with a single word: "for", "against", "neutral", or "unrelated".</p>
<p>statement: {statement}</p>
<p>Arguments that the attitude is in favor: {for_opinion}</p>
<p>Arguments that the attitude is against: {against_opinion} Choose the stance from "for", "against", "neutral", or "unrelated".</p>
<p>Answer with only the option above that is most accurate and nothing else.stance:"'</p>
<p>Manuscript submitted to ACM</p>
<p>B GRAPHICAL RESULTS</p>
<p>This section presents a graphical view of the results in terms of heatmap.Figure 2 presents the accuracy scores of all the results.Figure 3 presents the proportion of outputs that returns a valid stance prediction.Figure 4 presents the scores where the LLM correctly predicted the stance, given it had a valid stance.Manuscript submitted to ACM</p>
<p>)( 6 )( 7 )( 8 )( 9 )( 10 )
678910
Mistral v0.1 7B Instruct:[19] Mistral is a decoder-only model, similar to Falcon or Llama-2, but with different architectural features like Grouped-Query Attention and Sliding Window Attention.The Mistral model is 7 billion parameters and has been instruction tuned.UL2 20B Flan:[45] UL2 20B Flan is an encoder-decoder model of 20 billion parameters based on the T5 architecture and is instruction tuned.Llama-2 13B Chat:[47] Llama-2 is a decoder-only model that incorporates elements such as a different token encoding model and Flash Attention 2. This version of the Llama series of models has 13 billion parameters and has been tuned on chat datasets.Llama-2 7B Chat:[47] This version of the Llama-2 architecture has 7 billion parameters and is optimized for dialogue use cases.Llama-2 7B:[47] This version of the Llama-2 architecture has 7 billion parameters, but has not been further tuned beyond pre-training of the model.Phi-2:[16] Designed by Microsoft, Phi-2 is a decoder-only transformer-based LLM with 2.7 billion parameters.It is trained on Python codes, synthetic texts and website content.</p>
<p>)</p>
<p>Context Analyze: In this prompting scheme, we add contextual information about what the statement is and the target of the stance classification to the task of classifying the stance of the statement (e.g."... Analyze the following social media statement and determine its stance towards the provided [target].Manuscript submitted to ACM</p>
<p>Fig. 1 .
1
Fig. 1.Overarching Prompting Scheme for Stance Classification Text highlights indicate the information available for each of the prompting schemes.Black text indicates the basic task-only prompt, whose elements are common to all of the other prompts.Blue indicates the addition of the task definition.Yellow indicates the addition of context, whereas the orange color is the context question variation of the context prompt.Green indicates the prompt additions from few-shot prompting.Finally, red indicates the additional reasoning or framing prompts introduced in prompting methods like CoT and CoDA that attempt to elicit preliminary reasoning about the statement before trying to classify the stance of the statement.Those words bracketed in square brackets are dataset dependent, while those bracketed in curly braces are instance-dependent.</p>
<p>Few-shot</p>
<p>Prompt (FSP) "'Stance classification is the task of determining the expressed or implied opinion, or stance, of a statement toward a certain, specified target.The following statements are social media posts expressing opinions about entities.Each statement can either be for, against, neutral, or unrelated toward their associated entity.entity: Atheism statement: Leaving Christianity enables you to love the people you once rejected.#freethinker #Christianity #SemST stance: for entity: Climate Change is a Real Concern statement: AlharbiF I'll bomb anything I can get my hands on, especially if THEY aren't christian.#graham2016 #GOP #SemST stance: neutral entity: Feminist Movement statement: Always a delight to see chest-drumming alpha males hiss and scuttle backwards up the wall when a feminist enters the room.#manly #SemST stance: for entity: Hillary Clinton statement: Would you wanna be in a long term relationship with some bitch that hides her emails, &amp; lies to your face?Then #Dontvote #SemST stance: against entity: Legalization of Abortion statement: k_yoder That lady needs help, mental illness is a serious issue.#SemST stance: neutralAnalyze the following social media statement and determine its stance towards the provided entity.Respond with a single word: "for", "against", "neutral", or "unrelated".Only return the stance as a single word, and no other text.classification is the task of determining the expressed or implied opinion, or stance, of a statement toward a certain, specified target.Think step-by-step and explain the stance (for, against, neutral, or unrelated) of the social media statement towards the entity., based on your explanation, {stance_reason}, what is the final stance?Respond with a single word: "supports", "denies", "neutral", or "unrelated".Only return the stance as a single word, and no other text.and concisely explain the linguistic elements in the statment and how these elements affect meaning, including grammatical structure, tense and inflection, virtual speech, rhetorical devices, lexical choices and so on.Do nothing else.statement: {statement} explanation:"' LLM gives linguist response "'Accurately and concisely explain the key elements contained in the following statement, such as characters, events, parties, religions, etc.Also explain their relationship with {event}.Do nothing else.statement: {statement} explanation:"' LLM gives expert response "'Analyze the following statement, focusing on the content, hashtags, Internet slang and colloquialisms, emotional tone, implied meaning, and so on.Do nothing else.statement: {statement} explanation:"' LLM gives social media user response "'You think the attitude behind the following statement is in support of {event}.Following the statement are explanations of the statement by various personas.Identify the top three pieces of evidence from these that best support your opinion and argue for your opinion.statement: {statement} linguist explanation: {linguist_analysis} expert explanation: {expert_analysis} heavy social media user explanation: {user_analysis} opinion:"' LLM gives 'for' argument "'You think the attitude behind the following statement is against {event}.Folloiwng the statement are explanations of the statement by various personas.Identify the top three pieces of evidence from these that best support your opinion and argue for your opinion.statement: {statement} linguist explanation: {linguist_analysis} expert explanation: {expert_analysis} heavy social media user explanation: {user_analysis} opinion:"' LLM gives 'against' argument "'Determine whether the following statement is in favor of, neutral, against, or unrelated to {event}.</p>
<p>Fig. 2 .
2
Fig. 2. Graphical Representation of all results by F1 score accuracy</p>
<p>Fig. 3 .Fig. 4 .
34
Fig. 3. Graphical presentation of the proportion of ooutputs that returns a stance prediction</p>
<p>Table 1 .
1
Current Work in Stance Detection using Large Language Models prompting techniques that can elicit better outcomes from LLMs, the best means of interacting with the LLMs, especially in the domain of stance detection, is still an open research question.</p>
<p>Table 2 .
2
Summary of data sets used with our descriptions of the events and best-reported unweighted, macro F1-score from the original data set.</p>
<p>Table
Stance OptionsTargetcovid-liessupports, denies, neutral, unrelatedbelief about COVIDelection2016for, against, neutral, unrelatedpoliticianphemerumors supports, denies, neutral, unrelatedrumorSemEval2016for, against, neutral, unrelatedentitysrqsupports, denies, neutral, unrelatedsocial media statement about an eventwtwtsupports, denies, neutral, unrelatedcorporate merger happening</p>
<p>Table 4 .
4
Average unweighted F1-scores.The benchmark results are given in the first row from the original paper.The highest scoring result for each data set is in bold.Note that we were unable to run the CoDA architecture on the larger prompts.</p>
<p>Table 5 .
5
Summary of Results of Fine-Tuning selected LLMs.The dataset tested on is the out-of-domain data, while the models were fine-tuned with all other data.</p>
<p>Table 6 .
6
F1 score of accuracy of 'good outputs' of zero-shot LLMs
Scheme Task-Only Task Definition ContextContextFew Shot Zero-shotCODAAnalyzeQuestionCoTLlama-2 13B Chat0.830.810.570.700.990.840.74Llama-2 7B Chat0.900.720.890.700.990.910.72Llama-2 7B0.400.560.760.570.330.68Mistral v0.1 7B Instruct0.990.990.990.990.990.990.81Falcon 40B Instruct0.990.990.990.990.990.84Falcon 7B Instruct0.960.940.950.960.880.960.84T5 3B Flan-Alpaca0.990.990.990.990.990.990.99T5 11B Flan-Alpaca0.990.990.99100100100100UL2 20B Flan1001001001001001000.99Phi-20.740.610.870.710.580.670.37Manuscript submitted to ACM</p>
<p>Table 7 .
7
F1 score of accuracy of the 'good outputs' of the fine-tuned models
ModelAvg Response LengthFalcon 40B Instruct9.59Falcon 7B Instruct1.60T5 3B Flan-Alpaca1.0T5 11B Flan-Alpaca1.0Llama-2 13B Chat11.65Llama-2 7B Chat7.40Llama-2 7B20.97Mistral v0.1 7B Instruct 2.71UL2 20B Flan1.0Phi-222.05</p>
<p>Table 8 .
8
Average response length of models by the number of words the model outputs for a stance label</p>
<p>Table 9 .
9
Examples of prompts constructed for each prompting scheme.These examples are constructed for the SemEval2016 dataset.</p>
<p>https://huggingface.co/ Manuscript submitted to ACM
Manuscript submitted to ACM
ACKNOWLEDGMENTSThe research for this paper was supported in part by the Center for Informed Democracy and Social-cybersecurity (IDeaS) and the Center for Computational Analysis of Social and Organizational Systems (CASOS) at Carnegie Mellon University.This work was also conducted within the Cognitive Security Research Lab at the Army Cyber Institute at West Point and supported in part by the Office of Naval Research (ONR) under Support Agreement No. USMA 20057.The views and conclusions are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Department of Defense, the U.S. Army, or the U.S. Government.
Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn, arXiv:2303.12767Can we trust the evaluation on ChatGPT?. 2023. 2023arXiv preprint</p>
<p>Stance detection on social media: State of the art and trends. Abeer Aldayel, Walid Magdy, Information Processing &amp; Management. 581025972021. 2021</p>
<p>Characterizing the role of bots' in polarized stance on social media. Abeer Aldayel, Walid Magdy, Social Network Analysis and Mining. 12302022. 2022</p>
<p>Capturing stance dynamics in social media: open challenges and research directions. Rabab Alkhalifa, Arkaitz Zubiaga, International Journal of Digital Humanities. 32022. 2022</p>
<p>Zero-shot stance detection: A dataset and model using generalized topic representations. Emily Allaway, Kathleen Mckeown, arXiv:2010.036402020. 2020arXiv preprint</p>
<p>Zero-shot stance detection: Paradigms and challenges. Emily Allaway, Kathleen Mckeown, Frontiers in Artificial Intelligence. 510704292023. 2023</p>
<p>A systematic review of machine learning techniques for stance detection and its applications. Nora Alturayeif, Hamzah Luqman, Moataz Ahmed, Neural Computing and Applications. 352023. 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>When do you need Chain. Jiuhai Chen, Lichang Chen, Heng Huang, Tianyi Zhou, arXiv:2304.032622023. 2023of-Thought Prompting for ChatGPT? arXiv preprint</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022. 2022arXiv preprint</p>
<p>Will-They-Won't-They: A Very Large Dataset for Stance Detection on Twitter. Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, Nigel Collier, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Unsupervised user stance detection on Twitter. Kareem Darwish, Peter Stefanov, Michaël Aupetit, Preslav Nakov, Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social Media202014</p>
<p>Parameter-efficient fine-tuning of large-scale pre-trained language models. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Nature Machine Intelligence. 52023. 2023</p>
<p>Cu-gwu perspective at semeval-2016 task 6: Ideological stance detection in informal text. Heba Elfardy, Mona Diab, Proceedings of the 10th International Workshop on Semantic Evaluation. the 10th International Workshop on Semantic Evaluation2016. SemEval-2016</p>
<p>. Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, Tat-Seng Chua, arXiv:2305.112552023. 2023Reasoning Implicit Sentiment with Chain-of-Thought Prompting. arXiv preprint</p>
<p>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, arXiv:2306.11644Textbooks Are All You Need. 2023. 2023arXiv preprint</p>
<p>COVIDLies: Detecting COVID-19 Misinformation on Social Media. Tamanna Hossain, Robert L Logan, I V , Arjuna Ugarte, Yoshitomo Matsubara, Sean Young, Sameer Singh, Proceedings of the 1st Workshop on NLP for COVID-19. the 1st Workshop on NLP for COVID-192020EMNLP 2020</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021. 2021arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>Knowledge Enhanced Masked Language Model for Stance Detection. Kornraphop Kawintiranon, Lisa Singh, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Kiana Kheiri, Hamid Karimi, arXiv:2307.10234SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. 2023. 2023arXiv preprint</p>
<p>All-in-one: Multi-task Learning for Rumour Verification. Elena Kochkina, Maria Liakata, Arkaitz Zubiaga, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational Linguistics2018</p>
<p>Fine-tuning can distort pretrained features and underperform out-of-distribution. Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang, arXiv:2202.100542022. 2022arXiv preprint</p>
<p>Stance evolution and twitter interactions in an italian political debate. Mirko Lai, Viviana Patti, Giancarlo Ruffo, Paolo Rosso, International Conference on Applications of Natural Language to Information Systems. Springer2018</p>
<p>Stance detection with collaborative role-infused llm-based agents. Xiaochong Lan, Chen Gao, Depeng Jin, Yong Li, arXiv:2310.104672023. 2023arXiv preprint</p>
<p>Stance Detection on Social Media with Background Knowledge. Ang Li, Bin Liang, Jingqian Zhao, Bowen Zhang, Min Yang, Ruifeng Xu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>GPT-4 as a Twitter Data Annotator: Unraveling Its Performance on a Stance Classification Task. Chandreen Liyanage, Ravi Gokani, Vijay Mago, Authorea Preprints. 2023. 2023</p>
<p>. Ilya Loshchilov, Frank Hutter, arXiv:1711.051012017. 2017Decoupled weight decay regularization. arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021. 2021arXiv preprint</p>
<p>Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media. Mark Mets, Andres Karjus, Indrek Ibrus, Maximilian Schich, arXiv:2305.130472023. 2023arXiv preprint</p>
<p>. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, arXiv:1710.037402017. 2017Mixed precision training. arXiv preprint</p>
<p>Semeval-2016 task 6: Detecting stance in tweets. Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, Colin Cherry, Proceedings of the 10th international workshop on semantic evaluation. the 10th international workshop on semantic evaluation2016. SemEval-2016</p>
<p>Support or oppose? Classifying positions in online debates from reply activities and opinion expressions. Akiko Murakami, Rudy Raymond, Coling 2010: Posters. 2010</p>
<p>Is my stance the same as your stance? A cross validation study of stance detection datasets. Lynnette Hui, Xian Ng, Kathleen M Carley, Information Processing &amp; Management. 591030702022. 2022</p>
<p>Pro or Anti? a social influence model of online stance flipping. Lynnette Hui, Xian Ng, Kathleen M Carley, IEEE Transactions on Network Science and Engineering. 102022. 2022</p>
<p>STEM: unsupervised structural embedding for stance detection. Ron Korenblum Pick, Vladyslav Kozhukhov, Dan Vilenchik, Oren Tsur, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Sunil Ramlochan, What is Prompt Engineering. 2023</p>
<p>Data programming: Creating large training sets, quickly. Christopher M De Alexander J Ratner, Sen Sa, Daniel Wu, Christopher Selsam, Ré, Advances in neural information processing systems. 292016. 2016</p>
<p>Inside the Secret List of Websites that make AI like ChatGPT Sound Smart. Kevin Schaul, Szu Yu Chen, Nitasha Tiku, 2023</p>
<p>. Jesse Douglas C Schmidt, Quchen Spencer-Smith, Jules Fu, White, n. d.</p>
<p>Cataloging Prompt Patterns to Enhance the Discipline of Prompt Engineering. </p>
<p>Fast transformer decoding: One write-head is all you need. Noam Shazeer, arXiv:1911.021502019. 2019arXiv preprintManuscript submitted to ACM</p>
<p>A dataset for multi-target stance detection. Parinaz Sobhani, Diana Inkpen, Xiaodan Zhu, Proceedings of the 15th Conference of the European Chapter. Short Papers. the 15th Conference of the European Chapterthe Association for Computational Linguistics20172</p>
<p>Marek Šuppa, Daniel Skala, Daniela Jašš, Samuel Sučík, Andrej Švec, Peter Hraška, arXiv:2402.06549Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA. 2024. 2024arXiv preprint</p>
<p>Yi Tay, Mostafa Dehghani, Xavier Vinh Q Tran, Dara Garcia, Tal Bahri, Huaixiu Schuster, Neil Steven Zheng, Donald Houlsby, Metzler, arXiv:2205.05131Unifying language learning paradigms. 2022. 2022arXiv preprint</p>
<p>Stance and Hate Event Detection in Tweets Related to Climate Activism -Shared Task at CASE 2024. Surendrabikram Thapa, Kritesh Rauniyar, Ahmad Farhan, Shuvam Jafri, Hariram Shiwakoti, Raghav Veeramani, Guneet Jain, Ali Singh Kohli, Usman Hürriyetoğlu, Naseem, Proceedings of the 7th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE). the 7th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)2024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023. 2023arXiv preprint</p>
<p>A Multi-task Model for Sentiment Aided Stance Detection of Climate Change Tweets. Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl, Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social Media202317</p>
<p>Ramon Villa-Cox, Sumeet Kumar, Matthew Babcock, Kathleen M Carley, arXiv:2006.00691Stance in replies and quotes (srq): A new dataset for learning stance in twitter conversations. 2020. 2020arXiv preprint</p>
<p>Secular vs. Islamist Polarization in Egypt on Twitter. Ingmar Weber, R Kiran Venkata, Alaa Garimella, Batayneh, 10.1145/2492517.2492557Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and MiningNiagara, Ontario, Canada; New York, NY, USAAssociation for Computing Machinery2013ASONAM '13)</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023. 2023arXiv preprint</p>
<p>pkudblab at semeval-2016 task 6: A specific convolutional neural network system for effective stance detection. Wan Wei, Xiao Zhang, Xuqin Liu, Wei Chen, Tengjiao Wang, Proceedings of the 10th international workshop on semantic evaluation. the 10th international workshop on semantic evaluation2016. SemEval-2016</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023. 2023arXiv preprint</p>
<p>UniLog: Automatic Logging via LLM and In-Context Learning. Junjielong Xu, Ziang Cui, Yuan Zhao, Xu Zhang, Shilin He, Pinjia He, Liqun Li, Yu Kang, Qingwei Lin, Yingnong Dang, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>BLCU_NLP at SemEval-2019 Task 7: An Inference Chain-based GPT Model for Rumour Evaluation. Ruoyao Yang, Wanying Xie, Chunhua Liu, Dong Yu, 10.18653/v1/S19-2191Proceedings of the 13th International Workshop on Semantic Evaluation. the 13th International Workshop on Semantic EvaluationMinneapolis, Minnesota, USAAssociation for Computational Linguistics2019</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023. 2023arXiv preprint</p>
<p>Evaluating instruction-tuned large language models on code comprehension and generation. Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, Yiling Lou, arXiv:2308.012402023. 2023arXiv preprint</p>
<p>MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection. Guido Zarrella, Amy Marsh, Proceedings of the 10th International Workshop on Semantic Evaluation. the 10th International Workshop on Semantic Evaluation2016. SemEval-2016</p>
<p>How would stance detection techniques evolve after the launch of chatgpt?. Bowen Zhang, Daijun Ding, Liwen Jing, arXiv:2212.145482022. 2022arXiv preprint</p>
<p>A Logically Consistent Chain-of-Thought Approach for Stance Detection. Bowen Zhang, Daijun Ding, Liwen Jing, Hu Huang, arXiv:2312.160542023. 2023arXiv preprint</p>
<p>Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Yangyang Li, Liwen Jing, arXiv:2304.03087Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media. 2023. 2023arXiv preprint</p>
<p>When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method. Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Chong Zhang, Zhenkun Zhou, Xingyu Peng, Ke Xu, arXiv:2301.08774DoubleH: Twitter User Stance Detection via Bipartite Graph Neural Networks. 2023. 2023arXiv preprint</p>
<p>Pretrained embeddings for stance detection with hierarchical capsule network on social media. Guangzhen Zhao, Peng Yang, ACM Transactions on Information Systems (TOIS). 392020. 2020</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, arXiv:2306.05685[cs.CL]and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. Manuscript submitted to ACM</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1859 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1859</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1859</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-af861b96ccdbae9b302f949c05ec2bfe8db13453</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/af861b96ccdbae9b302f949c05ec2bfe8db13453" target="_blank">RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> A 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations is introduced, to enable robust and effective 3D learning from dense regional language supervision.</p>
                <p><strong>Paper Abstract:</strong> We propose a lightweight and scalable Regional Point-Language Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, aiming to identify and recognize open-set objects and categories. Specifically, based on our empirical studies, we introduce a 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations. Subsequently, we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3D learning from dense regional language supervision. We carry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets, and our model outperforms prior 3D open-world scene understanding approaches by an average of 17.2% and 9.1% for semantic and instance segmentation, respectively, while maintaining greater scalability and lower resource demands. Furthermore, our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3D reasoning without extra task-specific training. Code will be released at github.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1859.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1859.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Language–Image Pretraining (CLIP) text encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastively pretrained vision-language model whose text encoder provides category embeddings; RegionPLC uses CLIP's text encoder as the open-vocabulary classifier by replacing classification weights with text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP text encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A text encoder from the CLIP family that produces category embeddings from natural language prompts; used as the classifier/label embedding bank for open-vocabulary 3D prediction by dot-product similarity with 3D point features.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–text pairs / natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world 3D semantic and instance segmentation (ScanNet, ScanNet200, nuScenes)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Dense 3D point-cloud semantic segmentation and instance segmentation where the model must classify and localize points for both base (annotated) and novel (unseen) categories in indoor and outdoor reconstructed 3D scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>The paper replaces the final classification layer weights with CLIP category embeddings and computes similarity between point-wise 3D features and text embeddings (dot product followed by softmax/sigmoid), directly mapping language concepts to classifier weights (see §3.1 formula).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>3D point clouds as primary input for inference; multi-view RGB images are used only during training to obtain regional captions (not required at inference).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>RegionPLC (using CLIP text encoder as classifier) achieves strong open-world 3D segmentation: e.g., ScanNet B12/N7 reported hIoU=68.2, mIoU^B=69.9, mIoU^N=66.6 (Table 2); annotation-free ScanNet mIoU^† up to 59.6 (SparseUNet32) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Methods without language supervision (e.g., 3DGenZ / 3DTZSL) obtain much lower novel-category performance (examples in Table 2: 3DGenZ B12/N7 novel mIoU ~13.3); fully-supervised 3D models (with dense 3D labels) obtain higher closed-set results but are not open-vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Training RegionPLC with language supervision: ~12.5–13.0 training hours on 8 GPUs (A100) for SparseUNet16/32 (ScanNet), uses regional captions mined from multi-view images (default 125K frames); extra storage for 3D-language pairs ~5.5 GB (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>OpenScene (pixel-aligned 2D feature distillation baseline) required ~25.3 h and ~117.3 GB extra storage in comparable settings; PLA required ~11.5–12.0 h and ~1.1 GB storage (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Using CLIP-based open-vocabulary supervision within RegionPLC yields comparable or improved open-world performance while requiring roughly ~50% less training time than OpenScene and ~20× less extra storage (5.5 GB vs 117.3 GB) in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Directly using CLIP text embeddings aligns language concepts to classifier weights (clear semantic interface); the region-level dense language supervision and region-aware contrastive loss produce point-discriminative features; multi-source caption fusion (SFusion) reduces noisy/conflicting supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Gaps arise when 2D caption sources have limited vocabulary or produce sparse/noisy captions; conflicts between overlapping captions can confuse optimization; CLIP text embeddings alone do not provide spatial grounding without region-level association.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacing classifier weights with CLIP text embeddings enables open-vocabulary 3D prediction when combined with dense region-level language supervision and point-discriminative contrastive training; this yields strong novel-category segmentation while being more training- and storage-efficient than pixel-aligned 2D feature distillation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1859.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1859.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OFA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OFA (Unified seq-to-seq image captioning model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image captioning foundation model used in RegionPLC to generate region-level captions from image crops (sliding windows and detector-cropped patches) which are then associated to 3D point sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OFA image captioning model</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A sequence-to-sequence vision–language captioner used to produce textual descriptions for 2D regions (sliding-window crops and detector proposals) to create 3D–language pairs for training the 3D backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–caption pairs (vision-language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world 3D semantic segmentation and instance segmentation (ScanNet etc.) via language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See above: dense 3D point-cloud segmentation and instance grouping; OFA provides text supervision associated by projecting 3D points into image regions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Generate captions for 2D regions (sliding windows or detector crops) and associate each caption to the corresponding 3D points by projecting 3D scenes into the 2D images and linking points that fall inside the 2D region to the caption (Sec. 3.2).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images for caption generation during training; 3D point clouds for final model training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Sliding-window captions from OFA (t^sw) provided strong regional supervision and competitive segmentation performance compared to other caption sources; OFA-based captions improve annotation-free segmentation (see Table 1 and discussion in §3.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Two-stage visual-prompt + caption approach (sliding windows) is less end-to-end and scales less well with number of views than end-to-end dense captioners; numeric training-hour statistics reported for overall RegionPLC (see CLIP entry).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Covers broad regions (sliding-window) enabling open-vocabulary descriptions and denser supervision; complements detectors and dense captioners when fused correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Two-stage captioning (visual prompting + captioner) can be less consistent across views and may scale worse to more images; sliding windows trade localization precision for coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Image captioners like OFA can produce dense, open-vocabulary regional descriptions that, when mapped to 3D points, provide useful supervision for open-world 3D segmentation, but they must be fused carefully with other sources to avoid redundancy and conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1859.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1859.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Detic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detic (Detecting twenty-thousand classes using image-level supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-vocabulary 2D object detector used to produce object proposals and category predictions; RegionPLC uses Detic outputs both as direct regional labels (template-based) and as proposals to be captioned.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting twenty-thousand classes using image-level supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Detic object detector</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A detection model trained for a very large vocabulary (LVIS-level) that provides 2D object proposals and (label) predictions; used to obtain localized region-language pairs (templates and proposals) for 3D supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image-level labels / detection supervision (large-vocabulary detection)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world 3D semantic and instance segmentation (via 3D-language pairing)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Detection-derived captions are projected into 3D to supervise point-wise open-vocabulary segmentation and instance segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Use 2D detector boxes and predicted categories to create templated captions (t^det-t) or proposals to crop and caption; associate detector box regions with 3D points via 2D projection to obtain 3D-language pairs (Sec. 3.2).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images for 2D detection during training; 3D point clouds for mapping and training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Detic-derived template captions (t^det-t) excel at small/distant object localization and performed particularly well in base-annotated partitions with many views (e.g., B12/N7 with 125K images), but performed worse in annotation-free settings with many novel classes due to limited predefined vocabulary (Table 1, §3.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong localization for small objects thanks to detection architecture; complements caption-based sources by precise bounding boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limited by pre-defined vocabulary (LVIS labels) and thus can underperform in annotation-free/open-vocabulary scenarios where dense captioners capture richer semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Detectors like Detic provide high-quality localization supervision that aids open-world 3D learning for categories within their vocabulary, but their limited lexical coverage reduces utility in fully open-vocabulary/annotation-free settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1859.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1859.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kosmos-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kosmos-2 (grounded multimodal large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal dense captioner / grounding-capable foundation model used by RegionPLC to generate region-level dense captions (t^kos) that are mapped to 3D points for supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kosmos-2: Grounding multimodal large language models to the world.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Kosmos-2 dense captioner</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A multimodal LLM / dense captioner trained to generate region-level captions and grounding information; used to produce semantically rich, salient object captions that are associated with 3D point regions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal image–text grounding data (dense caption/box–text pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world 3D semantic segmentation and instance segmentation (ScanNet, ScanNet200, nuScenes)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Kosmos-2 captions are projected onto 3D via 2D-to-3D association to produce supervisory signals for open-vocabulary 3D parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Produce dense box-level captions in 2D; associate 2D boxed regions with 3D points via projection (see Sec. 3.2); used as one source in the SFusion multi-source caption fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images for caption generation; 3D point clouds for mapping and downstream training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Dense captioners such as Kosmos-2 (t^kos) produce semantically rich captions that yield superior results in annotation-free settings and scale well when more views are used (Table 1, §3.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Rich vocabulary and grounded region descriptions provide valuable supervision for novel categories; scales well with more views due to end-to-end training consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Tends to focus on salient objects and may ignore small/distant objects; overlapping/conflicting captions across sources require fusion (SFusion) to avoid noisy supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end dense captioners like Kosmos-2 provide high-quality vocabulary-rich regional descriptions that improve annotation-free open-world 3D segmentation, especially when fused carefully with complementary sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1859.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1859.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRiT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRiT (Generative region-to-text transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative dense captioning model used to provide precise object localization and rich vocabulary for RegionPLC's regional 3D-language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grit: A generative region-to-text transformer for object understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GRiT dense captioner</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A generative region-to-text transformer that outputs box-level captions; used to generate 2D dense captions that are mapped into 3D supervisory pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Dense region (box) to text pairs (region caption datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world 3D semantic and instance segmentation (via mapped dense captions)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>GRiT captions associated to 3D points provide regional semantic supervision for open-vocabulary 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>2D box captions are projected to 3D by linking 3D points within projected 2D boxes to caption text (Sec. 3.2).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images (training) and 3D point clouds.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>GRiT-generated captions (t^giti) provide semantically rich supervision and show superior performance in annotation-free settings (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Precise localization combined with rich language descriptions helps segment salient objects without 3D labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>May ignore small/distant objects; overlapping/conflicting captions across sources require SFusion.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative region-to-text models are effective sources of dense 3D-language pairs when their outputs are mapped to 3D geometry and fused to reduce conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1859.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1859.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViT-GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViT-GPT2 image captioning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image captioning backbone used by prior work (PLA) to generate point-language pairs for training 3D models; cited as a caption source in related work and comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vit-gpt2 image captioning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>ViT-GPT2 image captioner</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An image-captioning pipeline (ViT encoder + GPT-2 decoder) used by previous work to produce textual captions associated with 3D points for language-driven 3D learning (PLA).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–caption pairs</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world 3D scene understanding (as used in PLA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Generates captions used as supervision for 3D open-vocabulary segmentation in PLA and related pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Produce image-level or view-level captions which are associated to partial 3D point sets by projection (as in PLA referenced in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images for caption generation; 3D point clouds for mapping and training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>PLA (which used ViT-GPT2 captions as one source) demonstrated open-world 3D segmentation performance but was outperformed by RegionPLC due to sparser/coarser captions (see Table 2 & §3.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>PLA reported training hours ~11.5–12.0 h and extra storage ~1.1 GB for storing 3D-language pairs (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Scalable pipeline producing image-level language supervision that can be mapped to 3D with low storage overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Captions are relatively coarse/sparse (view- and entity-level), limiting fine-grained point-discriminative learning for dense prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>View- and entity-level caption supervision (e.g., from ViT-GPT2) enables language-driven 3D learning but is sparser/coarser than region-level captions; denser regional captions improve downstream open-world 3D performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1859.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1859.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-3.5 (dialogue-optimized LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used in RegionGR (RegionPLC + LLM) to perform open-ended grounded 3D reasoning by consuming regional 3D-language pairs as environment context and answering user queries grounded to detected 3D objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt: Optimizing language models for dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-3.5 (LLM used for RegionGR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A dialogue-optimized LLM that ingests an environment context composed of regional 3D-language pairs and a user query, then outputs reasoning/plans which are parsed and grounded to 3D object detections from RegionPLC.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Web-scale text and dialogue fine-tuning (large-scale language corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-ended grounded 3D reasoning (RegionGR) — reasoning/planning over 3D scenes</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>LLM uses the regional 3D-language knowledge base to answer user queries, produce step-by-step instructions, summarize scene contents, and identify objects; outputs are grounded by RegionPLC which locates the referred objects in 3D.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language prompts / instructions</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM consumes 'environment context' (regional captions) and user query to produce textual responses; RegionGR parses object mentions from LLM output and grounds them to 3D detections produced by RegionPLC (Sec. 6).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Region-level 3D-language pairs (constructed from multi-view images and 3D projection) plus user-specified region constraints; RegionPLC provides geometric grounding capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Qualitative: enables open-ended, grounded Q&A and planning over 3D scenes (examples shown in Fig. 4); no numeric benchmark provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>LLM's strong language understanding combined with region-level 3D-language pairs provides a contextual grounding substrate; RegionPLC supplies geometric grounding so LLM outputs can be localized in 3D.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>LLMs lack direct perception and must rely on the quality and completeness of regional captions; hallucination or omission by LLM can propagate to grounding errors if not constrained by reliable environment context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LLMs can be integrated with regional 3D-language knowledge to enable flexible, grounded 3D reasoning (RegionGR); success depends critically on the quality and spatial coverage of the region-level captions used as environment context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1859.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1859.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-driven open-vocabulary 3d scene understanding (PLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previous method that constructs point–language supervision via image captioning (ViT-GPT2) and trains 3D backbones with point-language contrastive learning; RegionPLC compares to and improves upon PLA by using denser region-level captions and region-aware losses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language-driven open-vocabulary 3d scene understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PLA (point-language contrastive learning pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pipeline that mines captions (view- and entity-level) from image captioners and aligns them to 3D points for open-vocabulary 3D segmentation using contrastive losses; criticized here for relatively sparse/coarse supervision compared to RegionPLC.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image captioning outputs (image–text pairs used to produce point-language pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-world 3D semantic and instance segmentation (ScanNet, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Train 3D backbone using point–language contrastive objectives derived from image caption outputs; perform open-vocabulary inference with CLIP text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Associate caption text from images to partial 3D point sets via projection; use point-language contrastive learning to align 3D features with text.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB image captions for supervision; 3D point clouds for training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>PLA improves open-vocabulary 3D understanding compared with non-language baselines; e.g., Table 2 shows PLA numbers (e.g., B12/N7: hIoU ~55.3 / mIoU^B 69.5 / mIoU^N 45.9) but is outperformed by RegionPLC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>PLA training hours reported ~11.5–12.0 h and extra storage ~1.1 GB for 3D-language pairs (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Simple and low-storage pipeline to obtain point–language supervision from image captioning; easily scalable and integrable with 3D backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Caption supervision is view- and entity-level (coarser), leading to sparse signal that limits point-wise discriminative learning; performance degrades in annotation-free and long-tail settings compared to region-level caption supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mining captions from image captioners and aligning them to 3D enables open-vocabulary 3D learning, but denser region-level captions and point-discriminative, region-aware contrastive objectives (RegionPLC) substantially improve novel-category performance and localization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language-driven open-vocabulary 3d scene understanding <em>(Rating: 2)</em></li>
                <li>Openscene: 3d scene understanding with open vocabularies <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Detecting twenty-thousand classes using image-level supervision <em>(Rating: 2)</em></li>
                <li>Kosmos-2: Grounding multimodal large language models to the world. <em>(Rating: 2)</em></li>
                <li>Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. <em>(Rating: 2)</em></li>
                <li>Grit: A generative region-to-text transformer for object understanding. <em>(Rating: 2)</em></li>
                <li>Chatgpt: Optimizing language models for dialogue. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1859",
    "paper_id": "paper-af861b96ccdbae9b302f949c05ec2bfe8db13453",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "CLIP",
            "name_full": "Contrastive Language–Image Pretraining (CLIP) text encoder",
            "brief_description": "A contrastively pretrained vision-language model whose text encoder provides category embeddings; RegionPLC uses CLIP's text encoder as the open-vocabulary classifier by replacing classification weights with text embeddings.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_agent_name": "CLIP text encoder",
            "model_agent_description": "A text encoder from the CLIP family that produces category embeddings from natural language prompts; used as the classifier/label embedding bank for open-vocabulary 3D prediction by dot-product similarity with 3D point features.",
            "pretraining_data_type": "Image–text pairs / natural language supervision",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-world 3D semantic and instance segmentation (ScanNet, ScanNet200, nuScenes)",
            "embodied_task_description": "Dense 3D point-cloud semantic segmentation and instance segmentation where the model must classify and localize points for both base (annotated) and novel (unseen) categories in indoor and outdoor reconstructed 3D scenes.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "The paper replaces the final classification layer weights with CLIP category embeddings and computes similarity between point-wise 3D features and text embeddings (dot product followed by softmax/sigmoid), directly mapping language concepts to classifier weights (see §3.1 formula).",
            "perception_requirements": "3D point clouds as primary input for inference; multi-view RGB images are used only during training to obtain regional captions (not required at inference).",
            "transfer_successful": true,
            "performance_with_pretraining": "RegionPLC (using CLIP text encoder as classifier) achieves strong open-world 3D segmentation: e.g., ScanNet B12/N7 reported hIoU=68.2, mIoU^B=69.9, mIoU^N=66.6 (Table 2); annotation-free ScanNet mIoU^† up to 59.6 (SparseUNet32) (Table 4).",
            "performance_without_pretraining": "Methods without language supervision (e.g., 3DGenZ / 3DTZSL) obtain much lower novel-category performance (examples in Table 2: 3DGenZ B12/N7 novel mIoU ~13.3); fully-supervised 3D models (with dense 3D labels) obtain higher closed-set results but are not open-vocabulary.",
            "sample_complexity_with_pretraining": "Training RegionPLC with language supervision: ~12.5–13.0 training hours on 8 GPUs (A100) for SparseUNet16/32 (ScanNet), uses regional captions mined from multi-view images (default 125K frames); extra storage for 3D-language pairs ~5.5 GB (Table 4).",
            "sample_complexity_without_pretraining": "OpenScene (pixel-aligned 2D feature distillation baseline) required ~25.3 h and ~117.3 GB extra storage in comparable settings; PLA required ~11.5–12.0 h and ~1.1 GB storage (Table 4).",
            "sample_complexity_gain": "Using CLIP-based open-vocabulary supervision within RegionPLC yields comparable or improved open-world performance while requiring roughly ~50% less training time than OpenScene and ~20× less extra storage (5.5 GB vs 117.3 GB) in the reported comparisons.",
            "transfer_success_factors": "Directly using CLIP text embeddings aligns language concepts to classifier weights (clear semantic interface); the region-level dense language supervision and region-aware contrastive loss produce point-discriminative features; multi-source caption fusion (SFusion) reduces noisy/conflicting supervision.",
            "transfer_failure_factors": "Gaps arise when 2D caption sources have limited vocabulary or produce sparse/noisy captions; conflicts between overlapping captions can confuse optimization; CLIP text embeddings alone do not provide spatial grounding without region-level association.",
            "key_findings": "Replacing classifier weights with CLIP text embeddings enables open-vocabulary 3D prediction when combined with dense region-level language supervision and point-discriminative contrastive training; this yields strong novel-category segmentation while being more training- and storage-efficient than pixel-aligned 2D feature distillation baselines.",
            "uuid": "e1859.0",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "OFA",
            "name_full": "OFA (Unified seq-to-seq image captioning model)",
            "brief_description": "An image captioning foundation model used in RegionPLC to generate region-level captions from image crops (sliding windows and detector-cropped patches) which are then associated to 3D point sets.",
            "citation_title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.",
            "mention_or_use": "use",
            "model_agent_name": "OFA image captioning model",
            "model_agent_description": "A sequence-to-sequence vision–language captioner used to produce textual descriptions for 2D regions (sliding-window crops and detector proposals) to create 3D–language pairs for training the 3D backbone.",
            "pretraining_data_type": "Image–caption pairs (vision-language pretraining)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-world 3D semantic segmentation and instance segmentation (ScanNet etc.) via language supervision",
            "embodied_task_description": "See above: dense 3D point-cloud segmentation and instance grouping; OFA provides text supervision associated by projecting 3D points into image regions.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "Generate captions for 2D regions (sliding windows or detector crops) and associate each caption to the corresponding 3D points by projecting 3D scenes into the 2D images and linking points that fall inside the 2D region to the caption (Sec. 3.2).",
            "perception_requirements": "RGB images for caption generation during training; 3D point clouds for final model training and inference.",
            "transfer_successful": true,
            "performance_with_pretraining": "Sliding-window captions from OFA (t^sw) provided strong regional supervision and competitive segmentation performance compared to other caption sources; OFA-based captions improve annotation-free segmentation (see Table 1 and discussion in §3.3).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Two-stage visual-prompt + caption approach (sliding windows) is less end-to-end and scales less well with number of views than end-to-end dense captioners; numeric training-hour statistics reported for overall RegionPLC (see CLIP entry).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Covers broad regions (sliding-window) enabling open-vocabulary descriptions and denser supervision; complements detectors and dense captioners when fused correctly.",
            "transfer_failure_factors": "Two-stage captioning (visual prompting + captioner) can be less consistent across views and may scale worse to more images; sliding windows trade localization precision for coverage.",
            "key_findings": "Image captioners like OFA can produce dense, open-vocabulary regional descriptions that, when mapped to 3D points, provide useful supervision for open-world 3D segmentation, but they must be fused carefully with other sources to avoid redundancy and conflicts.",
            "uuid": "e1859.1",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Detic",
            "name_full": "Detic (Detecting twenty-thousand classes using image-level supervision)",
            "brief_description": "A large-vocabulary 2D object detector used to produce object proposals and category predictions; RegionPLC uses Detic outputs both as direct regional labels (template-based) and as proposals to be captioned.",
            "citation_title": "Detecting twenty-thousand classes using image-level supervision",
            "mention_or_use": "use",
            "model_agent_name": "Detic object detector",
            "model_agent_description": "A detection model trained for a very large vocabulary (LVIS-level) that provides 2D object proposals and (label) predictions; used to obtain localized region-language pairs (templates and proposals) for 3D supervision.",
            "pretraining_data_type": "Image-level labels / detection supervision (large-vocabulary detection)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-world 3D semantic and instance segmentation (via 3D-language pairing)",
            "embodied_task_description": "Detection-derived captions are projected into 3D to supervise point-wise open-vocabulary segmentation and instance segmentation.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "Use 2D detector boxes and predicted categories to create templated captions (t^det-t) or proposals to crop and caption; associate detector box regions with 3D points via 2D projection to obtain 3D-language pairs (Sec. 3.2).",
            "perception_requirements": "RGB images for 2D detection during training; 3D point clouds for mapping and training.",
            "transfer_successful": true,
            "performance_with_pretraining": "Detic-derived template captions (t^det-t) excel at small/distant object localization and performed particularly well in base-annotated partitions with many views (e.g., B12/N7 with 125K images), but performed worse in annotation-free settings with many novel classes due to limited predefined vocabulary (Table 1, §3.3).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong localization for small objects thanks to detection architecture; complements caption-based sources by precise bounding boxes.",
            "transfer_failure_factors": "Limited by pre-defined vocabulary (LVIS labels) and thus can underperform in annotation-free/open-vocabulary scenarios where dense captioners capture richer semantics.",
            "key_findings": "Detectors like Detic provide high-quality localization supervision that aids open-world 3D learning for categories within their vocabulary, but their limited lexical coverage reduces utility in fully open-vocabulary/annotation-free settings.",
            "uuid": "e1859.2",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Kosmos-2",
            "name_full": "Kosmos-2 (grounded multimodal large language model)",
            "brief_description": "A multimodal dense captioner / grounding-capable foundation model used by RegionPLC to generate region-level dense captions (t^kos) that are mapped to 3D points for supervision.",
            "citation_title": "Kosmos-2: Grounding multimodal large language models to the world.",
            "mention_or_use": "use",
            "model_agent_name": "Kosmos-2 dense captioner",
            "model_agent_description": "A multimodal LLM / dense captioner trained to generate region-level captions and grounding information; used to produce semantically rich, salient object captions that are associated with 3D point regions.",
            "pretraining_data_type": "Multimodal image–text grounding data (dense caption/box–text pairs)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-world 3D semantic segmentation and instance segmentation (ScanNet, ScanNet200, nuScenes)",
            "embodied_task_description": "Kosmos-2 captions are projected onto 3D via 2D-to-3D association to produce supervisory signals for open-vocabulary 3D parsing.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "Produce dense box-level captions in 2D; associate 2D boxed regions with 3D points via projection (see Sec. 3.2); used as one source in the SFusion multi-source caption fusion.",
            "perception_requirements": "RGB images for caption generation; 3D point clouds for mapping and downstream training.",
            "transfer_successful": true,
            "performance_with_pretraining": "Dense captioners such as Kosmos-2 (t^kos) produce semantically rich captions that yield superior results in annotation-free settings and scale well when more views are used (Table 1, §3.3).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Rich vocabulary and grounded region descriptions provide valuable supervision for novel categories; scales well with more views due to end-to-end training consistency.",
            "transfer_failure_factors": "Tends to focus on salient objects and may ignore small/distant objects; overlapping/conflicting captions across sources require fusion (SFusion) to avoid noisy supervision.",
            "key_findings": "End-to-end dense captioners like Kosmos-2 provide high-quality vocabulary-rich regional descriptions that improve annotation-free open-world 3D segmentation, especially when fused carefully with complementary sources.",
            "uuid": "e1859.3",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "GRiT",
            "name_full": "GRiT (Generative region-to-text transformer)",
            "brief_description": "A generative dense captioning model used to provide precise object localization and rich vocabulary for RegionPLC's regional 3D-language supervision.",
            "citation_title": "Grit: A generative region-to-text transformer for object understanding.",
            "mention_or_use": "use",
            "model_agent_name": "GRiT dense captioner",
            "model_agent_description": "A generative region-to-text transformer that outputs box-level captions; used to generate 2D dense captions that are mapped into 3D supervisory pairs.",
            "pretraining_data_type": "Dense region (box) to text pairs (region caption datasets)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-world 3D semantic and instance segmentation (via mapped dense captions)",
            "embodied_task_description": "GRiT captions associated to 3D points provide regional semantic supervision for open-vocabulary 3D tasks.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "2D box captions are projected to 3D by linking 3D points within projected 2D boxes to caption text (Sec. 3.2).",
            "perception_requirements": "RGB images (training) and 3D point clouds.",
            "transfer_successful": true,
            "performance_with_pretraining": "GRiT-generated captions (t^giti) provide semantically rich supervision and show superior performance in annotation-free settings (Table 1).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Precise localization combined with rich language descriptions helps segment salient objects without 3D labels.",
            "transfer_failure_factors": "May ignore small/distant objects; overlapping/conflicting captions across sources require SFusion.",
            "key_findings": "Generative region-to-text models are effective sources of dense 3D-language pairs when their outputs are mapped to 3D geometry and fused to reduce conflicts.",
            "uuid": "e1859.4",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ViT-GPT2",
            "name_full": "ViT-GPT2 image captioning",
            "brief_description": "An image captioning backbone used by prior work (PLA) to generate point-language pairs for training 3D models; cited as a caption source in related work and comparisons.",
            "citation_title": "Vit-gpt2 image captioning.",
            "mention_or_use": "mention",
            "model_agent_name": "ViT-GPT2 image captioner",
            "model_agent_description": "An image-captioning pipeline (ViT encoder + GPT-2 decoder) used by previous work to produce textual captions associated with 3D points for language-driven 3D learning (PLA).",
            "pretraining_data_type": "Image–caption pairs",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-world 3D scene understanding (as used in PLA)",
            "embodied_task_description": "Generates captions used as supervision for 3D open-vocabulary segmentation in PLA and related pipelines.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "Produce image-level or view-level captions which are associated to partial 3D point sets by projection (as in PLA referenced in the paper).",
            "perception_requirements": "RGB images for caption generation; 3D point clouds for mapping and training.",
            "transfer_successful": true,
            "performance_with_pretraining": "PLA (which used ViT-GPT2 captions as one source) demonstrated open-world 3D segmentation performance but was outperformed by RegionPLC due to sparser/coarser captions (see Table 2 & §3.3).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "PLA reported training hours ~11.5–12.0 h and extra storage ~1.1 GB for storing 3D-language pairs (Table 4).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Scalable pipeline producing image-level language supervision that can be mapped to 3D with low storage overhead.",
            "transfer_failure_factors": "Captions are relatively coarse/sparse (view- and entity-level), limiting fine-grained point-discriminative learning for dense prediction.",
            "key_findings": "View- and entity-level caption supervision (e.g., from ViT-GPT2) enables language-driven 3D learning but is sparser/coarser than region-level captions; denser regional captions improve downstream open-world 3D performance.",
            "uuid": "e1859.5",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "GPT-3.5 (ChatGPT)",
            "name_full": "ChatGPT / GPT-3.5 (dialogue-optimized LLM)",
            "brief_description": "A large language model used in RegionGR (RegionPLC + LLM) to perform open-ended grounded 3D reasoning by consuming regional 3D-language pairs as environment context and answering user queries grounded to detected 3D objects.",
            "citation_title": "Chatgpt: Optimizing language models for dialogue.",
            "mention_or_use": "use",
            "model_agent_name": "GPT-3.5 (LLM used for RegionGR)",
            "model_agent_description": "A dialogue-optimized LLM that ingests an environment context composed of regional 3D-language pairs and a user query, then outputs reasoning/plans which are parsed and grounded to 3D object detections from RegionPLC.",
            "pretraining_data_type": "Web-scale text and dialogue fine-tuning (large-scale language corpora)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-ended grounded 3D reasoning (RegionGR) — reasoning/planning over 3D scenes",
            "embodied_task_description": "LLM uses the regional 3D-language knowledge base to answer user queries, produce step-by-step instructions, summarize scene contents, and identify objects; outputs are grounded by RegionPLC which locates the referred objects in 3D.",
            "action_space_text": "Natural language prompts / instructions",
            "action_space_embodied": null,
            "action_mapping_method": "LLM consumes 'environment context' (regional captions) and user query to produce textual responses; RegionGR parses object mentions from LLM output and grounds them to 3D detections produced by RegionPLC (Sec. 6).",
            "perception_requirements": "Region-level 3D-language pairs (constructed from multi-view images and 3D projection) plus user-specified region constraints; RegionPLC provides geometric grounding capabilities.",
            "transfer_successful": true,
            "performance_with_pretraining": "Qualitative: enables open-ended, grounded Q&A and planning over 3D scenes (examples shown in Fig. 4); no numeric benchmark provided.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "LLM's strong language understanding combined with region-level 3D-language pairs provides a contextual grounding substrate; RegionPLC supplies geometric grounding so LLM outputs can be localized in 3D.",
            "transfer_failure_factors": "LLMs lack direct perception and must rely on the quality and completeness of regional captions; hallucination or omission by LLM can propagate to grounding errors if not constrained by reliable environment context.",
            "key_findings": "Large LLMs can be integrated with regional 3D-language knowledge to enable flexible, grounded 3D reasoning (RegionGR); success depends critically on the quality and spatial coverage of the region-level captions used as environment context.",
            "uuid": "e1859.6",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PLA",
            "name_full": "Language-driven open-vocabulary 3d scene understanding (PLA)",
            "brief_description": "A previous method that constructs point–language supervision via image captioning (ViT-GPT2) and trains 3D backbones with point-language contrastive learning; RegionPLC compares to and improves upon PLA by using denser region-level captions and region-aware losses.",
            "citation_title": "Language-driven open-vocabulary 3d scene understanding",
            "mention_or_use": "mention",
            "model_agent_name": "PLA (point-language contrastive learning pipeline)",
            "model_agent_description": "A pipeline that mines captions (view- and entity-level) from image captioners and aligns them to 3D points for open-vocabulary 3D segmentation using contrastive losses; criticized here for relatively sparse/coarse supervision compared to RegionPLC.",
            "pretraining_data_type": "Image captioning outputs (image–text pairs used to produce point-language pairs)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-world 3D semantic and instance segmentation (ScanNet, etc.)",
            "embodied_task_description": "Train 3D backbone using point–language contrastive objectives derived from image caption outputs; perform open-vocabulary inference with CLIP text embeddings.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "Associate caption text from images to partial 3D point sets via projection; use point-language contrastive learning to align 3D features with text.",
            "perception_requirements": "RGB image captions for supervision; 3D point clouds for training and inference.",
            "transfer_successful": true,
            "performance_with_pretraining": "PLA improves open-vocabulary 3D understanding compared with non-language baselines; e.g., Table 2 shows PLA numbers (e.g., B12/N7: hIoU ~55.3 / mIoU^B 69.5 / mIoU^N 45.9) but is outperformed by RegionPLC.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "PLA training hours reported ~11.5–12.0 h and extra storage ~1.1 GB for 3D-language pairs (Table 4).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Simple and low-storage pipeline to obtain point–language supervision from image captioning; easily scalable and integrable with 3D backbones.",
            "transfer_failure_factors": "Caption supervision is view- and entity-level (coarser), leading to sparse signal that limits point-wise discriminative learning; performance degrades in annotation-free and long-tail settings compared to region-level caption supervision.",
            "key_findings": "Mining captions from image captioners and aligning them to 3D enables open-vocabulary 3D learning, but denser region-level captions and point-discriminative, region-aware contrastive objectives (RegionPLC) substantially improve novel-category performance and localization.",
            "uuid": "e1859.7",
            "source_info": {
                "paper_title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language-driven open-vocabulary 3d scene understanding",
            "rating": 2
        },
        {
            "paper_title": "Openscene: 3d scene understanding with open vocabularies",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Detecting twenty-thousand classes using image-level supervision",
            "rating": 2
        },
        {
            "paper_title": "Kosmos-2: Grounding multimodal large language models to the world.",
            "rating": 2
        },
        {
            "paper_title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.",
            "rating": 2
        },
        {
            "paper_title": "Grit: A generative region-to-text transformer for object understanding.",
            "rating": 2
        },
        {
            "paper_title": "Chatgpt: Optimizing language models for dialogue.",
            "rating": 1
        }
    ],
    "cost": 0.02761625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding</h1>
<p>Jihan Yang ${ }^{1 <em>}$ Runyu Ding ${ }^{1 </em>}$ Weipeng Deng ${ }^{1}$ Zhe Wang ${ }^{2}$ Xiaojuan Qi ${ }^{1}$<br>${ }^{1}$ The University of Hong Kong ${ }^{2}$ SenseTime Research<br>https://jihanyang.github.io/projects/RegionPLC</p>
<h4>Abstract</h4>
<p>We propose a lightweight and scalable Regional PointLanguage Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, aiming to identify and recognize open-set objects and categories. Specifically, based on our empirical studies, we introduce a 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations. Subsequently, we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3D learning from dense regional language supervision. We carry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets, and our model outperforms prior 3D open-world scene understanding approaches by an average of $17.2 \%$ and $9.1 \%$ for semantic and instance segmentation, respectively, while maintaining greater scalability and lower resource demands. Furthermore, our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3D reasoning without extra task-specific training. Code is available at github.</p>
<h2>1. Introduction</h2>
<p>Open-world 3D scene understanding aims to equip models with the ability to accurately perceive and identify open-set objects and categories from 3D data, such as point clouds. This ability is crucial for real-world applications where objects from open-set categories are prevalent [3, 43]. However, this task poses significant challenges due to the scarcity of dense 3D semantic annotations, which are difficult to gather and scale to a large vocabulary space.</p>
<p>Fortunately, the abundance of paired image and text data from the Internet, featuring a vast semantic vocabulary, has enabled 2D vision-language models to exhibit exceptional open-world image comprehension capabilities. These abilities span various tasks, such as image caption-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ing [2, 35], grounding [27, 36], and dense semantic prediction [22, 23, 48]. Consequently, recent research has been inspired to leverage these models to generate pseudo supervision such as dense semantic features [26, 44] and language descriptions [9, 10] for training 3D models, thereby enabling open-world inference without relying on image modalities.</p>
<p>Despite advancements, existing solutions still exhibit limitations. For instance, feature distillation-based methods [26, 44]- despite harvesting dense supervision- suffer from the constraints of 2D feature qualities and require resource-intensive feature extraction, fusion, and storage processes, preventing them from being scaled up with more advanced 3D architectures and larger 3D datasets. Additionally, while [9] utilizes pseudo 3D-language pairs to enable direct learning from large-vocabulary language supervision, it suffers from sparse supervision provided by image captioning models. Considering the recent success of 2D foundation models in image- and region-level visionlanguage learning, we explore combining their strengths to enrich vocabulary and construct high-quality region-level 3D-language associations. By doing so, our method can yield denser 3D-language supervision and circumvent the knowledge limitations of a single foundation model, facilitating resource-efficient and large-vocabulary 3D learning.</p>
<p>To this end, we propose a holistic Regional Point Language Contrastive learning framework, named RegionPLC. This framework generates and fuses diverse regionlevel captions from powerful 2D vision-language models, which are subsequently mapped to 3D for constructing region-level 3D and language pairs. These paired data are then incorporated into a region-aware point-discriminative contrastive learning framework, enabling 3D open-world learning from dense language supervision.</p>
<p>Specifically, we begin by conducting a comprehensive examination of various 2D foundation models (e.g., image captioning [35], dense captioning [27, 36], and detection models [48]) along with visual prompting techniques for their capability to generate region-level 3D-language pairs. Based on our examination, we propose a supplementary-</p>
<p>oriented fusion strategy that leverages the geometric relationship of regions in 3D space to alleviate ambiguities and conflicts encountered when combining paired 3D-language data from multiple 2D models, ultimately delivering high-quality dense region-level 3D-language pairs. Furthermore, with region-level language data, we introduce a region-aware point-discriminative contrastive loss that prevents the optimization of point-wise embeddings from being disturbed by nearby points from unrelated semantic categories, enhancing the discriminativeness of learned point-wise embeddings. The region-aware design further normalizes the contribution of multiple region-level 3D-language pairs, regardless of their region sizes, making feature learning more robust. Finally, by harvesting the 3D-language associations, our RegionPLC can be effortlessly integrated with language models to enable open-ended 3D reasoning with grounding abilities without requiring task-specific data for training.</p>
<p>We conduct extensive experiments on ScanNet [8], ScanNet200 [29], and nuScenes [4] datasets, covering both 3D indoor and outdoor scenarios. Our method significantly outperforms existing open-world scene understanding methods, achieving an average of 17.2% gains in terms of unseen category mIoU for semantic segmentation and an average of 9.1% gains in terms of unseen category mAP50 for instance segmentation. RegionPLC demonstrates promising zero-shot segmentation performance, attaining 40.5% and 1.8% higher foreground mIoU compared to PLA [9] and OpenScene [26], respectively. Notably, it achieves this performance while consuming only 17% of OpenScene's [26] training cost and 5% of its storage requirements. Furthermore, RegionPLC can also be combined with OpenScene to deliver 5.8% and 10.0% gains in foreground mIoU and mAcc, respectively.</p>
<h2>2. Related Work</h2>
<p>3D Scene Understanding. 3D semantic and instance segmentation are two fundamental tasks for scene understanding, which predict each point's semantic meaning (and instance IDs) in a 3D point cloud. For semantic feature extraction and prediction, existing approaches design customized point convolutions applied on raw point clouds [32, 37, 39] or employ sparse convolution [13] to develop voxel-based networks [7, 14] or transformers [21] based on 3D grids. For instance-level prediction, representative approaches often use a bottom-up strategy that groups points to form object proposals [19, 33, 34], or first predicts 3D bounding boxes and then refines the object masks using a top-down solution [20, 40, 42]. Though achieving outstanding results on close-set benchmark datasets, they always struggle with open-world recognition.</p>
<p>Open-world 3D Understanding. Open-world 3D understanding [9, 24, 26, 45] aims to recognize novel categories that are unseen during training. Most recently, the high open-world capability of 2D foundation models [2, 28] trained on massive multi-modality data has inspired recent approaches to leverage them for 3D open-world understanding. One line of work [16–18, 26, 31, 46] focuses on incorporating these 2D foundation models in the inference stage for open-world recognition, which mainly conducts open-world semantic prediction on the image modality using vision-language models [22, 28, 41] and fuses 2D prediction results into 3D if required. Though promising, they suffer from significant computation and storage overheads during inference and can be sub-optimal to address 3D understanding without learning from 3D geometries.</p>
<p>This paper focuses on another open-world research direction that concentrates on open-world point cloud learning. It requires training 3D backbones to enable their open-world capabilities without the image modality dependence during inference and thus have more applicability potential. Along this line of research, some [26, 44, 45] have attempted to distill 2D dense features [22, 41] into 3D backbones for 3D feature learning. However, they still incur high training costs and might inherit 2D prediction failure modes. In addition, Ding et al. [9, 10] obtain point-language paired data through image captioning by VL foundation models for training 3D backbones. These methods are scalable toward a large vocabulary space and can be easily integrated with advanced 3D backbones. Despite the advantages, they still suffer from the coarse text supervision.</p>
<h2>3. RegionPLC</h2>
<h3>3.1. Overview</h3>
<p>We focus on 3D open-world scene understanding at both semantic and instance levels. During training, given a point cloud of a scene $\mathcal{P}={\mathbf{p}}$, the model can utilize human annotations $\mathcal{Y}$ for base categories $\mathcal{C}^{B}$, but cannot access annotations for novel categories $\mathcal{C}^{N}$. During the inference phase, the trained model needs to classify and localize points associated with both base and novel categories ($\mathcal{C}^{B} \cup \mathcal{C}^{N}$).</p>
<p>To achieve open-world understanding, apart from the common 3D encoder $\mathrm{F}<em _text="{text">{3 \mathrm{D}}$, we follow [9] to replace the classification layer weights with category embeddings $\mathbf{f}^{l}$ extracted from a pretrained text encoder $\mathrm{F}</em>$ of CLIP [28] (See Figure 1: Upper). Hence, the prediction process is shown as follows:}</p>
<p>$$
\mathbf{f}^{p}=\mathrm{F}<em 3="3" _mathrm_D="\mathrm{D">{\theta}\left(\mathrm{F}</em>}}(\mathbf{p})\right), \mathbf{s}=\sigma\left(\mathbf{f}^{l} \cdot \mathbf{f}^{p}\right), \mathbf{o}=\mathrm{F<em 3="3" _mathrm_D="\mathrm{D">{\mathrm{loc}}\left(\mathrm{F}</em>\right)
$$}}(\mathbf{p}), \mathbf{s</p>
<p>where $\mathrm{F}<em _loc="{loc" _text="\text">{\theta}$ is the vision-language (VL) adapter to align the feature dimension of the 3D point-wise features $\mathbf{f}^{p}$ and category embeddings $\mathbf{f}^{l}$, $\mathbf{s}$ is the semantic classification score, $\sigma$ is the softmax function, $\mathbf{o}$ is the instance proposal output, and $\mathrm{F}</em>$ is the localization network [34] for instance segmentation. With these modifications, the model can predict any desired categories by computing similarity between point-wise features and queried category embeddings for open-world inference.}</p>
<p>The goal of our RegionPLC is to train such an open-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: Overview of our regional point-language contrastive learning framework. For regional 3D-language association, We develop a 3D-aware SFusion strategy effectively combining 3D vision-language pairs obtained from multiple 2D foundation models (refer to Sec. 3.2). Upon these 3D-language data, we propose region-aware point-discriminative contrastive learning to facilitate more distinctive and robust representation learning (detailed in Sec. 3.5). Different point &amp; box colors in the bottom-right indicate various 3D-caption pairs.</p>
<p>world 3D backbone via dense region-level 3D-language supervision leveraging powerful and diverse 2D foundation models, as shown in Figure 1. we first obtain region-level 2D-language pairs through three streams of 2D VL models (i.e. image captioning <em>[35]</em>, object detection <em>[48]</em> and dense captioning <em>[27, 36]</em>) and then associate them to 3D points (see Sec. 3.2). Then, we comprehensively benchmark and examine these 3D-language pairs from different sources, deriving their merits and shortcomings for 3D learning (see Sec. 3.3). Based on our study, we propose a simple Supplementary-oriented Fusion (SFusion) strategy leveraging their 3D relationships to alleviate redundancies and conflicts in Sec. 3.4, obtaining vocabulary-enriched and denser region-level 3D-language paired data. Finally, upon the 3D-language data, we design a region-aware point-discriminative contrastive learning objective to replace CLIP-style loss for more robust and discriminative feature learning from language supervisions in Sec. 3.5.</p>
<h3>3.2 Regional 3D-Language Association from 2D Foundation Models</h3>
<p>Here, we first introduce three streams of methods along with two types of visual prompts to extract regional language descriptions from 2D vision-language foundation models: $i)$ object detector with language template; $ii)$ explicit visual prompted image captioning; $iii)$ dense captioning.</p>
<p>Object Detector with language template $\mathcal{G}^{\text{det}}$. The most straightforward manner to obtain regional language supervision is to leverage the category prediction from 2D object detector Detic <em>[48]</em> and then fill the category into a language template as CLIP <em>[28]</em>, as illustrated in Figure 2. Thanks to the multi-scale training strategy, object detectors can capture remote and small objects. We denote such regional captions as $\mathbf{t}^{\text{det-1}}$.</p>
<p>Figure 2: Comparisons of different advanced manners for extracting regional language descriptions with 2D foundation models.</p>
<p>Explicit visual prompted image captioning $\mathcal{G}^{\text{prompt}}$. Another intuitive paradigm is first to generate explicit visual prompts such as boxes and then caption these image patches via image captioning model OFA <em>[35]</em> (refer to Figure 2). As for obtaining explicit visual prompts, we attempt two types: sliding windows and object proposals. Sliding-window-cropped image patches cover all potential semantic regions without being constrained by pre-defined vocabulary space, benefiting open-world tasks but sacrificing precise localization. In contrast, 2D object proposals from detectors provide more accurate object localization but suffer from the limited vocabulary space with pre-defined label space such as LVIS <em>[15]</em>. The obtained dense region-level captions through sliding-window prompts and detector prompts are denoted as $\mathbf{t}^{\text{vw}}$ and $\mathbf{t}^{\text{det-c}}$, respectively.</p>
<p>Dense Captioning $\mathcal{G}^{\text{cap}}$. Apart from the powerful image detectors and image captioning models, recent advances in dense captions and grounding models such as GRiT <em>[36]</em> and Kosmos-2 <em>[27]</em> are trained on the large-scale 2D box</p>
<p>| Method | ScanNet B12/N7 | | ScanNet annotation-free | |
| | 25K | 125K | 25K | 125K |
| --- | --- | --- | --- | --- |
| $\mathbf{t}^{\text {det-t }}$ | $63.4 / \mathbf{7 0 . 3 / 5 7 . 7}$ | $\mathbf{6 7 . 4 / 7 0 . 5 / 6 4 . 6}$ | $37.7(59.7)$ | $41.9(64.5)$ |
| $\mathbf{t}^{\text {sw }}$ | $\mathbf{6 5 . 9 / 7 0 . 2 / 6 2 . 1}$ | $66.0 / 70.2 / 62.3$ | $48.1(69.2)$ | $47.8(69.2)$ |
| $\mathbf{t}^{\text {det-c }}$ | $64.4 / 69.9 / 59.7$ | $65.6 / 70.7 / 61.2$ | $41.4(64.1)$ | $43.8(65.6)$ |
| $\mathbf{t}^{\text {giti }}$ | $62.7 / \mathbf{7 0 . 3 / 5 6 . 6}$ | $64.9 / \mathbf{7 0 . 8 / 5 9 . 9}$ | $\mathbf{5 0 . 5 ( 7 2 . 2 )}$ | $51.3(\mathbf{7 4 . 2})$ |
| $\mathbf{t}^{\text {kos }}$ | $64.4 / \mathbf{7 0 . 3 / 5 9 . 4}$ | $64.6 / 69.8 / 60.2$ | $50.1(70.7)$ | $\mathbf{5 1 . 7 ( 7 2 . 7 )}$ |
| $\mathbf{t}^{\text {kos }} \cup \mathbf{t}^{\text {det-t }}$ | $64.6 / 69.8 / 60.2$ | $\mathbf{6 7 . 0 / 6 9 . 9 / 6 4 . 4}$ | $44.4(66.3)$ | $\mathbf{5 4 . 0 ( 7 4 . 5 )}$ |
| $\mathbf{t}^{\text {kos }} \cup \mathbf{t}^{\text {sw }}$ | $\mathbf{6 5 . 9 / 6 9 . 9 / 6 2 . 4}$ | $65.6 / \mathbf{7 0 . 9 / 6 1 . 0}$ | $\mathbf{5 3 . 5}(72.9)$ | $53.1(73.9)$ |
| $\mathcal{L}<em _gloss="{gloss" _text="\text">{\text {glos }}+\mathcal{L}</em>$ | $64.4 / 69.1 / 60.2$ | $51.3(70.7)$ | $51.2(72.4)$ |
| $\mathcal{L}}}$ | $65.0 / \mathbf{7 0 . 2 / 6 0 . 5<em _loss="{loss" _text="\text">{\text {glos }}+\mathcal{L}</em>$ | $52.7(73.8)$ |}}$ | $65.4 / 70.0 / 61.4$ | $64.6 / 70.2 / 59.8$ | $52.9(\mathbf{7 3 . 6 )</p>
<p>Table 1. Results of regional caption fusion on base-annotated (hIoU / mIoU ${ }^{\mathrm{ff}} / \mathrm{mIoU}^{\mathrm{ff}}$ ) and annotation-free (mIoU ${ }^{\dagger}\left(\mathrm{mAcc}^{\dagger}\right)$, tested on foreground classes only) 3D ScanNet semantic segmentation. $\mathbf{t}^{\text {kos }} \cup \mathbf{t}^{\text {det-t }}$ and $\mathcal{L}<em _gloss="{gloss" _text="\text">{\text {glos }}+\mathcal{L}</em>$ indicate data-level and multiloss fusion, respectively. Best results are presented in bold.
and box description pairs. As shown in Figure 2, dense captioners offer precise object localization and rich vocabulary spaces but tend to focus on only salient objects and ignore small and distant objects. We denote captions generated through GRiT [36], Kosmos-2 [27] and Detic [48] with a caption template as $\mathbf{t}^{\text {giti }}$, and $\mathbf{t}^{\text {kos }}$ and $\mathbf{t}^{\text {det-t }}$, respectively.
Associate Points to Dense Captions. Upon above 5 types of regional captions $\mathbf{t}^{r}=\left{\mathbf{t}^{\mathrm{sw}}, \mathbf{t}^{\text {det-c }}, \mathbf{t}^{\text {det-t }}, \mathbf{t}^{\text {giti }}, \mathbf{t}^{\text {kos }}\right}$, we associate them to partial point sets through 3D geometry, similar to $[9,26]$, to pair points and language. Specifically, we begin by projecting the 3D scenes onto 2D images to align points with pixels. Then by connecting the points $\hat{\mathbf{p}}$ within each 2D region to their respective captions, we obtain the regional 3D-language pairs $\left\langle\hat{\mathbf{p}}, \mathbf{t}^{r}\right\rangle$.}</p>
<h3>3.3. Benchmark and Analysis on Regional 3DLanguage Pairs</h3>
<p>With the constructed five types of regional 3D-language pairs $\mathbf{t}^{r}=\left{\mathbf{t}^{\text {sw }}, \mathbf{t}^{\text {det-c }}, \mathbf{t}^{\text {det-t }}, \mathbf{t}^{\text {giti }}, \mathbf{t}^{\text {kos }}\right}$, the follow-up question is which delivers the best performance on learning 3D open-world representation and how to combine them to obtain enriched vocabulary space and denser regional 3Dlanguage association. Hence, we benchmark them on ScanNet [9] semantic segmentation tasks with different novel categories and 2D image quantities ( $25 \mathrm{~K} v s .125 \mathrm{~K}$ ). Our benchmark encompasses two settings: $i$ ) the B12/N7 setting including 12 annotated base categories and 7 unannotated novel categories, which requires a strong comprehension of a large vocabulary corpus; ii) the annotation-free setting, wherein all categories are novel ones, and thus necessitates both open-vocabulary recognition and precise object localization with only sparse 3D-language pairs.
Complementary cues. As shown in the upper of Table 1, no single type of 3D-language source consistently outperforms others in all settings, and each association has its own merits. For example, $\mathbf{t}^{\text {det-t }}$ inherits the advanced small object localization capabilities (refer to Figure 2 middle for "traffic light" and "wheel" descriptions.), excelling others in the ScanNet B12/N7 (125K). However, it suffers from the limited pre-defined vocabulary space and obtains the
worst performance in the annotation-free setting with 17 novel categories. In contrast, dense captioners $\mathbf{t}^{\text {kos }}$ and $\mathbf{t}^{\text {giti }}$ offer salient object localization with semantic-rich vocabulary (refer to Figure 2 right for attribute descriptions), exhibiting superior results on the annotation-free setting. This suggests that different VL models and visual prompts offer various merits and might complement each other.
End-to-end manners scale better. When comparing the performance of utilizing 25 K and 125 K images, we find that end-to-end trained dense captioners and detectors (i.e. $\mathbf{t}^{\text {kos }}, \mathbf{t}^{\text {giti }}$ and $\mathbf{t}^{\text {det-t }}$ ) scale better than the two-stage image captioning manners with visual prompts. The reason might be that end-to-end trained dense caption sources are more consistent on different views and thus yield fewer semantic conflicts when scaling up to more views.
Common combinations are not always effective. As abovementioned, different 3D-language pairs can offer complementary cues. Hence, we examine their synergy effect for better performance. As shown in the bottom of Table 1, we attempt to combine the representatives from three streams of regional caption generation manners $\mathbf{t}^{\text {kos }}, \mathbf{t}^{\text {det-t }}$ and $\mathbf{t}^{\text {sw }}$ via data-level and multi-loss fusion. Nevertheless, the performance lift across different settings is not consistent or only shows incremental increases, which suggests the need for a more dedicated fusion strategy to accommodate extensive dense language supervision from multiple sources.</p>
<h3>3.4. Boost Synergy of Diverse 3D-language Sources</h3>
<p>Motivated by the observations of complementary merits of individual 3D-language sources and their unsatisfactory synergy results, we further study how to combine these varied 3D-language sources effectively and efficiently. In this regard, we propose a Supplementary-orientated Fusion (SFusion) strategy to integrate the most diverse semantic clues while filtering out potential conflicts from different caption sources. As data-level mixing delivers better performance than loss-level combination, we focus on tackling the bottleneck of data-level 3D-language pairs fusion here. When training 3D models on data-level mixed 3Dlanguage pairs, they are learning from a more informative language description, but suffer from sub-optimal performance. This suggests that the main challenges in straightforward data-level mixing are the redundancy and conflicts from different caption sources, especially for highly overlapped point cloud regions (see Figure 1). For those highly overlapped 3D regions with multiple language sources, mutually conflicting descriptions will confuse models, and the overabundance of repetitive language descriptions tends to overwhelm optimization toward easily identifiable areas, leading to sub-optimal performance.</p>
<p>Hence, our SFusion addresses these problems by fusing 3D-language pairs with low 3D overlaps to alleviate potential conflicts in overlapped areas and obtain spatially supplementary 3D-language pairs. Specifically, we first select</p>
<p>the most reliable caption source that performs best as the primary 3D-language source $\mathbf{t}^{\text {pri }}$. Then, we compute the overlap ratio $\tau$ of point sets between the primary source and candidate caption sources $\mathbf{t}^{\text {can }}$ on $i$-th 3D scene as follows,</p>
<p>$$
\tau_{j k}=\operatorname{overlap}\left(\tilde{\mathbf{p}}<em i="i" k="k">{i j}^{\mathrm{pri}}, \tilde{\mathbf{p}}</em>}^{\mathrm{can}}\right), \quad \hat{\tau<em j="j">{k}=\max </em>
$$} \tau_{j k</p>
<p>where overlap measures the intersection over union (IoU) between two point sets, $\tilde{\mathbf{p}}<em i="i" k="k">{i j}^{\mathrm{pri}}$ and $\tilde{\mathbf{p}}</em>}^{\text {can }}$ are the $j$-th and $k$-th point set in the $i$-th scene from the primary source and candidate source, respectively. Then, we define thresholds $T_{l}$ and $T_{h}$ to filter out 3D-language pairs with high overlap ratios from $\mathbf{t}^{\text {can }}$, which might result in redundant or conflict supervision to the primary source. Hence, only candidate 3D-language pairs $\left\langle\tilde{\mathbf{p}<em i="i" k="k">{i k}^{\text {can }}, \mathbf{t}</em>}^{\text {can }}\right\rangle$ with $T_{l}&lt;\hat{\tau<em h="h">{k}&lt;T</em>$ set to zero) are fused with the primary source. This procedure can be iteratively applied across all candidate caption sources to obtain a collection of 3D-language pairs with low geometrical overlaps. This refined set will serve as the supervision for the follow-up contrastive training. Notice that we also introduce a hyper-parameter $\epsilon \in[0,1]$ to control the ratio of the primary source and candidate source during fusion, as maintaining the majority of primary sources is beneficial during training with multi-source 3D-language pairs. Our experimental results in Table 7 verify our above claims and demonstrate that our SFusion strategy can significantly boost the combination of multiple language sources.}$ ( $T_{l</p>
<h3>3.5. Region-aware Point-discriminative Contrastive Learning</h3>
<p>After obtaining 3D-language pairs $\langle\tilde{\mathbf{p}}, \mathbf{t}\rangle$ for supervision, we proceed to train $\mathbf{F}<em _theta="\theta">{3 \mathrm{D}}$ and $\mathbf{F}</em>$ to align 3D features with language features for open-world learning. We introduce region-aware point-discriminative contrastive loss as below.
CLIP-style Contrastive Loss. We can pull paired 3D features and language features closer while pushing away the unmatched ones through CLIP-style [28] contrastive loss (refer to Figure 1 top right). It can be formulated as:</p>
<p>$$
\begin{gathered}
\mathbf{f}^{\tilde{p}}=\operatorname{Pool}\left(\tilde{\mathbf{p}}, \mathbf{f}^{p}\right), \quad \hat{\mathbf{z}}=\mathbf{f}^{\tilde{p}} \cdot \mathbf{F}^{t}, \quad \hat{\mathbf{s}}=\sigma(\hat{\mathbf{z}}) \
\mathcal{L}_{c}=-\mathbf{y}^{t} \cdot \ln \hat{\mathbf{s}}
\end{gathered}
$$</p>
<p>where $\mathbf{f}^{\tilde{p}}$ is the average-pooled region feature, $\operatorname{Pool}\left(\tilde{\mathbf{p}}, \mathbf{f}^{\tilde{p}}\right)$ is our custom CUDA operator to gather features $\mathbf{f}^{\tilde{p}}$ over point set $\tilde{\mathbf{p}}, \mathbf{F}^{t}=\left[\mathbf{f}<em 2="2">{1}^{t}, \mathbf{f}</em>}^{t}, \cdots \mathbf{f<em t="t">{n</em>$ optimized in}}^{t}\right]$ concatenates all caption embeddings in a scene, $\hat{\mathbf{z}}$ and $\hat{\mathbf{s}}$ measure the similarity and the score probability between a 3D region and all captions, $\sigma$ is sigmoid function and $\mathbf{y}^{t}$ is the one-hot label highlighting the position paired with $\tilde{\mathbf{p}}$. While CLIP [28] targets learning a global image-level feature for the classification task, it neglects the demand of learning point-wise discriminative features for dense prediction tasks. As shown in Figure 1, the pooling operation will average the point-wise features and make all points in the same region $\tilde{\mathbf{p}</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the same direction, preventing the learning of discriminative representations for dense prediction tasks. We present more analysis on this undesired effect in the suppl..</p>
<p>Point-discriminative Contrastive Loss. Considering the limitation of CLIP-style loss, we propose a pointdiscriminative contrastive loss $\mathcal{L}<em _pdc="{pdc" _text="\text">{\text {pdc }}$ to make the learning of point embedding discriminative. Specifically, for each regional 3D-language pair, instead of aggregating point features into an averaged region-level feature, our $\mathcal{L}</em>$ as follows,}}$ directly computes the similarity between point-wise embeddings and caption embeddings. We then pool the logarithm of predicted point-wise probability within $\tilde{\mathbf{p}}$ to compute the cross-entropy loss regarding one-hot label $\mathbf{y}^{t</p>
<p>$$
\mathbf{z}=\mathbf{f}^{p} \cdot \mathbf{F}^{t}, \quad \mathbf{s}=\sigma(\mathbf{z}), \quad \mathcal{L}_{\mathrm{pdc}}=-\mathbf{y}^{t} \cdot \operatorname{Pool}(\tilde{\mathbf{p}}, \ln \mathbf{s})
$$</p>
<p>where $\mathbf{z}$ and $\mathbf{s}$ indicate the similarity and probability matrix between point-wise features and all caption embeddings. By doing so, the optimization direction of each point will be adapted to its own point embeddings and thus make them discriminative (refer to Figure 1). More details are included in the supplementary materials.</p>
<p>Region-aware Normalization. Though discriminative, the $\mathcal{L}<em _pdc="{pdc" _text="\text">{\text {pdc }}$ will back-propagate smaller gradients to points in large regions due to the pooling operation, leading to an implicit bias towards region size which can be harmful to representation learning. To alleviate this issue, we propose a regionaware factor to normalize $\mathcal{L}</em>$ as follows,}}$ by the region size, to ensure an equivalent gradient scale on points in each region regardless of its size.Obtained region-aware loss $\mathcal{L}_{\text {rpdc }</p>
<p>$$
\mathcal{L}<em r="r">{\mathrm{rpdc}}=-\alpha</em>} \mathcal{L<em r="r">{\mathrm{pdc}}, \quad \alpha</em>
$$}=\frac{n_{t} \cdot \operatorname{card}(\tilde{\mathbf{p}})}{\sum_{i}^{n_{t}} \operatorname{card}\left(\tilde{\mathbf{p}}_{i}\right)</p>
<p>where $\alpha_{r}$ is the region-aware normalization factor, $n_{t}$ is the number of 3D-language pairs each scene.</p>
<p>Analysis. With point-discriminative and region-aware properties, our $\mathcal{L}<em _rpdc="{rpdc" _text="\text">{\text {rpdc }}$ facilitates more superior and robust representation learning. It allows each point to grasp its unique semantics without disruptions from other unrelated points (refer to Figure 1 right). This is especially vital for annotation-free dense prediction to segment object boundaries without any annotation (see Table 6 for verification). Moreover, the region-aware factor in $\mathcal{L}</em>$ provides a more robust optimization procedure. As depicted in the right section of Figure 1, points associated with multiple captions are normalized to a similar gradient scale. When multiple captions reach a consensus, this leads to consistent gradient directions, thereby encouraging them to be optimized in a unified direction. Conversely, when multiple captions conflict, this leads to inconsistent gradient directions and thus discourages the noisy optimization.}</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>ScanNet<em>[8]</em></th>
<th></th>
<th></th>
<th>nuScenes<em>[4]</em></th>
<th></th>
<th>ScanNet200<em>[29]</em></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>B15/N4</td>
<td>B12/N7</td>
<td>B10/N9</td>
<td>B12/N3</td>
<td>B10/N5</td>
<td>B170/N30</td>
<td>B150/N50</td>
</tr>
<tr>
<td>3DGenZ<em>[25]</em></td>
<td>20.6 / 56.0 / 12.6</td>
<td>19.8 / 35.5 / 13.3</td>
<td>12.0 / 63.6 / 6.6</td>
<td>1.6 / 53.3 / 0.8</td>
<td>1.9 / 44.6 / 1.0</td>
<td>2.6 / 15.8 / 1.4</td>
<td>3.3 / 14.1 / 1.9</td>
</tr>
<tr>
<td>3DTZSL<em>[6]</em></td>
<td>10.5 / 36.7 / 6.1</td>
<td>3.8 / 36.6 / 2.0</td>
<td>7.8 / 55.5 / 4.2</td>
<td>1.2 / 21.0 / 0.6</td>
<td>6.4 / 17.1 / 3.9</td>
<td>0.9 / 4.0 / 0.5</td>
<td>0.7 / 3.8 / 0.4</td>
</tr>
<tr>
<td>OVSeg-3D<em>[9]</em></td>
<td>0.0 / 64.4 / 0.0</td>
<td>0.9 / 55.7 / 0.1</td>
<td>1.8 / 68.4 / 0.9</td>
<td>0.6 / 74.4 / 0.3</td>
<td>0.0 / 71.5 / 0.0</td>
<td>1.5 / 21.1 / 0.8</td>
<td>3.0 / 20.6 / 1.6</td>
</tr>
<tr>
<td>PLA<em>[9]</em></td>
<td>65.3 / 68.3 / 62.4</td>
<td>55.3 / 69.5 / 45.9</td>
<td>53.1 / 76.2 / 40.8</td>
<td>47.7 / 73.4 / 35.4</td>
<td>24.3 / 73.1 / 14.5</td>
<td>11.4 / 20.9 / 7.8</td>
<td>10.1 / 20.9 / 6.6</td>
</tr>
<tr>
<td>RegionPLC</td>
<td>69.4 / 68.2 / 70.7</td>
<td>68.2 / 69.9 / 66.6</td>
<td>64.3 / 76.3 / 55.6</td>
<td>64.4 / 75.8 / 56.0</td>
<td>49.0 / 75.8 / 36.3</td>
<td>16.6 / 21.6 / 13.9</td>
<td>14.6 / 22.4 / 10.8</td>
</tr>
<tr>
<td>Fully-Sup.</td>
<td>73.3 / 68.4 / 79.1</td>
<td>70.6 / 70.0 / 71.8</td>
<td>69.9 / 75.8 / 64.9</td>
<td>73.7 / 76.6 / 71.1</td>
<td>74.8 / 76.8 / 72.8</td>
<td>20.9 / 21.7 / 20.1</td>
<td>20.6 / 22.0 / 19.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for open-world 3D semantic segmentation on ScanNet, nuScenes and ScanNet200 in terms of hIoU / mIoU^{B} / mIoU^{N}.
Best open-world results are presented in bold.</p>
<h2>4 Experiments</h2>
<h3>4.1 Basic Setups</h3>
<p>Datasets and Validation Settings. To test the effectiveness of RegionPLC, we evaluate it on three popular datasets: ScanNet<em>[8]</em>, ScanNet200<em>[29]</em> and nuScenes<em>[4]</em>, covering indoor and outdoor scenarios. We validate the open-world capability of our method with different numbers of annotated categories, including base-annotated open world (i.e. part of categories annotated) and annotation-free open world (i.e. no category annotated). We evaluate our method’s performance on both semantic segmentation and instance segmentation tasks.</p>
<p>Category Partition. We split categories into base and novel on ScanNet<em>[8]</em> following PLA<em>[9]</em>. For nuScenes<em>[4]</em>, we ignore the “otherflat” class and randomly divide the rest classes into B12/N3 (i.e. 12 base and 3 novel categories) and B10/N5. For ScanNet200<em>[29]</em>, we randomly split 200 classes to B170/N30 and B150/N50. See Suppl. for details.</p>
<p>Evaluation Metrics. For semantic segmentation, we follow<em>[9, 38]</em> to employ mIoU^{B}, mIoU^{N} and harmonic mean IoU (hIoU) for evaluating base, novel categories and their harmonic mean separately. Similarly, for instance segmentation, we employ mAP${}<em 50="50">{50}^{\mathrm{B}}$, mAP${}</em>) excluding “wall”, “floor” and “ceiling” for evaluation.}^{\mathrm{N}}$ and hAP_{50}.For annotation-free semantic segmentation, we use mean IoU and mean accuracy on foreground classes (i.e. mIoU^{†} and mAcc^{†</p>
<p>Implementation Details. We adopt the sparse-convolution-based UNet<em>[14]</em> as the 3D encoder with CLIP<em>[28]</em> text encoder as the final classifier for 3D semantic segmentation, and SoftGroup<em>[34]</em> for instance segmentation as<em>[9]</em>. We use category prompts to replace ambiguous category names such as “manmade” and “drivable surface” with a list of concrete category names when encoding category embeddings. We run all experiments with a batch size of 32 on 8 NVIDIA V100 or A100 (see Suppl. for more details).</p>
<h3>4.2 Base-annotated Open World</h3>
<p>Comparison Methods. We compare RegionPLC to previous open-world or zero-shot works. 3DGenZ<em>[25]</em> and 3DTZSL<em>[6]</em> are early works for 3D zero-shot learning reproduced by<em>[9]</em>. OVSeg-3D extends LSeg to 3D<em>[22]</em>, reported by<em>[9]</em>. PLA<em>[9]</em> is the previous cutting-edge method.</p>
<p>3D Semantic Segmentation. As shown in Table 2, compared to the previous state-of-the-art method PLA<em>[9]</em>, our method largely lifts the mIoU of unseen categories by $8.3\%\sim 21.8\%$ among various partitions on ScanNet and nuScenes. Furthermore, when compared to baselines without language supervision, i.e. 3DGenZ<em>[25]</em> and 3DTZSL<em>[6]</em>, our method even obtains $30.6\%\sim 42.7\%$ performance gains regarding mIoU on novel categories among different partitions and datasets. These significant and consistent improvements across indoor and outdoor scenarios show the effectiveness of our RegionPLC framework.</p>
<p>Furthermore, when facing more long-tail dataset ScanNet200<em>[29]</em>, our method still obtains notable mIoU^{N} gains ranging from $4.2\%$ to $5.1\%$ compared to PLA<em>[9]</em> as shown in Table 2. In this regard, our proposed region-level language supervision and region-aware point-discriminative contrastive loss show its potential to address 3D open-world understanding in complex and long-tail scenarios.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>ScanNet</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>B13/N4</td>
<td>B10/N7</td>
<td>B8/N9</td>
</tr>
<tr>
<td>OVSeg-3D<em>[22]</em></td>
<td>5.1 / 57.9 / 2.6</td>
<td>2.0 / 50.7 / 1.0</td>
<td>2.4 / 59.4 / 1.2</td>
</tr>
<tr>
<td>PLA<em>[9]</em></td>
<td>55.5 / 58.5 / 52.9</td>
<td>31.2 / 54.6 / 21.9</td>
<td>35.9 / 63.1 / 25.1</td>
</tr>
<tr>
<td>RegionPLC</td>
<td>58.2 / 59.2 / 57.2</td>
<td>40.6 / 53.9 / 32.5</td>
<td>46.8 / 62.5 / 37.4</td>
</tr>
<tr>
<td>Fully-Sup.</td>
<td>64.5 / 59.4 / 70.5</td>
<td>62.5 / 57.6 / 62.0</td>
<td>62.0 / 65.1 / 62.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for open-world 3D instance segmentation on ScanNet in terms of hAP_{50} / mAP${}<em 50="50">{50}^{\mathrm{B}}$ / mAP${}</em>$.}^{\mathrm{N}</p>
<p>3D Instance Segmentation. As our pipeline provides local language descriptions to fine-grained point sets and encourage points to learn discriminative features, it also benefits instance-level localization task. As shown in Table 3, our method consistently brings $4.3\%\sim 12.3\%$ gains compared to the state-of-the-art PLA<em>[9]</em> across three partitions on ScanNet. It is noteworthy that our method obtains more obvious improvements for partitions with fewer base categories (i.e. B10/N7 and B8/N9), demonstrating the effectiveness of our RegionPLC in enabling the model to distinguish unseen instances without human annotations.</p>
<h3>4.3 Annotation-free Open World</h3>
<p>Comparison Methods. As shown in Table 4, we compare two streams of methods: $i)$ Training-free methods using multi-view images for inference<em>[26, 47]</em>. $ii)$ Methods leveraging 2D vision-language models during training<em>[9, 26]</em>.</p>
<p>3D Semantic Segmentation. As shown in Table 4, our RegionPLC with SparseUNet32<em>[14]</em> backbone significantly outperforms all other competitive methods by $1.8\%\sim$ $57.5\%$ mIoU^{†} and $7.2\%\sim 72\%$ mAcc^{†}. This is the first time that a 3D open-world model achieves state-of-the-art</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Network</th>
<th>mIoU^{†}</th>
<th>mAcc^{†}</th>
<th>Multi-view Infer</th>
<th>GT Instance Mask</th>
<th>Train Hours</th>
<th>Extra Storage</th>
<th>Latency</th>
</tr>
</thead>
<tbody>
<tr>
<td>MaskCLIP^{†} [47]</td>
<td>CLIP [28]</td>
<td>23.1</td>
<td>40.9</td>
<td>✓</td>
<td>$\times$</td>
<td>-</td>
<td>-</td>
<td>1.7 s</td>
</tr>
<tr>
<td>OpenScene-2D [26]</td>
<td>LSeg [22]</td>
<td>58.0</td>
<td>68.5</td>
<td>✓</td>
<td>$\times$</td>
<td>-</td>
<td>-</td>
<td>106.1s</td>
</tr>
<tr>
<td>OpenScene-3D^{‡} [26]</td>
<td>SparseUNet16 [14]</td>
<td>57.2</td>
<td>69.9</td>
<td>$\times$</td>
<td>$\times$</td>
<td>24.7 h</td>
<td>117.3 G</td>
<td>0.08 s</td>
</tr>
<tr>
<td>OpenScene-3D^{‡} [26]</td>
<td>SparseUNet32 [14]</td>
<td>57.8</td>
<td>70.3</td>
<td>$\times$</td>
<td>$\times$</td>
<td>25.3 h</td>
<td>117.3 G</td>
<td>0.10 s</td>
</tr>
<tr>
<td>PLA^{‡} [9]</td>
<td>SparseUNet16 [14]</td>
<td>17.7</td>
<td>33.5</td>
<td>$\times$</td>
<td>$\times$</td>
<td>11.5 h</td>
<td>1.1 G</td>
<td>0.08 s</td>
</tr>
<tr>
<td>PLA^{‡} [9]</td>
<td>SparseUNet32 [14]</td>
<td>19.1</td>
<td>41.5</td>
<td>$\times$</td>
<td>$\times$</td>
<td>12.0 h</td>
<td>1.1 G</td>
<td>0.10 s</td>
</tr>
<tr>
<td>RegionPLC</td>
<td>SparseUNet16 [14]</td>
<td>56.9</td>
<td>75.6</td>
<td>$\times$</td>
<td>$\times$</td>
<td>12.5 h</td>
<td>5.5 G</td>
<td>0.08 s</td>
</tr>
<tr>
<td>RegionPLC</td>
<td>SparseUNet32 [14]</td>
<td>59.6</td>
<td>77.5</td>
<td>$\times$</td>
<td>$\times$</td>
<td>13.0 h</td>
<td>5.5 G</td>
<td>0.10 s</td>
</tr>
<tr>
<td>RegionPLC + OpenScene-3D^{‡}</td>
<td>SparseUNet16 [14]</td>
<td>60.1</td>
<td>74.4</td>
<td>$\times$</td>
<td>$\times$</td>
<td>25.9 h</td>
<td>122.8 G</td>
<td>0.08 s</td>
</tr>
<tr>
<td>RegionPLC + OpenScene-3D^{‡}</td>
<td>SparseUNet32 [14]</td>
<td>63.6</td>
<td>80.3</td>
<td>$\times$</td>
<td>$\times$</td>
<td>26.4 h</td>
<td>122.8 G</td>
<td>0.10 s</td>
</tr>
<tr>
<td>Fully-Sup.</td>
<td>SparseUNet16 [14]</td>
<td>75.9</td>
<td>84.8</td>
<td>$\times$</td>
<td>$\times$</td>
<td>9.6 h</td>
<td>-</td>
<td>0.08 s</td>
</tr>
<tr>
<td>Fully-Sup.</td>
<td>SparseUNet32 [14]</td>
<td>77.9</td>
<td>86.2</td>
<td>$\times$</td>
<td>$\times$</td>
<td>10.5 h</td>
<td>-</td>
<td>0.10 s</td>
</tr>
</tbody>
</table>
<p>Table 4: Annotation-free 3D semantic segmentation on ScanNet. ^{‡} and ^{‡} mean results reproduced by us and Uni3D, independently.
performance without any 3D annotation or 2D pixel-aligned image features but only sparse language supervision for learning. Moreover, our RegionPLC can scale up by scaling the 3D backbone from SparseUNet16 to SparseUNet32, obtaining 2.6% mIoU^{†} gains, which shows the advantage of learning from sparse language supervision instead of pixel-aligned feature distillation from 2D encoders [26]. It is also noteworthy that RegionPLC can function as a lightweight plug-and-play module and thus be integrated with other methods such as OpenScene [26] to further boost about 4% mIoU^{†}. Notably, our method is training-efficient, requiring less disk storage and training time compared to OpenScene.</p>
<table>
<thead>
<tr>
<th>[26]</th>
<th>[9]</th>
<th>RegionPLC</th>
<th>RegionPLC + [26]</th>
<th>Fully-Sup.</th>
</tr>
</thead>
<tbody>
<tr>
<td>5.9 (10.2)</td>
<td>1.8 (3.1)</td>
<td>9.1 (17.3)</td>
<td>9.6 (17.8)</td>
<td>23.9 (32.9)</td>
</tr>
</tbody>
</table>
<p>Table 5: Annotation-free open-world semantic segmentation on ScanNet200 [29] in terms of mIoU^{†} (mAcc^{†}).
Long-tail Scenario. As shown in Table 5, we set up comparisons on the more challenging long-tail dataset ScanNet200 [29]. Notably, our RegionPLC surpasses other counterparts by $3.2\%\sim 7.4\%$ mIoU^{†} and $7.1\%\sim 14.2\%$ mAcc^{†}. Specifically, OpenScene is less effective on ScanNet200 with a large number of fine-grained categories as it inherits the shortcomings or bias of the 2D segmentation model that forgets a large number of concepts during finetuning, as verified in [11, 18]. In contrast, our RegionPLC directly learns in a rich vocabulary space with dense and diverse captions which is closer to real open-world scenarios.</p>
<h3>4.4 Qualitative Studies</h3>
<p>To demonstrate the open-world capability of our RegionPLC, we provide compelling qualitative results showcasing its capability in recognizing and localizing novel categories. As illustrated in Figure 3 (a), RegionPLC successfully identifies numerous categories without any human annotation, demonstrating the quality and richness of our region-level captions and the effectiveness of our region-aware point-discriminative learning objective. For base-annotated cases, our model can recognize challenging tail classes such “keyboard” and “laddar” with precise segmentation in indoor scenarios (see Figure 3 (b)) and small-scale objects with only a few points such as “motorcycle” in outdoor scenarios (see Figure 3 (c)). Moreover, RegionPLC shows a strong localization ability in open-world instance segmentation, ac- curately grouping novel objects as shown in Figure 3 (d).</p>
<h2>5 Ablation Study</h2>
<p>In this section, we examine key components of our framework through in-depth ablation studies. Results for base-annotated and annotation-free experiments are measured in hIoU / mIoU^{B} / mIoU^{N} and mIoU^{†} (mAcc^{†}) separately.</p>
<table>
<thead>
<tr>
<th>Components</th>
<th></th>
<th></th>
<th></th>
<th>ScanNet</th>
<th>ScanNet</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathbf{t}^{×+\epsilon}$</td>
<td>$\mathbf{t}^{r}$</td>
<td>$\mathcal{L}_{\text{pdc}}$</td>
<td>$\mathcal{L}_{\text{spdc}}$</td>
<td>SFusion</td>
<td>B0/N17</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>0.3 (5.3)</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>17.7 (33.5)</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td></td>
<td></td>
<td>21.7 (37.1)</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td></td>
<td>50.6 (71.1)</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>51.7 (72.7)</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>56.9 (75.5)</td>
</tr>
</tbody>
</table>
<p>Table 6: Component analysis on ScanNet. $\mathbf{t}^{×+\epsilon}$ and $\mathbf{t}^{r}$ denotes the combination of view and entity language supervision [9] and best region-level language supervision, respectively.
Component Analysis. Here, we study the effectiveness of our proposed regional captions $\mathbf{t}^{r}$, the SFusion strategy for caption integration, point-discriminative contrastive loss $\mathcal{L}<em _text_spdc="\text{spdc">{\text{pdc}}$ and its region-aware variants $\mathcal{L}</em>}}$. As shown in Table 6, when compared to view- and entity-level captions used in PLA [9], our region-level language supervision delivers consistent boosts about $4\%\sim 10.8\%$ across different category partitions. Additionally, $\mathcal{L<em _text_spdc="\text{spdc">{\text{pdc}}$ achieves considerable gains when paired with $\mathbf{t}^{r}$. Particularly, it brings $28.9\%$ mIoU^{†} gains in the annotation-free setting, illustrating its superiority in learning point-discriminative features for dense parsing tasks. When combined with the region-aware factor, $\mathcal{L}</em>}}$ surpasses $\mathcal{L<em l="l">{\text{pdc}}$ by $1.1\%\sim 3.8\%$ mIoU^{†}. Lastly, $2\%\sim 5.2\%$ improvements yielded from SFusion strategy confirm its effectiveness in eliminating redundancy and conflicts from multiple captions for training.
SFusion. We also study the effectiveness of our SFusion strategy. As shown in Table 7 (b), with a similar ratio $\epsilon$ of the main caption source relative to all merged captions, fusing 3D-language pairs that have a low spatial overlap ratio (i.e., less than 0.5) yields superior results compared to fusing pairs that are highly overlapped (i.e., greater than 0.5). Besides, as shown in Table 7 (a), our SFusion largely outperforms the naive data-mixing strategy ( $\left[T</em>\right]=[0.0$, 1.0]) with $1.9\%\sim 3.5\%$ gains. These experimental results affirm that potential conflicts and redundancies introduced}, T_{h</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3. Qualitative results of our RegionPLC. The examples above show annotation-free open-world scene parsing where no human annotation is available (see (a)), and base-annotated open-world learning where a limited number of base classes are annotated (see (b), (c), (d)) for semantic and instance segmentation covering both indoor and outdoor scenarios. Unseen categories are highlighted in colors.</p>
<table>
<thead>
<tr>
<th>Caption source</th>
<th>$[T_i, T_h]$</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$[0.0,1.0]$</td>
<td>$[0.5,1.0]$</td>
<td>$[0.0,0.5]$</td>
<td>$[0.0,0.2]$</td>
</tr>
<tr>
<td>$\mathbf{t}^{\text{low}}$, $\mathbf{t}^{\text{on}}$</td>
<td>53.1 (73.9)</td>
<td>54.6 (75.3)</td>
<td>54.3 (74.4)</td>
<td>56.6 (74.7)</td>
</tr>
<tr>
<td>$\mathbf{t}^{\text{low}}$, $\mathbf{t}^{\text{den-t}}$</td>
<td>54.0 (74.5)</td>
<td>54.1 (73.7)</td>
<td>54.3 (73.9)</td>
<td>55.9 (76.1)</td>
</tr>
<tr>
<td>$\mathbf{t}^{\text{low}}$, $\mathbf{t}^{\text{on}}$, $\mathbf{t}^{\text{den-t}}$</td>
<td>54.9 (74.2)</td>
<td>55.2 (73.7)</td>
<td>55.4 (75.3)</td>
<td>56.9 (75.5)</td>
</tr>
</tbody>
</table>
<p>(a) Ablation of various caption sources and caption overlap thresholds. * equals to the data-mixing baseline. We report mIoU (mAcc) here.</p>
<table>
<thead>
<tr>
<th>$[T_i, T_h]$</th>
<th>$[0.5,1.0]$</th>
<th>$[0.0,0.5]$</th>
<th>$[0.0,0.5]$</th>
<th>$[0.0,0.2]$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ratio ( $\epsilon$ )</td>
<td>$0.75^{\triangleleft}$</td>
<td>$0.34^{\triangleleft}$</td>
<td>0.75</td>
<td>$0.72^{\triangleleft}$</td>
</tr>
<tr>
<td>mIoU (mAcc)</td>
<td>54.6 (75.3)</td>
<td>54.3 (74.4)</td>
<td>55.4 (75.6)</td>
<td>56.6 (74.7)</td>
</tr>
</tbody>
</table>
<p>(b) Ablation of caption overlap thresholds and their ratios when fusing $\mathbf{t}^{\text{low}}$ and $\mathbf{t}^{\text{on}}$. ${ }^{\triangleleft}$ means the raw ratio for $\mathbf{t}^{\text{low}}$ and $\mathbf{t}^{\text{low}}+\mathbf{t}^{\text{on}}$ with no $\epsilon$ applied.</p>
<p>Table 7. SFusion results for zero-shot semantic segmentation considering caption sources, overlap thresholds, and ratios.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. (a) Visualizations of RegionGR that integrates LLM for open-ended grounded 3D reasoning. (b) Demonstrating the versatility of RegionGR via more examples of answering user queries.</p>
<p>by regions with a high degree of overlap restrict the benefits derived from multiple language sources. Fortunately, our SFusion method effectively tackles this challenge.</p>
<h2>6. Open-ended Grounded 3D Reasoning</h2>
<p>Recently, there has been a growing interest in employing language as an interface for connecting human intentions with visual understanding, which facilitates high-level reasoning and planning in the development of embodied agents. Without specific design, RegionPLC can be seamlessly integrated with large language models to enable open-ended grounded 3D reasoning, referred to as RegionGR. As depicted in Figure 4 (a), RegionGR integrates large language models (LLM) for 3D reasoning with regional 3D-language pairs as a knowledge base and utilizes RegionPLC to coarsely locate and identify corresponding objects within the 3D scene for grounded reasoning. Moreover, Figure 4 (b) further exhibits the versatility of RegionGR in responding to user intentions, from summarization to reasoning and planning, particularly within user-specified regions of interest (as shown in the summarization example).</p>
<p>Specifically, RegionGR runs in three steps: (i) We first initialize "environment context" with our regional 3D-language pairs, which enables LLM to understand a given scene. If a user specifies an interested 3D region (see Figure 4 (b) left), highly overlapped 3D-language pairs are kept. (ii) We then feed our prompt in Sec. S4 of suppl. along with "user query" and "environment context" into LLM to generate answers. (iii) Finally, we parse objects from LLM's response and ground them with RegionPLC.</p>
<h2>7. Conclusion</h2>
<p>We present RegionPLC, a holistic regional point-language contrastive learning framework to recognize and localize unseen categories in open-world 3D scene understanding. By leveraging advanced VL models and our SFusion strategy, RegionPLC effectively builds comprehensive regional point-language pairs. Furthermore, our region-aware point-discriminative contrastive loss aids in learning distinctive and robust features from regional captions. Extensive experiments demonstrate that RegionPLC remarkably outperforms prior open-world methods in both indoor and outdoor scenarios and excels in challenging long-tail or annotation-free scenarios. Besides, RegionPLC can be effortlessly integrated with LLM for grounded 3D visual reasoning.</p>
<p>Acknowledgement This work has been supported by Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27209621), General Research Fund Scheme (Grant No. 17202422), and RGC Matching Fund Scheme (RMGS). Part of the described research work is conducted in the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust.</p>
<h2>References</h2>
<p>[1] Vit-gpt2 image captioning. https://huggingface. co / nlpconnect / vit - gpt2 - image captioning/discussions. 13
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. 1, 2
[3] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. 1
[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621-11631, 2020. 2, 6, 11
[5] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022. 13
[6] Ali Cheraghian, Shafin Rahman, Dylan Campbell, and Lars Petersson. Transductive zero-shot learning for 3d point cloud classification. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 923-933, 2020. 6
[7] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30753084, 2019. 2
[8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niellner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828-5839, 2017. 2, 4, 6, 11, 13, 14
[9] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Language-driven openvocabulary 3d scene understanding. arXiv preprint arXiv:2211.16312, 2022. 1, 2, 4, 6, 7, 11, 12, 13, 14
[10] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Lowis3d: Language-driven open-world instance-level 3d scene understanding. arXiv preprint arXiv:2308.00353, 2023. 1, 2
[11] Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, and Haoxuan Ding. Don't stop learning: Towards
continual learning for the clip model. arXiv preprint arXiv:2207.09248, 2022. 7
[12] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In European Conference on Computer Vision, 2021. 13
[13] Benjamin Graham and Laurens van der Maaten. Submanifold sparse convolutional networks. arXiv preprint arXiv:1706.01307, 2017. 2
[14] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9224-9232, 2018. 2, 6, 7
[15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356-5364, 2019. 3, 11
[16] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang, and Wangmeng Zuo. Clip2point: Transfer clip to point cloud classification with image-depth pre-training. arXiv preprint arXiv:2210.01055, 2022. 2
[17] Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, and Joan Lasenby. Openins3d: Snap and lookup for 3d open-vocabulary instance segmentation. arXiv preprint arXiv:2309.00616, 2023.
[18] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023. 2, 7, 12, 13
[19] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, ChiWing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2
[20] Maksim Kolodiazhnyi, Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Top-down beats bottom-up in 3d instance segmentation. arXiv preprint arXiv:2302.02871, 2023. 2
[21] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8500-8509, 2022. 2
[22] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In International Conference on Learning Representations, 2022. 1, 2, 6, 7, 13
[23] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965-10975, 2022. 1
[24] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and</p>
<p>Hao Su. Openshape: Scaling up 3d shape representation towards open-world understanding. arXiv preprint arXiv:2305.10764, 2023. 2
[25] Björn Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet. Generative zero-shot learning for semantic segmentation of 3d point clouds. In 2021 International Conference on 3D Vision (3DV), pages 992-1002. IEEE, 2021. 6
[26] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. arXiv preprint arXiv:2211.15654, 2022. 1, 2, 4, 6, 7, 11, 12, 13,15
[27] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 1, 3, 4, 13
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 2, 3, 5, 6, 7, 11, 12, 13
[29] David Rozenberszki, Or Litany, and Angela Dai. Languagegrounded indoor 3d semantic segmentation in the wild. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIII, pages 125-141. Springer, 2022. 2, 6, 7, 11, 12
[30] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog, 2022. 13
[31] Ayça Takmaz, Elisabetta Fedele, Robert W Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Openmask3d: Open-vocabulary 3d instance segmentation. arXiv preprint arXiv:2306.13631, 2023. 2
[32] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. Proceedings of the IEEE International Conference on Computer Vision, 2019. 2
[33] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, Junyeong Kim, and Chang D Yoo. Softgroup++: Scalable 3d instance segmentation with octree pyramid grouping. arXiv preprint arXiv:2209.08263, 2022. 2
[34] Thang Vu, Kookhoi Kim, Tung M. Luu, Xuan Thanh Nguyen, and Chang D. Yoo. Softgroup for 3d instance segmentation on 3d point clouds. In CVPR, 2022. 2, 6
[35] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. CoRR, abs/2202.03052, 2022. 1, 3, 11, 13
[36] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280, 2022. 1, 3, 4
[37] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 9621-9630, 2019. 2
[38] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero-and few-label semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8256-8265, 2019. 6
[39] Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan Qi. Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3173-3182, 2021. 2
[40] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on point clouds. In Advances in Neural Information Processing Systems, pages 6737-6746, 2019. 2
[41] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. arXiv preprint arXiv:2209.09407, 2022. 2
[42] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative shape proposal network for 3d instance segmentation in point cloud. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3947-3956, 2019. 2
[43] Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Learning synergies between pushing and grasping with self-supervised deep reinforcement learning. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4238-4245. IEEE, 2018. 1
[44] Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, and Hang Xu. Clip2: Contrastive language-image-point pretraining from real-world point cloud data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15244-15253, 2023. 1, 2
[45] Junbo Zhang, Runpei Dong, and Kaisheng Ma. Clip-fo3d: Learning free open-world 3d scene representations from 2d dense clip. arXiv preprint arXiv:2303.04748, 2023. 2
[46] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8552-8562, 2022. 2, 12
[47] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European Conference on Computer Vision (ECCV), 2022. 6, 7, 12
[48] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krähenbühl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In ECCV, 2022. 1, 3, $4,11,13$</p>
<h1>RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding</h1>
<h2>Supplementary Material</h2>
<h2>Outline</h2>
<p>In this supplementary file, we provide more experimental results and details not elaborated in our main paper due to page length limits:</p>
<ul>
<li>Sec. A: Implementation details of RegionPLC and baseline methods.</li>
<li>Sec. B: Additional experimental results on per-class performance, zero-shot domain transfer results, and error-bar analysis.</li>
<li>Sec. C: Qualitative results of RegionPLC.</li>
<li>Sec. D: Prompts for LLM in RegionGR.</li>
<li>Sec. E: Limitation and open problems.</li>
</ul>
<h2>A. Implementation Details</h2>
<p>Here, we present the implementation details of our RegionPLC, dataset category partition and the implementation of baseline methods.</p>
<h2>A.1. Implementation Details of RegionPLC</h2>
<p>Network Architecture. The network architecture of RegionPLC is the same as PLA [9] (i.e. SparseUNet16), ensuring a fair comparison on ScanNet [8]. For ScanNet200 [29], we augment the base hidden dimension (i.e. the hidden dimension of the bottleneck) of sparse UNet from 16 to 32 (i.e. SparseUNet32), yielding better performance on this complex, long-tail dataset. In nuScenes [4], we adopt the same backbone used in ScanNet200 with 5 residual blocks and a base hidden dimension of 32. In addition, in the baseannotated open-world setting, we follow PLA [9] to employ the binary encoder with binary loss, classification head with semantic segmentation loss and instance head with instance loss on base categories.
Training Schedule. We train 512 epochs on ScanNet and ScanNet200, and 50 epochs on nuScenes for semantic segmentation, and 640 epochs for ScanNet instance segmentation. The initial learning rate is set as 0.004 for ScanNet and ScanNet200 and 0.01 for nuScenes. The learning rate decays in cosine and polynomials on ScanNet and nuScenes, respectively.
Regional 3D-Language pairs. Regarding vision-language (VL) models that generate captions, we use OFA [35] for generating $\mathbf{t}^{\text {sw }}$ and $\mathbf{t}^{\text {det-c }}$ as the caption model. As for the object proposal in $\mathbf{t}^{\text {det-t }}$ and $\mathbf{t}^{\text {det-c }}$, we use Detic [48] with LVIS [15] vocabulary space. The prompt template of $\mathbf{t}^{\text {det-t }}$ is the same as CLIP [28]. Other VL foundation models are also feasible, and a more robust VL foundation model
should enhance our methods' performance through providing higher-quality object proposals and language descriptions. By default, we use 125 K frames in the ScanNet dataset and all images in the nuScenes dataset to extract their regional captions, respectively.
Inference Cost Analysis. In the annotation-free setting of the main paper, we evaluate the efficiency of open-world models in terms of training hours (on 8 NVIDIA A100 GPUs), extra storage usage and inference latency (on a single NVIDIA 2080Ti GPU), since they impose a significant overhead during training or inference. The extra storage for RegionPLC and PLA [9] is to store 3D-language pairs, while for OpenScene [26] is to save fused 2D features.
Category Prompt in nuScenes. We use a category prompt for nuScenes to replace ambiguous words within the category names such as "manmade" and "driveable_surface". The concrete category mapping is illustrated in Table 8.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category Name</th>
<th style="text-align: center;">Category Prompts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">barrier</td>
<td style="text-align: center;">barrier or fence</td>
</tr>
<tr>
<td style="text-align: center;">bicycle</td>
<td style="text-align: center;">bicycle or bike or cycle</td>
</tr>
<tr>
<td style="text-align: center;">bus</td>
<td style="text-align: center;">bus</td>
</tr>
<tr>
<td style="text-align: center;">car</td>
<td style="text-align: center;">car</td>
</tr>
<tr>
<td style="text-align: center;">construction_vehicle</td>
<td style="text-align: center;">construction vehicle or bulldozer or excavator or concrete mixer or crane or dump truck</td>
</tr>
<tr>
<td style="text-align: center;">motorcycle</td>
<td style="text-align: center;">motorcycle or motorbike</td>
</tr>
<tr>
<td style="text-align: center;">pedestrian</td>
<td style="text-align: center;">person or people or man or woman</td>
</tr>
<tr>
<td style="text-align: center;">traffic_cone</td>
<td style="text-align: center;">traffic cone</td>
</tr>
<tr>
<td style="text-align: center;">trailer</td>
<td style="text-align: center;">trailer</td>
</tr>
<tr>
<td style="text-align: center;">truck</td>
<td style="text-align: center;">truck</td>
</tr>
<tr>
<td style="text-align: center;">driveable_surface</td>
<td style="text-align: center;">road or street</td>
</tr>
<tr>
<td style="text-align: center;">other flat</td>
<td style="text-align: center;">other flat</td>
</tr>
<tr>
<td style="text-align: center;">sidewalk</td>
<td style="text-align: center;">sidewalk</td>
</tr>
<tr>
<td style="text-align: center;">terrain</td>
<td style="text-align: center;">grass or rolling hills or soil or gravel</td>
</tr>
<tr>
<td style="text-align: center;">manmade</td>
<td style="text-align: center;">building or wall or fence or pole or sign or traffic light</td>
</tr>
<tr>
<td style="text-align: center;">vegetation</td>
<td style="text-align: center;">bushes or plants or trees or potted plants</td>
</tr>
</tbody>
</table>
<p>Table 8. The category prompt for nuScenes.</p>
<h2>A.2. Category Partition for Base-annotated Results</h2>
<p>In the base-annotated open-world setting, we divide all categories into base and novel. As for ScanNet [8], we follow the category partition of PLA [9]. As for nuScenes [4], we discard the ambiguous category "otherflat" and split</p>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Base Categories</th>
<th>Novel Categories</th>
</tr>
</thead>
<tbody>
<tr>
<td>B12/N3</td>
<td>barrier, bicycle, bus, car, construction_vehicle, trailer, truck,</td>
<td>traffic_cone, motorcycle, pedestrian</td>
</tr>
<tr>
<td></td>
<td>diveable_surface, sidewalk, terrain, manmade, vegetation</td>
<td></td>
</tr>
<tr>
<td>B10/N5</td>
<td>bicycle, bus, car, construction_vehicle, trailer, truck,</td>
<td>barrier, motorcycle, pedestrian, traffic_cone, sidewalk</td>
</tr>
<tr>
<td></td>
<td>driveable_surface, terrain, manmade, vegetation</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 9: Category partitions for open-world semantic segmentation on nuScenes.</p>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Novel Categories</th>
</tr>
</thead>
<tbody>
<tr>
<td>B170/N30</td>
<td>pillow, box, clothes, counter, dresser, keyboard, backpack, printer, shower curtain, bin, copier, sofa chair,</td>
</tr>
<tr>
<td></td>
<td>recycling bin, clock, guitar, seat, ladder, cup, toaster, ironing board, toilet seat cover dispenser, furniture, cart,</td>
</tr>
<tr>
<td></td>
<td>projector, shower floor, laundry detergent, bathroom stall door, dumbbell, folded chair, mattress</td>
</tr>
<tr>
<td>B150/N50</td>
<td>couch, window, bookshelf, coffee table, kitchen cabinet, clothes, counter, end table, bag, backpack, printer,</td>
</tr>
<tr>
<td></td>
<td>microwave, shoe, bin, washing machine, sofa chair, paper, blinds, radiator, recycling bin, soap dispenser,</td>
</tr>
<tr>
<td></td>
<td>bucket, stand, light, pipe, bathroom stall, cup, storage bin, coffee maker, machine, fireplace, mini fridge, hat,</td>
</tr>
<tr>
<td></td>
<td>cart, light switch, decoration, plunger, stuffed animal, dish rack, broom, range hood, water pitcher, paper bag,</td>
</tr>
<tr>
<td></td>
<td>bathroom vanity ceiling light, trash bin, stair rail, coat rack, calendar, poster</td>
</tr>
</tbody>
</table>
<p>Table 10: Novel Categories for open-world semantic segmentation on ScanNet200. We only present novel categories here as there are too many base categories to show here. The partition of base categories can be easily obtained by carrying on the set difference between all categories and novel categories.
the remaining 15 categories as illustrated in Table 9. We also randomly split 30 and 50 novel categories for ScanNet200 [29], as shown in Table 10. Notably, 11 categories absent from the ScanNet200 validation set are consistently partitioned into base categories (i.e. training set) to guarantee sufficient novel categories for validation. These 11 train-only categories in ScanNet200 are "bicycle", "storage container", "candle", "guitar case", "purse", "alarm clock", "music stand", "cd case", "structure", "storage organizer" and "luggage".</p>
<h2>A.3. Implementation of Baseline Methods</h2>
<p>We re-produce the baseline methods including MaskCLIP [47], PointCLIP-Seg [46] and OpenScene [26] for annotation-free open-world semantic segmentation in ScanNet. Details are as follows.
PointCLIP-Seg and MaskCLIP. To apply MaskCLIP [47] on 3D segmentation, we assemble its predictions on multiview images and back-project them to 3D space as [9]. As for PointCLIP [46], it cannot be directly utilized for the semantic segmentation task, so we extend a segmentation version by modifying the attentive pooling layer of CLIP [28], as per the method used in MaskCLIP [47]. It is named as PointCLIP-Seg. The major distinction between PointCLIPSeg and MaskCLIP lies in that PointCLIP-Seg uses depth images rather than RGB images for extracting 2D features.
OpenScene. We use the official fused feature released by OpenScene [26] and its prompt engineering techniques to obtain OpenScene-2D results. To ensure a fair comparison, we train OpenScene-3D using the same training schedule and 3D backbone as our RegionPLC. This allows us to com-
pare performance under the same conditions and analyze the results more accurately.</p>
<p>PLA. As for PLA [9] in the annotation-free open-world setting, we only carry on the point-language contrastive learning and discard its binary encoder as there is no annotated base category in the training set.</p>
<h2>A.4. Comparisons of 3D Open-world Scene Understanding Methods</h2>
<p>As shown in Table 11, we compare our RegionPLC to other three cutting-edge 3D open-world scene understanding methods: ConceptFusion [18], OpenScene [26] and PLA [9]. ConceptFusion [18] relies on a multi-view fusion of image predictions during its inference phase. However, its inability to learn from 3D point clouds makes it difficult to extract 3D geometric information. On the other hand, OpenScene-3D [26] can learn directly from the 3D point cloud, but this approach necessitates significant additional storage for saving fused 2D features, making it unsuitable for handling large-scale datasets. Furthermore, the ceiling of its performance is limited by the 2D semantic feature and distillation strategy, making it harder to integrate with more advanced 3D backbones. PLA [9], while only requiring minimal additional storage and being scalable due to only 3D-language supervisions during training, is restricted in its performance by the sparseness and roughness of its language supervisions. In contrast, our RegionPLC inherits all the strengths of PLA [9] and incorporates more advanced 2D models for regional 3D-language association, thereby boosting its open-world capability.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>2D models</th>
<th>Multi-view inference</th>
<th>Learning in 3D</th>
<th>Scale up with better 3D backbone</th>
<th>Extra storage</th>
<th>Supervision</th>
</tr>
</thead>
<tbody>
<tr>
<td>ConceptFusion <em>[18]</em></td>
<td>Mask2Former <em>[5]</em> &amp; CLIP <em>[28]</em></td>
<td>✓</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
<td>$\times$</td>
</tr>
<tr>
<td>OpenScene-3D <em>[26]</em></td>
<td>LSeg <em>[22]</em> &amp; OpenSeg <em>[12]</em></td>
<td>$\times$</td>
<td>✓</td>
<td>$\times$</td>
<td>high</td>
<td>Pixel-aligned 2D features</td>
</tr>
<tr>
<td>PLA <em>[9]</em></td>
<td>ViT-GPT2 <em>[1]</em></td>
<td>$\times$</td>
<td>✓</td>
<td>✓</td>
<td>low</td>
<td>Sparse language supervision</td>
</tr>
<tr>
<td>RegionPLC</td>
<td>OFA <em>[35]</em> &amp; Detic <em>[48]</em> &amp; Kosmos-2 <em>[27]</em></td>
<td>$\times$</td>
<td>✓</td>
<td>✓</td>
<td>low</td>
<td>Dense language supervision</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison between different 3D open-world scene understanding methods.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Partition</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
<th>Dendro</th>
</tr>
</thead>
<tbody>
<tr>
<td>PLA <em>[9]</em></td>
<td>B15/N4</td>
<td>84.6</td>
<td>95.0</td>
<td>64.9</td>
<td>81.1</td>
<td>87.9</td>
<td>75.9</td>
<td>72.2</td>
<td>61.9</td>
<td>62.1</td>
<td>69.5</td>
<td>30.9</td>
<td>60.1</td>
<td>46.5</td>
<td>70.7</td>
<td>50.5</td>
<td>66.1</td>
<td>56.8</td>
<td>59.0</td>
</tr>
<tr>
<td></td>
<td>B12/N7</td>
<td>84.7</td>
<td>95.1</td>
<td>65.3</td>
<td>57.8</td>
<td>44.2</td>
<td>75.9</td>
<td>34.5</td>
<td>62.5</td>
<td>62.3</td>
<td>62.1</td>
<td>20.5</td>
<td>57.8</td>
<td>61.4</td>
<td>72.4</td>
<td>47.9</td>
<td>64.9</td>
<td>85.9</td>
<td>28.4</td>
</tr>
<tr>
<td></td>
<td>B10/N9</td>
<td>83.8</td>
<td>95.2</td>
<td>64.3</td>
<td>80.9</td>
<td>88.0</td>
<td>78.5</td>
<td>73.2</td>
<td>60.6</td>
<td>61.5</td>
<td>68.6</td>
<td>17.7</td>
<td>23.4</td>
<td>51.3</td>
<td>70.6</td>
<td>25.7</td>
<td>38.2</td>
<td>51.3</td>
<td>27.3</td>
</tr>
<tr>
<td>RegionPLC</td>
<td>B15/N4</td>
<td>84.2</td>
<td>95.1</td>
<td>66.6</td>
<td>81.2</td>
<td>88.2</td>
<td>81.3</td>
<td>72.6</td>
<td>61.4</td>
<td>60.7</td>
<td>75.3</td>
<td>30.4</td>
<td>57.7</td>
<td>53.4</td>
<td>70.6</td>
<td>46.1</td>
<td>64.6</td>
<td>72.6</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>B12/N7</td>
<td>84.9</td>
<td>95.1</td>
<td>65.2</td>
<td>76.3</td>
<td>79.5</td>
<td>75.8</td>
<td>64.3</td>
<td>60.0</td>
<td>64.3</td>
<td>77.9</td>
<td>31.1</td>
<td>56.7</td>
<td>65.7</td>
<td>72.7</td>
<td>49.5</td>
<td>65.6</td>
<td>83.4</td>
<td>55.5</td>
</tr>
<tr>
<td></td>
<td>B10/N9</td>
<td>84.3</td>
<td>95.2</td>
<td>65.5</td>
<td>80.6</td>
<td>89.2</td>
<td>82.7</td>
<td>73.8</td>
<td>59.6</td>
<td>62.0</td>
<td>79.7</td>
<td>25.0</td>
<td>47.7</td>
<td>56.3</td>
<td>69.8</td>
<td>38.0</td>
<td>53.2</td>
<td>74.4</td>
<td>46.6</td>
</tr>
</tbody>
</table>
<p>Table 12: Per-class results of base-annotated open-world 3D semantic segmentation on ScanNet in terms of IoU. Performance on novel categories is marked in blue.</p>
<h2>B More Experimental Results</h2>
<p>In this section, we present some supplementary experimental results, in addition to the ones provided in our main paper. This part consists of a detailed analysis of the per-class performance, an error-bar analysis and the zero-shot domain transfer experiments.</p>
<h3>B.1 Per-category Results</h3>
<p>Here, we show the per-category performance comparison between PLA <em>[9]</em> and RegionPLC for base-annotated open-world 3D semantic segmentation on ScanNet <em>[8]</em>. As shown in Table 12, our RegionPLC obtains improvements on all novel categories across different partitions, which demonstrates its effectiveness.</p>
<h3>B.2 Error Bar</h3>
<p>Here, we provide an error bar for our open-world 3D scene understanding framework on both base-annotated and annotation-free settings by reproducing each experiment 3 times. As shown in Table 13, the performance of RegionPLC is generally stable on ScanNet open-world segmentation, demonstrating its robustness.</p>
<h3>B.3 Zero-shot Domain Transfer</h3>
<p>We study the zero-shot domain generalization capability of open-world methods by transferring the ScanNet-trained model to S3DIS without fine-tuning. As shown in Table 14, RegionPLC enjoys $6.8\%\sim 37.1\%$ boosts compared to PLA <em>[9]</em> in mIoU^{†} on different splits. Notice that more base categories on ScanNet can hinder the generalization on S3DIS, indicating that dataset-specific annotation penalizes the model’s transferability. In contrast, solely learning from semantic-rich caption supervision achieves great out-of-domain generalization ability.</p>
<h2>C Qualitative Results for Annotation-free Open World</h2>
<p>Here, we provide more qualitative results of RegionPLC in the most challenging annotation-free open-world scenario. As shown in Figure 5, our RegionPLC can distinguish different semantics with remarkable segmentation results covering a wide range of categories.</p>
<p>On the other hand, we also explore the potential of our RegionPLC to discover tail and rare categories in real-world scenarios. As shown in Figure 6, we visualize the heat maps of the point-wise response given a text query. Our RegionPLC can discover a lot of tail categories such as “trash can”, “shoe” and “nightstand” without any human annotation. These results demonstrate the effectiveness of our regional point-language contrastive learning framework in solving open-world 3D scene understanding problems.</p>
<h2>D Prompts for RegionGR</h2>
<p>As highlighted in the main paper, our RegionPLC is capable of incorporating large language models (LLM), such as GPT-3.5 <em>[30]</em>, to execute grounded 3D reasoning, a pipeline we refer to as RegionGR. The LLM is given human queries and regional captions for the purpose of reasoning. Note</p>
<table>
<thead>
<tr>
<th>Round</th>
<th>Base-annotated ScanNet [8]</th>
<th></th>
<th></th>
<th>Annotation-free ScanNet [8]</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>B15/N4</td>
<td>B12/N7</td>
<td>B10/N9</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>$69.4 / 68.2 / 70.7$</td>
<td>$68.2 / 69.9 / 66.6$</td>
<td>$64.3 / 76.3 / 55.6$</td>
<td>$59.6(77.5)$</td>
</tr>
<tr>
<td>2</td>
<td>$69.5 / 68.6 / 70.4$</td>
<td>$67.6 / 69.8 / 65.4$</td>
<td>$63.9 / 76.4 / 54.9$</td>
<td>$59.2(78.0)$</td>
</tr>
<tr>
<td>3</td>
<td>$69.7 / 68.6 / 70.8$</td>
<td>$67.7 / 69.4 / 66.1$</td>
<td>$63.8 / 76.1 / 54.9$</td>
<td>$59.1(76.6)$</td>
</tr>
</tbody>
</table>
<p>Table 13. Repeated results for base-annotated and annotation-free open-world 3D semantic segmentation on ScanNet. Base-annotated results are measured in $\mathrm{hIoU} / \mathrm{mIoU}^{\mathrm{ff}} / \mathrm{mIoU}^{N}$, while annotation-free experiments are measured in $\mathrm{mIoU}^{\dagger}\left(\mathrm{mAcc}^{\dagger}\right)$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Qualitative results of annotation-free semantic segmentation on ScanNet.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ScanNet</th>
<th style="text-align: center;">S3DIS Semantic Segmentation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">partition</td>
<td style="text-align: center;">OVSeg-3D [9]</td>
<td style="text-align: center;">PLA [9]</td>
<td style="text-align: center;">RegionPLC</td>
</tr>
<tr>
<td style="text-align: center;">B15/N4</td>
<td style="text-align: center;">$31.1(46.6)$</td>
<td style="text-align: center;">$39.1(56.2)$</td>
<td style="text-align: center;">$\mathbf{5 2 . 2 ( 6 4 . 5 )}$</td>
</tr>
<tr>
<td style="text-align: center;">B12/N7</td>
<td style="text-align: center;">$23.6(42.7)$</td>
<td style="text-align: center;">$35.4(60.4)$</td>
<td style="text-align: center;">$\mathbf{4 5 . 0 ( 6 1 . 5 )}$</td>
</tr>
<tr>
<td style="text-align: center;">B10/N9</td>
<td style="text-align: center;">$36.0(50.9)$</td>
<td style="text-align: center;">$43.7(60.4)$</td>
<td style="text-align: center;">$\mathbf{5 0 . 5 ( 6 3 . 2 )}$</td>
</tr>
<tr>
<td style="text-align: center;">B0/N17</td>
<td style="text-align: center;">$01.7(11.2)$</td>
<td style="text-align: center;">$13.4(25.1)$</td>
<td style="text-align: center;">$\mathbf{5 0 . 5 ( 6 7 . 6 )}$</td>
</tr>
</tbody>
</table>
<p>Table 14. Zero-shot domain transfer results for semantic segmentation in items of $\mathrm{mIoU}^{\dagger}\left(\mathrm{mAcc}^{\dagger}\right)$ on ScanNet $\rightarrow$ S3DIS.
that if a human query pertains to a particular 3D region, we will filter captions, retaining only those that show significant overlap with the specified 3D region as the input. The prompt example we used is as follows.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Role]</span>
<span class="na">You are a household manager.</span>
<span class="na">Your job is to understand human</span>
<span class="na">instructions, and you should give</span>
<span class="na">step-by-step suggestions according</span>
</code></pre></div>

<p>to the provided environmental context.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Task]</span>
<span class="na">Your task is to give a suitable</span>
<span class="na">response to the &lt;question&gt; according</span>
<span class="na">to the &lt;env_context&gt;; if possible,</span>
<span class="na">respond in detail with clear logic.</span>
<span class="na">Both the question and env context</span>
<span class="na">are given, delimited by triple</span>
<span class="na">quotes.</span>
</code></pre></div>

<p>[Env Context]
Here is the env context, containing some words, phrases, or short sentences describing contents in a 3D room. Answer the user's request based on this.
$&lt;{$ env $}&gt;$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. Visualization in heat map of tail classes with the annotation-free model on ScanNet.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Rules]</span>
<span class="na">Return answers closely related to</span>
<span class="na">the provided information, especially</span>
<span class="na">the objects mentioned in the</span>
<span class="na">provided context. Keep the final</span>
<span class="na">answer simple and short, within 30</span>
<span class="na">words. Use natural language like</span>
<span class="na">humans do in daily life.</span>
</code></pre></div>

<p>[Steps]
According to the query, understand the intention behind "What do I want/need to do?" Find the objects related to my question from the env context. To give the final answer, you should tell me the operation I need to do and the object I need to interact with. The answer needs to be realistic, and the objects in your answer need to be based on the provided env context.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Dialog Style]</span>
<span class="na">You should respond in a polite,</span>
<span class="na">kind, and natural language tone.</span>
<span class="na">Try to talk like a human, but</span>
<span class="na">keep it short.</span>
</code></pre></div>

<p>Begin Task
The question: &lt;{question}&gt;</p>
<h2>E. Limitation and Future Works</h2>
<p>Although our RegionPLC has yielded impressive results in 3D open-world scene understanding with a broad spectrum of unseen categories, certain limitations and potential avenues for enhancement remain. On the one hand, the promising results obtained by the combination of RegionPLC and OpenScene [26] demonstrate the strong potential to introduce 2D image features as auxiliary supervision for training RegionPLC. The current loss combination</p>
<p>is straightforward, and we believe that more advanced combination strategies that integrate language, 3D and image features can lead to better performance.</p>
<p>Another aspect warranting improvement is our utilization of visual prompts, which are pre-defined prior to training and remain unchanged throughout the process. Better and more adaptive visual prompting techniques might improve the quality of language supervision. Moving forward, we are interested in further developing an open-world 3D scene understanding framework that addresses these two limitations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://kaldir.vc.in.tum.de/scannet_benchmark/ documentation&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
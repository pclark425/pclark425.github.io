<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2534 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2534</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2534</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-6ab849bba2a40e706ebacd4e666ce7e21d170c0f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6ab849bba2a40e706ebacd4e666ce7e21d170c0f" target="_blank">AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes AgentQuest – a framework where both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; and offers two new evaluation metrics that can reliably track LLM agent progress while solving a task.</p>
                <p><strong>Paper Abstract:</strong> The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest – a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GAIA: a benchmark for General AI Assistants <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra of Human Behavior <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 1)</em></li>
                <li>AgentBench: Evaluating LLMs as Agents <em>(Rating: 1)</em></li>
                <li>Gorilla: Large Language Model Connected with Massive APIs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2534",
    "paper_id": "paper-6ab849bba2a40e706ebacd4e666ce7e21d170c0f",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GAIA: a benchmark for General AI Assistants",
            "rating": 2
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 1
        },
        {
            "paper_title": "AgentBench: Evaluating LLMs as Agents",
            "rating": 1
        },
        {
            "paper_title": "Gorilla: Large Language Model Connected with Massive APIs",
            "rating": 1
        }
    ],
    "cost": 0.007,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents</h1>
<p>Luca Gioacchini ${ }^{1,2}$, Giuseppe Siracusano ${ }^{1}$, Davide Sanvito ${ }^{1}$, Kiril Gashteovski ${ }^{1,3}$, David Friede ${ }^{1}$, Roberto Bifulco ${ }^{1}$, Carolin Lawrence ${ }^{1}$<br>${ }^{1}$ NEC Laboratories Europe, Heidelberg, Germany<br>${ }^{2}$ Politecnico di Torino, Turin, Italy<br>${ }^{3}$ CAIR, Ss. Cyril and Methodius University, Skopje, North Macedonia</p>
<h4>Abstract</h4>
<p>The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest ${ }^{1}$ - a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https: //github.com/nec-research/agentquest.</p>
<h2>1 Introduction</h2>
<p>Generative Agents (Kiela et al., 2023) are software systems that leverage foundation models like Large Language Models (LLMs) to perform complex tasks, take decisions, devise multi-steps plans and use tools (API calls, coding, etc.) to build solutions in heterogeneous contexts (Wang et al., 2023; Weng, 2023). The potential ability to solve heterogeneous tasks with high degrees of autonomy has catalysed the interest of both research and industrial communities. Nonetheless, it is still unclear to which extent current systems are successfully able to fulfil their promises. In fact, methodologies to benchmark, evaluate and advance these systems are still in their early days.</p>
<p>We identify a couple of gaps. Firstly, benchmarking agents requires combining different benchmark types (Liu et al., 2023; Chalamalasetti et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of agent-benchmark interactions in existing frameworks and in AgentQuest. AgentQuest defines a common interface to interact with the benchmarks and to compute progress metrics, easing the addition of new benchmarks and allowing researchers to evaluate and debug their agent architectures.
2023). For example, some benchmarks focus on specific capabilities and provide gaming environments, which we refer to as "closed-box" - i.e. with a finite set of actions (Liu et al., 2023; Patil et al., 2023; Chalamalasetti et al., 2023) - whereas other benchmarks provide open-ended tasks and access to general tools, like web browsing (Zhuang et al., 2023; Zheng et al., 2023; Mialon et al., 2023). As benchmarks are developed independently, significant effort goes into custom integration of new agent architectures with each benchmark.</p>
<p>Secondly, and more critically, existing benchmarks mostly focus on providing a success rate measure, i.e. a binary success/fail evaluation for each of the proposed tasks. While success rate is helpful to measure overall advances of an agent technology, it has limited use in guiding improvements for new generative agent architectures. Here, it is important to consider that generative agents often combine foundation models with multiple other components, such as memory and tools. Develop-</p>
<p>ers can reason about these individual components in terms of architecture and their inter-dependence, and could actively change and evolve them using deeper insights about how an agent performs in a benchmark. That is, developers need benchmarks to both evaluate and debug agents.</p>
<p>For example, current benchmarks make it hard to answer questions like does the agent fail completely the tasks or does it partially solve them? Does the agent fail consistently at a certain step? Would extra run time lead to finding a solution? Answering these questions would require tracing and inspecting the execution of the agent. We argue that providing a more efficient approach that is consistent over multiple benchmarks is a stepping stone towards evolving generative agents.</p>
<p>We address these gaps introducing AgentQuest, a modular framework to support multiple diverse benchmarks and agent architectures (See Figure 1), alongside with two new metrics - i.e. progress rate and repetition rate - to debug an agent architecture behaviour. AgentQuest defines a standard interface to connect an arbitrary agent architecture with diverse benchmarks, and to compute progress and repetition rates from them.</p>
<p>We showcase the framework, implementing 4 benchmarks in AgentQuest: ALFWorld (Shridhar et al., 2020), Lateral Thinking Puzzles (Sloane, 1992), Mastermind and Sudoku. The latter two are newly introduced with AgentQuest. Additional benchmarks can be easily added, while requiring no changes to the tested agents.</p>
<p>Our final contribution is to present our experience leveraging the proposed metrics to debug and improve existing agent architectures as implemented in LangChain (Chase, 2022). In particular, we show that in the Mastermind benchmark the combination of progress rate and repetition rate identifies a limitation in the ability of the agent to explore the full space of potential solutions. Guided by this insight we could improve the success rate in this benchmark by up to $\approx 20 \%$. In Lateral Thinking Puzzles we show that partially repeating actions is part of the agent strategy, whereas in ALFWorld, we show that monitoring the progress rate makes it possible to identify that the final success rate is limited by the allowed runtime of the agent, and that more steps lead to a better performance. Finally, in the Sudoku benchmark, we show that the low success rate is actually paired with low progress rate, making clear that the tested agent is unable to solve this type of tasks.</p>
<h2>2 Generative AI Agents in a Nutshell</h2>
<p>Generative AI agents are automated systems relying on software components integrated with LLMs pre-trained on large amount of data for language understanding and processing. When assigned a task, an agent engages in a systematic process: it iteratively formulates self-generated instructions, executes them, and observes the outcomes until the ultimate objective is achieved. Next, we showcase the basic interaction between agents and the environment in which they operate and describe the standard benchmarking techniques.</p>
<h3>2.1 Agent-Environment interaction</h3>
<p>Closely following the terminology in Reinforcement Learning (RL) ${ }^{2}$ (Sutton and Barto, 2018), the core elements defining the agent-environment interaction are environment, state, observation and action (see Figure 1a).</p>
<p>Environment and states. The environment refers to the external system the agent interacts with. In this context, we treat the benchmark and the environment as synonyms. It is typically described through a finite set of hidden states, which are not directly observable by the agent and represent the benchmark configuration.</p>
<p>Observations and actions. The agent interacts with the environment for multiple execution steps. At each step, the environment produces an observation providing information about its current hidden state. The agent uses the internal LLM to process the received observation. Being pre-trained on general knowledge data, the LLM engages a reasoning process generating a thought on the observation (e.g. the planned strategy to follow in the current step or the usage of a tool). According to this thought, the agent provides the environment an action to modify the current hidden state. ${ }^{3}$</p>
<p>The following is an example of an agent interacting with a closed-box environment to solve a code-breaking challenge, i.e. the Mastermind game (Stuckman and Zhang, 2005). The agent is tasked to guess a four-digit code. At each step, the agent devises a strategy and acts accordingly to guess the code. The environment provides a feedback on the number of guessed digits in correct</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and wrong positions. The process continues until a stopping condition is met.</p>
<p>ENVIRONMENT: Initializing a new environment...
OBSERVATION: Start guessing the 4 digits code.
THERAPHY: I'll start by guessing 1234 to get an idea of which of these numbers are in the secret number and if any are in the correct position.
ACTION: 1234
ENVIRONMENT: Comparing the guessed code with the truth...
OBSERVATION: Your guess has 1 correct numbers in the wrong position and 0 correct numbers in the correct position. Keep guessing...
THERAPHY: One of the digits 1, 2, 3, or 4 is correct but in the wrong position. I'll try to move each digit to a different position to see if I can get a correct position. ACTION: 2143
...</p>
<h3>2.2 Benchmarking an agent</h3>
<p>When evaluating agents performance on a benchmark, the following two metrics are commonly used (Liu et al., 2023): (i) Success Rate (SR), i.e. the ratio of successful tasks to the total instances; (ii) Time to Success, i.e. the average time required to obtain a solution. While important and trending metrics (Chalamalasetti et al., 2023; Hessel et al., 2022; Zhang et al., 2020a), they exclusively address the final success. They cannot measure intermediate success or failure and therefore make it difficult to understand why agents might systematically fail and how they can be improved. In contrast, we want to define intermediate metrics that allow us to easily assess and compare the performance of agents across a wide range of tasks.</p>
<h2>3 AgentQuest Overview</h2>
<p>We designed AgentQuest as a separation layer between agent and environment (see Figure 1b). Essentially, it offers (i) a unified interface (i.e. the driver) ensuring compatibility between different agent architectures and benchmarks with minimal programming efforts (Section 3.1); (ii) the implementation of two metrics beyond task success (i.e. progress rate and repetition rate) aimed at monitoring the agent advancement toward the final goal and allowing us to understand the reasons behind failures (Section 3.2); (iii) a unique vantage point and interface for implementing new metrics to monitoring and measuring the execution (Section 3.3).</p>
<h3>3.1 Benchmarks common interface</h3>
<p>Different benchmarks require invoking distinct functions, using specific formats, and performing parsing and post-processing of observations and agent actions. To integrate different agent architectures, the common trend is hardcoding such
benchmark-specific requirements directly in the framework (Liu et al. 2023; Chalamalasetti et al. 2023, inter alia). This results in many custom interfaces tailored on each environment, making it difficult to easily move to other benchmarks and agent architectures.</p>
<p>Instead, AgentQuest exposes a single unified Python interface, i.e. the Driver and two classes reflecting the agent-environment interaction components (i.e. Observation, Action).</p>
<p>Observations and actions. We provide two simple classes: Observation and Action. The first has two required attributes: (i) output, a string reporting information about the environment state; (ii) done, a Boolean variable indicating if the final task is currently accomplished or not. The Action class has one required attribute, action_value. It is a string directly output by the agent. Once processed and provided to the environment, it triggers the environment change. To customise the interactions, developers can define optional attributes.</p>
<p>Driver. We provide the Driver class with two mandatory methods: (i) the reset method initialises a new instance of the environment and returns the first observation; (ii) the step method performs one single execution step. It accepts one instance of the Action class from the agent, processes the action (e.g. parses the action_value string) and uses it to modify the environment state. It always returns an observation. The driver supports also the benchmark-specific state attribute, acting as a simple API. It exposes the environment state at step $t$, useful to compute the progress rate.</p>
<p>We here provide an example of the implemented interaction for Mastermind:</p>
<div class="codehilite"><pre><span></span><code><span class="n">From</span> <span class="n">agentquest</span><span class="o">.</span><span class="n">drivers</span> <span class="kn">import</span><span class="w"> </span><span class="nn">MasterMindDriver</span>
<span class="n">From</span> <span class="n">agentquest</span><span class="o">.</span><span class="n">utils</span> <span class="kn">import</span><span class="w"> </span><span class="nn">Action</span>
<span class="n">From</span> <span class="n">agentquest</span><span class="o">.</span><span class="n">metrics</span> <span class="kn">import</span><span class="w"> </span><span class="nn">get_progress</span><span class="o">,</span><span class="w"> </span><span class="nn">get_repetition</span>
<span class="n">agent</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># Initialize your agent</span>
<span class="n">actions</span><span class="p">,</span> <span class="n">progress</span><span class="p">,</span> <span class="n">repetitions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="c1"># Initialize the environment and reset round</span>
<span class="n">driver</span> <span class="o">=</span> <span class="n">MasterMindDriver</span><span class="p">(</span><span class="n">truth</span><span class="o">=</span><span class="s1">&#39;5618&#39;</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Agent loop</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">obs</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>
    <span class="n">guess</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">output</span><span class="p">)</span> <span class="c1"># Get the agent output</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">Action</span><span class="p">(</span><span class="n">action_value</span><span class="o">=</span><span class="n">guess</span><span class="p">)</span> <span class="c1"># Create action</span>
    <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">action_value</span><span class="p">)</span> <span class="c1"># Store action</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># Execute step</span>
    <span class="c1"># Compute current progress and repetition</span>
    <span class="n">progress</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_progress</span><span class="p">(</span><span class="n">driver</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="s1">&#39;5618&#39;</span><span class="p">))</span>
    <span class="n">repetitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_repetitions</span><span class="p">(</span><span class="n">actions</span><span class="p">))</span>
    <span class="c1"># Extend with your custom metrics here ...</span>
<span class="c1"># Compute final metrics</span>
<span class="n">PR</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">/</span><span class="nb">len</span><span class="p">[</span><span class="s1">&#39;5618&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">progress</span><span class="p">]</span>
<span class="n">RR</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">repetitions</span><span class="p">]</span>
</code></pre></div>

<h3>3.2 Understanding agent advancements</h3>
<p>Getting insights on how they tackle a specific task is key to comprehend agent behaviours, capabilities and limitations. Furthermore, identifying systematic agent failures allows to pinpoint necessary adjustments within the architecture to effectively address the underlying issues.</p>
<p>AgentQuest contributes towards this direction introducing two cross-benchmark metrics, the progress rate and the repetition rate. While the first expresses how much the agent is advancing towards the final goal, the latter indicates how it is reaching it, with a specific focus on the amount of repeated (i.e. similar) actions the agent performs.</p>
<p>Milestones and progress rate. To quantify the agent advancement towards the final goal, AgentQuest uses a set of milestones $\mathcal{M}$. In a nutshell, we break down the final solution into a series of environment hidden states the agent needs to reach to get the final solution of the task, hence, $\mathcal{M} \subseteq \mathcal{S}$, where $\mathcal{S}$ is the set of hidden states. The magnitude of $\mathcal{M}$ determines the level of granularity in the evaluation process. Specifically, when $\mathcal{M}$ aligns closely with $\mathcal{S}$, it offers a more comprehensive insight into the agent progress, resulting in finer granularity, whereas for $|\mathcal{M}|=1$ the evaluation coincides with the success rate.</p>
<p>We assign a score to all the states included in $\mathcal{M}$ through a scoring function $f$ and, at execution step $t$, we define the progress rate $\mathrm{PR}_{t}: \mathcal{S} \rightarrow[0,1]$ dependant of such scoring function, as an indication of how far the agent is from the goal, allowing to track agent progress over time. Depending on the benchmark, the progress rate might also decrease during the execution. Milestones can either be manually annotated, or internally computed.</p>
<p>Repetition rate. The repetition rate $\mathrm{RR}_{t}$ is a measure of the agent tendency of repeating actions. Depending on the benchmark, we do not consider repetitions as a limitation, - e.g. solving a maze requires repetitions, such as going left repeatedly. See also Section 4 for a positive and negative example of repetitions.</p>
<p>At execution step $t$, we consider the set of unique actions taken by the agent up to $t-1, \mathcal{A}<em t="t">{t-1}$. Then, we compute the similarity function $g$ between the current action $a</em>}$ and all the previous ones in $\mathcal{A<em t="t">{t-1}$. As any action generated by the LLM is considered valid, we consider the action $a</em>$ such that}$ as repeated if it exists at least one previous action $a \in \mathcal{A}_{t-1</p>
<p>Table 1: Attributes exposing components of the agentenvironment interaction useful to define new metrics.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Class</th>
<th style="text-align: left;">Attribute</th>
<th style="text-align: left;">Access to</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Driver</td>
<td style="text-align: left;">state</td>
<td style="text-align: left;">Hidden states</td>
</tr>
<tr>
<td style="text-align: left;">Observation</td>
<td style="text-align: left;">output</td>
<td style="text-align: left;">Observations</td>
</tr>
<tr>
<td style="text-align: left;">Action</td>
<td style="text-align: left;">action_value</td>
<td style="text-align: left;">Agent actions</td>
</tr>
</tbody>
</table>
<p>$g\left(a_{t}, a\right) \geq \theta_{a}$, where $\theta_{a} \in[0,1]$ is the resolution. ${ }^{4}$ If the action is not repeated, we update the set of unique actions as $A_{t}=A_{t-1} \cup a_{t}$.</p>
<p>Based on this, we define the repetition rate at step $t$ as the cumulative number of repeated actions normalised by the number of execution steps, $T$, except for the first. Formally, $\mathrm{RR}<em t="t">{t}=\frac{t-|A</em>$.}|}{T-1</p>
<h3>3.3 Adding new metrics</h3>
<p>We rely on the progress and repetition rates to show how AgentQuest can be extended with new metrics through a simple function template. We then show the implementations of the functions adapted to the considered benchmark.</p>
<p>Metric function template. We use a Python function template to easily define the elements of the agent-environment interactions required for computing a given metric. Table 1 provides a recap of the main attributes and reference classes that can be used as input for the custom metrics. Additionally, users can provide external data, like milestones or action history.</p>
<p>Implement progress rate. Depending on the benchmark, developers need to implement the custom scoring function $f$ through the get_progress function and define the set of milestones $\mathcal{M}$. Milestones can either be user-defined or internally computed within get_progress. Here, we show the definition of get_progress to quantify the achieved milestones for Mastermind. The milestones are the digits of the final solution and the progress indicates the count of correctly guessed digits in their positions:</p>
<div class="codehilite"><pre><span></span><code>def get_progress(state, milestones):
    reached_milestones = 0 # Digits in correct position
    for i, j in zip(state, milestones):
        if i == j: reached_milestones += 1
    return reached_milestones
<span class="gh">#</span> Usage example. The code to guess is &#39;5618&#39;
progress = get_progress(&#39;2218&#39;, &#39;5618&#39;) # Reached milestones
&gt;&gt;&gt; 2
progress/len(&#39;5618&#39;) # Compute Progress Rate
&gt;&gt;&gt; 0.5
</code></pre></div>

<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Overview of the benchmarks provided in AgentQuest.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Milestones</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mastermind</td>
<td style="text-align: left;">Guessing a numeric code with feedback on guessed digits and positions.</td>
<td style="text-align: left;">Digits of the code to guess.</td>
</tr>
<tr>
<td style="text-align: left;">LTP</td>
<td style="text-align: left;">Solving riddles by asking Yes/No questions.</td>
<td style="text-align: left;">Guessed riddle key aspects.</td>
</tr>
<tr>
<td style="text-align: left;">ALFWorld</td>
<td style="text-align: left;">Finding an object in a textual world and using it.</td>
<td style="text-align: left;">Sequence of actions.</td>
</tr>
<tr>
<td style="text-align: left;">Sudoku</td>
<td style="text-align: left;">9x9 grid puzzle. Digits 1-9 fill each column, row, and 3x3 sub-grid <br> without repetition.</td>
<td style="text-align: left;">Total number of correct <br> inserted digits.</td>
</tr>
</tbody>
</table>
<p>Implement repetition rate. To determine if an action is repeated, the end user must define the similarity function $g$ according to the considered benchmark. We provide the get_repetitions template function to compute the number of repeated actions. Here, we illustrate its implementation in Python and provide a usage example for Mastermind, where $g$ is the Levenshtein similarity (Levenshtein, 1966).</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">Levenshtein</span><span class="w"> </span><span class="kn">import</span> <span class="n">ratio</span> <span class="k">as</span> <span class="n">g</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_repetitions</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">THETA_A</span><span class="p">):</span>
    <span class="n">unique_act</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="c1"># Initialise unique actions</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">actions</span><span class="p">):</span>
        <span class="c1"># Check for repetitions</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">([</span><span class="n">g</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">actions</span><span class="p">(</span><span class="n">x</span><span class="p">])</span><span class="o">&lt;</span><span class="n">THETA_A</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">)]):</span>
                <span class="n">unique_act</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">unique_act</span><span class="p">)</span>
<span class="c1"># Usage example. The code to guess is &#39;58!8&#39;</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;1234&#39;</span><span class="p">,</span> <span class="s1">&#39;2143&#39;</span><span class="p">,</span> <span class="s1">&#39;1234&#39;</span><span class="p">,</span> <span class="s1">&#39;58!8&#39;</span><span class="p">]</span> <span class="c1"># Actions history</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="n">get_repetitions</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">1</span> <span class="n">repeated</span> <span class="n">action</span>
<span class="c1"># Compute Repetition Rate</span>
<span class="n">repetitions</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.33</span>
</code></pre></div>

<p>In other cases, where $a$ can be any text string, we can use standard metrics, such as BLEU (Papineni et al., 2002), ROGUE (Lin, 2004) or BERTScore (Zhang et al., 2020b).</p>
<h2>4 Insights via AgentQuest</h2>
<p>We investigate agent behaviours in different reasoning scenarios by proposing a starting set of four benchmarks. We implemented from scratch Sudoku (Felgenhauer and Jarvis, 2006) and Mastermind (Stuckman and Zhang, 2005) environments, while ALFWorld (Shridhar et al., 2020) and Lateral Thinking Puzzles (LTP)(Sloane, 1992) are existing implementations (Liu et al., 2023). Table 2 provides an overview of the benchmarks and their respective milestones used to measure progress.</p>
<p>We emphasise that this evaluation is not aimed at providing a thorough evaluation and comparison of agent architectures, but rather to show how to use AgentQuest and how monitoring progress and action repetition can provide relevant insights to developers, even after a few executions.</p>
<p>Table 3: Average existing and proposed metrics for the tested benchmarks. We report the metrics, Success Rate (SR), Steps, Progress Rate at step $60\left(\mathrm{PR}<em 60="60">{60}\right)$ and Repetition Rate at final step $60\left(\mathrm{RR}</em>\right)$. We denote with * the improved results after modifying the agent architecture.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Existing Metrics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AgentQuest</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;">$\mathbf{P R}_{60}$</td>
<td style="text-align: center;">$\mathbf{R R}_{60}$</td>
</tr>
<tr>
<td style="text-align: left;">Mastermind</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">41.87</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: left;">LTP</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">ALFWorld</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: left;">Sudoku</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">59.67</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: left;">Mastermind*</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">39.73</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">ALFWorld*</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">25.86</td>
<td style="text-align: center;">$0.80^{\dagger}$</td>
<td style="text-align: center;">$0.07^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>${ }^{\dagger}$ Metrics referred to the extended runtime up to 120 steps, hence $\mathrm{PR}<em 120="120">{120}$ and $\mathrm{RR}</em>$.</p>
<p>Experimental setup. We use as reference architecture the off-the-shelf chat agent provided by LangChain (Chase, 2022) powered by GPT-4 (OpenAI, 2023b) as LLM because it is intuitive, easy to extend and open source. We run 15 instances of the four benchmarks within AgentQuest, setting the maximum number of execution steps as $60^{5}$. In Appendix B we provide examples on how to use AgentQuest with two additional agent architectures and GAIA (Mialon et al., 2023) as open-ended environment.</p>
<p>Experimental results. For Mastermind, Figure 2a shows the progress rate $\mathrm{PR}<em _mathrm_f="\mathrm{f">{\mathrm{f}}$ and repetition rate $\mathrm{RR}</em>}}$. In the first 22 steps, the agent explores different solutions $\left(\mathrm{RR<em 22="22">{[0,22]}&lt;5 \%\right)$. This leads to growing progress towards the final goal, reaching half of the milestones $\left(\mathrm{PR}</em> \approx 55 \%\right)$. Then, the agent starts performing the same actions, exhibiting a repetitive pattern (see also Figure 3a rightmost part) and failing to reach the final goal within the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average Progress rate $\mathrm{PR}<em t="t">{t}$ and the repetition rate $\mathrm{RR}</em>}$ on Mastermind and LTP. Mastermind: It starts out with a low $\mathrm{RR<em t="t">{t}$ but this increases after step 22 while the progress rate also stall at $55 \%$. LTP: at first a higher $\mathrm{RR}</em>$ allows the agent to progress by making small variations that lead to success, but later this plateaus.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples of repeated actions in Mastermind and LTP. Mastermind: there is a set of unique actions at first, but then gets stuck repeating the same actions over and over. LTP: repeated actions are small variations of the same question that lead to progress.
next 38 steps. This results in a rise of the repetitions to $\mathrm{RR}<em 60="60">{60}=30 \%$ and a saturation of the progress rate at $\mathrm{PR}</em>=55 \%$. Hence, AgentQuest offered us a crucial insights on why the current agent cannot solve the Mastermind game.</p>
<p>To overcome this agent limitation we incorporate a memory component (Park et al., 2023) into the agent architecture. The agent stores the past guesses in a local buffer. Then, at each step, if the agent outputs an action already in the buffer, it is prompted to provide a new one. Table 3 (Mastermind*) shows that this simple change in agent architecture has a big impact: the agent can now solve more instances, increasing the final SR from $47 \%$ to $60 \%$ and preventing repetitions ( $\mathrm{RR}_{60}=0 \%$ ). This highlights how studying the interplay between progress and repetition rates can allow us to improve agent architecture, sometimes even with simple remedies. We support our intuition extending the evaluation to more instances of Mastermind from 15 to 60 achieving comparable results - i.e. $43 \%$ of SR with the standard architecture and $62 \%$ with the simple memory ( $19 \%$ of improvement).</p>
<p>For LTP, the AgentQuest metrics reveal a dif-
ferent agent behaviour, where repetitions are part of the agent reasoning strategy, enhancing the progress rate (Figure 2b). From the initial steps, the agent changes aspects of the same questions until a local solution emerges. This leads to horizontal indicators in Figure 3b and $\mathrm{RR}<em 60="60">{20} \approx 30 \%$. Despite solving only a few riddles ( $\mathrm{SR}=0.2$ ), these repetitions contribute to progress, achieving $46 \%$ of the milestones by the end of the execution, with a final repetition rate of $\mathrm{RR}</em>=81 \%$. This shows us how the interplay of progress and repetition rates provides an insight on how agents behave across the different time steps.</p>
<p>Consider the benchmark ALFWorld in Table 3 (we report the metrics trend in Appendix A). It requires the exploration of a textual world to locate an object. While the agent explores the solution space and limits action repetitions ( $\mathrm{RR}<em 60="60">{60}=6 \%$ ), it fails to solve all the games $\left(\mathrm{PR}</em>=74 \%\right)$. This discrepancy may arise from the more exploration steps required to discover the object. We support this intuition extending the benchmark runtime to 120 steps resulting in a success and progress rates increase by $6 \%$ (ALFWorld* in Table 3). This confirms the usefulness of AgentQuest in understanding the agent failures. We support our intuition also extending the evaluation to more instances of ALFWorld from 15 to 60 achieving comparable results - i.e. $83 \%$ of SR with 60 steps as limit and $87 \%$ with 120 steps as limit ( $4 \%$ of improvement).</p>
<p>Finally, we look at Sudoku, known for its high level of difficulty (Felgenhauer and Jarvis, 2006). The low progress and repetition rates achieved after 60 steps $\left(\mathrm{PR}<em 60="60">{60}=8 \%\right.$ and $\left.\mathrm{RR}</em>=22 \%\right)$ indicate that the current agent architecture struggles in finding correct solutions solving this task. We report the metrics trend in Appendix A.</p>
<h2>5 Conclusions</h2>
<p>AgentQuest allows the research community to keep track of agent progress in a holistic manner. Starting out with a first set of four benchmarks and two new metrics, AgentQuest is easily extendable. Furthermore, the two proposed metrics, progress and repetition rates, have the great advantage of allowing to track how agents advance toward the final goal over time. Especially studying their interplay can lead to important insights that will allow the research community to improve agent performance. Finally, we believe that promptly sharing AgentQuest with the research community will fa-</p>
<p>cilitate benchmarking and debugging agents, and will foster the creation and use of new benchmarks and metrics.</p>
<h2>Ethical Considerations</h2>
<p>The complexity of LLM agents poses challenges in comprehending their decision-making processes. Ethical guidelines must demand transparency in such systems, ensuring that developers and endusers comprehend how decisions are reached.</p>
<p>We are not aware of any direct ethical impact generated by our work. However, we hope that insights into Generative AI agents' decision-making processes will be applied to improve and promote transparency and fairness.</p>
<h2>Acknowledgements</h2>
<p>This project has received funding from the European Union's Horizon Europe research and innovation programme (SNS-JU) under the Grant Agreement No 101139285 ("NATWORK").</p>
<h2>References</h2>
<p>Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David Schlangen. 2023. Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents.</p>
<p>Harrison Chase. 2022. LangChain - Building applications with LLMs through composability.</p>
<p>Bertram Felgenhauer and Frazer Jarvis. 2006. Mathematics of Sudoku I. Mathematical Spectrum.</p>
<p>Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2022. CLIPScore: A Reference-free Evaluation Metric for Image Captioning.</p>
<p>Douwe Kiela, Tristan Thrush, Kawin Ethayarajh, and Amanpreet Singh. 2023. Plotting Progress in AI. Contextual AI Blog.</p>
<p>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet Physics Doklady.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. AgentBench: Evaluating LLMs as Agents.</p>
<p>Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. GAIA: a benchmark for General AI Assistants.</p>
<p>OpenAI. 2023a. Assistants API.
OpenAI. 2023b. GPT-4 Technical Report.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation.</p>
<p>Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior.</p>
<p>Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large Language Model Connected with Massive APIs.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.</p>
<p>Paul Sloane. 1992. Lateral Thinking Puzzlers. Sterling Publishing Company, Inc.</p>
<p>Jeff Stuckman and Guo-Qiang Zhang. 2005. Mastermind is NP-complete.</p>
<p>Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. A Survey on Large Language Model based Autonomous Agents.</p>
<p>Lilian Weng. 2023. LLM-powered Autonomous Agents.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. ReAct: Synergizing Reasoning and Acting in Language Models.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020a. BERTScore: Evaluating Text Generation with BERT.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. BERTScore: Evaluating Text Generation with BERT.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.</p>
<p>Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. ToolQA: A Dataset for LLM Question Answering with External Tools.</p>
<h2>A Appendix: ALFWorld and Sudoku benchmarks</h2>
<p>In this section we report the detailed metrics for each step for the ALFWorld and Sudoku benchmarks, omitted for the sake of brevity from the main paper.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Progress rate $\mathrm{PR}<em t="t">{t}$ and the repetition rate $\mathrm{RR}</em>$ on ALFWorld and Sudoku averaged over 15 runs. ALFWorld: It starts out with a low repetition rate and quick increase of the progress rate. Then a slow increase of the repetition rate enables to further increase the progress rate although less quickly. Sudoku: The progress rate quickly reaches $8 \%$. The repetition rate then slowly increases without any positive change in the progress rate.</p>
<p>Figure 4a reports the progress rate and repetition rate for ALFWorld. The repetition rate is close to $0 \%$ for the first 20 steps, then it slowly increases up to $6 \%$ after 60 steps. The progress rate quickly reaches over $50 \%$ in 10 steps, then keeps increasing, although slowly, up to $74 \%$. The consistent improvement of the progress rate even for steps close to 60 together with the low repetition rate suggests that higher values may be reached by increasing the maximum number of steps. We validate this hypothesis by extending the benchmark runtime to 120 steps. As previously reported in Table 3, this results in an improvement of 6 percentage points for both the success rate the progress rate, i.e. $\mathrm{SR}=93 \%$ and $\mathrm{PR}_{120}=80 \%$.</p>
<p>Figure 4 b includes the two metrics for the Sudoku benchmark. We can observe that the progress rate quickly reaches a plateau at $8 \%$ in very few steps. The repetition rate is close to $0 \%$ for the first 10 steps, then it slowly increases up to $22 \%$ after 60 steps without any improvement of the progress rate.</p>
<h2>B Appendix: Additional agents architectures and benchmarks</h2>
<p>In this section we highlight the plug-and-play aspect of AgentQuest showing the implementation of Mastermind with two additional agents archi-
tectures, i.e. ReAct (Yao et al., 2022) as the most used architecture in literature and OpenAI Assistant (OpenAI, 2023a), as the most recent proprietary architecture. Additionally, we show how to implement the open-ended benchmark GAIA (Mialon et al., 2023) requiring the usage of external tools. For brevity, in the following snippets we omit details, like error handling or full agent definition. The complete code is available in the GitHub repository.</p>
<h2>B. 1 ReAct for Closed-box Environments</h2>
<p>We show an example of how to execute a closedbox benchmark (i.e. ALFWorld) with an agent based on the ReAct architecture (Yao et al., 2022). Such architecture forces the agent decision making process to generate both textual reasoning traces and actions pertaining to a task in an interleaved manner. Common implementations (Chase, 2022; Yao et al., 2022) rely on external tools to perform actions. Here, we ensure compatibility with existing implementations providing a single tool (i.e. ProxyTool) that forwards the actions to the driver. In a nutshell, the agent reflects on the action to take and invokes the tool. Then, we feed the tool input to the driver to perform the interaction with the environment. At each step, we provide the agent the updated history of the actions and observations through the intermediate_steps variable.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.drivers</span><span class="w"> </span><span class="kn">import</span> <span class="n">MasterMindDriver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="o">...</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Action</span>
<span class="c1"># Define a dummy tool for closed-box environments</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ProxyTool</span><span class="p">(</span><span class="n">BaseTool</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;proxytool&quot;</span>
    <span class="n">description</span> <span class="o">=</span> <span class="s2">&quot;Provide the action you want to perform&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
<span class="c1"># Instantiate custom prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">CustomPromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=...</span><span class="p">,</span> <span class="c1"># LLM prompt</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">ProxyTool</span><span class="p">()],</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;intermediate_steps&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="p">)</span>
<span class="c1"># Initialise the agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">create_react_agent</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="p">[</span><span class="n">ProxyTool</span><span class="p">()],</span> <span class="n">prompt</span><span class="p">)</span>
<span class="n">intermediate_steps</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Initialise the driver</span>
<span class="n">driver</span> <span class="o">=</span> <span class="n">MasterMindDriver</span><span class="p">(</span><span class="n">game</span><span class="p">)</span>
<span class="c1"># Get the first observation</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Agent Loop</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">obs</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>
    <span class="c1"># Retrieve the agent output</span>
    <span class="n">agent_choice</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
        <span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">:</span><span class="n">obs</span><span class="o">.</span><span class="n">output</span><span class="p">,</span>
            <span class="s1">&#39;intermediate_steps&#39;</span><span class="p">:</span><span class="n">intermediate_steps</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">Action</span><span class="p">(</span><span class="n">action_value</span><span class="o">=</span><span class="n">agent_choice</span><span class="o">.</span><span class="n">tool_input</span><span class="p">)</span>
    <span class="c1"># Perform the step</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="c1"># Update intermediate steps</span>
    <span class="n">intermediate_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="p">(</span><span class="n">agent_choice</span><span class="p">,</span> <span class="n">obs</span><span class="o">.</span><span class="n">output</span><span class="p">))</span>
    <span class="c1"># Get current metrics ...</span>
</code></pre></div>

<h2>B. 2 OpenAI Assistant for Closed-box Environments</h2>
<p>The OpenAI Assistant (OpenAI, 2023a) is a proprietary architecture. It allows users to define custom agents by specifying the tasks to accomplish and the set of tools the agent can use. While the decision-making process is not directly accessible by the end-users (the agent and the LLM are hosted on the proprietary cloud environment), the tools can be invoked both remotely or locally. In the latter, users have control on the tool invocation managing the agent loop.</p>
<p>Similarly to ReAct, we here rely on the ProxyTool, acting as a proxy between the agent and the environment. We invoke the remote agent with the initial task (e.g. first ALFWorld observation) and process the output of its decision making process, i.e. the action to perform provided as tool input. Then, we bypass the tool invocation, directly forwarding the action to the driver to perform the execution step and retrieve the next observation. Finally, we invoke the agent with the new observation concluding the execution step.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.drivers</span><span class="w"> </span><span class="kn">import</span> <span class="n">MasterMindDriver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.metrics</span><span class="w"> </span><span class="kn">import</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Action</span>
<span class="o">...</span>
<span class="c1"># Define a dummy tool for closed-box environments</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ProxyTool</span><span class="p">(</span><span class="n">BaseTool</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;proxytool&quot;</span>
    <span class="n">description</span> <span class="o">=</span> <span class="s2">&quot;Provide the action you want to perform&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
<span class="c1"># Initialise the agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">OpenAIAssistantRunnable</span><span class="o">.</span><span class="n">create_assistant</span><span class="p">(</span>
    <span class="n">instructions</span><span class="o">=...</span> <span class="c1"># LLM prompt</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">ProxyTool</span><span class="p">()],</span>
    <span class="n">model</span><span class="o">=...</span> <span class="c1"># Chosen LLM</span>
    <span class="n">as_agent</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="c1"># Initialise the driver</span>
<span class="n">driver</span> <span class="o">=</span> <span class="n">MasterMindDriver</span><span class="p">(</span><span class="n">game</span><span class="p">)</span>
<span class="c1"># Get the first observation</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Get the first action</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">obs</span><span class="o">.</span><span class="n">output</span><span class="p">})</span>
<span class="c1"># Agent Loop</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">obs</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>
    <span class="c1"># Retrieve the agent output</span>
    <span class="n">agent_guess</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="c1">#].tool_input</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">Action</span><span class="p">(</span><span class="n">action_value</span><span class="o">=</span><span class="n">agent_guess</span><span class="p">)</span>
    <span class="c1"># Perform the step</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="c1"># Get current metrics ...</span>
    <span class="c1"># Manage Proxy Tool output</span>
    <span class="n">tool_outputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">obs</span><span class="o">.</span><span class="n">output</span><span class="p">,</span>
            <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="c1">#].tool_call_id}</span>
    <span class="p">]</span>
    <span class="c1"># Invoke the agent to get the next action</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;tool_outputs&quot;</span><span class="p">:</span> <span class="n">tool_outputs</span><span class="p">,</span>
            <span class="s2">&quot;run_id&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="c1">#].run_id,</span>
            <span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="c1">#].thread_id}</span>
    <span class="p">)</span>
</code></pre></div>

<h2>B. 3 OpenAI Assistant for Open-ended Environments</h2>
<p>When interacting with an open-ended environment, the agent is not restricted to the pre-defined actions of the closed-box environment and it is allowed to select any user-defined tool (e.g. retrieving information online or executing code). Hence, we provide the agent the list of tools via the tool variable. The agent relies on its reasoning process to choose which tool to invoke. Omitted here for the sake of brevity, we rely of the manual annotations of the GAIA questions (Mialon et al., 2023) as milestones to compute the progress rate.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.drivers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaiaDriver</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="o">...</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">agentquest.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">Action</span>
<span class="o">...</span>
<span class="c1"># Define the tools</span>
<span class="n">tools</span><span class="o">=</span><span class="p">{</span>
    <span class="n">OnlineSearch</span><span class="p">(),</span> <span class="c1"># Retrieve a web page link</span>
    <span class="n">WebContentParser</span><span class="p">(),</span> <span class="c1"># Read the web page</span>
    <span class="n">FinalAnswerRetriever</span><span class="p">(),</span> <span class="c1"># Provide the final answer</span>
    <span class="o">...</span>
<span class="p">]</span>
<span class="c1"># Initialise the agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">OpenAIAssistantRunnable</span><span class="o">.</span><span class="n">create_assistant</span><span class="p">(</span>
    <span class="n">instructions</span><span class="o">=...</span> <span class="c1"># LLM prompt</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=...</span> <span class="c1"># Chosen LLM</span>
    <span class="n">as_agent</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="c1"># Initialise the driver</span>
<span class="n">driver</span> <span class="o">=</span> <span class="n">GaiaDriver</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">tools</span><span class="p">)</span>
<span class="c1"># Get the first observation</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Get the first action</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">obs</span><span class="o">.</span><span class="n">output</span><span class="p">})</span>
<span class="c1"># Agent Loop</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">obs</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>
    <span class="c1"># Retrieve the agent output</span>
    <span class="n">act</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;(response[#].tool):(response[#].tool_input)&#39;</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">Action</span><span class="p">(</span><span class="n">action_value</span><span class="o">=</span><span class="n">act</span><span class="p">)</span>
    <span class="c1"># Perform the step invoking the local tool</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="c1"># Get current metrics ...</span>
    <span class="c1"># Manage tool output as observation</span>
    <span class="n">tool_outputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">obs</span><span class="o">.</span><span class="n">output</span><span class="p">,</span>
            <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="c1">#].tool_call_id}</span>
    <span class="p">]</span>
    <span class="c1"># Invoke the agent to get the next action</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;tool_outputs&quot;</span><span class="p">:</span> <span class="n">tool_outputs</span><span class="p">,</span>
            <span class="s2">&quot;run_id&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="c1">#].run_id,</span>
            <span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="c1">#].thread_id}</span>
    <span class="p">)</span>
</code></pre></div>

<p>Here, the driver acts as a wrapper, executing the tool with the parameters provided by the agent (tool_input) and forwards the output to the agent in the correct format:</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">GaiaDriver</span><span class="p">():</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">tools</span><span class="p">,</span><span class="w"> </span><span class="o">...</span><span class="p">):</span>
<span class="w">        </span><span class="c1"># Initialise the tool lookup</span>
<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">tool_lookup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">:</span><span class="n">x</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">tools</span><span class="p">}</span>
<span class="w">    </span><span class="o">...</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="c1"># Parse the action</span>
<span class="w">        </span><span class="k">tool</span><span class="p">,</span><span class="w"> </span><span class="n">tool_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">action</span><span class="o">.</span><span class="n">action_value</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="w">        </span><span class="c1"># Invoke the tool</span>
<span class="w">        </span><span class="n">tool_out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">tool_lookup</span><span class="p">(</span><span class="k">tool</span><span class="p">]</span><span class="o">.</span><span class="n">_run</span><span class="p">(</span><span class="n">tool_input</span><span class="p">)</span>
<span class="w">        </span><span class="c1"># Parse the tool output here</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">Observation</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">tool_out</span><span class="p">)</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\text {a }}$ We limit the number of instances in our experiments for two main reasons: (i) the work primarily serves as a demonstration of the developed framework itself, rather than an extensive evaluation of the agent performance; (ii) extensive tests could have significantly impacted the ability to reproduce the experiments due to the expensive nature of API calls.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4237 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4237</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4237</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-279155102</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.03587v1.pdf" target="_blank">Preface to the Special Issue of the TAL Journal on Scholarly Document Processing</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scholarly literature makes it increasingly difficult for researchers to keep up with new knowledge. Automated tools are now more essential than ever to help navigate and interpret this vast body of information. Scientific papers pose unique difficulties, with their complex language, specialized terminology, and diverse formats, requiring advanced methods to extract reliable and actionable insights. Large language models (LLMs) offer new opportunities, enabling tasks such as literature reviews, writing assistance, and interactive exploration of research. This special issue of the TAL journal highlights research addressing these challenges and, more broadly, research on natural language processing and information retrieval for scholarly and scientific documents.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4237.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4237.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generic mention of large pre-trained transformer language models used to analyze and synthesize information across multiple scientific papers for tasks such as literature review, writing assistance, and interactive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>broad / cross-domain (scholarly literature generally)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>general synthesis of findings, generation of novel research directions and summaries (implicit mention of extracting high-level patterns and directions rather than explicit formal laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The preface reports that LLMs enable new applications on scientific literature including literature reviews, generating novel research directions, accelerating discovery, assisting writing, and interactive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>High-level concerns are noted in the preface: computational/environmental costs of LLMs and ethical issues including bias, fairness and transparency; no specific empirical limitations about law-extraction are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4237.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4237.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArxivDIGESTables</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system (paper) that uses language models to synthesize scientific literature into structured tables, i.e., condensing multiple papers' content into tabular summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Not described in the preface; the title implies an approach that uses LLMs to parse multiple papers and produce tabular syntheses, but no prompt engineering, retrieval or multi-step pipeline details are given in this preface.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>arXiv / multi-domain scholarly literature</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>tabular synthesis of empirical findings and structured summaries (enables extraction of recurring patterns and empirical generalizations across papers)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of applying language models to synthesize literature into more structured, machine-readable summaries (tables), which supports distillation of collective findings across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No implementation or evaluation details are provided in the preface; general concerns about reliability and hallucination of LLM outputs apply but are not reported specifically for this work here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4237.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4237.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON: Scientific Inspiration Machines Optimized for Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that aims to generate novel scientific inspirations or directions by processing scientific literature, using methods presented in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciMON: Scientific Inspiration Machines Optimized for Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Not provided in the preface. The referenced title suggests use of models to mine literature for novel or inspiring research directions, but no prompt/retrieval/iterative procedure is described here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>computational linguistics / general scientific literature (as per ACL proceedings context)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>generation of novel research directions and identification of high-level conceptual patterns across papers (inspirational/idea-generation outputs rather than formal laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited in the preface as an example of work that uses LLM-style systems to produce novel research directions from collections of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No specifics given in the preface; general limitations of LLM-driven discovery (e.g., reliability, validation of generated directions) are implied but not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4237.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4237.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HoneyComb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HoneyComb: A Flexible LLM-Based Agent System for Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that uses LLM-based agents to assist tasks in materials science, suggesting application of LLMs to domain-specific scientific literature and workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HoneyComb: A Flexible LLM-Based Agent System for Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Not described in the preface. The paper title indicates an agent-based LLM system tailored to materials science workflows, likely processing domain literature and data, but no procedural details are available here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>domain-specific design principles, hypotheses or patterns relevant to materials discovery (implied by materials-science focus), though the preface does not specify exact output types</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of domain-specific LLM agents that can assist experts, implying extraction or synthesis of domain patterns or candidate hypotheses from the literature/data.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No evaluation details provided in the preface; general concerns about domain adaptation, factuality and validation of model outputs are relevant but not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4237.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4237.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenResearcher: Unleashing AI for Accelerated Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system/demonstration that uses AI to accelerate scientific research workflows, likely by processing scholarly documents to help generate research directions and assist exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenResearcher: Unleashing AI for Accelerated Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Not described in the preface. The referenced demo title suggests an interactive system using LLMs to process and explore papers, but no implementation details are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>general scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>generation of research directions and curated summaries; potential extraction of cross-paper patterns and hypotheses (implied)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Listed among recent works showcasing how LLM-based tools enable interactive exploration and accelerate research by synthesizing across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No concrete limitations reported in the preface for this system; broader issues with LLM deployment (cost, ethics, factuality) are noted elsewhere in the preface.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4237.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4237.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Survey (Zhang et al., 2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced comprehensive survey paper that reviews scientific LLMs and their applications to scientific discovery, which likely covers instances where models extract high-level principles or generate hypotheses from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>meta-study across scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>survey-level discussion of applications including hypothesis generation, discovery and synthesis (likely includes discussion of pattern/principle extraction methods, but not detailed in the preface)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that LLMs are being developed and applied for accelerating discovery and generating novel research directions across scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey likely discusses limitations (not reproduced in the preface), and the preface itself highlights broader concerns such as cost and ethical issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models <em>(Rating: 2)</em></li>
                <li>SciMON: Scientific Inspiration Machines Optimized for Novelty <em>(Rating: 2)</em></li>
                <li>HoneyComb: A Flexible LLM-Based Agent System for Materials Science <em>(Rating: 2)</em></li>
                <li>OpenResearcher: Unleashing AI for Accelerated Scientific Research <em>(Rating: 2)</em></li>
                <li>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery <em>(Rating: 2)</em></li>
                <li>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4237",
    "paper_id": "paper-279155102",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "LLMs (general mention)",
            "name_full": "Large Language Models",
            "brief_description": "Generic mention of large pre-trained transformer language models used to analyze and synthesize information across multiple scientific papers for tasks such as literature review, writing assistance, and interactive exploration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": null,
            "number_of_papers": null,
            "domain_or_field": "broad / cross-domain (scholarly literature generally)",
            "type_of_laws_extracted": "general synthesis of findings, generation of novel research directions and summaries (implicit mention of extracting high-level patterns and directions rather than explicit formal laws)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "The preface reports that LLMs enable new applications on scientific literature including literature reviews, generating novel research directions, accelerating discovery, assisting writing, and interactive exploration.",
            "challenges_limitations": "High-level concerns are noted in the preface: computational/environmental costs of LLMs and ethical issues including bias, fairness and transparency; no specific empirical limitations about law-extraction are provided.",
            "uuid": "e4237.0",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ArxivDIGESTables",
            "name_full": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "brief_description": "A referenced system (paper) that uses language models to synthesize scientific literature into structured tables, i.e., condensing multiple papers' content into tabular summaries.",
            "citation_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": "Not described in the preface; the title implies an approach that uses LLMs to parse multiple papers and produce tabular syntheses, but no prompt engineering, retrieval or multi-step pipeline details are given in this preface.",
            "number_of_papers": null,
            "domain_or_field": "arXiv / multi-domain scholarly literature",
            "type_of_laws_extracted": "tabular synthesis of empirical findings and structured summaries (enables extraction of recurring patterns and empirical generalizations across papers)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Cited as an example of applying language models to synthesize literature into more structured, machine-readable summaries (tables), which supports distillation of collective findings across papers.",
            "challenges_limitations": "No implementation or evaluation details are provided in the preface; general concerns about reliability and hallucination of LLM outputs apply but are not reported specifically for this work here.",
            "uuid": "e4237.1",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SciMON",
            "name_full": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
            "brief_description": "A referenced system that aims to generate novel scientific inspirations or directions by processing scientific literature, using methods presented in the cited paper.",
            "citation_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": "Not provided in the preface. The referenced title suggests use of models to mine literature for novel or inspiring research directions, but no prompt/retrieval/iterative procedure is described here.",
            "number_of_papers": null,
            "domain_or_field": "computational linguistics / general scientific literature (as per ACL proceedings context)",
            "type_of_laws_extracted": "generation of novel research directions and identification of high-level conceptual patterns across papers (inspirational/idea-generation outputs rather than formal laws)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Cited in the preface as an example of work that uses LLM-style systems to produce novel research directions from collections of papers.",
            "challenges_limitations": "No specifics given in the preface; general limitations of LLM-driven discovery (e.g., reliability, validation of generated directions) are implied but not detailed.",
            "uuid": "e4237.2",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "HoneyComb",
            "name_full": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "brief_description": "A referenced system that uses LLM-based agents to assist tasks in materials science, suggesting application of LLMs to domain-specific scientific literature and workflows.",
            "citation_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": "Not described in the preface. The paper title indicates an agent-based LLM system tailored to materials science workflows, likely processing domain literature and data, but no procedural details are available here.",
            "number_of_papers": null,
            "domain_or_field": "materials science",
            "type_of_laws_extracted": "domain-specific design principles, hypotheses or patterns relevant to materials discovery (implied by materials-science focus), though the preface does not specify exact output types",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Mentioned as an example of domain-specific LLM agents that can assist experts, implying extraction or synthesis of domain patterns or candidate hypotheses from the literature/data.",
            "challenges_limitations": "No evaluation details provided in the preface; general concerns about domain adaptation, factuality and validation of model outputs are relevant but not specified.",
            "uuid": "e4237.3",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OpenResearcher",
            "name_full": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
            "brief_description": "A referenced system/demonstration that uses AI to accelerate scientific research workflows, likely by processing scholarly documents to help generate research directions and assist exploration.",
            "citation_title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": "Not described in the preface. The referenced demo title suggests an interactive system using LLMs to process and explore papers, but no implementation details are given here.",
            "number_of_papers": null,
            "domain_or_field": "general scientific research",
            "type_of_laws_extracted": "generation of research directions and curated summaries; potential extraction of cross-paper patterns and hypotheses (implied)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Listed among recent works showcasing how LLM-based tools enable interactive exploration and accelerate research by synthesizing across papers.",
            "challenges_limitations": "No concrete limitations reported in the preface for this system; broader issues with LLM deployment (cost, ethics, factuality) are noted elsewhere in the preface.",
            "uuid": "e4237.4",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Survey (Zhang et al., 2024b)",
            "name_full": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "brief_description": "A referenced comprehensive survey paper that reviews scientific LLMs and their applications to scientific discovery, which likely covers instances where models extract high-level principles or generate hypotheses from papers.",
            "citation_title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": null,
            "number_of_papers": null,
            "domain_or_field": "meta-study across scientific domains",
            "type_of_laws_extracted": "survey-level discussion of applications including hypothesis generation, discovery and synthesis (likely includes discussion of pattern/principle extraction methods, but not detailed in the preface)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Cited as evidence that LLMs are being developed and applied for accelerating discovery and generating novel research directions across scientific domains.",
            "challenges_limitations": "Survey likely discusses limitations (not reproduced in the preface), and the preface itself highlights broader concerns such as cost and ethical issues.",
            "uuid": "e4237.5",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        },
        {
            "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "rating": 2,
            "sanitized_title": "honeycomb_a_flexible_llmbased_agent_system_for_materials_science"
        },
        {
            "paper_title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
            "rating": 2,
            "sanitized_title": "openresearcher_unleashing_ai_for_accelerated_scientific_research"
        },
        {
            "paper_title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "rating": 2,
            "sanitized_title": "a_comprehensive_survey_of_scientific_large_language_models_and_their_applications_in_scientific_discovery"
        },
        {
            "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        }
    ],
    "cost": 0.0100795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Preface to the Special Issue of the TAL Journal on Scholarly Document Processing
4 Jun 2025</p>
<p>Florian Boudin 
JFLI
CNRS
Nantes University
France</p>
<p>Akiko Aizawa 
National Institute of Informatics
Japan</p>
<p>Preface to the Special Issue of the TAL Journal on Scholarly Document Processing
4 Jun 202541F16CB88D7E3BD5A12DFCDDE286FDD4arXiv:2506.03587v1[cs.DL]Scholarly Document ProcessingNatural Language Processing for ScienceLarge Language Models (LLMs) MOTS-CLS : Traitement automatique de documents scientifiquesTraitement Automatique des Langue pour la scienceGrands modles de langues (LLMs)
The rapid growth of scholarly literature makes it increasingly difficult for researchers to keep up with new knowledge.Automated tools are now more essential than ever to help navigate and interpret this vast body of information.Scientific papers pose unique difficulties, with their complex language, specialized terminology, and diverse formats, requiring advanced methods to extract reliable and actionable insights.Large language models (LLMs) offer new opportunities, enabling tasks such as literature reviews, writing assistance, and interactive exploration of research.This special issue of the TAL journal highlights research addressing these challenges and, more broadly, research on natural language processing and information retrieval for scholarly and scientific documents.RSUM.La croissance rapide de la littrature scientifique rend de plus en plus difficile pour les chercheurs de suivre l'volution des connaissances.Le recours  des outils automatiss est aujourd'hui indispensable pour naviguer et interprter cette immense masse d'informations.Les articles scientifiques posent des difficults uniques en raison de leur langage complexe, de leur terminologie spcialise et de leurs formats varis, ce qui ncessite des mthodes avances pour extraire des informations fiables et exploitables.Les grands modles de langage (LLMs) ouvrent de nouvelles perspectives, permettant des tches telles que les revues de littrature, l'assistance  la rdaction et l'exploration interactive des travaux scientifiques.Ce numro spcial de la revue TAL met en lumire des recherches qui s'attaquent  ces dfis et, plus largement, des recherches sur le traitement automatique des langues et la recherche d'information appliqus aux documents scientifiques et acadmiques.</p>
<p>Introduction</p>
<p>The volume of scholarly literature is expanding rapidly.A compelling example is the ACL Anthology 1 , a repository for scientific contributions within the fields of computational linguistics and Natural Language Processing (NLP), which recently surpassed 100,000 papers, doubling its size in just four years (Bollmann et al., 2023).As the rate of publication continues to accelerate, researchers and institutions face increasing challenges in keeping up with the flood of new knowledge.This highlights the critical need for automated methods to help navigate, understand and distill the growing body of scientific information.</p>
<p>To address this pressing challenge, researchers across various fields-including computational linguistics, NLP, text mining, information retrieval, digital libraries and scientometrics-have dedicated significant efforts into developing methods and resources designed to process scientific documents.This led to a surge in publications on the matter, alongside the successful hosting of numerous international events, such as the workshops Scholarly Document Processing (SDP) (Ghosal et al., 2024), SCIentific DOCument Analysis (SCIDOCA) (Nguyen and Matsumoto, 2024), Natural Language Processing for Scientific Text (SciNLP) (Cohan et al., 2021) and Bibliometricenhanced Information Retrieval (BIR) (Frommholz et al., 2024).</p>
<p>At the national level in France, scholarly document processing is also gaining momentum.This interest is exemplified by the success of the workshop Analyse et Recherche de Textes Scientifiques (ARTS) 2 (Boudin et al., 2023), held at the TALN-CORIA 2023 conference.The event, which saw the presentation of 12 papers and attracted over 40 participants, highlighted the relevance of the topic and prompted the call for this special issue of the TAL journal.</p>
<p>Scientific papers present unique challenges for document processing methods due to their inherent complexity.They are characterized by intricate technical language, discipline-specific terminology, distinct structural conventions, and frequent use of mathematical expressions, all of which pose significant challenges for current methods (Ramesh Kashyap et al., 2023).Additionally, the multi-modal nature of scientific papers, with their tables, figures and diagrams, further complicates their processing (Shen et al., 2022).Beyond these document-level challenges, effective methods should also account for features present at the collection level, such as citation networks, and leverage rich metadata, including authors, keywords, and publication venues, each introducing its own set of difficulties.</p>
<p>Developing methods to extract reliable, valuable and verifiable information from scientific papers is crucial for many downstream applications, including retrieval (Boudin et al., 2020;Wang et al., 2023), recommendation (Kreutz and Schenkel, 2022;Huang et al., 2024), summarization (Luo et al., 2023), questionanswering (Saikh et al., 2022;Auer et al., 2023) and document understanding (Wright and Augenstein, 2021;Veyseh et al., 2021).With the rise of large language models (LLMs) and their enhanced ability to analyze and synthetize insights across multiple scientific papers, new applications are continuously emerging.Promising developments include accelerating scientific discovery (Zhang et al., 2024b), generating novel research directions (Wang et al., 2024), reviewing of the literature (Newman et al., 2024), assisting scientific writing (Jourdan et al., 2024) and enabling interactive exploration of papers (Zheng et al., 2024).</p>
<p>LLMs are also being developed for specialized scientific domains, such as healthcare and medicine (Labrak et al., 2024) or material sciences (Zhang et al., 2024a).These domain-specific models assist experts and researchers with complex tasks, including drug discovery (Savage, 2023), diagnosis generation (Abdullahi et al., 2024), and science education (Cooper, 2023).</p>
<p>Efforts are also underway to reduce the growing computational and environmental costs associated with training and deploying LLMs (Hershcovich et al., 2022;Sadat Moosavi et al., 2023).At the same time, ethical concerns are being addressed, with research focused on the responsible use of LLMs in science, including issues of bias, fairness, and transparency in AI-driven research (Peled-Cohen et al., 2024).This special issue of the TAL journal focuses on research addressing these challenges, with an emphasis on NLP and information retrieval for scholarly and scientific documents.</p>
<p>Call, Reviewing and Selection of Papers</p>
<p>The call for submissions to this special issue of the TAL journal on scholarly document processing was announced in December 2023, and the submission platform 3 closed in March 2024.The scope of relevant topics extended beyond NLP and information retrieval tasks, tools, and resources designed for scientific documents, encompassing areas such as bibliometrics, scientometrics, citation analysis and recommendation, claim verification, plagiarism detection and scientific writing assistance.</p>
<p>A total of five articles were submitted (two in French and three in English) by authors from Iran, India and France.Each article was reviewed by three experts, two members of the scientific committee and one member of the Editorial Board.In May 2024, the Editorial Board and the guest editors met to discuss the first round of reviews and notified the authors of the outcomes.One paper was selected for a second round of reviews and was ultimately accepted, resulting in a selection rate of 20%.</p>
<ol>
<li>https://tal-65-2.sciencesconf.org/3.Accepted paperThis issue of the TAL journal features one paper: valuation de la qualit de rapport des essais cliniques avec des larges modles de langue (Evaluating clinical trials research article quality with large language models) by Mathieu La-king and Patrick Paroubek.The paper focuses on the biomedical domain, specifically investigating the use of LLMs to evaluate the quality of Randomized Controlled Trials (RCTs), a type of clinical research article.The authors frame the evaluation task as a questionanswering problem, prompting LLMs to assess articles according to the Consolidated Standards of Reporting Trials (CONSORT) framework.Through extensive experiments, the study demonstrates the high effectiveness of LLMs, achieving an accuracy of up to 85%, thus paving the way for advancements in automating quality assessment in clinical research.</li>
</ol>
<p>AcknowledgementsWe would like to thank the editorial committee of the TAL journal for inviting us to coordinate the scientific committee for this special issue.We are especially grateful to the editors-in-chief for their support and invaluable assistance throughout this process.Finally, we would like to thank all the reviewers and members of the scientific committee who joined us for this special issue and generously volunteered their time to help us select the articles published here.
Learning to Make Rare and Complex Diagnoses With Generative AI Assistance: Qualitative Study of Popular Large Language Models. References Abdullahi, T Singh, R Eickhoff, C , JMIR Med Educ. 10e51391Feb, 2024</p>
<p>The sciqa scientific question answering benchmark for scholarly knowledge. S Auer, D A Barone, C Bartz, E G Cortes, M Y Jaradeh, O Karras, M Koubarakis, D Mouromtsev, D Pliukhin, D Radyush, Scientific Reports. 1372402023</p>
<p>M Bollmann, N Schneider, A Khn, M Post, Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023). L Tan, D Milajevs, G Chauhan, J Gwinnup, E Rippeth, the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)SingaporeAssociation for Computational LinguisticsDecember, 2023Two Decades of the ACL Anthology: Development, Impact, and Open Challenges</p>
<p>F Boudin, B Daille, R Dufour, O El, M Houbre, L Jourdan, N Kooli, Actes de CORIA-TALN 2023. Actes de l'atelier "Analyse et Recherche de Textes Scientifiques" (ARTS)@TALN 2023. s de CORIA-TALN 2023. s de l'atelier "Analyse et Recherche de Textes Scientifiques" (ARTS)@TALN 2023Paris, France, 62023</p>
<p>Keyphrase Generation for Scientific Document Retrieval. F Boudin, Y Gallina, A Aizawa, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly, 2020</p>
<p>A Cohan, P Dasigi, T Hope, K Lo, S Mohan, A Wade, L Wang, I Williams, D Zhang, Proceedings of the 2nd Workshop on Natural Language Processing for Scientific Text. the 2nd Workshop on Natural Language Processing for Scientific TextSciNLP 2021. August, 2021</p>
<p>Examining science education in ChatGPT: An exploratory study of generative artificial intelligence. G Cooper, Journal of Science Education and Technology. 322023</p>
<p>I Frommholz, P Mayr, G Cabanac, Proceedings of the 14th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2024). S Verberne, the 14th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2024)Glasgow, ScotlandMarch, 2024</p>
<p>T Ghosal, A Singh, A Waard, P Mayr, A Naik, O Weller, Y Lee, S Shen, Y Qin, Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). the Fourth Workshop on Scholarly Document Processing (SDP 2024)Bangkok, ThailandAugust, 2024</p>
<p>Towards Climate Awareness in NLP Research. D Hershcovich, N Webersinke, M Kraus, J Bingler, M Leippold, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember, 2022</p>
<p>A scientific paper recommendation method using the time decay heterogeneous graph. Z Huang, D Tang, R Zhao, W Rao, Scientometrics. 1292024</p>
<p>CASIMIR: A Corpus of Scientific Articles Enhanced with Multiple Author-Integrated Revisions. Jourdan L Boudin, F Hernandez, N Dufour, R , Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), ELRA and ICCL. N Calzolari, M.-Y Kan, V Hoste, A Lenci, S Sakti, N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), ELRA and ICCLTorino, ItaliaMay, 2024</p>
<p>Scientific paper recommendation systems: a literature review of recent publications. C K Kreutz, R Schenkel, International journal on digital libraries. 232022</p>
<p>BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains. Y Labrak, A Bazoge, E Morin, P.-A Gourraud, M Rouvier, R Dufour, in L.-W</p>
<p>Findings of the Association for Computational Linguistics: ACL 2024. A Ku, V Martins, Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust, 2024</p>
<p>CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization. Z Luo, Q Xie, S Ananiadou, Proceedings of the ACM Web Conference 2023, WWW '23. the ACM Web Conference 2023, WWW '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models. B Newman, Y Lee, A Naik, P Siangliulue, R Fok, J Kim, D S Weld, J C Chang, K Lo, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember, 2024</p>
<p>M L Nguyen, Y Matsumoto, Proceedings of the Eighth International Workshop on SCIentific DOCument Analysis (SCIDOCA 2024). the Eighth International Workshop on SCIentific DOCument Analysis (SCIDOCA 2024)Hamamatsu, Shizuoka, JapanAugust, 2024</p>
<p>L Peled-Cohen, N Calderon, S Lissak, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). R Reichart, the 1st Workshop on NLP for Science (NLP4Science)Miami, FL, USAAssociation for Computational LinguisticsNovember, 2024</p>
<p>Scientific document processing: challenges for modern learning methods. Ramesh Kashyap, A Yang, Y Kan, M.-Y , Int. J. Digit. Libr. 24mar, 2023</p>
<p>Sadat Moosavi, N Gurevych, I Hou, Y Kim, G Kim, Y J Schuster, T , Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP). A Agrawal, The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)Toronto, CanadaAssociation for Computational LinguisticsJuly, 2023</p>
<p>Scienceqa: A novel resource for question answering on scholarly articles. T Saikh, T Ghosal, A Mittal, A Ekbal, P Bhattacharyya, International Journal on Digital Libraries. 2332022</p>
<p>Drug discovery companies are customizing ChatGPT: here's how. N Savage, Nat Biotechnol. 4152023</p>
<p>VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups. Z Shen, K Lo, L L Wang, B Kuehl, D S Weld, D Downey, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Acronym Identification and Disambiguation Shared Tasks for Scientific Document Understanding. A P B Veyseh, F Dernoncourt, T H Nguyen, W Chang, L A Celi, 2021</p>
<p>Scientific Document Retrieval using Multi-level Aspect-based Queries. J A Wang, K Wang, X Wang, P Naidu, L Bergen, R Paturi, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>SciMON: Scientific Inspiration Machines Optimized for Novelty. Q Wang, D Downey, H Ji, T Hope, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust, 20241</p>
<p>CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding. D Wright, I Augenstein, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. C Zong, F Xia, W Li, R Navigli, Association for Computational LinguisticsAugust, 2021</p>
<p>HoneyComb: A Flexible LLM-Based Agent System for Materials Science. H Zhang, Y Song, Z Hou, S Miret, B Liu, Findings of the Association for Computational Linguistics: EMNLP 2024. Y Al-Onaizan, M Bansal, Y.-N Chen, Miami, Florida, USAAssociation for Computational LinguisticsNovember, 2024a</p>
<p>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember, 2024b</p>
<p>OpenResearcher: Unleashing AI for Accelerated Scientific Research. Y Zheng, S Sun, L Qiu, D Ru, C Jiayang, X Li, J Lin, B Wang, Y Luo, R Pan, Y Xu, Q Min, Z Zhang, Y Wang, W Li, P Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. D I Hernandez Farias, T Hope, M Li, the 2024 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsMiami, Florida, USAAssociation for Computational LinguisticsNovember, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-616 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-616</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-616</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-a078d53c1eff50123e2b065276663de539a40aa1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a078d53c1eff50123e2b065276663de539a40aa1" target="_blank">Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper performs a survey of recent commonsense QA methods and provides a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets, and shows that attention-based injection seems to be a preferable choice for knowledge integration.</p>
                <p><strong>Paper Abstract:</strong> Non-extractive commonsense QA remains a challenging AI task, as it requires systems to reason about, synthesize, and gather disparate pieces of information, in order to generate responses to queries. Recent approaches on such tasks show increased performance, only when models are either pre-trained with additional information or when domain-specific heuristics are used, without any special consideration regarding the knowledge resource type. In this paper, we perform a survey of recent commonsense QA methods and we provide a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for knowledge integration and that the degree of domain overlap, between knowledge bases and datasets, plays a crucial role in determining model success.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e616.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e616.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OCN+CN-inject</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Option Comparison Network with ConceptNet attention-based knowledge injection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid neuro-symbolic system that injects ConceptNet commonsense triples into a BERT-backed Option Comparison Network (OCN) via an attention-based module to augment textual representations for multiple-choice commonsense QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Option Comparison Network (OCN) + ConceptNet attention injection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architecture: a BERT encoder produces contextual token representations for (dialogue / question / option); the OCN module models pairwise option comparisons (tri-linear attention, gating, co-attention, self-attention) to produce option representations for multiple-choice prediction. Declarative ConceptNet triples are converted into pseudo-sentences, embedded (BiLSTM), and projected to the encoder dimension. An attention module computes similarity between text encodings and the knowledge matrix to produce text-to-knowledge and knowledge-to-text attention outputs; these are concatenated with the original text encoding and passed through an MLP (ReLU) to produce a knowledge-aware text representation that replaces the original encoder output for downstream OCN layers.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ConceptNet knowledge graph represented as (concept1, relation, concept2) triples; triples are turned into pseudo-sentences (e.g., 'book at location library') and embedded via a BiLSTM to form a knowledge matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural components: BERT contextual encoder (Whole-Word Masking Uncased), Option Comparison Network (tri-linear attention, gating, co-attention, self-attention), BiLSTM for knowledge encoding; all trained with gradient-based learning (fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Attention-based injection: project encoded ConceptNet triple embeddings into encoder space (W_proj), compute tri-linear attention similarity matrix S between encoder tokens and knowledge entries, derive text-to-knowledge (A_m) and knowledge-to-text (A_t) attended vectors, concatenate [T_enc; A_m; T_enc ∘ A_m; T_enc ∘ A_t], then apply linear + ReLU to produce T_out which is used in place of T_enc in subsequent OCN layers (end-to-end fine-tuning of the neural parts; the knowledge graph is injected as fixed embeddings derived from pseudo-sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved ability to select answers when explicit ConceptNet relations link question/dialogue tokens to answer options (better retrieval of multi-hop, relational cues); model exhibits option-comparison behavior augmented by explicit symbolic relations, enabling stronger disambiguation for AtLocation, Causes, and related relation types compared to the neural baseline; provides human-interpretable supporting triples as evidence for some predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multiple-choice commonsense question answering (DREAM and CommonsenseQA datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>DREAM: Dev accuracy 70.5%, Test accuracy 69.6%; CommonsenseQA: Dev accuracy 67.3% (see Table 5 and Table 6 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>OCN (neural baseline) — DREAM: Dev 70.0%, Test 69.8%; CommonsenseQA: Dev 64.1% (paper reports OCN baseline). BERT Large (neural baseline reported elsewhere) — DREAM: Dev 66.0%, Test 66.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Generalization depends strongly on domain alignment: when ConceptNet relation types align with question types in the dataset (e.g., CommonsenseQA constructed from ConceptNet), injection yields consistent improvements across relation categories; when alignment is weaker (mismatch between KB domain and dataset question types), gains are smaller or absent. The paper reports that attention-injection does not severely degrade performance under imperfect alignment, unlike pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>The declarative component (ConceptNet triples) offers explicit, human-readable evidence (extracted triples and paths) that can be inspected to justify model choices; however the paper does not provide a formal explanation module — interpretability is bounded to inspection of the injected triples and attention weights.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Improvements are modest and focused on question types that align with ConceptNet; injection can slightly hurt some question categories (e.g., Matching) and overall gains are limited. Performance benefit depends on accurate extraction/matching of concepts; phrase-matching relaxation was necessary. No purely symbolic baseline was evaluated; complex, compositional reasoning beyond ConceptNet paths is not demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Informal complementary-strengths framework: symbolic KBs provide explicit relational constraints and facts while neural networks provide flexible pattern recognition and contextual composition; attention-based injection enables fusion by aligning token-level neural representations with symbolic facts. Authors also describe a 'weight distribution shift' view for pre-training effects and advocate identifying question types to select appropriate KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e616.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e616.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OCN+AT-inject</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Option Comparison Network with ATOMIC (COMET-generated) attention injection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system that injects ATOMIC procedural/folk-psychology knowledge (generated via COMET) into the OCN/BERT pipeline, using a similar attention-based injection mechanism but with adaptation for ATOMIC sentence-format triples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Option Comparison Network (OCN) + ATOMIC (COMET) attention injection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architecture: same OCN+BERT backbone as above. ATOMIC relations are not direct concept triples but short sentences/verb-phrases; the authors generate ATOMIC-style tails using the COMET model for each candidate sentence/sub-sentence (speaker heads are replaced with speaker tokens W/M), convert relation tokens to natural tokens (e.g., 'xReact'→'react'), form pseudo-sentences, embed them, and inject via the same attention-based projection and concatenation procedure into corresponding dialogue or option encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ATOMIC commonsense knowledge base (if-then event implications) represented as (event, relation, attribute/effect/persona/mental-state); the paper uses COMET to generate ATOMIC tails for candidate utterances and options, producing sentence-like knowledge entries.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural components: BERT encoder, OCN option-comparison modules, COMET model (transformer-based generative model) used to produce ATOMIC inferences; BiLSTM embeddings for pseudo-sentences; end-to-end fine-tuning of OCN pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Attention-based injection of COMET-generated ATOMIC relations into textual encodings: map ATOMIC pseudo-sentences into embedding space, compute attention between text encodings and ATOMIC embeddings, derive attended vectors and concatenate with original encodings to obtain knowledge-aware representations fed to OCN layers. Dialogue relations are injected into dialogue encoder (D_enc) and option relations into option encoder (O_enc) separately.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved handling of questions about agents' mental states, intentions, reactions, and procedural commonsense (e.g., speaker feelings / intentions) due to ATOMIC's folk-psychology coverage; when task questions probe agent attributes or reactions, ATOMIC injection can increase accuracy on those categories relative to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multiple-choice commonsense QA (DREAM and CommonsenseQA).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>DREAM: Dev accuracy 69.6%, Test accuracy 70.1% (OCN + AT injection). CommonsenseQA: AT-only injection results not shown as primary — ATOMIC pre-training numbers reported separately (see below).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>OCN baseline — DREAM: Dev 70.0%, Test 69.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Gains are specific to question types aligned with ATOMIC (folk psychology, intents, reactions). ATOMIC injection improved accuracy on DREAM questions involving commonsense inference and on CommonsenseQA categories that map to ATOMIC relations (e.g., Causes/Desires). When domain alignment is poor, pre-training on ATOMIC hurts overall performance (see pre-training results), but attention-injection tends to be less harmful.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>COMET-generated ATOMIC inferences are sentence-like and provide interpretable candidate implications for each utterance/option; these can be inspected to understand why an option's attributes were favored. Still, reasoning steps remain embedded in neural attention and not fully symbolic/executable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>ATOMIC heads/tails require generation (COMET) because raw ATOMIC uses placeholders (PersonX/Y), potentially introducing generation noise. Injection gains on DREAM are small; pre-training on ATOMIC as a masked-language objective degrades performance substantially, indicating domain sensitivity. Extraction/generation errors and imperfect alignment limit benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Same informal complementary view: ATOMIC supplies procedural/faux-psychological rules while the neural pipeline composes context; authors stress that domain overlap between ATOMIC and dataset questions determines benefit and warn against global pre-training shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e616.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e616.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KB-pretrain(OMCS/ATOMIC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge pre-training of BERT on OMCS / ATOMIC-derived sentences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid training procedure that pre-trains BERT on corpora derived from commonsense knowledge resources (OMCS sentences for ConceptNet, or COMET-converted ATOMIC sentences) with a masked-language objective, then fine-tunes the pre-trained model within the OCN pipeline on QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERT pre-training on OMCS (for ConceptNet) or ATOMIC-derived sentences + OCN fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure: convert knowledge-base content into text corpora (OMCS direct sentences for ConceptNet; COMET-generated sentence-format ATOMIC data with special tokens for relations/blanks). Pre-train/fine-tune BERT using masked language modeling (15% mask) on the knowledge-derived corpora (OMCS ~930K sentences; ATOMIC masked tails), then load this BERT into OCN and fine-tune on DREAM and CommonsenseQA datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>OMCS corpus (natural language sentences collected for ConceptNet) or ATOMIC-derived sentences produced by COMET; these corpora are treated as declarative text approximations of the knowledge bases.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transformer neural language model (BERT) pre-trained with masked-language objective and subsequently fine-tuned within the OCN architecture; gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Sequential integration: knowledge is injected into the neural model via pre-training on KB-derived text (model weights shift to encode KB distribution) before supervised fine-tuning on downstream QA; no explicit attention-based injection at inference unless combined with injection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>When domain alignment is high (OMCS with ConceptNet-derived CommonsenseQA types), pre-training can slightly improve downstream accuracy and allows model weights to encode KB distributional patterns; when alignment is poor, pre-training produces a substantial weight-distribution shift that degrades performance on unrelated question types. Pre-training tends to bias the model towards KB-specific inference patterns (e.g., better on Causes/Desires-related questions if pre-trained on ATOMIC-like data).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multiple-choice commonsense QA (DREAM and CommonsenseQA).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>OMCS pre-train: DREAM: Dev 64.0%, Test 62.6% (degraded vs baseline); CommonsenseQA: Dev 65.2% (small boost vs OCN baseline). ATOMIC pre-train: DREAM: Dev 60.3%, Test 58.8% (degraded); CommonsenseQA: Dev 61.2% (degraded). Combined OMCS pre-train + CN injection: CommonsenseQA Dev 69.0% (best in their experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>OCN baseline — DREAM: Dev 70.0%, Test 69.8%; CommonsenseQA: Dev 64.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Pre-training causes notable weight distribution shifts; if KB corpus matches downstream task domain the model can generalize better on aligned question types, but overall out-of-domain generalization is worse — pre-training often reduced performance on Matching and many non-commonsense question types in DREAM. Hence, pre-training helps domain-specific generalization but harms broad generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Pre-training encodes KB statistics into distributed weights; it does not yield explicit, inspectable symbolic traces at inference time. Some implicit alignment to KB relation types can be observed in per-category performance, but no explicit explanation module is produced by pre-training alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Pre-training on ATOMIC or OMCS can substantially degrade overall performance when the KB-domain does not match task distribution; pre-training effects are hard to control and can harm 'matching' style questions. Authors note instability (e.g., XLNet instability) and that pre-training must be applied carefully; no differentiable symbolic reasoning is learned, only distributional bias.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Authors explain effects via 'weight distribution shift' — pre-training biases model weights toward KB domain; they recommend conditional or auxiliary objectives (multi-task constrained optimization) to identify and apply the right KB per-question to avoid harmful shifts. They propose a complementary division of labor but offer no formal mathematical theory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e616.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e616.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OMCS+CN-inject (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combined OMCS pre-trained BERT with ConceptNet attention injection into OCN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined hybrid approach that first pre-trains BERT on OMCS (ConceptNet source corpus) and then uses attention-based ConceptNet injection within the OCN pipeline, showing additive improvement when KB-domain aligns with the task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OMCS pre-trained BERT + ConceptNet attention injection into OCN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline: pre-train BERT on OMCS masked-language objective to bias weights toward ConceptNet-like knowledge; during QA fine-tuning, inject ConceptNet pseudo-sentences via the attention-based injection module into OCN as described for CN-inject. This combination leverages both weight-level encoding of KB statistics and explicit symbolic attention at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>OMCS sentences (textualized ConceptNet source) and ConceptNet triples converted into pseudo-sentences for attention injection.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Pre-trained BERT encoder (fine-tuned), OCN neural architecture, BiLSTM embeddings for injected triples; gradient-based fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Sequential (pre-training) + attention-based injection during fine-tuning: first shift model weights via OMCS pre-training, then fuse explicit ConceptNet triples at token-level using the tri-linear attention projection and concatenation into OCN pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>When KB domain and dataset align (CommonsenseQA is derived from ConceptNet), combination yields additive gains (better coverage of ConceptNet relations both implicitly in weights and explicitly via attention), improving per-relation accuracies substantially (e.g., large boosts on CausesDesire, AtLocation, Causes categories).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>CommonsenseQA (primary combined improvement reported); also evaluated on DREAM.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>CommonsenseQA: Dev accuracy 69.0% for OCN + OMCS pre-train + CN injection (table 6). DREAM: combined results not explicitly tabulated beyond individual effects.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>OCN baseline — CommonsenseQA: Dev 64.1%. OMCS pre-train alone — CommonsenseQA: Dev 65.2%. CN injection alone — CommonsenseQA: Dev 67.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Combined method shows strong gains when KB and dataset are aligned (CommonsenseQA) — authors report additive improvement over either method alone; however the approach is still sensitive to domain mismatch and may not generalize across datasets with different question distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Combination retains interpretability benefits of explicit ConceptNet triples (inspectable attention targets) while also encoding KB statistics implicitly; still no formal explanation-generation mechanism is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires good KB-dataset alignment to realize additive benefits; pre-training could still shift weights undesirably for datasets outside the KB domain. The approach relies on heuristics for extracting/matching concepts and on COMET generation for ATOMIC, which can introduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Empirical complementarity hypothesis: implicit pre-training encodes broad KB distributional patterns while explicit attention injection supplies targeted symbolic constraints at inference; together they can be additive when domains align. Authors recommend future multi-task constrained objectives to conditionally apply KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Commonsense for generative multi-hop question answering tasks <em>(Rating: 2)</em></li>
                <li>COMET: Commonsense transformers for automatic knowledge graph construction <em>(Rating: 2)</em></li>
                <li>Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Dynamic integration of background knowledge in neural nlu systems <em>(Rating: 2)</em></li>
                <li>KAGNET: Knowledge-aware graph networks for commonsense reasoning <em>(Rating: 2)</em></li>
                <li>Improving question answering with external knowledge <em>(Rating: 2)</em></li>
                <li>Improving question answering by commonsense-based pre-training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-616",
    "paper_id": "paper-a078d53c1eff50123e2b065276663de539a40aa1",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "OCN+CN-inject",
            "name_full": "Option Comparison Network with ConceptNet attention-based knowledge injection",
            "brief_description": "A hybrid neuro-symbolic system that injects ConceptNet commonsense triples into a BERT-backed Option Comparison Network (OCN) via an attention-based module to augment textual representations for multiple-choice commonsense QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Option Comparison Network (OCN) + ConceptNet attention injection",
            "system_description": "Architecture: a BERT encoder produces contextual token representations for (dialogue / question / option); the OCN module models pairwise option comparisons (tri-linear attention, gating, co-attention, self-attention) to produce option representations for multiple-choice prediction. Declarative ConceptNet triples are converted into pseudo-sentences, embedded (BiLSTM), and projected to the encoder dimension. An attention module computes similarity between text encodings and the knowledge matrix to produce text-to-knowledge and knowledge-to-text attention outputs; these are concatenated with the original text encoding and passed through an MLP (ReLU) to produce a knowledge-aware text representation that replaces the original encoder output for downstream OCN layers.",
            "declarative_component": "ConceptNet knowledge graph represented as (concept1, relation, concept2) triples; triples are turned into pseudo-sentences (e.g., 'book at location library') and embedded via a BiLSTM to form a knowledge matrix.",
            "imperative_component": "Neural components: BERT contextual encoder (Whole-Word Masking Uncased), Option Comparison Network (tri-linear attention, gating, co-attention, self-attention), BiLSTM for knowledge encoding; all trained with gradient-based learning (fine-tuning).",
            "integration_method": "Attention-based injection: project encoded ConceptNet triple embeddings into encoder space (W_proj), compute tri-linear attention similarity matrix S between encoder tokens and knowledge entries, derive text-to-knowledge (A_m) and knowledge-to-text (A_t) attended vectors, concatenate [T_enc; A_m; T_enc ∘ A_m; T_enc ∘ A_t], then apply linear + ReLU to produce T_out which is used in place of T_enc in subsequent OCN layers (end-to-end fine-tuning of the neural parts; the knowledge graph is injected as fixed embeddings derived from pseudo-sentences).",
            "emergent_properties": "Improved ability to select answers when explicit ConceptNet relations link question/dialogue tokens to answer options (better retrieval of multi-hop, relational cues); model exhibits option-comparison behavior augmented by explicit symbolic relations, enabling stronger disambiguation for AtLocation, Causes, and related relation types compared to the neural baseline; provides human-interpretable supporting triples as evidence for some predictions.",
            "task_or_benchmark": "Multiple-choice commonsense question answering (DREAM and CommonsenseQA datasets).",
            "hybrid_performance": "DREAM: Dev accuracy 70.5%, Test accuracy 69.6%; CommonsenseQA: Dev accuracy 67.3% (see Table 5 and Table 6 in paper).",
            "declarative_only_performance": null,
            "imperative_only_performance": "OCN (neural baseline) — DREAM: Dev 70.0%, Test 69.8%; CommonsenseQA: Dev 64.1% (paper reports OCN baseline). BERT Large (neural baseline reported elsewhere) — DREAM: Dev 66.0%, Test 66.8%.",
            "has_comparative_results": true,
            "generalization_properties": "Generalization depends strongly on domain alignment: when ConceptNet relation types align with question types in the dataset (e.g., CommonsenseQA constructed from ConceptNet), injection yields consistent improvements across relation categories; when alignment is weaker (mismatch between KB domain and dataset question types), gains are smaller or absent. The paper reports that attention-injection does not severely degrade performance under imperfect alignment, unlike pre-training.",
            "interpretability_properties": "The declarative component (ConceptNet triples) offers explicit, human-readable evidence (extracted triples and paths) that can be inspected to justify model choices; however the paper does not provide a formal explanation module — interpretability is bounded to inspection of the injected triples and attention weights.",
            "limitations_or_failures": "Improvements are modest and focused on question types that align with ConceptNet; injection can slightly hurt some question categories (e.g., Matching) and overall gains are limited. Performance benefit depends on accurate extraction/matching of concepts; phrase-matching relaxation was necessary. No purely symbolic baseline was evaluated; complex, compositional reasoning beyond ConceptNet paths is not demonstrated.",
            "theoretical_framework": "Informal complementary-strengths framework: symbolic KBs provide explicit relational constraints and facts while neural networks provide flexible pattern recognition and contextual composition; attention-based injection enables fusion by aligning token-level neural representations with symbolic facts. Authors also describe a 'weight distribution shift' view for pre-training effects and advocate identifying question types to select appropriate KBs.",
            "uuid": "e616.0",
            "source_info": {
                "paper_title": "Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "OCN+AT-inject",
            "name_full": "Option Comparison Network with ATOMIC (COMET-generated) attention injection",
            "brief_description": "A hybrid system that injects ATOMIC procedural/folk-psychology knowledge (generated via COMET) into the OCN/BERT pipeline, using a similar attention-based injection mechanism but with adaptation for ATOMIC sentence-format triples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Option Comparison Network (OCN) + ATOMIC (COMET) attention injection",
            "system_description": "Architecture: same OCN+BERT backbone as above. ATOMIC relations are not direct concept triples but short sentences/verb-phrases; the authors generate ATOMIC-style tails using the COMET model for each candidate sentence/sub-sentence (speaker heads are replaced with speaker tokens W/M), convert relation tokens to natural tokens (e.g., 'xReact'→'react'), form pseudo-sentences, embed them, and inject via the same attention-based projection and concatenation procedure into corresponding dialogue or option encodings.",
            "declarative_component": "ATOMIC commonsense knowledge base (if-then event implications) represented as (event, relation, attribute/effect/persona/mental-state); the paper uses COMET to generate ATOMIC tails for candidate utterances and options, producing sentence-like knowledge entries.",
            "imperative_component": "Neural components: BERT encoder, OCN option-comparison modules, COMET model (transformer-based generative model) used to produce ATOMIC inferences; BiLSTM embeddings for pseudo-sentences; end-to-end fine-tuning of OCN pipeline.",
            "integration_method": "Attention-based injection of COMET-generated ATOMIC relations into textual encodings: map ATOMIC pseudo-sentences into embedding space, compute attention between text encodings and ATOMIC embeddings, derive attended vectors and concatenate with original encodings to obtain knowledge-aware representations fed to OCN layers. Dialogue relations are injected into dialogue encoder (D_enc) and option relations into option encoder (O_enc) separately.",
            "emergent_properties": "Improved handling of questions about agents' mental states, intentions, reactions, and procedural commonsense (e.g., speaker feelings / intentions) due to ATOMIC's folk-psychology coverage; when task questions probe agent attributes or reactions, ATOMIC injection can increase accuracy on those categories relative to baseline.",
            "task_or_benchmark": "Multiple-choice commonsense QA (DREAM and CommonsenseQA).",
            "hybrid_performance": "DREAM: Dev accuracy 69.6%, Test accuracy 70.1% (OCN + AT injection). CommonsenseQA: AT-only injection results not shown as primary — ATOMIC pre-training numbers reported separately (see below).",
            "declarative_only_performance": null,
            "imperative_only_performance": "OCN baseline — DREAM: Dev 70.0%, Test 69.8%.",
            "has_comparative_results": true,
            "generalization_properties": "Gains are specific to question types aligned with ATOMIC (folk psychology, intents, reactions). ATOMIC injection improved accuracy on DREAM questions involving commonsense inference and on CommonsenseQA categories that map to ATOMIC relations (e.g., Causes/Desires). When domain alignment is poor, pre-training on ATOMIC hurts overall performance (see pre-training results), but attention-injection tends to be less harmful.",
            "interpretability_properties": "COMET-generated ATOMIC inferences are sentence-like and provide interpretable candidate implications for each utterance/option; these can be inspected to understand why an option's attributes were favored. Still, reasoning steps remain embedded in neural attention and not fully symbolic/executable.",
            "limitations_or_failures": "ATOMIC heads/tails require generation (COMET) because raw ATOMIC uses placeholders (PersonX/Y), potentially introducing generation noise. Injection gains on DREAM are small; pre-training on ATOMIC as a masked-language objective degrades performance substantially, indicating domain sensitivity. Extraction/generation errors and imperfect alignment limit benefits.",
            "theoretical_framework": "Same informal complementary view: ATOMIC supplies procedural/faux-psychological rules while the neural pipeline composes context; authors stress that domain overlap between ATOMIC and dataset questions determines benefit and warn against global pre-training shifts.",
            "uuid": "e616.1",
            "source_info": {
                "paper_title": "Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "KB-pretrain(OMCS/ATOMIC)",
            "name_full": "Knowledge pre-training of BERT on OMCS / ATOMIC-derived sentences",
            "brief_description": "A hybrid training procedure that pre-trains BERT on corpora derived from commonsense knowledge resources (OMCS sentences for ConceptNet, or COMET-converted ATOMIC sentences) with a masked-language objective, then fine-tunes the pre-trained model within the OCN pipeline on QA tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BERT pre-training on OMCS (for ConceptNet) or ATOMIC-derived sentences + OCN fine-tuning",
            "system_description": "Procedure: convert knowledge-base content into text corpora (OMCS direct sentences for ConceptNet; COMET-generated sentence-format ATOMIC data with special tokens for relations/blanks). Pre-train/fine-tune BERT using masked language modeling (15% mask) on the knowledge-derived corpora (OMCS ~930K sentences; ATOMIC masked tails), then load this BERT into OCN and fine-tune on DREAM and CommonsenseQA datasets.",
            "declarative_component": "OMCS corpus (natural language sentences collected for ConceptNet) or ATOMIC-derived sentences produced by COMET; these corpora are treated as declarative text approximations of the knowledge bases.",
            "imperative_component": "Transformer neural language model (BERT) pre-trained with masked-language objective and subsequently fine-tuned within the OCN architecture; gradient-based optimization.",
            "integration_method": "Sequential integration: knowledge is injected into the neural model via pre-training on KB-derived text (model weights shift to encode KB distribution) before supervised fine-tuning on downstream QA; no explicit attention-based injection at inference unless combined with injection methods.",
            "emergent_properties": "When domain alignment is high (OMCS with ConceptNet-derived CommonsenseQA types), pre-training can slightly improve downstream accuracy and allows model weights to encode KB distributional patterns; when alignment is poor, pre-training produces a substantial weight-distribution shift that degrades performance on unrelated question types. Pre-training tends to bias the model towards KB-specific inference patterns (e.g., better on Causes/Desires-related questions if pre-trained on ATOMIC-like data).",
            "task_or_benchmark": "Multiple-choice commonsense QA (DREAM and CommonsenseQA).",
            "hybrid_performance": "OMCS pre-train: DREAM: Dev 64.0%, Test 62.6% (degraded vs baseline); CommonsenseQA: Dev 65.2% (small boost vs OCN baseline). ATOMIC pre-train: DREAM: Dev 60.3%, Test 58.8% (degraded); CommonsenseQA: Dev 61.2% (degraded). Combined OMCS pre-train + CN injection: CommonsenseQA Dev 69.0% (best in their experiments).",
            "declarative_only_performance": null,
            "imperative_only_performance": "OCN baseline — DREAM: Dev 70.0%, Test 69.8%; CommonsenseQA: Dev 64.1%.",
            "has_comparative_results": true,
            "generalization_properties": "Pre-training causes notable weight distribution shifts; if KB corpus matches downstream task domain the model can generalize better on aligned question types, but overall out-of-domain generalization is worse — pre-training often reduced performance on Matching and many non-commonsense question types in DREAM. Hence, pre-training helps domain-specific generalization but harms broad generalization.",
            "interpretability_properties": "Pre-training encodes KB statistics into distributed weights; it does not yield explicit, inspectable symbolic traces at inference time. Some implicit alignment to KB relation types can be observed in per-category performance, but no explicit explanation module is produced by pre-training alone.",
            "limitations_or_failures": "Pre-training on ATOMIC or OMCS can substantially degrade overall performance when the KB-domain does not match task distribution; pre-training effects are hard to control and can harm 'matching' style questions. Authors note instability (e.g., XLNet instability) and that pre-training must be applied carefully; no differentiable symbolic reasoning is learned, only distributional bias.",
            "theoretical_framework": "Authors explain effects via 'weight distribution shift' — pre-training biases model weights toward KB domain; they recommend conditional or auxiliary objectives (multi-task constrained optimization) to identify and apply the right KB per-question to avoid harmful shifts. They propose a complementary division of labor but offer no formal mathematical theory.",
            "uuid": "e616.2",
            "source_info": {
                "paper_title": "Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "OMCS+CN-inject (combined)",
            "name_full": "Combined OMCS pre-trained BERT with ConceptNet attention injection into OCN",
            "brief_description": "A combined hybrid approach that first pre-trains BERT on OMCS (ConceptNet source corpus) and then uses attention-based ConceptNet injection within the OCN pipeline, showing additive improvement when KB-domain aligns with the task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "OMCS pre-trained BERT + ConceptNet attention injection into OCN",
            "system_description": "Pipeline: pre-train BERT on OMCS masked-language objective to bias weights toward ConceptNet-like knowledge; during QA fine-tuning, inject ConceptNet pseudo-sentences via the attention-based injection module into OCN as described for CN-inject. This combination leverages both weight-level encoding of KB statistics and explicit symbolic attention at inference.",
            "declarative_component": "OMCS sentences (textualized ConceptNet source) and ConceptNet triples converted into pseudo-sentences for attention injection.",
            "imperative_component": "Pre-trained BERT encoder (fine-tuned), OCN neural architecture, BiLSTM embeddings for injected triples; gradient-based fine-tuning.",
            "integration_method": "Sequential (pre-training) + attention-based injection during fine-tuning: first shift model weights via OMCS pre-training, then fuse explicit ConceptNet triples at token-level using the tri-linear attention projection and concatenation into OCN pipeline.",
            "emergent_properties": "When KB domain and dataset align (CommonsenseQA is derived from ConceptNet), combination yields additive gains (better coverage of ConceptNet relations both implicitly in weights and explicitly via attention), improving per-relation accuracies substantially (e.g., large boosts on CausesDesire, AtLocation, Causes categories).",
            "task_or_benchmark": "CommonsenseQA (primary combined improvement reported); also evaluated on DREAM.",
            "hybrid_performance": "CommonsenseQA: Dev accuracy 69.0% for OCN + OMCS pre-train + CN injection (table 6). DREAM: combined results not explicitly tabulated beyond individual effects.",
            "declarative_only_performance": null,
            "imperative_only_performance": "OCN baseline — CommonsenseQA: Dev 64.1%. OMCS pre-train alone — CommonsenseQA: Dev 65.2%. CN injection alone — CommonsenseQA: Dev 67.3%.",
            "has_comparative_results": true,
            "generalization_properties": "Combined method shows strong gains when KB and dataset are aligned (CommonsenseQA) — authors report additive improvement over either method alone; however the approach is still sensitive to domain mismatch and may not generalize across datasets with different question distributions.",
            "interpretability_properties": "Combination retains interpretability benefits of explicit ConceptNet triples (inspectable attention targets) while also encoding KB statistics implicitly; still no formal explanation-generation mechanism is presented.",
            "limitations_or_failures": "Requires good KB-dataset alignment to realize additive benefits; pre-training could still shift weights undesirably for datasets outside the KB domain. The approach relies on heuristics for extracting/matching concepts and on COMET generation for ATOMIC, which can introduce noise.",
            "theoretical_framework": "Empirical complementarity hypothesis: implicit pre-training encodes broad KB distributional patterns while explicit attention injection supplies targeted symbolic constraints at inference; together they can be additive when domains align. Authors recommend future multi-task constrained objectives to conditionally apply KBs.",
            "uuid": "e616.3",
            "source_info": {
                "paper_title": "Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Commonsense for generative multi-hop question answering tasks",
            "rating": 2
        },
        {
            "paper_title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "rating": 2
        },
        {
            "paper_title": "Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge",
            "rating": 2
        },
        {
            "paper_title": "Dynamic integration of background knowledge in neural nlu systems",
            "rating": 2
        },
        {
            "paper_title": "KAGNET: Knowledge-aware graph networks for commonsense reasoning",
            "rating": 2
        },
        {
            "paper_title": "Improving question answering with external knowledge",
            "rating": 2
        },
        {
            "paper_title": "Improving question answering by commonsense-based pre-training",
            "rating": 1
        }
    ],
    "cost": 0.01480075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering</h1>
<p>Kaixin Ma ${ }^{\text {T }}$ Jonathan Francis ${ }^{\text {T }}$ Quanyang Lu ${ }^{\dagger}$ Eric Nyberg ${ }^{\text {T }}$ Alessandro Oltramari ${ }^{\S}$<br>${ }^{\text {T }}$ Language Technologies Institute, School of Computer Science, Carnegie Mellon University<br>${ }^{\dagger}$ Department of Mechanical Engineering, College of Engineering, Carnegie Mellon University<br>${ }^{\S}$ Intelligent IoT, Bosch Research and Technology Center (Pittsburgh, USA)<br>{kaixinm, jmf1, qlv, ehn}@cs.cmu.edu<br>alessandro.oltramari@us.bosch.com</p>
<h4>Abstract</h4>
<p>Non-extractive commonsense QA remains a challenging AI task, as it requires systems to reason about, synthesize, and gather disparate pieces of information, in order to generate responses to queries. Recent approaches on such tasks show increased performance, only when models are either pre-trained with additional information or when domain-specific heuristics are used, without any special consideration regarding the knowledge resource type. In this paper, we perform a survey of recent commonsense QA methods and we provide a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for knowledge integration and that the degree of domain overlap, between knowledge bases and datasets, plays a crucial role in determining model success.</p>
<h2>1 Introduction</h2>
<p>With the recent success of large pre-trained language models (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), model performance has reached or surpassed human-level capability on many previous question-answering (QA) benchmarks (Hermann et al., 2015; Rajpurkar et al., 2016; Lai et al., 2017). However, these benchmarks do not directly challenge model reasoning capability, as they require only marginal use of external knowledge to select the correct answer, i.e., all the evidence required to solve questions in these benchmarks is explicit in the context lexical space. Efforts have been made towards building more challenging datasets that, by design, require models to synthesize external commonsense</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>knowledge and leverage more sophisticated reasoning mechanisms (Zhang et al., 2018; Ostermann et al., 2018), showing that the previous state-of-the-art models often struggle to solve these newer tasks reliably. As a result, commonsense has received a lot of attention in other areas as well, such as natural language inference (Zellers et al., 2018b, 2019) and visual question answering (Zellers et al., 2018a). Despite the importance of commonsense knowledge, however, previous work on QA methods takes a coarse-grained view of commonsense, without considering the subtle differences across the various knowledge types and resources. Such differences have been discussed at length in AI by philosophers, computational linguists, cognitive psychologists (see for instance (Davis, 2014)): at the high level, we can identify declarative commonsense, whose scope encompassess factual knowledge, e.g., 'the sky is blue', 'Paris is in France'; taxonomic knowledge, e.g., 'football players are athletes', 'cats are mammals'; relational knowledge, e.g., 'the nose is part of the skull', 'handwriting requires a hand and a writing instrument'; procedural commonsense, which includes prescriptive knowledge, e.g., 'one needs an oven before baking cakes', 'the electricity should be off while the switch is being repaired' (Hobbs et al., 1987); sentiment knowledge, e.g., 'rushing to the hospital makes people worried', 'being in vacation makes people relaxed'; and metaphorical knowledge (e.g., 'time flies', 'raining cats and dogs'). We believe that it is important to identifiy the most appropriate commonsense knowledge type required for specific tasks, in order to get better downstream performance. Once the knowledge type is identified, we can then select the appropriate knowledge-base(s), and the suitable neural integration mechanisms (e.g., attention-based injection, pre-training, or auxiliary training objectives).</p>
<p>Accordingly, in this work we conduct a comparison study of different knowledge bases and knowledge integration methods, and we evaluate model performance on two multiple-choice QA datasets that explicitly require commonsense reasoning. In particular, we used ConceptNet (Speer et al., 2016) and the recently-introduced ATOMIC (Sap et al., 2019) knowledge resources, integrating them with the Option Comparison Network model (OCN; Ran et al. (2019)), a recent state-of-the-art model for multiple choice QA tasks. We evalutate our models on the DREAM (Sun et al., 2019) and CommonsenseQA (Talmor et al., 2019) datasets. An example from DREAM that requires commonsense is shown in Table 1, and an example from CommonsenseQA is shown in Table 2. Our experimental results and analysis suggest that attention-based injection is preferable for knowledge integration and that the degree of domain overlap, between knowledge-base and dataset, is vital to model success. ${ }^{1}$</p>
<h2>Dialogue:</h2>
<p>M: I hear you drive a long way to work every day.
W: Oh, yes. it's about sixty miles. but it doesn't seem that far, the road is not bad, and there's not much traffic. Question:
How does the woman feel about driving to work? Answer choices:
A. She doesn't mind it as the road conditions are good.*
B. She is unhappy to drive such a long way everyday.
C. She is tired of driving in heavy traffic.</p>
<p>Table 1: An example from the DREAM dataset; the asterisk (*) denotes the correct answer.</p>
<h2>Question:</h2>
<p>A revolving door is convenient for two direction travel, but it also serves as a security measure at a what? Answer choices:
A. Bank*; B. Library; C. Department Store;
D. Mall; E. New York</p>
<p>Table 2: An example from the CommonsenseQA dataset; the asterisk (*) denotes the correct answer.</p>
<h2>2 Related Work</h2>
<p>It has been recognized that many recent QA tasks require external knowledge or commonsense to solve, and numerous efforts have been made in injecting commonsense in neural models. Bauer</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al. (2018) introduced a pipeline for extracting grounded multi-hop commonsense relation paths from ConceptNet and proposed to inject commonsense knowledge into neural models' intermediate representations, using attention. Similarly, Mihaylov and Frank (2018) also proposed to extract relevant knowledge triples from ConceptNet and use Key-Value Retrieval (Miller et al., 2016) to gather information from knowledge to enhance the neural representation. Zhong et al. (2018) proposed to pre-train a scoring function using knowledge triples from ConceptNet, to model the direct and indirect relation between concepts. This scoring function was then fused with QA models to make the final prediction. Pan et al. (2019a) introduced an entity discovery and linking system to identify the most salient entities in the question and answer-options. Wikipedia abstracts of these entities are then extracted and appended to the reference documents to provide additional information. Weissenborn et al. (2018) proposed a strategy of dynamically refining word embeddings by reading input text as well as external knowledge, such as ConceptNet and Wikipedia abstracts. More recently, Lin et al. (2019) proposed to extract subgraphs from ConceptNet and embed the knowledge using Graph Convolutional Networks (Kipf and Welling, 2016). Then the knowledge representation is integrated with word representation through an LSTM layer and hierarchical attention mechnism. Lv et al. (2019) introduced graphbased reasoning modules that takes both ConceptNet knowledge triples and Wikipedia text as inputs to refine word representations from a pretrained language model and make predictions.</p>
<p>Commonsense knowledge integration has also received a lot of attention on many other tasks. Tandon et al. (2018) proposed to use commonsense knowledge as hard/soft constraints to bias the neural model's prediction on a procedural text comprehension task. Ma et al. (2018) proposed to used embedded affective commonsense knowledge inside LSTM cell to control the information flow in each gate for sentiment analysis task. Li and Srikumar (2019) presented a framework to convert declarative knowlegde into first-order logic that enhance neural networks' training and prediction. Peters et al. (2019) and Levine et al. (2019) both tried to injecting knowlegde into language models by pretraining on knowledge bases.</p>
<p>Previous works only focus on using external</p>
<p>knowledge sources to improve model performance on certain tasks, disregarding the type of commonsense knowledge and how the domain of the knowledge resource affects results on downstream tasks. In this paper, we examine the roles of knowledge-base domain and specific integration mechanisms on model performance.</p>
<h2>3 Approach Overview</h2>
<p>In this section, we describe the model architecture used in our experiments. Next, we introduce two popular knowledge resources, we define our knowledge-extraction method, then we illustrate various neural knowledge-integration mechanisms.</p>
<h3>3.1 Model architecture</h3>
<p>The BERT model (Devlin et al., 2019) has been applied to numerous QA tasks and has achieved very promising performance, particularly on the DREAM and CommonsenseQA datasets. When utilizing BERT on multiple-choice QA tasks, the standard approach is to concatenate the dialogue context and the question with each answer-option, in order to generate a list of tokens which is then fed into BERT encoder; a linear layer is added on top, in order to predict the answer. One aspect of this strategy is that each answeroption is encoded independently: from a cognitive perspective, this aspect contradicts how humans typically solve multiple-choice QA tasks, namely by weighing each option to find correlations within them, in addition to correlations with respect to the question. To address this issue, Ran et al. (2019) introduced the Option Comparison Network (OCN) that explicitly models pairwise answer-option interactions, making OCN bettersuited for multiple-choice QA task structures. We re-implemented OCN while keeping BERT as its upstream encoder. ${ }^{2}$ Specifically, given a dialogue $D$, a question $Q$, and an answer-option $O_{k}$, we concatenate them and encode with BERT to get hidden representation $T_{e n c} \in \mathbb{R}^{n \times d}$ :</p>
<p>$$
T_{e n c}=\operatorname{BERT}\left(D ; Q ; O_{k}\right)
$$</p>
<p>Where $d$ is the size of BERT's hidden representation and $n$ is the total number of words. Next,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the dialogue encoding $D_{\text {enc }} \in \mathbb{R}^{n_{d} \times d}$, question encoding $Q_{\text {enc }} \in \mathbb{R}^{n_{q} \times d}$, and answer-option encoding $O_{k, \text { enc }} \in \mathbb{R}^{n_{o} \times d}$ are separated from $T_{\text {enc }}$. Here, option-encoding consists both of question and option, i.e. $Q_{\text {enc }} \subseteq O_{k, \text { enc }}$ and $n_{d}+n_{o}=n$, as suggested by Ran et al. (2019). Given a set of options $O_{k}(k=1,2, \ldots)$, these options are compared, pairwise, using standard tri-linear attention (Seo et al., 2016):</p>
<p>$$
\operatorname{Att}(u, v)=W_{1} \cdot u+W_{2} \cdot v+\left(W_{3} \circ v\right) \cdot u
$$</p>
<p>Where, $W_{1}, W_{2}, W_{3} \in \mathbb{R}^{d}$ are trainable weights and $u \in \mathbb{R}^{x \times d}, v \in \mathbb{R}^{y \times d}$ are input matrices; $x$ and $y$ here are generic placeholder for input lengths; matrix multiplication and elementwise multiplication are denoted by $(\cdot)$ and $(\circ)$, respectively. Next, we gather information from all other options, to form a new option representation $O_{k, \text { new }} \in \mathbb{R}^{n_{o} \times d}$. Formally, given option $O_{k, \text { enc }}$ and another option $O_{l, \text { enc }} \in \mathbb{R}^{n_{l} \times d}, O_{k, \text { new }}$ is computed as follows:</p>
<p>$$
\begin{aligned}
O_{k}^{l} &amp; =O_{l, e n c} \cdot \operatorname{Att}\left(O_{l, e n c}, O_{k, e n c}\right) \
\widetilde{O}<em c="c" e="e" k_="k," n="n">{k}^{l} &amp; =\left[O</em>\right] \
O_{k, \text { new }} &amp; =\tanh \left(W_{c} \cdot\left[O_{k, e n c} ;\left{\widetilde{O}}-O_{k}^{l} ; O_{k, e n c} \circ O_{k}^{l<em _neq="\neq" k="k" l="l">{k}^{l}\right}</em>\right]\right)
\end{aligned}
$$</p>
<p>Where, $W_{c} \in \mathbb{R}^{(d+2 d(|O|-1)) \times d},|O|$ denotes total number of options and $n_{l}$ denotes the number of words in the compared option. Then, a gating mechanism is used to fuse the option-wise correlation information $O_{k, \text { new }}$ with the current optionencoding $O_{k, e n c}$. Gating values are computed as:</p>
<p>$$
\begin{aligned}
G &amp; =\operatorname{sigmoid}\left(W_{g}\left[O_{k, e n c} ; O_{k, \text { new }} ; \widetilde{Q}\right]\right) \
\widetilde{Q} &amp; =Q_{\text {enc }} \cdot \operatorname{softmax}\left(Q_{\text {enc }} \cdot V_{a}\right)^{T} \
O_{f u s e} &amp; =G \circ O_{k, e n c}+(1-G) \circ O_{k, \text { new }}
\end{aligned}
$$</p>
<p>Here, $W_{g} \in \mathbb{R}^{3 d \times d}$ and $V_{a} \in \mathbb{R}^{d \times 1}$. Co-attention (Xiong et al., 2016) is applied to re-read the dialogue, given the fused option-correlation features:</p>
<p>$$
\begin{aligned}
A_{d o} &amp; =\operatorname{Att}\left(D_{\text {enc }}, O_{f u s e}\right) \
A_{o d} &amp; =\operatorname{Att}\left(O_{f u s e}, D_{e n c}\right) \
O_{d}=A_{o d} &amp; \cdot\left[D_{e n c} ; A_{d o} \cdot O_{f u s e}\right] \
\widetilde{O}<em p="p">{d} &amp; =\operatorname{ReLU}\left(W</em>\right]\right)\right)
\end{aligned}
$$}\left(\left[O_{d} ; O_{f u s e</p>
<p>Here, $W_{p} \in \mathbb{R}^{3 d \times d}$. Finally, self-attention (Wang et al., 2017) is used to compute final option repre-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Option Comparison Network with Knowledge Injection
sentation $\overrightarrow{O_{f}} \in \mathbb{R}^{n_{o} \times d}$ :</p>
<p>$$
\begin{gathered}
O_{s}=\overrightarrow{O_{d}} \cdot \operatorname{Att}\left(\overrightarrow{O_{d}}, \overrightarrow{O_{d}}\right) \
O_{f}=\left[\overrightarrow{O_{d}} ; O_{s}, \overrightarrow{O_{d}}-O_{s} ; \overrightarrow{O_{d}} \circ O_{s}\right] \
\overrightarrow{O_{f}}=\operatorname{ReLU}\left(W_{f} \cdot O_{f}\right)
\end{gathered}
$$</p>
<p>Unlike the vanilla BERT model, which takes the first token to predict the answer, max-pooling is applied on the sequence dimension of $\overrightarrow{O_{f}} \in$ $\mathbb{R}^{n_{o} \times d}$, in order to generate the final prediction.</p>
<h3>3.2 Knowledge bases</h3>
<p>The first knowledge-base we consider for our experiments is ConceptNet (Speer et al., 2016). ConceptNet contains over 21 million edges and 8 million nodes ( 1.5 million nodes in the partition for the English vocabulary), generating triples of the form $(C 1, r, C 2)$ : the natural-language concepts $C 1$ and $C 2$ are associated by commonsense relation $r$, e.g., (dinner, AtLocation, restaurant). Thanks to its coverage, ConceptNet is one of the most popular semantic networks for commonsense. ATOMIC (Sap et al., 2019) is a new knowledge-base that focuses on procedural knowledge. Triples are of the form (Event, $r,{$ Effect $\mid$ Persona $\mid$ Mental-state $}$ ), where head and tail are short sentences or verb phrases and $r$ represents an if-then relation type. An example would be: ( $X$ compliments $Y$, xIntent, $X$ wants to be nice). Since both DREAM and CommonsenseQA datasets are open-domain and require general commonsense, we think these knowledge-bases are most appropriate for our investigation.</p>
<h3>3.3 Knowledge elicitation</h3>
<p>ConceptNet. For the DREAM dataset, we find ConceptNet relations that connect dialogues and questions to the answer-options. The intuition is that these relation paths would provide explicit evidence that would help the model find the answer. Formally, given a dialogue $D$, a question $Q$, and an answer-option $O$, we find all ConceptNet relations ( $C 1, r, C 2$ ), such that $C 1 \in(D+Q)$ and $C 2 \in O$, or vice versa. This rule works well for single-word concepts. However, a large number of concepts in ConceptNet are actually phrases, and finding exactly matching phrases in $D / Q / O$ is much harder. To fully utilize phrase-based ConceptNet relations, we relaxed the exact-match constraint to the following:</p>
<p>$$
\frac{# \text { words in } \mathrm{C} \cap \mathrm{~S}}{# \text { words in } \mathrm{C}}&gt;0.5
$$</p>
<p>Here, $S$ represents $D / Q / O$, depending on which sequence we try to match the concept $C$ to. Additionally, when the part-of-speech (POS) tag for a concept is available, we make sure it matches the POS tag of the corresponding word in $D / Q / O$. For CommonsenseQA, we use the same procedure to find ConceptNet relations for each answeroption, except that only $Q$ is present and used. Table 3 shows the extracted ConceptNet triples for the CommonsenseQA example in Table 2. It is worth noting that we are able to extract the original ConceptNet sub-graph that was used to create the question, along with some extra triples. Although not perfect, the bold ConceptNet triple does provide some clue that could help the model resolve the correct answer.</p>
<table>
<thead>
<tr>
<th>Options</th>
<th>Extracted ConceptNet triples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bank</td>
<td>(revolving door AtLocation bank) (bank RelatedTo security)</td>
</tr>
<tr>
<td>Library</td>
<td>(revolving door AtLocation library)</td>
</tr>
<tr>
<td>Department Store</td>
<td>(revolving door AtLocation store) (security IsA department)</td>
</tr>
<tr>
<td>Mall</td>
<td>(revolving door AtLocation mall)</td>
</tr>
<tr>
<td>New York</td>
<td>(revolving door AtLocation New York)</td>
</tr>
</tbody>
</table>
<p>Table 3: Extracted ConceptNet relations for sample shown in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input sentence</th>
<th style="text-align: left;">Generated ATOMIC relations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Utterance 1</td>
<td style="text-align: left;">(xAttr dedicated) (xWant to get to work)</td>
</tr>
<tr>
<td style="text-align: center;">Utterance 2</td>
<td style="text-align: left;">(xAttr far) (xReact happy) (xWant to get to their destination)</td>
</tr>
<tr>
<td style="text-align: center;">Option A</td>
<td style="text-align: left;">(xAttr calm) (xWant to avoid the road)</td>
</tr>
<tr>
<td style="text-align: center;">Option B</td>
<td style="text-align: left;">(xAttr careless) (xReact annoyed) (xEffect get tired)</td>
</tr>
<tr>
<td style="text-align: center;">Option C</td>
<td style="text-align: left;">(xAttr frustrated) (xEffect get tired) (xWant to get out of car)</td>
</tr>
</tbody>
</table>
<p>Table 4: Sample generated ATOMIC relations for sample shown in Table 1.</p>
<p>ATOMIC. We observe that many questions in DREAM inquire about agent's opinion and feeling. Superficially, this particular question type seems well-suited for ATOMIC, whose focus is on folk psychology and related general implications; we could frame our goal as evaluating whether ATOMIC can provide relevant knowledge to help answer these questions. However, one challenge to this strategy is that heads and tails of knowledge triples in ATOMIC are short sentences or verb phrases, while rare words and person-references are reduced to blanks and PersonX/PersonY, respectively. This calls for a new matching procedure, different from the ConceptNet extraction strategy, for eliciting ATOMIC-specific relations: we rely on the recently-published COMET model (Bosselut et al., 2019) to generate new ATOMIC relations, with intermediate phrasal resolutions. In particular, we first segmented all dialogues, questions, and answer-options into sentences. We further segment long sentences into sub-sentences, using commas as seperators. Because only verb-phrases satisfy the definition of an "event" in ATOMIC (i.e., relations are only invoked by verbs), we remove all sentences/subsentences that do not contain any verb. Next, we use a pre-trained COMET model (Bosselut et al., 2019) to generate all possible ATOMIC relations, for all candidate sentences/sub-sentences and we use greedy-decoding to take the 1-best sequences. Table 4 shows the sample ATOMIC relations, generated using the DREAM example in Table 1. It is interesting to note that the reaction for the woman agent (second utterance) is identified as happy, since she said that 'the road is not bad.' If we compare the identified attributes for answer-options,
the one from correct answer seems to be sematically closer than the other two.</p>
<h3>3.4 Knowledge injection</h3>
<p>Given previously extracted/generated knowledge triples, we need to integrate them with the OCN model. Inspired by Bauer et al. (2018), we propose to use attention-based injection. For ConceptNet knowledge triples, we first convert conceptrelation tokens into regular tokens, in order to generate a pseudo-sentence. For example, "(book, AtLocation, library)" would be converted to "book at location library." Next, we use the BERT embedding layer to generate an embedding of this pseudo-sentence, with $C$ denoting a ConceptNet relation:</p>
<p>$$
H_{C}=\operatorname{BiLSTM}(C)
$$</p>
<p>If we let $H_{C} \in \mathbb{R}^{1 \times 2 l}$ be the concatenation of the final hidden states and $l$ be the number of hidden units in the LSTM layer, then $m$ ConceptNet relations would yield the commonsense knowledge matrix $H_{M} \in \mathbb{R}^{m \times 2 l}$. We adopt the attention mechanism used in QAnet (Yu et al., 2018) to model the interaction between $H_{M}$ and the BERT encoding output $T_{\text {enc }}$ (from Equation 1):</p>
<p>$$
\begin{aligned}
\widetilde{H}<em M="M">{M} &amp; =H</em> \
\mathcal{S} &amp; =\operatorname{Att}\left(H_{M}, T_{\text {enc }}\right) \
A_{m} &amp; =\operatorname{softmax}(\mathcal{S}) \cdot \widetilde{H}} \cdot W_{\text {proj }<em t="t">{M} \
A</em> \
T_{C}=\left[T_{\text {enc }} ; A_{m} ; T_{\text {enc }} \circ A_{m} ; T_{\text {enc }} \circ A_{t}\right] \
T_{\text {out }} &amp; =\operatorname{ReLU}\left(T_{C} \cdot W_{a}\right)
\end{aligned}
$$}=\operatorname{softmax}(\mathcal{S}) \cdot \operatorname{softmax}\left(\mathcal{S}^{T}\right) \cdot T_{\text {enc }</p>
<p>Specifically, $H_{M}$ is first projected into the same dimension as $T_{\text {enc }}$, using $W_{\text {proj }} \in \mathbb{R}^{2 l \times d}$. Then,</p>
<p>the similarty matrix $\mathcal{S} \in \mathbb{R}^{n \times m}$ is computed using tri-linear attention, as in Equation 2. We then use $\mathcal{S}$ to compute text-to-knowledge attention $A_{m} \in \mathbb{R}^{n \times d}$ and knowledge-to-text attention $A_{t} \in \mathbb{R}^{n \times d}$. Finally, the knowledge-aware textual representation $T_{\text {out }} \in \mathbb{R}^{n \times d}$ is computed, where $W_{a} \in \mathbb{R}^{4 d \times d} . T_{\text {out }}$ is fed to subsequent layers (in place of $T_{\text {enc }}$ ), in order to generate the prediction. The model structure with knowledge-injection is summarized in Figure 1.</p>
<p>For ATOMIC knowledge triples, the injection method is slightly different. Because heads of these knowledge triples are sentences/utterances and the tails contain attributes of the persons (i.e., subject and object of the sentence), it is not possible to directly inject the knowledge triples, asis. We replace the heads of the ATOMIC knowledge triples with the corresponding speaker for dialogues and leave as blank for the answeroptions. Next, we convert the special relation tokens into regular tokens, e.g., "xIntent" $\Rightarrow$ "intent" and "oEffect" $\Rightarrow$ "others effect", to make pseudosentences. As a result, an ATOMIC relation "(the road is not bad, xReact, happy)" would be converted to "(W, react, happy)." Moreover, as the ATOMIC knowledge triples are associated with dialogues and answer-options, independently, we inject option relations into $O_{\text {enc }} \in \mathbb{R}^{n_{o} \times d}$ and dialogue relations into $D_{\text {enc }}$, respectively, using the injection method described above.</p>
<h3>3.5 Knowledge pre-training</h3>
<p>Pre-training large-capacity models (e.g., BERT, GPT (Radford et al., 2019), XLNet (Yang et al., 2019)) on large corpora, then fine-tuning on more domain-specific information, has led to performance improvements on various tasks. Inspired by this, our goal in this section is to observe the effect of pre-training BERT on commonsense knowledge and refining the model on task-specific content from our DREAM and CommonsenseQA corpora. Essentially, we would like to test if pretraining on our external knowledge resources can help the model acquire commonsense. For the ConceptNet pre-training procedure, pre-training BERT on pseudo-sentences formulated from ConceptNet knowledge triples does not provide much gain on performance. Instead, we trained BERT on the Open Mind Common Sense (OMCS) corpus (Singh et al., 2002), the original corpus that was used to create ConceptNet. We extracted about 930K English sentences from OMCS and randomly masked out $15 \%$ of the tokens; we then fine-tuned BERT, using a masked language model objective. Then we load this fine-tuned model into OCN and trained on DREAM and CommonsenseQA tasks. As for pre-training on ATOMIC, we again use COMET to convert ATOMIC knowledge triples into sentences; we created special tokens for 9 types of relations as well as blanks. Next, we randomly masked out $15 \%$ of the tokens, only masking out tail-tokens. We use the same OMCS pre-training procedure.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Dev Acc</th>
<th style="text-align: center;">Test Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BERT Large(*)</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">66.8</td>
</tr>
<tr>
<td style="text-align: center;">XLNet(*)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{7 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: center;">OCN</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: center;">OCN + CN injection</td>
<td style="text-align: center;">$\mathbf{7 0 . 5}$</td>
<td style="text-align: center;">69.6</td>
</tr>
<tr>
<td style="text-align: center;">OCN + AT injection</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">$\mathbf{7 0 . 1}$</td>
</tr>
<tr>
<td style="text-align: center;">OCN + OMCS pre-train</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">62.6</td>
</tr>
<tr>
<td style="text-align: center;">OCN + ATOMIC pre-train</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">58.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on DREAM; the asterisk (*) denotes results taken from leaderboard.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Dev Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BERT + OMCS pre-train(*)</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa + CSPT(*)</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">OCN</td>
<td style="text-align: center;">64.1</td>
</tr>
<tr>
<td style="text-align: center;">OCN + CN injection</td>
<td style="text-align: center;">67.3</td>
</tr>
<tr>
<td style="text-align: center;">OCN + OMCS pre-train</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">OCN + ATOMIC pre-train</td>
<td style="text-align: center;">61.2</td>
</tr>
<tr>
<td style="text-align: center;">OCN + OMCS pre-train + CN inject</td>
<td style="text-align: center;">$\mathbf{6 9 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on CommonsenseQA; the asterisk (*) denotes results taken from leaderboard.</p>
<h2>4 Experiments</h2>
<h3>4.1 Datasets</h3>
<p>We choose to evaluate our hypotheses using the DREAM and CommonsenseQA datasets, because some / all questions require commonsense reasoning and because there remains a large gap between state-of-the-art models and human performance.</p>
<p>DREAM is a dialogue-based multiple-choice QA dataset, introduced by Sun et al. (2019). It was collected from English-as-a-foreign-language examinations, designed by human experts. The dataset contains 10,197 questions for 6,444 dialogues in total, and each question is associated with 3 answer-options. The authors point out that $34 \%$ of questions require commonsense knowledge to answer, which includes social implication, speaker's intention, or general world knowledge.</p>
<p>CommonsenseQA is a multiple-choice QA dataset that specifically measure commonsense reasoning (Talmor et al., 2019). This dataset is constructed based on ConceptNet (Speer et al.,</p>
<p>2016). Specifically, a source concept is first extracted from ConceptNet, along with 3 target concepts that are connected to the source concept, i.e., a sub-graph. Crowd-workers are then asked to generate questions, using the source concept, such that only one of the target concepts can correctly answer the question. Additionally, 2 more distractor concepts are selected by crowd-workers so that each question is associated with 5 answeroptions. In total, the dataset contains 12,247 questions. For CommonsenseQA, we evaluate models on the development-set only, since test-set answers are not publicly available.</p>
<h3>4.2 Training details</h3>
<p>For ease of comparison, we borrow hyperparameter settings from Pan et al. (2019b); we used the BERT Whole-Word Masking Uncased model (Devlin et al., 2018) for all experiments. For DREAM experiments, we used a max sequencelength of 512 , batch-size of 24 , learning rate of $1 e^{-5}$, and we trained the model for 16 epochs. For CommonsenseQA, we used a max sequence length of 60 , batch-size of 32 , learning rate of $1 e^{-5}$, and trained for 8 epochs. For pre-training on OMCS, we used max sequence length of 35 , batch-size of 32 , learning rate of $3 e^{-5}$, and trained for 3 epochs. For pre-training on ATOMIC, the max sequence length is changed to 45 , other hyperparameters remain the same, and we only use the ATOMIC training set. When using OCN on CommonsenseQA, since there is no dialogue, we compute co-attention with $Q_{\text {enc }}$, in place of $D_{\text {enc }}$, in order to keep the model structure consistent.</p>
<h3>4.3 Results</h3>
<p>DREAM results are shown in Table 5, and CommonsenseQA results are shown in Table 6. For all of our experiments, we run 3 trials with different random seeds and we report average scores in the tables. Evaluated on DREAM, our OCN model got a significant performance boost $(+3.0 \%)$, compared to BERTlarge from previous work. We think the reasons are that OCN is better-suited for the task and that we used BERT Whole-Word Masking Uncased model. OCN with ConceptNet knowledge-injection achieves slightly better results on the development-set, while ATOMIC knowledge-injection helps achieve a small improvement on the test-set. However, we recognize that these improvements are very limited; to our
surprise, OCN pre-trained on OMCS or ATOMIC got significantly lower performance.</p>
<p>As for results on CommonsenseQA, ConceptNet knowledge-injection provides a significant performance boost $(+2.8 \%)$, compared to the OCN baseline, suggesting that explicit links from question to answer-options help the model find the correct answer. Pre-training on OMCS also provides a small performance boost to the OCN baseline. Since both ConceptNet knowledge-injection and OMCS pre-training are helpful, we combine both approaches with OCN and we are able to achieve further improvement ( $+4.9 \%$ ). Finally, similar to the results on DREAM, OCN pre-trained on ATOMIC yields a siginificant performance drop.</p>
<h2>5 Error Analysis</h2>
<p>To better understand when a model performs better or worse with knowledge-integration, we analyzed model predictions. DREAM dataset provides annotations for about 1000 questions: 500 questions in the development-set and 500 in the testset. Specifically, questions are manually classified into 5 categories: Matching, Summary, Logic inference, Commonsense inference, and Arithmetic inference; and each question can be classified under multiple categories. We refer readers to Sun et al. (2019) for additional category information. We extracted model predictions for these annotated questions in test-set and grouped them by types. The accuracies for each questiongroup are shown in Table 7. Note that we omitted 2 categories that have less than 10 questions. For the ConceptNet and the ATOMIC knowledgeinjection models, we can see that they did better on questions that involve commonsense (last 3 columns in the table), and the performance on other types are about the same or slightly worse, compared to baseline OCN. As for models pretrained on OMCS corpus or ATOMIC knowledgebase, we already saw that these model performances drop, compared to the baseline. When we look at the performance difference in each question type, it is clear that some categories account for the performance drop more than others. For example, for both the OMCS pre-trained model and the ATOMIC pre-trained model, performance drops significantly for Matching questions, in particular. On the other hand, for questions that require both commonsense inference and summarization, both models' performances only dropped</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">$\mathbf{M}(\mathbf{5 4})$</th>
<th style="text-align: center;">$\mathbf{S}(\mathbf{1 5})$</th>
<th style="text-align: center;">$\mathbf{A}+\mathbf{L}(\mathbf{1 1})$</th>
<th style="text-align: center;">$\mathbf{L}(\mathbf{2 2 8})$</th>
<th style="text-align: center;">$\mathbf{C}+\mathbf{L}(\mathbf{1 2 2})$</th>
<th style="text-align: center;">$\mathbf{C}(\mathbf{1 4})$</th>
<th style="text-align: center;">$\mathbf{C}+\mathbf{S}(\mathbf{6 0})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OCN</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">OCN + CN injection</td>
<td style="text-align: center;">$83.3(-5.6)$</td>
<td style="text-align: center;">$86.7(+0.0)$</td>
<td style="text-align: center;">$18.2(-9.2)$</td>
<td style="text-align: center;">$76.8(+0.9)$</td>
<td style="text-align: center;">$59.8(-0.9)$</td>
<td style="text-align: center;">$64.3(-7.1)$</td>
<td style="text-align: center;">$78.3(\mathbf{+ 8 . 3})$</td>
</tr>
<tr>
<td style="text-align: center;">OCN + AT injection</td>
<td style="text-align: center;">$88.9(+0.0)$</td>
<td style="text-align: center;">$80.0(-6.7)$</td>
<td style="text-align: center;">$27.3(+0.0)$</td>
<td style="text-align: center;">$75.9(+0.0)$</td>
<td style="text-align: center;">$66.4(\mathbf{+ 5 . 7 )}$</td>
<td style="text-align: center;">$71.4(+0.0)$</td>
<td style="text-align: center;">$75(\mathbf{+ 5 . 0})$</td>
</tr>
<tr>
<td style="text-align: center;">OCN + OMCS pre-train</td>
<td style="text-align: center;">$70.4(-18.5)$</td>
<td style="text-align: center;">$73.3(-13.4)$</td>
<td style="text-align: center;">$45.4(+18.1)$</td>
<td style="text-align: center;">$69.7(-6.2)$</td>
<td style="text-align: center;">$48.4(-12.3)$</td>
<td style="text-align: center;">$57.1(-14.3)$</td>
<td style="text-align: center;">$68.3(\mathbf{- 1 . 7})$</td>
</tr>
<tr>
<td style="text-align: center;">OCN + ATOMIC pre-train</td>
<td style="text-align: center;">$66.6(-22.3)$</td>
<td style="text-align: center;">$86.7(+0.0)$</td>
<td style="text-align: center;">$18.2(-9.2)$</td>
<td style="text-align: center;">$64.0(-11.9)$</td>
<td style="text-align: center;">$51.6(-9.1)$</td>
<td style="text-align: center;">$42.9(-28.5)$</td>
<td style="text-align: center;">$70.0(\mathbf{+ 0 . 0})$</td>
</tr>
</tbody>
</table>
<p>Table 7: Accuracies for each DREAM question type: M means Matching, S means Summary, L means Logic inference, $\mathbf{C}$ means Commonsense inference, and $\mathbf{A}$ means Arithmetic inference. Numbers beside types denote the number of questions of that type.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">AtLoc.(596)</th>
<th style="text-align: center;">Cau.(194)</th>
<th style="text-align: center;">Cap.(109)</th>
<th style="text-align: center;">Ant.(92)</th>
<th style="text-align: center;">H.Pre.(46)</th>
<th style="text-align: center;">H.Sub.(39)</th>
<th style="text-align: center;">C.Des.(28)</th>
<th style="text-align: center;">Des.(27)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OCN</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">66.7</td>
</tr>
<tr>
<td style="text-align: center;">+CN inj.</td>
<td style="text-align: center;">$67.4(+2.5)$</td>
<td style="text-align: center;">$70.6(+4.1)$</td>
<td style="text-align: center;">$66.1(+1.0)$</td>
<td style="text-align: center;">$60.9(+5.5)$</td>
<td style="text-align: center;">$73.9(+4.3)$</td>
<td style="text-align: center;">$66.7(+2.6)$</td>
<td style="text-align: center;">$64.3(+7.2)$</td>
<td style="text-align: center;">$77.8(+11.1)$</td>
</tr>
<tr>
<td style="text-align: center;">+OMCS</td>
<td style="text-align: center;">$68.8(+3.9)$</td>
<td style="text-align: center;">$63.9(-2.6)$</td>
<td style="text-align: center;">$62.4(-2.7)$</td>
<td style="text-align: center;">$60.9(+5.5)$</td>
<td style="text-align: center;">$71.7(+2.1)$</td>
<td style="text-align: center;">$59.0(-5.1)$</td>
<td style="text-align: center;">$64.3(+7.2)$</td>
<td style="text-align: center;">$74.1(+7.4)$</td>
</tr>
<tr>
<td style="text-align: center;">+ATOMIC</td>
<td style="text-align: center;">$62.8(-2.1)$</td>
<td style="text-align: center;">$66.0(-0.5)$</td>
<td style="text-align: center;">$60.6(-4.5)$</td>
<td style="text-align: center;">$52.2(-3.2)$</td>
<td style="text-align: center;">$63.0(-6.6)$</td>
<td style="text-align: center;">$56.4(-7.7)$</td>
<td style="text-align: center;">$60.7(+3.6)$</td>
<td style="text-align: center;">$74.1(+7.4)$</td>
</tr>
<tr>
<td style="text-align: center;">+OMCS+CN</td>
<td style="text-align: center;">$71.6(+6.7)$</td>
<td style="text-align: center;">$71.6(+5.1)$</td>
<td style="text-align: center;">$64.2(+0.9)$</td>
<td style="text-align: center;">$59.8(+4.4)$</td>
<td style="text-align: center;">$69.6(+0.0)$</td>
<td style="text-align: center;">$69.2(+5.1)$</td>
<td style="text-align: center;">$75.0(+17.9)$</td>
<td style="text-align: center;">$70.4(+3.7)$</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracies for each CommonsenseQA question type: AtLoc. means AtLocation, Cau. means Causes, Cap. means CapableOf, Ant. means Antonym, H.Pre. means HasPrerequiste, H.Sub means HasSubevent, C.Des. means CausesDesire, and Des. means Desires. Numbers beside types denote the number of questions of that type.
slightly or did not change. Based on these results, we infer that commonsense knowledge-injection with attention is making an impact on models' weight distributions. The model is able to do better on questions that require commonsense but is losing performance on other types, suggesting a direction for future research in developing more robust (e.g., conditional) injection methods. Moreover, pre-training on knowledge-bases seems to have a larger impact on models' weight distributions, resulting in inferior performance. This weight distribution shift also favors of commonsense, as we see that commonsense types are not affected as much as other types. We also conducted similar analysis for CommonsenseQA. Since all questions in CommonsenseQA require commonsense reasoning, we classify questions based on the ConceptNet relation between the question concept and correct answer concept. The intuition is that the model needs to capture this relation in order to answer the question. The accuracies for each question type are shown in Table 8. Note that we have omitted question types that have less than 25 questions. We can see that with ConceptNet relation-injection, all question types got performance boosts, for both OCN model and OCN pre-trained on OMCS, suggesting that knowledge is indeed helpful for the task. In the case of OCN pre-trained on ATOMIC, although the overall performance is much lower than OCN baseline, it is interesting to see that performance for the "Causes" type is not significantly affected. Moreover, performance for "CausesDesire" and "Desires" types actually got much better. As noted by (Sap et al., 2019), "Causes" in ConceptNet is similar to "Effects" and "Reac-
tions" in ATOMIC; and "CausesDesire" in ConceptNet is similar to "Wants" in ATOMIC. This result also correlates with our findings from our analysis on DREAM, wherein we found that models with knowledge pre-training perform better on questions that fit knowledge domain but perform worse on others. In this case, pre-training on ATOMIC helps the model do better on questions that are similar to ATOMIC relations, even though overall performance is inferior. Finally, we noticed that questions of type "Antonym" appear to be the hardest ones. Many questions that fall into this category contain negations, and we hypothesize that the models still lack the ability to reason over negation sentences, suggesting another direction for future improvement.</p>
<h2>6 Discussion</h2>
<p>Based on our experimental results and error analysis, we see that external knowledge is only helpful when there is alignment between questions and knowledge-base types. Thus, it is crucial to identify the question type and apply the best-suited knowledge. In terms of knowledge-integration methods, attention-based injection seems to be the better choice for pre-trained language models such as BERT. Even when alignment between knowledge-base and dataset is sub-optimal, the performance would not degrade. On the other hand, pre-training on knowledge-bases would shift the language model's weight distribution toward its own domain, greatly. If the task domain does not fit knowledge-base well, model performance is likely to drop. When the domain of the knowledge-base aligns with that of the dataset perfectly, both knowledge-integration methods bring</p>
<p>performance boosts and a combination of them could bring further gain.</p>
<h2>7 Future Work</h2>
<p>We have presented a survey on two popular knowledge bases (ConceptNet and ATOMIC) and recent knowledge-integration methods (attention and pre-training), on commonsense QA tasks. Evaluation on two QA datasets suggests that alignment between knowledge-bases and datasets plays a crucial role in knowledge-integration. We believe it is worth conducting a more comprehensive study of datasets and knowledge-bases and putting more effort towards defining an auxiliary learning objective, in a constrained-optimization (i.e., multi-task learning) framework, that identifies the type of knowledge required, based on data characteristics. In parallel, we are also interested in building a global commonsense knowledge base by aggregating ConceptNet, ATOMIC, and potentially other resources like FrameNet (Baker et al., 1998) and MetaNet (Dodge et al., 2015), on the basis of a shared-reference ontology (following the approaches described in (Gangemi et al., 2010) and (Scheffczyk et al., 2010)): the goal would be to assess whether injecting knowledge structures from a semantically-cohesive lexical knowledge base of commonsense guarantees stable model accuracy across datasets.</p>
<h2>References</h2>
<p>Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pages 86-90. Association for Computational Linguistics.</p>
<p>Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question answering tasks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4220-4230, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47624779, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ernest Davis. 2014. Representations of commonsense knowledge. Morgan Kaufmann.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ellen Dodge, Jisup Hong, and Elise Stickles. 2015. Metanet: Deep semantic automatic metaphor analysis. In Proceedings of the Third Workshop on Metaphor in NLP, pages 40-49.</p>
<p>Aldo Gangemi, Nicola Guarino, Claudio Masolo, and Alessandro Oltramari. 2010. Interfacing wordnet with dolce: towards ontowordnet. Ontology and the Lexicon: A Natural Language Processing Perspective, pages 36-52.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1693-1701. Curran Associates, Inc.</p>
<p>Jerry R Hobbs, William Croft, Todd Davies, Douglas Edwards, and Kenneth Laws. 1987. Commonsense metaphysics and lexical semantics. Computational linguistics, 13(3-4):241-250.</p>
<p>Thomas N. Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. CoRR, abs/1609.02907.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: large-scale reading comprehension dataset from examinations. CoRR, abs/1704.04683.</p>
<p>Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham. 2019. Sensebert: Driving some sense into bert. ArXiv, abs/1908.05646.</p>
<p>Tao Li and Vivek Srikumar. 2019. Augmenting neural networks with first-order logic. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 292-302, Florence, Italy. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. Kagnet: Knowledge-aware graph networks for commonsense reasoning. ArXiv, abs/1909.02151.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, and Songlin Hu. 2019. Graphbased reasoning over heterogeneous external knowledge for commonsense question answering. ArXiv, abs/1909.05311.</p>
<p>Yukun Ma, Haiyun Peng, and Erik Cambria. 2018. Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm. In $A A A I$.</p>
<p>Todor Mihaylov and Anette Frank. 2018. Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 821-832, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Alexander Miller, Adam Fisch, Jesse Dodge, AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1400-1409, Austin, Texas. Association for Computational Linguistics.</p>
<p>Simon Ostermann, Michael Roth, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. 2018. SemEval2018 task 11: Machine comprehension using commonsense knowledge. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 747-757, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. 2019a. Improving question answering with external knowledge. CoRR, abs/1902.00993.</p>
<p>Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. 2019b. Improving question answering with external knowledge. CoRR, cs.CL/1902.00993v1.</p>
<p>Matthew E. Peters, Mark Neumann, IV RobertL.Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. ArXiv, abs/1909.04164.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. 2019. Option comparison network for multiple-choice reading comprehension. CoRR, abs/1903.03033.</p>
<p>Maarten Sap, Ronan LeBras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In $A A A I$.</p>
<p>Jan Scheffczyk, Collin F Baker, and Srini Narayanan. 2010. Reasoning over natural language text by means of framenet and ontologies. Ontology and the lexicon: A natural language processing perspective, pages 53-71.</p>
<p>Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. ArXiv, abs/1611.01603.</p>
<p>Push Singh, Thomas Lin, Erik T. Mueller, Grace Lim, Travell Perkins, and Wan Li Zhu. 2002. Open mind common sense: Knowledge acquisition from the general public. In On the Move to Meaningful Internet Systems, 2002 - DOA/CoopIS/ODBASE 2002 Confederated International Conferences DOA, CoopIS and ODBASE 2002, pages 1223-1237, Berlin, Heidelberg. Springer-Verlag.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2016. Conceptnet 5.5: An open multilingual graph of general knowledge. In AAAI Conference on Artificial Intelligence.</p>
<p>Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. 2019. Dream: A challenge dataset and models for dialogue-based reading comprehension. Transactions of the Association for Computational Linguistics, 7:217-231.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Niket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih, Antoine Bosselut, and Peter Clark. 2018. Reasoning about actions and state changes by injecting commonsense knowledge. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 57-66, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 189-198, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Dirk Weissenborn, Tom'avs Kovcisk'y, and Chris Dyer. 2018. Dynamic integration of background knowledge in neural nlu systems.</p>
<p>Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering. ArXiv, abs/1611.01604.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Cite arxiv:1906.08237Comment: Pretrained models and code are available at https://github.com/zihangdai/xlnet.</p>
<p>Adams Wei Yu, David Dohan, Thang Luong, Rui Zhao, Kai Chen, and Quoc Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension.</p>
<p>Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2018a. From recognition to cognition: Visual commonsense reasoning. CoRR, abs/1811.10830.</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018b. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93-104, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885.</p>
<p>Wanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2018. Improving question answering by commonsense-based pre-training. CoRR, abs/1809.03568.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Because the newly-released XLNet has out-performed BERT on various tasks, we considered using XLNet as the OCN's encoder. However, from our initial experiments, XLNet is very unstable, in that it easily provides degenerate solutions-a problem noted by Devlin et al. (2019) for small datasets. We found BERT to be more stable in our study.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
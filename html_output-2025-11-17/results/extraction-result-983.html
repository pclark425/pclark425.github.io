<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-983 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-983</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-983</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-1e76e2fbf27198986271a672f462dc38d790d00f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1e76e2fbf27198986271a672f462dc38d790d00f" target="_blank">The Risks of Invariant Risk Minimization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> In this setting, the first analysis of classification under the IRM objective is presented, and it is found that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.</p>
                <p><strong>Paper Abstract:</strong> Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective$-$as well as these recently proposed alternatives$-$under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution$-$this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e983.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e983.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An objective to learn feature representations whose optimal (linear) classifier is the same across labeled training environments, intended to recover invariant (causal) features and avoid relying on environment-specific correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant risk minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Risk Minimization (IRM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Search for a feature map Phi and classifier beta such that beta is simultaneously optimal for each training environment; operationalized as a constrained bilevel program and approximated by a Lagrangian penalty that adds the squared norm of the gradient of each environment's risk w.r.t. beta (||∇_beta R^e(Phi,beta)||^2) to the average risk.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic SEM environments with labeled training environments (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive simulated structural equation model (SEM) with latent invariant features z_c and environmental (non-invariant) features z_e; E discrete training environments differ in parameters (means/variances) of z_e; infinite-sample, offline setting (no active interventions).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Enforce invariance of the optimal classifier across environments via constraints/penalty (gradient-norm penalty) so that features whose predictive relationship with y varies across environments are (intended to be) excluded from Phi.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Non-invariant correlated features whose correlation with label varies across environments (label-correlated but non-causal features / environment-specific correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via checking whether the optimal linear predictor on top of Phi differs across environments (measured via gradients of per-environment risk); large per-environment gradient norms indicate environment-specific optimality violations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>No explicit statistical downweighting of specific features; uses a global penalty that discourages representations where per-environment optimal betas differ (regularization of gradient norms).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Constrained optimization requiring beta to be argmin of each environment's risk (or approximated via gradient=0 penalty) which, if feasible, rules out predictors that exploit environment-varying cues; the paper shows this can be insufficient in many latent/non-linear settings.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: IRM recovers invariant predictor and generalizes under distribution shift only when the number of distinct training environments exceeds the dimensionality of environmental (distractor) features (E > d_e) in the linear setting; otherwise it typically fails and behaves like ERM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Qualitative baseline (ERM): ERM often achieves lower empirical risk on training environments by exploiting non-invariant features and thus fails under moderate distribution shift (can be worse-than-chance when correlations reverse). In many settings IRM gives no improvement over ERM unless E>d_e.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>d_e environmental dimensions (varied; e.g., d_e=6 in experiments in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRM explicitly targets distractors by enforcing invariant optimal classifiers, but (1) in linear latent models it only reliably recovers invariant features when the number of distinct training environments exceeds the dimensionality of non-invariant features (E>d_e); (2) when E<=d_e there exist feasible IRM solutions that use only environmental features and achieve lower training risk than the optimal invariant predictor, causing IRM to prefer spurious solutions; (3) in non-linear/injective observation models IRM can be 'fooled' by predictors that are identical to the invariant solution on almost all training datapoints yet rely on non-invariant features on different regions, causing catastrophic failure under modest test shifts. Overall IRM often reduces to ERM in practice unless restrictive coverage of environments is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e983.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e983.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses invariance of the conditional distribution of the target given a subset of covariates across environments to identify causal parents (variables) of the target when covariates are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference by using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Search over subsets of observed covariates for those whose conditional distribution p(y | S) is invariant across environments; statistical tests (e.g., conditional independence/invariance tests) are used to accept/reject subsets, producing sets of variables that are plausible causal parents with confidence guarantees under SEM assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not evaluated in this paper (general multiple-environment observational datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>ICP assumes labeled environments corresponding to interventions or distributional shifts; offline observational data with observed covariates (not the latent SEM of this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection via invariance testing: rejects variables/subsets whose conditional predictive distribution of y changes across environments, thereby excluding distractors that induce non-invariant correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Non-causal correlated covariates that induce environment-varying predictive relationships (spurious correlations, selection biases across environments).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical invariance / conditional independence tests across environments to detect subsets that yield invariant predictions; uses confidence intervals to control false inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not applicable — ICP performs subset selection (exclude variables) rather than continuous downweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Rejection of subsets whose conditional predictive distributions differ across environments; identification accompanied by confidence intervals to refute spurious sets.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as the canonical causal prediction approach with formal identification guarantees when covariates are (partially) observed; the paper contrasts ICP's assumptions (observed covariates) with IRM's goal of representation learning from high-dimensional observations where latents are unobserved, highlighting that ICP's guarantees do not directly transfer to latent-variable deep learning settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e983.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e983.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Risk Extrapolation (REx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative objective to IRM that penalizes variance of risks across environments to encourage predictors whose risk is similar across training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Out-of-distribution generalization via risk extrapolation (rex)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Risk Extrapolation (REx)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Minimize the average training risk while adding a penalty on the variance (or another dispersion measure) of per-environment risks, encouraging solutions with equalized risks across environments; motivated as enforcing invariance of predictive behavior across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not evaluated in this paper (mentioned as an alternative objective)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Typically applied in multi-environment supervised learning with labeled environments; offline, non-interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variance-of-risk regularization to discourage reliance on features that make risks differ across environments (implicitly downweights features that produce high variability in per-environment losses).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-specific correlations that cause variability in per-environment losses (non-invariant predictors).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Monitors variance/dispersion of per-environment risks as a proxy signal for environment-specific reliance on spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Global penalty that reduces solutions whose per-environment risks vary; does not target individual features explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>No quantitative metrics in this paper; authors state REx (and related objectives) share similar shortcomings as IRM in the latent/non-linear setting and can also fail to recover invariant predictors absent strong environment coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper notes that REx (variance-penalizing objectives) produce the optimal invariant predictor as a stationary point, but provides theoretical constructions and corollaries showing these alternatives can fail like IRM in linear and non-linear latent settings when training environments do not sufficiently 'cover' the space of environment variations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e983.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e983.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RiskVarPen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Risk Variance Penalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of objectives that penalize higher moments (variance or square-root variance) of per-environment risks to enforce stronger invariance properties (including p(y | Phi(x))).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Risk variance penalization: From distributional robustness to causality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Risk variance penalization (and square-root variants)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Augment average training risk with a term penalizing the variance (or square-root variance) of environment-specific losses, encouraging representations whose conditional predictive distributions are stable across environments; some variants aim to control higher moments to approach invariance of full conditional p(y|Phi(x)).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not evaluated in this paper (mentioned as an alternative)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Multi-environment labeled datasets; offline.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Penalize variability in per-environment risk/moments to discourage learning features whose predictive mapping to y varies across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-varying predictive relationships arising from non-invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Variance (or higher-moment) of per-environment risk used as a proxy detection signal.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Global regularization reduces emphasis on features that cause risk variability; no per-feature attribution provided.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>No numeric measures provided; paper states that these methods, while promising, lack formal guarantees in latent non-linear settings and inherit failure modes similar to IRM unless environments sufficiently cover distractor variability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Although such variance-penalizing objectives make the optimal invariant predictor a stationary point, the paper argues (and provides corollaries) that they can be deceived by representations that look invariant on training mass but rely on non-invariant features on other regions, thus failing OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e983.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e983.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-Adv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-adversarial representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that learn representations invariant to domain (environment) by adversarial training to minimize domain distinguishability (used for domain adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain-adversarial training of neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-adversarial representation learning (e.g., DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learn representation Phi that minimizes source classification loss while an adversary attempts to predict domain label from Phi(x); training seeks representations where p(Phi(x)) is domain-invariant, typically used for domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not evaluated in this paper (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Typically offline multi-domain datasets with labeled/unlabeled target data in domain adaptation settings; not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Aims to remove domain-specific distributional differences in p(Phi(x)) but does not guarantee conditional invariance p(y|Phi(x)), so may not eliminate distractors that correlate with label similarly across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Adversarial training reduces domain-identifying variation in representation (implicit downweighting of domain-specific features).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a contrasting line of work that targets domain-invariance of feature distributions p(Phi(x)) (for adaptation) rather than invariant predictive relationships; the paper notes that such invariance may be insufficient for domain generalization and causal recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e983.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e983.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CounterfactualAug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual / data augmentation techniques</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data augmentation approaches that create counterfactual or perturbed examples to break spurious correlations and encourage learning of invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalizing to unseen domains via adversarial data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Counterfactual / adversarial data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Create modified or adversarially transformed training examples (including counterfactuals that intervene on putative spurious factors) to reduce reliance on non-causal cues and to force models to learn features robust across augmented variations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not evaluated in this paper (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Augmented offline datasets; augmentation can be structured (counterfactual) or adversarial; not inherently interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Introduce examples that break the spurious correlation (counterfactual augmentation) so the model cannot rely on distractor features; adversarial augmentation expands distributional support.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations between observed features and labels induced by dataset bias or environment-specific factors.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Indirect: by expanding support and showing spurious cues are non-robust, the model learns to downweight them.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites data-augmentation and specifically counterfactual augmentation as an approach to reduce spurious feature reliance, but notes these are complementary to invariant objectives and also lack formal guarantees in latent/non-linear observational settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e983.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e983.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RegressionInvariance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Invariance for causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use invariance in regression relationships across environments to learn causal structure (e.g., detect variables whose regression coefficient is stable).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning causal structures using regression invariance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Regression-invariance based causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Exploit invariance of regression coefficients or functional relationships across environments to infer causal parents/structure; compare regression fits across environments to find variables with stable predictive relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not evaluated in this paper (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline multi-environment observational data with observed covariates; algorithmic approaches for structure learning, not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects variables whose regression contribution varies across environments and excludes them as non-causal; effectively variable selection based on coefficient stability.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-dependent correlations and non-causal predictors whose regression contribution shifts under interventions/environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Comparing regression parameters / invariance tests across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Exclusion or reduced weight for variables with non-stable regression coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned among related causal-discovery approaches that motivate invariant objectives; the paper emphasizes that such methods rely on observed covariates and offer identification guarantees that do not straightforwardly extend to latent high-dimensional observation models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e983.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e983.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nonlinear-ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant causal prediction for nonlinear models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extensions of ICP to nonlinear models that attempt to identify invariant predictive relationships under broader functional classes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant causal prediction for nonlinear models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Nonlinear Invariant Causal Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generalize ICP to nonlinear relationships (e.g., via nonlinear regression or conditional independence testing) to search for sets of covariates yielding invariant conditional distributions across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not evaluated in this paper (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline multi-environment observational data; the methods assume observed covariates and perform nonlinear tests/estimations.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Nonlinear invariance testing / conditional independence testing to identify and exclude non-invariant (distractor) covariates.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Nonlinear environment-dependent correlations, non-causal features.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Nonlinear conditional independence / invariance tests across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as part of the literature extending invariant prediction to nonlinear observed-variable settings; the paper highlights that formal guarantees for such methods exist when covariates are observed, but not when latent factors drive the observations as in deep representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Risks of Invariant Risk Minimization', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Invariant risk minimization <em>(Rating: 2)</em></li>
                <li>Causal inference by using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Out-of-distribution generalization via risk extrapolation (rex) <em>(Rating: 2)</em></li>
                <li>Risk variance penalization: From distributional robustness to causality <em>(Rating: 2)</em></li>
                <li>Learning causal structures using regression invariance <em>(Rating: 1)</em></li>
                <li>Invariant causal prediction for nonlinear models <em>(Rating: 1)</em></li>
                <li>Generalizing to unseen domains via adversarial data augmentation <em>(Rating: 1)</em></li>
                <li>Learning the difference that makes a difference with counterfactually-augmented data <em>(Rating: 1)</em></li>
                <li>Preventing failures due to dataset shift: Learning predictive models that transport <em>(Rating: 1)</em></li>
                <li>Domain-adversarial training of neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-983",
    "paper_id": "paper-1e76e2fbf27198986271a672f462dc38d790d00f",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "IRM",
            "name_full": "Invariant Risk Minimization",
            "brief_description": "An objective to learn feature representations whose optimal (linear) classifier is the same across labeled training environments, intended to recover invariant (causal) features and avoid relying on environment-specific correlations.",
            "citation_title": "Invariant risk minimization",
            "mention_or_use": "use",
            "method_name": "Invariant Risk Minimization (IRM)",
            "method_description": "Search for a feature map Phi and classifier beta such that beta is simultaneously optimal for each training environment; operationalized as a constrained bilevel program and approximated by a Lagrangian penalty that adds the squared norm of the gradient of each environment's risk w.r.t. beta (||∇_beta R^e(Phi,beta)||^2) to the average risk.",
            "environment_name": "Synthetic SEM environments with labeled training environments (this paper)",
            "environment_description": "Non-interactive simulated structural equation model (SEM) with latent invariant features z_c and environmental (non-invariant) features z_e; E discrete training environments differ in parameters (means/variances) of z_e; infinite-sample, offline setting (no active interventions).",
            "handles_distractors": true,
            "distractor_handling_technique": "Enforce invariance of the optimal classifier across environments via constraints/penalty (gradient-norm penalty) so that features whose predictive relationship with y varies across environments are (intended to be) excluded from Phi.",
            "spurious_signal_types": "Non-invariant correlated features whose correlation with label varies across environments (label-correlated but non-causal features / environment-specific correlations).",
            "detection_method": "Implicit detection via checking whether the optimal linear predictor on top of Phi differs across environments (measured via gradients of per-environment risk); large per-environment gradient norms indicate environment-specific optimality violations.",
            "downweighting_method": "No explicit statistical downweighting of specific features; uses a global penalty that discourages representations where per-environment optimal betas differ (regularization of gradient norms).",
            "refutation_method": "Constrained optimization requiring beta to be argmin of each environment's risk (or approximated via gradient=0 penalty) which, if feasible, rules out predictors that exploit environment-varying cues; the paper shows this can be insufficient in many latent/non-linear settings.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: IRM recovers invariant predictor and generalizes under distribution shift only when the number of distinct training environments exceeds the dimensionality of environmental (distractor) features (E &gt; d_e) in the linear setting; otherwise it typically fails and behaves like ERM.",
            "performance_without_robustness": "Qualitative baseline (ERM): ERM often achieves lower empirical risk on training environments by exploiting non-invariant features and thus fails under moderate distribution shift (can be worse-than-chance when correlations reverse). In many settings IRM gives no improvement over ERM unless E&gt;d_e.",
            "has_ablation_study": true,
            "number_of_distractors": "d_e environmental dimensions (varied; e.g., d_e=6 in experiments in the paper)",
            "key_findings": "IRM explicitly targets distractors by enforcing invariant optimal classifiers, but (1) in linear latent models it only reliably recovers invariant features when the number of distinct training environments exceeds the dimensionality of non-invariant features (E&gt;d_e); (2) when E&lt;=d_e there exist feasible IRM solutions that use only environmental features and achieve lower training risk than the optimal invariant predictor, causing IRM to prefer spurious solutions; (3) in non-linear/injective observation models IRM can be 'fooled' by predictors that are identical to the invariant solution on almost all training datapoints yet rely on non-invariant features on different regions, causing catastrophic failure under modest test shifts. Overall IRM often reduces to ERM in practice unless restrictive coverage of environments is available.",
            "uuid": "e983.0",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "ICP",
            "name_full": "Invariant Causal Prediction (ICP)",
            "brief_description": "A framework that uses invariance of the conditional distribution of the target given a subset of covariates across environments to identify causal parents (variables) of the target when covariates are observed.",
            "citation_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant Causal Prediction (ICP)",
            "method_description": "Search over subsets of observed covariates for those whose conditional distribution p(y | S) is invariant across environments; statistical tests (e.g., conditional independence/invariance tests) are used to accept/reject subsets, producing sets of variables that are plausible causal parents with confidence guarantees under SEM assumptions.",
            "environment_name": "Not evaluated in this paper (general multiple-environment observational datasets)",
            "environment_description": "ICP assumes labeled environments corresponding to interventions or distributional shifts; offline observational data with observed covariates (not the latent SEM of this paper).",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection via invariance testing: rejects variables/subsets whose conditional predictive distribution of y changes across environments, thereby excluding distractors that induce non-invariant correlations.",
            "spurious_signal_types": "Non-causal correlated covariates that induce environment-varying predictive relationships (spurious correlations, selection biases across environments).",
            "detection_method": "Statistical invariance / conditional independence tests across environments to detect subsets that yield invariant predictions; uses confidence intervals to control false inclusion.",
            "downweighting_method": "Not applicable — ICP performs subset selection (exclude variables) rather than continuous downweighting.",
            "refutation_method": "Rejection of subsets whose conditional predictive distributions differ across environments; identification accompanied by confidence intervals to refute spurious sets.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as the canonical causal prediction approach with formal identification guarantees when covariates are (partially) observed; the paper contrasts ICP's assumptions (observed covariates) with IRM's goal of representation learning from high-dimensional observations where latents are unobserved, highlighting that ICP's guarantees do not directly transfer to latent-variable deep learning settings.",
            "uuid": "e983.1",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "REx",
            "name_full": "Risk Extrapolation (REx)",
            "brief_description": "An alternative objective to IRM that penalizes variance of risks across environments to encourage predictors whose risk is similar across training environments.",
            "citation_title": "Out-of-distribution generalization via risk extrapolation (rex)",
            "mention_or_use": "mention",
            "method_name": "Risk Extrapolation (REx)",
            "method_description": "Minimize the average training risk while adding a penalty on the variance (or another dispersion measure) of per-environment risks, encouraging solutions with equalized risks across environments; motivated as enforcing invariance of predictive behavior across environments.",
            "environment_name": "Not evaluated in this paper (mentioned as an alternative objective)",
            "environment_description": "Typically applied in multi-environment supervised learning with labeled environments; offline, non-interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variance-of-risk regularization to discourage reliance on features that make risks differ across environments (implicitly downweights features that produce high variability in per-environment losses).",
            "spurious_signal_types": "Environment-specific correlations that cause variability in per-environment losses (non-invariant predictors).",
            "detection_method": "Monitors variance/dispersion of per-environment risks as a proxy signal for environment-specific reliance on spurious features.",
            "downweighting_method": "Global penalty that reduces solutions whose per-environment risks vary; does not target individual features explicitly.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "No quantitative metrics in this paper; authors state REx (and related objectives) share similar shortcomings as IRM in the latent/non-linear setting and can also fail to recover invariant predictors absent strong environment coverage.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Paper notes that REx (variance-penalizing objectives) produce the optimal invariant predictor as a stationary point, but provides theoretical constructions and corollaries showing these alternatives can fail like IRM in linear and non-linear latent settings when training environments do not sufficiently 'cover' the space of environment variations.",
            "uuid": "e983.2",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RiskVarPen",
            "name_full": "Risk Variance Penalization",
            "brief_description": "A family of objectives that penalize higher moments (variance or square-root variance) of per-environment risks to enforce stronger invariance properties (including p(y | Phi(x))).",
            "citation_title": "Risk variance penalization: From distributional robustness to causality",
            "mention_or_use": "mention",
            "method_name": "Risk variance penalization (and square-root variants)",
            "method_description": "Augment average training risk with a term penalizing the variance (or square-root variance) of environment-specific losses, encouraging representations whose conditional predictive distributions are stable across environments; some variants aim to control higher moments to approach invariance of full conditional p(y|Phi(x)).",
            "environment_name": "Not evaluated in this paper (mentioned as an alternative)",
            "environment_description": "Multi-environment labeled datasets; offline.",
            "handles_distractors": true,
            "distractor_handling_technique": "Penalize variability in per-environment risk/moments to discourage learning features whose predictive mapping to y varies across environments.",
            "spurious_signal_types": "Environment-varying predictive relationships arising from non-invariant features.",
            "detection_method": "Variance (or higher-moment) of per-environment risk used as a proxy detection signal.",
            "downweighting_method": "Global regularization reduces emphasis on features that cause risk variability; no per-feature attribution provided.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "No numeric measures provided; paper states that these methods, while promising, lack formal guarantees in latent non-linear settings and inherit failure modes similar to IRM unless environments sufficiently cover distractor variability.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Although such variance-penalizing objectives make the optimal invariant predictor a stationary point, the paper argues (and provides corollaries) that they can be deceived by representations that look invariant on training mass but rely on non-invariant features on other regions, thus failing OOD generalization.",
            "uuid": "e983.3",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Domain-Adv",
            "name_full": "Domain-adversarial representation learning",
            "brief_description": "Approaches that learn representations invariant to domain (environment) by adversarial training to minimize domain distinguishability (used for domain adaptation).",
            "citation_title": "Domain-adversarial training of neural networks",
            "mention_or_use": "mention",
            "method_name": "Domain-adversarial representation learning (e.g., DANN)",
            "method_description": "Learn representation Phi that minimizes source classification loss while an adversary attempts to predict domain label from Phi(x); training seeks representations where p(Phi(x)) is domain-invariant, typically used for domain adaptation.",
            "environment_name": "Not evaluated in this paper (mentioned in related work)",
            "environment_description": "Typically offline multi-domain datasets with labeled/unlabeled target data in domain adaptation settings; not interactive.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Aims to remove domain-specific distributional differences in p(Phi(x)) but does not guarantee conditional invariance p(y|Phi(x)), so may not eliminate distractors that correlate with label similarly across domains.",
            "detection_method": null,
            "downweighting_method": "Adversarial training reduces domain-identifying variation in representation (implicit downweighting of domain-specific features).",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as a contrasting line of work that targets domain-invariance of feature distributions p(Phi(x)) (for adaptation) rather than invariant predictive relationships; the paper notes that such invariance may be insufficient for domain generalization and causal recovery.",
            "uuid": "e983.4",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "CounterfactualAug",
            "name_full": "Counterfactual / data augmentation techniques",
            "brief_description": "Data augmentation approaches that create counterfactual or perturbed examples to break spurious correlations and encourage learning of invariant features.",
            "citation_title": "Generalizing to unseen domains via adversarial data augmentation",
            "mention_or_use": "mention",
            "method_name": "Counterfactual / adversarial data augmentation",
            "method_description": "Create modified or adversarially transformed training examples (including counterfactuals that intervene on putative spurious factors) to reduce reliance on non-causal cues and to force models to learn features robust across augmented variations.",
            "environment_name": "Not evaluated in this paper (mentioned in related work)",
            "environment_description": "Augmented offline datasets; augmentation can be structured (counterfactual) or adversarial; not inherently interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Introduce examples that break the spurious correlation (counterfactual augmentation) so the model cannot rely on distractor features; adversarial augmentation expands distributional support.",
            "spurious_signal_types": "Spurious correlations between observed features and labels induced by dataset bias or environment-specific factors.",
            "detection_method": null,
            "downweighting_method": "Indirect: by expanding support and showing spurious cues are non-robust, the model learns to downweight them.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Paper cites data-augmentation and specifically counterfactual augmentation as an approach to reduce spurious feature reliance, but notes these are complementary to invariant objectives and also lack formal guarantees in latent/non-linear observational settings.",
            "uuid": "e983.5",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RegressionInvariance",
            "name_full": "Regression Invariance for causal discovery",
            "brief_description": "Approaches that use invariance in regression relationships across environments to learn causal structure (e.g., detect variables whose regression coefficient is stable).",
            "citation_title": "Learning causal structures using regression invariance",
            "mention_or_use": "mention",
            "method_name": "Regression-invariance based causal discovery",
            "method_description": "Exploit invariance of regression coefficients or functional relationships across environments to infer causal parents/structure; compare regression fits across environments to find variables with stable predictive relationships.",
            "environment_name": "Not evaluated in this paper (mentioned in related work)",
            "environment_description": "Offline multi-environment observational data with observed covariates; algorithmic approaches for structure learning, not interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects variables whose regression contribution varies across environments and excludes them as non-causal; effectively variable selection based on coefficient stability.",
            "spurious_signal_types": "Environment-dependent correlations and non-causal predictors whose regression contribution shifts under interventions/environments.",
            "detection_method": "Comparing regression parameters / invariance tests across environments.",
            "downweighting_method": "Exclusion or reduced weight for variables with non-stable regression coefficients.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned among related causal-discovery approaches that motivate invariant objectives; the paper emphasizes that such methods rely on observed covariates and offer identification guarantees that do not straightforwardly extend to latent high-dimensional observation models.",
            "uuid": "e983.6",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Nonlinear-ICP",
            "name_full": "Invariant causal prediction for nonlinear models",
            "brief_description": "Extensions of ICP to nonlinear models that attempt to identify invariant predictive relationships under broader functional classes.",
            "citation_title": "Invariant causal prediction for nonlinear models",
            "mention_or_use": "mention",
            "method_name": "Nonlinear Invariant Causal Prediction",
            "method_description": "Generalize ICP to nonlinear relationships (e.g., via nonlinear regression or conditional independence testing) to search for sets of covariates yielding invariant conditional distributions across environments.",
            "environment_name": "Not evaluated in this paper (mentioned in related work)",
            "environment_description": "Offline multi-environment observational data; the methods assume observed covariates and perform nonlinear tests/estimations.",
            "handles_distractors": true,
            "distractor_handling_technique": "Nonlinear invariance testing / conditional independence testing to identify and exclude non-invariant (distractor) covariates.",
            "spurious_signal_types": "Nonlinear environment-dependent correlations, non-causal features.",
            "detection_method": "Nonlinear conditional independence / invariance tests across environments.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as part of the literature extending invariant prediction to nonlinear observed-variable settings; the paper highlights that formal guarantees for such methods exist when covariates are observed, but not when latent factors drive the observations as in deep representation learning.",
            "uuid": "e983.7",
            "source_info": {
                "paper_title": "The Risks of Invariant Risk Minimization",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Invariant risk minimization",
            "rating": 2
        },
        {
            "paper_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "rating": 2
        },
        {
            "paper_title": "Out-of-distribution generalization via risk extrapolation (rex)",
            "rating": 2
        },
        {
            "paper_title": "Risk variance penalization: From distributional robustness to causality",
            "rating": 2
        },
        {
            "paper_title": "Learning causal structures using regression invariance",
            "rating": 1
        },
        {
            "paper_title": "Invariant causal prediction for nonlinear models",
            "rating": 1
        },
        {
            "paper_title": "Generalizing to unseen domains via adversarial data augmentation",
            "rating": 1
        },
        {
            "paper_title": "Learning the difference that makes a difference with counterfactually-augmented data",
            "rating": 1
        },
        {
            "paper_title": "Preventing failures due to dataset shift: Learning predictive models that transport",
            "rating": 1
        },
        {
            "paper_title": "Domain-adversarial training of neural networks",
            "rating": 1
        }
    ],
    "cost": 0.019775749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Risks of Invariant Risk Minimization</h1>
<p>Elan Rosenfeld, Pradeep Ravikumar, Andrej Risteski<br>Machine Learning Department<br>Carnegie Mellon University<br>elan@cmu.edu, pradeepr@cs.cmu.edu, aristesk@andrew.cmu.edu</p>
<h4>Abstract</h4>
<p>Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective-as well as these recently proposed alternatives-under a fairly natural and general model. In the linear case, we give simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution-this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.</p>
<h2>1 INTRODUCTION</h2>
<p>Prediction algorithms are evaluated by their performance on unseen test data. In classical machine learning, it is common to assume that such data are drawn i.i.d. from the same distribution as the data set on which the learning algorithm was trained-in the real world, however, this is often not the case. When this discrepancy occurs, algorithms with strong in-distribution generalization guarantees, such as Empirical Risk Minimization (ERM), can fail catastrophically. In particular, while deep neural networks achieve superhuman performance on many tasks, there is evidence that they rely on statistically informative but non-causal features in the data (Beery et al., 2018; Geirhos et al., 2018; Ilyas et al., 2019). As a result, such models are prone to errors under surprisingly minor distribution shift (Su et al., 2019; Recht et al., 2019). To address this, researchers have investigated alternative objectives for training predictors which are robust to possibly egregious shifts in the test distribution.</p>
<p>The task of generalizing under such shifts, known as Out-of-Distribution (OOD) Generalization, has led to many separate threads of research. One approach is Bayesian deep learning, accounting for a classifier's uncertainty at test time (Neal, 2012). Another technique that has shown promise is data augmentation-this includes both automated data modifications which help prevent overfitting (Shorten \&amp; Khoshgoftaar, 2019) and specific counterfactual augmentations to ensure invariance in the resulting features (Volpi et al., 2018; Kaushik et al., 2020).</p>
<p>A strategy which has recently gained particular traction is Invariant Causal Prediction (ICP; Peters et al. 2016), which views the task of OOD generalization through the lens of causality. This framework assumes that the data are generated according to a Structural Equation Model (SEM; Bollen 2005), which consists of a set of so-called mechanisms or structural equations that specify variables given their parents. ICP assumes moreover that the data can be partitioned into environments, where each environment corresponds to interventions on the SEM (Pearl, 2009), but where the mechanism by which the target variable is generated via its direct parents is unaffected. Thus the causal mechanism of the target variable is unchanging but other aspects of the distribution can vary broadly. As a result, learning mechanisms that are the same across environments ensures recovery of the invariant features which generalize under arbitrary interventions. In this work, we consider objectives that attempt to</p>
<p>learn what we refer to as the "optimal invariant predictor"-this is the classifier which uses and is optimal with respect to only the invariant features in the SEM. By definition, such a classifier does not overfit to environment-specific properties of the data distribution, so it will generalize even under major distribution shift at test time. In particular, we focus our analysis on one of the more popular objectives, Invariant Risk Minimization (IRM; Arjovsky et al. (2019)), but our results can easily be extended to similar recently proposed alternatives.</p>
<p>Various works on invariant prediction (Muandet et al., 2013; Ghassami et al., 2017; Heinze-Deml et al., 2018; Rojas-Carulla et al., 2018; Subbaswamy et al., 2019; Christiansen et al., 2020) consider regression in both the linear and non-linear setting, but they exclusively focus on learning with fully or partially observed covariates or some other source of information. Under such a condition, results from causal inference (Maathuis et al., 2009; Peters et al., 2017) allow for formal guarantees of the identification of the invariant features, or at least a strict subset of them. With the rise of deep learning, more recent literature has developed objectives for learning invariant representations when the data are a non-linear function of unobserved latent factors, a common assumption when working with complex, high-dimensional data such as images. Causal discovery and inference with unobserved confounders or latents is a much harder problem (Peters et al., 2017), so while empirical results seem encouraging, these objectives are presented with few formal guarantees. IRM is one such objective for invariant representation learning. The goal of IRM is to learn a feature embedder such that the optimal linear predictor on top of these features is the same for every environment-the idea being that only the invariant features will have an optimal predictor that is invariant. Recent works have pointed to shortcomings of IRM and have suggested modifications which they claim prevent these failures. However, these alternatives are compared in broad strokes, with little in the way of theory.</p>
<p>In this work, we present the first formal analysis of classification under the IRM objective under a fairly natural and general model which carefully formalizes the intuition behind the original work. Our results show that despite being inspired by invariant prediction, this objective can frequently be expected to perform no better than ERM. In the linear setting, we present simple, exact conditions under which solving to optimality succeeds or, more often, breaks down in recovering the optimal invariant predictor. We also demonstrate another major failure case-under mild conditions, there exists a feasible point that uses only non-invariant features and achieves lower empirical risk than the optimal invariant predictor; thus it will appear as a more attractive solution, yet its reliance on non-invariant features mean it will fail to generalize. As corollaries, we present similar settings where all recently suggested alternatives to IRM likewise fail. Futhermore, we present the first results in the non-linear regime: we demonstrate the existence of a classifier with exponentially small suboptimality which nevertheless heavily relies on non-invariant features on most test inputs, resulting in worse-than-chance performance on distributions that are sufficiently dissimilar from the training environments. These findings strongly suggest that existing approaches to ICP for high-dimensional latent variable models do not cleanly achieve their stated objective and that future work would benefit from a more formal treatment.</p>
<h1>2 RELATED WORK</h1>
<p>Works on learning deep invariant representations vary considerably: some search for a domaininvariant representation (Muandet et al., 2013; Ganin et al., 2016), i.e. invariance of the distribution $p(\Phi(x))$, typically used for domain adaptation (Ben-David et al., 2010; Ganin \&amp; Lempitsky, 2015; Zhang et al., 2015; Long et al., 2018), with assumed access to labeled or unlabeled data from the target distribution. Other works instead hope to find representations that are conditionally domain-invariant, with invariance of $p(\Phi(x) \mid y)$ (Gong et al., 2016; Li et al., 2018). However, there is evidence that invariance may not be sufficient for domain adaptation (Zhao et al., 2019; Johansson et al., 2019). In contrast, this paper focuses instead on domain generalization (Blanchard et al., 2011; Rosenfeld et al., 2021), where access to the test distribution is not assumed.</p>
<p>Recent works on domain generalization, including the objectives discussed in this paper, suggest invariance of the feature-conditioned label distribution. In particular, Arjovsky et al. (2019) only assume invariance of $\mathbb{E}[y \mid \Phi(x)]$; follow-up works rely on a stronger assumption of invariance of higher conditional moments (Krueger et al., 2020; Xie et al., 2020; Jin et al., 2020; Mahajan et al., 2020; Bellot \&amp; van der Schaar, 2020). Though this approach has become popular in the last year, it is somewhat similar to the existing concept of covariate shift (Shimodaira, 2000; Bickel et al., 2009),</p>
<p>which considers the same setting. The main difference is that these more recent works assume that the shifts in $p(\Phi(x))$ occur between discrete, labeled environments, as opposed to more generally from train to test distributions.</p>
<p>Some concurrent lines of work study different settings yet give results which are remarkably similar to ours. Xu et al. (2021) show that an infinitely wide two-layer network extrapolates linear functions when the training data is sufficiently diverse. In the context of domain generalization specifically, Rosenfeld et al. (2021) prove that ERM remains optimal for both interpolation and extrapolation in the linear setting and that the latter is exponentially harder than the former. These results mirror our findings that none of the studied objectives outperform ERM.</p>
<h1>3 Model and Informal Results</h1>
<p>We consider an SEM with explicit separation of invariant features $z_{c}$, whose joint distribution with the label is fixed for all environments, and environmental features $z_{e}$ ("non-invariant"), whose distribution can vary. This choice is to ensure that our model properly formalizes the intuition behind invariant prediction techniques such as IRM, whose objective is to ensure generalizing predictors by recovering only the invariant features-we put off a detailed description of these objectives until after we have introduced the necessary terminology.</p>
<p>We assume that data are drawn from a set of $E$ training environments $\mathcal{E}=\left{e_{1}, e_{2}, \ldots, e_{E}\right}$ and that we know from which environment each sample is drawn. For a given environment $e$, the data are defined by the following process: first, a label $y \in{ \pm 1}$ is drawn according to a fixed probability:</p>
<p>$$
y= \begin{cases}1, &amp; \text { w.p. } \eta \ -1, &amp; \text { otherwise }\end{cases}
$$</p>
<p>Next, both invariant features and environmental features are drawn according to a Gaussian: ${ }^{1}$</p>
<p>$$
z_{c} \sim \mathcal{N}\left(y \cdot \mu_{c}, \sigma_{c}^{2} I\right), \quad z_{e} \sim \mathcal{N}\left(y \cdot \mu_{e}, \sigma_{e}^{2} I\right)
$$</p>
<p>with $\mu_{c} \in \mathbb{R}^{d_{c}}, \mu_{e} \in \mathbb{R}^{d_{e}}$-typically, for complex, high-dimensional data we would expect $E&lt;$ $d_{c} \ll d_{e}$. Finally, the observation $x$ is generated as a function of the latent features:</p>
<p>$$
x=f\left(z_{c}, z_{e}\right)
$$</p>
<p>The complete data generating process is displayed in Figure 3.1. We assume $f$ is injective, so that it is in principle possible to recover the latent features from the observations, i.e. there exists a function $\Phi$ such that $\Phi\left(f\left(z_{c}, z_{e}\right)\right)=\left[z_{c}, z_{e}\right]^{T}$. We remark that this our only assumption on $f$, even when it is non-linear. Further, note that we model class-conditional means as direct opposites merely for clarity, as it greatly simplifies the calculations. None of our proofs require this condition: it is straightforward to extend our results to arbitrary means, and the non-linear setting also allows for arbitrary covariances. In fact, our proof technique for non-linear $f$ could be applied to any distribution that sufficiently concentrates about its mean (e.g., sub-Gaussian). We write the joint and marginal distributions as $p^{e}\left(x, y, z_{c}, z_{e}\right)$. When clear from context, we omit the specific arguments.</p>
<p>Remarks on the model. This model is natural and flexible; it generalizes several existing models used to analyze learning under the existence of adversarial distribution shift or non-invariant correlations (Schmidt et al., 2018; Sagawa et al., 2020). The fundamental facet of this model is the constancy of the invariant parameters $\eta, \mu_{c}, \sigma_{c}^{2}, f$ across environments-the dependence of $\mu_{e}, \sigma_{e}$ on the environment allows for varying distributions, while the true causal process remains unchanged. Here we make a few clarifying remarks:</p>
<ul>
<li>We do not impose any constraints on the model parameters. In particular, we do not assume a prior over the environmental parameters. Observe that $\mu_{c}, \sigma_{c}^{2}$ are the same for all environments,</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>hence the subscript indicates the invariant relationship. In contrast, with some abuse of notation, the environmental subscript is used to indicate both dependence on the environment and the index of the environment itself (e.g., $\mu_{i}$ represents the mean specific to environment $i$ ).</p>
<ul>
<li>While we have framed the model as $y$ causing $z_{c}$, the causation can just as easily be viewed in the other direction. The log-odds of $y$ are a linear function of $z_{c}$-this matches logistic regression with an invariant regression vector $\beta_{c}=2 \mu_{c} / \sigma_{c}^{2}$ and bias $\beta_{0}=\log \frac{\eta}{1-\eta}$. We present the model as above to emphasize that the causal relationships between $y$ and the $z_{c}, z_{e}$ are a priori indistinguishable, and because we believe this direction is more intuitive.</li>
</ul>
<p>We consider the setting where we are given infinite samples from each environment; this allows us to isolate the behavior of the objectives themselves, rather than finite-sample effects. Upon observing samples from this model, our objective is thus to learn a feature embedder $\Phi$ and classifier $\beta$ to minimize the risk on an unseen environment $e$ :</p>
<p>$$
\mathcal{R}^{e}(\Phi, \hat{\beta}):=\mathbb{E}_{(x, y) \sim p^{e}}\left[\ell\left(\sigma\left(\hat{\beta}^{T} \Phi(x)\right), y\right)\right]
$$</p>
<p>The function $\ell$ can be any loss appropriate to classification: in this work we consider the logistic and the 0-1 loss. Note that we are not hoping to minimize risk in expectation over the environments; this is already accomplished via ERM or distributionally robust optimization (DRO; Bagnell 2005; Ben-Tal et al. 2009). Rather, we hope to extract and regress on invariant features while ignoring environmental features, such that our predictor generalizes to all unseen environments regardless of their parameters. In other words, the focus is on minimizing risk in the worst-case. We refer to the predictor which will minimize worst-case risk under arbitrary distribution shift as the optimal invariant predictor. To discuss this formally, we define precisely what we mean by this term.
Definition 1. Under the model described by Equations 1-3, the optimal invariant predictor is the predictor defined by the composition of a) the featurizer which recovers the invariant features and b) the classifier which is optimal with respect to those features:</p>
<p>$$
\Phi^{<em>}(x):=\left[\begin{array}{ll}
I &amp; 0 \
0 &amp; 0
\end{array}\right] \circ f^{-1}(x)=\left[z_{c}\right], \quad \hat{\beta}^{</em>}:=\left[\begin{array}{c}
\beta_{c} \
\beta_{0}
\end{array}\right]:=\left[\begin{array}{c}
2 \mu_{c} / \sigma_{c}^{2} \
\log \frac{\eta}{1-\eta}
\end{array}\right]
$$</p>
<p>Observe that this definition closely resembles Definition 3 of Arjovsky et al. (2019); the only difference is that here the optimal invariant predictor must recover all invariant features. As Arjovsky et al. (2019) do not posit a data model, the concept of recovering "all invariant features" is not welldefined for their setting; technically, a featurizer which outputs the empty set would elicit an invariant predictor, but this would not satisfy the above definition. The classifier $\hat{\beta}^{*}$ is optimal with respect to the invariant features and so it achieves the minimum possible risk without using environmental features. Observe that the optimal invariant predictor is distinct from the Bayes classifier; the Bayes classifier uses environmental features which are informative of the label but non-invariant; the optimal invariant predictor explicitly ignores these features.
With the model defined, we can informally present our results; we defer the formal statements to first give a background on the IRM objective in the next section. With a slight abuse of notation, we identify a predictor by the tuple $\Phi, \hat{\beta}$ which parametrizes it. First, we show that the usefulness of IRM exhibits a "thresholding" behavior depending on $E$ and $d_{e}$ :
Theorem 3.1 (Informal, Linear). For linear $f$, consider solving the IRM objective to learn a linear $\Phi$ with invariant optimal classifier $\hat{\beta}$. If $E&gt;d_{e}$, then $\Phi, \hat{\beta}$ is precisely the optimal invariant predictor; it uses only invariant features and generalizes to all environments with minimax-optimal risk. If $E \leq d_{e}$, then $\Phi, \hat{\beta}$ relies upon non-invariant features.</p>
<p>In fact, when $E \leq d_{e}$ it is even possible to learn a classifier solely relying on environmental features that achieves lower risk on the training environments than the optimal invariant predictor:</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Theorem 3.2 (Informal, Linear). For linear $f$ and $E \leq d_{e}$ there exists a linear predictor $\Phi, \hat{\beta}$ which uses only environmental features, yet achieves lower risk than the optimal invariant predictor.</p>
<p>Finally, in the non-linear case, we show that IRM fails unless the training environments approximately "cover" the space of possible environments, and therefore it behaves similarly to ERM:
Theorem 3.3 (Informal, Non-linear). For arbitrary $f$, there exists a non-linear predictor $\Phi, \hat{\beta}$ which is nearly optimal under the penalized objective and furthermore is nearly identical to the optimal invariant predictor on the training distribution. However, for any test environment with a mean sufficiently different from the training means, this predictor will be equivalent to the ERM solution on nearly all test points. For test distributions where the environmental feature correlations with the label are reversed, this predictor has almost 0 accuracy.</p>
<p>Extensions to other objectives. Many follow-up works have suggested alternatives to IRM—some are described in the next section. Though these objectives perform better on various baselines, there are few formal guarantees and no results beyond the linear case. Due to their collective similarities, we can easily derive corollaries which extend every theorem in this paper to these objectives, demonstrating that they all suffer from the same shortcomings. Appendix E contains example corollaries for each of the results presented in this work.</p>
<h1>4 BACKGROUND ON IRM AND ITS ALTERNATIVES</h1>
<p>During training, a classifier will learn to leverage correlations between features and labels in the training data to make its predictions. If a correlation varies with the environment, it may not be present in future test distributions-worse yet, it may be reversed-harming the classifier's predictive ability. IRM (Arjovsky et al., 2019) is a recently proposed approach to learning environmentally invariant representations to facilitate invariant prediction.</p>
<p>The IRM objective. IRM posits the existence of a feature embedder $\Phi$ such that the optimal classifier on top of these features is the same for every environment. The authors argue that such a function will use only invariant features, since non-invariant features will have different joint distributions with the label and therefore a fixed classifier on top of them won't be optimal in all environments. To learn this $\Phi$, the IRM objective is the following constrained optimization problem:</p>
<p>$$
\min <em _in="\in" _mathcal_E="\mathcal{E" e="e">{\Phi, \hat{\beta}} \quad \frac{1}{|\mathcal{E}|} \sum</em>
$$}} \mathcal{R}^{e}(\Phi, \hat{\beta}) \quad \text { s.t. } \quad \hat{\beta} \in \underset{\beta}{\arg \min } \mathcal{R}^{e}(\Phi, \beta) \quad \forall e \in \mathcal{E</p>
<p>This bilevel program is highly non-convex and difficult to solve. To find an approximate solution, the authors consider a Langrangian form, whereby the sub-optimality with respect to the constraint is expressed as the squared norm of the gradients of each of the inner optimization problems:</p>
<p>$$
\min <em _in="\in" _mathcal_E="\mathcal{E" e="e">{\Phi, \hat{\beta}} \quad \frac{1}{|\mathcal{E}|} \sum</em>\right]
$$}}\left[\mathcal{R}^{e}(\Phi, \hat{\beta})+\lambda\left|\nabla_{\hat{\beta}} \mathcal{R}^{e}(\Phi, \hat{\beta})\right|_{2}^{2</p>
<p>Assuming the inner optimization problem is convex, achieving feasibility is equivalent to the penalty term being equal to 0 . Thus, Equations 4 and 5 are equivalent if we set $\lambda=\infty$.</p>
<p>Alternative objectives. IRM is motivated by the existence of a featurizer $\Phi$ such that $\mathbb{E}[y \mid \Phi(x)]$ is invariant. Follow-up works have proposed variations on this objective, based instead on the strictly stronger desideratum of the invariance of $p(y \mid \Phi(x))$. Krueger et al. (2020) suggest penalizing the variance of the risks, while Xie et al. (2020) give the same objective but taking the square root of the variance. Many papers have suggested similar alternatives (Jin et al., 2020; Mahajan et al., 2020; Bellot \&amp; van der Schaar, 2020). These objectives are compelling-indeed, it is easy to show that the optimal invariant predictor constitutes a stationary point of each of these objectives:
Proposition 4.1. Suppose the observed data are generated according to Equations 1-3. Then the (parametrized) optimal invariant predictor $\Phi^{<em>}, \hat{\beta}^{</em>}$ is a stationary point for Equation 4.</p>
<p>The stationarity of the optimal invariant predictor for the other objectives is a trivial corollary. However, in the following sections we will demonstrate that such a result is misleading and that a more careful investigation is necessary.</p>
<h1>5 The Difficulties of IRM in the Linear Regime</h1>
<p>In their work proposing IRM, Arjovsky et al. (2019) present specific conditions for an upper bound on the number of training environments needed such that a feasible linear featurizer $\Phi$ will have an invariant optimal regression vector $\hat{\beta}$. Our first result is similar in spirit but presents a substantially stronger (and simplified) upper bound in the classification setting, along with a matching lower bound: we demonstrate that observing a large number of environments-linear in the number of environmental features-is necessary for generalization in the linear regime.
Theorem 5.1 (Linear case). Assume $f$ is linear. Suppose we observe E training environments. Then the following hold:</p>
<ol>
<li>Suppose $E&gt;d_{e}$. Consider any linear featurizer $\Phi$ which is feasible under the IRM objective (4), with invariant optimal classifier $\hat{\beta} \neq 0$, and write $\Phi\left(f\left(z_{c}, z_{e}\right)\right)=A z_{c}+B z_{e}$. Then under mild non-degeneracy conditions, it holds that $B=0$. Consequently, $\hat{\beta}$ is the optimal classifier for all possible environments.</li>
<li>If $E \leq d_{e}$ and the environmental means $\mu_{e}$ are linearly independent, then there exists a linear $\Phi$-where $\Phi\left(f\left(z_{c}, z_{e}\right)\right)=A z_{c}+B z_{e}$ with $\operatorname{rank}(B)=d_{e}+1-E$-which is feasible under the IRM objective. Further, both the logistic and 0-1 risks of this $\Phi$ and its corresponding optimal $\hat{\beta}$ are strictly lower than those of the optimal invariant predictor.</li>
</ol>
<p>Similar to Arjovsky et al. (2019), the set of environments which do not satisfy Theorem 5.1 has measure zero under any absolutely continuous density over environmental parameters. Further details, and the full proof, can be found in Appendix C.1. Since the optimal invariant predictor is Bayes with respect to the invariant features, by the data-processing inequality the only way a predictor can achieve lower risk is by relying on environmental features. Thus, Theorem 5.1 directly implies that when $E \leq d_{e}$, the global minimum necessarily uses these non-invariant features and therefore will not universally generalize to unseen environments. On the other hand, in the (perhaps unlikely) case that $E&gt;d_{e}$, any feasible solution will generalize, and the optimal invariant predictor has the minimum (and minimax) risk of all such predictors:
Corollary 5.2. For both logistic and 0-1 loss, the optimal invariant predictor is the global minimum of the IRM objective if and only if $E&gt;d_{e}$.</p>
<p>Let us compare our theoretical findings to those of Arjovsky et al. (2019). Suppose the observations $x$ lie in $\mathbb{R}^{d}$. Roughly, their theorem says that for a learned $\Phi$ of rank $r$ with invariant optimal coefficient $\hat{\beta}$, if the training set contains $d-r+d / r$ "non-degenerate" environments, then $\hat{\beta}$ will be optimal for all environments. There are several important issues with this result: first, they present no result tying the rank of $\Phi$ to their actual objective; their theory thus motivates the objective, but does not provide any performance guarantees for its solution. Next, observe when $x$ is high-dimensional (i.e. $d \gg d_{e}+d_{c}$ )—in which case $\Phi$ will be comparatively low-rank (i.e. $r \leq d_{e}+d_{c}$ )-their result requires $\Omega(d)$ environments, which is extreme. For example, think of images on a low-dimensional manifold embedded in very high-dimensional space. Even when $d=d_{e}+d_{c}$, the "ideal" $\Phi$ which recovers precisely $z_{c}$ would have rank $d_{c}$, and therefore their condition for invariance would require $E&gt;d_{e}+d_{e} / d_{c}$, a stronger requirement than ours; this inequality also seems unlikely to hold in most real-world settings. Finally, they give no lower bound on the number of required environments-prior to this work, there were no existing results for the performance of the IRM objective when their conditions are not met. We also run a simple synthetic experiment to verify our theoretical results, drawing samples according to our model and learning a predictor with the IRM objective. Details and results of this experiment can be found in Appendix C.2. We now sketch a constructive proof of part 2 of the theorem for when $E=d_{e}$ :</p>
<p>Proof Sketch. Since $f$ has an inverse over its range, we can define $\Phi$ as a linear function directly over the latents $\left[z_{c}, z_{e}\right]$. Specifically, define $\Phi(x)=\left[z_{c}, p^{T} z_{e}\right]$. Here, $p$ is a unit-norm vector such that $\forall e \in \mathcal{E}, p^{T} \mu_{e}=\sigma_{e}^{2} \tilde{\mu} ; \tilde{\mu}$ is a fixed scalar that depends on the geometry of $\mu_{e}, \sigma_{e}^{2}$-such a vector exists so long as the means are linearly independent. Observe that this $\Phi$ also has the desired rank. Since this is a linear function of a multivariate Gaussian, the label-conditional distribution of each environment's non-invariant latents has a simple closed form: $p^{T} z_{e} \mid y \sim \mathcal{N}\left(y \cdot p^{T} \mu_{e},|p|<em e="e">{2}^{2} \sigma</em>\right)$.}^{2}\right) \stackrel{d}{=} \mathcal{N}\left(y \cdot \sigma_{e}^{2} \tilde{\mu}, \sigma_{e}^{2</p>
<p>For separating two Gaussians, the optimal linear classifier is $\Sigma^{-1}\left(\mu_{1}-\mu_{0}\right)$-here, the optimal classifier on $p^{T} z_{e}$ is precisely $2 \hat{\mu}$, which does not depend on the environment (and neither do the optimal coefficients for $z_{e}$ ). Though the distribution varies across environments, the optimal classifier is the same! Thus, $\Phi$ directly depends on the environmental features, yet the optimal regression vector $\hat{\beta}$ for each environment is constant. To see that it has lower risk than the optimal invariant predictor, note that this classifier is Bayes with respect to its features and that the optimal invariant predictor uses a strict subset of these features, and therefore it has less information for its predictions.</p>
<p>A purely environmental predictor. The precise value of $\hat{\mu}$ in the proof sketch above represents how strongly this non-invariant feature is correlated with the label. In theory, a predictor that achieves a lower objective value could do so by a very small margin-incorporating an arbitrarily small amount of information from a non-invariant feature would suffice. This result would be less surprising, since achieving low empirical risk might still ensure that we are "close" to the optimal invariant predictor. Our next result shows that this is not the case: there exists a feasible solution which uses only the environmental features yet performs better than the optimal invariant predictor on all $e \in \mathcal{E}$ for which $\hat{\mu}$ is large enough.
Theorem 5.3. Suppose we observe $E \leq d_{e}$ environments, such that all environmental means are linearly independent. Then there exists a feasible $\Phi, \hat{\beta}$ which uses only environmental features and achieves lower 0-1 risk than the optimal invariant predictor on every environment $e$ such that $\sigma_{e} \hat{\mu}&gt;\sigma_{e}^{-1}\left|\mu_{c}\right|<em e="e">{2}$ and $2 \sigma</em>\right|} \hat{\mu} \sigma_{c}^{-1}\left|\mu_{c<em 0="0">{2} \geq\left|\beta</em>\right|$.</p>
<p>The latter of these two conditions is effectively trivial, requiring only a small separation of the means and balance in class labels. From the construction of $\hat{\mu}$ in the proof of Lemma C.1, we can see that the former condition is more likely to be met when $E \ll d_{e}$ and in environments where some non-invariant features are reasonably correlated with the label-both of which can be expected to hold in the high-dimensional setting. Figure C. 2 in the appendix plots the results for a few toy examples for various dimensionalities and variances to see how often this condition holds in practice. For all settings, the number of environments observed before the condition ceases to hold is quite high-on the order of $d_{e}-d_{c}$.</p>
<h1>6 The Failure of IRM in the Non-Linear Regime</h1>
<p>We've demonstrated that OOD generalization is difficult in the linear case, but it is achievable given enough training environments. Our results-and those of Arjovsky et al. (2019)—intuitively proceed by observing that each environment reduces a "degree of freedom" of the solution, such that only the invariant features remain feasible if enough environments are seen. In the non-linear case, it's not clear how to capture this idea of restricting the "degrees of freedom"-and in fact our results imply that this intuition is simply wrong. Instead, we show that the solution generalizes only to test environments that are sufficiently similar to the training environments. Thus, these objectives present no real improvement over ERM or DRO.</p>
<p>Non-linear transformations of the latent variables make it hard to characterize the optimal linear classifier, which makes reasoning about the constrained solution to Equation 4 difficult. Instead we turn our attention to Equation 5, the penalized IRM objective. In this section we demonstrate a foundational flaw of IRM in the non-linear regime: unless we observe enough environments to "cover" the space of non-invariant features, a solution that appears to be invariant can still wildly underperform on a new test distribution. We begin with a definition about the optimality of a coefficient vector $\hat{\beta}$ :
Definition 2. For $0&lt;\gamma&lt;1$, a coefficient vector $\hat{\beta}$ is $\gamma$-close to optimal for a label-conditional feature distribution $z \sim \mathcal{N}(y \cdot \mu, \Sigma)$ if</p>
<p>$$
\hat{\beta}^{T} \mu \geq(1-\gamma) 2 \mu^{T} \Sigma^{-1} \mu
$$</p>
<p>Since the optimal coefficient vector is precisely $2 \Sigma^{-1} \mu$, being $\gamma$-close implies that $\hat{\beta}$ is reasonably aligned with that optimum. Observe that the definition does not account for magnitude-the set of vectors which is $\gamma$-close to optimal is therefore a halfspace which is normal to the optimal vector. One of our results in the non-linear case uses the following assumption, which says that the observed environmental means are sufficiently similar to one another.</p>
<p>Assumption 1. There exists a $0 \leq \gamma&lt;1$ such that the ERM-optimal classifier for the non-invariant features,</p>
<p>$$
\beta_{e ; \text { ERM }}:=\underset{\beta_{e}}{\arg \min } \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} \mathbb{E}<em c="c">{z</em>}, z_{e}, y \sim p^{e}}\left[\ell\left(\sigma\left(\beta_{c}^{T} z_{c}+\hat{\beta<em e="e">{e}^{T} z</em>\right), y\right)\right]
$$}+\beta_{0</p>
<p>is $\gamma$-close to optimal for every environmental feature distribution in $\mathcal{E}$.
This assumption says the environmental distributions are similar enough such that the optimal "average classifier" is reasonably predictive for each environment individually. This is a natural expectation: we are employing IRM precisely because we expect the ERM classifier to do well on the training set but fail to generalize. If the environmental parameters are sufficiently orthogonal, we might instead expect ERM to ignore the features which are not at least moderately predictive across all environments. Finally, we note that if this assumption only holds for a subset of features, our result still applies by marginalizing out the dimensions for which it does not hold.
We are now ready to give our main result in the non-linear regime. We present a simplified version, assuming that that $\sigma_{e}^{2}=1 \forall e$. This is purely for clarity of presentation; the full theorem is presented in Appendix D. We make use of two constants in the following proof-the average squared norm of the environmental means, $\overline{|\mu|}<em _in="\in" _mathcal_E="\mathcal{E" e="e">{2}^{2}:=\frac{1}{E} \sum</em> |}} \mathbb{|} \mu_{e<em _ERM="{ERM" _text="\text">{2}^{2}$; and the standard deviation of the response variable of the ERM-optimal classifier, $\sigma</em>\right|}}:=\sqrt{\left|\beta_{c<em e="e">{2}^{2} \sigma</em>\right|}^{2}+\left|\beta_{e ; E R M<em e="e">{2}^{2} \sigma</em>$.
Theorem 6.1 (Non-linear case, simplified). Suppose we observe $E$ environments $\mathcal{E}=\left{e_{1}, \ldots, e_{E}\right}$, where $\sigma_{e}^{2}=1 \forall e$. Then, for any $\epsilon&gt;1$, there exists a featurizer $\Phi_{\epsilon}$ which, combined with the ERM-optimal classifier $\hat{\beta}=\left[\beta_{c}, \beta_{e ; E R M}, \beta_{0}\right]^{T}$, satisfies the following properties, where we define $p_{\epsilon}:=\exp \left{-d_{e} \min \left(\epsilon-1,(\epsilon-1)^{2}\right) / 8\right}$ :}^{2}</p>
<ol>
<li>The regularization term of $\Phi_{\epsilon}, \hat{\beta}$ as in Equation 5 is bounded as</li>
</ol>
<p>$$
\frac{1}{E} \sum_{e \in \mathcal{E}}\left|\nabla_{\hat{\beta}} \mathcal{R}^{e}\left(\Phi_{\epsilon}, \hat{\beta}\right)\right|<em _epsilon="\epsilon">{2}^{2} \in \mathcal{O}\left(p</em>\right)\right)
$$}^{2}\left(c_{\epsilon} d_{e}+\overline{|\mu|_{2}^{2}</p>
<p>for some constant $c_{\epsilon}$ that depends only on $\epsilon$.
2. $\Phi_{\epsilon}, \hat{\beta}$ exactly matches the optimal invariant predictor on at least a $1-p_{\epsilon}$ fraction of the training set. On the remaining inputs, it matches the ERM-optimal solution.</p>
<p>Further, for any test distribution, suppose its environmental mean $\mu_{E+1}$ is sufficiently far from the training means:</p>
<p>$$
\forall e \in \mathcal{E}, \min <em E_1="E+1">{y \in{ \pm 1}}\left|\mu</em>\right|}-y \cdot \mu_{e<em e="e">{2} \geq(\sqrt{\epsilon}+\delta) \sqrt{d</em>
$$}</p>
<p>for some $\delta&gt;0$, and define $q:=\frac{2 E}{\sqrt{\pi \delta}} \exp \left{-\delta^{2}\right}$. Then the following holds:
3. $\Phi_{\epsilon}, \hat{\beta}$ is equivalent to the ERM-optimal predictor on at least a $1-q$ fraction of the test distribution.
4. Under Assumption 1, suppose it holds that $\mu_{E+1}=-\sum_{e \in \mathcal{E}} \alpha_{e} \mu_{e}$ for some set of coefficients $\left{\alpha_{e}\right}_{e \in \mathcal{E}}$. Then so long as</p>
<p>$$
\sum_{e \in \mathcal{E}} \alpha_{e}\left|\mu_{e}\right|<em c="c">{2}^{2} \geq \frac{\left|\mu</em>\right|<em c="c">{2}^{2} / \sigma</em>
$$}^{2}+\left|\beta_{0}\right| / 2+\sigma_{E R M}}{1-\gamma</p>
<p>the 0-1 risk of $\Phi_{\epsilon}, \hat{\beta}$ on the new environment is greater than $.975-q$.
We give a brief intuition for each of the claims made in this theorem, followed by a sketch of the proof-the full proof can be found in Appendix D.</p>
<ol>
<li>
<p>The first claim says that the predictor we construct will have a gradient squared norm scaling as $p_{c}^{2}$ which is exponentially small in $d_{e}$. Thus, in high dimensions, it will appear as a perfectly reasonable solution to the objective (5).</p>
</li>
<li>
<p>The second claim says that this predictor is identical to the invariant optimal predictor on all but an exponentially small fraction of the training data; on the remaining fraction, it matches the ERM-optimal solution, which has lower risk. The correspondence between constrained and penalized optimization implies that for large enough $d_{e}$, the "fake" predictor will often be a preferred solution. In the finite-sample setting, we would need exponentially many samples to even distinguish between the two!</p>
</li>
<li>The third claim is the crux of the theorem; it says that this predictor we've constructed will completely fail to use invariant prediction on most environments. Recall, the intent of IRM is to perform well precisely when ERM breaks down: when the test distribution differs greatly from the training distribution. Assuming a Gaussian prior on the training environment means, they will have separation in $\mathcal{O}\left(\sqrt{d_{e}}\right)$ with high probability. Observe that $q$ will be vanishingly small so long as $\delta \geq \operatorname{polylog}(E)$. Part 3 says that IRM fails to use invariant prediction on any environment that is slightly outside the high probability region of the prior; even a separation of $\Omega\left(\sqrt{d_{e} \log E}\right)$ suffices. If we expect the new environments to be similar, ERM already guarantees reasonable performance at test-time; thus, IRM fundamentally does not improve over ERM in this regime.</li>
<li>The final statement demonstrates a particularly egregious failure case of this predictor: just like ERM, if the correlation between the non-invariant features and the label reverses at test-time, our predictor will have significantly worse than chance performance.</li>
</ol>
<p>Proof Sketch. We give a construction which is almost identical to the optimal invariant predictor on the training data yet behaves like the ERM solution at test time. We partition the environmental feature space into two sets, $\mathcal{B}, \mathcal{B}^{c} . \mathcal{B}$ is the union of balls centered at each $\mu_{e}$, each with a large enough radius that it contains most of the samples from that environment; thus $\mathcal{B}$ represents the vast majority of the training distribution. On this set, define $\Phi(x)=\left[z_{c}\right]$, so our construction is equal to the optimal invariant predictor. Now consider $\mathcal{B}^{c}=\mathbb{R}^{d_{e}} \backslash \mathcal{B}$. We use standard concentration results to upper bound the measure of $\mathcal{B}^{c}$ under the training distribution by $p$. Next, we show how choosing $\Phi(x)=f^{-1}(x)=\left[z_{c}, z_{e}\right]^{T}$ on this set results in the sub-optimality bound, which is of order $p^{2}$. It is also clear that our constructed predictor is equivalent to the ERM-optimal solution on $\mathcal{B}^{c}$. Thus, our predictor will often have lower empirical risk on $\mathcal{B}^{c}$, countering the regularization penalty.
The second part of the proof shows that while $\mathcal{B}$ has large measure under the training environments, it will have very small measure under any moderately different test environment. We can see this by considering the separation of means (Equation 7); the measure of each ball in $\mathcal{B}$ can be bounded by the measure of the halfspace containing it; if each ball is far enough away from $\mu_{E+1}$, then the total measure of these halfspaces must be small. At test time, our predictor will therefore match the ERM solution on all but $q$ of the observations (part 3). Finally, we lower bound the 0-1 risk of the ERM predictor under such a distribution shift by analyzing the distribution of the response variable. The proof is completed by observing that our predictor's risk can differ from this by at most $q$.</p>
<p>Theorem 6.1 shows that it's possible for the IRM solution to perform poorly on environments which differ even moderately from the training data. We can of course guarantee generalization if the training distributions "cover" (or approximately cover) the full space of environments in order to tie down the performance on future distributions. But in such a scenario, there would no longer be a need for ICP; we could expect ERM or DRO to perform just as well. Once more, we find that our result trivially extends to the alternative objectives; we again refer to Appendix E.</p>
<h1>7 CONCLUSION</h1>
<p>Out-of-distribution generalization is an important direction for future research, and Invariant Causal Prediction remains a promising approach. However, formal results for latent variable models are lacking, particularly in the non-linear setting with fully unobserved covariates. This paper demonstrates that Invariant Risk Minimization and subsequent related works have significant under-explored risks and issues with their formulation. This raises the question: what is the correct formulation for invariant prediction when the observations are complex, non-linear functions of unobserved latent factors? We hope that this work will inspire further theoretical study on the effectiveness of IRM and similar objectives for invariant prediction.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We thank Adarsh Prasad, Jeremy Cohen, and Zack Lipton for helpful feedback. Special thanks to Adarsh Prasad for noticing our initial formatting error. E.R. and P.R. acknowledge the support of NSF via IIS-1909816, IIS-1955532 and ONR via N000141812861.</p>
<h2>REFERENCES</h2>
<p>Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.</p>
<p>J Andrew Bagnell. Robust supervised learning. In Proceedings of the 20th national conference on Artificial intelligence-Volume 2, pp. 714-719, 2005.</p>
<p>Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 456-473, 2018.</p>
<p>Alexis Bellot and Mihaela van der Schaar. Generalization and invariances in the presence of unobserved confounding. arXiv preprint arXiv:2007.10653, 2020.</p>
<p>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010.</p>
<p>Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28. Princeton University Press, 2009.</p>
<p>Steffen Bickel, Michael Brückner, and Tobias Scheffer. Discriminative learning under covariate shift. Journal of Machine Learning Research, 10(9), 2009.</p>
<p>Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24:21782186, 2011.</p>
<p>Kenneth A Bollen. Structural equation models. Encyclopedia of biostatistics, 7, 2005.
Rune Christiansen, Niklas Pfister, Martin Emil Jakobsen, Nicola Gnecco, and Jonas Peters. A causal framework for distribution generalization. arXiv preprint arXiv:2006.07433, 2020.</p>
<p>Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pp. 1180-1189. PMLR, 2015.</p>
<p>Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.</p>
<p>Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.</p>
<p>AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Kun Zhang. Learning causal structures using regression invariance. In Advances in Neural Information Processing Systems, pp. $3011-3021,2017$.</p>
<p>Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Schölkopf. Domain adaptation with conditional transferable components. In International conference on machine learning, pp. 2839-2848, 2016.</p>
<p>Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal prediction for nonlinear models. Journal of Causal Inference, 6(2), 2018.</p>
<p>Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems 32, pp. 125-136. Curran Associates, Inc., 2019.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Domain extrapolation via regret minimization. arXiv preprint arXiv:2006.03908, 2020.</p>
<p>Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domaininvariant representations. arXiv preprint arXiv:1903.03448, 2019.</p>
<p>Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations, 2020.</p>
<p>David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). arXiv preprint arXiv:2003.00688, 2020.</p>
<p>Frank R Kschischang. The complementary error function. 2017. URL https://www. comm. utoronto.ca/frank/notes/erfc.pdf.</p>
<p>Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624-639, 2018.</p>
<p>Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.</p>
<p>Marloes H Maathuis, Markus Kalisch, Peter Bühlmann, et al. Estimating high-dimensional intervention effects from observational data. The Annals of Statistics, 37(6A):3133-3164, 2009.</p>
<p>Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. arXiv preprint arXiv:2006.07500, 2020.</p>
<p>Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via invariant feature representation. volume 28 of Proceedings of Machine Learning Research, pp. 10-18, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL http://proceedings.mir.press/ v28/muandet13.html.</p>
<p>Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science \&amp; Business Media, 2012.</p>
<p>Judea Pearl. Causality. Cambridge university press, 2009.
J Peters, D Janzing, and B Schölkopf. Elements of causal inference-foundations and learning algorithms. 2017.</p>
<p>Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society, 2016.</p>
<p>Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019.</p>
<p>Mateo Rojas-Carulla, Bernhard Schölkopf, Richard Turner, and Jonas Peters. Invariant models for causal transfer learning. The Journal of Machine Learning Research, 19(1):1309-1342, 2018.</p>
<p>Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to interpolation and extrapolation in domain generalization. arXiv preprint arXiv:2102.13128, 2021.</p>
<p>Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In Proceedings of the 37th International Conference on Machine Learning, 2020.</p>
<p>Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 5014-5026. Curran Associates, Inc., 2018.</p>
<p>Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.</p>
<p>Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):60, 2019.</p>
<p>Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural networks. IEEE Transactions on Evolutionary Computation, 23(5):828-841, 2019.</p>
<p>Adarsh Subbaswamy, Peter Schulam, and Suchi Saria. Preventing failures due to dataset shift: Learning predictive models that transport. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 3118-3127. PMLR, 2019.</p>
<p>Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In Advances in neural information processing systems, pp. 5334-5344, 2018.</p>
<p>Chuanlong Xie, Fei Chen, Yue Liu, and Zhenguo Li. Risk variance penalization: From distributional robustness to causality. arXiv preprint arXiv:2006.07544, 2020.</p>
<p>Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. In International Conference on Learning Representations, 2021. URL https: / openreview. net/forum?id=UH-cmocLJC.</p>
<p>Kun Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation: a causal view. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 31503157, 2015.</p>
<p>Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations for domain adaptation. In International Conference on Machine Learning, pp. $7523-7532,2019$.</p>
<h1>A ADDITIONAL NOTATION FOR THE APPENDIX</h1>
<p>To avoid overloading, we use $\phi, F$ for the standard Gaussian PDF and CDF respectively. We write $\mathcal{S}^{c}$ to denote the set complement of a set $\mathcal{S}$. We write $|\cdot|$ to denote entrywise absolute value.</p>
<h2>B Proof of Proposition 4.1</h2>
<p>Recall the IRM objective:</p>
<p>$$
\begin{array}{ll}
\min <em _sim="\sim" _x_="(x," p_x_="p(x," y_="y)">{\Phi, \beta} &amp; \mathbb{E}</em> \Phi(x)\right)\right] \
\text { subject to } &amp; \frac{\partial}{\partial \hat{\beta}} \mathbb{E}_{(x, y) \sim p^{e}}\left[-\log \sigma\left(y \cdot \hat{\beta}^{T} \Phi(x)\right)\right]=0 . \forall e \in \mathcal{E}
\end{array}
$$}\left[-\log \sigma\left(y \cdot \hat{\beta}^{T</p>
<p>Concretely, we represent $\Phi$ as some parametrized function $\Phi_{\theta}$, over whose parameters $\theta$ we then optimize. The derivative of the negative log-likelihood for logistic regression with respect to the $\beta$ coefficients is well known:</p>
<p>$$
\frac{\partial}{\partial \hat{\beta}}\left[-\log \sigma\left(y \cdot \hat{\beta}^{T} \Phi_{\theta}(x)\right)\right]=\left(\sigma\left(\hat{\beta}^{T} \Phi_{\theta}(x)\right)-\mathbf{1}{y=1}\right) \Phi_{\theta}(x)
$$</p>
<p>Suppose we recover the true invariant features $\Phi_{\theta}(x)=\left[\begin{array}{c}z_{c} \ \mathbf{0}\end{array}\right]$ and coefficients $\hat{\beta}=\left[\begin{array}{l}\beta \ \mathbf{0}\end{array}\right]$ (in other words, we allow for the introduction of new features). Then the IRM constraint becomes:</p>
<p>$$
\begin{aligned}
0 &amp; =\frac{\partial}{\partial \hat{\beta}} \mathbb{E}<em _theta="\theta">{(x, y) \sim p^{e}}\left[-\log \sigma\left(y \cdot \hat{\beta}^{T} \Phi</em>(x)\right)\right] \
&amp; =\int_{\mathcal{Z}} p^{e}\left(z_{c}\right) \sum_{y \in{ \pm 1}} p^{e}\left(y \mid z_{c}\right) \frac{\partial}{\partial \hat{\beta}}\left[-\log \sigma\left(y \cdot \beta^{T} z_{c}\right)\right] d z_{c} \
&amp; =\int_{\mathcal{Z}} p^{e}\left(z_{c}\right) \Phi_{\theta}(x)\left[\sigma\left(\hat{\beta}^{T} z_{c}\right)\left(\sigma\left(\hat{\beta}^{T} z_{c}\right)-1\right)+\left(1-\sigma\left(\hat{\beta}^{T} z_{c}\right)\right) \sigma\left(\hat{\beta}^{T} z_{c}\right)\right] d z_{c}
\end{aligned}
$$</p>
<p>Since $\hat{\beta}$ is constant across environments, this constraint is clearly satisfied for every environment, and is therefore also the minimizing $\hat{\beta}$ for the training data as a whole.
Considering now the derivative with respect to the featurizer $\Phi_{\theta}$ :</p>
<p>$$
\frac{\partial}{\partial \theta}\left[-\log \sigma\left(y \cdot \hat{\beta}^{T} \Phi_{\theta}(x)\right)\right]=\left(\sigma\left(\hat{\beta}^{T} \Phi_{\theta}(x)\right)-\mathbf{1}{y=1}\right) \frac{\partial}{\partial \theta} \hat{\beta}^{T} \Phi_{\theta}(x)
$$</p>
<p>Then the derivative of the loss with respect to these parameters is</p>
<p>$$
\int_{\mathcal{Z}} p^{e}\left(z_{c}\right)\left(\frac{\partial}{\partial \theta} \hat{\beta}^{T} \Phi_{\theta}(x)\right)\left[\sigma\left(\beta^{T} z_{c}\right)\left(\sigma\left(\beta^{T} z_{c}\right)-1\right)+\left(1-\sigma\left(\beta^{T} z_{c}\right)\right) \sigma\left(\beta^{T} z_{c}\right)\right] d z_{c}=0
$$</p>
<p>So, the optimal invariant predictor is a stationary point with respect to the feature map parameters as well.</p>
<h2>C ReSults From SECTION 5</h2>
<h2>C. 1 Proof of THEOREM 5.1</h2>
<p>We begin by formally stating the non-degeneracy condition. Consider any environmental mean $\mu_{e}$, and suppose it can be written as a linear combination of the others means with coefficients $\alpha^{e}$ :</p>
<p>$$
\mu_{e}=\sum_{i} \alpha_{i}^{e} \mu_{i}
$$</p>
<p>Then the environments are considered non-degenerate if the following inequality holds for any such set of coefficients:</p>
<p>$$
\sum_{i} \alpha_{i}^{e} \neq 1
$$</p>
<p>and furthermore that the following ratio is different for at least two different environments $a, b$ :</p>
<p>$$
\exists \alpha^{a}, \alpha^{b}, \frac{\sigma_{a}^{2}-\sum_{i} \alpha_{i}^{a} \sigma_{i}^{2}}{1-\sum_{i} \alpha_{i}^{a}} \neq \frac{\sigma_{b}^{2}-\sum_{i} \alpha_{i}^{b} \sigma_{i}^{2}}{1-\sum_{i} \alpha_{i}^{b}}
$$</p>
<p>The first inequality says that none of the environmental means are are an affine combination of the others; in other words, they lie in general linear position, which is the same requirement as Arjovsky et al. (2019). The other inequality is a similarly lax non-degeneracy requirement regarding the relative scale of the variances. It is clear that the set of environmental parameters that do not satisfy Equations 9 and 10 has measure zero under any absolutely continuous density, and similarly, if $E \leq d_{e}$ then the environmental means will be linearly independent almost surely.
We can now proceed with the proof, beginning with some helper lemmas:
Lemma C.1. Suppose we observe $E$ environments $\mathcal{E}=\left{e_{1}, e_{2}, \ldots, e_{E}\right}$, each with environmental mean of dimension $d_{e} \geq E$, such that all environmental means are linearly independent. Then there is a unique unit-norm vector $p$ such that</p>
<p>$$
p^{T} \mu_{e}=\sigma_{e}^{2} \tilde{\mu} \forall e \in \mathcal{E}
$$</p>
<p>where $\tilde{\mu}$ is the largest scalar which admits such a solution.
Proof. Let $v_{1}, v_{2}, \ldots, v_{E}$ be a set of basis vectors for $\operatorname{span}\left{\mu_{1}, \mu_{2}, \ldots, \mu_{E}\right}$. Each mean can then be expressed as a combination of these basis vectors: $u_{i}=\sum_{j=1}^{E} \alpha_{i j} v_{j}$. Since the means are linearly independent, we can combine these coefficients into a single invertible matrix</p>
<p>$$
U=\left[\begin{array}{cccc}
\alpha_{11} &amp; \alpha_{21} &amp; \ldots &amp; \alpha_{E 1} \
\alpha_{12} &amp; \alpha_{22} &amp; \ldots &amp; \alpha_{E 2} \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
\alpha_{1 E} &amp; \alpha_{2 E} &amp; \ldots &amp; \alpha_{E E}
\end{array}\right]
$$</p>
<p>We can then combine the constraints (11) as</p>
<p>$$
U^{T} p_{\alpha}=\boldsymbol{\sigma} \triangleq\left[\begin{array}{c}
\sigma_{1}^{2} \
\sigma_{2}^{2} \
\vdots \
\sigma_{E}^{2}
\end{array}\right]
$$</p>
<p>where $p_{\alpha}$ denotes our solution expressed in terms of the basis vectors $\left{v_{i}\right}_{i=1}^{E}$. This then has the solution</p>
<p>$$
p_{\alpha}=U^{-T} \boldsymbol{\sigma}
$$</p>
<p>This defines the entire space of solutions, which consists of $p_{\alpha}$ plus any element of the remaining $\left(d_{e}-E\right)$-dimensional orthogonal subspace. However, we want $p$ to be unit-norm- observe that the current vector solves Equation 11 with $\tilde{\mu}=1$, which means that after normalizing we get $\tilde{\mu}=\frac{1}{\left|p_{\alpha}\right|_{2}}$. Adding any element of the orthogonal subspace would only increase the norm of $p$, decreasing $\tilde{\mu}$. Thus, the unique maximizing solution is</p>
<p>$$
p_{\alpha}=\frac{U^{-T} \boldsymbol{\sigma}}{\left|U^{-T} \boldsymbol{\sigma}\right|<em 2="2">{2}}, \quad \text { with } \quad \tilde{\mu}=\frac{1}{\left|U^{-T} \boldsymbol{\sigma}\right|</em>
$$}</p>
<p>Finally, $p_{\alpha}$ has to be rotated back into the original space by defining $p=\sum_{i=1}^{E} p_{\alpha i} v_{i}$.
Lemma C.2. Assume $f$ is linear. Suppose we observe $E \leq d_{e}$ environments whose means are linearly independent. Then there exists a linear $\Phi$ with $\operatorname{rank}(\Phi)=d_{e}+d_{e}+1-E$ whose output depends on the environmental features, yet the optimal classifier on top of $\Phi$ is invariant.</p>
<p>Proof. We will begin with the case when $E=d_{e}$ and then show how to modify this construction for when $E&lt;d_{e}$. Consider defining</p>
<p>$$
\Phi=\left[\begin{array}{cc}
I &amp; 0 \
0 &amp; M
\end{array}\right] \circ f^{-1}
$$</p>
<p>with</p>
<p>$$
M=\left[\begin{array}{rrr}
- &amp; p^{T} &amp; - \
- &amp; 0 &amp; - \
&amp; \vdots &amp; \
- &amp; 0 &amp; -
\end{array}\right]
$$</p>
<p>Here, $p \in \mathbb{R}^{d_{c}}$ is defined as the unit-norm vector solution to</p>
<p>$$
p^{T} \mu_{e}=\sigma_{e}^{2} \tilde{\mu} \quad \forall e
$$</p>
<p>such that $\tilde{\mu}$ is maximized-such a vector is guaranteed to exist by Lemma C.1. Thus we get $\Phi(x)=\left[\begin{array}{c}z_{e} \ p^{T} z_{e}\end{array}\right]$, which is of rank $d_{c}+1$ as desired. Define $\tilde{z}<em e="e">{e}=p^{T} z</em>}$, which means that $\tilde{z<em e="e">{e} \mid y \sim$ $\mathcal{N}\left(y \cdot \sigma</em>\right)$. For each environment we have}^{2} \tilde{\mu}, \sigma_{e}^{2</p>
<p>$$
\begin{aligned}
p\left(y \mid z_{c}, \tilde{z}<em c="c">{e}\right) &amp; =\frac{p\left(z</em>}, \tilde{z<em c="c">{e}, y\right)}{p\left(z</em>}, \tilde{z<em c="c">{e}\right)} \
&amp; =\frac{\sigma\left(y \cdot \beta</em>}^{T} z_{c}\right) p\left(\tilde{z<em e="e">{e} \mid y \cdot \sigma</em>}^{e 2} \tilde{\mu}, \sigma_{e}^{2}\right)}{\left[\sigma\left(y \cdot \beta_{c}^{T} z_{c}\right) p\left(\tilde{z<em e="e">{e} \mid y \cdot \sigma</em>}^{e 2} \tilde{\mu}, \sigma_{e}^{2}\right)+\sigma\left(-y \cdot \beta_{c}^{T} z_{c}\right) p\left(\tilde{z<em e="e">{e} \mid-y \cdot \sigma</em> \
&amp; =\frac{\sigma\left(y \cdot \beta_{c}^{T} z_{c}\right) \exp \left(y \cdot \tilde{z}}^{e 2} \tilde{\mu}, \sigma_{e}^{2}\right)\right]<em c="c">{e} \tilde{\mu}\right)}{\left[\sigma\left(y \cdot \beta</em>}^{T} z_{c}\right) \exp \left(y \cdot \tilde{z<em c="c">{e} \tilde{\mu}\right)+\sigma\left(-y \cdot \beta</em>}^{T} z_{c}\right) \exp \left(-y \cdot \tilde{z<em c="c">{e} \tilde{\mu}\right)\right]} \
&amp; =\frac{1}{1+\exp \left(-y \cdot\left(\beta</em>
\end{aligned}
$$}^{T} z_{c}+2 \tilde{z}_{e} \tilde{\mu}\right)\right)</p>
<p>The log-odds of $y$ is linear in these features, so the optimal classifier is</p>
<p>$$
\hat{\beta}=\left[\begin{array}{l}
\beta_{c} \
2 \tilde{\mu}
\end{array}\right]
$$</p>
<p>which is the same for all environments.
Now we show how to modify this construction for when $E&lt;d_{e}$. If we remove one of the environmental means, $\Phi$ trivially maintains its feasibility. Note that since they are linearly independent, the mean which was removed has a component in a direction orthogonal to the remaining means. Call this component $p^{\prime}$, and consider redefining $M$ as</p>
<p>$$
M=\left[\begin{array}{rrr}
- &amp; p^{T} &amp; - \
- &amp; p^{\prime T} &amp; - \
- &amp; 0 &amp; - \
&amp; &amp; \vdots &amp; \
- &amp; 0 &amp; -
\end{array}\right]
$$</p>
<p>The distribution of $\tilde{z}<em c="c">{e}$ in each of the remaining dimensions is normal with mean 0 , which means a corresponding coefficient of 0 is optimal for all environments. So the classifier $\hat{\beta}$ remains optimal for all environments, yet we've added another row to $M$ which increases the dimensionality of its span, and therefore the rank of $\Phi$, by 1 . Working backwards, we can repeat this process for each additional mean, such that $\operatorname{rank}(\Phi)=d</em>-E\right)$, as desired. Note that for $E=1$ any $\Phi$ will be vacuously feasible.}+1+\left(d_{e</p>
<p>Lemma C.3. Suppose we observe $E$ environments $\mathcal{E}=\left{e_{1}, e_{2}, \ldots, e_{E}\right}$ whose parameters satisfy the non-degeneracy conditions $(9,10)$. Let $\Phi(x)=A z_{c}+B z_{e}$ be any feature vector which is a linear function of the invariant and environmental features, and suppose the optimal $\hat{\beta}$ on top of $\Phi$ is invariant. If $E&gt;d_{e}$, then $\hat{\beta}=0$ or $B=0$.</p>
<p>Proof. Write $\Phi=[A \mid B]$ where $A \in \mathbb{R}^{d \times d_{c}}, B \in \mathbb{R}^{d \times d_{e}}$ and define</p>
<p>$$
\begin{aligned}
\bar{\mu}<em e="e">{e} &amp; =\Phi\left[\begin{array}{c}
\mu</em> \
\mu_{e}
\end{array}\right] &amp; =A \mu_{c}+B \mu_{e} \
\bar{\Sigma}<em e="e">{e} &amp; =\Phi\left[\begin{array}{cc}
\sigma</em> &amp; 0 \
0 &amp; \sigma_{e}^{2} I_{d_{e}}
\end{array}\right] \Phi^{T} &amp; =\sigma_{c}^{2} A A^{T}+\sigma_{e}^{2} B B^{T}
\end{aligned}
$$}^{2} I_{d_{c}</p>
<p>Without loss of generality we assume $\bar{\Sigma}$ is invertible (if it is not, we can consider just the subspace in which it is-outside of this space, the features have no variance and therefore cannot carry information about the label). By Lemma F.2, the optimal coefficient for each environment is $2 \bar{\Sigma}<em e="e">{e}^{-1} \bar{\mu}</em>}$. In order for this vector to be invariant, it must be the same across environments; we write it as a constant $\hat{\beta}$. Suppose $\bar{\mu<em e="e">{e}=0$ for some environment $e$-then the claim is trivially true with $\hat{\beta}=0$. We therefore proceed under the assumption that $\bar{\mu}</em>$.
With this fact, we have that $\forall e \in \mathcal{E}$,} \neq 0 \forall e \in \mathcal{E</p>
<p>$$
\begin{gathered}
\hat{\beta}=2\left(\sigma_{c}^{2} A A^{T}+\sigma_{e}^{2} B B^{T}\right)^{-1}\left(A \mu_{c}+B \mu_{e}\right) \
\Longleftrightarrow\left(\sigma_{c}^{2} A A^{T}+\sigma_{e}^{2} B B^{T}\right) \hat{\beta}=2 A \mu_{c}+2 B \mu_{e} \
\Longleftrightarrow \sigma_{e}^{2} B B^{T} \hat{\beta}-2 B \mu_{e}=2 A \mu_{c}-\sigma_{c}^{2} A A^{T} \hat{\beta}
\end{gathered}
$$</p>
<p>Define the vector $\mathbf{v}=2 A \mu_{c}-\sigma_{c}^{2} A A^{T} \hat{\beta}$. We will show that for any $\hat{\beta}, A$, with probability 1 only $B=0$ can satisfy Equation 12 for every environment. If $E&gt;d_{e}$, then there exists at least one environmental mean which can be written as a linear combination of the others. Without loss of generality, denote the parameters of this environment as $\left(\bar{\mu}, \bar{\sigma}^{2}\right)$ and write $\bar{\mu}=\sum_{i=1}^{d_{e}} \alpha_{i} \mu_{i}$. However, note that by assumption the means lie in general linear position, and therefore we actually have at least $d_{e}$ sets of coefficients $\alpha$ for which this holds. Rearranging Equation 12, we get</p>
<p>$$
\begin{aligned}
\bar{\sigma}^{2} B B^{T} \hat{\beta}-\mathbf{v} &amp; =2 B \bar{\mu} \
&amp; =\sum_{i=1}^{d_{e}} \alpha_{i} 2 B \mu_{i} \
&amp; =\sum_{i=1}^{d_{e}} \alpha_{i}\left[\sigma_{i}^{2} B B^{T} \hat{\beta}-\mathbf{v}\right]
\end{aligned}
$$</p>
<p>and rearranging once more yields</p>
<p>$$
\left(\bar{\sigma}^{2}-\sum \alpha_{i} \sigma_{i}^{2}\right) B B^{T} \hat{\beta}=\left(1-\sum \alpha_{i}\right) \mathbf{v}
$$</p>
<p>By assumption, $\left(1-\sum \alpha_{i}\right)$ is non-zero. We can therefore rewrite this as</p>
<p>$$
\hat{\alpha} B B^{T} \hat{\beta}=\mathbf{v}
$$</p>
<p>where $\hat{\alpha}=\frac{\bar{\sigma}^{2}-\sum \alpha_{i} \sigma_{i}^{2}}{1-\sum \alpha_{i}}$ is a scalar. As the vectors $B B^{T} \hat{\beta}$ and $\mathbf{v}$ are both independent of the environment, this can only hold true if $\hat{\alpha}$ is fixed for all environments or if both $B B^{T} \hat{\beta}, \mathbf{v}$ are 0 . The former is false by assumption, so the the latter must hold.
As a result, we see that Equation 12 reduces to</p>
<p>$$
B \mu_{e}=0 \quad \forall e \in \mathcal{E}
$$</p>
<p>As the span of the observed $\mu_{e}$ is all of $\mathbb{R}^{d_{e}}$, this is only possible if $B=0$.</p>
<p>We are now ready to prove the main claim. We restate the theorem here for convenience:
Theorem 5.1 (Linear case). Assume $f$ is linear. Suppose we observe $E$ training environments. Then the following hold:</p>
<ol>
<li>Suppose $E&gt;d_{e}$. Consider any linear featurizer $\Phi$ which is feasible under the IRM objective (4), with invariant optimal classifier $\hat{\beta} \neq 0$, and write $\Phi\left(f\left(z_{c}, z_{e}\right)\right)=A z_{c}+B z_{e}$. Then under mild non-degeneracy conditions, it holds that $B=0$. Consequently, $\hat{\beta}$ is the optimal classifier for all possible environments.</li>
<li>If $E \leq d_{e}$ and the environmental means $\mu_{e}$ are linearly independent, then there exists a linear $\Phi$-where $\Phi\left(f\left(z_{c}, z_{e}\right)\right)=A z_{c}+B z_{e}$ with $\operatorname{rank}(B)=d_{e}+1-E$-which is feasible under the IRM objective. Further, both the logistic and 0-1 risks of this $\Phi$ and its corresponding optimal $\hat{\beta}$ are strictly lower than those of the optimal invariant predictor.</li>
</ol>
<p>Proof. 1. Since $\Phi, f$ are linear, we can write $\Phi(x)=A z_{c}+B z_{e}$ for some matrices $A, B$. Assume the non-degeneracy conditions $(9,10)$ hold. By Lemma C.3, one of $B=0$ or $\hat{\beta}=0$ holds. Thus, $\Phi, \hat{\beta}$ uses only invariant features. Since the joint distribution $p^{e}\left(z_{c}, y\right)$ is invariant, this predictor has identical risk across all environments.
2. The existence of such a predictor is proven by Lemma C.2. It remains to show that the risk of this discriminator is lower than that of the optimal invariant predictor. Observe that these features are non-degenerate independent random variables with support over all of $\mathbb{R}$, and therefore by Lemma F.1, dropping the $\tilde{z}_{e}$ term and using</p>
<p>$$
\Phi(x)=\left[z_{c}\right], \quad \hat{\beta}=\left[\begin{array}{l}
\beta_{e} \
\beta_{0}
\end{array}\right]
$$</p>
<p>results in strictly higher risk. The proof is completed by noting that this definition is precisely the optimal invariant predictor.</p>
<h1>C. 2 EXPERIMENTS FOR THEOREM 5.1</h1>
<p>To corroborate our theoretical findings, we run an experiment on data drawn from our model to see at what point IRM is able to recover a generalizing predictor. We generated data precisely according to our model in the linear setting, with $d_{c}=3, d_{e}=6$. The environmental means were drawn from a multivariate Gaussian prior; we randomly generated the invariant parameters and the parameters of the prior such that using the invariant features gave reasonable accuracy ( $71.9 \%$ ) but the environmental features would allow for almost perfect accuracy on in-distribution test data ( $99.8 \%$ ). Thus, the goal was to see if IRM could successfully learn a predictor which ignores meaningful covariates $z_{e}$, to the detriment of its training performance but to the benefit of OOD generalization. We chose equal class marginals $(\eta=0.5)$.</p>
<p>Figure C. 1 shows the result of five runs of IRM, each with different environmental parameters but the same invariant parameters (the training data itself was redrawn for each run). We found that optimizing for the IRM objective was quite unstable, frequently collapsing to the ERM solution unless $\lambda$ and the optimizer learning rate were carefully tuned. This echoes the results of Krueger et al. (2020) who found that tuning $\lambda$ during training to specific values at precisely the right time is essential for good performance. To prevent collapse, we kept the same environmental prior and found a single setting for $\lambda$ and the learning rate which resulted in reasonable performance across all five runs. At test time, we evaluated the trained predictors on additional, unseen environments whose parameters were drawn from the same prior. To simulate distribution shift, we evaluated the predictors on the same data but with the environmental means negated. Thus the correlations between the environmental features $z_{e}$ and the label $y$ were reversed.</p>
<p>Observe that the results closely track the expected outcome according to Theorem 5.1: up until $E=d_{e}$, IRM essentially matches ERM in performance both in-distribution and under distribution shift. As soon as we cross that threshold of observed environments, the predictor learned via IRM begins to perform drastically better under distribution shift, behaving more like the optimal invariant predictor. We did however observe that occasionally the invariant solution would be found after only $E=d_{e}=6$ environments; we conjecture that this is because at this point the feasible-yet-notinvariant predictor with lower objective value presented in Theorem 5.1 is precisely a single point, as opposed to a multi-dimensional subspace, and therefore might be difficult for the optimizer to find.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure C.1: Performance of predictors learned with IRM (5 different runs) and ERM (dashed lines) on test distributions where the correlation between environmental features and the label is consistent (no shift) or reversed (shift). The dashed green line is the performance of the optimal invariant predictor. Observe that up until $E=d_{e}$, IRM consistently returns a predictor with performance similar to ERM: good generalization without distribution shift, but catastrophic failure when the correlation is reversed. In contrast, once $E&gt;d_{e}$, IRM is able to recover a $\Phi, \hat{\beta}$ with performance similar to that of the invariant optimal predictor.</p>
<h1>C. 3 Proof of Theorem 5.3</h1>
<p>Theorem 5.3. Suppose we observe $E \leq d_{e}$ environments, such that all environmental means are linearly independent. Then there exists a feasible $\Phi, \hat{\beta}$ which uses only environmental features and achieves lower 0-1 risk than the optimal invariant predictor on every environment $e$ such that $\sigma_{e} \tilde{\mu}&gt;\sigma_{e}^{-1}\left|\mu_{e}\right|<em e="e">{2}$ and $2 \sigma</em>\right|} \tilde{\mu} \sigma_{e}^{-1}\left|\mu_{e<em 0="0">{2} \geq\left|\beta</em>\right|$.</p>
<p>Proof. We consider the non-invariant predictor constructed as described in Lemma C.2, but dropping the invariant features and coefficients. By Lemma F.2, the optimal coefficients for the invariant and non-invariant predictors are</p>
<p>$$
\hat{\beta}<em e="e">{\text {caus }}=\left[\begin{array}{c}
2 \sigma</em> \
\beta_{0}
\end{array}\right] \quad \text { and } \quad \hat{\beta}}^{-2} \mu_{e<em 0="0">{\text {non-caus }}=\left[\begin{array}{c}
2 \tilde{\mu} \
\beta</em>
\end{array}\right]
$$</p>
<p>respectively. Therefore, the $0-1$ risk of the optimal invariant predictor is precisely</p>
<p>$$
\begin{aligned}
&amp; \eta \mathbb{P}\left(2 \sigma_{e}^{-2} \mu_{e}^{T} z_{c}+\beta_{0}&lt;0\right)+(1-\eta) \mathbb{P}\left(-2 \sigma_{e}^{-2} \mu_{e}^{T} z_{c}+\beta_{0}&gt;0\right) \
= &amp; \eta F\left(-\sigma_{e}^{-1}\left|\mu_{e}\right|<em 0="0">{2}-\frac{\beta</em>\right|} \sigma_{e}}{2\left|\mu_{e<em e="e">{2}}\right)+(1-\eta) F\left(-\sigma</em>\right|}^{-1}\left|\mu_{e<em 0="0">{2}+\frac{\beta</em>\right)
\end{aligned}
$$} \sigma_{e}}{2\left|\mu_{e}\right|_{2}</p>
<p>where $F$ is the Gaussian CDF. By the same reasoning, the $0-1$ risk of the non-invariant predictor is</p>
<p>$$
\eta F\left(-\sigma_{e} \tilde{\mu}-\frac{\beta_{0}}{2 \sigma_{e} \tilde{\mu}}\right)+(1-\eta) F\left(-\sigma_{e} \tilde{\mu}+\frac{\beta_{0}}{2 \sigma_{e} \tilde{\mu}}\right)
$$</p>
<p>Define $\alpha=\sigma_{e}^{-1}\left|\mu_{e}\right|<em e="e">{2}$ and $\gamma=\sigma</em>$. By monotonicity of the Gaussian CDF, the former risk is higher than the latter if} \tilde{\mu</p>
<p>$$
\begin{aligned}
&amp; \alpha+\frac{\beta_{0}}{2 \alpha} \leq \gamma+\frac{\beta_{0}}{2 \gamma} \
&amp; \alpha-\frac{\beta_{0}}{2 \alpha}&lt;\gamma-\frac{\beta_{0}}{2 \gamma}
\end{aligned}
$$</p>
<p>Without loss of generality, we will prove these inequalities for $\beta_{0} \geq 0$; an identical argument proves it for $\beta_{0}&lt;0$ but with the ' $\leq$ ' and ' $&lt;$ ' swapped.</p>
<p>Suppose $\gamma&gt;\alpha$ (the first condition). Then Equation 14 is immediate. Finally, for Equation 13, observe that</p>
<p>$$
\begin{aligned}
&amp; \gamma+\frac{\beta_{0}}{2 \gamma} \geq \alpha+\frac{\beta_{0}}{2 \alpha} \
\Longleftrightarrow &amp; \gamma-\alpha \geq \frac{\beta_{0}}{2 \alpha}-\frac{\beta_{0}}{2 \gamma}=\frac{(\gamma-\alpha) \beta_{0}}{2 \gamma \alpha} \
\Longleftrightarrow &amp; 2 \gamma \alpha \geq \beta_{0}
\end{aligned}
$$</p>
<p>which is the second condition.</p>
<h1>C. 4 Simulations of Magnitude of Environmental Features</h1>
<p>As discussed in Section 5, analytically quantifying the solution $\hat{\mu}$ to the equation in Lemma C. 1 is difficult; instead, we present simulations to give a sense of how often these conditions would hold in practice.
For each choice of environmental dimension $d_{e}$, we generated a "base" correlation $b \sim \mathcal{N}\left(0, I_{d_{e}}\right)$ as the mean of the prior over environmental means $\mu_{e}$. Each of these $\mu_{e}$ was then drawn from $\mathcal{N}\left(b, 4 I_{d_{e}}\right)$-thus, while they all came from the same prior, the noise in the draw of each $\mu_{e}$ was significantly larger than the bias induced by the prior. We then solved for the precise value $\sigma_{e} \hat{\mu}$, with the same variance $\sigma_{e}^{2}$ for all environments, chosen as a hyperparameter. The shaded area represents a $95 \%$ confidence interval over 20 runs.
The dotted lines are $\sqrt{d_{c}}$. If we imagine the invariant parameters are drawn from a standard Gaussian prior, then this is precisely $\mathbb{E}\left[\sigma_{e}^{-1}\left|\mu_{e}\right|<em e="e">{2}\right]$. Thus, the point where $\sigma</em>$.
} \hat{\mu}$ crosses these dotted lines is approximately how many environments would need to be observed before the non-invariant predictor has higher risk than the optimal invariant predictor. We note that this value is quite large, on the order of $d_{e}-d_{c<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure C.2: Simulations to evaluate $\sigma_{e} \hat{\mu}$ for varying ratios of $\frac{d_{e}}{d_{c}}$. When $\sigma_{e}^{2}=1$ the value closely tracks $\sqrt{d_{e}-E}$, and the crossover point is approximately $d_{e}-\sigma_{e}^{2} d_{c}$. These results imply the conditions of Theorem 5.3 are very likely to hold in the high-dimensional setting.</p>
<h2>D THEOREM 6.1 AND DISCUSSION</h2>
<h2>D. 1 Proof of THEOREM 6.1</h2>
<p>We again begin with helper lemmas.</p>
<p>Our featurizer $\Phi$ is constructed to recover the environmental features only if they fall within a set $\mathcal{B}^{e}$. The following lemma shows that since only the environmental features contribute to the gradient penalty, the penalty can be bounded as a function of the measure and geometry of that set. This is used together with Lemmas F. 3 and F. 4 to bound the overall penalty of our constructed predictor.
Lemma D.1. Suppose we observe environments $\mathcal{E}=\left{e_{1}, e_{2}, \ldots\right}$. Given a set $\mathcal{B} \subseteq \mathbb{R}^{d_{e}}$, consider the predictor defined by Equation 19. Then for any environment $e$, the penalty term of this predictor in Equation 5 is bounded as</p>
<p>$$
\left|\nabla_{\hat{\beta}} \mathcal{R}^{e}(\Phi, \hat{\beta})\right|<em e="e">{2}^{2} \leq\left|\mathbb{P}\left(z</em>
$$} \in \mathcal{B}^{e}\right) \mathbb{E}\left[\left|z_{e}\right| \mid z_{e} \in \mathcal{B}^{e}\right]\right|_{2}^{2</p>
<p>Proof. We write out the precise form of the gradient for an environment $e$ :</p>
<p>$$
\nabla_{\hat{\beta}} \mathcal{R}^{e}(\Phi, \hat{\beta})=\int_{\mathcal{Z}<em e="e">{e} \times \mathcal{Z}</em>\right)
$$}} p^{e}\left(z_{c}, z_{e}\right)\left[\sigma\left(\hat{\beta}^{T} \Phi\left(f\left(z_{c}, z_{e}\right)\right)\right)-p^{e}\left(y=1 \mid z_{c}, z_{e}\right)\right] \Phi\left(f\left(z_{c}, z_{e}\right)\right) d\left(z_{c}, z_{e</p>
<p>Observe that since $z_{c} \Perp z_{e} \mid y$, the optimal invariant coefficients are unchanged, and therefore the gradient in the invariant dimensions is 0 . We can split the gradient in the environmental dimensions into two integrals:</p>
<p>$$
\begin{aligned}
&amp; \int_{\mathcal{Z}<em c="c">{e} \times \mathcal{B}} p^{e}\left(z</em>\right) \
&amp; \quad+\int_{\mathcal{Z}}, z_{e}\right)\left[\sigma\left(\beta_{c}^{T} z_{c}+\beta_{0}\right)-p^{e}\left(y=1 \mid z_{c}, z_{e}\right)\right][0] d\left(z_{c}, z_{e<em c="c">{e} \times \mathcal{B}^{e}} p^{e}\left(z</em>\right)
\end{aligned}
$$}, z_{e}\right)\left[\sigma\left(\beta_{c}^{T} z_{c}+\beta_{e ; \mathrm{ERM}}^{T} z_{e}+\beta_{0}\right)-\sigma\left(\beta_{c}^{T} z_{c}+\beta_{e}^{T} z_{e}+\beta_{0}\right)\right]\left[z_{e}\right] d\left(z_{c}, z_{e</p>
<p>Since the features are 0 within $\mathcal{B}$, the first term reduces to 0 . For the second term, note that $\forall x, y \in \mathbb{R},|\sigma(x)-\sigma(y)| \leq 1$, and therefore</p>
<p>$$
\left|\nabla_{\hat{\beta}} \mathcal{R}^{e}(\Phi, \hat{\beta})\right| \leq \int_{\mathcal{Z}<em c="c">{e} \times \mathcal{B}^{e}} p^{e}\left(z</em>\right)
$$}, z_{e}\right)\left[\left|z_{e}\right|\right] d\left(z_{c}, z_{e</p>
<p>We can marginalize out $z_{c}$, and noting that we want to bound the squared norm,</p>
<p>$$
\begin{aligned}
\left|\nabla_{\hat{\beta}} \mathcal{R}^{e}(\Phi, \hat{\beta})\right|<em _mathcal_B="\mathcal{B">{2}^{2} &amp; \leq\left|\int</em>\right|}^{e}} p^{e}\left(z_{e}\right)\left[\left|z_{e}\right|\right] d z_{e<em e="e">{2}^{2} \
&amp; =\left|\mathbb{P}\left(z</em>
\end{aligned}
$$} \in \mathcal{B}^{e}\right) \mathbb{E}\left[\left|z_{e}\right| \mid z_{e} \in \mathcal{B}^{e}\right]\right|_{2}^{2</p>
<p>This next lemma says that if the environmental mean of the test distribution is sufficiently separated from each of the training means, with high probability a sample from this distribution will fall outside of $\mathcal{B}<em _epsilon="\epsilon">{r}$, and therefore $\Phi</em>$ will be equivalent to the ERM solution.
Lemma D.2. For a set of $E$ environments $\mathcal{E}=\left{e_{1}, e_{2}, \ldots, e_{E}\right}$ and any $\epsilon&gt;1$, construct $\mathcal{B}}, \hat{\beta<em _epsilon="\epsilon">{r}$ as in Equation 18 and define $\Phi</em>}$ using $\mathcal{B<em E_1="E+1">{r}$ as in Equation 19. Suppose we now test on a new environment with parameters $\left(\mu</em>\right)$, and assume Equation 15 holds with parameter $\delta$. Define $k=\min }, \sigma_{E+1}^{2<em e="e">{e \in \mathcal{E}} \frac{\sigma</em>\right}$ over the draw of an observation from this new environment, we have}^{2}}{\sigma_{E+1}^{2}}$. Then with probability $\geq 1-\frac{2 E}{\sqrt{k \pi \delta}} \exp \left{-k \delta^{2</p>
<p>$$
\Phi_{\epsilon}(x)=f^{-1}(x)=\left[\begin{array}{l}
z_{c} \
z_{e}
\end{array}\right]
$$</p>
<p>Proof. By Equation 15 our new environmental mean is sufficiently far away from all the labelconditional means of the training environments. In particular, for any environment $e \in \mathcal{E}$ and any label $y \in{ \pm 1}$, the $\ell_{2}$ distance from that mean to $\mu_{E+1}$ is at least $(\sqrt{\epsilon}+\delta) \sigma_{e} \sqrt{d_{e}}$.
Recall that $\mathcal{B}<em e="e">{r}$ is the union of balls $\pm B</em>$. For each environment $e$, consider constructing the halfspace which is perpendicular to the line connecting}$, where $B_{e}$ is the ball of $\ell_{2}$ radius $\sqrt{\epsilon \sigma_{e}^{2} d_{e}}$ centered at $\mu_{e</p>
<p>$\mu_{e}$ and $\mu_{E+1}$ and tangent to $B_{e}$. This halfspace fully contains $B_{e}$, and therefore the measure of $B_{e}$ is upper bounded by that of the halfspace.</p>
<p>By rotational invariance of the Gaussian distribution, we can rotate this halfspace into one dimension and the measure will not change. The center of the ball is $(\sqrt{\epsilon}+\delta) \sigma_{e} \sqrt{d_{e}}$ away from the mean $\mu_{E+1}$, so accounting for its radius, the distance from the mean to the halfspace is $\delta \sigma_{e} \sqrt{d_{e}}$. The variance of the rotated distribution one dimension is $\sigma_{E+1}^{2} d_{e}$, so the measure of this halfspace is upper bounded by</p>
<p>$$
\begin{aligned}
1-\Phi\left(\frac{\delta \sigma_{e} \sqrt{d_{e}}}{\sqrt{\sigma_{E+1}^{2} d_{e}}}\right) &amp; \leq \Phi(-\sqrt{k} \delta) \
&amp; \leq \frac{1}{\sqrt{k \pi} \delta} \exp \left{-k \delta^{2}\right}
\end{aligned}
$$</p>
<p>using results from Kschischang (2017). There are $2 E$ such balls comprising $\mathcal{B}_{r}$, which can be combined via union bound.</p>
<p>With these two lemmas, we now state the full version of Theorem 6.1, with the main difference being that it allows for any environmental variance.
Theorem D. 3 (Non-linear case, full). Suppose we observe $E$ environments $\mathcal{E}=\left{e_{1}, e_{2}, \ldots, e_{E}\right}$. Then, for any $\epsilon&gt;1$, there exists a featurizer $\Phi_{\epsilon}$ which, combined with the ERM-optimal classifier $\hat{\beta}=\left[\beta_{c}, \beta_{e} ; E R M, \beta_{0}\right]^{T}$, satisfies the following properties, where we define $p_{\epsilon, d_{e}}:=\exp \left{-d_{e} \min ((\epsilon-\right.$ $\left.1),(\epsilon-1)^{2}\right) / 8}$ :</p>
<ol>
<li>Define $\sigma_{\max }^{2}=\max <em e="e">{e} \sigma</em>$ is bounded as}^{2}$. Then the regularization term of $\Phi_{\epsilon}, \hat{\beta</li>
</ol>
<p>$$
\frac{1}{E} \sum_{e \in \mathcal{E}}\left|\nabla_{\hat{\beta}} \mathcal{R}^{e}\left(\Phi_{\epsilon}, \hat{\beta}\right)\right|<em _epsilon_="\epsilon," d__e="d_{e">{2}^{2} \in \mathcal{O}\left(p</em>\right]\right)
$$}}^{2}\left[\epsilon d_{e} \sigma_{\max }^{4} \exp \left{2 \epsilon \sigma_{\max }^{2}\right}+|\mu|_{2}^{2</p>
<ol>
<li>$\Phi_{\epsilon}, \hat{\beta}$ exactly matches the optimal invariant predictor on at least a $1-p_{\epsilon, d_{e}}$ fraction of the training set. On the remaining inputs, it matches the ERM-optimal solution.</li>
</ol>
<p>Further, for any test distribution with environmental parameters $\left(\mu_{E+1}, \sigma_{E+1}^{2}\right)$, suppose the environmental mean $\mu_{E+1}$ is sufficiently far from the training means:</p>
<p>$$
\forall e \in \mathcal{E}, \min <em E_1="E+1">{y \in{ \pm 1}}\left|\mu</em>\right|}-y \cdot \mu_{e<em e="e">{2} \geq(\sqrt{\epsilon}+\delta) \sigma</em>
$$} \sqrt{d_{e}</p>
<p>for some $\delta&gt;0$. Define the constants:</p>
<p>$$
\begin{aligned}
k &amp; =\min <em e="e">{e \in \mathcal{E}} \frac{\sigma</em> \
q &amp; =\frac{2 E}{\sqrt{k \pi} \delta} \exp \left{-k \delta^{2}\right}
\end{aligned}
$$}^{2}}{\sigma_{E+1}^{2}</p>
<p>Then the following holds:
3. $\Phi_{\epsilon}, \hat{\beta}$ is equivalent to the ERM-optimal predictor on at least a $1-q$ fraction of the test distribution.
4. Under Assumption 1, suppose it holds that</p>
<p>$$
\mu_{E+1}=-\sum_{e \in \mathcal{E}} \alpha_{e} \mu_{e}
$$</p>
<p>for some set of coefficients $\left{\alpha_{e}\right}_{e \in \mathcal{E}}$. Then for any $c \in \mathbb{R}$, so long as</p>
<p>$$
\sum_{e \in \mathcal{E}} \alpha_{e} \frac{\left|\mu_{e}\right|<em e="e">{2}^{2}}{\sigma</em>\right|}^{2}} \geq \frac{\left|\mu_{c<em c="c">{2}^{2} / \sigma</em>
$$}^{2}+\left|\beta_{0}\right| / 2+c \sigma_{E R M}}{1-\gamma</p>
<p>the 0-1 risk of $\Phi_{\epsilon}, \hat{\beta}$ is lower bounded by $F(2 c)-q$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Following the terminology of Arjovsky et al. (2019), we refer to the regression vector $\hat{\beta}$ as a "classifier" and the composition of $\Phi, \hat{\beta}$ as a "predictor".&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
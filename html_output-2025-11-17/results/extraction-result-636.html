<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-636 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-636</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-636</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-8b9d77d5e52a70af37451d3db3d32781b83ea054</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8b9d77d5e52a70af37451d3db3d32781b83ea054" target="_blank">On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper analyzes BERT, RoBERTa, and ALBERT, fine-tuned on three commonly used datasets from the GLUE benchmark and shows that the observed instability is caused by optimization difficulties that lead to vanishing gradients.</p>
                <p><strong>Paper Abstract:</strong> Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and a small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on three commonly used datasets from the GLUE benchmark and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than previously proposed approaches. Code to reproduce our results is available online: this https URL .</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e636.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e636.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuning stability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning stability (std. dev. of task performance over random seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The variance (standard deviation) of downstream task performance (accuracy / MCC / F1) across multiple fine-tuning runs with different random seeds; used as the paper's primary measure of stochastic variability in fine-tuning pre-trained transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT, RoBERTa, ALBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Fine-tuning pre-trained transformer language models on GLUE tasks (RTE, MRPC, CoLA, QNLI) and measuring variability across random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random seed (initialization + data order), optimizer configuration (Adam bias correction on/off), learning rate, number of training iterations/epochs (effective LR schedule), warmup schedule, dataset size indirectly via iterations, model architecture differences (BERT vs RoBERTa vs ALBERT).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation of development-set performance (accuracy, F1, MCC). Statistical test: Levene's test for equality of variances (p-values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 1 (25 seeds): RTE std dev — Devlin et al. (2019): 4.5; Lee et al. (2020): 7.9; Ours: 2.7. MRPC std dev — Devlin: 3.9; Lee: 3.8; Ours: 0.8. CoLA std dev — Devlin: 25.6; Lee: 20.9; Ours: 1.8. Levene's test comparing our method to second-best variance yielded p < 0.001 for all three datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Reproducibility assessed via repeated independent runs (random seeds) with measurement of distribution of dev performance (mean, std, max); Levene's test used to compare variances between methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Our proposed baseline both reduced variance (see std devs above) and increased mean performance (e.g., RTE mean improved from 50.9 to 67.3 compared to Devlin et al. (2019)); Levene's test p < 0.001 indicates significantly smaller variance vs the next-best method.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>High sensitivity to random seed; optimization failures (vanishing gradients) causing 'failed runs'; differences in generalization between runs with similar training loss; use of a fixed small number of epochs (so smaller datasets get fewer iterations) changing effective LR schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use Adam with bias correction (implicit warmup), small learning rate (2e-5), linear warmup for first 10% of steps, increase number of iterations substantially (train to ~zero training loss; recommended 20 epochs for small datasets), gradient clipping, and keep other regularization (dropout, weight decay) as typical.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative reductions in std dev (see variability_results). Mean performance also improved (e.g., RTE mean: Devlin 50.9 -> Ours 67.3; MRPC mean: 84.0 -> 90.3; CoLA mean: 45.6 -> 62.1). Levene's test p<0.001 confirms variance reductions are significant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Typically 25 independent random-seed runs; ablation study included ~450 models (many seed/hyperparam combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning stochasticity is large but quantifiable via std dev across seeds; most variability can be reduced by changing the optimization scheme (bias correction, smaller LR, longer training) yielding markedly lower std dev and higher mean performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e636.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e636.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanishing gradients</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanishing gradients in lower layers during fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An observed optimization failure mode where gradient norms in the bottom (earlier) layers of the transformer collapse (become orders-of-magnitude smaller) in failed fine-tuning runs, preventing effective optimization and producing trivial/unimproved training loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (also observed for RoBERTa and ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Layerwise monitoring of ℓ2 gradient norms during fine-tuning on GLUE tasks (RTE shown) to diagnose causes of failed runs.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Optimization dynamics (early training step sizes), absence of effective warmup (e.g., Adam bias correction off), learning rate too large for early steps, random seed interactions that place optimizer in basin with vanishing gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Layerwise ℓ2 gradient norms (plotted on log scale), training loss curves, masked language modeling (MLM) perplexity on WikiText-2 to measure catastrophic forgetting in top layers.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Failed runs show vanishing gradients in bottom layers (gradients ~2 orders of magnitude smaller than successful runs); training loss for failed runs remains near initialization level (close to -ln(1/2)) and does not decrease; top layers still change and MLM perplexity degrades in failed runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Repeated seed runs showing consistent dichotomy of failed vs successful gradient-norm trajectories; loss-surface visualizations (2D) show failed runs converge to a 'bad' valley separated by a barrier.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Pattern of vanishing gradients and failed-training-loss plateau replicated across tasks (RTE, MRPC, CoLA) and across models (BERT, RoBERTa, ALBERT) in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Vanishing gradients make optimization extremely sensitive to early-step effective learning rates and seed; pre-trained weight statistics constrain re-initialization remedies; barriers in loss surface prevent escaping bad valleys once entered.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Reduce effective early-step step-size via Adam bias correction or explicit warmup, use smaller base learning rate, increase number of iterations (train longer with smaller LR), gradient clipping.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Enabling bias correction and/or training longer substantially reduces occurrences of failed runs (quantified via fewer failed runs and lower std dev in final performance—see Table 1 and ablations). Exact layer-norm/gradient magnitude restoration shown qualitatively in gradient plots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>25 seeds (per main experiments); additional ablations across seeds/hyperparams included.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanishing gradients in lower transformer layers are a primary cause of unstable fine-tuning; controlling early-step effective step-size (bias correction/warmup) and training longer mitigates this optimization-induced variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e636.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e636.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adam bias correction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias correction in the Adam optimizer (implicit warmup)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The bias-correction multipliers in Adam reduce the effective step size in early iterations (an implicit warmup); enabling bias correction during fine-tuning reduces early-step divergence/vanishing-gradient issues and improves stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adam: A method for stochastic optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT, RoBERTa, ALBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Optimization for Deep Learning / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Comparing fine-tuning runs with Adam optimizer configured with and without bias correction across learning rates and number of epochs on RTE.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Presence vs absence of Adam bias correction changes implicit early-step effective learning rate, impacting whether runs fall into vanishing-gradient failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Distribution (boxplots) of dev-set accuracy across 25 random seeds for combinations of learning rates and bias-correction on/off.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>For BERT and ALBERT, enabling bias correction yields visibly more stable distributions and allows larger learning rates; disabling bias-correction hurts performance in 3-epoch training. (Figure 5 and ablation results; qualitative and distributional improvements reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Repeated-seed experiments and ablation grid across learning rate and epoch count; Levene's test used elsewhere to confirm variance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Bias correction consistently improved stability in their experiments, particularly for BERT and ALBERT; with bias correction, larger LR could be used without increasing failure rate.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Prior standard fine-tuning recipes (Devlin et al.) omitted bias correction, increasing susceptibility to early-step instability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Enable Adam bias correction (the paper recommends using AdamW with bias correction set True) and/or use explicit learning-rate warmup and smaller base LR.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Enabling bias correction in combination with longer training produced the best performance in ablation; disabling bias correction while training for only 3 epochs clearly hurt performance (ablation Fig.9). Exact numeric effect per hyperparam setting shown in ablation plots and aggregate Table 1 improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>25 seeds per configuration in plotted ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adam bias correction acts like an implicit warmup that stabilizes early optimization; enabling it substantially reduces seed-dependent failures and allows safer use of larger base learning rates during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e636.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e636.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset-size vs iterations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of dataset size being confounded with number of training iterations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The commonly-stated hypothesis that small fine-tuning datasets cause instability is refined: instability arises largely because smaller datasets are often trained for fewer iterations (fixed epochs), and when matched for iterations, the instability largely disappears.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (also experiments on SciTail)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Experimental methodology</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Down-sampling GLUE datasets to 1,000 samples and comparing fine-tuning when training for 3 epochs vs training for the same number of iterations as the full dataset; additional SciTail experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Dataset size per se; but critical confound is number of training iterations (fixed-epochs on smaller sets -> fewer parameter updates -> different effective LR schedule).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Development-set accuracy distributions and count of failed runs across 25 seeds; standard deviation of dev performance.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Training on 1k samples for 3 epochs increased instability and the number of failed runs; when training for the same number of iterations as on the full dataset, variance nearly fully recovered: MRPC and QNLI had no failed runs and CoLA had only one failed run. SciTail (1k samples): Devlin 3 epochs std=17.9 (mean 69.8); Devlin 71 epochs std=0.9 (mean 87.5).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Repeated-seed experiments (25 seeds) comparing epoch/iteration regimes; standard deviation and failed-run counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Matching iteration counts recovers the original variance seen when training on full data, demonstrating that dataset-size per se is not the causal factor—iteration count / effective LR schedule is.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Common practice of reporting a fixed number of epochs irrespective of dataset size leads to fewer updates on smaller datasets and apparent instability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>When fine-tuning on small datasets, increase number of iterations (epochs or steps) to match the update count used on larger datasets; use smaller LR with warmup/bias correction.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Substantial: nearly complete recovery of variance and elimination of failed runs in experiments; SciTail example shows std dev drop from 17.9 to 0.9 when increasing epochs from 3 to 71 on 1k samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>25 seeds per setting; some extended runs (e.g., 71 epochs) also with 25 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small dataset size correlates with instability only because of fewer parameter-updates in typical fixed-epoch training; increasing iterations restores stability and improves mean performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e636.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e636.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Failed run</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Failed fine-tuning run (majority-baseline threshold)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A run is labeled 'failed' if final accuracy on the task is less than or equal to the majority-class baseline for that dataset; used to quantify catastrophic optimization failures during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT, RoBERTa, ALBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Classify individual fine-tuning runs as failed/successful by comparing final dev accuracy to a majority-class baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random seed and optimizer dynamics causing some runs to collapse to trivial majority-class behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Count of runs with final accuracy <= dataset majority baseline (per-task majority baselines provided in Appendix Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Default fine-tuning (Devlin et al.) produced many failed runs on small tasks like RTE; using the proposed baseline nearly eliminated failed runs (e.g., on downsampled experiments and main dataset experiments the number of failed runs dropped substantially).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Repeated-seed experiments and tabulation of failed-run counts across configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Failed-run counts decreased to near zero under the proposed stable fine-tuning recipe (bias correction + small LR + longer training).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Failure threshold depends on dataset majority baseline; some hyperparameter regimes produce all runs below majority baseline (stable but low performance), which is not helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use stable fine-tuning baseline (bias correction, lower LR, more iterations); ablation shows combination reduces failed runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative and count-based improvements reported across experiments (see figures and tables); exact per-task counts available in experiment plots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>25 seeds per reported experimental setting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Defining 'failed runs' relative to the majority baseline allows quantifying catastrophic optimization failures; stable optimization settings dramatically reduce failed-run frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e636.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e636.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stable fine-tuning baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proposed stable fine-tuning baseline: AdamW with bias correction, LR=2e-5, warmup 10%, 20 epochs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple recipe for fine-tuning BERT-family models on small datasets that reduces stochastic variability: use AdamW with bias correction, base learning rate 2e-5, linear warmup for first 10% of steps, train for 20 epochs (instead of 3), and decay LR to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (evaluated also on RoBERTa and ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Model fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Fine-tuning pre-trained transformers on GLUE tasks (RTE, MRPC, CoLA) to demonstrate improvement in stability and mean performance across seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Addresses sources identified: random seed sensitivity, early-step optimizer dynamics, and insufficient iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation, mean, and maximum of development-set performance across 25 seeds; Levene's test for variance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 1 improvements (25 seeds): RTE std reduced to 2.7 (mean 67.3) vs Devlin 4.5 (mean 50.9) and Lee 7.9 (mean 65.3). MRPC std 0.8 (mean 90.3) vs Devlin 3.9 (84.0) and Lee 3.8 (87.8). CoLA std 1.8 (mean 62.1) vs Devlin 25.6 (45.6) and Lee 20.9 (51.9).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Repeated-seed evaluation (25 seeds) and ablation over learning rate, epochs, and bias correction (Fig. 9); Levene's test for variance comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>The baseline substantially reduced variance and improved mean performance across evaluated tasks; Levene's test p < 0.001 vs second-best variance.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Computational cost increased when training many epochs on small datasets (but acceptable since datasets are small); recipe targeted for small-data fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>As above: enable Adam bias correction, use small LR (2e-5), linear warmup of 10% steps, increase epochs to ~20 for small datasets, linear decay to zero, gradient clipping retained.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Large quantitative reduction in std dev across tasks and improved mean performance (see variability_results). On downsampled SciTail, increasing epochs from 3 to 71 reduced std from 17.9 to 0.9 and raised mean from 69.8 to 87.5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>25 seeds for main comparisons; ablation encompassed many combinations totaling ~450 trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A straightforward change to the optimizer configuration and training duration (enable Adam bias correction, lower LR, longer training) is sufficient to make fine-tuning of large pre-trained transformers far more stable and improves average task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping <em>(Rating: 2)</em></li>
                <li>Mixout: Effective regularization to finetune large-scale pretrained language models <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Adam: A method for stochastic optimization <em>(Rating: 2)</em></li>
                <li>Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks <em>(Rating: 1)</em></li>
                <li>On layer normalization in the transformer architecture <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-636",
    "paper_id": "paper-8b9d77d5e52a70af37451d3db3d32781b83ea054",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Fine-tuning stability",
            "name_full": "Fine-tuning stability (std. dev. of task performance over random seeds)",
            "brief_description": "The variance (standard deviation) of downstream task performance (accuracy / MCC / F1) across multiple fine-tuning runs with different random seeds; used as the paper's primary measure of stochastic variability in fine-tuning pre-trained transformers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT, RoBERTa, ALBERT",
            "model_size": null,
            "scientific_domain": "Natural Language Processing",
            "experimental_task": "Fine-tuning pre-trained transformer language models on GLUE tasks (RTE, MRPC, CoLA, QNLI) and measuring variability across random seeds.",
            "variability_sources": "Random seed (initialization + data order), optimizer configuration (Adam bias correction on/off), learning rate, number of training iterations/epochs (effective LR schedule), warmup schedule, dataset size indirectly via iterations, model architecture differences (BERT vs RoBERTa vs ALBERT).",
            "variability_measured": true,
            "variability_metrics": "Standard deviation of development-set performance (accuracy, F1, MCC). Statistical test: Levene's test for equality of variances (p-values reported).",
            "variability_results": "Table 1 (25 seeds): RTE std dev — Devlin et al. (2019): 4.5; Lee et al. (2020): 7.9; Ours: 2.7. MRPC std dev — Devlin: 3.9; Lee: 3.8; Ours: 0.8. CoLA std dev — Devlin: 25.6; Lee: 20.9; Ours: 1.8. Levene's test comparing our method to second-best variance yielded p &lt; 0.001 for all three datasets.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Reproducibility assessed via repeated independent runs (random seeds) with measurement of distribution of dev performance (mean, std, max); Levene's test used to compare variances between methods.",
            "reproducibility_results": "Our proposed baseline both reduced variance (see std devs above) and increased mean performance (e.g., RTE mean improved from 50.9 to 67.3 compared to Devlin et al. (2019)); Levene's test p &lt; 0.001 indicates significantly smaller variance vs the next-best method.",
            "reproducibility_challenges": "High sensitivity to random seed; optimization failures (vanishing gradients) causing 'failed runs'; differences in generalization between runs with similar training loss; use of a fixed small number of epochs (so smaller datasets get fewer iterations) changing effective LR schedule.",
            "mitigation_methods": "Use Adam with bias correction (implicit warmup), small learning rate (2e-5), linear warmup for first 10% of steps, increase number of iterations substantially (train to ~zero training loss; recommended 20 epochs for small datasets), gradient clipping, and keep other regularization (dropout, weight decay) as typical.",
            "mitigation_effectiveness": "Quantitative reductions in std dev (see variability_results). Mean performance also improved (e.g., RTE mean: Devlin 50.9 -&gt; Ours 67.3; MRPC mean: 84.0 -&gt; 90.3; CoLA mean: 45.6 -&gt; 62.1). Levene's test p&lt;0.001 confirms variance reductions are significant.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Typically 25 independent random-seed runs; ablation study included ~450 models (many seed/hyperparam combinations).",
            "key_findings": "Fine-tuning stochasticity is large but quantifiable via std dev across seeds; most variability can be reduced by changing the optimization scheme (bias correction, smaller LR, longer training) yielding markedly lower std dev and higher mean performance.",
            "uuid": "e636.0",
            "source_info": {
                "paper_title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Vanishing gradients",
            "name_full": "Vanishing gradients in lower layers during fine-tuning",
            "brief_description": "An observed optimization failure mode where gradient norms in the bottom (earlier) layers of the transformer collapse (become orders-of-magnitude smaller) in failed fine-tuning runs, preventing effective optimization and producing trivial/unimproved training loss.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT (also observed for RoBERTa and ALBERT)",
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Optimization",
            "experimental_task": "Layerwise monitoring of ℓ2 gradient norms during fine-tuning on GLUE tasks (RTE shown) to diagnose causes of failed runs.",
            "variability_sources": "Optimization dynamics (early training step sizes), absence of effective warmup (e.g., Adam bias correction off), learning rate too large for early steps, random seed interactions that place optimizer in basin with vanishing gradients.",
            "variability_measured": true,
            "variability_metrics": "Layerwise ℓ2 gradient norms (plotted on log scale), training loss curves, masked language modeling (MLM) perplexity on WikiText-2 to measure catastrophic forgetting in top layers.",
            "variability_results": "Failed runs show vanishing gradients in bottom layers (gradients ~2 orders of magnitude smaller than successful runs); training loss for failed runs remains near initialization level (close to -ln(1/2)) and does not decrease; top layers still change and MLM perplexity degrades in failed runs.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Repeated seed runs showing consistent dichotomy of failed vs successful gradient-norm trajectories; loss-surface visualizations (2D) show failed runs converge to a 'bad' valley separated by a barrier.",
            "reproducibility_results": "Pattern of vanishing gradients and failed-training-loss plateau replicated across tasks (RTE, MRPC, CoLA) and across models (BERT, RoBERTa, ALBERT) in their experiments.",
            "reproducibility_challenges": "Vanishing gradients make optimization extremely sensitive to early-step effective learning rates and seed; pre-trained weight statistics constrain re-initialization remedies; barriers in loss surface prevent escaping bad valleys once entered.",
            "mitigation_methods": "Reduce effective early-step step-size via Adam bias correction or explicit warmup, use smaller base learning rate, increase number of iterations (train longer with smaller LR), gradient clipping.",
            "mitigation_effectiveness": "Enabling bias correction and/or training longer substantially reduces occurrences of failed runs (quantified via fewer failed runs and lower std dev in final performance—see Table 1 and ablations). Exact layer-norm/gradient magnitude restoration shown qualitatively in gradient plots.",
            "comparison_with_without_controls": true,
            "number_of_runs": "25 seeds (per main experiments); additional ablations across seeds/hyperparams included.",
            "key_findings": "Vanishing gradients in lower transformer layers are a primary cause of unstable fine-tuning; controlling early-step effective step-size (bias correction/warmup) and training longer mitigates this optimization-induced variability.",
            "uuid": "e636.1",
            "source_info": {
                "paper_title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Adam bias correction",
            "name_full": "Bias correction in the Adam optimizer (implicit warmup)",
            "brief_description": "The bias-correction multipliers in Adam reduce the effective step size in early iterations (an implicit warmup); enabling bias correction during fine-tuning reduces early-step divergence/vanishing-gradient issues and improves stability.",
            "citation_title": "Adam: A method for stochastic optimization",
            "mention_or_use": "use",
            "model_name": "BERT, RoBERTa, ALBERT",
            "model_size": null,
            "scientific_domain": "Optimization for Deep Learning / NLP",
            "experimental_task": "Comparing fine-tuning runs with Adam optimizer configured with and without bias correction across learning rates and number of epochs on RTE.",
            "variability_sources": "Presence vs absence of Adam bias correction changes implicit early-step effective learning rate, impacting whether runs fall into vanishing-gradient failure modes.",
            "variability_measured": true,
            "variability_metrics": "Distribution (boxplots) of dev-set accuracy across 25 random seeds for combinations of learning rates and bias-correction on/off.",
            "variability_results": "For BERT and ALBERT, enabling bias correction yields visibly more stable distributions and allows larger learning rates; disabling bias-correction hurts performance in 3-epoch training. (Figure 5 and ablation results; qualitative and distributional improvements reported.)",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Repeated-seed experiments and ablation grid across learning rate and epoch count; Levene's test used elsewhere to confirm variance differences.",
            "reproducibility_results": "Bias correction consistently improved stability in their experiments, particularly for BERT and ALBERT; with bias correction, larger LR could be used without increasing failure rate.",
            "reproducibility_challenges": "Prior standard fine-tuning recipes (Devlin et al.) omitted bias correction, increasing susceptibility to early-step instability.",
            "mitigation_methods": "Enable Adam bias correction (the paper recommends using AdamW with bias correction set True) and/or use explicit learning-rate warmup and smaller base LR.",
            "mitigation_effectiveness": "Enabling bias correction in combination with longer training produced the best performance in ablation; disabling bias correction while training for only 3 epochs clearly hurt performance (ablation Fig.9). Exact numeric effect per hyperparam setting shown in ablation plots and aggregate Table 1 improvements.",
            "comparison_with_without_controls": true,
            "number_of_runs": "25 seeds per configuration in plotted ablations.",
            "key_findings": "Adam bias correction acts like an implicit warmup that stabilizes early optimization; enabling it substantially reduces seed-dependent failures and allows safer use of larger base learning rates during fine-tuning.",
            "uuid": "e636.2",
            "source_info": {
                "paper_title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Dataset-size vs iterations",
            "name_full": "Effect of dataset size being confounded with number of training iterations",
            "brief_description": "The commonly-stated hypothesis that small fine-tuning datasets cause instability is refined: instability arises largely because smaller datasets are often trained for fewer iterations (fixed epochs), and when matched for iterations, the instability largely disappears.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT (also experiments on SciTail)",
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Experimental methodology",
            "experimental_task": "Down-sampling GLUE datasets to 1,000 samples and comparing fine-tuning when training for 3 epochs vs training for the same number of iterations as the full dataset; additional SciTail experiments.",
            "variability_sources": "Dataset size per se; but critical confound is number of training iterations (fixed-epochs on smaller sets -&gt; fewer parameter updates -&gt; different effective LR schedule).",
            "variability_measured": true,
            "variability_metrics": "Development-set accuracy distributions and count of failed runs across 25 seeds; standard deviation of dev performance.",
            "variability_results": "Training on 1k samples for 3 epochs increased instability and the number of failed runs; when training for the same number of iterations as on the full dataset, variance nearly fully recovered: MRPC and QNLI had no failed runs and CoLA had only one failed run. SciTail (1k samples): Devlin 3 epochs std=17.9 (mean 69.8); Devlin 71 epochs std=0.9 (mean 87.5).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Repeated-seed experiments (25 seeds) comparing epoch/iteration regimes; standard deviation and failed-run counts reported.",
            "reproducibility_results": "Matching iteration counts recovers the original variance seen when training on full data, demonstrating that dataset-size per se is not the causal factor—iteration count / effective LR schedule is.",
            "reproducibility_challenges": "Common practice of reporting a fixed number of epochs irrespective of dataset size leads to fewer updates on smaller datasets and apparent instability.",
            "mitigation_methods": "When fine-tuning on small datasets, increase number of iterations (epochs or steps) to match the update count used on larger datasets; use smaller LR with warmup/bias correction.",
            "mitigation_effectiveness": "Substantial: nearly complete recovery of variance and elimination of failed runs in experiments; SciTail example shows std dev drop from 17.9 to 0.9 when increasing epochs from 3 to 71 on 1k samples.",
            "comparison_with_without_controls": true,
            "number_of_runs": "25 seeds per setting; some extended runs (e.g., 71 epochs) also with 25 seeds.",
            "key_findings": "Small dataset size correlates with instability only because of fewer parameter-updates in typical fixed-epoch training; increasing iterations restores stability and improves mean performance.",
            "uuid": "e636.3",
            "source_info": {
                "paper_title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Failed run",
            "name_full": "Failed fine-tuning run (majority-baseline threshold)",
            "brief_description": "A run is labeled 'failed' if final accuracy on the task is less than or equal to the majority-class baseline for that dataset; used to quantify catastrophic optimization failures during fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT, RoBERTa, ALBERT",
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Evaluation methodology",
            "experimental_task": "Classify individual fine-tuning runs as failed/successful by comparing final dev accuracy to a majority-class baseline.",
            "variability_sources": "Random seed and optimizer dynamics causing some runs to collapse to trivial majority-class behavior.",
            "variability_measured": true,
            "variability_metrics": "Count of runs with final accuracy &lt;= dataset majority baseline (per-task majority baselines provided in Appendix Table 2).",
            "variability_results": "Default fine-tuning (Devlin et al.) produced many failed runs on small tasks like RTE; using the proposed baseline nearly eliminated failed runs (e.g., on downsampled experiments and main dataset experiments the number of failed runs dropped substantially).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Repeated-seed experiments and tabulation of failed-run counts across configurations.",
            "reproducibility_results": "Failed-run counts decreased to near zero under the proposed stable fine-tuning recipe (bias correction + small LR + longer training).",
            "reproducibility_challenges": "Failure threshold depends on dataset majority baseline; some hyperparameter regimes produce all runs below majority baseline (stable but low performance), which is not helpful.",
            "mitigation_methods": "Use stable fine-tuning baseline (bias correction, lower LR, more iterations); ablation shows combination reduces failed runs.",
            "mitigation_effectiveness": "Qualitative and count-based improvements reported across experiments (see figures and tables); exact per-task counts available in experiment plots.",
            "comparison_with_without_controls": true,
            "number_of_runs": "25 seeds per reported experimental setting.",
            "key_findings": "Defining 'failed runs' relative to the majority baseline allows quantifying catastrophic optimization failures; stable optimization settings dramatically reduce failed-run frequency.",
            "uuid": "e636.4",
            "source_info": {
                "paper_title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Stable fine-tuning baseline",
            "name_full": "Proposed stable fine-tuning baseline: AdamW with bias correction, LR=2e-5, warmup 10%, 20 epochs",
            "brief_description": "A simple recipe for fine-tuning BERT-family models on small datasets that reduces stochastic variability: use AdamW with bias correction, base learning rate 2e-5, linear warmup for first 10% of steps, train for 20 epochs (instead of 3), and decay LR to zero.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT (evaluated also on RoBERTa and ALBERT)",
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Model fine-tuning",
            "experimental_task": "Fine-tuning pre-trained transformers on GLUE tasks (RTE, MRPC, CoLA) to demonstrate improvement in stability and mean performance across seeds.",
            "variability_sources": "Addresses sources identified: random seed sensitivity, early-step optimizer dynamics, and insufficient iterations.",
            "variability_measured": true,
            "variability_metrics": "Standard deviation, mean, and maximum of development-set performance across 25 seeds; Levene's test for variance differences.",
            "variability_results": "Table 1 improvements (25 seeds): RTE std reduced to 2.7 (mean 67.3) vs Devlin 4.5 (mean 50.9) and Lee 7.9 (mean 65.3). MRPC std 0.8 (mean 90.3) vs Devlin 3.9 (84.0) and Lee 3.8 (87.8). CoLA std 1.8 (mean 62.1) vs Devlin 25.6 (45.6) and Lee 20.9 (51.9).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Repeated-seed evaluation (25 seeds) and ablation over learning rate, epochs, and bias correction (Fig. 9); Levene's test for variance comparison.",
            "reproducibility_results": "The baseline substantially reduced variance and improved mean performance across evaluated tasks; Levene's test p &lt; 0.001 vs second-best variance.",
            "reproducibility_challenges": "Computational cost increased when training many epochs on small datasets (but acceptable since datasets are small); recipe targeted for small-data fine-tuning.",
            "mitigation_methods": "As above: enable Adam bias correction, use small LR (2e-5), linear warmup of 10% steps, increase epochs to ~20 for small datasets, linear decay to zero, gradient clipping retained.",
            "mitigation_effectiveness": "Large quantitative reduction in std dev across tasks and improved mean performance (see variability_results). On downsampled SciTail, increasing epochs from 3 to 71 reduced std from 17.9 to 0.9 and raised mean from 69.8 to 87.5.",
            "comparison_with_without_controls": true,
            "number_of_runs": "25 seeds for main comparisons; ablation encompassed many combinations totaling ~450 trained models.",
            "key_findings": "A straightforward change to the optimizer configuration and training duration (enable Adam bias correction, lower LR, longer training) is sufficient to make fine-tuning of large pre-trained transformers far more stable and improves average task performance.",
            "uuid": "e636.5",
            "source_info": {
                "paper_title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
            "rating": 2
        },
        {
            "paper_title": "Mixout: Effective regularization to finetune large-scale pretrained language models",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Adam: A method for stochastic optimization",
            "rating": 2
        },
        {
            "paper_title": "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
            "rating": 1
        },
        {
            "paper_title": "On layer normalization in the transformer architecture",
            "rating": 1
        }
    ],
    "cost": 0.01874825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the Stability of Fine-tuning BERT: MisconCEPTIONS, EXPLANATIONS, AND STRONG BASELINES</h1>
<p>Marius Mosbach<br>Spoken Language Systems (LSV)<br>Saarland Informatics Campus, Saarland University<br>mmosbach@lsv.uni-saarland.de</p>
<p>Maksym Andriushchenko<br>Theory of Machine Learning Lab<br>École polytechnique fédérale de Lausanne<br>maksym.andriushchenko@epfl.ch</p>
<h2>Dietrich Klakow</h2>
<p>Spoken Language Systems (LSV)
Saarland Informatics Campus, Saarland University
dietrich.klakow@lsv.uni-saarland.de</p>
<h4>Abstract</h4>
<p>Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches. Code to reproduce our results is available online: https://github.com/uds-lsv/bert-stable-fine-tuning.</p>
<h2>1 INTRODUCTION</h2>
<p>Pre-trained transformer-based masked language models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020) have had a dramatic impact on the NLP landscape in the recent year. The standard recipe for using such models typically involves training a pretrained model for a few epochs on a supervised downstream dataset, which is known as fine-tuning. While fine-tuning has led to impressive empirical results, dominating a large variety of English NLP benchmarks such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), it is still poorly understood. Not only have fine-tuned models been shown to pick up spurious patterns and biases present in the training data (Niven and Kao, 2019; McCoy et al., 2019), but also to exhibit a large training instability: fine-tuning a model multiple times on the same dataset, varying only the random seed, leads to a large standard deviation of the fine-tuning accuracy (Devlin et al., 2019; Dodge et al., 2020).</p>
<p>Few methods have been proposed to solve the observed instability (Phang et al., 2018; Lee et al., 2020), however without providing a sufficient understanding of why fine-tuning is prone to such failure. The goal of this work is to address this shortcoming. More specifically, we investigate the following question:</p>
<p>Why is fine-tuning prone to failures and how can we improve its stability?</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our proposed fine-tuning strategy leads to very stable results with very concentrated development set performance over 25 different random seeds across all three datasets on BERT. In particular, we significantly outperform the recently proposed approach of Lee et al. (2020) in terms of fine-tuning stability.</p>
<p>We start by investigating two common hypotheses for fine-tuning instability: catastrophic forgetting and small size of the fine-tuning datasets and demonstrate that both hypotheses fail to explain fine-tuning instability. We then investigate fine-tuning failures on datasets from the popular GLUE benchmark and show that the observed fine-tuning instability can be decomposed into two separate aspects: (1) optimization difficulties early in training, characterized by vanishing gradients, and (2) differences in generalization late in training, characterized by a large variance of development set accuracy for runs with almost equivalent training loss.</p>
<p>Based on our analysis, we present a simple but strong baseline for fine-tuning pre-trained language models that significantly improves the fine-tuning stability compared to previous works (Fig. 1). Moreover, we show that our findings apply not only to the widely used BERT model but also to more recent pre-trained models such as RoBERTa and ALBERT.</p>
<h1>2 RELATED WORK</h1>
<p>The fine-tuning instability of BERT has been pointed out in various studies. Devlin et al. (2019) report instabilities when fine-tuning BERT ${ }_{\text {LARGE }}$ on small datasets and resort to performing multiple restarts of fine-tuning and selecting the model that performs best on the development set. Recently, Dodge et al. (2020) performed a large-scale empirical investigation of the fine-tuning instability of BERT. They found dramatic variations in fine-tuning accuracy across multiple restarts and argue how it might be related to the choice of random seed and the dataset size.</p>
<p>Few approaches have been proposed to directly address the observed fine-tuning instability. Phang et al. (2018) study intermediate task training (STILTS) before fine-tuning with the goal of improving performance on the GLUE benchmark. They also find that their proposed method leads to improved fine-tuning stability. However, due to the intermediate task training, their work is not directly comparable to ours. Lee et al. (2020) propose a new regularization technique termed Mixout. The authors show that Mixout improves stability during fine-tuning which they attribute to the prevention of catastrophic forgetting.</p>
<p>Another line of work investigates optimization difficulties of pre-training transformer-based language models (Xiong et al., 2020; Liu et al., 2020). Similar to our work, they highlight the importance of the learning rate warmup for optimization. Both works focus on pre-training and we hence view them as orthogonal to our work.</p>
<h2>3 BACKGROUND</h2>
<h3>3.1 DATASETS</h3>
<p>We study four datasets from the GLUE benchmark (Wang et al., 2019b) following previous work studying instability during fine-tuning: CoLA, MRPC, RTE, and QNLI. Detailed statistics for each of the datasets can be found in Section 7.2 in the Appendix.</p>
<p>CoLA. The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is a sentence-level classification task containing sentences labeled as either grammatical or ungrammatical. Fine-tuning on CoLA was observed to be particularly stable in previous work (Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020). Performance on CoLA is reported in Matthew’s correlation coefficient (MCC).</p>
<p>MRPC. The Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) is a sentence-pair classification task. Given two sentences, a model has to judge whether the sentences paraphrases of each other. Performance on MRPC is measured using the $F_{1}$ score.</p>
<p>RTE. The Recognizing Textual Entailment dataset is a collection of sentence-pairs collected from a series of textual entailment challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). RTE is the second smallest dataset in the GLUE benchmark and fine-tuning on RTE was observed to be particularly unstable (Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020). Accuracy is used to measure performance on RTE.</p>
<p>QNLI. The Question-answering Natural Language Inference dataset contains sentence pairs obtained from SQuAD (Rajpurkar et al., 2016). Wang et al. (2019b) converted SQuAD into a sentence pair classification task by forming a pair between each question and each sentence in the corresponding paragraph. The task is to determine whether the context sentence contains the answer to the question, i.e. entails the answer. Accuracy is used to measure performance on QNLI.</p>
<h1>3.2 Fine-tuning</h1>
<p>Unless mentioned otherwise, we follow the default fine-tuning strategy recommended by Devlin et al. (2019): we fine-tune uncased BERT ${ }_{\text {LARGE }}$ (henceforth BERT) using a batch size of 16 and a learning rate of $2 e-5$. The learning rate is linearly increased from 0 to $2 e-5$ for the first $10 \%$ of iterations-which is known as a warmup-and linearly decreased to 0 afterward. We apply dropout with probability $p=0.1$ and weight decay with $\lambda=0.01$. We train for 3 epochs on all datasets and use global gradient clipping. Following Devlin et al. (2019), we use the AdamW optimizer (Loshchilov and Hutter, 2019) without bias correction.</p>
<p>We decided to not show results for BERT $<em _BASE="{BASE" _text="\text">{\text {BASE }}$ since previous works observed no instability when fine-tuning BERT $</em>}}$ which we also confirmed in our experiments. Instead, we show additional results on RoBERTa ${ <em _LARGE-V2="{LARGE-V2" _text="\text">{\text {LARGE }}$ (Liu et al., 2019) and ALBERT ${ }</em>$ (Lan et al., 2020) using the same fine-tuning strategy. We note that compared to BERT, both RoBERTa and ALBERT have slightly different hyperparameters. In particular, RoBERTa uses weight decay with $\lambda=0.1$ and no gradient clipping, and ALBERT does not use dropout. A detailed list of all default hyperparameters for all models can be found in Section 7.3 of the Appendix. Our implementation is based on HuggingFace’s transformers library (Wolf et al., 2019) and is available online: https://github. com/uds-lsv/bert-stable-fine-tuning.}</p>
<p>Fine-tuning stability. By fine-tuning stability we mean the standard deviation of the fine-tuning performance (measured, e.g., in terms of accuracy, MCC or $F_{1}$ score) over the randomness of an algorithm. We follow previous works (Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020) and measure fine-tuning stability using the development sets from the GLUE benchmark. We discuss alternative notions of stability in Section 7.1 in the Appendix.</p>
<p>Failed runs. Following Dodge et al. (2020), we refer to a fine-tuning run as a failed run if its accuracy at the end of training is less or equal to that of a majority classifier on the respective dataset. The majority baselines for all tasks are found in Section 7.2 in the Appendix.</p>
<h2>4 INVESTIGATING PREVIOUS HYPOTHESES FOR FINE-TUNING INSTABILITY</h2>
<p>Previous works on fine-tuning predominantly state two hypotheses for what can be related to finetuning instability: catastrophic forgetting and small training data size of the downstream tasks. Despite the ubiquity of these hypotheses (Devlin et al., 2019; Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020), we argue that none of them has a causal relationship with fine-tuning instability.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Language modeling perplexity for three failed (a) and successful (b) fine-tuning runs of BERT on RTE where we replace the weights of the top- $k$ layers with their pre-trained values. We can observe that it is often sufficient to reset around 10 layers out of 24 to recover back the language modeling abilities of the pre-trained model. (c) shows the average training loss and development accuracy ( $\pm 1$ std) for 10 failed fine-tuning runs on RTE. Failed fine-tuning runs lead to a trivial training loss suggesting an optimization problem.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Development set results on down-sampled MRPC, CoLA, and QNLI using the default fine-tuning scheme of BERT (Devlin et al., 2019). The leftmost boxplot in each sub-figure shows the development accuracy when training on the full training set.</p>
<h1>4.1 DOES CATASTROPHIC FORGETTING CAUSE FINE-TUNING INSTABILITY?</h1>
<p>Catastrophic forgetting (McCloskey and Cohen, 1989; Kirkpatrick et al., 2017) refers to the phenomenon when a neural network is sequentially trained to perform two different tasks, and it loses its ability to perform the first task after being trained on the second. More specifically, in our setup, it means that after fine-tuning a pre-trained model, it can no longer perform the original masked language modeling task used for pre-training. This can be measured in terms of the perplexity on the original training data. Although the language modeling performance of a pre-trained model correlates with its fine-tuning accuracy (Liu et al., 2019; Lan et al., 2020), there is no clear motivation for why preserving the original masked language modeling performance after fine-tuning is important. ${ }^{1}$
In the context of fine-tuning BERT, Lee et al. (2020) suggest that their regularization method has an effect of alleviating catastrophic forgetting. Thus, it is important to understand how exactly catastrophic forgetting occurs during fine-tuning and how it relates to the observed fine-tuning instability. To better understand this, we perform the following experiment: we fine-tune BERT on RTE, following the default strategy by Devlin et al. (2019). We select three successful and three failed fine-tuning runs and evaluate their masked language modeling perplexity on the test set of the WikiText-2 language modeling benchmark (Merity et al., 2016). ${ }^{2}$ We sequentially substitute the top- $k$ layers of the network varying $k$ from 0 (i.e. all layers are from the fine-tuned model) to 24 (i.e. all layers are from the pre-trained model). We show the results in Fig. 2 (a) and (b).
We can observe that although catastrophic forgetting occurs for the failed models (Fig. 2a) — perplexity on WikiText-2 is indeed degraded for $k=0$ - the phenomenon is much more nuanced. Namely, catastrophic forgetting affects only the top layers of the network - in our experiments often around</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>10 out of 24 layers, and the same is however also true for the successfully fine-tuned models, except for a much smaller increase in perplexity.</p>
<p>Another important aspect of our experiment is that catastrophic forgetting typically requires that the model at least successfully learns how to perform the new task. However, this is not the case for the failed fine-tuning runs. Not only is the development accuracy equal to that of the majority classifier, but also the training loss on the fine-tuning task (here RTE) is trivial, i.e. close to $-\ln (1 / 2)$ (see Fig. 2 (c)). This suggests that the observed fine-tuning failure is rather an optimization problem causing catastrophic forgetting in the top layers of the pre-trained model. We will show later that the optimization aspect is actually sufficient to explain most of the fine-tuning variance.</p>
<h1>4.2 DO SMALL TRAINING DATASETS CAUSE FINE-TUNING INSTABILITY?</h1>
<p>Having a small training dataset is by far the most commonly stated hypothesis for fine-tuning instability. Multiple recent works (Devlin et al., 2019; Phang et al., 2018; Lee et al., 2020; Zhu et al., 2020; Dodge et al., 2020; Pruksachatkun et al., 2020) that have observed BERT fine-tuning to be unstable relate this finding to the small number of training examples.</p>
<p>To test if having a small training dataset inherently leads to fine-tuning instability we perform the following experiment: ${ }^{3}$ we randomly sample 1,000 training samples from the CoLA, MRPC, and QNLI training datasets and fine-tune BERT using 25 different random seeds on each dataset. We compare two different settings: first, training for 3 epochs on the reduced training dataset, and second, training for the same number of iterations as on the full training dataset. We show the results in Fig. 3 and observe that training on less data does indeed affect the fine-tuning variance, in particular, there are many more failed runs. However, when we simply train for as many iterations as on the full training dataset, we almost completely recover the original variance of the fine-tuning performance. We also observe no failed runs on MRPC and QNLI and only a single failed run on CoLA which is similar to the results obtained by training on the full training set. Further, as expected, we observe that training on fewer samples affects the generalization of the model, leading to a worse development set performance on all three tasks. ${ }^{4}$</p>
<p>We conclude from this experiment, that the role of training dataset size per se is orthogonal to fine-tuning stability. What is crucial is rather the number of training iterations.</p>
<p>As our experiment shows, the observed increase in instability when training with smaller datasets can rather be attributed to the reduction of the number of iterations (that changes the effective learning rate schedule) which, as we will show in the next section, has a crucial influence on the fine-tuning stability.</p>
<h2>5 DISENTANGLING OPTIMIZATION AND GENERALIZATION IN FINE-TUNING INSTABILITY</h2>
<p>Our findings in Section 4 detail that while both catastrophic forgetting and small size of the datasets indeed correlate with fine-tuning instability, none of them are causing it. In this section, we argue that the fine-tuning instability is an optimization problem, and it admits a simple solution. Additionally, we show that even though a large fraction of the fine-tuning instability can be explained by optimization, the remaining instability can be attributed to generalization issues where fine-tuning runs with the same training loss exhibit noticeable differences in the development set performance.</p>
<h3>5.1 THE ROLE OF OPTIMIZATION</h3>
<p>Failed fine-tuning runs suffer from vanishing gradients. We observed in Fig. 2c that the failed runs have practically constant training loss throughout the training (see Fig. 14 in the Appendix for a comparison with successful fine-tuning). In order to better understand this phenomenon, in Fig. 4 we plot the $\ell_{2}$ gradient norms of the loss function with respect to different layers of BERT, for one</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Gradient norms (plotted on a <em>logarithmic scale</em>) of different layers on RTE for a failed and successful run of BERT fine-tuning. We observe that the failed run is characterized by <em>vanishing gradients</em> in the bottom layers of the network. Additional plots for other weight matrices can be found in the Appendix.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Box plots showing the fine-tuning performance of (a) BERT, (b) RoBERTa, (c) ALBERT for different learning rates $\alpha$ with and without bias correction (BC) on RTE. For BERT and ALBERT, having bias correction leads to more stable results and allows to train using larger learning rates. For RoBERTa, the effect is less pronounced but still visible.</p>
<p>failed and successful fine-tuning run. For the failed run we see large enough gradients only for the top layers and <em>vanishing gradients</em> for the bottom layers. This is in large contrast to the successful run. While we also observe small gradients at the beginning of training (until iteration 70), gradients start to grow as training continues. Moreover, at the end of fine-tuning, we observe gradient norms nearly 2× orders of magnitude larger than that of the failed run. Similar visualizations for additional layers and weights can be found in Fig. 10 in the Appendix. Moreover, we observe the same behavior also for RoBERTa and ALBERT models, and the corresponding figures can be found in the Appendix as well (Fig. 11 and 12).</p>
<p>Importantly, we note that the vanishing gradients we observe during fine-tuning are harder to resolve than the standard <em>vanishing gradient problem</em> (Hochreiter, 1991; Bengio et al., 1994). In particular, common weight initialization schemes (Glorot and Bengio, 2010; He et al., 2015) ensure that the pre-activations of each layer of the network have zero mean and unit variance in expectation. However, we cannot simply modify the weights of a pre-trained model on each layer to ensure this property since this would conflict with the idea of using the pre-trained weights.</p>
<p>Importance of bias correction in ADAM. Following Devlin et al. (2019), subsequent works on fine-tuning BERT-based models use the ADAM optimizer (Kingma and Ba, 2015). A subtle detail of the fine-tuning scheme of Devlin et al. (2019) is that it <em>does not</em> include the bias correction in ADAM. Kingma and Ba (2015) already describe the effect of the bias correction as to reduce the learning rate at the beginning of training. By rewriting the update equations of ADAM as follows, we can clearly see this effect of bias correction:</p>
<p>$$
\begin{aligned}
\alpha_t &amp; \leftarrow \alpha \cdot \sqrt{1 - \beta_{2}^{t}} / (1 - \beta_{1}^{t}), \
\theta_t &amp; \leftarrow \theta_{t-1} - \alpha_t \cdot m_t / (\sqrt{v_t} + \epsilon),
\end{aligned}
$$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: 2D loss surfaces in the subspace spanned by $\delta_{1}=\theta_{f}-\theta_{p}$ and $\delta_{2}=\theta_{s}-\theta_{p}$ on RTE, MRPC, and CoLA. $\theta_{p}, \theta_{f}, \theta_{s}$ denote the parameters of the pre-trained, failed, and successfully trained model, respectively.</p>
<p>Here $m_{t}$ and $v_{t}$ are biased first and second moment estimates respectively. Equation (1) shows that bias correction simply boils down to reducing the original step size $\alpha$ by a multiplicative factor $\sqrt{1-\beta_{2}^{t}} /\left(1-\beta_{1}^{t}\right)$ which is significantly below 1 for the first iterations of training and approaches 1 as the number of training iterations $t$ increases (see Fig. 6). Along the same lines, You et al. (2020) explicitly remark that bias correction in ADAM has a similar effect to the warmup which is widely used in deep learning to prevent divergence early in training (He et al., 2016; Goyal et al., 2017; Devlin et al., 2019; Wong et al., 2020).
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: The bias correction term of ADAM $\left(\beta_{1}=0.9\right.$ and $\left.\beta_{2}=0.999\right)$.</p>
<p>The implicit warmup of ADAM is likely to be an important factor that contributed to its success. We argue that fine-tuning BERT-based language models is not an exception. In Fig. 5 we show the results of fine-tuning on RTE with and without bias correction for BERT, RoBERTa, and ALBERT models. ${ }^{3}$ We observe that there is a significant benefit in combining warmup with bias correction, particularly for BERT and ALBERT. Even though for RoBERTa fine-tuning is already more stable even without bias correction, adding bias correction gives an additional improvement.</p>
<p>Our results show that bias correction is useful if we want to get the best performance within 3 epochs, the default recommendation by Devlin et al. (2019). An alternative solution is to simply train longer with a smaller learning rate, which also leads to much more stable fine-tuning.</p>
<p>We provide a more detailed ablation study in Appendix (Fig. 9) with analogous box plots for BERT using various learning rates, numbers of training epochs, with and without bias correction. Finally, concurrently to our work, Zhang et al. (2021) also make a similar observation about the importance of bias correction and longer training which gives further evidence to our findings.</p>
<p>Loss surfaces. To get further intuition about the fine-tuning failure, we provide loss surface visualizations (Li et al., 2018; Hao et al., 2019) of failed and successful runs when fine-tuning BERT. Denote by $\theta_{p}, \theta_{f}, \theta_{s}$ the parameters of the pre-trained model, failed model, and successfully trained model, respectively. We plot a two-dimensional loss surface $f(\alpha, \beta)=\mathcal{L}\left(\theta_{p}+\alpha \delta_{1}+\beta \delta_{2}\right)$ in the subspace spanned by $\delta_{1}=\theta_{f}-\theta_{p}$ and $\delta_{2}=\theta_{s}-\theta_{p}$ centered at the weights of the pre-trained model $\theta_{p}$. Additional details are specified in Section 7.6 in the Appendix.</p>
<p>Contour plots of the loss surfaces for RTE, MRPC, and CoLA are shown in Fig. 7. They provide additional evidence to our findings on vanishing gradients: for failed fine-tuning runs gradient descent converges to a "bad" valley with a sub-optimal training loss. Moreover, this bad valley is separated from the local minimum (to which the successfully trained run converged) by a barrier (see also Fig. 13 in the Appendix). Interestingly, we observe a highly similar geometry for all three datasets providing further support for our interpretation of fine-tuning instability as a primarily optimization issue.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Development set accuracy for multiple fine-tuning runs on RTE. The models for (a) are trained with 10 different seeds, and models for (b) are taken at the end of the training, and trained with different seeds and hyperparameters.</p>
<h1>5.2 THE ROLE OF GENERALIZATION</h1>
<p>We now turn to the generalization aspects of fine-tuning instability. In order to show that the remaining fine-tuning variance on the development set can be attributed to generalization, we perform the following experiment: we fine-tune BERT on RTE for 20 epochs and show the development set accuracy for 10 successful runs in Fig. 8a. Further, we show in Fig. 8b the development set accuracy vs. training loss of all BERT models fine-tuned on RTE for the full ablation study (shown in Fig. 9 in the Appendix), in total 450 models.</p>
<p>We find that despite achieving close to zero training loss overfitting is not an issue during fine-tuning. This is consistent with previous work (Hao et al., 2019), which arrived at a similar conclusion. Based on our results, we argue that it is even desirable to train for a larger number of iterations since the development accuracy varies considerably during fine-tuning and it does not degrade even when the training loss is as low as $10^{-5}$.</p>
<p>Combining these findings with results from the previous section, we conclude that the fine-tuning instability can be decomposed into two aspects: optimization and generalization. In the next section, we propose a simple solution addressing both issues.</p>
<h2>6 A SIMPLE BUT HARD-TO-BEAT BASELINE FOR FINE-TUNING BERT</h2>
<p>As our findings in Section 5 show, the empirically observed instability of fine-tuning BERT can be attributed to vanishing gradients early in training as well as differences in generalization late in training. Given the new understanding of fine-tuning instability, we propose the following guidelines for fine-tuning transformer-based masked language models on small datasets:</p>
<ul>
<li>Use small learning rates with bias correction to avoid vanishing gradients early in training.</li>
<li>Increase the number of iterations considerably and train to (almost) zero training loss.</li>
</ul>
<p>This leads to the following simple baseline scheme: we fine-tune BERT using ADAM with bias correction and a learning rate of $2 \mathrm{e}-5$. The training is performed for 20 epochs, and the learning rate is linearly increased for the first $10 \%$ of steps and linearly decayed to zero afterward. All other hyperparameters are kept unchanged. A full ablation study on RTE testing various combinations of the changed hyperparameters is presented in Section 7.4 in the Appendix.</p>
<h2>Results.</h2>
<p>Despite the simplicity of our proposed fine-tuning strategy, we obtain strong empirical performance. Table 1 and Fig. 1 show the results of fine-tuning BERT on RTE, MRPC, and CoLA. We compare to the default strategy of Devlin et al. (2019) and the recent Mixout method from Lee et al. (2020). First, we observe that our method leads to a much more stable fine-tuning performance on all three datasets as evidenced by the significantly smaller standard deviation of the final performance. To</p>
<p>Table 1: Standard deviation, mean, and maximum performance on the development set of RTE, MRPC, and CoLA when fine-tuning BERT over 25 random seeds. Standard deviation: lower is better, i.e. fine-tuning is more stable. ${ }^{*}$ denotes significant difference $(p&lt;0.001)$ when compared to the second smallest standard deviation.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>RTE</th>
<th></th>
<th></th>
<th>MRPC</th>
<th></th>
<th></th>
<th>CoLA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>std</td>
<td>mean</td>
<td>max</td>
<td>std</td>
<td>mean</td>
<td>max</td>
<td>std</td>
<td>mean</td>
<td>max</td>
</tr>
<tr>
<td>Devlin et al. (2019)</td>
<td>4.5</td>
<td>50.9</td>
<td>67.5</td>
<td>3.9</td>
<td>84.0</td>
<td>91.2</td>
<td>25.6</td>
<td>45.6</td>
<td>64.6</td>
</tr>
<tr>
<td>Lee et al. (2020)</td>
<td>7.9</td>
<td>65.3</td>
<td>$\mathbf{7 4 . 4}$</td>
<td>3.8</td>
<td>87.8</td>
<td>$\mathbf{9 1 . 8}$</td>
<td>20.9</td>
<td>51.9</td>
<td>64.0</td>
</tr>
<tr>
<td>Ours</td>
<td>$\mathbf{2 . 7}^{*}$</td>
<td>$\mathbf{6 7 . 3}$</td>
<td>71.1</td>
<td>$\mathbf{0 . 8}^{*}$</td>
<td>$\mathbf{9 0 . 3}$</td>
<td>$\mathbf{9 1 . 7}$</td>
<td>$\mathbf{1 . 8}^{*}$</td>
<td>$\mathbf{6 2 . 1}$</td>
<td>$\mathbf{6 5 . 3}$</td>
</tr>
</tbody>
</table>
<p>further validate our claim about the fine-tuning stability, we run Levene's test (Levene, 1960) to check the equality of variances for the distributions of the final performances on each dataset. For all three datasets, the test results in a p-value less than 0.001 when we compare the variances between our method and the method achieving the second smallest variance. Second, we also observe that our method improves the overall fine-tuning performance: in Table 1 we achieve a higher mean value on all datasets and also comparable or better maximum performance on MRPC and CoLA respectively.</p>
<p>Finally, we note that we suggest to increase the number of fine-tuning iterations only for small datasets, and thus the increased computational cost of our proposed scheme is not a problem in practice. Moreover, we think that overall our findings lead to more efficient fine-tuning because of the significantly improved stability which effectively reduces the number of necessary fine-tuning runs.</p>
<h1>7 CONCLUSIONS</h1>
<p>In this work, we have discussed the existing hypotheses regarding the reasons behind fine-tuning instability and proposed a new baseline strategy for fine-tuning that leads to significantly improved fine-tuning stability and overall improved results on commonly used datasets from the GLUE benchmark.</p>
<p>By analyzing failed fine-tuning runs, we find that neither catastrophic forgetting nor small dataset sizes sufficiently explain fine-tuning instability. Instead, our analysis reveals that fine-tuning instability can be characterized by two distinct problems: (1) optimization difficulties early in training, characterized by vanishing gradients, and (2) differences in generalization, characterized by a large variance of development set accuracy for runs with almost equivalent training performance.</p>
<p>Based on our analysis, we propose a simple but strong baseline strategy for fine-tuning BERT which outperforms the previous works in terms of fine-tuning stability and overall performance.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Anna Khokhlova for her help with the language modeling experiments, Cheolhyoung Lee and Jesse Dodge for providing us with details of their works, and Badr Abdullah and Aditya Mogadala for their helpful comments on a draft of this paper.</p>
<p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 232722074 - SFB 1102.</p>
<h2>REFERENCES</h2>
<p>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. 2006. The second pascal recognising textual entailment challenge. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</p>
<p>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166.</p>
<p>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In Proceedings of the Second Text Analysis Conference (TAC 2009).</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, page 177-190, Berlin, Heidelberg. Springer-Verlag.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305.</p>
<p>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE '07, page 1-9, USA. Association for Computational Linguistics.</p>
<p>Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256.</p>
<p>Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour.</p>
<p>Yaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visualizing and understanding the effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 4143-4152, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $770-778$.</p>
<p>Sepp Hochreiter. 1991. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universität München, 91(1).</p>
<p>Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations.</p>
<p>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, and et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.</p>
<p>Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. 2020. Mixout: Effective regularization to finetune large-scale pretrained language models. In International Conference on Learning Representations.</p>
<p>Howard Levene. 1960. Robust tests for equality of variances. Contributions to probability and statistics: Essays in honor of Harold Hotelling, pages 278-292.</p>
<p>Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. Visualizing the loss landscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 6389-6399. Curran Associates, Inc.</p>
<p>Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. 2020. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.</p>
<p>Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109-165. Elsevier.
R. Thomas McCoy, Junghyun Min, and Tal Linzen. 2020. BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 217-227, Online. Association for Computational Linguistics.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.</p>
<p>Timothy Niven and Hung-Yu Kao. 2019. Probing neural network comprehension of natural language arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jason Phang, Thibault Févry, and Samuel R Bowman. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088.</p>
<p>Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231-5247, Online. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3266-3280. Curran Associates, Inc.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Transformers: State-of-theart natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Eric Wong, Leslie Rice, and J. Zico Kolter. 2020. Fast is better than free: Revisiting adversarial training. In International Conference on Learning Representations.</p>
<p>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. arXiv preprint arXiv:2002.04745.</p>
<p>Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2020. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations.</p>
<p>Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. 2021. Revisiting few-sample BERT fine-tuning. In International Conference on Learning Representations.</p>
<p>Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2020. Freelb: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations.</p>
<h1>APPENDIX</h1>
<h3>7.1 ALTERNATIVE NOTIONS OF STABILITY</h3>
<p>Here, we elaborate on other possible definitions of fine-tuning stability. The definition that we use throughout the paper follows the previous work (Phang et al., 2018; Dodge et al., 2020; Lee et al., 2020). For example, while Dodge et al. (2020) do not directly define fine-tuning stability, they report and analyze the standard deviation of the validation performance (e.g., see Section 4.1 of their paper). Along the same lines, an earlier work of Phang et al. (2018), which studies the influence of intermediate fine-tuning, discusses the variance of the validation performance (see Section 4: Results, paragraph Fine-Tuning Stability therein) and shows the standard deviation over multiple random seeds in Figure 1.</p>
<p>For simplicity, let us assume that the performance metric is accuracy and we have two classes. Let $A$ be a randomized fine-tuning algorithm that produces a classifier $f_{A}$, and let us denote data points as $(x, y) \sim \mathcal{D}$ where $\mathcal{D}$ is the data-generating distribution. Our definition of fine-tuning stability can be formalized as follows:</p>
<p>$$
S_{\text {ours }}(A)=\operatorname{Var}<em x_="x," y="y">{A}\left[\mathrm{E}</em>}\left[\mathbb{1<em A="A">{f</em>}(x)=y}\right]\right]=\operatorname{Var<em A="A">{A}\left[\operatorname{Accuracy}\left(f</em>\right)\right]
$$</p>
<p>This definition directly measures the variance of the performance metric and aims to answer the question: If we perform fine-tuning multiple times, how large will the difference in performance be?
An alternative definition of fine-tuning stability that could be considered is per-point stability where the expectation and variance are interchanged:</p>
<p>$$
S_{\text {per-point }}(A)=\mathrm{E}<em A="A">{x, y}\left[\operatorname{Var}</em>}\left[\mathbb{1<em A="A">{f</em>\right]\right]
$$}(x)=y</p>
<p>This definition captures a different notion of stability. Namely, it captures stability per data point by measuring how much the classifiers $f_{A}$ differ on the same point $x$ given label $y$. Studying the per-point fine-tuning stability can be useful to better understand the properties of fine-tuned models and we refer to McCoy et al. (2020) for a study in this direction.</p>
<h3>7.2 TASK STATISTICS</h3>
<p>Statistics for each of the datasets studied in this paper are shown in Table 2. All datasets are publicly available. The GLUE datasets can be downloaded here: https://github.com/nyu-mll/ jiant. SciTail is available at https://github.com/allenai/scitail.</p>
<p>Table 2: Dataset statistics and majority baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">RTE</th>
<th style="text-align: left;">MRPC</th>
<th style="text-align: left;">CoLA</th>
<th style="text-align: left;">QNLI</th>
<th style="text-align: left;">SciTail</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: left;">2491</td>
<td style="text-align: left;">3669</td>
<td style="text-align: left;">8551</td>
<td style="text-align: left;">104744</td>
<td style="text-align: left;">23596</td>
</tr>
<tr>
<td style="text-align: left;">Development</td>
<td style="text-align: left;">278</td>
<td style="text-align: left;">409</td>
<td style="text-align: left;">1043</td>
<td style="text-align: left;">5464</td>
<td style="text-align: left;">1304</td>
</tr>
<tr>
<td style="text-align: left;">Majority baseline</td>
<td style="text-align: left;">0.53</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">Acc.</td>
<td style="text-align: left;">F1 score</td>
<td style="text-align: left;">MCC</td>
<td style="text-align: left;">Acc.</td>
<td style="text-align: left;">Acc.</td>
</tr>
</tbody>
</table>
<h3>7.3 HYPERPARAMETERS</h3>
<p>Hyperparameters for BERT, RoBERTa, and ALBERT used for all our experiments are shown in Table 3 .</p>
<h3>7.4 Ablation STUDIES</h3>
<p>Figure 9 shows the results of fine-tuning on RTE with different combinations of learning rate, number of training epochs, and bias correction. We make the following observations:</p>
<ul>
<li>When training for only 3 epochs, disabling bias correction clearly hurts performance.</li>
<li>With bias correction, training with larger learning rates is possible.</li>
<li>Combining the usage of bias correction with training for more epochs leads to the best performance.</li>
</ul>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Full ablation of fine-tuning BERT on RTE. For each setting, we vary only the number of training steps, learning rate, and usage of bias correction (BC). All other hyperparameters are unchanged. We fine-tune 25 models for each setting. $\star$ shows the setting which we recommend as a new baseline fine-tuning strategy.</p>
<p>Table 3: Hyperparameters used for fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparam</th>
<th style="text-align: left;">BERT</th>
<th style="text-align: left;">RoBERTa</th>
<th style="text-align: left;">ALBERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">$3,10,20$</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: left;">$1 \mathrm{e}-5-5 \mathrm{e}-5$</td>
<td style="text-align: left;">$1 \mathrm{e}-5-3 \mathrm{e}-5$</td>
<td style="text-align: left;">$1 \mathrm{e}-5-3 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate schedule</td>
<td style="text-align: left;">warmup-linear</td>
<td style="text-align: left;">warmup-linear</td>
<td style="text-align: left;">warmup-linear</td>
</tr>
<tr>
<td style="text-align: left;">Warmup ratio</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">16</td>
</tr>
<tr>
<td style="text-align: left;">Adam $\epsilon$</td>
<td style="text-align: left;">$1 \mathrm{e}-6$</td>
<td style="text-align: left;">$1 \mathrm{e}-6$</td>
<td style="text-align: left;">$1 \mathrm{e}-6$</td>
</tr>
<tr>
<td style="text-align: left;">Adam $\beta_{1}$</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Adam $\beta_{2}$</td>
<td style="text-align: left;">0.999</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">Adam bias correction</td>
<td style="text-align: left;">${$ True, False $}$</td>
<td style="text-align: left;">${$ True, False $}$</td>
<td style="text-align: left;">${$ True, False $}$</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Clipping gradient norm</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Number of random seeds</td>
<td style="text-align: left;">25</td>
<td style="text-align: left;">25</td>
<td style="text-align: left;">25</td>
</tr>
</tbody>
</table>
<h1>7.5 ADDITIONAL GRADIENT NORM VISUALIZATIONS</h1>
<p>We provide additional visualizations for the vanishing gradients observed when fine-tuning BERT, RoBERTa, and ALBERT in Figures 10, 11, 12. Note that for ALBERT besides the pooler and classification layers, we plot only the gradient norms of a single hidden layer (referred to as layer0) because of weight sharing.
Gradient norms and MLM perplexity. We can see from the gradient norm visualizations for BERT in Figures 4 and 10 that the gradient norm of the pooler and classification layer remains large. Hence, even though the gradients on most layers of the model vanish, we still update the weights on the top layers. In fact, this explains the large increase in MLM perplexity for the failed models which is shown in Fig. 2a. While most of the layers do not change as we continue training, the top layers of the network change dramatically.</p>
<h3>7.6 LOSS SURFACES</h3>
<p>For Fig. 7, we define the range for both $\alpha$ and $\beta$ as $[-1.5,1.5]$ and sample 40 points for each axis. We evaluate the loss on 128 samples from the training dataset of each task using all model parameters, including the classification layer. We disabled dropout for generating the surface plots.</p>
<p>Fig. 13 shows contour plots of the total gradient norm. We can again see that the point to which the failed model converges to $\left(\theta_{f}\right)$ is separated from the point the successful model converges to $\left(\theta_{s}\right)$ by a barrier. Moreover, on all the three datasets we can clearly see the valley around $\theta_{f}$ with a small gradient norm.</p>
<h3>7.7 TRAINING CURVES</h3>
<p>Fig. 14 shows training curves for 10 successful and 10 failed fine-tuning runs on RTE. We can clearly observe that all 10 failed runs have a common pattern: throughout the training, their training loss stays close to that at initialization. This implies an optimization problem and suggests to reconsider the optimization scheme.</p>
<h3>7.8 ADDITIONAL FINE-TUNING RESULTS</h3>
<p>We report additional fine-tuning results on the SciTail dataset (Khot et al., 2018) in Table 4 to demonstrate that our findings generalize to datasets from other domains.</p>
<p>Due to its comparatively large size ( 28 k training samples) fine-tuning on SciTail with the Devlin et al. (2019) scheme is already very stable even when trained for 3 epochs. This is comparable to what we find for QNLI in Section 4.1. When applying our fine-tuning scheme to SciTail, the results are very close to that of Devlin et al. (2019). On the other hand, when training on a smaller subset of SciTail</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Gradient norms (plotted on a logarithmic scale) of additional weight matrices of BERT fine-tuned on RTE. Corresponding layer names are in the captions. We show gradient norms corresponding to a single failed and single successful, respectively.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Gradient norms (plotted on a logarithmic scale) of additional weight matrices of RoBERTa fine-tuned on RTE. Corresponding layer names are in the captions. We show gradient norms corresponding to a single failed and single successful, respectively.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Gradient norms (plotted on a logarithmic scale) of additional weight matrices of ALBERT fine-tuned on RTE. Corresponding layer names are in the captions. We show gradient norms corresponding to a single failed and single successful, respectively.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: 2D gradient norm surfaces in the subspace spanned by $\delta_{1}=\theta_{f}-\theta_{p}$ and $\delta_{2}=\theta_{s}-\theta_{p}$ for BERT fine-tuned on RTE, MRPC and CoLA. $\theta_{p}, \theta_{f}, \theta_{s}$ denote the parameters of the pre-trained, failed, and successfully trained model, respectively.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: The test accuracy and training loss of (a) 10 successful runs with our fine-tuning scheme and (b) 10 failed runs with fine-tuning scheme Devlin on RTE. Solid line shows the mean, error bars show $\pm 1$ std.</p>
<p>Table 4: Standard deviation, mean, and maximum performance on the development set of SciTail when finetuning BERT over 25 random seeds. Standard deviation: lower is better, i.e. fine-tuning is more stable.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">SciTail</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">$\max$</td>
</tr>
<tr>
<td style="text-align: left;">Devlin et al. (2019), full train set, 3 epochs</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">96.0</td>
</tr>
<tr>
<td style="text-align: left;">Devlin et al. (2019), 1k samples, 3 epochs</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">89.4</td>
</tr>
<tr>
<td style="text-align: left;">Devlin et al. (2019), 1k samples, 71 epochs</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">89.1</td>
</tr>
<tr>
<td style="text-align: left;">Ours, full train set</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">96.8</td>
</tr>
</tbody>
</table>
<p>(1k training samples) we can clearly see the same results as also observed in Fig. 3 for MRPC, CoLA, and QNLI, i.e. using more training iterations improves the fine-tuning stability.</p>
<p>We conclude from this experiment that our findings and guidelines generalize to datasets from other domains as well. This gives further evidence that the observed instability is not a property of any particular dataset but rather a result of too few training iterations based on the common fine-tuning practice of using a fixed number of epochs (and not iterations) independently of the training data size.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Some of the hyperparameter settings lead to a small fine-tuning variance where all runs lead to a performance below the majority baseline. Obviously, such fine-tuning stability is of limited use.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
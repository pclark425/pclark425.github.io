<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2481 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2481</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2481</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-261076132</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.11787v3.pdf" target="_blank">HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses</a></p>
                <p><strong>Paper Abstract:</strong> Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here, we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate improved seed samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2481.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2481.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HypBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilevel Bayesian optimization method that injects multiple expert-formulated hypotheses (as constrained subspaces) as local Gaussian-process surrogates to generate seed samples that augment a global BO surrogate; hypotheses are evaluated and pruned dynamically using a multi-armed-bandit style selection and plateau-based convergence rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HypBO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>HypBO organizes optimization as a bilevel process: a lower level fits local Gaussian Process (GP) models within expert-specified hypothesis subspaces H_j (hyperrectangles or linear constraints) and runs local BO to produce a set of top-performing seed samples; an upper level maintains a global GP over the full search space X and runs global BO. Hypotheses are treated as soft constraints; seeds from promising hypotheses augment the global dataset. Hypothesis selection is implemented implicitly as a multi-armed-bandit (MAB) where hypotheses are arms; the method alternates between lower- and upper-level optimization until plateauing criteria are met. Convergence/switching is controlled by parameters l_max (lower-level plateau length), u_max (upper-level plateau length), γ (improvement threshold), T (number of local seeds to keep), and a hard iteration budget i_max. Initialisation guarantees at least one random sample per hypothesis and extra random samples for diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery / experimental chemistry (general black-box scientific optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experimental budget between a lower-level pool of hypothesis-local BO runs and an upper-level global BO run. At each outer iteration the lower level proposes T top seeds (selected by maximizing a local acquisition α_ϕj) from each hypothesis GP; the upper level then evaluates global acquisition α on the augmented dataset and runs until it plateaus. Allocation between lower and upper is governed by plateau counters (l_max for lower, u_max for upper) and improvement threshold γ; hypotheses are implicitly selected via a MAB-style policy where hypotheses that produce better seeds are favored and weaker hypotheses are de-prioritized (but not permanently excluded). Initial allocation ensures one sample per hypothesis and remaining initial budget drawn uniformly to preserve diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>number of objective evaluations / iterations (i_max), and internal GP model fits per local hypothesis and global GP; the paper does not report FLOPs or wall-clock timings explicitly (computational cost is managed via counts of iterations and GP fits).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Standard BO acquisition functions (e.g., Expected Improvement or other α(x,D)) used to score candidate seeds and select evaluations; expected improvement (EI) is used in selection/analysis in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Bilevel alternation: the lower level focuses exploitation of hypothesis subspaces via local BO (exploitation of expert priors) and the upper level performs global BO to enable exploration; switching and time spent in each are controlled by l_max, u_max and γ so the system can allocate more budget to promising hypotheses or to global exploration if hypotheses are weak. MAB-style implicit selection of hypotheses decides which hypothesis-local BOs to continue based on seed utility.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initialisation guarantees one random sample per hypothesis plus additional random samples from the whole space to preserve diversity; seeds are chosen from multiple hypotheses (not a single prior), and weaker hypotheses are pruned rather than hard-removed to allow occasional re-evaluation, encouraging hypothesis diversity over time.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments / iteration budget (i_max) and per-level plateau counters (l_max, u_max) acting as implicit budget controls.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Enforces a hard iteration limit i_max and uses plateau-based stopping: lower-level generates seeds until no significant improvement over y_max for l_max consecutive attempts; the upper-level continues until u_max consecutive non-significant improvements. γ defines what counts as significant improvement. These parameters allocate experimental budget adaptively between hypothesis-focused and global exploration phases.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective value (y_max) / reduction in regret (simple and cumulative regret); in materials application measured as hydrogen evolution rate (HER) for photocatalysis experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simple and cumulative regret, Wilcoxon signed-rank statistical tests (95% confidence with Bonferroni correction), best-found objective value y_max; experimental budgets: synthetic tasks: i_max = 100 iterations, 50 repeated trials; photocatalysis: i_max = 300 iterations, 50 trials. Hyperparameter defaults: l_max = 2, u_max = 5, T = 1, γ = 0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random Search (RS), TuRBO (one trust region), LA-MCTS, LA-MCTS with hypothesis-based initial design (LA-MCTS+), πBO, and a discretized Bayesian optimizer (DBO) for the photocatalysis task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HypBO significantly outperforms RS, LA-MCTS, and TuRBO on many synthetic benchmarks (Wilcoxon tests at 95% with Bonferroni correction). Against LA-MCTS+ and πBO differences were not always statistically significant; p-values reported vs πBO were 0.06 and 0.15 for weak and poor hypotheses respectively. In the photocatalysis simulation, HypBO with realistic virtual-chemist hypotheses modestly improved performance over DBO and RS; Perfect Hindsight hypotheses produced much faster optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative and task-relative gains: faster convergence when good hypotheses are available (described as 'dramatic' on some benchmarks); specific numeric example: in ablations u10/l1 produced mean regret ≈5% lower than u5/l2. No single universal % acceleration is reported across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors analyze tradeoffs via l_max/u_max and γ ablation studies: increasing l_max (more focus on hypotheses) tends to increase regret in general (over-focusing harms performance), while larger u_max reduces regret; best empirical balance found around u=10,l=1 for regret minimization but u=5,l=2 chosen as practical default. Increasing γ (requiring larger improvements to continue) increases time spent in lower level and can hurt performance when hypotheses are weak but can slightly help when hypotheses are good. Overall, the paper concludes that adaptive allocation between hypothesis exploitation and global exploration is necessary to avoid bias and balance discovery probability vs wasted evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key insights: (1) Always include diversity in initialization (one sample per hypothesis plus random samples) to avoid missing hypotheses; (2) use a bilevel schedule with tunable plateau lengths (l_max,u_max) to flexibly allocate budget between hypothesis-focused exploitation and global exploration; (3) over-emphasizing hypothesis exploitation (large l_max) harms performance unless hypotheses are very accurate; (4) modest γ values (≤10%) recommended to avoid excessive oscillation; (5) allow hypotheses to be soft (seeds augment global model) rather than hard constraints to prevent long-term negative bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2481.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TuRBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trust-Region Bayesian Optimization (TuRBO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO method that runs multiple local BO instances inside trust regions and uses a bandit-like policy to select which trust regions to continue, thereby allocating budget to promising subregions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TuRBO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses multiple independent Gaussian-process surrogate models each operating in a local trust region; the trust-region sizes adapt (expand or shrink) according to local success/failure criteria, and a multi-armed-bandit-style controller chooses which trust regions to continue sampling. The method partitions allocation across concurrent local optimizers to cope with high dimensional or multimodal landscapes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General black-box optimization, used in materials/chemistry benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluation budget among multiple trust regions; regions that show success are given more evaluations while failing regions are reduced or dropped via thresholds τ_succ and τ_fail.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses standard BO acquisition functions per trust region (implicit expected improvement/other acquisition functions).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Local exploitation inside trust regions with adaptive region resizing and selection across regions via MAB-like policy to preserve exploration across multiple regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Maintains multiple trust regions to explore different parts of the space in parallel, promoting spatial diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experiment/iteration budget (number of evaluations); success/failure thresholds control local continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Adaptive per-region continuation rules (τ_succ, τ_fail) and trust-region resizing implicitly control allocation of remaining budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective or reduction in regret (not explicitly re-described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baseline performance in experiments (regret curves) compared to HypBO and others; specific TuRBO hyperparameters listed in supplement (m=1, τ_succ=3, τ_fail=[d/q], q=1, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline against HypBO and other methods in synthetic function benchmarks and photocatalysis task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HypBO outperforms TuRBO on many synthetic benchmarks when informative hypotheses are available; HypBO is robust and can recover from poor hypotheses and sometimes outperforms TuRBO in high-dimensional tasks due to more diverse initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discussed indirectly via HypBO comparisons: TuRBO's single trust-region setup can be less effective than hypothesis-guided seeding when informative priors exist; over-reliance on local trust regions without human guidance can slow discovery in some high-dimensional, 'needle-in-haystack' problems.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Paper does not present new allocation rules for TuRBO but treats it as a comparator; the HypBO analysis suggests benefit to supplementing trust-region strategies with hypothesis-informed seeds and diversity in initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2481.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LA-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Action Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses Monte Carlo Tree Search to learn a partitioning of the search space into subregions and guides BO by recursively selecting and refining subregions that are likely to contain good objective values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning search space partition for black-box optimization using monte carlo tree search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LA-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses Monte Carlo Tree Search (MCTS) to adaptively partition the input space into subregions; each node represents a subregion and is evaluated via BO to estimate its promise; MCTS expansion/backpropagation learns which subregions to refine, leading to a hierarchical allocation of evaluations to promising subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box optimization, used as a BO baseline in materials and synthetic benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates sampling budget by selecting subregions (tree nodes) that MCTS estimates as promising; exploration/exploitation in MCTS balances sampling unexplored partitions and exploiting high-value partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses BO surrogate-based acquisition within nodes and MCTS value/backpropagation signals; explicit information-theoretic metric not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>MCTS intrinsic UCT-style selection balances exploration of unexplored partitions and exploitation of promising ones; within each partition BO acquisition balances exploitation/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Space partitioning naturally spreads evaluations across different tree branches (subregions) promoting diverse exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Iteration/evaluation budget (i_max) and tree search computational budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>MCTS selection/expansion rules implicitly allocate the finite budget to tree branches with the best expected return given past outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective / regret in benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Regret curves on synthetic benchmarks; used as a competitive baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against HypBO, TuRBO, RS, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HypBO generally outperforms LA-MCTS on benchmarks when informative hypotheses are present; LA-MCTS may still be strong in unguided global search.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper references LA-MCTS as a related method for space segmentation; comparative tradeoffs are discussed empirically in HypBO experiments rather than analytically.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>LA-MCTS's MCTS-based allocation is complementary to hypothesis-guided seeding: combining explicit human hypotheses (as in HypBO) with learned partitioning may further improve allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2481.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LA-MCTS+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LA-MCTS with hypothesis-based initial design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of LA-MCTS modified to initialise all initial samples inside expert-specified hypothesis subspaces before continuing with the standard LA-MCTS search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LA-MCTS+</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Same LA-MCTS algorithm but with initialization restricted to hypothesis subspaces (all initial points sampled from hypotheses). This baseline was implemented by the authors to test the impact of initializing purely within hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box scientific optimization (benchmarks & materials tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates initial sample budget exclusively to hypothesis regions; thereafter follows LA-MCTS allocation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses the same BO acquisition functions within LA-MCTS; initialization biases the search toward hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Biases early exploitation toward hypothesis regions via initialization; later MCTS-driven exploration occurs as usual.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initialization is less diverse (hypothesis-only) so diversity mechanisms are reduced compared to HypBO's mixed initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed iteration budget (i_max).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Initialization choice changes early budget allocation; no additional budget handling beyond LA-MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective / regret.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Regret curves in synthetic experiments; compared to HypBO and πBO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against HypBO, LA-MCTS, TuRBO, RS, πBO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LA-MCTS+ lags behind HypBO in cases where hypothesis-only initialization was insufficiently diverse; HypBO's mixed initialization provided better robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Shows the risk of over-concentrating early budget in hypotheses: poor initial diversity can slow discovery; HypBO's mixed strategy reduces this risk.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Do not restrict all initial budget to hypotheses without additional diversity; combine hypothesis samples with global random samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2481.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>πBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π-BO (Prior-injected Bayesian Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO variant that injects expert prior beliefs into the acquisition function as a decaying multiplicative factor to bias sampling toward regions suggested by the prior while allowing the influence to decrease over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>πBO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates a pseudo-posterior by multiplying the acquisition or posterior by a user-provided prior belief distribution; the prior influence decays over iterations controlled by a decay hyperparameter β to avoid lifelong bias. In the experiments, HypBO authors converted their constraint hypotheses into Gaussian priors centered on the hypothesis centers to run πBO as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General BO for scientific experiments, particularly when a single expert prior is available</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Biases global acquisition toward the prior distribution; allocation is determined by acquisition function modified with prior weighting which decays over time, influencing where evaluations are made.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Standard BO acquisitions (e.g., EI) adjusted by multiplicative prior factor; expected improvement remains core metric but weighted by prior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration-exploitation balance remains via acquisition (e.g., EI), but exploitation is biased by prior; decay parameter β governs how quickly the optimizer forgets the prior to resume standard BO behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly designed to promote diversity across multiple hypotheses; limited to single prior-induced bias.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed iteration budget; decay parameter β is set relative to i_max.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Controls prior influence over the fixed budget via β (default β = i_max/10 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective / regret.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Regret and statistical tests; compared with HypBO, LA-MCTS, etc. Reported p-values vs HypBO (p=0.06 and 0.15 for weak and poor hypotheses), indicating non-significant differences in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a competitive baseline that uses an injected prior for the optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HypBO generally outperforms πBO on the tested tasks when multiple or complex hypotheses are present because πBO is limited to a single prior and can be harmed if the prior is inaccurate or the decay is mis-specified.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>πBO embodies a direct tradeoff: strong early prior weighting can speed search if prior is good, but risks negative bias; decay β must be tuned to manage this tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Single-prior approaches should decay prior influence and/or allow multiple priors to avoid long-term negative bias; HypBO's multi-hypothesis and soft-constraint approach is proposed as a remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2481.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOPrO / prior-injection methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BOPrO and related prior-injection Bayesian optimization methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of BO methods that incorporate expert priors into the optimization loop either by modifying the surrogate/posterior or by combining prior and data to form a pseudo-posterior to bias sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian optimization with a prior for the optimum</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BOPrO / Prior-injection BO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Examples include BOPrO which uses a user-specified prior and a data-driven model to produce a pseudo-posterior for sampling, and related methods (e.g., πBO, ColaBO) that integrate priors into acquisition/posterior; typically these methods multiply or combine prior density with GP predictions or adjust acquisition functions to bias sampling toward expert beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian optimization in scientific experiments where expert prior beliefs exist</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate evaluations by weighting acquisition/posterior with the prior distribution so that regions with higher prior mass receive more evaluations, with some methods decaying prior influence over time.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Standard BO acquisition measures (EI, UCB) modified by priors; not generally framed as explicit information-theoretic gain in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Standard acquisition-based exploration-exploitation but biased towards prior; decay schedules or weighting control shift back to data-driven exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not typically designed to manage multiple conflicting hypotheses; prior-based methods commonly assume a single prior or require ad-hoc combination strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations; prior decay governs allocation over the budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prior weighting and decay schedule determine allocation emphasis during the available budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective / regret.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Compared in literature and used as baselines; specific metrics depend on implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to HypBO and other BO variants in this paper; authors note BOPrO and similar are limited when only one prior is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HypBO is argued to be more flexible when multiple, possibly conflicting, expert hypotheses exist because prior-injection methods typically handle a single prior.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights the risk of negative bias from inaccurate priors and advocates soft, multi-hypothesis seeding as an alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When multiple expert beliefs exist, converting them into a single prior is limiting; better to treat them as multiple soft hypotheses and adaptively allocate budget among them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2481.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gryffin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gryffin (descriptor-informed Bayesian optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that uses user-provided physicochemical descriptors to measure similarity between categorical choices and guide BO over mixed continuous/categorical spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gryffin</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Incorporates descriptor information for categorical inputs to compute similarity and guide the BO surrogate/acquisition process; useful when domain experts can provide meaningful descriptors mapping categories into a descriptor space used by the optimizer to generalize across categories.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical/materials optimization where categorical design choices exist (e.g., molecule selections, catalyst choices)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluations by using descriptor-informed similarity to generalize successes across categories and bias sampling toward similar promising categories.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Standard BO acquisition functions augmented by descriptor-based similarity; explicit information-theoretic metrics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploitation through descriptor-based generalization; exploration enabled via acquisition function and descriptor uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Descriptor-based similarity can reduce spurious exploration of unrelated categories but does not explicitly enforce hypothesis diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Iteration budget (evaluations), typical to BO frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not described in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective / regret.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in original Gryffin literature; in this paper Gryffin is discussed as related work rather than benchmarked.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Discussed among methods that inject domain knowledge; not directly compared in HypBO experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors note that descriptor overabundance can create spurious correlations, reducing effectiveness — a tradeoff between leveraging descriptors and risking irrelevant bias.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When using many descriptors, expert oversight is needed to avoid spurious correlations; descriptor selection or weighting is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2481.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZoMBI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ZoMBI (Zooming memory-based initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An initialization method for BO that iteratively keeps best sample points and shrinks the sampling bounds to 'zoom' toward promising regions for needle-in-a-haystack problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fast Bayesian optimization of needle-in-a-Haystack problems using zooming memory-based initialization (ZoMBI)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ZoMBI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iteratively retains the best samples and tightens sampling bounds around them to focus search (zooming). Used to improve initialization for BO in problems where optima occupy a tiny fraction of the space.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Needle-in-a-haystack black-box optimization problems, e.g., materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates initial sampling budget to find good seeds, then reduces subsequent sampling domain to concentrate evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicitly aims to increase probability of sampling near optima via domain zooming; uses acquisition-based BO subsequently.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Initially broad exploration then progressive exploitation via bounding/zooming.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initial broad sampling preserves diversity, after which zooming reduces diversity to exploit promising regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget; zooming focuses remaining budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Adjust sample bounds to concentrate remaining budget on promising region.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best observed objective / regret.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported improvements on needle-in-a-haystack synthetic problems; referenced as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to BO baseline methods in cited ZoMBI work; discussed here as conceptually related.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Zooming trades off full-space exploration for focused exploitation after promising seeds are found; risk if seeds are misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Useful when good initial seeds can be found; HypBO uses a related idea of seeding via hypotheses but keeps seeds soft to avoid overcommitment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2481.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discretized Bayesian Optimizer (DBO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discretized variant of Bayesian optimization used in the referenced photocatalysis robotic-experiment study to handle large combinatorial discrete chemical spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Discretized Bayesian Optimizer (DBO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Optimizes over a large discrete/combinatorial design space by discretizing continuous inputs and applying a BO-style procedure (used in [Burger et al., 2020]'s autonomous robotic chemist). In HypBO experiments the same DBO implementation was used as a baseline for the photocatalytic hydrogen production task.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Combinatorial materials/chemical composition optimization, autonomous laboratory robotics</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experimental budget via acquisition optimization over a discretized candidate set; used in robotics run where experiments correspond to discrete choices/combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses BO acquisition over discretized set (e.g., EI over discrete candidates) to choose experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition-driven exploration/exploitation over discrete candidate set; discretization reduces continuous search complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly designed for hypothesis diversity; its discretized nature constrains candidate diversity to the discretization design.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget (robotic experiments) and time/resource constraints inherent to physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Discretized candidate pool and BO acquisition control selection under real experimental budget limits; used as baseline in the photocatalysis retrospective and virtual-chemist tests.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Hydrogen evolution rate (HER) maximization in photocatalysis experiments; best observed HER.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In original robotics study, used best HER found over robotic experiments (688 runs originally); in HypBO comparison, DBO baseline performance curves are plotted against HypBO under the same oracle model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as baseline to compare HypBO's retrospective hypotheses and virtual-chemist experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HypBO modestly improves upon DBO when realistic expert hypotheses are provided; HypBO with Perfect Hindsight substantially outperforms DBO.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>DBO's discretization simplifies the optimization but can fail to incorporate soft expert hypotheses that HypBO can inject; HypBO's soft-hypothesis seeding can provide information advantages without full discretization.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For large combinatorial physical experiment spaces, discretization + BO is practical; augmenting discretized BO with expert hypotheses (soft seeds) can improve early-stage discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2481.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2481.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZiatdinovHypRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis learning with reinforcement learning (Ziatdinov et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach that co-navigates a hypothesis space and experimental space by representing hypotheses as probabilistic models and using reinforcement learning to select hypotheses and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesis learning + RL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines multiple probabilistic hypothesis models and an RL controller to jointly explore hypothesis and experimental spaces. The paper notes drawbacks such as high computational cost of applying Bayesian inference per model and the assumption that only one hypothesis is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated scientific hypothesis testing and experimental design</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates computation and experiments across candidate hypothesis models via RL policy that chooses which hypothesis/model to test or which experimental action to take.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>High computational cost due to per-hypothesis Bayesian inference (not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>RL policy balances selecting high-utility hypotheses vs exploring less-certain hypotheses, but specific mechanism depends on implementation (cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Represents multiple hypotheses explicitly but criticized for assuming a single true hypothesis and being computationally expensive to maintain many hypothesis models.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational cost and experiment counts; primary concern is per-hypothesis inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not detailed in this paper; cited as a limitation due to heavy computational burden.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as related work (contrasted with HypBO's lighter-weight approach).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HypBO argued to be more computationally practical by avoiding heavy per-hypothesis Bayesian inference and not assuming a single true hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Cited tradeoffs: expressivity of probabilistic hypothesis models vs computational/time cost of inference for each hypothesis; risk of assuming single true hypothesis reduces robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Paper suggests lighter-weight hypothesis evaluation (HypBO's local GPs + seed augmentation) can be a pragmatic alternative to heavy per-hypothesis Bayesian inference combined with RL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning search space partition for black-box optimization using monte carlo tree search <em>(Rating: 2)</em></li>
                <li>Fast Bayesian optimization of needle-in-a-Haystack problems using zooming memory-based initialization (ZoMBI) <em>(Rating: 2)</em></li>
                <li>Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge <em>(Rating: 2)</em></li>
                <li>Bayesian optimization with a prior for the optimum <em>(Rating: 2)</em></li>
                <li>Trego: a trust-region framework for efficient global optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2481",
    "paper_id": "paper-261076132",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "HypBO",
            "name_full": "Hypothesis Bayesian Optimization",
            "brief_description": "A bilevel Bayesian optimization method that injects multiple expert-formulated hypotheses (as constrained subspaces) as local Gaussian-process surrogates to generate seed samples that augment a global BO surrogate; hypotheses are evaluated and pruned dynamically using a multi-armed-bandit style selection and plateau-based convergence rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "HypBO",
            "system_description": "HypBO organizes optimization as a bilevel process: a lower level fits local Gaussian Process (GP) models within expert-specified hypothesis subspaces H_j (hyperrectangles or linear constraints) and runs local BO to produce a set of top-performing seed samples; an upper level maintains a global GP over the full search space X and runs global BO. Hypotheses are treated as soft constraints; seeds from promising hypotheses augment the global dataset. Hypothesis selection is implemented implicitly as a multi-armed-bandit (MAB) where hypotheses are arms; the method alternates between lower- and upper-level optimization until plateauing criteria are met. Convergence/switching is controlled by parameters l_max (lower-level plateau length), u_max (upper-level plateau length), γ (improvement threshold), T (number of local seeds to keep), and a hard iteration budget i_max. Initialisation guarantees at least one random sample per hypothesis and extra random samples for diversity.",
            "application_domain": "Materials discovery / experimental chemistry (general black-box scientific optimization)",
            "resource_allocation_strategy": "Allocates experimental budget between a lower-level pool of hypothesis-local BO runs and an upper-level global BO run. At each outer iteration the lower level proposes T top seeds (selected by maximizing a local acquisition α_ϕj) from each hypothesis GP; the upper level then evaluates global acquisition α on the augmented dataset and runs until it plateaus. Allocation between lower and upper is governed by plateau counters (l_max for lower, u_max for upper) and improvement threshold γ; hypotheses are implicitly selected via a MAB-style policy where hypotheses that produce better seeds are favored and weaker hypotheses are de-prioritized (but not permanently excluded). Initial allocation ensures one sample per hypothesis and remaining initial budget drawn uniformly to preserve diversity.",
            "computational_cost_metric": "number of objective evaluations / iterations (i_max), and internal GP model fits per local hypothesis and global GP; the paper does not report FLOPs or wall-clock timings explicitly (computational cost is managed via counts of iterations and GP fits).",
            "information_gain_metric": "Standard BO acquisition functions (e.g., Expected Improvement or other α(x,D)) used to score candidate seeds and select evaluations; expected improvement (EI) is used in selection/analysis in experiments.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Bilevel alternation: the lower level focuses exploitation of hypothesis subspaces via local BO (exploitation of expert priors) and the upper level performs global BO to enable exploration; switching and time spent in each are controlled by l_max, u_max and γ so the system can allocate more budget to promising hypotheses or to global exploration if hypotheses are weak. MAB-style implicit selection of hypotheses decides which hypothesis-local BOs to continue based on seed utility.",
            "diversity_mechanism": "Initialisation guarantees one random sample per hypothesis plus additional random samples from the whole space to preserve diversity; seeds are chosen from multiple hypotheses (not a single prior), and weaker hypotheses are pruned rather than hard-removed to allow occasional re-evaluation, encouraging hypothesis diversity over time.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of experiments / iteration budget (i_max) and per-level plateau counters (l_max, u_max) acting as implicit budget controls.",
            "budget_constraint_handling": "Enforces a hard iteration limit i_max and uses plateau-based stopping: lower-level generates seeds until no significant improvement over y_max for l_max consecutive attempts; the upper-level continues until u_max consecutive non-significant improvements. γ defines what counts as significant improvement. These parameters allocate experimental budget adaptively between hypothesis-focused and global exploration phases.",
            "breakthrough_discovery_metric": "Best observed objective value (y_max) / reduction in regret (simple and cumulative regret); in materials application measured as hydrogen evolution rate (HER) for photocatalysis experiments.",
            "performance_metrics": "Simple and cumulative regret, Wilcoxon signed-rank statistical tests (95% confidence with Bonferroni correction), best-found objective value y_max; experimental budgets: synthetic tasks: i_max = 100 iterations, 50 repeated trials; photocatalysis: i_max = 300 iterations, 50 trials. Hyperparameter defaults: l_max = 2, u_max = 5, T = 1, γ = 0.",
            "comparison_baseline": "Random Search (RS), TuRBO (one trust region), LA-MCTS, LA-MCTS with hypothesis-based initial design (LA-MCTS+), πBO, and a discretized Bayesian optimizer (DBO) for the photocatalysis task.",
            "performance_vs_baseline": "HypBO significantly outperforms RS, LA-MCTS, and TuRBO on many synthetic benchmarks (Wilcoxon tests at 95% with Bonferroni correction). Against LA-MCTS+ and πBO differences were not always statistically significant; p-values reported vs πBO were 0.06 and 0.15 for weak and poor hypotheses respectively. In the photocatalysis simulation, HypBO with realistic virtual-chemist hypotheses modestly improved performance over DBO and RS; Perfect Hindsight hypotheses produced much faster optimization.",
            "efficiency_gain": "Qualitative and task-relative gains: faster convergence when good hypotheses are available (described as 'dramatic' on some benchmarks); specific numeric example: in ablations u10/l1 produced mean regret ≈5% lower than u5/l2. No single universal % acceleration is reported across experiments.",
            "tradeoff_analysis": "Authors analyze tradeoffs via l_max/u_max and γ ablation studies: increasing l_max (more focus on hypotheses) tends to increase regret in general (over-focusing harms performance), while larger u_max reduces regret; best empirical balance found around u=10,l=1 for regret minimization but u=5,l=2 chosen as practical default. Increasing γ (requiring larger improvements to continue) increases time spent in lower level and can hurt performance when hypotheses are weak but can slightly help when hypotheses are good. Overall, the paper concludes that adaptive allocation between hypothesis exploitation and global exploration is necessary to avoid bias and balance discovery probability vs wasted evaluations.",
            "optimal_allocation_findings": "Key insights: (1) Always include diversity in initialization (one sample per hypothesis plus random samples) to avoid missing hypotheses; (2) use a bilevel schedule with tunable plateau lengths (l_max,u_max) to flexibly allocate budget between hypothesis-focused exploitation and global exploration; (3) over-emphasizing hypothesis exploitation (large l_max) harms performance unless hypotheses are very accurate; (4) modest γ values (≤10%) recommended to avoid excessive oscillation; (5) allow hypotheses to be soft (seeds augment global model) rather than hard constraints to prevent long-term negative bias.",
            "uuid": "e2481.0",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "TuRBO",
            "name_full": "Trust-Region Bayesian Optimization (TuRBO)",
            "brief_description": "A BO method that runs multiple local BO instances inside trust regions and uses a bandit-like policy to select which trust regions to continue, thereby allocating budget to promising subregions.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "TuRBO",
            "system_description": "Uses multiple independent Gaussian-process surrogate models each operating in a local trust region; the trust-region sizes adapt (expand or shrink) according to local success/failure criteria, and a multi-armed-bandit-style controller chooses which trust regions to continue sampling. The method partitions allocation across concurrent local optimizers to cope with high dimensional or multimodal landscapes.",
            "application_domain": "General black-box optimization, used in materials/chemistry benchmarks",
            "resource_allocation_strategy": "Allocates evaluation budget among multiple trust regions; regions that show success are given more evaluations while failing regions are reduced or dropped via thresholds τ_succ and τ_fail.",
            "computational_cost_metric": null,
            "information_gain_metric": "Uses standard BO acquisition functions per trust region (implicit expected improvement/other acquisition functions).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Local exploitation inside trust regions with adaptive region resizing and selection across regions via MAB-like policy to preserve exploration across multiple regions.",
            "diversity_mechanism": "Maintains multiple trust regions to explore different parts of the space in parallel, promoting spatial diversity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experiment/iteration budget (number of evaluations); success/failure thresholds control local continuation.",
            "budget_constraint_handling": "Adaptive per-region continuation rules (τ_succ, τ_fail) and trust-region resizing implicitly control allocation of remaining budget.",
            "breakthrough_discovery_metric": "Best observed objective or reduction in regret (not explicitly re-described in this paper).",
            "performance_metrics": "Reported baseline performance in experiments (regret curves) compared to HypBO and others; specific TuRBO hyperparameters listed in supplement (m=1, τ_succ=3, τ_fail=[d/q], q=1, etc.).",
            "comparison_baseline": "Used as a baseline against HypBO and other methods in synthetic function benchmarks and photocatalysis task.",
            "performance_vs_baseline": "HypBO outperforms TuRBO on many synthetic benchmarks when informative hypotheses are available; HypBO is robust and can recover from poor hypotheses and sometimes outperforms TuRBO in high-dimensional tasks due to more diverse initialization.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Discussed indirectly via HypBO comparisons: TuRBO's single trust-region setup can be less effective than hypothesis-guided seeding when informative priors exist; over-reliance on local trust regions without human guidance can slow discovery in some high-dimensional, 'needle-in-haystack' problems.",
            "optimal_allocation_findings": "Paper does not present new allocation rules for TuRBO but treats it as a comparator; the HypBO analysis suggests benefit to supplementing trust-region strategies with hypothesis-informed seeds and diversity in initialization.",
            "uuid": "e2481.1",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LA-MCTS",
            "name_full": "Latent Action Monte Carlo Tree Search",
            "brief_description": "An approach that uses Monte Carlo Tree Search to learn a partitioning of the search space into subregions and guides BO by recursively selecting and refining subregions that are likely to contain good objective values.",
            "citation_title": "Learning search space partition for black-box optimization using monte carlo tree search",
            "mention_or_use": "use",
            "system_name": "LA-MCTS",
            "system_description": "Uses Monte Carlo Tree Search (MCTS) to adaptively partition the input space into subregions; each node represents a subregion and is evaluated via BO to estimate its promise; MCTS expansion/backpropagation learns which subregions to refine, leading to a hierarchical allocation of evaluations to promising subspaces.",
            "application_domain": "Black-box optimization, used as a BO baseline in materials and synthetic benchmarks",
            "resource_allocation_strategy": "Allocates sampling budget by selecting subregions (tree nodes) that MCTS estimates as promising; exploration/exploitation in MCTS balances sampling unexplored partitions and exploiting high-value partitions.",
            "computational_cost_metric": null,
            "information_gain_metric": "Uses BO surrogate-based acquisition within nodes and MCTS value/backpropagation signals; explicit information-theoretic metric not specified in this paper.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "MCTS intrinsic UCT-style selection balances exploration of unexplored partitions and exploitation of promising ones; within each partition BO acquisition balances exploitation/exploration.",
            "diversity_mechanism": "Space partitioning naturally spreads evaluations across different tree branches (subregions) promoting diverse exploration.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Iteration/evaluation budget (i_max) and tree search computational budget.",
            "budget_constraint_handling": "MCTS selection/expansion rules implicitly allocate the finite budget to tree branches with the best expected return given past outcomes.",
            "breakthrough_discovery_metric": "Best observed objective / regret in benchmarks.",
            "performance_metrics": "Regret curves on synthetic benchmarks; used as a competitive baseline in experiments.",
            "comparison_baseline": "Compared against HypBO, TuRBO, RS, etc.",
            "performance_vs_baseline": "HypBO generally outperforms LA-MCTS on benchmarks when informative hypotheses are present; LA-MCTS may still be strong in unguided global search.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper references LA-MCTS as a related method for space segmentation; comparative tradeoffs are discussed empirically in HypBO experiments rather than analytically.",
            "optimal_allocation_findings": "LA-MCTS's MCTS-based allocation is complementary to hypothesis-guided seeding: combining explicit human hypotheses (as in HypBO) with learned partitioning may further improve allocation.",
            "uuid": "e2481.2",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LA-MCTS+",
            "name_full": "LA-MCTS with hypothesis-based initial design",
            "brief_description": "A variant of LA-MCTS modified to initialise all initial samples inside expert-specified hypothesis subspaces before continuing with the standard LA-MCTS search.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LA-MCTS+",
            "system_description": "Same LA-MCTS algorithm but with initialization restricted to hypothesis subspaces (all initial points sampled from hypotheses). This baseline was implemented by the authors to test the impact of initializing purely within hypotheses.",
            "application_domain": "Black-box scientific optimization (benchmarks & materials tasks)",
            "resource_allocation_strategy": "Allocates initial sample budget exclusively to hypothesis regions; thereafter follows LA-MCTS allocation rules.",
            "computational_cost_metric": null,
            "information_gain_metric": "Uses the same BO acquisition functions within LA-MCTS; initialization biases the search toward hypotheses.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Biases early exploitation toward hypothesis regions via initialization; later MCTS-driven exploration occurs as usual.",
            "diversity_mechanism": "Initialization is less diverse (hypothesis-only) so diversity mechanisms are reduced compared to HypBO's mixed initialization.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed iteration budget (i_max).",
            "budget_constraint_handling": "Initialization choice changes early budget allocation; no additional budget handling beyond LA-MCTS.",
            "breakthrough_discovery_metric": "Best observed objective / regret.",
            "performance_metrics": "Regret curves in synthetic experiments; compared to HypBO and πBO.",
            "comparison_baseline": "Compared against HypBO, LA-MCTS, TuRBO, RS, πBO.",
            "performance_vs_baseline": "LA-MCTS+ lags behind HypBO in cases where hypothesis-only initialization was insufficiently diverse; HypBO's mixed initialization provided better robustness.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Shows the risk of over-concentrating early budget in hypotheses: poor initial diversity can slow discovery; HypBO's mixed strategy reduces this risk.",
            "optimal_allocation_findings": "Do not restrict all initial budget to hypotheses without additional diversity; combine hypothesis samples with global random samples.",
            "uuid": "e2481.3",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "πBO",
            "name_full": "π-BO (Prior-injected Bayesian Optimization)",
            "brief_description": "A BO variant that injects expert prior beliefs into the acquisition function as a decaying multiplicative factor to bias sampling toward regions suggested by the prior while allowing the influence to decrease over time.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "πBO",
            "system_description": "Generates a pseudo-posterior by multiplying the acquisition or posterior by a user-provided prior belief distribution; the prior influence decays over iterations controlled by a decay hyperparameter β to avoid lifelong bias. In the experiments, HypBO authors converted their constraint hypotheses into Gaussian priors centered on the hypothesis centers to run πBO as a baseline.",
            "application_domain": "General BO for scientific experiments, particularly when a single expert prior is available",
            "resource_allocation_strategy": "Biases global acquisition toward the prior distribution; allocation is determined by acquisition function modified with prior weighting which decays over time, influencing where evaluations are made.",
            "computational_cost_metric": null,
            "information_gain_metric": "Standard BO acquisitions (e.g., EI) adjusted by multiplicative prior factor; expected improvement remains core metric but weighted by prior.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration-exploitation balance remains via acquisition (e.g., EI), but exploitation is biased by prior; decay parameter β governs how quickly the optimizer forgets the prior to resume standard BO behavior.",
            "diversity_mechanism": "Not explicitly designed to promote diversity across multiple hypotheses; limited to single prior-induced bias.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed iteration budget; decay parameter β is set relative to i_max.",
            "budget_constraint_handling": "Controls prior influence over the fixed budget via β (default β = i_max/10 in experiments).",
            "breakthrough_discovery_metric": "Best observed objective / regret.",
            "performance_metrics": "Regret and statistical tests; compared with HypBO, LA-MCTS, etc. Reported p-values vs HypBO (p=0.06 and 0.15 for weak and poor hypotheses), indicating non-significant differences in some settings.",
            "comparison_baseline": "Used as a competitive baseline that uses an injected prior for the optimum.",
            "performance_vs_baseline": "HypBO generally outperforms πBO on the tested tasks when multiple or complex hypotheses are present because πBO is limited to a single prior and can be harmed if the prior is inaccurate or the decay is mis-specified.",
            "efficiency_gain": null,
            "tradeoff_analysis": "πBO embodies a direct tradeoff: strong early prior weighting can speed search if prior is good, but risks negative bias; decay β must be tuned to manage this tradeoff.",
            "optimal_allocation_findings": "Single-prior approaches should decay prior influence and/or allow multiple priors to avoid long-term negative bias; HypBO's multi-hypothesis and soft-constraint approach is proposed as a remedy.",
            "uuid": "e2481.4",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "BOPrO / prior-injection methods",
            "name_full": "BOPrO and related prior-injection Bayesian optimization methods",
            "brief_description": "A class of BO methods that incorporate expert priors into the optimization loop either by modifying the surrogate/posterior or by combining prior and data to form a pseudo-posterior to bias sampling.",
            "citation_title": "Bayesian optimization with a prior for the optimum",
            "mention_or_use": "mention",
            "system_name": "BOPrO / Prior-injection BO",
            "system_description": "Examples include BOPrO which uses a user-specified prior and a data-driven model to produce a pseudo-posterior for sampling, and related methods (e.g., πBO, ColaBO) that integrate priors into acquisition/posterior; typically these methods multiply or combine prior density with GP predictions or adjust acquisition functions to bias sampling toward expert beliefs.",
            "application_domain": "Bayesian optimization in scientific experiments where expert prior beliefs exist",
            "resource_allocation_strategy": "Allocate evaluations by weighting acquisition/posterior with the prior distribution so that regions with higher prior mass receive more evaluations, with some methods decaying prior influence over time.",
            "computational_cost_metric": null,
            "information_gain_metric": "Standard BO acquisition measures (EI, UCB) modified by priors; not generally framed as explicit information-theoretic gain in this paper.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Standard acquisition-based exploration-exploitation but biased towards prior; decay schedules or weighting control shift back to data-driven exploitation.",
            "diversity_mechanism": "Not typically designed to manage multiple conflicting hypotheses; prior-based methods commonly assume a single prior or require ad-hoc combination strategies.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of evaluations; prior decay governs allocation over the budget.",
            "budget_constraint_handling": "Prior weighting and decay schedule determine allocation emphasis during the available budget.",
            "breakthrough_discovery_metric": "Best observed objective / regret.",
            "performance_metrics": "Compared in literature and used as baselines; specific metrics depend on implementation.",
            "comparison_baseline": "Compared to HypBO and other BO variants in this paper; authors note BOPrO and similar are limited when only one prior is used.",
            "performance_vs_baseline": "HypBO is argued to be more flexible when multiple, possibly conflicting, expert hypotheses exist because prior-injection methods typically handle a single prior.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper highlights the risk of negative bias from inaccurate priors and advocates soft, multi-hypothesis seeding as an alternative.",
            "optimal_allocation_findings": "When multiple expert beliefs exist, converting them into a single prior is limiting; better to treat them as multiple soft hypotheses and adaptively allocate budget among them.",
            "uuid": "e2481.5",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Gryffin",
            "name_full": "Gryffin (descriptor-informed Bayesian optimization)",
            "brief_description": "An algorithm that uses user-provided physicochemical descriptors to measure similarity between categorical choices and guide BO over mixed continuous/categorical spaces.",
            "citation_title": "Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge",
            "mention_or_use": "mention",
            "system_name": "Gryffin",
            "system_description": "Incorporates descriptor information for categorical inputs to compute similarity and guide the BO surrogate/acquisition process; useful when domain experts can provide meaningful descriptors mapping categories into a descriptor space used by the optimizer to generalize across categories.",
            "application_domain": "Chemical/materials optimization where categorical design choices exist (e.g., molecule selections, catalyst choices)",
            "resource_allocation_strategy": "Allocates evaluations by using descriptor-informed similarity to generalize successes across categories and bias sampling toward similar promising categories.",
            "computational_cost_metric": null,
            "information_gain_metric": "Standard BO acquisition functions augmented by descriptor-based similarity; explicit information-theoretic metrics not detailed in this paper.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploitation through descriptor-based generalization; exploration enabled via acquisition function and descriptor uncertainty.",
            "diversity_mechanism": "Descriptor-based similarity can reduce spurious exploration of unrelated categories but does not explicitly enforce hypothesis diversity.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Iteration budget (evaluations), typical to BO frameworks.",
            "budget_constraint_handling": "Not described in detail in this paper.",
            "breakthrough_discovery_metric": "Best observed objective / regret.",
            "performance_metrics": "Reported in original Gryffin literature; in this paper Gryffin is discussed as related work rather than benchmarked.",
            "comparison_baseline": "Discussed among methods that inject domain knowledge; not directly compared in HypBO experiments.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Authors note that descriptor overabundance can create spurious correlations, reducing effectiveness — a tradeoff between leveraging descriptors and risking irrelevant bias.",
            "optimal_allocation_findings": "When using many descriptors, expert oversight is needed to avoid spurious correlations; descriptor selection or weighting is crucial.",
            "uuid": "e2481.6",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ZoMBI",
            "name_full": "ZoMBI (Zooming memory-based initialization)",
            "brief_description": "An initialization method for BO that iteratively keeps best sample points and shrinks the sampling bounds to 'zoom' toward promising regions for needle-in-a-haystack problems.",
            "citation_title": "Fast Bayesian optimization of needle-in-a-Haystack problems using zooming memory-based initialization (ZoMBI)",
            "mention_or_use": "mention",
            "system_name": "ZoMBI",
            "system_description": "Iteratively retains the best samples and tightens sampling bounds around them to focus search (zooming). Used to improve initialization for BO in problems where optima occupy a tiny fraction of the space.",
            "application_domain": "Needle-in-a-haystack black-box optimization problems, e.g., materials discovery",
            "resource_allocation_strategy": "Allocates initial sampling budget to find good seeds, then reduces subsequent sampling domain to concentrate evaluations.",
            "computational_cost_metric": null,
            "information_gain_metric": "Implicitly aims to increase probability of sampling near optima via domain zooming; uses acquisition-based BO subsequently.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Initially broad exploration then progressive exploitation via bounding/zooming.",
            "diversity_mechanism": "Initial broad sampling preserves diversity, after which zooming reduces diversity to exploit promising regions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budget; zooming focuses remaining budget.",
            "budget_constraint_handling": "Adjust sample bounds to concentrate remaining budget on promising region.",
            "breakthrough_discovery_metric": "Best observed objective / regret.",
            "performance_metrics": "Reported improvements on needle-in-a-haystack synthetic problems; referenced as related work.",
            "comparison_baseline": "Compared to BO baseline methods in cited ZoMBI work; discussed here as conceptually related.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Zooming trades off full-space exploration for focused exploitation after promising seeds are found; risk if seeds are misleading.",
            "optimal_allocation_findings": "Useful when good initial seeds can be found; HypBO uses a related idea of seeding via hypotheses but keeps seeds soft to avoid overcommitment.",
            "uuid": "e2481.7",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DBO",
            "name_full": "Discretized Bayesian Optimizer (DBO)",
            "brief_description": "A discretized variant of Bayesian optimization used in the referenced photocatalysis robotic-experiment study to handle large combinatorial discrete chemical spaces.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Discretized Bayesian Optimizer (DBO)",
            "system_description": "Optimizes over a large discrete/combinatorial design space by discretizing continuous inputs and applying a BO-style procedure (used in [Burger et al., 2020]'s autonomous robotic chemist). In HypBO experiments the same DBO implementation was used as a baseline for the photocatalytic hydrogen production task.",
            "application_domain": "Combinatorial materials/chemical composition optimization, autonomous laboratory robotics",
            "resource_allocation_strategy": "Allocates experimental budget via acquisition optimization over a discretized candidate set; used in robotics run where experiments correspond to discrete choices/combinations.",
            "computational_cost_metric": null,
            "information_gain_metric": "Uses BO acquisition over discretized set (e.g., EI over discrete candidates) to choose experiments.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition-driven exploration/exploitation over discrete candidate set; discretization reduces continuous search complexity.",
            "diversity_mechanism": "Not explicitly designed for hypothesis diversity; its discretized nature constrains candidate diversity to the discretization design.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed experimental budget (robotic experiments) and time/resource constraints inherent to physical experiments.",
            "budget_constraint_handling": "Discretized candidate pool and BO acquisition control selection under real experimental budget limits; used as baseline in the photocatalysis retrospective and virtual-chemist tests.",
            "breakthrough_discovery_metric": "Hydrogen evolution rate (HER) maximization in photocatalysis experiments; best observed HER.",
            "performance_metrics": "In original robotics study, used best HER found over robotic experiments (688 runs originally); in HypBO comparison, DBO baseline performance curves are plotted against HypBO under the same oracle model.",
            "comparison_baseline": "Used as baseline to compare HypBO's retrospective hypotheses and virtual-chemist experiments.",
            "performance_vs_baseline": "HypBO modestly improves upon DBO when realistic expert hypotheses are provided; HypBO with Perfect Hindsight substantially outperforms DBO.",
            "efficiency_gain": null,
            "tradeoff_analysis": "DBO's discretization simplifies the optimization but can fail to incorporate soft expert hypotheses that HypBO can inject; HypBO's soft-hypothesis seeding can provide information advantages without full discretization.",
            "optimal_allocation_findings": "For large combinatorial physical experiment spaces, discretization + BO is practical; augmenting discretized BO with expert hypotheses (soft seeds) can improve early-stage discovery.",
            "uuid": "e2481.8",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ZiatdinovHypRL",
            "name_full": "Hypothesis learning with reinforcement learning (Ziatdinov et al.)",
            "brief_description": "A referenced approach that co-navigates a hypothesis space and experimental space by representing hypotheses as probabilistic models and using reinforcement learning to select hypotheses and experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Hypothesis learning + RL",
            "system_description": "Combines multiple probabilistic hypothesis models and an RL controller to jointly explore hypothesis and experimental spaces. The paper notes drawbacks such as high computational cost of applying Bayesian inference per model and the assumption that only one hypothesis is correct.",
            "application_domain": "Automated scientific hypothesis testing and experimental design",
            "resource_allocation_strategy": "Allocates computation and experiments across candidate hypothesis models via RL policy that chooses which hypothesis/model to test or which experimental action to take.",
            "computational_cost_metric": "High computational cost due to per-hypothesis Bayesian inference (not quantified here).",
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "RL policy balances selecting high-utility hypotheses vs exploring less-certain hypotheses, but specific mechanism depends on implementation (cited work).",
            "diversity_mechanism": "Represents multiple hypotheses explicitly but criticized for assuming a single true hypothesis and being computationally expensive to maintain many hypothesis models.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Computational cost and experiment counts; primary concern is per-hypothesis inference cost.",
            "budget_constraint_handling": "Not detailed in this paper; cited as a limitation due to heavy computational burden.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": "Mentioned as related work (contrasted with HypBO's lighter-weight approach).",
            "performance_vs_baseline": "HypBO argued to be more computationally practical by avoiding heavy per-hypothesis Bayesian inference and not assuming a single true hypothesis.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Cited tradeoffs: expressivity of probabilistic hypothesis models vs computational/time cost of inference for each hypothesis; risk of assuming single true hypothesis reduces robustness.",
            "optimal_allocation_findings": "Paper suggests lighter-weight hypothesis evaluation (HypBO's local GPs + seed augmentation) can be a pragmatic alternative to heavy per-hypothesis Bayesian inference combined with RL.",
            "uuid": "e2481.9",
            "source_info": {
                "paper_title": "HypBO: Accelerating Black-Box Scientific Experiments Using Experts’ Hypotheses",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning search space partition for black-box optimization using monte carlo tree search",
            "rating": 2,
            "sanitized_title": "learning_search_space_partition_for_blackbox_optimization_using_monte_carlo_tree_search"
        },
        {
            "paper_title": "Fast Bayesian optimization of needle-in-a-Haystack problems using zooming memory-based initialization (ZoMBI)",
            "rating": 2,
            "sanitized_title": "fast_bayesian_optimization_of_needleinahaystack_problems_using_zooming_memorybased_initialization_zombi"
        },
        {
            "paper_title": "Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge",
            "rating": 2,
            "sanitized_title": "gryffin_an_algorithm_for_bayesian_optimization_of_categorical_variables_informed_by_expert_knowledge"
        },
        {
            "paper_title": "Bayesian optimization with a prior for the optimum",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_with_a_prior_for_the_optimum"
        },
        {
            "paper_title": "Trego: a trust-region framework for efficient global optimization",
            "rating": 1,
            "sanitized_title": "trego_a_trustregion_framework_for_efficient_global_optimization"
        }
    ],
    "cost": 0.0235615,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HypBO: Accelerating Black-Box Scientific Experiments Using Experts' Hypotheses
28 Jan 2024</p>
<p>Abdoulatif Cissé abdoulatif.cisse@liverpool.ac.uk 
Department of Chemistry
University of Liverpool
England, UK</p>
<p>Leverhulme Research Centre for Functional Materials Design
University of Liverpool
England, UK</p>
<p>Xenophon Evangelopoulos 
Department of Chemistry
University of Liverpool
England, UK</p>
<p>Leverhulme Research Centre for Functional Materials Design
University of Liverpool
England, UK</p>
<p>Sam Carruthers 
Department of Chemistry
University of Liverpool
England, UK</p>
<p>Leverhulme Research Centre for Functional Materials Design
University of Liverpool
England, UK</p>
<p>Vladimir V Gusev vladimir.gusev@liverpool.ac.uk 
Department of Computer Science
University of Liverpool
England, UK</p>
<p>Andrew I Cooper aicooper@liverpool.ac.uk 
Department of Chemistry
University of Liverpool
England, UK</p>
<p>Leverhulme Research Centre for Functional Materials Design
University of Liverpool
England, UK</p>
<p>HypBO: Accelerating Black-Box Scientific Experiments Using Experts' Hypotheses
28 Jan 2024E5DDD72F542463D24B8C4F4F976DADD4arXiv:2308.11787v3[cs.LG]
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large.Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known.Here, we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space.Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks.Also, such distributions cannot capture intricate hypotheses.Our proposed method, which we call HypBO, uses expert human hypotheses to generate improved seed samples.Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling.This process continues in a global versus local search fashion, organized in a bilevel optimization framework.We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.</p>
<p>Introduction</p>
<p>Bayesian Optimization (BO) is a valuable tool for optimizing experiments in chemistry and materials science, where experiments are costly and time-consuming [Shahriari et al., 2016].Experimental design methods often involve exhaustive exploration of the parameter space.By contrast, BO offers an efficient framework leveraging Bayesian inference to guide the iterative exploration of the space, ultimately maximizing the target experiment property [Jones et al., 1998].</p>
<p>Formally, BO aims to find the global optimum in the following problem:
x * = argmax x∈X f (x),(1)
where f : X → R is a continuous function over the ddimensional input space X ∈ R d .Generally, the underlying analytical form of f (•) is unknown, making it a blackbox function.The core principle of BO lies in the construction of a probabilistic model, typically a Gaussian Process (GP) [Ebden, 2015], which serves as a surrogate model for f (•).This surrogate model is updated iteratively as new experimental data become available, allowing for the refinement of target predictions.The model's uncertainty is quantified, and an acquisition function is employed to select the next set of experimental parameters to evaluate, balancing exploration (sampling in unexplored regions) and exploitation (focusing on promising regions).</p>
<p>Injecting domain-specific knowledge into BO to boost optimization performance has gained significant recent attention, especially for scientific tasks [Ramachandran et al., 2020], aiming to alleviate the resource-intensive surrogate model construction.In particular, recent studies have used expert knowledge as user-specified priors over possible optima to guide the search toward promising regions [Hvarfner et al., 2022;Li et al., 2020].While this has shown promising performance in various tasks, it is difficult in many scientific problems to realize external knowledge in the form of a prior distribution.Furthermore, the optimization landscapes of such problems often resemble a needle-in-a-haystack manifold [Siemenn et al., 2023], and inaccurate prior knowledge distributions can introduce negative bias in the problem and quickly degrade performance.More recently, humanin-the-loop (HIL) approaches have emerged where an interactive optimization framework enables experts to implicitly add knowledge to the problem in the form of feedback on the quality of the samples within the experimental loop [Huang et al., 2022].However, this knowledge is implicit and sample-specific and can often lead to local optima entrapment.Another category of methods introduces domainspecific knowledge in the form of hard constraints in the problem [Hernández-Lobato et al., 2015], which can, however, over-restrict the search in practice.</p>
<p>In this paper, we propose a novel approach to inject domain knowledge using input from domain experts to direct the search to more fruitful regions.We specifically represent domain knowledge as human hypotheses or conjectures that are realized as intervals of confidence, i.e., constraints on the pa- rameter space.Figure 1 demonstrates three representative hypothesis regions within the input space of a one-dimensional Ackley function where the input region around zero is clearly the most promising hypothesis.The various hypotheses are realized as local Gaussian processes (GPs) restricted to the constrained space, and their utility is being iteratively evaluated by a global GP, which in turn expands or shrinks the global search space accordingly.Our approach treats the human hypotheses as soft constraints and hence avoids overrestricting the search or getting stuck in local optima.We formulate our approach in a bilevel optimization framework where the lower level evaluates the various hypotheses, and the upper level integrates the useful ones in the search.The methodology is further detailed in Section 3.</p>
<p>We test the proposed methodology on a materials design simulation where a set of different chemical hypotheses are injected to guide the search to more fruitful solutions faster.We show that hypotheses with favorable conditions accelerate the search and also improve performance.Interestingly, unfavorable hypotheses do not appear to bias the search negatively in the long run.Extensive synthetic tests further demonstrate that our method maintains a competitive and robust performance overall.</p>
<p>The remainder of this paper is organized as follows.Section 2 presents recent works about expert knowledge integration in BO, while Section 3 describes the proposed methodology.The robustness and performance of our algorithm are evaluated and discussed in Section 4. Finally, Section 5 summarizes our work and introduces future directions.</p>
<p>Related Works</p>
<p>Knowledge distillation has recently been at the center of attention in the BO literature to address issues such as the "cold" start problem where the initial points, usually selected randomly, fail to adequately capture the optimization objective's landscape.Transfer learning [Niu et al., 2020] has been widely used to extract and use knowledge from previous BO executions to aid in warming up and enhancing optimization [Theckel Joy et al., 2019].Furthermore, it has also been used effectively in chemical reaction optimization [Hickman et al., 2023] to bias the search space by weighting the current acquisition function with past predictions.</p>
<p>Another approach to improving BO through the incorporation of domain knowledge involves the use of similarities between points in the search space.Gryffin [Häse et al., 2021] uses user-provided physicochemical descriptors to navigate the search space more efficiently by identifying similarities between individual options based on those descriptors.However, when using a large number of descriptors, spurious correlations can occur between descriptors and the optimized objective, leading to irrelevant descriptors being considered important.[Morishita and Kaneko, 2023] suggest using a clustering-based initial sample selection method for optimizing chemical reaction conditions with BO based on a high correlation between molecular descriptors and clustering in chemical space.However, as clustering is based on unsupervised learning, there is a need for expert knowledge to connect it with experimental results; also, not all scientific problems can be codified using molecular descriptors.</p>
<p>Other approaches inject expert prior beliefs as priors to guide the optimization process.[Li et al., 2020] combined prior user beliefs with observed data to compute the posterior distribution via repeated Thompson sampling.This approximates new sampling points using a linear combination of posterior samplings.BOPrO [Souza et al., 2021] uses a prior that is provided by the user and a data-driven model to generate a pseudo-posterior.Similarly, πBO [Hvarfner et al., 2022] generates a pseudo-posterior by integrating prior beliefs into the acquisition function as a decaying multiplicative factor to improve sampling.Both of these methods, and ColaBO [Hvarfner et al., 2023] which augments the surrogate model with a user-defined prior, are limited to one expert prior, and the use of priors cannot capture intricate knowledge.[Ziatdinov et al., 2022] co-navigates a hypothesis space and the experimental space through a hypothesis learning approach that combines multiple hypotheses as probabilistic models with reinforcement learning.However, major drawbacks of this approach are the difficulty of representing a hypothesis into a probabilistic model from the functional form of the black-box model, the computational cost and time of applying Bayesian Inference for each model, and the assumption that only one out of the hypothesis pool is the correct one.</p>
<p>Preference learning can also enrich BO through domain knowledge.[Huang et al., 2022] obtain expert opinions by querying them with pairwise comparisons, thereby approximating the shape of the objective function.[Anjanapura Venkatesh et al., 2022] take a slightly different approach by allowing experts to provide a pair of good and bad points, which are then used to fine-tune the BO's surrogate model by replacing its current optimal hyperparameters with ones that align more closely with the expert's cognitive model.Such approaches are promising but risk biasing the optimizer, which could mimic the user's beliefs and result in suboptimal solutions.</p>
<p>Various methods can restrict the search space to regions assumed by the optimizer to contain the optimum.TuRBO [Eriksson et al., 2019] uses multiple independent GP surrogate models within different trust regions to conduct simultaneous BO runs, and a multi-armed bandit (MAB) strategy [Vermorel and Mohri, 2005] to choose which local optimizations to continue.TREGO [Diouane et al., 2021] proposed alternating between global BO and a trust region-based policy for the local phase when the global BO is failing.Al-ternatively, LA-MCTS [Wang et al., 2020] proposes the use of Monte Carlo tree search to learn which subregions of the search space are more likely to contain good objective values.The space is then recursively partitioned based on optimization performance.Similarly, ZoMBI [Siemenn et al., 2023] iteratively keeps the best sample points found so far and "zooms" in the sampling search bounds towards the region formed by those samples.While our approach shares similarities with the above in terms of segmenting the search space, ours sets itself apart by integrating human-friendly hypothesis-based constraints that avoid over-restricting the search.This differentiation emphasizes the innovative use of expert knowledge in our approach, particularly in complex scenarios where such insights are crucial.More importantly, we propose a generalized strategy that allows the injections of multiple expert hypotheses, such as those derived from a multi-person research team, where promising seeds from those hypotheses augment BO's surrogate model data to achieve better-informed sampling.</p>
<p>Methodology</p>
<p>In this section, we propose a novel BO methodology that uses experts' background knowledge in the form of optimality hypotheses to guide search space exploration more effectively.Let {H} J j=1 be a set of manually designed hypotheses w.r.t.promising areas (subspaces) formulated in hyperrectangles and explicitly specified by experts through a system of p equations and q inequalities describing an interval of confidence in the search space:
Ax = b, Bx ≤ c,(2)
where A ∈ R p×d and B ∈ R q×d are coefficient matrices, and b ∈ R p and c ∈ R p are solution vectors.This system filters the space X and forms a solution set, which we refer to as hypothesis subspace H j .In scientific experiments, experts are accustomed to thinking about parameters and conditions in terms of ranges and relationships; formulating these as mathematical constraints is more intuitive than manually setting up a prior distribution.More details and examples on how to create a hypothesis are in the Supplementary Material (SM).</p>
<p>Our goal is to inject practitioners' expertise into the problem at hand by attending to specific regions of the search space based on domain hypotheses, minding at the same time not to over-restrict and negatively bias the search.We, therefore, model the various different hypotheses as local GPs acting on a constrained parameter space and using their output samples as seeds for the global search which in turn is realized by a global GP.The utility of the seeds can be measured using any standard acquisition function, and the topperforming seeds are selected and fed through to the global search.This iterative global-versus-local Bayesian search takes effect interchangeably and is organized in a parametric bilevel optimization framework, which in this instance can be solved sequentially as a two-stage decision problem with each level's variables treated as a parameter for the other [Köppe et al., 2010].The following paragraphs detail and formalize the proposed optimization framework.</p>
<p>Upper Level</p>
<p>In this level, we seek to find the global maximum of f (•) in (1) as in any standard BO task where an acquisition function α(•) is maximized to obtain new candidate samples and evaluate them across its iterations
x * = argmax x∈X α(x, D)(3)
with D = {x i , y i = f (x i )} n i=1 being the observation dataset.To compute α(•), BO relies on constructing a global surrogate model of the underlying function and greatly depends on the initial samples provided as a seed when building this.An appropriate initial sampling has been shown to significantly improve the performance of the search in practice [Morishita and Kaneko, 2023].At this level, one could use practically any variant of BO, but we have empirically observed that using the LA-MCTS algorithm [Wang et al., 2020] helps the search to focus on promising regions to avoid over-exploring.</p>
<p>Lower Level</p>
<p>The lower level initially uses the subspaces from the given hypotheses to perform a local search and yield a set of bestperforming seed samples {s} T t=1 , with T &lt; J, essentially acting as soft constraints on the target objective function f (•).Given that we do not have any analytical information about f (•), we approximate it in the hypothesis subspaces by multiple local GP models ϕ j ∼ N (µ j (x), k j (x, x ′ )) simultaneously, one for each hypothesis subspace H j .The local models ϕ j are chosen as GP surrogate models for their robustness to noise and uncertainty [Ebden, 2015].The local search is realized in a MAB fashion where getting a seed s t translates into selecting the most promising hypothesis via an implicit policy where the hypotheses are the arms before doing a local BO in that hypothesis region.This allows us to evaluate the hypotheses and steer the sampling toward promising regions.</p>
<p>In the initialization phase, before the optimization loop starts, we ensure the hypothesis regions are covered by a specific strategy.For each hypothesis subspace H j , one random sample is drawn, which provides more informative seeds for the global search.If the number of desired initial samples (n) is not exhausted after allocating one sample per hypothesis, the additional random points are drawn from the entire search space to enhance diversity and exploration.In the event that the number of hypotheses exceeds n, we ensure that at least one additional random point is taken from the whole search space, making m = max(1, n − J).This strategy guarantees that each hypothesis is represented from the dataset and prevents scenarios where no initial points fall within the hypothesis regions.</p>
<p>As the optimization progresses, the hypothesis subspaces H j will potentially have more samples, which will update the local models, better evaluating the hypotheses and producing better seeds.Stopping criteria for each level and global convergence are discussed below in Section 3.3.</p>
<p>The complete bilevel framework is formalized as follows
x * = argmax x∈X α(x, {(st, f (st))} T t=1 ∪ D) s.t (Upper) {s} T t=1 ∈ argmax x∈ J j=1 H j {max x∈H j
ϕj}.</p>
<p>(Lower)</p>
<p>Convergence Criteria</p>
<p>To maximize the information gained from good hypotheses (true), we allow the lower level to produce more seed samples until it plateaus.That is, the lower level returns seed samples until these fail to improve upon the best target value:
(1 + γ)y max ≥ f (s i ) if y max ≥ 0 or (1 − γ)y max ≥ f (s i ) if y max &lt; 0 for i = k + 1, . . . , k + l max (4)
where y max is the best value found, i is the current iteration number, k is the iteration number from which the plateauing started, γ ∈ R + is the growth step size, and l max ∈ N + dictates after how many consecutive iterations we deem the lower level plateauing.To mitigate weak and poor hypotheses (false), we allow the upper level to carry the optimization from the given seeds until it plateaus.It keeps maximizing α until it fails to improve upon the best target value, that is:
(1 + γ)y max ≥ f (x i ) if y max ≥ 0 or (1 − γ)y max ≥ f (x i ) if y max &lt; 0 for i = k + 1, . . . , k + u max(5)
where u max ∈ N + dictates after how many consecutive iterations we deem the upper level failed.We set l max ≪ u max to direct the search toward the hypotheses' regions if they are helping to improve while still giving the upper level the time to explore the entire search space X .γ sets the percentage of improvement over the best y found so far, which is considered "significant" for the optimization process.A larger γ means that the algorithm requires a larger improvement to consider the level optimization as still progressing.Conversely, a smaller γ makes the criterion for progress more strict, as even minor improvements will be considered significant.</p>
<p>The optimization steps are detailed in Algorithm 1.</p>
<p>Experiments</p>
<p>We showcase the effectiveness of our proposed method in optimizing various synthetic functions and real-world problems, such as discovering new materials.We test HypBO's performance and robustness using hypotheses of different qualities, ranging from good to poor.We also compare its performance against other BO algorithms.In Section 4.1, we outline the various experimental settings and comparison methods we used to benchmark our results.Sections 4.2 and 4.3 present the outcomes of an analytical function optimization task and a materials design problem [Burger et al., 2020], respectively.</p>
<p>Experimental Setup</p>
<p>We evaluate HypBO's performance empirically for the following two tasks:</p>
<p>• Synthetic Functions We test the precision, convergence speed, and robustness of HypBO using synthetic benchmark functions with various nonconvex landscapes and</p>
<p>Algorithm 1 Hypothesis Bayesian Optimization (HypBO) Input: Hypotheses {H j } J j=1 , Number of initial samples n, Maximum iteration number i max , Improvement growth size γ, Number of locally optimal samples T to keep, convergence parameters l max and u max Output: y max 1: Initialize the dataset D = {}; 2: for each hypothesis subspace H j do 3:</p>
<p>Sample x j randomly from H j ;</p>
<p>4:</p>
<p>Evaluate y j = f (x j ) and add (x j , y j ) to D; 5: end for 6: Randomly sample m = max(1, n − J) points from the entire search space X , evaluate and add them to D; 7: Set y max as the maximum y value in D and i = 0; 8: while i &lt; i max do Lower Level</p>
<p>9:</p>
<p>Set attempt without improvement count l = 0; 10: while l &lt; l max and i &lt; i max do 11:</p>
<p>for each expert-defined hypothesis H j do 12:</p>
<p>Fit a GP ϕ j within the hypothesis D ∩ H j ; 13:</p>
<p>Find the best sample s j maximizing α ϕj ; Set attempt without improvement count u = 0;</p>
<p>23:</p>
<p>while u &lt; u max and i &lt; i max do 24:</p>
<p>Fit a GP on the entire search space D;</p>
<p>25:</p>
<p>Find the best sample x * maximizing α;
26: Evaluate x * , y * = f (x * ); 27:
Increment u if there is no improvement, i.e. y * ≤ y max + γ, else reset u to 0;</p>
<p>28:</p>
<p>Update the records D ← D ∪ {(x * , y * )};
29: i ← i + 1; 30:
end while 31: end while 32: return The maximum value found, y max dimensionalities.The optimization performance is measured using simple and cumulative regrets, and Wilcoxon tests [Rey and Neuhäuser, 2011].The maximum number of iterations is limited to 100, and the result of 50 repeated trials is reported as the mean value.</p>
<p>• Photocatalytic Hydrogen Production Here, we replicate the materials design problem addressed in [Burger et al., 2020], aiming to maximize the hydrogen evolution rate (HER) from a mixture of different materials.We follow a more cost-effective approach and emulate that chemistry experiment by interpolating new HER measurements using a GP model trained on existing experimental data points.In section 4.3, we give a comprehensive explanation of this chemistry task.The maximum  number of iterations is set to 300, and the mean value of 50 repeated trials is reported.</p>
<p>We further evaluate HypBO against the following baselines whose hyperparameters' values are given in the SM:</p>
<p>• Random Search (RS) Random search under uniform distribution over the search space.</p>
<p>• Trust Region Bayesian Optimization (TuRBO) with one trust region.</p>
<p>• Latent Action Monte Carlo Tree Search (LA-MCTS)</p>
<p>• LA-MCTS with hypothesis-based initial design (LA-MCTS+) We modified the previous baseline to initialize it exclusively within the hypothesis subspaces before the regular search in the entire search space.</p>
<p>• πBO This baseline uses expert knowledge throughout optimization.As described in Section 2, it is one of the most competitive methods for priors over optimum.We converted our hypotheses into Gaussian priors centered around the hypothesis subspace center (see SM).</p>
<p>For all experiments, we use preset hyperparameters for HypBO.We set the lower level limit l max to 2, the upper level limit u max to 5, the number of locally optimal samples T to 1, and the growth rate γ to 0. This value of γ essentially means that as long as any improvement is being made (no matter how small), the level optimization will continue.Note that this could be beneficial in scenarios where even small gains are valuable, but it might also make the optimization process slower or more prone to getting stuck in flat regions where minute fluctuations might appear as improvements.Ablation studies can be found in the SM.Concerning the local GP models for the hypotheses, they have a zero mean and a Matérn (ν = 2.5, λ i = 1) kernel with constant scaling.Note that the kernel's hyperparameters are automatically optimized based on the experimental data to fit the models best.All experiments are warm-started with five initial points except for the photocatalyst hydrogen production experiment with mixed hypotheses, whose initial sample count is 10.Reproducibility details are available in the SM.</p>
<p>Synthetic Functions</p>
<p>Hypotheses A good hypothesis subspace is essentially an interval that contains the optimum, opt.By contrast, a weak/poor one does not.The further the weak hypothesis subspace is from the optimum, the worse it is.Here, the "poor" hypothesis is the furthest from the optimum.The hypotheses are hyperrectangles of width w = 2 units and centered as follows:</p>
<p>• Poor hypothesis at l b +w/2 where l b is the lower bound of the search space.</p>
<p>• Weak hypothesis at opt − 0.2 * (opt − l b ) − w/2.</p>
<p>• Good hypothesis at opt.</p>
<p>We assess HypBO empirically in two different settings.First, we evaluate its performance and robustness against the quality of the hypothesis.Second, we test its ability, when faced with mixed hypotheses simultaneously, to discard the weaker hypotheses and prioritize promising ones.</p>
<p>Optimization With a Single Hypothesis</p>
<p>Figure 2 shows that HypBO benefits from informative hypotheses and can also recover from weak ones.The method improves the search performance dramatically over RS, TuRBO, and LA-MCTS for a good hypothesis.The seeds from that hypothesis aid in recognizing the promising subspace and focusing efforts there, resulting in a faster location of the optimum.A similar behavior is observed in both LA-MCTS+ and πBO, whose initial sampling is entirely done in the good hypothesis region.Concerning the weak hypothesis, HypBO converges toward the optimum faster than LA-MCTS, TuRBO, and RS.In fact, the seeds coming from the weak hypothesis, by outperforming the existing samples in the dataset, direct HypBO towards the hypothesis' surroundings, too, which are more promising.As would be expected, poor hypotheses lead to a slower search in the early stages, but HypBO displays desired robustness by recovering from the poor seeds to approximately equal regret as LA-MCTS and TuRBO.However, it is interesting to note that HypBO with a poor hypothesis outperforms LA-MCTS and TuRBO in high-dimensional functions due to its more diverse initial sampling strategy.Its initial sampling strategy combines one sample from the poor hypothesis with others from across the search space, leading to a more comprehensive understanding of the overall landscape.As shown in Figure 3, this approach efficacy seems to increase with the search space complexity, i.e., its dimensionality, making the advantage of diverse sampling more pronounced.Both LA-MCTS+ and πBO lag behind HypBO in these two last hypothesis scenarios as their initial sampling being entirely made of samples from the weak (respectively poor) hypothesis region is not diverse enough, and πBO's trade-off decay hyperparameter β keeps it unnecessarily longer in that weak (respectively poor) region as shown in Figure 3.This highlights HypBO's ability to exploit the explicit and implicit information the hypothesis provides faster and more intelligently.Moreover, we conducted Wilcoxon signed-rank tests [Rey and Neuhäuser, 2011] (at a 95% confidence level with Bonferroni correction [Bonferroni, 1936]) and examined the mean and median cumulative regrets.The tests reveal that HypBO performs significantly better than RS, LA-MCTS, and TuRBO.Although the p-values for comparisons with LA-MCTS+ and πBO were not statistically significant, they were low for πBO (p = 0.06 and 0.15 for weak and poor hypotheses), indicating πBO's weaker performance.Moreover, HypBO showed higher median and mean regrets compared to LA-MCTS+ and πBO variants for weak and poor hypotheses, leading us to conclude that HypBO generally outperforms LA-MCTS+ and πBO.</p>
<p>Optimization With Mixed Hypotheses</p>
<p>We use three different binary combinations of hypotheses of varying quality to test HypBO's ability to uncover and prioritize promising hypotheses, and to discard bad ones from a pool of hypotheses.HypBO takes seeds from all the given hypotheses, which it uses to update its beliefs about each hypothesis via the MAB procedure.As the optimization progresses, it has a better representation of the hypotheses and can abandon the weaker one of the pair and select seeds from the more promising hypothesis.As shown in Figure 4, for the Ackley d9 function, this approach allows HypBO to deselect the weaker hypothesis early on.It keeps the remaining stronger hypothesis, which it uses to expedite the search as described in the previous subsection.Figure 5 shows that these findings are consistent when applied to a variety of synthetic functions of higher dimensions.For lower dimensions, HypBO with mixed hypotheses has approximately equal regret performance to TuRBO, LA-MCTS, and LA-MCTS+ as the search space is smaller, and it becomes easier to capture the underlying behavior of the objective function.For higher dimensions, even in the case of combined weak and poor hypotheses, HypBO outperforms the other methods, demonstrating its robustness when faced with multiple items of inaccurate knowledge and the ability to use these for better sampling.</p>
<p>Here, the Wilcoxon tests with Bonferroni correction show</p>
<p>Photocatalytic Hydrogen Production Optimization</p>
<p>We test HypBO on a real materials design problem where we seek an optimal composition of ten materials to maximize hydrogen production via photocatalysis [Wang et al., 2019].</p>
<p>Due to the combinatorially large search space (98,423,325 possible combinations), [Burger et al., 2020] used an autonomous mobile robotic chemist along with a discretized Bayesian optimizer (DBO), which can discretize the input space, to search for the optimal combination of materials.</p>
<p>We recast this experimental problem as a more costeffective multivariable simulation; that is, we mapped out the chemical space by interpolating available experimental observations using a Gaussian process regression (GPR).Specifically, this GPR model has a zero mean, a Matérn (ν = 2.5) kernel with constant scaling and homoscedastic noise; each variable lengthscale λ i is initialized as its discretization step.We fitted this model against a total "ground truth" dataset of 1119 experimental observations supplied by the authors of [Burger et al., 2020].While the interpolated model is only approximate, close inspection suggested that it is broadly representative of the known real chemical space and sufficiently accurate to draw safe conclusions here.Our main goal is to test whether we can capture and inject experts' knowledge and intuition towards a better-informed and faster search.For a fair comparison, in place of TuRBO, LA-MCTS, and LA-MCTS+, we use the same DBO developed by [Burger et al., 2020] for experimental photocatalysis hydrogen production, capable of discretization, as a baseline.</p>
<p>Retrospective Application of Knowledge</p>
<p>First, we used HypBO to fold in, retrospectively, knowledge of the underlying chemistry that was not captured in the [Burger et al., 2020] study to investigate whether injecting , 2020] study that could not have been injected using DBO; here, it is injected, retrospectively, using HypBO.</p>
<p>• Perfect Hindsight limits the search to within the optimal subspace based on post facto knowledge of the outcomes of all 1119 robotic experiments.</p>
<p>• Bizarro World purposefully focuses the search within the worst areas of the chemical space in all dimensions.</p>
<p>As shown in Figure 6, HypBO with 'What They Knew' boosts performance somewhat in the early stages of the search, and overall it improves upon DBO, thus validating the benefits of considering expert hypotheses in real-world problems.For example, one can posit that any or all of the three dye components (MB, AR87, RB) might be beneficial but that high values would be counterproductive, based on chemical reasoning.We captured this in 'What They Knew' by lowering the dyes' upper bounds (MB ≤ 0.5mL, AR87 ≤ 1mL, RB ≤ 0.5mL).The somewhat modest boost given by 'What They Knew' (Figure 6) can be explained by the partial knowledge available in 2019; indeed, some of 'What They Knew' was, in fact, wrong.For example, as reported in [Burger et al., 2020], all three dyes were strongly negative at all concentrations.We have not captured this post-experiment knowledge here; rather, 'What They Knew' captures the knowledge that was available to this team in 2019, building on their initial formulation of hypotheses, prior to any robotic experiments.</p>
<p>Unsurprisingly, 'Perfect Hindsight' leads to a much faster optimization.By contrast, although the artificially bad case of 'Bizarro World' does lead to a slower search than DBO, the effects are greatly mitigated because HypBO can abandon unproductive hypotheses.</p>
<p>Searching the Chemistry Experiment Space with Mixed Hypotheses</p>
<p>We test HypBO's ability to exploit good hypotheses and discard bad ones in a more realistic setting by creating a team of nine 'virtual chemists', each with a virtual hypothesis based on plausible chemical reasoning detailed in the SM.The combined "knowledge" of this virtual team was then used to redo the simulated experiment for [Burger et al., 2020] in tandem Figure 7: HypBO pruning the hypotheses and selecting seeds from the most promising ones to boost the optimization.Left: Scatter plot of the HypBO optimization with all nine virtual hypotheses; color denotes followed hypothesis, if any.Right: Best value obtained so far by HypBO using all nine hypotheses compared to DBO and RS.</p>
<p>with HypBO.The virtual team was designed to emulate the diverse and sometimes contradictory views of a real research team tackling a new problem.For example, some pairs of hypotheses (e.g., 'Halophile' / 'Halophobe') are in direct contradiction.Based on retrospective knowledge, 'Dye Sceptic' and 'Surfactant Sceptic' might be expected empirically to be the strongest hypotheses, while 'Dye Fanatic' is probably the weakest.We applied all nine virtual hypotheses simultaneously and used the same oracle model described in the section above.For the purposes of these initial tests, all virtual hypotheses were considered of equal weighting.Figure 7 illustrates the power of including human insights throughout the optimization.All hypotheses were selected initially, but as the optimization progressed, HypBO filtered out bad hypotheses and prioritized the most promising ones, improving the search compared to DBO and RS.While the least profitable hypotheses, such as 'AR87 Obsessed', 'Dye Fanatic', and 'Halophobe', were deselected early on, HypBO does not discard them completely.For example, 'Halophobe' was selected at times when its EI was greater than that of others that were available for evaluation.This captures the importance of re-evaluating hypotheses in the face of new data.Likewise, certain hypotheses are used while they are profitable and then discarded when they become delimiting; this can be observed for 'Scavenger Obssessive', where some scavenger is indeed required, but not too much.The modest search improvement of the virtual chemist team over DBO (Figure 5, right) is somewhat arbitrary because we purposefully built this virtual team to be mediocre, with both "good" (informative) and "bad" (uniformative or misleading) hypotheses in near equal numbers.</p>
<p>Conclusions</p>
<p>To fully exploit the opportunities in laboratory robotics and automation, we need optimization methods that work in tandem with teams of human scientists.So far, BO has not fully leveraged the experience and hunches of experimenters.We harness that knowledge here by allowing them to inject their hypotheses about which parts of the input space will yield the best performance.We propose a BO variant, HypBO, that achieves this in a bi-level framework by recursively pruning and turning the hypotheses into seeds that augment sampling as a springboard for global optimization.HypBO stands out in its ability to concurrently handle and evaluate multiple expert-formulated hypotheses.It can also use weak hypothe-ses to converge faster than cases with no hypotheses and recover from poor ones.This highlights the power of humancomputer interaction, re-imagining the role of humans in autonomous scientific discovery.Future work includes initializing the hypotheses with weights based on the experimenter's profile or confidence estimation.These weights might be particularly valuable in quick-starting hypothesis selection in large, diverse research teams, where expertise levels and domain specializations can vary quite widely.</p>
<p>Supplementary Material A Hypothesis Construction</p>
<p>In this section, we describe the process to construct your hypothesis.A hypothesis involves using your domain knowledge and empirical observations as well as hunches to define your belief about the subspace of the search space containing the optimum or promising samples.To construct a hypothesis, you must translate that belief in terms of ranges and relationships.As described in the Methodology section of our work, we represent a hypothesis H as a system of manually designed constraints w.r.t.promising subspaces specified by an expert through a system of p equations and q inequalities describing an interval of confidence in the search space, see Equation 2. Example 1 (Maximizing the yield of a vegetable farm).The task is to maximize the yield in a vegetable farm.Let us assume there are three parameters for vegetable production with these ranges in this order:</p>
<p>• Watering:
0.5 l/m 2 ≤ Water ≤ 7 l/m 2 • Fertilization: 5 g/m 2 ≤ Fertilizer ≤ 85 g/m 2
• CO 2 Levels: 300 ppm ≤ CO 2 ≤ 1000 ppm The farmer might hypothesize that optimal conditions involve specific ranges for watering 1.5 l/m 2 ≤ Water ≤ 2.5 l/m 2 and fertilization 20 g/m 2 ≤ Fertilizer ≤ 30 g/m 2 , but has no idea about the optimal CO 2 level.The hypothesis schema accepted by HypBO, which is a writing in matrix form of the previous sentence, will be:
       −1 0 0 1 0 0 0 −1 0 0 1 0 0 0 −1 0 0 1        ×x ≤        −1.5 2.5 −20 30 −300 1000        → means 1.5 l/m 2 ≤ Water → means Water ≤ 2.5 l/m 2 → means 20 g/m 2 ≤ Fertilizer → means Fertilizer ≤ 30 g/m 2 → means 300 ppm ≤ CO2 → means CO2 ≤ 1000 ppm
These soft constraints form a hypothesis that HypBO will assess and focus its search within these specified ranges or in its vicinity if found promising, to find the optimal combination for the highest yield.</p>
<p>More examples of hypothesis creation can be found in Section C (Photocatalytic Hydrogen Production) of this document.</p>
<p>A.1 Turning HypBO hypotheses into prior distribution for πBO</p>
<p>To use πBO as a baseline in the Experiments section of our work, we converted our three hypotheses of different qualities (good, weak, and poor) into prior distributions.πBO can Algorithm 2 Converting HypBO hypothesis into a πBO Gaussian prior Input: HypBO hypothesis H j , πBO's JSON prior file prior f ile.</p>
<p>Output: The updated πBO prior file, prior f ile 1: Get the HypBO hypothesis input parameter ranges by solving the system of equations and inequalities in Equation 2; 2: for each input parameter do 3:</p>
<p>The mean of the GP µ is the center of the input parameter range; 4:</p>
<p>The standard deviation of the GP σ is 6 th of the range so that 99% of the GP values fall within the range; 5:</p>
<p>Append the input parameter GP values to the πBO JSON file; 6: end for 7: return The updated πBO prior file, prior f ile work with Gaussian priors in the following format [Luinardi, 2024] Given that schema of prior injection and our schema of hypothesis, we converted each hypothesis into a πBO prior as described in Algorithm 2:</p>
<p>B Ten-dimensional Model for Hydrogen Production</p>
<p>In Section 4.3 of our work (Photocatalytic Hydrogen Production Optimization), we test HypBO against a real chemical problem.To do that, we constructed a ten-dimensional model describing hydrogen production for the experiments described in [Burger et al., 2020].To do this, we augmented the original dataset of 688 experiments with a further and supplied by the authors of Burger et al. to create a total 'ground truth' dataset of 1119 experimental observations.The model was then built by fitting a Gaussian process regression (GPR) against the augmented dataset with a composite kernel consisting of Matern similarity, constant scaling and homoscedastic noise kernels.This hybrid kernel allows for variable smoothness and simulated experimental noise.Figure 8, below, shows the experimental data (blue and red points) plotted along with 812 "virtual" data points derived from this model (green points) in all ten dimensions.These plots suggest that the model adequately captures the behavior described by the combined experimental dataset.It should be noted that the new experiments found compositions that produce more hydrogen than any compositions reported in [Burger et al., 2020].We believe that this is reasonable because the dimensionality of the search space (10 variables) is large compared to the original experiment run (688 experiments), and there was no guarantee of optimality in that search.Notably, the GPR model reproduces the strongly negative influence of the three dyes, RB, AR87 and MB, (Figure 8a-c).It also appears that the new set of experiments (red points) might have discovered a new sub-space where NaCl contributes to the hydrogen production (Figure 8h).</p>
<p>C Retrospective Hypotheses about the Photocatalytic Hydrogen Production</p>
<p>This section refers to the "Retrospective Application of Knowledge" subsection and describes the mathematical representation of the retrospective hypotheses we derived from the work of Burger et al. to use with HypBO.In that study, they detailed, in their description of the design of the experiment, some additional chemical knowledge they had about photocatalytic hydrogen production that was not captured by their discretized Bayesian optimizer (DBO).The optimizer could not account for this knowledge, which here constitutes the retrospective knowledge that we refer to in the paper as "What they knew".Additionally, Burger et al. 's chemistry experiment made 688 experimental observations that lasted eight days, extended here with a further 431 experiments supplied by the same team.By analyzing this augmented dataset of 1119 experiments, we derived two artificial items of knowledge.One is "Perfect Hindsight", which represents the subspace of best samples found to maximize hydrogen production, while the other is "Bizarro World", which represents the subspace of samples known to minimize hydrogen production.</p>
<p>C.1 What They Knew</p>
<p>To generate the hypothesis for the "What they knew" optimization test, a series of 10 sub-hypotheses was created based on mathematical constraints to capture the additional untapped chemistry knowledge of Burger et al., or at least what they thought they knew, prior to the experiments that were carried out in [Burger et al., 2020].These ten constraints, and the associated chemical rationale, are listed below.The hypotheses were constructed from a combination of facts stated in the Burger et al. paper and more general chem-ical reasoning that would be available to any typical chemist.For example, Burger et al. set up their experiments so that the total volume was always 5 mL: as such, if 5 mL of any single component is added, then this does not leave any space for other components, and hence this might be too much.</p>
<p>Hypothesis 1. P10 = 5 mg Chemical rationale: Generally, the amount of hydrogen produced in photocatalytic systems increases with the amount of semiconductor photocatalyst (in this case, P10).Hence, in the total range of 1-5 mg that Burger et al. allowed, there was no obvious reason not to add the maximum amount of P10 since the hydrogen evolution rate-the search objective-was an absolute value that was not normalized to the mass of P10.This constraint (P10 = 5 mg) effectively removes one dimension from the search space.</p>
<p>Hypothesis 2. Cys = 1 − 4 mL Chemical rationale: Burger et al. stated that some cysteine would be required; that is, Cys &gt; 0, because this is a sacrificial hydrogen production reaction, but it was not known, prior to experiments, how much cysteine would be optimal.However, because they constrained the maximum volume of the mixture of compounds to be 5 mL, then it might be undesirable to set, say, Cys &gt; 4mL since this would not then allow enough volume for adding other components.Using this simple reasoning, a range of Cys = 1 − 4mL can be hypothesized, rather than the range of Cys = 0 − 5mL that was used in the original experiments in [Burger et al., 2020].</p>
<p>Hypothesis 3. MB &lt; 0.5 mL Chemical rationale: Burger et al. expressed that adding dyes to the reaction was based on earlier observations that dyes could sensitize related photocatalysts, thus increasing hydrogen production [Wang et al., 2018].However, as mentioned in [Burger et al., 2020], it was not known before experiments whether these three dyes (MB, AR87, RB) would be positive or negative in the case of P10.Still, one might reasonably expect that very high dye concentrations (e.g., &gt; 4mL) could (a) absorb all of the light, thus lowering the hydrogen evolution rate, and (b) not leave enough volume for other components, as argued for Cys, above.Given the high light absorption coefficients of these three dyes, we conjecture that 'a little' might be enough; as such, a constraint of M B &lt; 0.5mL was formulated.</p>
<p>Hypothesis 4. RB &lt; 0.5 mL Chemical rationale: Exactly as for MB (Hypothesis 3).</p>
<p>Hypothesis 5. AR87 &lt; 1 mL Chemical rationale: As above for the dyes MB and AR87 except that here, the upper range, 1 mL, is larger because there was pre-existing evidence in [Wang et al., 2018] that this dye (called Eosin Y in that paper) could sensitize a related photocatalyst.As such, this dye might have been given a higher upper bound prior to experiments even though, in reality, Burger et al. found it to be a negative component in the reaction mixture.Hypothesis 6. NaOH &lt; 3 mL Chemical rationale: Burger et al.'s rationale for including NaOH was that solution pH might influence hydrogen production, but it was not known whether this would be positive or negative.A chemist might also have a potential concern that high concentrations of NaOH might degrade other components in the mixture, although this was not discussed by Burger et al.Also, as for Cys (Hypothesis 2), there is an argument for constraining the volume of NaOH to allow free volume for adding other components.These combined arguments led us to an estimated constraint of N aOH &lt; 3mL, allowing (unlike Hypothesis 2) for a value of NaOH = 0 since from our reading of Burger et al. it was not obvious, a priori before experiment, that NaOH addition would have any positive effect.</p>
<p>Hypothesis 7. NaCl &lt; 3 mL Chemical rationale: There was no a priori evidence that NaCl would be either positive or negative before experiments; as such, the constraint of 3 mL is to allow sufficient free volume for other components, exactly as for Hypothesis 6 above.</p>
<p>Hypothesis 8. SDS &lt; 1 mL Chemical rationale: It was argued in [Burger et al., 2020] that the primary rationale for adding surfactants was that it could aid in dispersing the photocatalysis, P10, in water.There was no evidence for this prior to the experiment.Still, given the highly surface-active nature of SDS, we might have hypothesized that a little surfactant could be enough, as argued for the three dyes (Hypotheses 3-5), hence SDS &lt; 1mL.</p>
<p>Hypothesis 9. PVP &lt; 2 mL Chemical rationale: PVP is also a surfactant, and hence the rationale is the same as for SDS (Hypothesis 7) -the larger range allowed here (maximum 2 mL instead of 1 mL) is because PVP is less surface-active than SDS at a given concentration.Hence, a higher volume might be required to be effective.</p>
<p>Hypothesis 10.NaDS &lt; 4 mL Chemical rationale: As for NaOH (Hypothesis 6) -no prior information was available for the effect of this component, so the constraint arises from the need to allow some residual volume for other components, e.g., for CyS, which should be non-zero (see Hypothesis 2).</p>
<p>C.2 Perfect Hindsight</p>
<p>This hypothesis limits the search to within the optimal subspace based on post facto knowledge of the outcomes of all 1119 robotic experiments (see Figure 8).It is described by the following ten constraints:</p>
<ol>
<li>P 10 ≥ 3.5 mg</li>
</ol>
<p>D Virtual Chemists</p>
<p>This section refers to the subsection "Searching the Chemistry Experiment Space with Mixed Hypotheses", in which we test HypBO's ability to exploit good hypotheses and discard bad ones in a more realistic setting.This test mimics an experiment designed by a research team that holds a range of different opinions.To do this, we created nine 'virtual chemists' who hold hypotheses based on plausible chemical reasoning, which is outlined below.We purposefully designed this virtual team to be in conflict, with 'good' and 'bad' hypotheses in near equal numbers.As such, this is a deliberately mediocre virtual team: it was built to test the ability of HypBO to sort good and bad hypotheses, rather than to create a performance boost in the optimization, although as shown in the main text, it does in fact give a small improvement to the search.This virtual chemist team reveals how each virtual chemist brings unique perspectives and strategies to the optimization process.It is also important to note a parallel with [Adachi et al., 2023].This work highlights the importance of integrating varied human expert perspectives into Bayesian optimization, and is a concept that aligns with our approach utilizing virtual chemists.</p>
<p>D.1 Virtual Chemist #1: "Dye Sceptic" This chemist does not believe that dyes will have a positive effect on the reaction but has no other opinions about the value of other components in the mixture (that is, all other variables are allowed to range between the original low-high values when this hypothesis is applied).This hypothesis is expressed by the following constraints:</p>
<ol>
<li>M B = 0 mL 2. AR87 = 0 mL 3. RB = 0 mL D.2 Virtual Chemist #2: "Dye Fanatic"</li>
</ol>
<p>This chemist believes that dyes will have a positive effect and that a high dye concentration is required to achieve this; for example, to push the chemical equilibrium to achieve surface dye absorption on the photocatalyst.However, this chemist does not know which dye is best (all three are equally likely).</p>
<p>Hence the total dye concentration is constrained to be greater than 3 mL; again, this chemist has no opinions about other components in the reaction.This hypothesis is expressed by the following constraints: 1. M B + AR87 + RB &gt; 3 mL D.3 Virtual Chemist #3: "AR87 Obsessed"</p>
<p>Like Virtual Chemist #2, this chemist believes that dyes will have a positive effect but is convinced moreover that a specific dye, AR87, is optimal having read the earlier report that this dye was successful with other related photocatalysts [Wang et al., 2018].Virtual Chemist #3 believes that the other two dyes are likely to be less effective than AR87.As such, the recommendation of Virtual Chemist #3 is to 'play in the dye space' but with a strong emphasis on AR87, as expressed by: 1. AR87 &gt; 3 mL 2. M B &lt; 0.5 mL 3. RB &lt; 0.5 mL D.4 Virtual Chemist #4: "Surfactant Sceptic"</p>
<p>This chemist believes that surfactants will be bad for the reaction but has no other opinions about any other components, as expressed by: 1. SDS = 0 mL 2. P V P = 0 mL D.5 Virtual Chemist #5: "Scavenger Obsessive"</p>
<p>This chemist is convinced that a very high concentration of the scavenger, Cys, is needed to achieve high levels of hydrogen production, as expressed by: 1. Cys &gt; 4 mL D.6 Virtual Chemist #6: "pH Fanatic"</p>
<p>This chemist believes high pH is needed to boost hydrogen production.Both NaOH and NaDS are bases and will increase the pH, and this chemist considers both to be interchangeable, leading to the hypothesis: 1. N aOH + N aDS &gt; 3.5 mL Table 1: Details of the HypBO variants used in the ablation study.</p>
<p>D.7 Virtual Chemist #7: "H-bond Lover"</p>
<p>This chemist believes that H-bonding with NaDS will lead to increased hydrogen production, based on reading earlier publications made similar hypotheses about NaDS, e.g., [Zhang et al., 2019], leading to the hypothesis:</p>
<ol>
<li>N aDS &gt; 3.5 mL D.8 Virtual Chemist #8: "Halophile"</li>
</ol>
<p>This chemist believes that high ionic strength is crucial for the success of the reaction-that is, a high concentration of salt; NaOH, NaDS and NaCl are all salts, leading to the hypothesis:</p>
<ol>
<li>N aOH + N aDS + N aCl &gt; 3.5 mL D.9 Virtual Chemist #9: "Halophobe"</li>
</ol>
<p>In contrast to Virtual Chemist #8, this chemist believes that high NaCl concentration is bad for the reaction because they half-remember seeing a conference presentation that claimed that NaCl addition could lead to chlorine gas production, rather than hydrogen production.This leads to the hypothesis:</p>
<ol>
<li>N aCl = 0 mL</li>
</ol>
<p>E Ablation Studies</p>
<p>Here, we study HypBO's sensitivity to its hyper-parameters.</p>
<p>In Section E.1, we focus on the impact of the choice of u max and l max before delving into γ's effect in Section E.2.</p>
<p>E.1 l max vs u max Ablation Study</p>
<p>We conducted an ablation study to analyze how sensitive HypBO is to the lower and upper failure thresholds l max and u max in the Convergence Criteria section 3.3 of our research.</p>
<p>Our goal was to find if there was an optimal balance of upper and lower level failure thresholds.To achieve this, we repeated the optimization experiments on the synthetic functions with a single hypothesis (Poor, Weak, and Good) and mixed hypotheses (Poor &amp; Good, Poor &amp; Weak, and Weak &amp; Good) as described in Section 4.2 (Synthetic Functions) of our work with different combinations of l max and u max .We fixed u max to 10 and incrementally increased l max from 1 to 10, getting ten variants of HypBO spanning from a slight to an equal focus on the hypothesis regions.We extended these variants with two other variants:</p>
<p>• The strong focus case on the hypothesis regions with u max = 1 and l max = 10.</p>
<p>• The default case in our work where u max = 5 and l max = 1.For notation simplicity, we will refer to u max as u, and l max as l.We ran the experiments against those twelve variants as reported in Table 1.The maximum number of iterations was limited to 100, and the result of 50 repeated trials was reported as the mean value.</p>
<p>Figure 9 and 10 show that there seems to be a trend with the regret increasing with l and decreasing with u, meaning a strong focus on the hypothesis regions might be detrimental.They also show that u10 l1 seems to be the optimal combination of u and l that minimizes regret.These findings are confirmed with a regression analysis under the assumption that the relation between u l and the regret is linear.</p>
<p>Concerning the sensitivity of HypBO, under each hypothesis scenario, to u and l, Figure 11 reveals that the sensitivity decreases with better hypothesis case qualities.Overall, all scenarios are robust to changes in u and l except for the pure poor hypothesis case.</p>
<p>Considerations of using u5 l2 as the default From the previous results, u10 l1 is the optimal choice.However, they are practical and other considerations to take into account for the choice of u and l, which might motivate choosing a different u l combination.In our work, we chose u5 l2.u10 l1's mean regret is just 5% less than u5 l2's mean regret.Figure 12 shows that u5 l2 performs relatively well compared to several other u l combinations but is outperformed by u10 l1.We extended this comparison with the following considerations:</p>
<p>• Balanced Approach: The combination u5 l2 offers a relatively balanced trade-off between exploring the entire input search (u = 5) and focusing on the hypothesis subspace (l = 2).This balance can be advantageous in various scenarios where neither extreme exploration nor focused exploitation is desired.• Robustness: Because u5 l2 shows consistent performance across different functions and hypothesis mixtures, it could be a more robust choice, providing reasonably good results in a wide range of scenarios.• Practicality: In black-box scientific experiments, we assume that good hypotheses with narrow ranges are rare.Most experimenters would provide weak ones and possibly a combination of weak and good hypotheses, as we saw with the "What They Knew" scenario of Section C. In those scenarios, Figure 10 (c and f) shows that u5 l2 can outperform u10 l1.In summary, although u5 l2 may not be the optimal choice for minimizing regret, it offers a well-balanced and potentially more robust and practical option as a default setting.However, it is essential to further validate this recommendation by taking into account the specific characteristics and requirements of the optimization problem being tackled with HypBO.</p>
<p>E.2 γ Ablation Study</p>
<p>γ sets the percentage of improvement over the best y found so far, which is considered "significant" for the optimization process.In this section, we present an analysis of the sensitivity of HypBO to the growth rate γ in the Convergence Criteria section 3.3 of our research.We conducted optimization experiments on the synthetic functions Branin d2 , Sphere d2 and Figure 13 demonstrates that HypBO is relatively robust to changes in γ with some slight trends.With a Good hypothesis, HypBO's mean regret slightly decreases with an increase in γ, while with a Weak / Poor hypothesis, its mean regret slightly increases with γ.This observation correlates with the time HypBO spends in the lower level, i.e., hypothesis region, increasing with γ as shown in Figure 14.As increasing γ makes the criteria for progress more strict, HypBO oscillates more between the upper and the lower levels.Consequently, this brings the percentage of the time spent in the lower level closer to the ratio l l+u .It is important to note that because the Good hypothesis contains the optimum, the percentage of time spent in its region is somewhat arbitrary.</p>
<p>In summary, the performance of HypBO remains consistent across γ.However, it is worth noting that the performance shows a marginal decline as γ increases under a Weak or Poor hypothesis, while under a Good hypothesis, HypBO is quite robust to changes in γ.As a result, we recommend a maximum γ value of 10%.</p>
<p>F Reproducibility</p>
<p>This section details the steps to reproduce the experiments and results in HypBO: Expert Hypotheses to Guide Bayesian Search in Material Discovery.</p>
<p>F.1 Experimental Setup</p>
<p>Here, we describe our experiments' specific configurations, parameters, and environments.This includes the detailed settings of the baselines and the software specifications and versions.</p>
<p>Baselines</p>
<p>• Random Search (RS) Random search under uniform distribution over the search space.</p>
<p>• Trust Region Bayesian Optimization (TuRBO) We set the number of trust regions m = 1, the success threshold τ succ = 3, the failure threshold τ f ail = [d/q] where d is the number of dimensions and q is the batch.The batch size is q = 1.The base side length L init = 0.8, its minimum L min = 2 −7 and maximum L max = 1.6.</p>
<p>• Latent Action Monte Carlo Tree Search (LA-MCTS)</p>
<p>We set LA-MCTS' hyperparameters as follows: C p = 1, θ = 10, gamma type is auto, SVM boundary kernel is RBF, and the solver is BO with a Matern kernel.</p>
<p>• LA-MCTS with hypothesis-based initial design (LA-MCTS+) We modified the previous baseline to initialize it exclusively within the hypothesis subspaces before the regular search in the entire search space.Its hyperparameters' values are the same as those of LA-MCTS.</p>
<p>• πBO We kept the default value of β = i max /10, where i max is the total number of iterations [Hvarfner et al., 2022].</p>
<p>Software Specifications</p>
<p>To ensure consistency in software environments, each baseline environment is a Docker container whose Dockerfile is saved in the baseline's folder in the code repository.Docker is a tool that simplifies application creation, deployment, and running using containers.Unlike Python environments, these containers encapsulate an application's entire runtime environment, including system libraries and settings.Because Docker containers are not affected by variations in host system configurations, Docker is well-suited for replicating complex applications' environments, which reduces "it works on my machine" problems, a level of consistency that Python virtual environments can't provide on their own.</p>
<p>F.2 Data Accessibility</p>
<p>All datasets used in this study, including the dataset used to train the GPR to replicate the Photocatalytic Hydrogen Production Optimization experiment in [Burger et al., 2020], as well as that replica and the synthetic functions experiments' results, and ablation studies, are made publicly available at the Anonymous Github repository, HypBO, in the data folder.We provide the link to the HypBO repository https://anonymous.4open.science/r/HypBO/,ensuring that researchers can access and utilize the exact data for replication.</p>
<p>F.3 Source Code</p>
<p>To ensure that our results can be replicated exactly, we have made available the complete source code of HypBO, including scripts for data preprocessing, model training, and result analysis.This source code also includes the source codes of the baseline methods LA-MCTS (https://github.com/facebookresearch/LaMCTS/),πBO (https://github.com/luinardi/hypermapper/wiki/Prior-Injection),and TuRBO (https://github.com/uberresearch/TuRBO)cloned from their respective repositories.</p>
<p>HypBO source code has been anonymized and can be accessed via the Anonymous Github https://anonymous.4open.science/r/HypBO/.</p>
<p>Furthermore, we have provided clear documentation on how to run the experiments and reproduce the results.</p>
<p>Figure 1 :
1
Figure 1: Illustration of three hypothesis locations in the form of confidence regions on the 1D Ackley function.The different colors (red, orange, and green) correspond to different levels of confidence (poor, weak, and good, respectively).</p>
<p>Figure 2 :
2
Figure 2: Comparison of RS, LA-MCTS, LA-MCTS+, πBO, and HypBO on Levyd20 for various hypothesis qualities (see Figure 1).Solid lines show the mean values, while the shaded areas represent the standard error.</p>
<p>Figure 3 :
3
Figure 3: Cumulative regret on functions with various dimensions.</p>
<p>Figure 4 :
4
Figure 4: HypBO on the 9D-Ackley function with three different mixtures of hypotheses of various qualities for 200 iterations.Colored sample points came from the hypotheses, i.e., the lower level, while the grey ones came from the upper level.</p>
<p>Figure 5 :
5
Figure 5: Regret on synthetic functions with mixed hypotheses.</p>
<p>Figure 6 :
6
Figure 6: Retrospective application of hypotheses derived from [Burger et al., 2020] using HypBO, compared to the no hypothesis run using DBO and RS.Shaded area is the standard deviation.</p>
<p>Figure 8 :
8
Figure 8: Plots showing the amount of hydrogen produced for 688 original experiments (blue points, [Burger et al., 2020]) and for 431 new experiments, supplied by the same authors, conducted under the same conditions (red points).The green points (812 data points shown here) are derived from a model fitted against a combination of these two experimental datasets.</p>
<ol>
<li>1
1
mL ≤ Cys ≤ 3.5 mL 3. 0.5 mL ≤ N aOH ≤ 2 mL 4. 0 mL ≤ N aDS ≤ 1.5 mL 5. 2 mL ≤ Cys + N aOH + N aDS ≤ 4.5 mL 6.1 mL ≤ N aCl + N aDS + N aOH ≤ 2This hypothesis limits the search to within the subspace of samples known to minimize the hydrogen production, based on post facto knowledge of the outcomes of all 1119 robotic experiments.It is described by the following ten constraints:</li>
</ol>
<p>Figure 9 :
9
Figure 9: Heatmap of the mean simple regret for different combinations of u and l</p>
<p>Figure 10 :
10
Figure 10: Evolution of the mean simple regret across all synthetic functions of different hypothesis mixtures for each combination of u and l</p>
<p>Figure 12 :
12
Figure 12: Visual comparison of the mean regrets of various parameter combinations, including u10 l1 and u5 l2.The green dashed line represents u5 l2 's mean regret, which allows for a direct comparison with other combinations</p>
<p>Figure 13 :
13
Figure 13: Evolution of the mean simple regret of different hypotheses with γ</p>
<p>:
prior_file = {..."input_parameters": {"x0": {"parameter_type" : "real","values" : [x0_min_range,→ x0_max_range],"prior" : "custom_gaussian","custom_gaussian_prior_means": [→ x0_mean],"custom_gaussian_prior_stds": [→ x0_std]},..."xn": {"parameter_type" : "real","values" : [xn_min_range,→ xn_max_range],"prior" : "custom_gaussian","custom_gaussian_prior_means": [→ xn_mean],"custom_gaussian_prior_stds": [→ xn_std]},},...}
AcknowledgementsThe authors acknowledge financial support from the Leverhulme Trust via the Leverhulme Research Centre for Functional Materials Design.AIC thanks the Royal Society for a Research Professorship (RSRP\S2\232003).
Adachi, Looping in the human: Collaborative and explainable bayesian optimization. 2023. 2023</p>
<p>Human-ai collaborative bayesian optimisation. Anjanapura Venkatesh, Teoria statistica delle classi e calcolo delle probabilità. Pubblicazioni del R. Istituto superiore di scienze economiche e commerciali di Firenze. Seeber. B Burger, Phillip M Maffettone, Vladimir V Gusev, Catherine M Aitchison, Yang Bai, Xiaobo Xiao Yan Wang, Ben M Li, Buyin Alston, Rob Li, Nicola Clowes, Brandon Rankin, Reiner Harris, Andrew I Sebastian Sprick, Cooper, Curran Associates, Inc2022. 2022. 1936. 2020. 202035A mobile robotic chemist</p>
<p>Trego: a trust-region framework for efficient global optimization. Diouane, Journal of Global Optimization. 862021. 2021. 2015Ebden, 2015] Mark Ebden. Gaussian processes: a quick introduction</p>
<p>Equipping data-driven experiment planning for self-driving laboratories with semantic memory: case studies of transfer learning in chemical reaction optimization. Eriksson, Advances in Neural Information Processing Systems. Hickman, PMLR2019. 2019. 2015. 2015. 202332Reaction Chemistry &amp; Engineering</p>
<p>Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge. Huang, 10th International Conference on Learning Representations, ICLR'22. Matthias Köppe, Maurice Queyranne, Christopher Thomas, Ryan , Cheng Li2022. 2022. 2022. April 2022. 2023. 2023. 2021. 07 2021. 1998. 1998. 2010. 2020. 20208Journal of Global Optimization. Sunil Gupta, Santu Rana, Vu Nguyen, Antonio Robles-Kelly, and Svetha Venkatesh. Incorporating expert prior knowledge into experimental design via posterior sampling</p>
<p>Luinardi, HyperMapper Team Luinardi. Prior Injection -HyperMapper Wiki. 2024. 2024</p>
<p>Toshiharu Morishita and Hiromasa Kaneko. Initial sample selection in bayesian optimization for combinatorial optimization of chemical compounds. Kaneko Morishita, ACS Omega. 822023. 2023</p>
<p>A decade survey of transfer learning. Niu, IEEE Transactions on Artificial Intelligence. 122020. 2010-2020. 2020</p>
<p>Incorporating expert prior in bayesian optimisation via space warping. Knowledge-Based Systems. Ramachandran, 2020. 2020195105663</p>
<p>Neuhäuser ; Rey, Denise Rey, Markus Neuhäuser, Wilcoxon-signed-rank test. 2011</p>
<p>Fast Bayesian optimization of needle-in-a-Haystack problems using zooming memory-based initialization (ZoMBI). Shahriari, Proceedings of the IEEE. npj Computational Mathematics. the IEEEBerlin Heidelberg, Berlin, HeidelbergInstitute of Electrical and Electronics Engineers Inc2011. 2016. January 2016. January 202310479Taking the human out of the loop: a review of bayesian optimization</p>
<p>Bayesian optimization with a prior for the optimum. Souza, Machine Learning and Knowledge Discovery in Databases. Research Track. Nuria Oliver, Fernando Pérez-Cruz, Stefan Kramer, Jesse Read, Jose A Lozano, ChamSpringer International Publishing2021. 2021</p>
<p>Sulfonecontaining covalent organic frameworks for photocatalytic hydrogen evolution from water. Theckel Joy, Machine Learning: ECML 2005. Can Wang, Kazunari Li, Domen, Berlin, Heidelberg; Berlin HeidelbergSpringer2019. 2019. 2005. 2005. 2018. 2018. 2019. 2019115Chem. Soc. Rev.</p>
<p>Learning search space partition for black-box optimization using monte carlo tree search. Wang , Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc2020. 202033</p>
<p>H-bonding effect of oxyanions enhanced photocatalytic degradation of sulfonamides by g-c3n4 in aqueous solution. Zhang, Journal of Hazardous Materials. 366202019. 2019. April 2022Advanced Materials</p>            </div>
        </div>

    </div>
</body>
</html>
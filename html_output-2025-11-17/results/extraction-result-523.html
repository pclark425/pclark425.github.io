<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-523 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-523</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-523</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-427805af4f4f659ac00198a0fc136e9660e472bf</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/427805af4f4f659ac00198a0fc136e9660e472bf" target="_blank">Visual Distant Supervision for Scene Graph Generation</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> This work proposes visual distant supervision, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data, and shows that the distantly supervised model outperforms strong weakly supervised and semi-supervised baselines.</p>
                <p><strong>Paper Abstract:</strong> Scene graph generation aims to identify objects and their relations in images, providing structured image representations that can facilitate numerous applications in computer vision. However, scene graph models usually require supervised learning on large quantities of labeled data with intensive human annotation. In this work, we propose visual distant supervision, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data. The intuition is that by aligning commonsense knowledge bases and images, we can automatically create large-scale labeled data to provide distant supervision for visual relation learning. To alleviate the noise in distantly labeled data, we further propose a framework that iteratively estimates the probabilistic relation labels and eliminates the noisy ones. Comprehensive experimental results show that our distantly supervised model outperforms strong weakly supervised and semi-supervised baselines. By further incorporating human-labeled data in a semi-supervised fashion, our model outperforms state-of-the-art fully supervised models by a large margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for predicate classification in Visual Genome evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/VisualDS.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e523.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e523.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Distant Supervision (VDS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Distant Supervision for Scene Graph Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that creates large-scale noisy relation labels for images by aligning a commonsense knowledge base (extracted from image captions) to visual object pairs and then training scene-graph models with an iterative denoising EM framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Motif (backbone for experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A scene-graph generation model (Neural Motif) with ResNeXt-101-FPN backbone used as the backbone scene-graph predictor; used here as the internal prediction model in an EM denoising loop for distantly supervised relation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Scene graph generation / predicate classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an image and object bounding boxes/categories, predict relation predicates between object pairs (predicate classification), and optionally object detection/classification and relation extraction (scene graph classification/detection).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational extraction / scene graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial (relation predicates include spatial relations such as 'on', 'near', 'standing on')</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>automatically constructed commonsense knowledge base from image captions (Conceptual Captions) aligned to image object categories; optionally external cross-modal model (CLIP) and human-labeled data for semi-supervised setting</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pretraining on distantly labeled data (distant supervision) followed by EM-style iterative denoising combining model predictions and optional external semantic signals; fine-tuning on human-labeled data in semi-supervised case</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit relation triples (c_i, r, c_j) in a commonsense KB aligned to visual object pairs; probabilistic relation distributions r^t over relation vocabulary maintained per object pair (soft labels), and scene graphs as structured outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro-recall@K (R@K), macro-recall@K (mR@K), human precision@K</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Distantly supervised + denoising + external signal (Motif + DNS + EXT) achieved predicate classification R@50 = 53.40, R@100 = 56.54, mR@50 = 37.68, mR@100 = 41.98 (Table 1). Semi-supervised pretrain+finetune variants reach up to R@50 ≈ 76.28 (Motif + DNS (Ours), semi-supervised) for combined tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides large-scale coverage of object-relations (addresses long-tail): distant labels can increase labeled instances by 1–3 orders of magnitude; iterative EM denoising plus external semantic signals substantially improves recall and macro-recall, indicating improved handling of less frequent relations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Raw distant labels are noisy because KB-only candidates do not account for image-specific relations (e.g., wrong relation between multiple same-category objects); incompleteness and naming mismatch of KB limits coverage; some spatial relations and fine-grained relations still require visual evidence and are mislabelled without denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms weakly supervised and semi-supervised baselines: e.g., Weak Supervision baseline R@50 ≈ 44.96/R@100 ≈ 47.19; Motif + DNS + EXT (distant) R@50 = 53.40; semi-supervised Motif + DNS (Ours) R@50 up to 76.28 vs Motif (fully supervised baseline) 67.93 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Adding external semantic signals (CLIP) to initialize/guide E-step improves results (Raw Label R@50 30.61 -> Raw Label + EXT R@50 38.21); iterative denoising improves metrics across iterations (DNS iter2 > iter1); semi-supervised integration yields further gains (pretrain then fine-tune beats direct finetuning alone).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-relational knowledge encoded in text (commonsense KB from captions) can supervise visual relation learning at scale, but is noisy; combining KB soft-labels with internal model predictions and a cross-modal semantic scoring function (CLIP) via EM denoising yields robust probabilistic relation representations per object pair that markedly improve scene-graph extraction and help address long-tail relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Distant Supervision for Scene Graph Generation', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e523.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e523.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Commonsense KB (captions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Commonsense Knowledge Base automatically constructed from Conceptual Captions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatically built KB of object-relation-object triples extracted using rule-based parsing of large-scale image captions (Conceptual Captions) used to provide relation candidates for visual object pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>rule-based textual parser -> extracted KB</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A knowledge base constructed by applying a rule-based textual parser to 3.3M Conceptual Captions, yielding ~1.88M distinct relational triples across ~18.6k object categories and ~63.2k relation categories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Distant supervision source for scene graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide candidate relation labels between object category pairs in images by matching object categories to KB entities and retrieving relation triples (c_i, r, c_j).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational knowledge provision (no direct embodiment execution)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (categorical relations and commonsense affordances), includes spatial predicates among relation vocabulary</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>textual pre-training data (Conceptual Captions) parsed into triple KB</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>lookup/align: for each visual object pair (subject category, object category) retrieve candidate relations from the KB; initialize multi-hot label vector d and optionally convert to probabilistic labels via external signals or model predictions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>symbolic relation triples (subject category, relation, object category) and multi-hot / probabilistic relation label vectors per object pair</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>coverage of Visual Genome relations; downstream scene graph recall/precision when used for distant supervision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>KB contains 1,876,659 distinct triples (avg 1.94 relations per object pair); after alignment and overlap filter, distant labels cover 70.3% of Visual Genome relation labels; using this KB for distant supervision with denoising yields predicate classification R@50 up to 53.40 (Motif + DNS + EXT).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides exhaustive relation candidates across many object pairs, increasing labeled examples dramatically and improving long-tail relation learning when denoised.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Incompleteness and naming mismatches limit coverage; KB-only labels are weak for image-specific relations and introduce noise when relations depend on visual context (e.g., multiple same-category objects in one image).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to human-labeled Visual Genome training: KB-based distant labels yield far more instances (1–3 orders of magnitude more) but are noisier; denoising is required to surpass weak/limited-label baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ideal KB (constructed from Visual Genome annotations) improves results substantially (Table 3): Motif + DNS + EXT with ideal KB shows large absolute gains in mR@50 (≈ +13.2), demonstrating the KB's coverage is a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-derived commonsense KBs encode rich object-relational knowledge useful for supervising visual relation tasks without human labels, but visual grounding and denoising are necessary because many KB relations are not image-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Distant Supervision for Scene Graph Generation', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e523.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e523.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (external semantic signal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP: Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large cross-modal model pre-trained on image-caption pairs; used here to compute a relatedness score between a textual relation triple and the masked visual region of an object pair to initialize or guide probabilistic relation labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastively pre-trained image-text encoder pair that maps images and text to a joint embedding space; used to compute cosine similarities as semantic relatedness between a textual triple and the visual crop/masked image region for an object pair.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>External semantic scoring for denoising distant supervision</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an object pair, produce an unnormalized relatedness score alpha_i between the visual input (masked image region covering the pair) and a textual snippet formed by concatenating subject-relation-object; normalized into a probabilistic distribution e over candidate relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>cross-modal semantic matching to support object-relational labeling</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational and contextual visual-textual semantics; can help disambiguate spatial/functional predicates by matching visual appearance to textual relation descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large-scale image-caption corpora (no human-labeled relation supervision in this usage)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot semantic scoring: compute cosine similarity between image crop embedding and textual triple embedding; softmax-normalize over candidate relations</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>continuous joint embeddings for images and text; outputs scalar relatedness scores used to form probabilistic label distributions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improvement in downstream recall/macro-recall when used to initialize/guide EM denoising</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using CLIP as external signal (EXT) improves Raw Label R@50 from 30.61 -> 38.21 and produces further gains when combined with denoising (Motif + DNS + EXT R@50 = 53.40), indicating substantial denoising benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Helps disambiguate which KB-proposed relations are visually plausible for a given object-pair crop, accelerates denoising convergence and improves both micro and macro recall.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>CLIP similarity cannot fully resolve image-specific fine-grained relations and may still score plausible-but-incorrect KB relations highly; relies on visual signal (not applicable when model operates purely from text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Initialization without EXT (r^1 = d) yields lower downstream performance; adding CLIP gives consistent gains across metrics compared to no external signal.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>EXT alone raises Raw Label performance substantially; combining EXT with EM denoising yields larger gains than either alone; iterative EM with EXT converges faster.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cross-modal semantic models like CLIP can serve as an effective external denoising signal by mapping textual relation candidates to visual evidence, converting symbolic KB candidates into probabilistic, image-grounded relation distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Distant Supervision for Scene Graph Generation', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e523.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e523.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Overlap Heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bounding-box Overlap Spatial Filter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple spatial heuristic that assigns distant relation labels only to object pairs whose bounding boxes overlap, used to filter out obviously noisy KB-derived labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (heuristic filter)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deterministic spatial constraint: when aligning KB relation candidates to image object pairs, only retain relation labels for pairs whose bounding boxes have non-zero overlap to reduce combinatorial noise from multiple same-category objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Noise reduction in distant supervision alignment</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Filter distant labels by requiring subject and object bounding boxes to overlap before assigning KB relation candidates to that object pair.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial filtering / preprocessing for object-relational labeling</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (bounding-box overlap) used to gate object-relational assignments</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>geometric information from object bounding boxes (vision)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>deterministic gating (overlap check) applied during KB-to-image alignment</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>binary overlap predicate (overlap / not overlap) used to filter multi-hot distant label vectors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>coverage reduction of noisy labels and downstream recall/precision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>After alignment and overlap filtering, distant labels cover 70.3% of Visual Genome relation labels; overlap heuristic removes many noisy assignments when multiple same-class objects exist.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively removes many obviously incorrect distant labels introduced by naively pairing every KB relation with every same-class object pair in an image (e.g., avoids assigning 'riding' to non-interacting far-apart person/horse pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Conservative: may remove valid non-overlapping relations (e.g., 'near', 'watching across a scene'); cannot resolve finer-grained relation correctness that depends on visual content rather than overlap alone.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Better than naive all-pair alignment (which produces much more noise); used in comparisons and ablations as a standard filtering baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Applying overlap filtering increases precision of distant label alignment and is a prerequisite to obtain the reported coverage (70.3%); no explicit numeric ablation beyond coverage reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple spatial constraints can substantially reduce combinatorial noise when mapping text-derived relation candidates onto visual object instances, but are not sufficient alone—visual semantic scoring and denoising are still needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Distant Supervision for Scene Graph Generation', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e523.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e523.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EM Denoising Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative EM-style Probabilistic Denoising for Distant Labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that treats ground-truth relation labels of distantly labeled data as latent variables and iteratively refines probabilistic relation labels by alternating between estimating soft labels (E-step) and updating the scene-graph model parameters (M-step).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EM denoising with Neural Motif as f(·;θ)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>EM-style optimization: E-step forms probabilistic relation distributions per object-pair by convex combination of model predictions and optional external semantic scores (CLIP); M-step optimizes an entropy-aware log-likelihood (or cross-entropy in semi-supervised variant) to update model parameters; noisy object pairs can be discarded based on NA logits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Denoising distant supervision for scene graph learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iteratively estimate per-instance probabilistic relation labels from KB candidates and model predictions, eliminate noisy pairs, and train the scene graph model to maximize a noise-aware likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>training-time denoising for object-relational learning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (probabilistic distributions over relation predicates); uses visual-textual semantics and internal model beliefs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>combination of (1) KB candidates (textual), (2) internal scene-graph model predictions (visual + learned parameters), (3) optional external cross-modal model (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>iterative EM: initialization (d or e), E-step: r^t = ω f(s,o;θ^{t-1}) + (1-ω) e (if EXT), discard noisy pairs by NA logits; M-step: maximize noise-aware likelihood (entropy-based) or cross-entropy for discretized labels</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>probabilistic relation distributions (soft labels) per object pair, later discretized for fine-tuning; noise-aware likelihood objective that weights label likelihoods by r^t</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>R@K, mR@K, precision@K (human evaluated), convergence speed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Iterative denoising produces consistent improvements: e.g., Motif + DNS + EXT R@50 = 53.40 vs Motif alone R@50 = 50.23 (Table 1); iterative second iteration further improves metrics (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively reduces noise and increases both micro and macro recall, helps long-tail relations by leveraging global coherence of internal statistics, and benefits from external signals and semi-supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Relies on reasonable initialization and KB coverage; if none of KB candidates are correct for a pair, model may need to discard the pair (they discard top-k% NA logits), potentially removing hard but valid instances; the method treats each pair largely in isolation (authors note exploring holistic scene-coherence is future work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms cleanness-loss and other instance-level denoising baselines; combining EXT and DNS outperforms either alone. Specific comparisons: Motif R@50 50.23 -> Motif + DNS (iter2) R@50 51.54 (Table 2) and with EXT further to 53.40.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing EXT (ω=1) lowers performance; single-iteration DNS yields less improvement than multi-iteration; semi-supervised M2 finetuning increases gains compared to distantly supervised-only M-step.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Probabilistic per-instance relation representations combined with EM optimization allow a scene-graph model to internalize and correct noisy object-relational labels coming from text KBs; integrating cross-modal semantic signals and human labels yields better and faster denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Distant Supervision for Scene Graph Generation', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scene graph prediction with limited labels <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Neural motifs: Scene graph parsing with global context <em>(Rating: 2)</em></li>
                <li>Distant supervision for relation extraction without labeled data <em>(Rating: 2)</em></li>
                <li>Weakly-supervised learning of visual relations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-523",
    "paper_id": "paper-427805af4f4f659ac00198a0fc136e9660e472bf",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Visual Distant Supervision (VDS)",
            "name_full": "Visual Distant Supervision for Scene Graph Generation",
            "brief_description": "A paradigm that creates large-scale noisy relation labels for images by aligning a commonsense knowledge base (extracted from image captions) to visual object pairs and then training scene-graph models with an iterative denoising EM framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Neural Motif (backbone for experiments)",
            "model_size": null,
            "model_description": "A scene-graph generation model (Neural Motif) with ResNeXt-101-FPN backbone used as the backbone scene-graph predictor; used here as the internal prediction model in an EM denoising loop for distantly supervised relation learning.",
            "task_name": "Scene graph generation / predicate classification",
            "task_description": "Given an image and object bounding boxes/categories, predict relation predicates between object pairs (predicate classification), and optionally object detection/classification and relation extraction (scene graph classification/detection).",
            "task_type": "object-relational extraction / scene graph generation",
            "knowledge_type": "object-relational + spatial (relation predicates include spatial relations such as 'on', 'near', 'standing on')",
            "knowledge_source": "automatically constructed commonsense knowledge base from image captions (Conceptual Captions) aligned to image object categories; optionally external cross-modal model (CLIP) and human-labeled data for semi-supervised setting",
            "has_direct_sensory_input": true,
            "elicitation_method": "pretraining on distantly labeled data (distant supervision) followed by EM-style iterative denoising combining model predictions and optional external semantic signals; fine-tuning on human-labeled data in semi-supervised case",
            "knowledge_representation": "explicit relation triples (c_i, r, c_j) in a commonsense KB aligned to visual object pairs; probabilistic relation distributions r^t over relation vocabulary maintained per object pair (soft labels), and scene graphs as structured outputs",
            "performance_metric": "micro-recall@K (R@K), macro-recall@K (mR@K), human precision@K",
            "performance_result": "Distantly supervised + denoising + external signal (Motif + DNS + EXT) achieved predicate classification R@50 = 53.40, R@100 = 56.54, mR@50 = 37.68, mR@100 = 41.98 (Table 1). Semi-supervised pretrain+finetune variants reach up to R@50 ≈ 76.28 (Motif + DNS (Ours), semi-supervised) for combined tasks.",
            "success_patterns": "Provides large-scale coverage of object-relations (addresses long-tail): distant labels can increase labeled instances by 1–3 orders of magnitude; iterative EM denoising plus external semantic signals substantially improves recall and macro-recall, indicating improved handling of less frequent relations.",
            "failure_patterns": "Raw distant labels are noisy because KB-only candidates do not account for image-specific relations (e.g., wrong relation between multiple same-category objects); incompleteness and naming mismatch of KB limits coverage; some spatial relations and fine-grained relations still require visual evidence and are mislabelled without denoising.",
            "baseline_comparison": "Outperforms weakly supervised and semi-supervised baselines: e.g., Weak Supervision baseline R@50 ≈ 44.96/R@100 ≈ 47.19; Motif + DNS + EXT (distant) R@50 = 53.40; semi-supervised Motif + DNS (Ours) R@50 up to 76.28 vs Motif (fully supervised baseline) 67.93 (Table 1).",
            "ablation_results": "Adding external semantic signals (CLIP) to initialize/guide E-step improves results (Raw Label R@50 30.61 -&gt; Raw Label + EXT R@50 38.21); iterative denoising improves metrics across iterations (DNS iter2 &gt; iter1); semi-supervised integration yields further gains (pretrain then fine-tune beats direct finetuning alone).",
            "key_findings": "Object-relational knowledge encoded in text (commonsense KB from captions) can supervise visual relation learning at scale, but is noisy; combining KB soft-labels with internal model predictions and a cross-modal semantic scoring function (CLIP) via EM denoising yields robust probabilistic relation representations per object pair that markedly improve scene-graph extraction and help address long-tail relations.",
            "uuid": "e523.0",
            "source_info": {
                "paper_title": "Visual Distant Supervision for Scene Graph Generation",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Commonsense KB (captions)",
            "name_full": "Commonsense Knowledge Base automatically constructed from Conceptual Captions",
            "brief_description": "An automatically built KB of object-relation-object triples extracted using rule-based parsing of large-scale image captions (Conceptual Captions) used to provide relation candidates for visual object pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "rule-based textual parser -&gt; extracted KB",
            "model_size": null,
            "model_description": "A knowledge base constructed by applying a rule-based textual parser to 3.3M Conceptual Captions, yielding ~1.88M distinct relational triples across ~18.6k object categories and ~63.2k relation categories.",
            "task_name": "Distant supervision source for scene graph generation",
            "task_description": "Provide candidate relation labels between object category pairs in images by matching object categories to KB entities and retrieving relation triples (c_i, r, c_j).",
            "task_type": "object-relational knowledge provision (no direct embodiment execution)",
            "knowledge_type": "object-relational (categorical relations and commonsense affordances), includes spatial predicates among relation vocabulary",
            "knowledge_source": "textual pre-training data (Conceptual Captions) parsed into triple KB",
            "has_direct_sensory_input": false,
            "elicitation_method": "lookup/align: for each visual object pair (subject category, object category) retrieve candidate relations from the KB; initialize multi-hot label vector d and optionally convert to probabilistic labels via external signals or model predictions",
            "knowledge_representation": "symbolic relation triples (subject category, relation, object category) and multi-hot / probabilistic relation label vectors per object pair",
            "performance_metric": "coverage of Visual Genome relations; downstream scene graph recall/precision when used for distant supervision",
            "performance_result": "KB contains 1,876,659 distinct triples (avg 1.94 relations per object pair); after alignment and overlap filter, distant labels cover 70.3% of Visual Genome relation labels; using this KB for distant supervision with denoising yields predicate classification R@50 up to 53.40 (Motif + DNS + EXT).",
            "success_patterns": "Provides exhaustive relation candidates across many object pairs, increasing labeled examples dramatically and improving long-tail relation learning when denoised.",
            "failure_patterns": "Incompleteness and naming mismatches limit coverage; KB-only labels are weak for image-specific relations and introduce noise when relations depend on visual context (e.g., multiple same-category objects in one image).",
            "baseline_comparison": "Compared to human-labeled Visual Genome training: KB-based distant labels yield far more instances (1–3 orders of magnitude more) but are noisier; denoising is required to surpass weak/limited-label baselines.",
            "ablation_results": "Ideal KB (constructed from Visual Genome annotations) improves results substantially (Table 3): Motif + DNS + EXT with ideal KB shows large absolute gains in mR@50 (≈ +13.2), demonstrating the KB's coverage is a limiting factor.",
            "key_findings": "Text-derived commonsense KBs encode rich object-relational knowledge useful for supervising visual relation tasks without human labels, but visual grounding and denoising are necessary because many KB relations are not image-specific.",
            "uuid": "e523.1",
            "source_info": {
                "paper_title": "Visual Distant Supervision for Scene Graph Generation",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "CLIP (external semantic signal)",
            "name_full": "CLIP: Learning transferable visual models from natural language supervision",
            "brief_description": "A large cross-modal model pre-trained on image-caption pairs; used here to compute a relatedness score between a textual relation triple and the masked visual region of an object pair to initialize or guide probabilistic relation labels.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_size": null,
            "model_description": "Contrastively pre-trained image-text encoder pair that maps images and text to a joint embedding space; used to compute cosine similarities as semantic relatedness between a textual triple and the visual crop/masked image region for an object pair.",
            "task_name": "External semantic scoring for denoising distant supervision",
            "task_description": "Given an object pair, produce an unnormalized relatedness score alpha_i between the visual input (masked image region covering the pair) and a textual snippet formed by concatenating subject-relation-object; normalized into a probabilistic distribution e over candidate relations.",
            "task_type": "cross-modal semantic matching to support object-relational labeling",
            "knowledge_type": "object-relational and contextual visual-textual semantics; can help disambiguate spatial/functional predicates by matching visual appearance to textual relation descriptions",
            "knowledge_source": "pre-training on large-scale image-caption corpora (no human-labeled relation supervision in this usage)",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot semantic scoring: compute cosine similarity between image crop embedding and textual triple embedding; softmax-normalize over candidate relations",
            "knowledge_representation": "continuous joint embeddings for images and text; outputs scalar relatedness scores used to form probabilistic label distributions",
            "performance_metric": "improvement in downstream recall/macro-recall when used to initialize/guide EM denoising",
            "performance_result": "Using CLIP as external signal (EXT) improves Raw Label R@50 from 30.61 -&gt; 38.21 and produces further gains when combined with denoising (Motif + DNS + EXT R@50 = 53.40), indicating substantial denoising benefit.",
            "success_patterns": "Helps disambiguate which KB-proposed relations are visually plausible for a given object-pair crop, accelerates denoising convergence and improves both micro and macro recall.",
            "failure_patterns": "CLIP similarity cannot fully resolve image-specific fine-grained relations and may still score plausible-but-incorrect KB relations highly; relies on visual signal (not applicable when model operates purely from text).",
            "baseline_comparison": "Initialization without EXT (r^1 = d) yields lower downstream performance; adding CLIP gives consistent gains across metrics compared to no external signal.",
            "ablation_results": "EXT alone raises Raw Label performance substantially; combining EXT with EM denoising yields larger gains than either alone; iterative EM with EXT converges faster.",
            "key_findings": "Cross-modal semantic models like CLIP can serve as an effective external denoising signal by mapping textual relation candidates to visual evidence, converting symbolic KB candidates into probabilistic, image-grounded relation distributions.",
            "uuid": "e523.2",
            "source_info": {
                "paper_title": "Visual Distant Supervision for Scene Graph Generation",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Overlap Heuristic",
            "name_full": "Bounding-box Overlap Spatial Filter",
            "brief_description": "A simple spatial heuristic that assigns distant relation labels only to object pairs whose bounding boxes overlap, used to filter out obviously noisy KB-derived labels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "N/A (heuristic filter)",
            "model_size": null,
            "model_description": "A deterministic spatial constraint: when aligning KB relation candidates to image object pairs, only retain relation labels for pairs whose bounding boxes have non-zero overlap to reduce combinatorial noise from multiple same-category objects.",
            "task_name": "Noise reduction in distant supervision alignment",
            "task_description": "Filter distant labels by requiring subject and object bounding boxes to overlap before assigning KB relation candidates to that object pair.",
            "task_type": "spatial filtering / preprocessing for object-relational labeling",
            "knowledge_type": "spatial (bounding-box overlap) used to gate object-relational assignments",
            "knowledge_source": "geometric information from object bounding boxes (vision)",
            "has_direct_sensory_input": true,
            "elicitation_method": "deterministic gating (overlap check) applied during KB-to-image alignment",
            "knowledge_representation": "binary overlap predicate (overlap / not overlap) used to filter multi-hot distant label vectors",
            "performance_metric": "coverage reduction of noisy labels and downstream recall/precision",
            "performance_result": "After alignment and overlap filtering, distant labels cover 70.3% of Visual Genome relation labels; overlap heuristic removes many noisy assignments when multiple same-class objects exist.",
            "success_patterns": "Effectively removes many obviously incorrect distant labels introduced by naively pairing every KB relation with every same-class object pair in an image (e.g., avoids assigning 'riding' to non-interacting far-apart person/horse pairs).",
            "failure_patterns": "Conservative: may remove valid non-overlapping relations (e.g., 'near', 'watching across a scene'); cannot resolve finer-grained relation correctness that depends on visual content rather than overlap alone.",
            "baseline_comparison": "Better than naive all-pair alignment (which produces much more noise); used in comparisons and ablations as a standard filtering baseline.",
            "ablation_results": "Applying overlap filtering increases precision of distant label alignment and is a prerequisite to obtain the reported coverage (70.3%); no explicit numeric ablation beyond coverage reported.",
            "key_findings": "Simple spatial constraints can substantially reduce combinatorial noise when mapping text-derived relation candidates onto visual object instances, but are not sufficient alone—visual semantic scoring and denoising are still needed.",
            "uuid": "e523.3",
            "source_info": {
                "paper_title": "Visual Distant Supervision for Scene Graph Generation",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "EM Denoising Framework",
            "name_full": "Iterative EM-style Probabilistic Denoising for Distant Labels",
            "brief_description": "A framework that treats ground-truth relation labels of distantly labeled data as latent variables and iteratively refines probabilistic relation labels by alternating between estimating soft labels (E-step) and updating the scene-graph model parameters (M-step).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "EM denoising with Neural Motif as f(·;θ)",
            "model_size": null,
            "model_description": "EM-style optimization: E-step forms probabilistic relation distributions per object-pair by convex combination of model predictions and optional external semantic scores (CLIP); M-step optimizes an entropy-aware log-likelihood (or cross-entropy in semi-supervised variant) to update model parameters; noisy object pairs can be discarded based on NA logits.",
            "task_name": "Denoising distant supervision for scene graph learning",
            "task_description": "Iteratively estimate per-instance probabilistic relation labels from KB candidates and model predictions, eliminate noisy pairs, and train the scene graph model to maximize a noise-aware likelihood.",
            "task_type": "training-time denoising for object-relational learning",
            "knowledge_type": "object-relational (probabilistic distributions over relation predicates); uses visual-textual semantics and internal model beliefs",
            "knowledge_source": "combination of (1) KB candidates (textual), (2) internal scene-graph model predictions (visual + learned parameters), (3) optional external cross-modal model (CLIP)",
            "has_direct_sensory_input": true,
            "elicitation_method": "iterative EM: initialization (d or e), E-step: r^t = ω f(s,o;θ^{t-1}) + (1-ω) e (if EXT), discard noisy pairs by NA logits; M-step: maximize noise-aware likelihood (entropy-based) or cross-entropy for discretized labels",
            "knowledge_representation": "probabilistic relation distributions (soft labels) per object pair, later discretized for fine-tuning; noise-aware likelihood objective that weights label likelihoods by r^t",
            "performance_metric": "R@K, mR@K, precision@K (human evaluated), convergence speed",
            "performance_result": "Iterative denoising produces consistent improvements: e.g., Motif + DNS + EXT R@50 = 53.40 vs Motif alone R@50 = 50.23 (Table 1); iterative second iteration further improves metrics (Table 2).",
            "success_patterns": "Effectively reduces noise and increases both micro and macro recall, helps long-tail relations by leveraging global coherence of internal statistics, and benefits from external signals and semi-supervised fine-tuning.",
            "failure_patterns": "Relies on reasonable initialization and KB coverage; if none of KB candidates are correct for a pair, model may need to discard the pair (they discard top-k% NA logits), potentially removing hard but valid instances; the method treats each pair largely in isolation (authors note exploring holistic scene-coherence is future work).",
            "baseline_comparison": "Outperforms cleanness-loss and other instance-level denoising baselines; combining EXT and DNS outperforms either alone. Specific comparisons: Motif R@50 50.23 -&gt; Motif + DNS (iter2) R@50 51.54 (Table 2) and with EXT further to 53.40.",
            "ablation_results": "Removing EXT (ω=1) lowers performance; single-iteration DNS yields less improvement than multi-iteration; semi-supervised M2 finetuning increases gains compared to distantly supervised-only M-step.",
            "key_findings": "Probabilistic per-instance relation representations combined with EM optimization allow a scene-graph model to internalize and correct noisy object-relational labels coming from text KBs; integrating cross-modal semantic signals and human labels yields better and faster denoising.",
            "uuid": "e523.4",
            "source_info": {
                "paper_title": "Visual Distant Supervision for Scene Graph Generation",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scene graph prediction with limited labels",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Neural motifs: Scene graph parsing with global context",
            "rating": 2
        },
        {
            "paper_title": "Distant supervision for relation extraction without labeled data",
            "rating": 2
        },
        {
            "paper_title": "Weakly-supervised learning of visual relations",
            "rating": 1
        }
    ],
    "cost": 0.01733025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Visual Distant Supervision for Scene Graph Generation</h1>
<p>Yuan Yao ${ }^{1 <em>}$, Ao Zhang ${ }^{1 </em>}$, Xu Han ${ }^{1}$, Mengdi Li ${ }^{2}$,<br>Cornelius Weber ${ }^{2}$, Zhiyuan Liu ${ }^{1 \dagger}$, Stefan Wermter ${ }^{2}$, Maosong Sun ${ }^{1}$<br>${ }^{1}$ Department of Computer Science and Technology<br>Institute for Artificial Intelligence, Tsinghua University, Beijing, China<br>Beijing National Research Center for Information Science and Technology, China<br>${ }^{2}$ Knowledge Technology Group, Department of Informatics, University of Hamburg, Hamburg, Germany<br>yuan-yao18@mails.tsinghua.edu.cn, zhanga6@outlook.com</p>
<h4>Abstract</h4>
<p>Scene graph generation aims to identify objects and their relations in images, providing structured image representations that can facilitate numerous applications in computer vision. However, scene graph models usually require supervised learning on large quantities of labeled data with intensive human annotation. In this work, we propose visual distant supervision, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data. The intuition is that by aligning commonsense knowledge bases and images, we can automatically create large-scale labeled data to provide distant supervision for visual relation learning. To alleviate the noise in distantly labeled data, we further propose a framework that iteratively estimates the probabilistic relation labels and eliminates the noisy ones. Comprehensive experimental results show that our distantly supervised model outperforms strong weakly supervised and semi-supervised baselines. By further incorporating human-labeled data in a semi-supervised fashion, our model outperforms state-of-the-art fully supervised models by a large margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for predicate classification in Visual Genome evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/VisualDS.</p>
<h2>1. Introduction</h2>
<p>Scene graph generation aims to identify objects and their relations in real-world images. For example, the scene graph shown in Figure 1 depicts the image with several relational triples, such as (person, riding, horse) and (horse, standing on, beach). Such structured representations</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. An example from Visual Genome [23] based on the refined relation schemes from Chen et al. [5], where human annotation from Visual Genome, weak supervision information from the corresponding caption, and raw relation labels from distant supervision are shown respectively. Correct relation labels are highlighted in bold. By aligning commonsense knowledge bases and images, visual distant supervision can create large-scale labeled data without any human efforts to facilitate visual relation learning. Best viewed in color.
provide deep understanding of the semantic content of images, and have facilitated state-of-the-art models in numerous applications in computer vision, such as visual question answering [18, 44], image retrieval [22, 40], image captioning $[53,13]$ and image generation [21].</p>
<p>Tremendous efforts have been devoted to generating scene graphs from images [51, 27, 52, 31, 60]. However, scene graph models usually require supervised learning on large quantities of human-labeled data. Manually constructing large-scale datasets for visual relation learning is extremely labor-intensive and time-consuming [31, 23]. Moreover, even with the human-labeled data, scene graph models usually suffer from the long-tail relation distribu-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Number of labeled instances of top 3,000 relationships from Visual Genome annotation and visual distant supervision.</p>
<p>tion in real-world scenarios. Figure 2 shows the statistics on Visual Genome [23], where over 98% of the top 3,000 relation categories do not have sufficient labeled instances and are thus ignored by most scene graph models.</p>
<p>To address the problems, a promising direction is to utilize large-scale unlabeled data with minimal human efforts via semi-supervised or weakly supervised learning. Chen <em>et al</em>. [5] propose to first learn a simple relation predictor using several human-labeled seed instances for each relation, and then assign soft labels to unlabeled data to train scene graph models. However, semi-supervised models still require human annotation that scales linearly with the number of relations. Moreover, learning from limited seed instances is vulnerable to high variance and subjective annotation bias. Some works have also explored learning from weakly supervised relation labels, which are obtained by parsing the captions of the corresponding images [61, 34]. However, due to reporting bias [12], captions only summarize a few salient relations in images, and ignore less salient and background relations, e.g., (<em>rock</em>, <em>covering</em>, <em>beach</em>) in Figure 1. The resultant models will thus be biased towards a few salient relations, which cannot well serve scene graph generation that aims to exhaustively extract all reasonable relational triples in the scene.</p>
<p>In this work, we propose <em>visual distant supervision</em>, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data. The intuition is that commonsense knowledge bases encode relation candidates between objects, which are likely to be expressed in images. For example, as shown in Figure 1, multiple relation candidates, e.g., <em>riding</em>, <em>sitting on</em> and <em>watching</em>, can be retrieved from commonsense knowledge bases for the object pair <em>person</em> and <em>horse</em>, where <em>riding</em> and <em>sitting on</em> are actually expressed in the given image. By aligning commonsense knowledge bases and images, we can create large-scale labeled data to provide distant supervision for visual relation learning without any human efforts. Since the distant supervision is provided by knowledge bases, the relations can be exhaustively labeled between all object pairs. We note that many reasonable relation labels from distant supervision are missing in Visual Genome even after intensive human annotations, e.g., (<em>wave</em>, <em>covering</em>, <em>beach</em>) in Figure 1.</p>
<p>Moreover, distant supervision can also alleviate the long-tail problem. As shown in Figure 2, using the same number of images, distant supervision can produce 1-3 orders of magnitude more labeled relation instances than its human-labeled counterpart. Note that the number of distantly labeled relation instances can be arbitrarily large, given the nearly unlimited image data on the Web.</p>
<p>Distant supervision is convenient in training scene graph models without human-labeled data. When human-annotated data is available, distantly labeled data can also be incorporated in a semi-supervised fashion to surpass fully supervised models. We show that after pre-training on distantly labeled data, simple fine-tuning on human-labeled data can lead to significant improvements over strong fully supervised models.</p>
<p>Despite its potential, we note distant supervision may introduce noise in relation labels, e.g., (<em>person</em>, <em>watching</em>, <em>horse</em>) in Figure 1. The reason is that distant supervision only provides relation candidates based on object categories, whereas the actual relations between two objects in an image usually depend on the image content. In principle, the noise in distantly labeled data can be alleviated by maximizing the coherence between distant labels and visual patterns of object pairs. Previous works have shown that without specially designed denoising methods, neural models are capable of detecting noisy labels to some extent, and learning meaningful signals from noisy data [20, 9]. In this work, to better alleviate the noise in distantly labeled data, we further propose a framework that iteratively estimates the probabilistic relation labels and eliminates noisy ones. The framework can be realized by optimizing the coherence of internal statistics of distantly labeled data, and can also be seamlessly integrated with external semantic signals (e.g., image-caption retrieval models), or human-labeled data to achieve better denoising results.</p>
<p>Comprehensive experimental results show that, without using any human-labeled data, our distantly supervised model outperforms strong weakly supervised and semi-supervised baseline methods. By further incorporating human-labeled data in a semi-supervised fashion, our model outperforms state-of-the-art fully supervised models by a large margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for predicate classification task in Visual Genome evaluation). Based on the experiments, we discuss multiple promising directions for future research.</p>
<p>Our contributions are threefold: (1) We propose visual distant supervision, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data, and also improve fully supervised models. (2) We propose a denoising framework to alleviate the noise in distantly labeled data. (3) We conduct comprehensive experiments which demonstrate the effectiveness of visual distant supervision and the denoising framework.</p>
<h2>2. Related Work</h2>
<p>Visual Relation Detection. Identifying visual relations between objects is critical for image understanding, which have received broad attention from the community [31, $60,14,15,7,37,3,55]$. Johnson et al. [22] further formulate scene graphs that encode all objects and their relations in images into structured graph representations. Tremendous efforts have been devoted to generating scene graphs, including refining contextualized graph features [6, 51, 27, 57], developing computationally efficient scene graph models [26, 52, 61] and designing effective loss functions [52, 62]. However, scene graph models usually require supervised learning on large amounts of humanlabeled data [31, 23].
Weakly Supervised Scene Graph Generation. To alleviate the heavy reliance on human-labeled data, recent works on scene graph generation have explored semi-supervised and weakly supervised learning methods. Chen et al. [5] propose to bootstrap scene graph models from several human-labeled seed instances for each relation, which still requires manual labor and is vulnerable to high variance. Other works attempt to obtain weakly supervised relation labels from the corresponding image captions. Peyre et al. [34] propose to ground the labels to object pairs by imposing global grounding constraints [10]. To improve the computational efficiency, Zhang et al. [61] design a network branch to select a pair of object proposals for each relation label. Baldassarre et al. [1] propose to first detect the relation via graph networks, and then recover the subject and object of the predicted relation. Zareian et al. [56] reformulate scene graphs as bipartite graphs of objects and relations, and align the predicted graphs to their weakly supervised labels. However, since the weakly supervised relation labels are parsed from the corresponding captions, the resultant models will be biased towards the most salient relations, ignoring many less salient and background relations.
Textual Distant Supervision. In natural language processing, there has been a long history of extracting relational triples from text (i.e., textual relation extraction) to complete knowledge bases [19, 33, 59, 63]. Supervised textual relation extraction models are usually limited by the sizes of human-annotated datasets. To address the issue, Mintz et al. [32] propose to align Freebase [2], a world knowledge base, to text to provide distant supervision for textual relation extraction. Although both targeting at extracting relations, we provide distant supervision for visual relation learning by aligning commonsense knowledge bases with visual concepts, in contrast to textual distant supervision that aligns world knowledge bases with textual entities.
Learning with Noisy Labels. Visual distant supervision may introduce noisy relation labels, which may hurt the performance of scene graph models. In textual distant super-
vision, many denoising methods have been developed under the multi-instance learning formulation [58, 30, 64, 16]. However, visual relation detection aims to extract relations on instance level (i.e., predicting relation instances in specific images), whereas textual relation extraction focuses on extracting global relations between entities (i.e., synthesizing information from all sentences containing the entity pair to identify their global relation). Therefore, denoising methods for distantly supervised textual relation extraction cannot well serve visual relation detection. Previous instance-level denoising methods have explored handling noisy labels in image classification [20, 38, 46, 49, 25] and object detection [24, 43, 11] based on internal data statistics. In comparison, our denoising framework cannot only leverage internal data statistics, but can also be seamlessly integrated with external semantic signals and human-labeled data for better denoising results.</p>
<h2>3. Problem Definition</h2>
<p>We first provide a formal definition of the problem and key terminologies in our work.
Scene Graphs. Formally, a scene graph consists of the following elements: (1) Objects. Each object $o b j=(b, c)$ is associated with a bounding box $b \in \mathbb{R}^{4}$ and a category $c \in \mathcal{C}$, where $\mathcal{C}$ is the object category set. (2) Relations, with $r \in \mathcal{R}$, where $\mathcal{R}$ is the relation category set (including NA indicating no relation). Given an image, scene graph models aim to extract the relational triple $(s, r, o)$.
Knowledge Bases. Most knowledge bases store relations between concepts in the form of relational triples $\left(c_{i}, r, c_{j}\right)$.
Distantly Supervised and Semi-supervised Learning. In the traditional fully supervised relation learning, human labeled data $D_{L}$ is required to train scene graph models. In distantly supervised relation learning, where no humanlabeled data is available, we automatically create distantly labeled data $D_{S}$ using images and knowledge bases to train scene graph models. When human-labeled data $D_{L}$ is available, we can further leverage $D_{S} \cup D_{L}$ in a semi-supervised fashion to surpass fully supervised models trained on $D_{L}$.</p>
<h2>4. Visual Distant Supervision</h2>
<p>In this section, we introduce the assumption and approach of visual distant supervision, which aims to create large-scale labeled data for visual relation learning.</p>
<p>The key insight of visual distant supervision is that visual relational triples correspond to commonsense knowledge. For example, the relational triple (person, riding, horse) expresses the commonsense "person can ride horses". Therefore, commonsense knowledge bases can provide possible relation candidates between visual objects to distantly supervise visual relation learning. To this end, we perform</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. The denoising framework for visual distant supervision. The framework iteratively estimates the probabilistic relation labels based on EM optimization, and can be realized in distantly supervised and semi-supervised fashion. Best viewed in color.</p>
<p>Visual distant supervision by first constructing a commonsense knowledge base, and then aligning it to images.</p>
<p>Knowledge Base Construction. Although several commonsense knowledge bases have been constructed [45], we find that they cannot well serve visual distant supervision due to their incompleteness. Therefore, instead of adopting existing knowledge bases, we automatically construct a commonsense knowledge base by extracting relational triples from Web-scale image captions. Specifically, we extract relational triples from Conceptual Captions [42], which contains 3.3M captions of images, using a rule-based textual parser [41]. The resultant knowledge base contains 18,618 object categories, 63,232 relation categories and 1,876,659 distinct relational triples, where each object pair has 1.94 relations on average.</p>
<p>Knowledge Base and Image Alignment. To align the knowledge base and images, we need to obtain the bounding boxes and categories of objects in each image. In this work, we utilize the images and object annotations from Visual Genome, while obtaining object information in open-domain images using object detectors [39] is also applicable. After that, for each object pair, we retrieve all the relation labels in the knowledge base as relation candidates.</p>
<p>Nevertheless, we observe that directly performing distant supervision will produce considerable noise. For example, if there are multiple <em>person</em> and <em>horse</em> objects in an image, there will be a riding relation label between each <em>person</em> and <em>horse</em> pair. Inspired by previous works [57], we adopt a simple but effective heuristic constraint to filter out a large number of noisy labels. Specifically, we assign distant relation labels for an object pair only if the bounding boxes of the subject and object have overlapping areas. After alignment, the relation labels from distant supervision can cover 70.3% relation labels from Visual Genome.</p>
<h1>5. Denoising for Visual Distant Supervision</h1>
<p>The relation labels from distant supervision can be readily used to train any scene graph models. However, distant supervision may introduce noisy relation labels, which may hurt the model performance. To alleviate the noise in visual distant supervision, we propose a denoising framework, as shown in Figure 3. Regarding the ground-truth relation labels of distantly labeled data as latent variables, we iteratively estimate the probabilistic relation labels, and eliminate the noisy ones to train any scene graph models. The framework can be realized by optimizing the coherence of internal statistics of distantly labeled data, and can also be seamlessly integrated with external semantic signals (e.g., image-caption retrieval models), or human-labeled data to achieve better denoising results. In this section, we introduce the framework in distantly supervised and semi-supervised settings respectively. We refer readers to the appendix for the pseudo-code of the framework.</p>
<h2>5.1. Distantly Supervised Framework</h2>
<p>In distantly supervised framework, where only distantly labeled data $D_S$ is available, we aim to refine the probabilistic relation labels of $D_S$ iteratively by maximizing the coherence of its internal statistics using EM optimization.</p>
<p>E step. In the E step of $t$-th iteration, we estimate the labels of distantly labeled data to obtain $D_S^t = {(s, \mathbf{r}^{t}, o)^{(k)}}<em _s_="(s," o_="o)">{k=1}^N$, where $\mathbf{r}^{t}$ indicates the latent relation between the object pair $(s, o)$ in an image. Specifically, $\mathbf{r}^{t} \in \mathbb{R}^{|\mathcal{R}|}$ is a probabilistic distribution over all relations in $\mathcal{R}$, which comes from either (1) raw labels from distant supervision (in the initial iteration), or (2) probabilistic relation labels estimated by the model. Given an object pair $(s, o)$, we denote the set of retrieved distant labels as $\mathcal{R}</em>}$. Note that during the EM optimization, we only refine the distant labels in $\mathcal{R<em _s_="(s," o_="o)">{(s, o)}$, and keep $\mathbf{r}_i^t = 0$, if $r_i \notin \mathcal{R}</em>$.</p>
<p>(1) In the initial iteration (i.e., $t = 1$), the relation labels are assigned by aligning knowledge bases and images (see Section 4), denoted as follows:</p>
<p>$\mathbf{d} = \Psi(s, o, \Lambda), \tag{1}$$</p>
<p>where $\Lambda$ is the knowledge base, $\Psi(\cdot)$ is the alignment operation. $\mathbf{d}$ is a multi-hot vector where $\mathbf{d}<em _s_="(s," o_="o)">i = 1$ if $r_i \in \mathcal{R}</em>$ and otherwise 0.</p>
<p>We argue that when available, external semantic signals are useful in distinguishing reasonable distant labels from</p>
<p><sup>1</sup>We omit the superscript $k$ in the following for simplicity.</p>
<p>noisy ones. Without losing generality, in this work, we adopt CLIP [36], a state-of-the-art cross-modal representation model pre-trained on large-scale image-caption pairs ${ }^{2}$, to measure the semantic relatedness between a textual relational triple from distant supervision, and the corresponding visual object pair. Specifically, given an object pair, we obtain the visual input by masking the area in the image that is not covered by the bounding boxes of the object pair. To obtain the textual input, we simply concatenate the subject, relation and object in the relational triple into a text snippet. Then the visual and textual inputs are fed into CLIP to obtain their unnormalized relatedness score (i.e., cosine similarity), summarized as follows:</p>
<p>$$
\alpha_{i}=\Phi\left(v, u_{i}\right)
$$</p>
<p>where $v$ is the visual input of the object pair, $u_{i}$ is the textual input of the distantly labeled relation $r_{i}, \Phi(\cdot, \cdot)$ denotes external semantic signals, and $\alpha_{i}$ is the relatedness score. After that, we normalize the relatedness score to obtain the probabilistic relation distribution over $\mathcal{R}_{(s, o)}$ :</p>
<p>$$
\mathbf{e}<em i="i">{i}=\frac{\exp \left(\alpha</em>}\right)}{\sum_{j=1}^{|\mathcal{R}|} \mathbb{1}\left[\mathbf{d<em j="j">{j}=1\right] \exp \left(\alpha</em>
$$}\right)}, r_{i} \in \mathcal{R}_{(s, o)</p>
<p>where $\mathbb{1}[x]$ is 1 if $x$ is true otherwise 0 . $\mathbf{e}$ is the probabilistic relation distribution given by external semantic signals, where $\mathbf{e}<em i="i">{i}=0$ if $r</em>$.} \notin \mathcal{R}_{(s, o)</p>
<p>The relation distribution can then be initialized by $\mathbf{r}^{1}=$ $\mathbf{e}$. Note that external signals are not necessarily required by the framework (i.e., initialize $\mathbf{r}^{1}=\mathbf{d}$ when such external signals are not available).
(2) In the non-initial iterations (i.e., $t&gt;1$ ), we infer the probabilistic relation distribution by the convex combination of the internal prediction from scene graph models, and external semantic signals:</p>
<p>$$
\mathbf{r}<em i="i">{i}^{t}=\omega f</em>
$$}\left(s, o ; \theta^{t-1}\right)+(1-\omega) \mathbf{e}_{i</p>
<p>where $f_{i}\left(s, o ; \theta^{t-1}\right)$ is the probability of $r_{i}$ from the scene graph model with parameter $\theta^{t-1}$. Here $f_{i}\left(s, o ; \theta^{t-1}\right)$ is obtained by normalizing the relation logits over $\mathcal{R}_{(s, o)}$ as in Equation 3. $\omega \in[0,1]$ is a weighting hyperparameter, where $\omega=1$ when external signals are not available.</p>
<p>We note it is possible that none of the distant labels between $(s, o)$ are correct (see (person, beach) in Figure 1 for example). Therefore, we eliminate noisy object pairs from $D_{S}^{t}$, by discarding object pairs with top $k \%$ NA relation logits given by the scene graph model.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>M step. In the M step, given the distant labels from the E step, we optimize the scene graph model parameters $\theta^{t-1}$ by maximizing the log-likelihood of $D_{S}^{t}$ as follows:</p>
<p>$$
\theta^{t}=\underset{\theta}{\arg \max } \mathcal{L}<em S="S">{p}\left(D</em>\right)
$$}^{t} ; \theta^{t-1</p>
<p>where $\mathcal{L}<em S="S">{p}\left(D</em>$ in a noise-aware approach as follows:}^{t} ; \theta^{t-1}\right)$ is the entropy-based log-likelihood function, which incorporates the probabilistic relation distribution of $D_{S}^{t</p>
<p>$$
\begin{aligned}
\mathcal{L}<em S="S">{p}\left(D</em>}^{t} ; \theta^{t-1}\right)= &amp; \sum_{\left(s, \mathbf{r}^{1}, o\right) \in D_{S}^{t}} \sum_{i=1}^{|\mathcal{R}|} \mathbf{r<em i="i">{i}^{t}\left(\mathbb{1}\left[\mathbf{d}</em>\right)\right. \
&amp; \left.+\mathbb{1}\left[\mathbf{d}}=1\right] \log f_{i}\left(s, o ; \theta^{t-1<em i="i">{i}=0\right] \log \left(1-f</em>\right)\right)\right)
\end{aligned}
$$}\left(s, o ; \theta^{t-1</p>
<p>where $\theta^{0}$ is randomly initialized.</p>
<h3>5.2. Semi-supervised Framework</h3>
<p>Distantly supervised models can be further integrated with human-labeled data to surpass fully supervised models. In fact, we find after pre-training on distantly supervised data (see Section 5.1), simple fine-tuning on humanlabeled data can lead to significant improvements over fully supervised models. This simple pre-training and fine-tuning paradigm is appealing, since it does not change the number of parameters, architectures and overhead in training specific downstream scene graph models.</p>
<p>Nevertheless, we find closely integrating human-labeled data in the denoising framework can yield better performance, since coherence can be achieved between distantly labeled data $D_{S}$ and human-labeled data $D_{L}$ for mutual enhancement. Our semi-supervised framework largely follows the distantly supervised framework in Section 5.1, where we estimate probabilistic relation labels in E step, and optimizing model parameters in M step. To integrate human-labeled data, we further decompose the M step into two sub-steps: pre-training on distantly labeled data (M1 step), and fine-tuning on human-labeled data (M2 step).
E step. In the E step of $t$-th iteration, we estimate the labels of distantly supervised data $D_{S}^{t}$. Here $\mathbf{r}_{t}$ is obtained by (1) first obtaining raw distant labels $\mathbf{d}$ as in Equation 1, and (2) then estimating probabilistic relation labels by the finetuned scene graph model as follows:</p>
<p>$$
\mathbf{r}<em i="i">{i}^{t}=f</em>
$$}\left(s, o ; \theta_{2}^{t-1}\right), \quad r_{i} \in \mathcal{R}_{(s, o)</p>
<p>where $f_{i}\left(\cdot ; \theta_{2}^{t-1}\right)$ is the fine-tuned scene graph model from the M2 step of the previous iteration. In the initial iteration, $f_{i}\left(\cdot ; \theta_{2}^{0}\right)$ is initialized by a fully supervised model. Note Equation 7 does not include external semantic signals, since models fine-tuned on human-labeled data can provide more direct denoising signals. After that, we discard noisy object pairs (see Section 5.1). To better cope with the fine-tuning</p>
<p>| Models | | Predicate Classification | | | | Scene Graph Classification | | | | Scene Graph Detection | | | | Mean |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | | R@50 | R@100 | mR@50 | mR@100 | R@50 | R@100 | mR@50 | mR@100 | R@50 | R@100 | mR@50 | mR@100 |
| $\hat{\mathbf{r}}$ | Freq [57]<em> | 20.80 | 20.98 | - | - | 10.92 | 11.08 | - | - | 11.01 | 11.64 | - | - |
|  | Freq-Overlap [57]</em> | 20.90 | 22.21 | - | - | 9.91 | 9.91 | - | - | 10.84 | 10.86 | - | - |
|  | Decision Tree [35]<em> | 33.02 | 33.35 | - | - | 14.51 | 14.57 | - | - | 12.58 | 13.23 | - | - |
|  | Label Propagation [65]</em> | 25.17 | 25.41 | - | - | 9.91 | 9.97 | - | - | 6.74 | 6.83 | - | - |
|  | Weak Supervision† | 44.96 | 47.19 | 24.58 | 27.14 | 19.27 | 19.93 | 6.97 | 7.54 | 19.78 | 21.33 | 5.01 | 5.41 |
|  | Limited Labels [5] | 49.68 | 50.73 | 37.43 | 38.91 | 24.65 | 25.08 | 13.30 | 13.94 | 22.87 | 24.16 | 12.66 | 13.39 |
| $\hat{\mathbf{r}}^{\prime}$ | EXT | 6.64 | 9.74 | 10.66 | 15.16 | 3.96 | 4.82 | 4.25 | 4.92 | 1.93 | 3.06 | 1.66 | 2.49 |
|  | Raw Label | 30.61 | 33.48 | 20.98 | 23.25 | 15.69 | 16.99 | 11.06 | 12.53 | 9.36 | 10.26 | 6.56 | 7.13 |
|  | Raw Label + EXT | 38.21 | 40.90 | 24.94 | 27.45 | 17.52 | 18.85 | 11.66 | 12.56 | 15.84 | 18.31 | 9.49 | 11.23 |
|  | Motif† | 48.88 | 51.73 | 34.40 | 39.69 | 23.15 | 24.18 | 15.81 | 16.66 | 18.73 | 22.10 | 10.89 | 13.34 |
|  | Motif | 50.23 | 53.18 | 33.99 | 40.62 | 24.90 | 26.00 | 16.50 | 18.03 | 20.09 | 22.74 | 12.21 | 14.42 |
|  | Motif + DNS + EXT | 53.40 | 56.54 | 37.68 | 41.98 | 26.12 | 27.46 | 17.20 | 18.39 | 23.69 | 25.59 | 13.84 | 15.23 |
| $\hat{\mathbf{r}}^{\prime \prime}$ | Motif [57] | 67.93 | 70.20 | 52.65 | 55.41 | 31.14 | 31.92 | 23.53 | 25.27 | 28.90 | 31.25 | 18.26 | 20.63 |
|  | Motif + Pretrain (Ours) | 73.22 | 75.04 | 60.44 | 63.67 | 34.11 | 34.88 | 26.51 | 27.94 | 30.70 | 33.32 | 24.76 | 27.45 |
|  | Motif + DNS (Ours) | 76.28 | 77.98 | 60.20 | 63.61 | 35.93 | 36.47 | 28.07 | 30.09 | 33.94 | 37.26 | 23.90 | 28.06 |</p>
<p>Table 1. Main results of visual distant supervision ( $\%$ ). DS: distantly supervised, FS: fully supervised, SS: semi-supervised. EXT: external semantic signal, DNS: denoising. * denotes results from Chen et al. [5], $\dagger$ indicates models trained on the images with captions.
procedure on human-labeled data, where models are usually optimized towards a single discrete relation label between an object pair, we discretize $\mathbf{r}^{t}$ into a one-hot vector $\hat{\mathbf{r}}^{i}$, where $\hat{\mathbf{r}}<em j="j">{i}^{i}=1$, if $i=\arg \max </em>} \mathbf{r<em 1="1">{j}^{i}$.
M1 step. After obtaining the distant label $\hat{\mathbf{r}}^{t}$ from the E step, we pre-train the scene graph model from scratch: $\theta</em>}^{t}=$ $\arg \max_{\theta} \mathcal{L<em S="S">{q}\left(D</em>$ is the cross-entropy based objective as follows:}^{t} ; \theta\right)$, where $\mathcal{L}_{q</p>
<p>$$
\mathcal{L}<em S="S">{q}\left(D</em>}^{t} ; \theta\right)=\sum_{\left(s, \hat{\mathbf{r}}^{i}, o\right) \in D_{S}^{t}} \sum_{i=1}^{|\mathcal{R}|} \mathbb{1}\left[\hat{\mathbf{r}<em i="i">{i}^{t}=1\right] \log f</em>(s, o ; \theta)
$$</p>
<p>M2 step. In the M2 step, we simply fine-tune the pretrained scene graph model on the human labeled data with $\theta_{2}^{t}=\arg \max <em q="q">{\theta} \mathcal{L}</em>\right)$.}\left(D_{L} ; \theta_{1}^{t</p>
<h2>6. Experiments</h2>
<p>In this section, we empirically evaluate visual distant supervision and the denoising framework on scene graph generation. We also show the advantage of visual distant supervision in dealing with long-tail problems, and its promising potential when equipped with ideal knowledge bases.</p>
<h3>6.1. Experimental Settings</h3>
<p>We first introduce the experimental settings, including datasets, evaluation metrics and baselines.</p>
<p>Datasets. We evaluate our models on Visual Genome [23], a widely adopted benchmark for scene graph generation [51, 57, 5, 56]. Each image in the dataset is manually annotated with objects (bounding boxes and object categories) and relations. In our experiments, during training distant supervision is performed using the intersection of relations from Visual Genome and the knowledge base. During evaluation, in the main experiments, we adopt the refined relation schemes and data split from Chen et al. [5], which removes hypernyms and redundant synonyms in the most frequent 50 relation categories in Visual Genome, resulting in 20 well-defined relation categories. We also report experimental results on the Visual Genome dataset with 50 relation categories in appendix. We refer readers to the appendix for more details about data statistics.</p>
<p>Evaluation Metrics. Following previous works [51, 57, 5], we assess our approach in three standard evaluation modes: (1) Predicate classification. Given the ground-truth bounding boxes and categories of objects in an image, models need to predict the predicates (i.e., relations) between object pairs. (2) Scene graph classification. Given the ground-truth bounding boxes of objects, models need to predict object categories and relations. (3) Scene graph detection. Given an image, models are asked to predict bounding boxes and categories of objects, and relations between objects. We adopt the widely used micro-recall@K (R@K) metric to evaluate the model performance [51, 57, 5], which calculates the recall in top K relation predictions. To investigate the model performance in dealing with long-tail relations, we also report macro-recall@K (mR@K) [4, 48], which calculates the mean recall of all relations in top K predictions. Following Zellers et al. [57], we also report the mean of these metrics to show the overall performance.</p>
<p>Baselines. We compare our models with strong baselines. (1) The first series of baselines learn visual relations from a few (i.e., 10) human-labeled seed instances for each relation. Frequency-based baseline (Freq) [57] predicts the most frequent relation between an object pair. Overlapenhanced frequency-based baseline (Freq-Overlap) [57]</p>
<table>
<thead>
<tr>
<th>Models</th>
<th></th>
<th>Predicate Classification</th>
<th></th>
<th></th>
<th></th>
<th>Scene Graph Classification</th>
<th></th>
<th></th>
<th></th>
<th>Scene Graph Detection</th>
<th></th>
<th></th>
<th></th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>R@50</td>
<td>R@100</td>
<td>mR@50</td>
<td>mR@100</td>
<td>R@50</td>
<td>R@100</td>
<td>mR@50</td>
<td>mR@100</td>
<td>R@50</td>
<td>R@100</td>
<td>mR@50</td>
<td>mR@100</td>
<td></td>
</tr>
<tr>
<td>Motif</td>
<td></td>
<td>50.23</td>
<td>53.18</td>
<td>33.99</td>
<td>40.62</td>
<td>24.90</td>
<td>26.00</td>
<td>16.50</td>
<td>18.03</td>
<td>20.09</td>
<td>22.74</td>
<td>12.21</td>
<td>14.42</td>
<td>27.74</td>
</tr>
<tr>
<td>Motif + Cleanness Loss [24]</td>
<td></td>
<td>51.10</td>
<td>54.23</td>
<td>34.69</td>
<td>42.67</td>
<td>24.06</td>
<td>24.98</td>
<td>16.46</td>
<td>18.56</td>
<td>21.94</td>
<td>23.89</td>
<td>13.21</td>
<td>14.49</td>
<td>28.36</td>
</tr>
<tr>
<td>Motif + DNS (iter 1)</td>
<td></td>
<td>50.23</td>
<td>53.18</td>
<td>33.99</td>
<td>40.62</td>
<td>24.90</td>
<td>26.00</td>
<td>16.50</td>
<td>18.03</td>
<td>20.09</td>
<td>22.74</td>
<td>12.21</td>
<td>14.42</td>
<td>27.74</td>
</tr>
<tr>
<td>Motif</td>
<td>+ DNS (iter 2)</td>
<td>51.54</td>
<td>54.53</td>
<td>36.93</td>
<td>41.97</td>
<td>24.81</td>
<td>26.08</td>
<td>16.13</td>
<td>17.56</td>
<td>22.83</td>
<td>24.36</td>
<td>13.48</td>
<td>14.45</td>
<td>28.72</td>
</tr>
<tr>
<td>Motif + DNS + EXT (iter 1)</td>
<td></td>
<td>52.82</td>
<td>55.98</td>
<td>36.25</td>
<td>41.66</td>
<td>25.79</td>
<td>26.98</td>
<td>17.39</td>
<td>18.56</td>
<td>22.63</td>
<td>25.12</td>
<td>13.30</td>
<td>15.40</td>
<td>29.32</td>
</tr>
<tr>
<td>mR</td>
<td>+ DNS + EXT (iter 2)</td>
<td>53.40</td>
<td>56.54</td>
<td>37.68</td>
<td>41.98</td>
<td>26.12</td>
<td>27.46</td>
<td>17.20</td>
<td>18.39</td>
<td>23.69</td>
<td>25.59</td>
<td>13.84</td>
<td>15.23</td>
<td>29.76</td>
</tr>
<tr>
<td>Motif [57]</td>
<td></td>
<td>67.93</td>
<td>70.20</td>
<td>52.65</td>
<td>55.41</td>
<td>31.14</td>
<td>31.92</td>
<td>23.53</td>
<td>25.27</td>
<td>28.90</td>
<td>31.25</td>
<td>18.26</td>
<td>20.63</td>
<td>38.09</td>
</tr>
<tr>
<td>mR</td>
<td>Motif + DNS (iter 1)</td>
<td>73.50</td>
<td>75.33</td>
<td>61.40</td>
<td>65.20</td>
<td>35.39</td>
<td>35.98</td>
<td>28.71</td>
<td>30.25</td>
<td>34.83</td>
<td>37.68</td>
<td>24.78</td>
<td>27.90</td>
<td>44.25</td>
</tr>
<tr>
<td>mR</td>
<td>+ DNS (iter 2)</td>
<td>76.28</td>
<td>77.98</td>
<td>60.20</td>
<td>63.61</td>
<td>35.93</td>
<td>36.47</td>
<td>28.07</td>
<td>30.09</td>
<td>33.94</td>
<td>37.26</td>
<td>23.90</td>
<td>28.06</td>
<td>44.31</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results of denoising visual distant supervision (%). Results of different denoising iterations are shown. DS: distantly supervised, FS: fully supervised, SS: semi-supervised. EXT: external semantic signal, DNS: denoising.
further filters out non-overlapping object pairs. Following Chen et al. [5], we also compare with learning Decision Tree [35] from seed instances. (2) For semi-supervised methods that further incorporate unlabeled data, following Chen et al. [5], we compare with Label Propagation [65] that propagates the labels of the seed data to unlabeled data based on data point communities. Limited Labels [5] is the state-of-the-art semi-supervised scene graph model, which first learns a relational generative model using seed instances, and then assigns soft labels to unlabeled data to train scene graph models. (3) We also compare with strong weakly supervised models (Weak Supervision?) that are supervised by the relation labels parsed from the captions of the corresponding images [34, 61]. Specifically, we use images with captions in Visual Genome to train the weakly supervised model. We label the object pairs with relations parsed from the corresponding caption, and employ the overlapping constraint to filter out noisy labels (see Section 4). For fair comparisons, we also train a distantly supervised model without denoising based on the same images with captions in Visual Genome (Motif?). (4) For fully supervised methods, we compare with the strong and widely adopted Neural Motif (Motif) [57]. For fair comparisons, all the neural models in our experiments are implemented based on the Neural Motif model, with ResNeXt-101-FPN [28, 50] as the backbone. (5) For denoising baselines, we adapt Cleanness Loss [24] that heuristically down-weight the relation labels with large loss. We refer readers to the appendix for more implementation details.
Ablations. To investigate the contribution of each component, we conduct ablation study. (1) In distantly supervised setting, we perform distant supervision based on the general knowledge base from Section 4. Raw Label predicts relations by raw distant relation labels. EXT indicates external semantic signals. Motif denotes training on raw distant relation labels, and DNS denotes denoising based on the proposed framework. (2) In semi-supervised setting, in addition to distantly labeled data, we assume access to full human-annotated relation data. Pretrain indicates directly fine-tuning the model pre-trained on distantly supervised data from the general knowledge base. DNS denotes denoising based on the targeted knowledge base constructed from Visual Genome training annotations.</p>
<h3>6.2 Effect of Visual Distant Supervision</h3>
<p>We report the main results of visual distant supervision in Table 1, from which we have the following observations: (1) Without using any human-labeled data, our denoised distantly supervised model significantly outperforms all baseline methods, including weakly supervised methods and even strong semi-supervised approaches that utilize human-labeled seed data. (2) By further incorporating human-labeled data, our semi-supervised models consistently outperform state-of-the-art fully supervised models by a large margin, e.g., $8.3 \mathrm{R} @ 50$ improvement for predicate classification. Specifically, simple fine-tuning of the pre-trained model can lead to significant improvement. Since the model is pre-trained on general knowledge bases, it can also be directly fine-tuned on any other scene graph dataset to achieve strong performance. Moreover, by closely integrating human-labeled data and distantly labeled data in the denoising framework, we can achieve even better performance. (3) Notably, our models achieve competitive macro-recall, which shows that our models are not biased towards a few frequent relations, and can better deal with the long-tail problem. In summary, visual distant supervision can effectively create large-scale labeled data to facilitate visual relation learning in both distantly supervised and semi-supervised scenarios.</p>
<h3>6.3 Effect of the Denoising Framework</h3>
<p>The experimental results of denoising distant supervision are shown in Table 2, from which we observe that: Equipped with the proposed denoising framework, our models show consistent improvements over baseline models in both distantly supervised and semi-supervised settings. Specifically, in distantly supervised setting, the model performance improves with the iterative optimization of the coherence of internal data statistics. Further incorporating external semantic signals and human-labeled data cannot</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>R@50</th>
<th>R@100</th>
<th>mR@50</th>
<th>mR@100</th>
</tr>
</thead>
<tbody>
<tr>
<td>Raw Label</td>
<td>35.62 (+5.01)</td>
<td>39.78 (+6.30)</td>
<td>34.83 (+13.85)</td>
<td>39.45 (+16.20)</td>
</tr>
<tr>
<td>Raw Label + EXT</td>
<td>45.07 (+6.86)</td>
<td>49.00 (+8.10)</td>
<td>44.18 (+19.24)</td>
<td>48.56 (+21.11)</td>
</tr>
<tr>
<td>Motif</td>
<td>53.02 (+2.79)</td>
<td>56.31 (+1.13)</td>
<td>46.65 (+12.66)</td>
<td>50.90 (+10.28)</td>
</tr>
<tr>
<td>Motif + DNS + EXT</td>
<td>55.54 (+2.14)</td>
<td>58.99 (+2.45)</td>
<td>50.87 (+13.19)</td>
<td>55.69 (+13.71)</td>
</tr>
<tr>
<td>Motif [57]</td>
<td>67.93</td>
<td>70.02</td>
<td>52.65</td>
<td>55.41</td>
</tr>
</tbody>
</table>
<p>Table 3. Experimental results of distant supervision with ideal knowledge bases on predicate classification (%). We also show the absolute improvements with respect to the results from general knowledge bases. DS: distantly supervised, FS: fully supervised.
only boost the denoising performance, but also speed up the convergence of the iterative algorithm. The reason is that external semantic signals and human-labeled data can provide strong auxiliary denoising signals for both better initialization and iteration of the framework. The results show that the proposed denoising framework can effectively alleviate the noise in visual distant supervision in both distantly supervised and semi-supervised settings.</p>
<h3>6.4. Analysis</h3>
<p>Distant Supervision with Ideal Knowledge Bases. The effectiveness of visual distant supervision may be limited by the incompleteness of knowledge bases, and mismatch in relation and object names between knowledge bases and images. We show the potential of visual distant supervision with ideal knowledge bases. Specifically, we construct an ideal knowledge base from the training annotations of Visual Genome, which better covers the relational knowledge in the dataset, and can be well aligned to Visual Genome images. Experimental results are shown in Table 3, from which we observe that: Equipped with ideal knowledge bases, the performance of distantly supervised models improves significantly. Notably, the macro-recall of the denoised distantly supervised model dramatically improves, e.g., with 13.2 absolute gain in mR@50, achieving comparable macro performance to fully supervised approaches. Therefore, we expect visual distant supervision will even better promote visual relation learning, given that knowledge bases are becoming increasingly complete.
Human Evaluation. Since relations in existing scene graph datasets are typically not exhaustively annotated, previous works have concentrated on evaluating models with recall metric [51, 57, 5]. To provide multi-dimensional evaluation, we further evaluate the precision of our scene graph models. Specifically, we randomly sample 200 images from the validation set of Visual Genome. For each image, we obtain 20 top-ranking relational triples extracted by a scene graph model. Then we ask human annotators to label whether each relational triple is correctly identified from the image. We show the human evaluation results in Table 4, where micro- and macro-precision@K are reported. (1) For raw labels from distant supervision, in addition to</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">P@10</th>
<th style="text-align: left;">P@20</th>
<th style="text-align: left;">mP@10</th>
<th style="text-align: left;">mP@20</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Raw Labels</td>
<td style="text-align: left;">12.07</td>
<td style="text-align: left;">10.85</td>
<td style="text-align: left;">11.41</td>
<td style="text-align: left;">12.04</td>
</tr>
<tr>
<td style="text-align: left;">Motif + DNS + EXT</td>
<td style="text-align: left;">$\mathbf{3 1 . 9 3}$</td>
<td style="text-align: left;">$\mathbf{2 5 . 2 9}$</td>
<td style="text-align: left;">$\mathbf{2 4 . 7 9}$</td>
<td style="text-align: left;">$\mathbf{2 0 . 7 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Motif [57]</td>
<td style="text-align: left;">42.22</td>
<td style="text-align: left;">31.09</td>
<td style="text-align: left;">39.60</td>
<td style="text-align: left;">29.22</td>
</tr>
<tr>
<td style="text-align: left;">Motif + DNS</td>
<td style="text-align: left;">$\mathbf{5 0 . 5 8}$</td>
<td style="text-align: left;">$\mathbf{3 8 . 6 8}$</td>
<td style="text-align: left;">$\mathbf{4 7 . 4 9}$</td>
<td style="text-align: left;">$\mathbf{3 8 . 5 2}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Human evaluation results on predicate classification (%), where micro- and macro-precision@K are reported. DS: distantly supervised, FS: fully supervised, SS: semi-supervised.
the $10.9 \%$ strictly correct labels in top 20 recommendations, we find that $32.6 \%$ of the distant labels are plausible, which also provides useful signals for visual relation learning. (2) The denoised distantly supervised model achieves competitive precision. Moreover, compared to recall evaluation, our semi-supervised model exhibits more significant relative advantage over the fully supervised model. The results show that with appropriate denoising, distant supervision can lead scene graph models to strong performance in both precision and recall.</p>
<h2>7. Discussion and Outlook</h2>
<p>In this work, we show the promising potential of visual distant supervision in visual relation learning. In the future, given the recent advances in textual relation extraction [30, 17, 54], we believe the following research directions are worth exploring: (1) Developing more sophisticated denoising methods to better realize the potential of visual distant supervision. (2) Completing commonsense knowledge bases by extracting global relations between object pairs from multiple images. (3) Reducing the overhead and bias in annotating large-scale visual relation learning datasets based on raw labels from distant supervision.</p>
<h2>8. Conclusion</h2>
<p>In this work, we propose visual distant supervision and a denoising framework for visual relation learning. Comprehensive experiments demonstrate the effectiveness of visual distant supervision and the denoising framework. In this work, we denoise distant labels for each object pair in isolation. In the future, we will explore modeling the holistic coherence of the produced scene graph to better alleviate the noise in visual distant supervision. It is also important to investigate whether visual distant supervision will introduce extra bias to scene graph models.</p>
<h2>9. Acknowledgement</h2>
<p>This work is jointly funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 62061136001 / DFG TRR-169. Yao is also supported by 2020 Tencent Rhino-Bird Elite Training Program.</p>
<h2>References</h2>
<p>[1] Federico Baldassarre, Kevin Smith, Josephine Sullivan, and Hossein Azizpour. Explanation-based weakly-supervised learning of visual relations with graph networks. In Proceedings of ECCV, pages 612-630, 2020.
[2] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of SIGMOD, pages 1247-1250, 2008.
[3] Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and Jia Deng. Hico: A benchmark for recognizing human-object interactions in images. In Proceedings of CVPR, pages 1017-1025, 2015.
[4] Tianshui Chen, Weihao Yu, Riquan Chen, and Liang Lin. Knowledge-embedded routing network for scene graph generation. In Proceedings of CVPR, pages 6163-6171, 2019.
[5] Vincent S Chen, Paroma Varma, Ranjay Krishna, Michael Bernstein, Christopher Re, and Li Fei-Fei. Scene graph prediction with limited labels. In Proceedings of CVPR, pages 2580-2590, 2019.
[6] Bo Dai, Yuqi Zhang, and Dahua Lin. Detecting visual relationships with deep relational networks. In Proceedings of CVPR, pages 3076-3086, 2017.
[7] Chaitanya Desai and Deva Ramanan. Detecting actions, poses, and objects with relational phraselets. In Proceedings of ECCV, pages 158-172, 2012.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, pages 4171-4186. Association for Computational Linguistics, 2019.
[9] Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xiaoyan Zhu. Reinforcement learning for relation classification from noisy data. In Proceedings of AAAI, volume 32, 2018.
[10] James R. Foulds and Eibe Frank. A review of multi-instance learning assumptions. The Knowledge Engineering Review, 25(1):1-25, 2010.
[11] Jiyang Gao, Jiang Wang, Shengyang Dai, Li-Jia Li, and Ram Nevatia. NOTE-RCNN: NOise Tolerant Ensemble RCNN for semi-supervised object detection. In Proceedings of CVPR, pages 9508-9517, 2019.
[12] Jonathan Gordon and Benjamin Van Durme. Reporting bias and knowledge acquisition. In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, pages 25-30, 2013.
[13] Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Handong Zhao, Xu Yang, and Gang Wang. Unpaired image captioning via scene graph alignments. In Proceedings of CVPR, pages 1032310332, 2019.
[14] Abhinav Gupta and Larry S Davis. Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers. In Proceedings of ECCV, pages 16-29, 2008.
[15] Abhinav Gupta, Aniruddha Kembhavi, and Larry S Davis. Observing human-object interactions: Using spatial and functional compatibility for recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(10):17751789, 2009.
[16] Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and Peng Li. Hierarchical relation extraction with coarse-to-fine grained attention. In Proceedings of EMNLP, pages 22362245, 2018.
[17] Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In Proceedings of EMNLP, pages 4803-4809, 2018.
[18] Drew A. Hudson and Christopher D. Manning. Learning by abstraction: The neural state machine. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'AlchéBuc, Emily B. Fox, and Roman Garnett, editors, Proceedings of NeurIPS, pages 5901-5914, 2019.
[19] Scott B Huffman. Learning information extraction patterns from examples. In International Joint Conference on Artificial Intelligence, pages 246-260, 1995.
[20] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In International Conference on Machine Learning, pages 2304-2313. PMLR, 2018.
[21] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings of CVPR, pages 1219-1228, 2018.
[22] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of CVPR, pages 3668-3678, 2015.
[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123(1):32-73, 2017.
[24] Hengduo Li, Zuxuan Wu, Chen Zhu, Caiming Xiong, Richard Socher, and Larry S Davis. Learning from noisy anchors for one-stage object detection. In Proceedings of CVPR, pages 10588-10597, 2020.
[25] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy labeled data. In Proceedings of CVPR, pages 5051-5059, 2019.
[26] Yikang Li, Wanli Ouyang, Bolei Zhou, Jianping Shi, Chao Zhang, and Xiaogang Wang. Factorizable net: an efficient subgraph-based framework for scene graph generation. In Proceedings of ECCV, pages 335-351, 2018.
[27] Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xiaogang Wang. Scene graph generation from objects, phrases and region captions. In Proceedings of CVPR, pages 12611270, 2017.
[28] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of CVPR, pages 2117-2125, 2017.
[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of ECCV, pages 740-755. Springer, 2014.</p>
<p>[30] Yankai Lin, Shiqì Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation extraction with selective attention over instances. In Proceedings of ACL, pages 21242133, 2016.
[31] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li FeiFei. Visual relationship detection with language priors. In Proceedings of ECCV, pages 852-869, 2016.
[32] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of ACL, pages 1003-1011, 2009.
[33] R Mooney. Relational learning of pattern-match rules for information extraction. In Proceedings of NCAI, volume 328, page 334, 1999.
[34] Julia Peyre, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Weakly-supervised learning of visual relations. In Proceedings of CVPR, pages 5179-5188, 2017.
[35] J. Ross Quinlan. Induction of decision trees. Machine Learning, 1(1):81-106, 1986.
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. Image, 2:T2.
[37] Vignesh Ramanathan, Congcong Li, Jia Deng, Wei Han, Zhen Li, Kunlong Gu, Yang Song, Samy Bengio, Charles Rosenberg, and Li Fei-Fei. Learning semantic relationships for better action retrieval in images. In Proceedings of CVPR, pages 1100-1109, 2015.
[38] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In Proceedings of ICML, pages 4334-4343. PMLR, 2018.
[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In Proceedings of NeurIPS, pages 91-99, 2015.
[40] Sebastian Schuster, Ranjay Krishna, Angel Chang, Li FeiFei, and Christopher D Manning. Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In Proceedings of the fourth workshop on vision and language, pages 70-80, 2015.
[41] Sebastian Schuster, Ranjay Krishna, Angel Chang, Li FeiFei, and Christopher D. Manning. Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In Proceedings of the Fourth Workshop on Vision and Language, pages 70-80, Lisbon, Portugal, Sept. 2015. Association for Computational Linguistics.
[42] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, pages 2556-2565, 2018.
[43] Yunhang Shen, Rongrong Ji, Zhiwei Chen, Xiaopeng Hong, Feng Zheng, Jianzhuang Liu, Mingliang Xu, and Qi Tian. Noise-aware fully webly supervised object detection. In Proceedings of CVPR, pages 11326-11335, 2020.
[44] Jiaxin Shi, Hanwang Zhang, and Juanzi Li. Explainable and explicit visual reasoning over scene graphs. In Proceedings of CVPR, pages 8376-8384, 2019.
[45] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of AAAI, volume 31, 2017.
[46] Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Proceedings of NeurIPS, 2018.
[47] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph generation from biased training. In Proceedings of CVPR, pages 3716-3725, 2020.
[48] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose dynamic tree structures for visual contexts. In Proceedings of CVPR, pages 66196628, 2019.
[49] Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. Combating label noise in deep learning using abstention. In Proceedings of ICML, pages 6234-6243. PMLR, 2019.
[50] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of CVPR, pages 1492-1500, 2017.
[51] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In Proceedings of CVPR, pages 5410-5419, 2017.
[52] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. Graph R-CNN for scene graph generation. In Proceedings of ECCV, pages 670-685, 2018.
[53] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. Auto-encoding scene graphs for image captioning. In Proceedings of CVPR, pages 10685-10694, 2019.
[54] Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. Docred: A large-scale document-level relation extraction dataset. In Proceedings of ACL, pages 764-777, 2019.
[55] Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Situation recognition: Visual semantic role labeling for image understanding. In Proceedings of CVPR, pages 5534-5542, 2016.
[56] Alireza Zareian, Svebor Karaman, and Shih-Fu Chang. Weakly supervised visual semantic parsing. In Proceedings of CVPR, pages 3736-3745, 2020.
[57] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In Proceedings of CVPR, pages 5831-5840, 2018.
[58] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of EMNLP, pages 1753-1762, 2015.
[59] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via convolutional deep neural network. In Proceedings of COLING, pages 2335-2344, 2014.
[60] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and TatSeng Chua. Visual translation embedding network for visual relation detection. In Proceedings of CVPR, pages 55325540, 2017.</p>
<p>[61] Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, and Shih-Fu Chang. PPR-FCN: Weakly supervised visual relation detection via parallel pairwise R-FCN. In Proceedings of CVPR, pages 4233-4241, 2017.
[62] Ji Zhang, Kevin J Shih, Ahmed Elgammal, Andrew Tao, and Bryan Catanzaro. Graphical contrastive losses for scene graph parsing. In Proceedings of CVPR, pages 1153511543, 2019.
[63] Shu Zhang, Dequan Zheng, Xinchen Hu, and Ming Yang. Bidirectional long short-term memory networks for relation classification. In Proceedings of PACLIC, pages 73-78, 2015.
[64] Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D Manning. Position-aware attention and supervised data improve slot filling. In Proceedings of EMNLP, pages 35-45, 2017.
[65] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002.</p>
<h2>A. The Denoising Framework: Pseudo-Code</h2>
<p>In this section, we provide the pseudo-code of the denoising framework in distantly supervised and semisupervised settings respectively.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Distantly</span><span class="w"> </span><span class="n">Supervised</span><span class="w"> </span><span class="n">Denoising</span><span class="w"> </span><span class="n">Framework</span>
<span class="n">Require</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Lambda</span>\<span class="p">)</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">commonsense</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="n">base</span>
<span class="n">Require</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">distantly</span><span class="w"> </span><span class="n">labeled</span><span class="w"> </span><span class="n">image</span><span class="w"> </span><span class="n">data</span>
<span class="n">Require</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span><span class="p">(</span>\<span class="n">cdot</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">theta</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">scene</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">parameter</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">theta</span>\<span class="p">)</span>
<span class="n">Optional</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Phi</span>\<span class="p">)</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="n">semantic</span><span class="w"> </span><span class="k">signal</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="n">Randomly</span><span class="w"> </span><span class="n">initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">parameter</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">Initial</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">step</span><span class="p">:</span><span class="w"> </span><span class="n">estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">probabilistic</span><span class="w"> </span><span class="n">distant</span><span class="w"> </span><span class="n">labels</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="n">Obtain</span><span class="w"> </span><span class="n">distant</span><span class="w"> </span><span class="n">labels</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">d</span><span class="p">}</span><span class="o">=</span>\<span class="n">Psi</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">o</span><span class="p">,</span><span class="w"> </span>\<span class="n">Lambda</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="k">signal</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Phi</span>\<span class="p">)</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">then</span>
<span class="w">        </span><span class="n">Initialize</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">r</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">=</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">e</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="k">else</span>
<span class="w">        </span><span class="n">Initialize</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">r</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">=</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">d</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="n">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="mi">9</span><span class="p">:</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">Initial</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="n">step</span><span class="p">:</span><span class="w"> </span><span class="n">optimize</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">parameter</span>
<span class="w">    </span><span class="mi">10</span><span class="p">:</span><span class="w"> </span><span class="n">Optimize</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="o">=</span>\<span class="n">arg</span><span class="w"> </span>\<span class="nb">max</span><span class="w"> </span><span class="n">_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="mi">11</span><span class="p">:</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">step</span><span class="p">:</span><span class="w"> </span><span class="n">estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">probabilistic</span><span class="w"> </span><span class="n">distant</span><span class="w"> </span><span class="n">labels</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="k">signal</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Phi</span>\<span class="p">)</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">then</span>
<span class="w">            </span><span class="n">Estimate</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">r</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="o">=</span>\<span class="n">omega</span><span class="w"> </span><span class="n">f_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">o</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span>\<span class="n">omega</span><span class="p">)</span><span class="w"> </span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">e</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="n">Estimate</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">r</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="o">=</span><span class="n">f_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">o</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="n">Eliminate</span><span class="w"> </span><span class="n">noisy</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">pairs</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="n">step</span><span class="p">:</span><span class="w"> </span><span class="n">optimize</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">parameter</span>
<span class="w">        </span><span class="n">Optimize</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="o">=</span>\<span class="n">arg</span><span class="w"> </span>\<span class="nb">max</span><span class="w"> </span><span class="n">_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">p</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<h2>B. Implementation Details</h2>
<p>In this section, we provide implementation details of our model and baseline methods. For fair comparisons, all the neural models in our experiments are implemented using the same object detector, scene graph model and backbone.
Object Detector. We adopt the object detector implementation from Tang et al. [47]. Specifically, the object detector is trained using SGD optimizer with learning rate $8 \times 10^{-3}$ and batch size 8 . During the training process, the learning rate is decreased two times by 10 in 30,000 and 40,000 iterations respectively.
Scene Graph Model. For the base scene graph model, we follow the implementation of Neural Motif [57], with ResNeXt-101-FPN [28, 50] as the backbone. For predicate classification and scene graph classification, the ratio of positive relation samples and negative relation samples in each image is at most $1: 3$. For scene graph detection, a relational triplet is considered as positive only if the detected object pairs match ground-truth annotations, i.e., with identical object categories and bounding box $\mathrm{IoU}&gt;0.5$. We</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="w"> </span><span class="nt">Semi-Supervised</span><span class="w"> </span><span class="nt">Denoising</span><span class="w"> </span><span class="nt">Framework</span>
<span class="nt">Require</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Lambda</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">commonsense</span><span class="w"> </span><span class="nt">knowledge</span><span class="w"> </span><span class="nt">base</span>
<span class="nt">Require</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">distantly</span><span class="w"> </span><span class="nt">labeled</span><span class="w"> </span><span class="nt">image</span><span class="w"> </span><span class="nt">data</span>
<span class="nt">Require</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">human-labeled</span><span class="w"> </span><span class="nt">image</span><span class="w"> </span><span class="nt">data</span>
<span class="nt">Require</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">f</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">any</span><span class="w"> </span><span class="nt">scene</span><span class="w"> </span><span class="nt">graph</span><span class="w"> </span><span class="nt">model</span><span class="o">,</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">parameter</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">f</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">^</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">fully</span><span class="w"> </span><span class="nt">supervised</span><span class="w"> </span><span class="nt">model</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="o">^</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">arg</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">q</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">while</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">done</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">3</span><span class="o">:</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">E</span><span class="w"> </span><span class="nt">step</span><span class="o">:</span><span class="w"> </span><span class="nt">estimate</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">probabilistic</span><span class="w"> </span><span class="nt">distant</span><span class="w"> </span><span class="nt">labels</span>
<span class="w">    </span><span class="nt">4</span><span class="o">:</span><span class="w"> </span><span class="nt">Estimate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">r</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="nt">f_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">s</span><span class="o">,</span><span class="w"> </span><span class="nt">o</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t-1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">5</span><span class="o">:</span><span class="w"> </span><span class="nt">Eliminate</span><span class="w"> </span><span class="nt">noisy</span><span class="w"> </span><span class="nt">object</span><span class="w"> </span><span class="nt">pairs</span>
<span class="w">    </span><span class="nt">6</span><span class="o">:</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">M1</span><span class="w"> </span><span class="nt">step</span><span class="o">:</span><span class="w"> </span><span class="nt">pre-train</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="nt">distantly</span><span class="w"> </span><span class="nt">labeled</span><span class="w"> </span><span class="nt">data</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">7</span><span class="o">:</span><span class="w"> </span><span class="nt">Optimize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">arg</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">q</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">8</span><span class="o">:</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">M2</span><span class="w"> </span><span class="nt">step</span><span class="o">:</span><span class="w"> </span><span class="nt">fine-tune</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="nt">human-labeled</span><span class="w"> </span><span class="nt">data</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">9</span><span class="o">:</span><span class="w"> </span><span class="nt">Optimize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">arg</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">q</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">10</span><span class="o">:</span><span class="w"> </span><span class="nt">end</span><span class="w"> </span><span class="nt">while</span>
</code></pre></div>

<p>find that this strict constraint leads to sparse positive supervision in experiments, especially in our distantly supervised setting. To address the issue, we change the ratio of positive and negative relation samples in distantly supervised setting to strictly 1:1. During evaluation, we only keep 64 object bounding box predictions. The models are trained using SGD optimizer on 2 NVIDIA GeForce RTX 2080 Ti, with momentum 0.9 , batch size 12 and weight decay $5 \times 10^{-4}$.
Our Model. All the hyperparameters of our model are selected by grid search on the validation set. (1) In distantly supervised setting, for the denoising framework, the weighting hyperparameter $\omega$ is 0.9 , and $75 \%$ noisy object pairs are discarded. In the first iteration, we train the model with learning rate 0.12 , and decrease the learning rate 3 times after the plateaus of validation performance. In the second iteration, the learning rate is 0.012 , and decays 1 times after the validation performance plateaus. Note that following Devlin et al. [8], the learning rate in the second iteration is smaller than the first iteration, since we are actually finetuning the model parameter inherited from the first iteration. (2) In semi-supervised setting, for the denoising framework, no object pairs are discarded. The initial fully supervised model is trained with learning rate 0.12 . In both iterations, the learning rate is 0.12 for pre-training, and 0.012 for finetuning. The learning rate decays 2 and 1 times in the first and second iterations respectively. To fine-tune the pretrained distantly supervised model without semi-supervised denoising, we optimize with learning rate 0.012 that decays 2 times. The decay rate is 10 for all models.
Baselines. For the Limited Labels [5], we train the decision trees for 200 different trails on 10 randomly sampled human-labeled seed instances for each relation, and select the best models according to the performance on the validation set. For the weakly supervised model, since Visual Genome does not have image-level captions, we utilize all the images in Visual Genome training set that have captions</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Dependencies of Visual Genome relations from Chen <em>et al.</em> [5]. Directed arrows: hypernyms. Stacked nodes: synonyms. Red nodes: removed relations. Green nodes: retained relations.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Predicate Classification</th>
<th>Scene Graph Classification</th>
<th>Scene Graph Detection</th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R@50</td>
<td>R@100</td>
<td>mR@50</td>
<td>mR@100</td>
</tr>
<tr>
<td>Raw Label</td>
<td>16.93</td>
<td>19.02</td>
<td>5.75</td>
<td>7.15</td>
</tr>
<tr>
<td>Motif</td>
<td>33.21</td>
<td>36.17</td>
<td>10.84</td>
<td>12.48</td>
</tr>
<tr>
<td>Motif + DNS</td>
<td>35.53</td>
<td>38.28</td>
<td>9.35</td>
<td>10.74</td>
</tr>
<tr>
<td>Motif + DNS + EXT</td>
<td>36.43</td>
<td>39.21</td>
<td>8.68</td>
<td>10.03</td>
</tr>
<tr>
<td>Motif [57]</td>
<td>63.96</td>
<td>65.93</td>
<td>15.15</td>
<td>16.24</td>
</tr>
<tr>
<td>Motif + DNS (Ours)</td>
<td>64.43</td>
<td>66.43</td>
<td>16.12</td>
<td>17.47</td>
</tr>
</tbody>
</table>
<p>Table 5. Results of visual distant supervision on Visual Genome 50 predicates (%). DS: distantly supervised, SS: semi-supervised.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Accuracy</th>
<th>Mean Accuracy</th>
<th># Non-zero Predicates</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>top-1</td>
<td>top-5</td>
<td>top-10</td>
</tr>
<tr>
<td>Motif [57]</td>
<td>64.05</td>
<td>81.35</td>
<td>85.05</td>
</tr>
<tr>
<td>Motif + DNS (Ours)</td>
<td>66.10</td>
<td>84.26</td>
<td>87.72</td>
</tr>
</tbody>
</table>
<p>Table 6. Results of visual distant supervision on Visual Genome 1,700 predicates (%). FS: fully supervised, SS: semi-supervised.</p>
<p>from COCO [29], resulting in 35,340 images with captions in total. Then we train the weakly supervised model with all Visual Genome object annotations from these images, and relation labels parsed from the corresponding captions. For the Cleanness Loss [24], we denoise with soft weight given by the confidence of the scene graph model.</p>
<h3>C. Data statistics</h3>
<p>In our main experiments, we adopt the refined relation schemes from Chen <em>et al.</em> [5], which removes hypernyms (e.g., near and on), redundant synonyms (e.g., lying on and laying on), and unclear relations (e.g., and) in the most frequent 50 relation categories in Visual Genome, resulting in 20 well-defined relation categories. The relation dependencies from Chen <em>et al.</em> [5] are shown in Figure 4. The dataset contains 10,986, 1,566 and 3,025 images in training, validation and test set respectively, where each image contains an average of 13.58 objects, 2.10 human-labeled relation instances and 15.60 distantly labeled relation instances.</p>
<h3>D. Supplementary Experiments</h3>
<p><strong>Case Study.</strong> We provide qualitative examples in Figure 5 for better understanding of different scene graph models.</p>
<p><strong>Results on 50 Visual Genome Predicates.</strong> We report the experimental results on 50 Visual Genome predicates in Table 5. We observe that although reasonable performance can be achieved, the improvement from distant supervision and the denoising framework shrinks. This is because the 50 relations are not well-defined, where the major relations are problematic hypernym (e.g., near and on), redundant synonym (e.g., lying on and laying on), and unclear (e.g., and) relations, as pointed out by Chen <em>et al.</em> [5]. The problematic relation schemes can bring difficulties to denoising distant supervision.</p>
<p><strong>Results on 1,700 Visual Genome Predicates.</strong> Visual distant supervision can alleviate the long-tail problem and therefore enables large-scale visual relation extraction. To investigate the effectiveness of visual distant supervision in handling large-scale visual relations, we refine the full Vi-</p>
<p>sual Genome predicates following the principles proposed by [5], resulting in 1,700 well-defined predicates. In addition to top-K accuracy, to better focus on the long-tail performance, we also report top-K mean accuracy and the number of non-zero predicates (i.e., predicates with at least one correctly predicted instance). From the experimental results in Table 6, we observe that our model significantly outperforms its fully supervised counterpart. Notably, our model nearly doubles the top-K mean accuracy and the number of non-zero predicates, demonstrating the promising potential of visual distant supervision in handling largescale visual relations in the future.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Qualitative examples of model predictions in predicate classification task. We show top 10 predictions from (1) models that do not utilize human-labeled data, including weakly supervised and distantly supervised model, and (2) models that leverage human-labeled data, including fully supervised model and our semi-supervised model. Green edges: predictions that match Visual Genome annotations, blue edges: plausible predictions that are not labeled in Visual Genome, red edges: implausible predictions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ In the distantly supervised framework, we are careful to not introduce human annotated relation data in any component. The knowledge base is constructed using a rule-based method from image captions, and CLIP is pre-trained on image-caption pairs only.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
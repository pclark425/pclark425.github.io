<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3516 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3516</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3516</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-124c17b7152a92ea8b15cbcb21eb8b697f7c99b2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/124c17b7152a92ea8b15cbcb21eb8b697f7c99b2" target="_blank">Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type is presented.</p>
                <p><strong>Paper Abstract:</strong> Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the ‘‘gold’’ program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2- to 5-step programs, CIPITR scores at least 3× higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5–10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3516.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3516.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIPITR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complex Imperative Program Induction from Terminal Rewards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural program-induction system that generates multi-step imperative programs to execute against a large knowledge base, trained with only final-answer (terminal) rewards; it constrains program search with KB-aware feasibility, auxiliary rewards, phase-based decomposition, and beam/entropy management.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CIPITR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An end-to-end neural programmer (NPI-style) with (i) query encoder (GRU over KB-annotated tokens), (ii) environment and program RNN core, (iii) operator and argument samplers over a typed variable memory, and (iv) an interpreter that executes induced programs on a KB. Trained with REINFORCE using terminal Jaccard reward plus auxiliary typed rewards; enforces KB/schema feasibility and pragmatic programming constraints (phase change, no action repetition), and uses beam search with stochastic exploration and entropy annealing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CSQA (Complex Sequential Question Answering over WikiData)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A large-scale KBQA dataset (1.15M QA pairs) with diverse complex queries requiring logical, set, quantitative and comparative multi-step reasoning over a large KB subgraph (up to ~10-step programs; results reported per question class such as simple, logical, verify, quantitative, quantitative-count, comparative, comparative-count).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>REINFORCE training with (i) auxiliary reward that gives partial credit for producing the predicted answer-type, (ii) feasibility constraints enforcing KB/schema consistency and valid variable instantiation, (iii) phase-change hierarchical decomposition (retrieval phase vs algorithm phase) to restrict operator sets at different timesteps, (iv) beam management (pruning by predicted answer type, length normalization, penalize no-op, stochastic beam exploration, entropy annealing), and (v) explicit avoidance of action repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CSQA (per Table 3): Top-beam F1 by question class — Simple 96.52%, Logical 87.72%, Verify 89.43%, Quantitative 23.91%, Quantitative-Count 51.33%, Comparative 15.12%, Comparative-Count 0.33%; Overall (All) top-beam F1 58.92%. Using best over top-10 beams, Overall F1 = 73.71% and Comparative = 32.98%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared baselines on CSQA: KVMnet Overall F1 26.67%; NSM top-beam Overall F1 10.63% and NSM best over top-10 beams Overall F1 14.36%. Per-class baselines: e.g., Comparative: KVMnet 1.63%, NSM (best top-beam) 0.17% (NSM improves slightly with larger beam sizes but remains very low).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>CIPITR substantially outperforms baselines on complex logical reasoning: overall F1 improvement vs KVMnet is ~32.25 percentage points (58.92 vs 26.67) on top-beam; vs NSM top-beam overall improvement ~48.29 points (58.92 vs 10.63). On the Comparative class CIPITR top-beam (15.12%) is ~9x KVMnet (1.63%) and ~89x NSM (0.17%), and with larger beams the gap remains large (e.g., CIPITR comparative top-10 = 32.98% vs NSM top-10 = 2.09%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still weak on the hardest classes (comparative-count: top-beam F1 0.33% up to 1.54% with larger beams); relies on oracle gold entity/type/relation linking in experiments (robustness to linking errors not evaluated); requires careful hyperparameter tuning and beam management; limited beam size due to model complexity (≤20) compared to NSM (50) which constrains exploration; reward sparsity and high variance remain challenges despite auxiliary rewards; comparative-count numeric classes still largely unsolved.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation on the Comparative class (original CIPITR F1 15.123%) shows removing individual components reduces performance: removing Beam Pruning => F1 10.34%, removing Auxiliary Reward => 9.85%, removing Biasing last operator => 7.52%, removing Valid variable instantiation => 3.08%, removing Phase change => 2.41%, removing Action repetition constraint => 1.68%. This indicates auxiliary rewards, beam pruning, and biasing toward predicted answer-type are among the most impactful interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3516.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3516.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIPITR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complex Imperative Program Induction from Terminal Rewards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same NPI-style system as above, evaluated on WebQuestionsSP to induce executable programs for 1-2 hop KBQA with constraints and auxiliary supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CIPITR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See description above; applied on WebQuestionsSP using human-annotated entity/relation links as inputs and learning to generate programs that replicate human semantic parses, trained with REINFORCE plus the feasibility and auxiliary mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>WebQuestionsSP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A KBQA benchmark (Freebase-based) where questions typically require up to 2-hop inference chains sometimes with temporal or other constraints; semantic parses by annotators provide oracle subgraphs for fair program induction evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Same as above (REINFORCE training with feasibility constraints, auxiliary answer-type reward, phase-change, beam management); also compared with a strong rule-based program executor derived from human semantic parses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On WebQuestionsSP test (1,639 queries) CIPITR overall F1 82.85%. Per-class (from Table 2): inference-chain-len-1 no constraint 89.09%, len-1 with constraint 79.94%, len-2 no constraint 88.69%, len-2 non-temporal constraint 63.07%, len-2 temporal constraint 48.86%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Rule-based model (derived from human parse) overall F1 81.19%; per-class: len-1 no constraint 87.34%, len-1 with constraint 93.64%, len-2 no constraint 82.85%, len-2 temporal constraint 35.63%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>CIPITR slightly outperforms the rule-based baseline overall (82.85% vs 81.19%). CIPITR performs better on some classes (e.g., len-2 temporal constraint 48.86% vs rule-based 35.63%), but worse on others (len-1 with constraint 79.94% vs 93.64%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Certain constraint types (e.g., non-temporal constraints in len-1) see lower CIPITR performance than rule-based oracle; experiments used human-annotated entity/relation links (oracle), so real-world performance could degrade with noisy linking.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper does not present an explicit per-feature ablation for WebQuestionsSP, but overall analysis and per-class comparisons indicate the auxiliary and feasibility mechanisms help especially on temporally constrained and multi-hop cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3516.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3516.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Symbolic Machines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural semantic-parsing framework that decodes structured program tokens (operators and memory variables) to form executable logical forms using weak supervision from question-answer pairs; uses a key-variable memory and REINFORCE-like learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Symbolic Machines (NSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model that translates natural language to a structured program token-by-token using sequence decoding; uses a key-variable memory to hold intermediate results and is trained with weak supervision (answer-level) using policy gradient methods. Token-by-token decoding limits incorporation of certain high-level constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CSQA (Complex Sequential Question Answering over WikiData)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: multi-hop and complex KBQA requiring logical, set, numeric, and comparative reasoning; NSM attempts to induce programs from answer-only supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Token-by-token program decoding with key-variable memory; beam search (large beams up to 50 in experiments) and REINFORCE-style training (as implemented by the authors of this paper for comparison). NSM does not incorporate high-level feasibility constraints, phase decomposition, or the auxiliary answer-type reward in its standard form.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CSQA (Table 3): NSM best at top beam Overall F1 10.63%; NSM best over top-10 beams Overall F1 14.36%. Per-class example (top-beam): Simple 78.38%, Logical 35.40%, Verify 28.70%, Quantitative 4.31%, Quantitative-Count 12.38%, Comparative 0.17%, Comparative-Count 0.00%. With larger beams (top-10), some classes improve (Simple 96.78%, Logical 69.86%, Verify 60.18%, but Comparative and Comparative-Count remain near zero).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to CIPITR (top-beam Overall 58.92%) and KVMnet (Overall 26.67%), NSM underperforms substantially on the complex CSQA dataset, particularly on longer/more complex programs (comparative categories).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable (NSM is a baseline here). Compared to CIPITR, NSM is much worse: e.g., Comparative top-beam CIPITR 15.12% vs NSM 0.17% (~89x improvement by CIPITR); overall CIPITR top-beam 58.92% vs NSM 10.63% (~5.5x).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Token-by-token decoding makes it difficult to enforce high-level syntactic and semantic constraints during generation, so NSM often generates syntactically or semantically invalid programs; requires large beam sizes to find valid programs for longer sequences; fails on long/comparative programs and shows near-zero performance on the hardest classes (comparative-count); cannot easily incorporate phase restrictions, action-repetition avoidance, or answer-type biasing in a generic way.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper's qualitative and quantitative analysis: NSM benefits from very large beams (performance improves with beam size), but even with beam up to 50 it remains far below CIPITR on complex classes; inability to restrict to feasible actions leads to many invalid candidate programs and poor sample efficiency, explaining much of the performance gap observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3516.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3516.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KVMnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key-Value Memory Network (KVMnet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable memory-attention model that stores KB subgraph tuples as key-value memory and decodes answers by attending to and aggregating memory entries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Key-value memory networks for directly reading documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Key-Value Memory Network (KVMnet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural model that stores knowledge base facts (or document content) in a key-value memory and answers queries by computing attention over keys and producing answer distributions over values or a small vocabulary of special tokens (including integers/booleans); end-to-end differentiable and trained with supervised signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CSQA (Complex Sequential Question Answering over WikiData)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: a dataset requiring diverse multi-step logical, set, numeric and comparative reasoning over a large KB; KVMnet attempts to solve by attention and direct decoding rather than inducing programs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>End-to-end differentiable attention over key-value memory storing KB tuples; decoder directly predicts answer entries or special tokens; the authors trained a KVMnet variant on a balanced resample for count queries to control for answer-skew.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CSQA (Table 3): Per-class F1: Simple 41.40%, Logical 37.56%, Verify 27.28%, Quantitative 0.89%, Quantitative-Count 17.80%, Comparative 1.63%, Comparative-Count 9.60%; Overall (All) F1 26.67%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to CIPITR (Overall 58.92% top-beam) and NSM (Overall 10.63% top-beam), KVMnet is intermediate: better than NSM on some count numerical skewed classes but far worse than CIPITR on complex logical/comparative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable; KVMnet is a baseline. CIPITR improves substantially over KVMnet on most harder query classes (e.g., overall +32.25 points), but KVMnet achieves unexpectedly high score on one hardest class (Comparative-Count in that reported run) likely due to memorization of skewed integer answer distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>KVMnet tends to 'memorize' numeric and boolean answers from skewed distributions rather than performing compositional reasoning, leading to unreliable generalization on complex multi-step reasoning; architecture cannot easily incorporate structured symbolic constraints used by CIPITR; poor on multi-step comparative/quantitative reasoning requiring program-like composition; requires large parameter size (~6x the parameters of CIPITR/NSM in the authors' configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors noted that balancing the integer distribution in training improved fairness of comparison for count queries; still, KVMnet's architecture limits incorporation of the symbolic priors and phase decomposition that helped CIPITR. No direct KVMnet ablations reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision <em>(Rating: 2)</em></li>
                <li>Key-value memory networks for directly reading documents <em>(Rating: 2)</em></li>
                <li>Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph <em>(Rating: 2)</em></li>
                <li>Neural programmer: Inducing latent programs with gradient descent <em>(Rating: 1)</em></li>
                <li>Neural programmer-interpreters <em>(Rating: 1)</em></li>
                <li>Learning to compose neural networks for question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3516",
    "paper_id": "paper-124c17b7152a92ea8b15cbcb21eb8b697f7c99b2",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "CIPITR",
            "name_full": "Complex Imperative Program Induction from Terminal Rewards",
            "brief_description": "A neural program-induction system that generates multi-step imperative programs to execute against a large knowledge base, trained with only final-answer (terminal) rewards; it constrains program search with KB-aware feasibility, auxiliary rewards, phase-based decomposition, and beam/entropy management.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CIPITR",
            "model_description": "An end-to-end neural programmer (NPI-style) with (i) query encoder (GRU over KB-annotated tokens), (ii) environment and program RNN core, (iii) operator and argument samplers over a typed variable memory, and (iv) an interpreter that executes induced programs on a KB. Trained with REINFORCE using terminal Jaccard reward plus auxiliary typed rewards; enforces KB/schema feasibility and pragmatic programming constraints (phase change, no action repetition), and uses beam search with stochastic exploration and entropy annealing.",
            "model_size": null,
            "reasoning_task_name": "CSQA (Complex Sequential Question Answering over WikiData)",
            "reasoning_task_description": "A large-scale KBQA dataset (1.15M QA pairs) with diverse complex queries requiring logical, set, quantitative and comparative multi-step reasoning over a large KB subgraph (up to ~10-step programs; results reported per question class such as simple, logical, verify, quantitative, quantitative-count, comparative, comparative-count).",
            "method_or_intervention": "REINFORCE training with (i) auxiliary reward that gives partial credit for producing the predicted answer-type, (ii) feasibility constraints enforcing KB/schema consistency and valid variable instantiation, (iii) phase-change hierarchical decomposition (retrieval phase vs algorithm phase) to restrict operator sets at different timesteps, (iv) beam management (pruning by predicted answer type, length normalization, penalize no-op, stochastic beam exploration, entropy annealing), and (v) explicit avoidance of action repetition.",
            "performance": "On CSQA (per Table 3): Top-beam F1 by question class — Simple 96.52%, Logical 87.72%, Verify 89.43%, Quantitative 23.91%, Quantitative-Count 51.33%, Comparative 15.12%, Comparative-Count 0.33%; Overall (All) top-beam F1 58.92%. Using best over top-10 beams, Overall F1 = 73.71% and Comparative = 32.98%.",
            "baseline_performance": "Compared baselines on CSQA: KVMnet Overall F1 26.67%; NSM top-beam Overall F1 10.63% and NSM best over top-10 beams Overall F1 14.36%. Per-class baselines: e.g., Comparative: KVMnet 1.63%, NSM (best top-beam) 0.17% (NSM improves slightly with larger beam sizes but remains very low).",
            "improvement_over_baseline": "CIPITR substantially outperforms baselines on complex logical reasoning: overall F1 improvement vs KVMnet is ~32.25 percentage points (58.92 vs 26.67) on top-beam; vs NSM top-beam overall improvement ~48.29 points (58.92 vs 10.63). On the Comparative class CIPITR top-beam (15.12%) is ~9x KVMnet (1.63%) and ~89x NSM (0.17%), and with larger beams the gap remains large (e.g., CIPITR comparative top-10 = 32.98% vs NSM top-10 = 2.09%).",
            "limitations_or_failures": "Still weak on the hardest classes (comparative-count: top-beam F1 0.33% up to 1.54% with larger beams); relies on oracle gold entity/type/relation linking in experiments (robustness to linking errors not evaluated); requires careful hyperparameter tuning and beam management; limited beam size due to model complexity (≤20) compared to NSM (50) which constrains exploration; reward sparsity and high variance remain challenges despite auxiliary rewards; comparative-count numeric classes still largely unsolved.",
            "ablation_or_analysis": "Ablation on the Comparative class (original CIPITR F1 15.123%) shows removing individual components reduces performance: removing Beam Pruning =&gt; F1 10.34%, removing Auxiliary Reward =&gt; 9.85%, removing Biasing last operator =&gt; 7.52%, removing Valid variable instantiation =&gt; 3.08%, removing Phase change =&gt; 2.41%, removing Action repetition constraint =&gt; 1.68%. This indicates auxiliary rewards, beam pruning, and biasing toward predicted answer-type are among the most impactful interventions.",
            "uuid": "e3516.0",
            "source_info": {
                "paper_title": "Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "CIPITR",
            "name_full": "Complex Imperative Program Induction from Terminal Rewards",
            "brief_description": "Same NPI-style system as above, evaluated on WebQuestionsSP to induce executable programs for 1-2 hop KBQA with constraints and auxiliary supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CIPITR",
            "model_description": "See description above; applied on WebQuestionsSP using human-annotated entity/relation links as inputs and learning to generate programs that replicate human semantic parses, trained with REINFORCE plus the feasibility and auxiliary mechanisms.",
            "model_size": null,
            "reasoning_task_name": "WebQuestionsSP",
            "reasoning_task_description": "A KBQA benchmark (Freebase-based) where questions typically require up to 2-hop inference chains sometimes with temporal or other constraints; semantic parses by annotators provide oracle subgraphs for fair program induction evaluation.",
            "method_or_intervention": "Same as above (REINFORCE training with feasibility constraints, auxiliary answer-type reward, phase-change, beam management); also compared with a strong rule-based program executor derived from human semantic parses.",
            "performance": "On WebQuestionsSP test (1,639 queries) CIPITR overall F1 82.85%. Per-class (from Table 2): inference-chain-len-1 no constraint 89.09%, len-1 with constraint 79.94%, len-2 no constraint 88.69%, len-2 non-temporal constraint 63.07%, len-2 temporal constraint 48.86%.",
            "baseline_performance": "Rule-based model (derived from human parse) overall F1 81.19%; per-class: len-1 no constraint 87.34%, len-1 with constraint 93.64%, len-2 no constraint 82.85%, len-2 temporal constraint 35.63%.",
            "improvement_over_baseline": "CIPITR slightly outperforms the rule-based baseline overall (82.85% vs 81.19%). CIPITR performs better on some classes (e.g., len-2 temporal constraint 48.86% vs rule-based 35.63%), but worse on others (len-1 with constraint 79.94% vs 93.64%).",
            "limitations_or_failures": "Certain constraint types (e.g., non-temporal constraints in len-1) see lower CIPITR performance than rule-based oracle; experiments used human-annotated entity/relation links (oracle), so real-world performance could degrade with noisy linking.",
            "ablation_or_analysis": "The paper does not present an explicit per-feature ablation for WebQuestionsSP, but overall analysis and per-class comparisons indicate the auxiliary and feasibility mechanisms help especially on temporally constrained and multi-hop cases.",
            "uuid": "e3516.1",
            "source_info": {
                "paper_title": "Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "NSM",
            "name_full": "Neural Symbolic Machines",
            "brief_description": "A neural semantic-parsing framework that decodes structured program tokens (operators and memory variables) to form executable logical forms using weak supervision from question-answer pairs; uses a key-variable memory and REINFORCE-like learning.",
            "citation_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "mention_or_use": "use",
            "model_name": "Neural Symbolic Machines (NSM)",
            "model_description": "A model that translates natural language to a structured program token-by-token using sequence decoding; uses a key-variable memory to hold intermediate results and is trained with weak supervision (answer-level) using policy gradient methods. Token-by-token decoding limits incorporation of certain high-level constraints.",
            "model_size": null,
            "reasoning_task_name": "CSQA (Complex Sequential Question Answering over WikiData)",
            "reasoning_task_description": "As above: multi-hop and complex KBQA requiring logical, set, numeric, and comparative reasoning; NSM attempts to induce programs from answer-only supervision.",
            "method_or_intervention": "Token-by-token program decoding with key-variable memory; beam search (large beams up to 50 in experiments) and REINFORCE-style training (as implemented by the authors of this paper for comparison). NSM does not incorporate high-level feasibility constraints, phase decomposition, or the auxiliary answer-type reward in its standard form.",
            "performance": "On CSQA (Table 3): NSM best at top beam Overall F1 10.63%; NSM best over top-10 beams Overall F1 14.36%. Per-class example (top-beam): Simple 78.38%, Logical 35.40%, Verify 28.70%, Quantitative 4.31%, Quantitative-Count 12.38%, Comparative 0.17%, Comparative-Count 0.00%. With larger beams (top-10), some classes improve (Simple 96.78%, Logical 69.86%, Verify 60.18%, but Comparative and Comparative-Count remain near zero).",
            "baseline_performance": "Compared to CIPITR (top-beam Overall 58.92%) and KVMnet (Overall 26.67%), NSM underperforms substantially on the complex CSQA dataset, particularly on longer/more complex programs (comparative categories).",
            "improvement_over_baseline": "Not applicable (NSM is a baseline here). Compared to CIPITR, NSM is much worse: e.g., Comparative top-beam CIPITR 15.12% vs NSM 0.17% (~89x improvement by CIPITR); overall CIPITR top-beam 58.92% vs NSM 10.63% (~5.5x).",
            "limitations_or_failures": "Token-by-token decoding makes it difficult to enforce high-level syntactic and semantic constraints during generation, so NSM often generates syntactically or semantically invalid programs; requires large beam sizes to find valid programs for longer sequences; fails on long/comparative programs and shows near-zero performance on the hardest classes (comparative-count); cannot easily incorporate phase restrictions, action-repetition avoidance, or answer-type biasing in a generic way.",
            "ablation_or_analysis": "Paper's qualitative and quantitative analysis: NSM benefits from very large beams (performance improves with beam size), but even with beam up to 50 it remains far below CIPITR on complex classes; inability to restrict to feasible actions leads to many invalid candidate programs and poor sample efficiency, explaining much of the performance gap observed.",
            "uuid": "e3516.2",
            "source_info": {
                "paper_title": "Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "KVMnet",
            "name_full": "Key-Value Memory Network (KVMnet)",
            "brief_description": "A differentiable memory-attention model that stores KB subgraph tuples as key-value memory and decodes answers by attending to and aggregating memory entries.",
            "citation_title": "Key-value memory networks for directly reading documents",
            "mention_or_use": "use",
            "model_name": "Key-Value Memory Network (KVMnet)",
            "model_description": "A neural model that stores knowledge base facts (or document content) in a key-value memory and answers queries by computing attention over keys and producing answer distributions over values or a small vocabulary of special tokens (including integers/booleans); end-to-end differentiable and trained with supervised signals.",
            "model_size": null,
            "reasoning_task_name": "CSQA (Complex Sequential Question Answering over WikiData)",
            "reasoning_task_description": "As above: a dataset requiring diverse multi-step logical, set, numeric and comparative reasoning over a large KB; KVMnet attempts to solve by attention and direct decoding rather than inducing programs.",
            "method_or_intervention": "End-to-end differentiable attention over key-value memory storing KB tuples; decoder directly predicts answer entries or special tokens; the authors trained a KVMnet variant on a balanced resample for count queries to control for answer-skew.",
            "performance": "On CSQA (Table 3): Per-class F1: Simple 41.40%, Logical 37.56%, Verify 27.28%, Quantitative 0.89%, Quantitative-Count 17.80%, Comparative 1.63%, Comparative-Count 9.60%; Overall (All) F1 26.67%.",
            "baseline_performance": "Compared to CIPITR (Overall 58.92% top-beam) and NSM (Overall 10.63% top-beam), KVMnet is intermediate: better than NSM on some count numerical skewed classes but far worse than CIPITR on complex logical/comparative tasks.",
            "improvement_over_baseline": "Not applicable; KVMnet is a baseline. CIPITR improves substantially over KVMnet on most harder query classes (e.g., overall +32.25 points), but KVMnet achieves unexpectedly high score on one hardest class (Comparative-Count in that reported run) likely due to memorization of skewed integer answer distribution.",
            "limitations_or_failures": "KVMnet tends to 'memorize' numeric and boolean answers from skewed distributions rather than performing compositional reasoning, leading to unreliable generalization on complex multi-step reasoning; architecture cannot easily incorporate structured symbolic constraints used by CIPITR; poor on multi-step comparative/quantitative reasoning requiring program-like composition; requires large parameter size (~6x the parameters of CIPITR/NSM in the authors' configurations).",
            "ablation_or_analysis": "Authors noted that balancing the integer distribution in training improved fairness of comparison for count queries; still, KVMnet's architecture limits incorporation of the symbolic priors and phase decomposition that helped CIPITR. No direct KVMnet ablations reported in this paper.",
            "uuid": "e3516.3",
            "source_info": {
                "paper_title": "Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "rating": 2
        },
        {
            "paper_title": "Key-value memory networks for directly reading documents",
            "rating": 2
        },
        {
            "paper_title": "Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph",
            "rating": 2
        },
        {
            "paper_title": "Neural programmer: Inducing latent programs with gradient descent",
            "rating": 1
        },
        {
            "paper_title": "Neural programmer-interpreters",
            "rating": 1
        },
        {
            "paper_title": "Learning to compose neural networks for question answering",
            "rating": 1
        }
    ],
    "cost": 0.015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs</h1>
<p>Amrita Saha ${ }^{1}$ Ghulam Ahmed Ansari ${ }^{1}$ Abhishek Laddha ${ }^{* 1}$<br>Karthik Sankaranarayanan ${ }^{1}$ Soumen Chakrabarti ${ }^{2}$<br>${ }^{1}$ IBM Research India, ${ }^{2}$ Indian Institute of Technology Bombay<br>amrsaha4@in.ibm.com, ansarigh@in.ibm.com, laddhaabhishek11@gmail.com, kartsank@in.ibm.com, soumen@cse.iitb.ac.in</p>
<h4>Abstract</h4>
<p>Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the "gold" program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2 - to 5 -step programs, CIPITR scores at least $3 \times$ higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5-10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times. ${ }^{1}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1 Introduction</h2>
<p>Structured knowledge bases (KB) like Wikidata and Freebase can support answering questions (KBQA) over a diverse spectrum of structural complexity. This includes queries with single-hop (Obama's birthplace) (Yao, 2015; Berant et al., 2013), or multi-hop (who voiced Meg in Family Guy) (Bast and Haußmann, 2015; Yih et al., 2015; Xu et al., 2016; Guu et al., 2015; McCallum et al., 2017; Das et al., 2017), or complex queries such as "how many countries have more rivers and lakes than Brazil?" (Saha et al., 2018). Complex queries require a proper assembly of selected operators from a library of graph, set, logical, and arithmetic operations into a complex procedure, and is the subject of this paper.</p>
<p>Relatively simple query classes, in particular, in which answers are KB entities, can be served with feed-forward (Yih et al., 2015) and seq2seq (McCallum et al., 2017; Das et al., 2017) networks. However, such systems show copying or rote learning behavior when Boolean or open numeric domains are involved. More complex queries need to be evaluated as an acyclic expression graph over nodes representing KB access, set, logical, and arithmetic operators (Andreas et al., 2016a). A practical alternative to inferring a stateless expression graph is to generate an imperative sequential program to solve the query. Each step of the program selects an atomic operator and a set of previously defined variables as arguments and writes the result to scratch memory, which can then be used in subsequent steps. Such imperative programs are preferable to opaque, monolithic networks for their interpretability and generalization to diverse domains. Another</p>
<p>motivation behind opting for the program induction paradigm for solving complex tasks, such as complex question answering, is modularizing the end-to-end complex reasoning process. With this approach it is now possible to first train separate modules for each of the atomic operations involved and then train a program induction model that learns to use these separately trained models and invoke the sub-modules in the correct fashion to solve the task. These sub-modules can even be task-agnostic generic models that can be pretrained with much more extensive training data, while the program induction model learns from examples pertaining to the specific task. This paradigm of program induction has been used for decades, with rule induction and probabilistic program induction techniques in Lake et al. (2015) and by constructing algorithms utilizing formal theorem-proving techniques in Waldinger and Lee (1969). These traditional approaches (e.g., Muggleton and Raedt, 1994) incorporated domain specific knowledge about programming languages instead of applying learning techniques. More recently, to promote generalizability and reduce dependecy on domain specific knowledge, neural approaches have been applied to problems like addition, sorting, and word algebra problems (Reed and de Freitas, 2016; Bosnjak et al., 2017) as well as for manipulating a physical environment (Bunel et al., 2018).</p>
<p>Program Induction has also seen initial promise in translating simple natural language queries into programs executable in one or two hops over a KB to obtain answers (Liang et al., 2017). In contrast, many of the complex queries from Saha et al. (2018), such as the one in Figure 1, require up to 10 -step programs involving multiple relations and several arithmetic and logical operations. Sample operations include gen_set: collecting ${t:(h, r, t) \in \mathrm{KB}}$, computing set_union, counting set sizes (set_count), comparing numbers or sets, and so forth. These operations need to be executed in the correct order, with correct parameters, sharing information via intermediate results to arrive at the correct answer. Note also that the actual gold program is not available for supervision and therefore the large space of possible translation actions at each step, coupled with a large number of steps needed to get any payoff, makes the reward very sparse. This renders complex KBQA in the absence of gold programs extremely challenging.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The CIPITR framework reads a natural language query and writes a program as a sequence of actions, guided at every step by constraints posed by the KB and the answer-type. Because the space of actions is discrete, REINFORCE is used to learn the action selection by computing the reward from the output answer obtained by executing the program and the target answer, which is the only source of supervision.</p>
<h2>Main Contributions</h2>
<ul>
<li>We present "Complex Imperative Program Induction from Terminal Rewards" (CIPITR), ${ }^{2}$ an advanced Neural Program Induction (NPI) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7 , using 20 atomic operators and 9 variable types. This, to our knowledge, is the first NPI system to be trained with only the gold answer as (very distant) supervision for inducing such complex programs.</li>
<li>CIPITR reduces the combinatorial program space to only semantically correct programs by (i) incorporating symbolic constraints guided by KB schema and inferred answer type, and (ii) adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of sub-goals, thereby mitigating the sparse reward problem by considering additional auxiliary rewards in a generic, task-independent way.</li>
</ul>
<p>We evaluate CIPITR on the following two challenging tasks: (i) complex KBQA posed by the recently-published CSQA data set (Saha et al., 2018) and (ii) multi-hop KBQA in one of the more</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>popularly used KBQA data sets WebQuestionsSP (Yih et al., 2016). WebQuestionsSP involves complex multi-hop inferencing, sometimes with additional constraints, as we will describe later. However, CSQA poses a much greater challenge, with its more diverse classes of complex queries and almost 20-times larger scale. On a data set such as CSQA, contemporary models like neural symbolic machines (NSM) fail to handle exponential growth of the program search space caused by a large number of operator choices at every step of a lengthy program. Key-value memory networks (KVMnet) (Miller et al., 2016) are also unable to perform the necessary complex multi-step inference. CIPITR outperforms them both by a significant margin while avoiding exploration of unwanted program space or memorization of low-entropy answer distributions. On even moderately complex programs of length $2-5$, CIPITR scored at least $3 \times$ higher F1 than both. On one of the hardest class of programs of around 5-10 steps (i.e., comparative reasoning), CIPITR outperformed NSM by a factor of 89 and KVMnet by a factor of 9 . Further, we empirically observe that among all the competing models, CIPITR shows the best generalization across diverse program classes.</p>
<h2>2 Related Work</h2>
<p>Whereas most of the earlier efforts to handle complex KBQA did not involve writable memory, some recent systems (Miller et al., 2016; Neelakantan et al., 2015, 2016; Andreas et al., 2016b; Dong and Lapata, 2016) used end-toend differentiable neural networks. One of the state-of-the-art neural models for KBQA, the keyvalue memory network KVMnet (Miller et al., 2016) learns to answer questions by attending on the relevant KB subgraph stored in its memory. Neelakantan et al. (2016) and Pasupat and Liang (2015) support simple queries over tables, for example, of the form "find the sum of a specified column" or "list elements in a column more than a given value." The query is read by a recurrent neural network (RNN), and then, in each translation step, the column and operator are selected using the query representation and history of operators and columns selected in the past. Andreas et al. (2016b) use a "stateless" model where neural network based subroutines are assembled using syntactic parsing.</p>
<p>Recently, Reed and de Freitas (2016) took an early influential step with the NPI compositional framework that learns to decompose high level tasks like addition and sorting into program steps (carry, comparison) aided by persistent memory. It is trained by high-level task input and output as well as all the program steps. Li et al. (2016) and Bosnjak et al. (2017) took another important step forward by replacing NPI's expensive strong supervision with supervision of the programsketch. This form of supervision at every intermediate step still keeps the problem simple, by arresting the program space to a tractable size. Although such data are easy to generate for simpler problems such as arithmetic and sorting, it is expensive for KBQA. Liang et al. (2017) proposed the NSM framework in absence of the gold program, which translates the KB query to a structured program token-by-token. While being a natural approach for program induction, NSM has several inherent limitations preventing generalization towards longer programs that are critical for complex KBQA. Subsequently, it was evaluated only on WebQuestionsSP (Yih et al., 2016), that requires relatively simpler programs. We consider NSM as the primary and KVMnet as an additional baseline and show that CIPITR significantly outperforms both, especially on the more complex query types.</p>
<h2>3 Complex KBQA Problem Set-up</h2>
<h3>3.1 CSQA Data Set</h3>
<p>The CSQA data set (Saha et al., 2018) contains 1.15 M natural language questions and its corresponding gold answer from WikiData Knowledge Base. Figure 1 shows a sample query from the data set along with its true program-decomposed form, the latter not provided by CSQA. CSQA is particularly suited to study the Complex Program Induction (CPI) challenge over other KBQA data sets because:</p>
<ul>
<li>It contains large-scale training data of question-answer pairs across diverse classes of complex queries, each requiring different inference tools over large KB sub-graphs.</li>
<li>
<p>Poor state-of-the-art performance of memory networks on it motivates the need for sweeping changes to the NPI's learning strategy.</p>
</li>
<li>
<p>The massive size of the KB involved ( 13 million entities and 50 million tuples) poses a scalability challenge for prior NPI techniques.</p>
</li>
<li>Availability of KB metadata helps standardize comparisons across techniques (explained subsequently).</li>
</ul>
<p>We adapt CSQA in two ways for the CPI problem.
Removal of extended conversations: To be consistent with the NSM work on KBQA, we discard QA pairs that depend on the previous dialogue context. This is possible as every query is annotated with information on whether it is self-contained or depends on the previous context. Relevant statistics of the resulting data set are presented in Table 3.</p>
<p>Use of gold entity, type, and relation annotations to standardize comparisons: Our focus being on the reasoning aspect of the KBQA problem, we use the gold annotations of canonical KB entities, types, and relations available in the data set along with the the queries, in order to remove a prominent source of confusion in comparing KBQA systems (i.e., all systems take as inputs the natural language query, with spans identified with KB IDs of entities, types, relations, and integers). Although annotation accuracy affects a complete KBQA system, our focus here is on complex, multi-step program generation with only final answer as the distant supervision, and not entity/type/relation linking.</p>
<h3>3.2 WebQuestionsSP Data Set</h3>
<p>In Figure 2 we illustrate one of the most complex questions from the the WebQuestionsSP data set and its semantic parsed version provided by human annotator. Questions in the WebQuestionsSP data set are answerable from the Freebase KB and tyically require up to 2-hop inference chains, sometimes with additional requirements of satisfying specific constraints. These constraints can be temporal (e.g., governing_position_held_from) or non-temporal (e.g., government_office_position_ or title). The human-annotated semantic parse of the questions provide the exact structure of the subgraph and the inference process on it to reach the final answer. As in this work, we are focusing on inducing programs where the gold entity relation annotations are known; for this data set as well, we use the human-annotations to collect all
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Semantic parsed form of a sample Question from WebQuestionsSP, along with the depiction of the reasoning over the subgraph to reach the answer.
the entities and relations in the oracle subgraph associated with the query. The NPI model has to understand the role of these gold program inputs in question-answering and learn to induce a program to reflect the same inferencing.</p>
<h2>4 Complex Imperative Program Induction from Terminal Rewards</h2>
<h3>4.1 Notation</h3>
<p>This subsection introduces the different notations commonly used by our model.</p>
<p>Nine variable-types: (distinct from KB types)</p>
<ul>
<li>KB artifacts: ent(entity), rel(relation), type</li>
<li>Base data types: int, bool, None (empty argument type used for padding)</li>
<li>Composite data types: set (i.e., set of KB entities) or map_set and map_int (i.e., a mapping function from an entity to a set of KB entities or an integer)</li>
</ul>
<h2>Twenty Operators:</h2>
<ul>
<li>gen_set(ent, rel, type) $\rightarrow$ set</li>
<li>verify(ent, rel, ent) $\rightarrow$ bool</li>
<li>gen_map_set(type, rel, type) $\rightarrow$ map_set</li>
<li>map_count(map_set) $\rightarrow$ map_int</li>
<li>set_{union/ints/diff $}$ (set, set) $\rightarrow$ set</li>
<li>map_{union/ints/diff $}$ (map_set, map_set) $\rightarrow$ map_set</li>
<li>
<p>set_count(set) $\rightarrow$ int</p>
</li>
<li>
<p>select_{atleast/atmost/more/less/ equal/approx}(map_int, int) $\rightarrow$ set</p>
</li>
<li>select_{max/min}(map_int) $\rightarrow$ ent</li>
<li>no_op() (i.e., no action taken)</li>
</ul>
<p>Symbols and Hyperparameters: (typical values)</p>
<ul>
<li>num_op: Number of operators (20)</li>
<li>num_var_types: Number of variable types (9)</li>
<li>max_var: Maximum number of variables accommodated in memory for each type (3)</li>
<li>$m$ : Maximum number of arguments for an operator (None padding for fewer arguments) (3)</li>
<li>$d^{\text {key }} \&amp; d^{\text {val }}$ : Dimension of the key and value embeddings $\left(d^{\text {key }} \ll d^{\text {val }}\right)(100,300)$</li>
<li>$n_{p} \&amp; n_{v}$ : Number of operators and argument variables sampled per operator each time $(4,10)$</li>
<li>$f$ with subscript: some feed-forward network</li>
</ul>
<p>Embedding Matrices: The model is trained with a vocabulary of operators and variable-types. In order to sample operators, two matrices $M^{\text {op.key }}$ $\in \mathbb{R}^{\text {num_op } \times d^{\text {key }}}$ and $M^{\text {op.val }} \in \mathbb{R}^{\text {num_op } \times d^{\text {val }}}$ are needed for encoding the operator's key and value embedding. The key embedding is used for looking up and retrieving an entry from the operator vocabulary and the corresponding value embedding encodes the operator information. The variable type has only the value embedding $M^{\text {vtype val }} \in \mathbb{R}^{\text {num_op } \times d^{\text {val }}}$ as no lookup is needed on it.</p>
<p>Operator Prototype Matrices: These matrices store the argument variable type information for the $m$ arguments of every operator in $M^{\text {op.ary }} \in{0,1, \ldots, \text { num_var_types }}^{\text {num_op } \times m}$ and the output variable type created by it in $M^{\text {op.out }} \in{0,1, \ldots$, num_var_types $}^{\text {num_op }}$.</p>
<p>Memory Matrices: This is the query-specific scratch memory for storing new program variables as they get created by CIPITR. For each variable type, we have separate key and value embedding matrices $M^{\text {var.key }} \in \mathbb{R}^{\text {num_var_type } \times \text { max_var } \times d^{\text {key }}}$ and $M^{\text {var val }} \in \mathbb{R}^{\text {num_var_type } \times \text { max_var } \times d^{\text {val }}}$, respectively for looking up a variable in memory
and accessing the information in it. In addition, we also have a variable attention matrix $M^{\text {var.att }} \in$ $\mathbb{R}^{\text {num_var_type } \times \text { max_var }}$ which stores the attention vector over the variables declared of each type. CIPITR consists of three components:</p>
<p>The preprocessor takes the input query and the KB and performs the task of entity, relation, and type linking which acts as input to the program induction. It also pre-populates the variable memory matrices with any entity, relation, type, or integer variable directly extracted from the query.</p>
<p>The programmer model takes as input the natural language question, the KB, and the pre-populated variable memory tables to generate a program (i.e., a sequence of operators invoked with past instantiated variables as their arguments and generating new variables in memory).</p>
<p>The interpreter executes the generated program with the help of the KB and scratch memory and outputs the system answer.</p>
<p>During training, the predicted answer is compared with the gold to obtain a reward, which is sent back to CIPITR to update its model parameters through a REINFORCE (Williams, 1992) objective. In the current version of CIPITR, the preprocessor consults an oracle to link entities, types and relations in the query to the KB. This is to isolate the programming performance of CIPITR from the effect of imperfect linkage. Extending earlier studies (Karimi et al., 2012; Khalid et al., 2008) to investigate robustness of CIPITR to linkage errors may be of future interest.</p>
<h3>4.2 Basic Memory Operations in CIPITR</h3>
<p>We describe some of the foundational modules invoked by the rest of CIPITR.</p>
<p>Memory Lookup: The memory lookup looks up scratch memory with a given probe, say $x$ (of arbitrary dimension), and retrieves the memory entry having closest key embedding to $x$. It first passes $x$ through a feed-forward layer to transform its dimension to key embedding dimension $x_{-} k e y$. Then, by computing softmax over the matrix multiplication of $M^{x \text {.key }}$ and $x^{\text {key }}$, the distribution over the memory variables for lookup is obtained.</p>
<p>$$
x^{\text {key }}=f(x), x^{\text {dist }}=\operatorname{softmax}\left(M^{x, \text { key }} x^{k e y}\right)
$$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: CIPITR Control Flow (with $n_{p}=1 \&amp; n_{v}=1$ for simplicity) depicting the order in which different modules (see Section 4) are invoked. A corresponding example execution trace of the CIPITR algorithm is given in Figure 4.</p>
<p>Feasibility Sampling: To restrict the search space to meaningful programs, CIPITR incorporates both high-level generic or task-specific constraints when sampling any action. The generic constraints can help it adopt more pragmatic programming styles like not repeating lines of code or avoiding syntactical errors. The task specific constraints ensure that the generated program is consistent as per the KB schema or on execution gives an answer of the desired variable type. To sample from the feasible subset using these constraints, the input sampling distribution, $x^{\text {dist }}$, is elementwise transformed by a feasibility vector $x^{\text {feas }}$ followed by a L1-normalization. Along with the transformed distribution, the top- $k$ entries $x^{\text {sampled }}$ is also returned.</p>
<h2>Algorithm 1 Feasibility Sampling</h2>
<p>Input:</p>
<ul>
<li>$x^{d i s t} \in \mathbb{R}^{N}$ (where $N$ is the size of the population set over which lookup needs to be done)</li>
<li>$x^{\text {feas }} \in{0,1}^{N}$ (boolean feasibility vector)</li>
<li>$k$ (top- $k$ sampled)</li>
</ul>
<p>Procedure: FeasSampling ( $x^{\text {dist }}, x^{\text {feas }}, k$ )
$x^{\text {dist }}=x^{\text {dist }} \odot x^{\text {feas }}$ (elementwise multiply)
$x^{\text {dist }}=\mathrm{L} 1$-Normalized $\left(x^{\text {dist }}\right)$
$x^{\text {sampled }}=k$-argmax $\left(x^{\text {dist }}\right)$
Output: $x^{\text {dist }}, x^{\text {sampled }}$</p>
<p>Writing a new variable to memory: This operation takes a newly generated variable, say $x$, of type $x^{\text {type }}$ and adds its key and value embedding
to the row corresponding to $x^{\text {type }}$ in the memory matrices. Further, it updates the attention vector for $x^{\text {type }}$ to provide maximum weight to the newest variable generated, thus, emulating a stack like behavior.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Write</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">memory</span>
<span class="n">Input</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y</span><span class="p">},</span><span class="w"> </span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">l</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">key</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">embedding</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span>\<span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">type</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">scalar</span><span class="w"> </span><span class="n">denoting</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span>\<span class="p">)</span>
<span class="n">Procedure</span><span class="p">:</span><span class="w"> </span><span class="n">WriteVarToMem</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y</span><span class="p">},</span><span class="w"> </span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">l</span><span class="p">},</span><span class="w"> </span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">e</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">i</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span>\<span class="p">(</span><span class="mi">1</span><span class="o">^</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">st</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">slot</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">row</span><span class="w"> </span>\<span class="p">(</span><span class="n">M</span><span class="o">^</span><span class="p">{</span><span class="n">x</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y</span><span class="p">}</span>\<span class="n">left</span><span class="p">[</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">e</span><span class="p">},:</span>\<span class="n">right</span><span class="p">]</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">M</span><span class="o">^</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y</span><span class="p">}</span>\<span class="n">left</span><span class="p">[</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">e</span><span class="p">},</span><span class="w"> </span><span class="n">i</span>\<span class="n">right</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">M</span><span class="o">^</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">l</span><span class="p">}</span>\<span class="n">left</span><span class="p">[</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">e</span><span class="p">},</span><span class="w"> </span><span class="n">i</span>\<span class="n">right</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">l</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">M</span><span class="o">^</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="n">t</span><span class="p">}</span>\<span class="n">left</span><span class="p">[</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">e</span><span class="p">},:</span>\<span class="n">right</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="o">^</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">val</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">Normalized</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">M</span><span class="o">^</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="n">t</span><span class="p">}</span>\<span class="n">left</span><span class="p">[</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">t</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">e</span><span class="p">},:</span>\<span class="n">right</span><span class="p">]</span>\<span class="n">right</span><span class="o">.</span>\<span class="p">)</span>
<span class="w">                                    </span>\<span class="p">(</span>\<span class="n">left</span><span class="o">.+</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">One</span><span class="o">-</span><span class="n">Hot</span><span class="w"> </span><span class="p">}(</span><span class="n">i</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
</code></pre></div>

<h3>4.3 CIPITR Architecture</h3>
<p>In Figure 3, we sketch the CIPITR components; in this section we describe them in the order they appear in the model.</p>
<p>Query Encoder: The query is first parsed into a sequence of KB-entities and non-KB words. KB entities $e$ are embedded with the concatenated vector $[\operatorname{TransE}(e), \mathbf{0}]$ using Bordes et al. (2013), and non-KB words $\omega$ with $[\mathbf{0}, \operatorname{GloVe}(\omega)]$. The final query representation is obtained from a GRU encoder as $q$.</p>
<p>NPI Core: The query representation $q$ is fed at the initial timestep to an environment encoding</p>
<p>RNN, which gives out the environment state $e_{t}$ at every timestep. This, along with the value embedding $u_{t-1}^{\text {val }}$ of the last output variable generated by the NPI engine, is fed at every timestep into another RNN that finally outputs the program state $h_{t} . h_{t}$ is then fed into the successive modules of the program induction engine as described below. The 'OutVarGen' algorithm describes how to obtain $u_{t-1}^{\text {val }}$.</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Procedure:</span><span class="w"> </span><span class="nf">NPI</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">Core</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="nv">e_</span><span class="err">{</span><span class="nv">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">h_</span><span class="err">{</span><span class="nv">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">u_</span><span class="err">{</span><span class="nv">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">val</span><span class="w"> </span><span class="err">}}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\(</span><span class="nf">e_</span><span class="err">{</span><span class="nv">t</span><span class="err">}</span><span class="o">=</span><span class="nv">G</span><span class="w"> </span><span class="nv">R</span><span class="w"> </span><span class="nv">U</span><span class="err">\</span><span class="nv">left</span><span class="p">(</span><span class="nv">e_</span><span class="err">{</span><span class="nv">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">u_</span><span class="err">{</span><span class="nv">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">val</span><span class="w"> </span><span class="err">}}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\(</span><span class="nf">h_</span><span class="err">{</span><span class="nv">t</span><span class="err">}</span><span class="o">=</span><span class="nv">G</span><span class="w"> </span><span class="nv">R</span><span class="w"> </span><span class="nv">U</span><span class="err">\</span><span class="nv">left</span><span class="p">(</span><span class="nv">e_</span><span class="err">{</span><span class="nv">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">u_</span><span class="err">{</span><span class="nv">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">val</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">h_</span><span class="err">{</span><span class="nv">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="nl">Output:</span><span class="w"> </span><span class="err">\(</span><span class="nf">e_</span><span class="err">{</span><span class="nv">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">h_</span><span class="err">{</span><span class="nv">t</span><span class="err">}\</span><span class="p">)</span>
</code></pre></div>

<p>Operator Sampler: It takes the program state $h_{t}$, a Boolean vector $p_{t}^{\text {feas }}$ denoting operator feasibility, and the number of operators to sample $n_{p}$. It passes $h_{t}$ through the Lookup operation followed by Feasibility Sampling to obtain the top- $n_{p}$ operations $\left(P_{t}\right)$.</p>
<p>Argument Variable Sampler: For each sampled operator $p$, it takes: (i) program state $h_{t}$, (ii) the list of variable types $V_{p}^{\text {type }}$ of the $m$ arguments obtained by looking up the operator prototype matrix $M^{\text {op.arg }}$, and (iii) a Boolean vector $V_{p}^{\text {feas }}$ that indicates the valid variable configurations for the $m$-tuple arguments of the operator $p$. For each of the $m$ arguments, a feed-forward network $f_{\text {vtype }}$ first transforms the program state $h_{t}$ to a vector in $\mathbb{R}^{\text {max_var }}$. It is then element-wise multiplied with the current attention state over the variables in memory of that type. This provides the program-state-specific attention over variables $v_{p, j}^{\text {att }}$ which is then passed through the Lookup function to obtain the distribution over the variables in memory. Next, feasibility sampling is applied over the joint distribution of its argument variables, comprised of the $m$ individual distributions. This provides the top- $n_{v}$ tuples of $m$-variable instantiations $V_{p}$.</p>
<p>Output Variable Generator: The new variable $u_{p}$ of type $u_{p}^{\text {type }}=M^{\text {op,ont }}[p]$ is generated by the procedure OutVarGen by invoking a sampled operator $p$ with $m$ variables $v_{p, 1} \cdots v_{p, m}$ of type $v_{p, 1}^{\text {type }} \cdots v_{p, m}^{\text {type }}$ as arguments. This also requires generating the key and value embedding, which are both obtained by applying different feedforward layers over the concatenated representation of the value embedding of the operator $M^{\text {op.val }}[p]$, argument types $\left(M^{\text {vtype.val }}\left[v_{p, 1}^{\text {type }}\right] \cdots\right.$
$\left.M^{\text {vtype.val }}\left[v_{p, m}^{\text {type }}\right]\right)$ and the instantiated variables $\left(M^{\text {var.val }}\left[v_{p, 1}^{\text {type }}, v_{p, 1}\right] \cdots M^{\text {var.val }}\left[v_{p, m}^{\text {type }}, v_{p, m}\right]\right)$. The newly generated variable is then written to memory using Algorithm WriteVarToMem.</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Procedure:</span><span class="w"> </span><span class="err">\(\</span><span class="nf">operatorname</span><span class="err">{</span><span class="nv">ArgVarSampler</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="nv">h_</span><span class="err">{</span><span class="nv">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">type</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">feas</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">n_</span><span class="err">{</span><span class="nv">v</span><span class="err">}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nf">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nv">j</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nv">cdots</span><span class="p">,</span><span class="w"> </span><span class="nv">m</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">do</span>
<span class="w">        </span><span class="err">\(</span><span class="nf">v_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="nv">j</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">att</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">softmax</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="nv">M</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">var.att</span><span class="w"> </span><span class="err">}}\</span><span class="nv">left</span><span class="p">(</span><span class="nv">V_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="nv">j</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">type</span><span class="w"> </span><span class="err">}}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nv">odot</span><span class="w"> </span><span class="nv">f_</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">vtype</span><span class="w"> </span><span class="err">}}\</span><span class="nv">left</span><span class="p">(</span><span class="nv">h_</span><span class="err">{</span><span class="nv">t</span><span class="err">}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\(</span><span class="nf">v_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="nv">j</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">dist</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">Lookup</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="nv">v_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="nv">j</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">att</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">f_</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">var</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">M</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">var.key</span><span class="w"> </span><span class="err">}}\</span><span class="nv">left</span><span class="p">[</span><span class="nv">V_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="nv">j</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">type</span><span class="w"> </span><span class="err">}}\</span><span class="nv">right</span><span class="p">]</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\(</span><span class="nf">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">dist</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="nv">v_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">dist</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="nv">times</span><span class="w"> </span><span class="nv">v_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">dist</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="nv">cdots</span><span class="w"> </span><span class="err">\</span><span class="nv">times</span><span class="w"> </span><span class="nv">v_</span><span class="err">{</span><span class="nv">p</span><span class="p">,</span><span class="w"> </span><span class="nv">m</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">dist</span><span class="w"> </span><span class="err">}}</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="nv">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">Joint</span><span class="w"> </span><span class="nv">Distribution</span>
<span class="w">    </span><span class="err">\(</span><span class="nf">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">dist</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">FeasSampling</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="nv">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">dist</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">feas</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="nv">n_</span><span class="err">{</span><span class="nv">v</span><span class="err">}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="nl">Output:</span><span class="w"> </span><span class="err">\(</span><span class="nf">V_</span><span class="err">{</span><span class="nv">p</span><span class="err">}\</span><span class="p">)</span>
</code></pre></div>

<p>End-to-End CIPITR training: CIPITR takes a natural language query and generates an output program in a number of steps. A program is composed of actions, which are operators applied over variables (as in Figure 3). In each step, it selects an operator and a set of previously defined variables as its arguments, and writes the operator output to a dynamic memory, to be subsequently used for further search of next actions. To reduce exposure bias (Ranzato et al., 2015), CIPITR uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. Algorithm 3 shows the pseudocode of the program induction algorithm (with beam size $b$ as 1 for simplicity), which goes over $T$ time steps, each time sampling $n_{p}$ feasible operators conditional to the program state. Then, for each of the $n_{p}$ operators, it samples $n_{v}$ feasible</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">CIPITR</span><span class="w"> </span><span class="n">pseudo</span><span class="o">-</span><span class="n">code</span><span class="w"> </span><span class="p">(</span><span class="n">beam</span><span class="w"> </span><span class="k">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Query</span><span class="w"> </span><span class="nl">Encoding</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">G</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="n">U</span><span class="p">(</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Query</span><span class="w"> </span><span class="err">\</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
<span class="nl">Initialization</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">e_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="o">=</span><span class="n">f</span><span class="p">(</span><span class="n">q</span><span class="p">),</span><span class="w"> </span><span class="n">A</span><span class="o">=</span><span class="err">[]\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">cdots</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">feas</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">FeasibleOp</span><span class="err">}</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">P_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">OperatorSampler</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">feas</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">n_</span><span class="err">{</span><span class="n">p</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="err">\{</span><span class="w"> </span><span class="err">\}\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">p</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="n">P_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">type</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="o">[</span><span class="n">v_{p, 1}^{\text {type }}, \cdots, v_{p, m}^{\text {type }}\right</span><span class="o">]=</span><span class="n">M</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">op</span><span class="p">.</span><span class="n">arg</span><span class="w"> </span><span class="err">}}</span><span class="o">[</span><span class="n">p</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">feas</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">FeasibleVar</span><span class="err">}</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">ArgVarSampler</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">h_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">type</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">feas</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">n_</span><span class="err">{</span><span class="n">v</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">V</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="w"> </span><span class="err">\</span><span class="n">bigcup</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">type</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">type</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="o">=</span><span class="err">\</span><span class="n">arg</span><span class="w"> </span><span class="err">\</span><span class="nf">max</span><span class="w"> </span><span class="p">(</span><span class="n">C</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">u_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">u_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">val</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">u_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">type</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">OutVarGen</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">V_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">type</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="n">WriteVarToMem</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">u_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">u_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">val</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">u_</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">type</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">e_</span><span class="err">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">NPICore</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">e_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">h_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">append</span><span class="err">}</span><span class="p">((</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="p">))</span><span class="err">\</span><span class="p">)</span>
<span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">A</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>variable instantiations, resulting in a total of $n_{p} * n_{v}$ candidates out of which $b$ most-likely actions are sampled for the $b$ beams and the corresponding newly generated variables written into memory. This way the algorithm progresses to finally output $b$ candidate programs, each of which will feed the model back with some reward. Finally, in order to learn from the discrete action samples, the REINFORCE objective (Williams, 1992) is used. Because of lack of space, we do not provide the equation for REINFORCE, but our objective formulation remains very similar to that in Liang et al. (2017). We next describe several learning challenges that arise in the context of this overall architecture.</p>
<h2>5 Mitigating Large Program Space and Sparse Reward</h2>
<p>Handling complex queries by expanding the operator set and generating longer programs blows up the program space to a huge size of (num_op * $\left.\left(\max _v a r\right)^{m}\right)^{T}$. This, in absence of gold programs, poses serious training challenges for the programmer. Additionally, whereas the relatively simple NSM architecture could explore a large beam size (50-100), the complex architecture of CIPITR entailed by the CPI problem could only afford to operate with a smaller beam size $(\leq 20)$, which further exacerbates the sparsity of the reward space. For example, for integer answers, only a single point in the integer space returns a positive reward, without any notion of partial reward. Such a delayed-indeed, terminalreward causes high variance, instability, and local minima issues. A problem as complex as ours requires not only generic constraints for producing semantically correct programs, but also incorporation of prior knowledge, if the model permits. We now describe how to guide CIPITR more efficiently through such a challenging environment using both generic and task-specific constraints.</p>
<p>Phase change network: For complex real-word problems, the reinforcement learning community has proposed various task-abstractions (Parr and Russell, 1998; Dietterich, 2000; Bakker and Schmidhuber, 2004; Barto and Mahadevan, 2003; Sutton et al., 1999) to address the curse of dimensionality in exponential action spaces. HAMs, proposed by Parr and Russell (1998), is one such important form of abstraction aimed at restricting
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An example of a CIPITR execution trace depicting the internals of memory and action sampling to generate the program: $(A=$ gen_set(Brazil, flow, river), $B=$ set_count $(A))$.
the realizable action sequences. Inspired by HAMs, we decompose the program synthesis into phases having restricted action spaces. The first phase (retrieval phase) constitutes gathering the information from the preprocessed input variables only (i.e., KB entities, relations, types, integers). This restricts the feasible operator set to gen_set, gen_map_set, and verify. In the second phase (algorithm phase) the model is allowed to operate on all the generated variables in order to reach the answer. The programmer learns whether to switch from the first phase to the second at any timestep $t$, based on parameter $\phi_{t}\left(\phi_{t}=1\right.$ indicating</p>
<p>change of phase, where $\phi_{0}=0$ ) which is obtained as $\phi_{t}=\mathbf{1}\left{\max \left(\operatorname{sigmoid}\left(f\left(h_{t}\right)\right), \phi_{t-1}\right) \geq\right.\right.$ $\left.\left.\phi_{\text {thresh }}\right}$ if $t&lt;T / 2$, else $1(T$ being total timesteps and $\phi_{\text {thresh }}$ is set to 0.8 in our experiments). The motivation behind this is similar to the multi-staged techniques that have been adopted in order to make QA tasks more tractable, as in Yih et al. (2015) and Iyyer et al. (2017). In contrast, here we further allow the model to learn when to switch from one stage to the next. Note that this is a generic characteristic, as for every task, this kind of phase division is possible.</p>
<p>Generating semantically correct programs: Other than the generic syntactical and semantic rules, the NPI paradigm also allows us to leverage prior knowledge and to incorporate task-specific symbolic constraints in the program representation learning in an end-to-end differentiable way.</p>
<ul>
<li>Enforcing KB consistency: Operators used in the retrieval phase (described above) must honor the KB-imposed constraints, so as not to initialize variables that are inconsistent with respect to the KB. For example, a set variable assigned from gen.set is considered valid only when the ent, rel, type arguments to gen.set are consistent with the KB.</li>
<li>Biasing the last operator using answer type predictor: Answer type prediction is a standard preprocessing step in question answering (Li and Roth, 2002). For this we use a rule-based predictor that has $98 \%$ accuracy. The predicted answer type helps in directing the program search toward the correct answer type by biasing the sampling towards feasible operators that can produce the desired answer type.</li>
<li>Auxiliary reward strategy: Jaccard scores of the executed program's output and the gold answer set is used as reward. An invalid program gets a reward of -1 . Further, to mitigate the sparsity of the extrinsic rewards, an additional auxiliary feedback is designed to reward the model on generating an answer of the predicted answer-type. A linear decay makes the effect of auxiliary reward vanish eventually. Such a curriculum learning mechanism, while being particularly useful for the more complex queries, is still
quite generic as it does not require any additional task-specific prior knowledge.</li>
</ul>
<h2>Beam Management and Action Sampling</h2>
<ul>
<li>Pruning beams by target answer type: Penalize beams that terminate with an answer type not matching the predicted answer type.</li>
<li>Length-based normalization of beam scores: To counteract the characteristic of beam search favoring shorter beams as more probable and to ensure the scoring is fair to the longer beams, we normalize the beam scores with respect to their length.</li>
<li>Penalizing beams for no..op operators: Another way of biasing the beams toward generating longer sequences, is by penalizing for the number of times a beam takes no..op as the action. Specifically, we reduce the beam score by a hyperparameter-controlled logarithmic factor of the number of no..op actions taken till now.</li>
<li>Stochastic beam exploration with entropy annealing: To avoid early local minima where the model severely biases towards specific actions, we added techniques like (i) a stochastic version of beam search to sample operators in an $\epsilon$-greedy fashion (ii) dropout, and (iii) entropy-based regularization of action distribution.</li>
</ul>
<p>Sampling only feasible actions: Sampling a feasible action requires first sampling a feasible operator and then its feasible variable arguments:</p>
<ul>
<li>The operator must be allowed in the current phase of the model's program induction.</li>
<li>Valid Variable instantiation: A feasible operator should be having at least one valid instantiation of its formal arguments with non-empty variable values that are also consistent with the KB.</li>
<li>Action Repetition: An action (i.e., an operator invoked with a specific argument instantiation) should not be repeated at any time step.</li>
<li>Some operators disallow some arguments; for example, union or intersection of a set with itself.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Simple</th>
<th style="text-align: center;">Logical</th>
<th style="text-align: center;">Verify</th>
<th style="text-align: center;">Quanti</th>
<th style="text-align: center;">Quant <br> Count</th>
<th style="text-align: center;">Comp</th>
<th style="text-align: center;">Comp <br> Count</th>
<th style="text-align: center;">WebQSP <br> All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Timesteps</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3 to 5</td>
</tr>
<tr>
<td style="text-align: left;">Entropy-Loss Wt.</td>
<td style="text-align: center;">$5 \mathrm{e}^{-4}$</td>
<td style="text-align: center;">$5 \mathrm{e}^{-4}$</td>
<td style="text-align: center;">$5 \mathrm{e}^{-6}$</td>
<td style="text-align: center;">$5 \mathrm{e}^{-3}$</td>
<td style="text-align: center;">$5 \mathrm{e}^{-3}$</td>
<td style="text-align: center;">$5 \mathrm{e}^{-2}$</td>
<td style="text-align: center;">$5 \mathrm{e}^{-2}$</td>
<td style="text-align: center;">$5 \mathrm{e}^{-3}$</td>
</tr>
<tr>
<td style="text-align: left;">Feasible Program <br> after iterations</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1300</td>
<td style="text-align: center;">1300</td>
<td style="text-align: center;">1500</td>
<td style="text-align: center;">1500</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Beam Pruning after <br> iterations</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1300</td>
<td style="text-align: center;">1300</td>
<td style="text-align: center;">1300</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Auxillary Reward <br> till iterations</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$1 \mathrm{e}^{-5}$</td>
<td style="text-align: center;">$1 \mathrm{e}^{-5}$</td>
<td style="text-align: center;">$1 \mathrm{e}^{-5}$</td>
<td style="text-align: center;">$1 \mathrm{e}^{-5}$</td>
<td style="text-align: center;">$1 \mathrm{e}^{-5}$</td>
<td style="text-align: center;">$1 \mathrm{e}^{-5}$</td>
<td style="text-align: center;">$1 \mathrm{e}^{-5}$</td>
<td style="text-align: center;">$1 \mathrm{e}^{-4}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Critical hyperparameters.</p>
<h2>6 Experiments</h2>
<p>We compare CIPITR against baselines (Miller et al., 2016; Liang et al., 2017) on complex KBQA and further identify the contributions of the ideas presented in Section 5 via ablation studies. For this work, we limit our effort on KBQA to the setting where the query is annotated with the gold KB-artifacts, which standardizes the input to the program induction for the competing models.</p>
<h3>6.1 Hyperparameters Settings</h3>
<p>We trained our model using the Adam Optimizer and tuned all hyperparameters on the validation set. Some parameters are selectively turned on/ off after few training iterations, which is itself a hyperparameter (see Table 1). We combined reward/ loss such as entropy annealing and auxiliary rewards using different weights detailed in Table 1. The key, value embedding dimensions are set to 100,300 .</p>
<h3>6.2 WebQuestionsSP Data Set</h3>
<p>We first evaluate our model on the more popularly used WebQuestionsSP data set.</p>
<h3>6.2.1 Rule-Based Model on WebQuestionsSP</h3>
<p>Though quite a few recent works on KBQA have evaluated their model on WebQuestionsSP, the reported performance is always in a setting where the gold entities/relations are not known. They either internally handle the entity and relationlinking problem or outsource it to some external or in-house model, which itself might have been trained with additional data. Additionally, the entity/relation linker outputs used by these models are also not made public, making it difficult to set up a fair ground for evaluating the program induction model, especially because we are interested in the program induction given the program inputs
and handling the entity/relation linking is beyond the scope of this work. To avoid these issues, we use the human-annotated entity/relation linking data available along with the questions as input to the program induction model. Consequently the performance reported here is not comparable to the previous works evaluated on this data set, as the query annotation is obtained here from an oracle linker.</p>
<p>Further, to gauge the proficiency of the proposed program induction model, we construct a rule-based model which is aware of the human annotated semantic parsed form of the query-that is, the inference chain of relations and the exact constraints that need to be additionally applied to reach the answer. The pseudocode below elaborates how the rule based model works on the human-annotated parse of the given query, taking as input the central entity, the inference chain, and associated constraints and their type. This</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Procedure:</span><span class="w"> </span><span class="nf">RuleBasedModel</span><span class="p">(</span><span class="nv">parse</span><span class="p">,</span><span class="w"> </span><span class="nv">KB</span><span class="p">)</span>
<span class="w">    </span><span class="nf">ent</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">parse</span><span class="p">[</span><span class="s">&#39;TopicEntityMid&#39;</span><span class="p">]</span>
<span class="w">    </span><span class="nf">rel</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nv">_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nv">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">parse</span><span class="p">[</span><span class="s">&#39;InferentialChain&#39;</span><span class="w"> </span><span class="err">\</span><span class="p">(][</span><span class="mi">0</span><span class="p">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nf">ans</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="err">\</span><span class="nv">left</span><span class="err">\{</span><span class="nv">x</span><span class="w"> </span><span class="err">\</span><span class="nv">mid</span><span class="err">\</span><span class="nv">right.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nv">ent</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">left._</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">rel</span><span class="err">}</span><span class="nv">_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">x</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="nv">K</span><span class="w"> </span><span class="nv">B</span><span class="err">\}\</span><span class="p">)</span>
<span class="w">    </span><span class="nf">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nv">c</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">parse</span><span class="p">[</span><span class="s">&#39;Constraints&#39;</span><span class="p">]</span>
<span class="w">        </span><span class="nf">c_rel</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="w"> </span><span class="nv">c</span><span class="err">\</span><span class="nv">left</span><span class="p">[</span><span class="err">\</span><span class="nv">right.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">NodePredicate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">^</span><span class="err">{\</span><span class="nv">prime</span><span class="err">}\</span><span class="nv">right</span><span class="p">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nf">c_op</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="w"> </span><span class="nv">c</span><span class="err">\</span><span class="nv">left</span><span class="p">[</span><span class="err">\</span><span class="nv">right.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">Operator</span><span class="err">&#39;</span><span class="w"> </span><span class="err">\</span><span class="p">(]</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nf">c_arg</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="w"> </span><span class="nv">c</span><span class="err">\</span><span class="nv">left</span><span class="p">[</span><span class="err">\</span><span class="nv">right.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">Argument</span><span class="err">&#39;</span><span class="w"> </span><span class="err">\</span><span class="p">(]</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nf">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nv">c</span><span class="err">\</span><span class="nv">left</span><span class="p">[</span><span class="err">\</span><span class="nv">right.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">ArgumentType</span><span class="s">&#39; \(]==\right.\) &#39;</span><span class="nv">Entity</span><span class="err">&#39;</span>
<span class="w">            </span><span class="nf">ans</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">ans</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">cap</span><span class="err">\</span><span class="nv">left</span><span class="err">\{</span><span class="nv">x</span><span class="w"> </span><span class="err">\</span><span class="nv">mid</span><span class="err">\</span><span class="nv">left</span><span class="p">(</span><span class="nv">c_</span><span class="err">{</span><span class="o">-</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">arg</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">c_</span><span class="err">{</span><span class="o">-</span><span class="err">}</span><span class="w"> </span><span class="nv">r</span><span class="w"> </span><span class="nv">e</span><span class="w"> </span><span class="nv">l</span><span class="p">,</span><span class="w"> </span><span class="nv">x</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="nv">K</span><span class="w"> </span><span class="nv">B</span><span class="err">\</span><span class="nv">right</span><span class="err">\}\</span><span class="p">)</span>
<span class="w">            </span><span class="nf">else</span>
<span class="w">                </span><span class="nf">ans</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nv">bigcup_</span><span class="err">{</span><span class="nv">x</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">n</span><span class="w"> </span><span class="nv">s</span><span class="err">}\{</span><span class="nv">x</span><span class="w"> </span><span class="err">\</span><span class="nv">mid</span><span class="p">(</span><span class="nv">x</span><span class="p">,</span><span class="w"> </span><span class="nv">c</span><span class="w"> </span><span class="err">\</span><span class="nv">_r</span><span class="w"> </span><span class="nv">e</span><span class="w"> </span><span class="nv">l</span><span class="p">,</span><span class="w"> </span><span class="nv">y</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="nv">K</span><span class="w"> </span><span class="nv">B</span><span class="err">\</span><span class="p">),</span>
<span class="w">    </span><span class="err">\(</span><span class="nf">c_</span><span class="err">{</span><span class="o">-</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nv">arg</span><span class="w"> </span><span class="err">\</span><span class="nv">geq_</span><span class="err">{</span><span class="nv">c_</span><span class="err">{</span><span class="o">-</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nv">text</span><span class="w"> </span><span class="err">{</span><span class="nv">op</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="nv">y</span><span class="err">\}\</span><span class="p">)</span>
<span class="w">    </span><span class="nf">if</span><span class="w"> </span><span class="nv">len</span><span class="p">(</span><span class="nv">parse</span><span class="p">[</span><span class="s">&#39;InferentialChain&#39;</span><span class="p">])</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">&gt;</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\(\</span><span class="nf">operatorname</span><span class="err">{</span><span class="nv">rel</span><span class="err">}</span><span class="nv">_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nv">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">parse</span><span class="p">[</span><span class="s">&#39;InferentialChain&#39;</span><span class="w"> </span><span class="err">\</span><span class="p">(][</span><span class="mi">1</span><span class="p">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nf">ans</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nv">underset</span><span class="err">{</span><span class="nv">x</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">n</span><span class="w"> </span><span class="nv">s</span><span class="err">}{\</span><span class="nv">bigcup</span><span class="err">}\</span><span class="nv">left</span><span class="err">\{</span><span class="nv">y</span><span class="w"> </span><span class="err">\</span><span class="nv">mid</span><span class="err">\</span><span class="nv">left</span><span class="p">(</span><span class="nv">x</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">rel</span><span class="err">}</span><span class="nv">_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">y</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="nv">K</span><span class="w"> </span><span class="nv">B</span><span class="err">\</span><span class="nv">right</span><span class="err">\}\</span><span class="p">)</span>
</code></pre></div>

<p>Output: ans</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question Type</th>
<th style="text-align: center;">Rule <br> Based</th>
<th style="text-align: center;">CIPITR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Inference-chain-len-1, no constraint</td>
<td style="text-align: center;">87.34</td>
<td style="text-align: center;">89.09</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len-1 with constraint</td>
<td style="text-align: center;">93.64</td>
<td style="text-align: center;">79.94</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len-2, no constraint</td>
<td style="text-align: center;">82.85</td>
<td style="text-align: center;">88.69</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len-2, with nontemporal constraint</td>
<td style="text-align: center;">61.26</td>
<td style="text-align: center;">63.07</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len-2, with temporal constraint</td>
<td style="text-align: center;">35.63</td>
<td style="text-align: center;">48.86</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">81.19</td>
<td style="text-align: center;">82.85</td>
</tr>
</tbody>
</table>
<p>Table 2: F1 scores(\%) of CIPITR and rule-based model(as in Sec.6.2.1) on WebQuestionsSP test set having 1,639 queries.
inference rule, manually derived, can be written out in a program form, which on execution will give the final answer. On the other hand, the task of CIPITR is to actually learn the program by looking at training examples of the query and corresponding answer. Both the models need to induce the program using the gold entity/relation data. Subsequently, the rule-based model is indeed a very strong competitor as it is generated by annotators having detailed knowledge about the KB.</p>
<h3>6.2.2 Results on WebQuestionsSP</h3>
<p>A comparative performance analysis of the proposed CIPITR model, the rule-based model and the SparQL executor is tabulated in Table 2. The main take-away from these results is that CIPITR is indeed able to learn the rules behind the multi-step inference process simply from the distance supervision provided by the questionanswer pairs and even perform slightly better in some of the query classes.</p>
<h3>6.3 CSQA Data Set</h3>
<p>We now showcase the performance of the proposed models and related baselines on the CSQA data set.</p>
<h3>6.3.1 Baselines on CSQA</h3>
<p>KVMnet with decoder (2016), which performed best on CSQA data set (Saha et al., 2018) (as discussed in Section 2), learns to attend on a KB subgraph in memory and decode the attention over memory-entries as their likelihood of being in the answer. Further, it can also decode a vocabulary of non-KB words like integers or booleans. However, because of the inherent architectural constraints, it is not possible to incorporate most of the symbolic constraints presented in Section 5 in this model, other than KB-guided consistency
and biasing towards answer-type. More importantly, recently the usage of these models have been criticized for numerical and boolean question answering as these deep networks can easily memorize answers without "understanding" the logic behind the queries simply because of the skew in the answer distribution. In our case this effect is more pronounced as CSQA evinces a curious skew in integer answers to "count" queries. Fifty-six percent of training and $52 \%$ of test count-queries have single digit answers. Ninety percent of training and $81 \%$ of test count-queries have answers less than 200. Though this makes it unfair to compare NPI models (that are oblivious to the answer vocabulary) with KVMnet on such queries, we still train a KVMnet version on a balanced resample of CSQA, where, for only the count queries, the answer distribution over integers has been made uniform.</p>
<p>NSM (2017) uses a key-variable memory and decodes the program as a sequence of operators and memory variables. As the NSM code was not available, we implemented it and further incorporated most of the six techniques presented in Table 4. However, constraints like action repetition, biasing last operator selection, and phase change cannot be incorporated in NSM while keeping the model generic, as it decodes the program token by token.</p>
<h3>6.3.2 Results on CSQA</h3>
<p>In Table 3 we compare the F1 scores obtained by our system, CIPITR, against the KVMnet and NSM baselines. For NSM and CIPITR, we train seven models with different hyperparameters tuned on each of the seven question types. For the train and valid splits, a rule-based query type classifier with $97 \%$ accuracy was used to bucket queries into the classes listed in Table 3. For each of these three systems, we also train and evaluate</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\downarrow$ Run name $\backslash$ Question type $\rightarrow$</th>
<th style="text-align: center;">Simple</th>
<th style="text-align: center;">Logical</th>
<th style="text-align: center;">Verify</th>
<th style="text-align: center;">Quanti.</th>
<th style="text-align: center;">Quant <br> Count</th>
<th style="text-align: center;">Compar.</th>
<th style="text-align: center;">Comp <br> Count</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training Size Stats.</td>
<td style="text-align: center;">462 K</td>
<td style="text-align: center;">93 K</td>
<td style="text-align: center;">43 K</td>
<td style="text-align: center;">99 K</td>
<td style="text-align: center;">122 K</td>
<td style="text-align: center;">41 K</td>
<td style="text-align: center;">42 K</td>
<td style="text-align: center;">904 K</td>
</tr>
<tr>
<td style="text-align: left;">Test Size Stats.</td>
<td style="text-align: center;">81 K</td>
<td style="text-align: center;">18 K</td>
<td style="text-align: center;">9 K</td>
<td style="text-align: center;">9 K</td>
<td style="text-align: center;">18 K</td>
<td style="text-align: center;">7 K</td>
<td style="text-align: center;">7 K</td>
<td style="text-align: center;">150 K</td>
</tr>
<tr>
<td style="text-align: left;">KVMnet</td>
<td style="text-align: center;">41.40</td>
<td style="text-align: center;">37.56</td>
<td style="text-align: center;">27.28</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">17.80</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">$\mathbf{9 . 6 0}$</td>
<td style="text-align: center;">26.67</td>
</tr>
<tr>
<td style="text-align: left;">NSM, best at top beam</td>
<td style="text-align: center;">78.38</td>
<td style="text-align: center;">35.40</td>
<td style="text-align: center;">28.70</td>
<td style="text-align: center;">4.31</td>
<td style="text-align: center;">12.38</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">10.63</td>
</tr>
<tr>
<td style="text-align: left;">NSM best over top 2 beams</td>
<td style="text-align: center;">80.12</td>
<td style="text-align: center;">41.23</td>
<td style="text-align: center;">35.67</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">15.34</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">11.02</td>
</tr>
<tr>
<td style="text-align: left;">NSM, best over top 5 beams</td>
<td style="text-align: center;">86.46</td>
<td style="text-align: center;">64.70</td>
<td style="text-align: center;">50.80</td>
<td style="text-align: center;">6.98</td>
<td style="text-align: center;">29.18</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">12.07</td>
</tr>
<tr>
<td style="text-align: left;">NSM, best over top 10 beams</td>
<td style="text-align: center;">96.78</td>
<td style="text-align: center;">69.86</td>
<td style="text-align: center;">60.18</td>
<td style="text-align: center;">10.69</td>
<td style="text-align: center;">30.71</td>
<td style="text-align: center;">2.09</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">14.36</td>
</tr>
<tr>
<td style="text-align: left;">CIPITR, best at top beam</td>
<td style="text-align: center;">$\mathbf{9 6 . 5 2}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 7 2}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 4 3}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 3 3}$</td>
<td style="text-align: center;">$\mathbf{1 5 . 1 2}$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\mathbf{5 8 . 9 2}$</td>
</tr>
<tr>
<td style="text-align: left;">CIPITR, best over top 2 beams</td>
<td style="text-align: center;">96.55</td>
<td style="text-align: center;">87.78</td>
<td style="text-align: center;">90.48</td>
<td style="text-align: center;">25.85</td>
<td style="text-align: center;">51.72</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">62.52</td>
</tr>
<tr>
<td style="text-align: left;">CIPITR, best over top 5 beams</td>
<td style="text-align: center;">97.18</td>
<td style="text-align: center;">87.96</td>
<td style="text-align: center;">90.97</td>
<td style="text-align: center;">27.19</td>
<td style="text-align: center;">52.01</td>
<td style="text-align: center;">29.45</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">69.25</td>
</tr>
<tr>
<td style="text-align: left;">CIPITR, best over top 10 beams</td>
<td style="text-align: center;">97.18</td>
<td style="text-align: center;">88.92</td>
<td style="text-align: center;">90.98</td>
<td style="text-align: center;">28.92</td>
<td style="text-align: center;">52.71</td>
<td style="text-align: center;">32.98</td>
<td style="text-align: center;">1.54</td>
<td style="text-align: center;">73.71</td>
</tr>
</tbody>
</table>
<p>Table 3: F1 score (\%) of KVMnet and NSM, and CIPITR. Bold numbers indicate the best among KVMnet and top beam score of NSM and CIPITR.
one single model over all question types. KVMnet does not have any beam search, the NSM model uses a beam size of 50, and CIPITR uses only 20 beams for exploring the program space.</p>
<p>Our manual inspection of these seven query categories show that simple and verify are simplest in nature requiring 1-line programs while logical is moderately difficult, with around 3 lines of code. The query categories next in order of complexity are quantitative and quantitative count, needing a sequence of $2-5$ operations. The hardest types are comparative and comparative count, which translate to an average of 5-10 lined programs.</p>
<p>Analysis: The experiments show that on the simple to moderately difficult (i.e., first three) query classes, CIPITR's performance at the top beam is up to 3 times better than both the baselines. The superiority of CIPITR over NSM is showcased better on the more complex classes where it outperforms the latter by 5-10 times, with the biggest impact (by a factor of 89 times) being on the "comparative" questions. Also, the $5 \times$ better performance of CIPITR over NSM over All category evinces the better generalizability of the abstract high-level program decomposition approach of the former.</p>
<p>On the other hand, training the KVMnet model on the balanced data helps showcase the real performance of the model, where CIPITR outperforms KVMnet significantly on most of the harder query classes. The only exception is the hardest class (Comp, Count with numerical answers) where the abrupt "best performance" of KVMnet can be attributed to its rote learning</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: center;">F1 (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Action Repetition</td>
<td style="text-align: center;">1.68</td>
</tr>
<tr>
<td style="text-align: left;">Phase Change</td>
<td style="text-align: center;">2.41</td>
</tr>
<tr>
<td style="text-align: left;">Valid Variable <br> Instantiation</td>
<td style="text-align: center;">3.08</td>
</tr>
<tr>
<td style="text-align: left;">Biasing last operator</td>
<td style="text-align: center;">7.52</td>
</tr>
<tr>
<td style="text-align: left;">Auxiliary Reward</td>
<td style="text-align: center;">9.85</td>
</tr>
<tr>
<td style="text-align: left;">Beam Pruning</td>
<td style="text-align: center;">10.34</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation testing on comparative questions: Top beam's F1 score obtained by omitting each of the above features from CIPITR, originally having F1 of $15.123 \%$.
abilities simply because of its knowledge of the answer vocabulary, which the program induction models are oblivious to, as they never see the actual answer.</p>
<p>Lastly, in our experimental configurations, whereas CIPITR and NSM's parameter-size is almost comparable, KVMnet's is approximately $6 \times$ larger.</p>
<p>Ablation Study: To quantitatively analyze the utility of the features mentioned in Section 5, we experiment with various ablations in Table 4 by turning off each feature, one at a time. We show the effect on the hardest question category ("comparative") on which our proposed model achieved reasonable performance. We see in the table that each of the 6 techniques helped the model significantly. Some of them boosted F1 by $1.5-4$ times, while others proved to be instrumental to obtained large improvements in F1 score of over 6-9 times.</p>
<p>To summarize, CIPITR has the following advantages, inducing programs more efficiently</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ques <br> Type</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CIPITR Program</th>
<th style="text-align: center;">NSM program</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">( E,R,T)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">Who is associated with Robert Emmett O'Malley ?</td>
<td style="text-align: center;">E0: Robert Emmett O'Malley <br> R0: associated with <br> T0: person</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_set(E0, R0, T0)</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_set(E0, R0, T0)</td>
</tr>
<tr>
<td style="text-align: center;">Verify</td>
<td style="text-align: center;">Is Sergio Mattarella the chief of state of Italy ?</td>
<td style="text-align: center;">E0: Sergio Mattarella, <br> E1: Italy, <br> R0: chief of state</td>
<td style="text-align: center;">$\mathrm{A}=$ verify(E0, R0, E1)</td>
<td style="text-align: center;">$\mathrm{A}=$ verify(E0, R0, E1)</td>
</tr>
<tr>
<td style="text-align: center;">Logical</td>
<td style="text-align: center;">Which cities were Animal Kingdom filmed on or share border with Pedralba ?</td>
<td style="text-align: center;">E0: Animal Kingdom, <br> E1: Pedralba, <br> R0: filmed_on, <br> R1: Share_border, <br> T0: cities</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_set(E0, R0, T0); <br> $\mathrm{B}=$ gen_set(E1, R1, T0); <br> $\mathrm{C}=$ set_union(A,B)</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_set(E1, R1, T0); <br> $\mathrm{B}=$ gen_set(E1, R1, None); <br> $\mathrm{C}=$ set_union(A,B)</td>
</tr>
<tr>
<td style="text-align: center;">Quant <br> Count</td>
<td style="text-align: center;">How many nucleic acid sequences encodes Calreticulin or Neurotensin/neuromedin-N ?</td>
<td style="text-align: center;">E0: Calreticulin, <br> E1: Neurotensin/neuromedin-N, <br> R0: encoded_by, <br> T0: nucleic acid</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_set(E0, R0, T0); <br> $\mathrm{B}=$ gen_set(E1, R0, T0); <br> $\mathrm{C}=$ set_union(A,B); <br> $\mathrm{D}=$ set_count(C)</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_set(E0, R0, T0); <br> $\mathrm{B}=$ set_count(A); <br> $\mathrm{C}=$ set_union(A,B)</td>
</tr>
<tr>
<td style="text-align: center;">Quant</td>
<td style="text-align: center;">What municipal councils are the legislative bodies for max US administrative territories?</td>
<td style="text-align: center;">T0: municipal council, <br> T1: US administrative territories <br> R0: legislative_body of</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_map_set(T0, R0, T1); <br> $\mathrm{B}=$ map_count(A); <br> $\mathrm{C}=$ select_max(B)</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_map_set(T0, R0, T1); <br> $\mathrm{B}=$ gen_map_set(T0, R0, T1); <br> $\mathrm{C}=$ map_count(A)</td>
</tr>
<tr>
<td style="text-align: center;">Comp</td>
<td style="text-align: center;">Which works did less number of people do the dubbing for than Herculesy el rey de Tesalia ?</td>
<td style="text-align: center;">E0: Herculesy el rey de Tesalia, R0: dubbed_by, <br> T0: works, <br> T1: people</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_map_set(T0, R0, T1); <br> $\mathrm{B}=$ gen_set(E0, R0, T1); <br> $\mathrm{C}=$ set_count(B); <br> $\mathrm{D}=$ map_count(A); <br> $\mathrm{E}=$ select_less(D,C)</td>
<td style="text-align: center;">$\mathrm{A}=$ gen_set(E0, R0, T1); <br> $\mathrm{B}=$ gen_set(E0, R0, T1); <br> $\mathrm{C}=$ gen_map_set(T0, R0, T1); <br> $\mathrm{D}=$ set_diff(A,B)</td>
</tr>
</tbody>
</table>
<p>Table 5: Qualitative analysis of programs for different type of question for CIPITR and NSM.
and pragmatically, as illustrated by the sample outputs in Table 5:</p>
<ul>
<li>Generating syntactically correct programs: Because of the token-by-token decoding of the program, NSM cannot restrict its search to only syntactically correct programs, but rather only resorts to a post-filtering step during training. However, at test time, it could still generate programs with wrong syntax, as shown in Table 5. For example, for the Logical question, it invokes a gen...set with a wrong argument type None and for the Quantitative count question, it invokes the set...union operator on a non-set argument. On the other hand, CIPITR, by design, can never generate a syntactically incorrect program because at every step it implicitly samples only feasible actions.</li>
<li>Generating semantically correct programs: CIPITR is capable of incorporating different generic programming styles as well as problemspecific constraints, restricting its search space to only semantically correct programs. As shown in Table 5, CIPITR is able to generate at least meaningful programs having the desired answer-type or without repeating lines of code. On the other hand the NSMgenerated programs are often semantically wrong, for instance, both in the Quantitative and Quantitative Count based questions, the
type of the answer is itself wrong, rendering the program meaningless. This arises once again, owing to the token-by-token decoding of the program by NSM which makes it hard to incorporate high level rules to guide or constrain the search.</li>
<li>Efficient search-space exploration: Owing to the different strategies used to explore the program space more intelligently, CIPITR scales better to a wide variety of complex queries by using less than half of NSM's beam size. We experimentally established that for programs of length 7 these various techniques reduced the average program space from $1.33 \times 10^{19}$ to 2,998 programs.</li>
</ul>
<h2>7 Conclusion</h2>
<p>We presented CIPITR, an advanced NPI framework that significantly pushes the frontier of complex program induction in absence of gold programs. CIPITR uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to constrain the combinatorial program space to only semantically correct programs. As future directions of work, CIPITR can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together.</p>
<p>Other potential directions of research could be toward learning to discover sub-goals to further decompose the most complex classes beyond just the two-level phase transition proposed here. Additionally, further improvements are required to induce complex programs without availability of gold program input variables.</p>
<h2>References</h2>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016a. Learning to compose neural networks for question answering. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1545-1554.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016b. Learning to compose neural networks for question answering. In NAACL-HLT, pages 1545-1554.
B. Bakker and J. Schmidhuber. 2004. Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization. In Proceedings of the 8th Conference on Intelligent Autonomous Systems IAS-8, pages 438-445.</p>
<p>Andrew G. Barto and Sridhar Mahadevan. 2003. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13(1-2):41-77.</p>
<p>Hannah Bast and Elmar Haußmann. 2015. More accurate question answering on freebase. In CIKM, pages 1431-1440.
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In EMNLP Conference, pages 1533-1544.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In NIPS Conference, pages 2787-2795.</p>
<p>Matko Bosnjak, Tim Rocktäschel, Jason Naradowsky, and Sebastian Riedel. 2017. Programming with a differentiable forth interpreter. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 547-556.</p>
<p>Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. 2018. Leveraging grammar and reinforcement learning for neural program synthesis. In International Conference on Learning Representations (ICLR).</p>
<p>Rajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. 2017. Question answering on knowledge bases and text using universal schema and memory networks. In ACL (2), pages 358-365.</p>
<p>Peter Dayan and Geoffrey E. Hinton. 1993. Feudal reinforcement learning. In Advances in Neural Information Processing Systems 5, [NIPS Conference], pages 271-278.</p>
<p>Thomas G. Dietterich. 2000. Hierarchical reinforcement learning with the maxq value function decomposition. Journal of Artificial Intelligence Research, 13(1):227-303.</p>
<p>Li Dong and Mirella Lapata. 2016. Language to logical form with neural attention. In $A C L$, volume 1, pages 33-43.</p>
<p>Kelvin Guu, John Miller, and Percy Liang. 2015. Traversing knowledge graphs in vector space. In EMNLP Conference.</p>
<p>Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In $A C L$, volume 1, pages 1821-1831.</p>
<p>Sarvnaz Karimi, Justin Zobel, and Falk Scholer. 2012. Quantifying the impact of concept recognition on biomedical information retrieval. Information Processing \&amp; Management, 48(1): 94-106.</p>
<p>Mahboob Alam Khalid, Valentin Jijkoun, and Maarten De Rijke. 2008. The impact of named entity normalization on information retrieval for question answering. In Proceedings of the IR Research, 30th European Conference on Advances in Information Retrieval, ECIR'08, pages 705-710.</p>
<p>Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338.</p>
<p>Chengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. 2016. Neural program lattices. In International Conference on Learning Representations (ICLR).
X. Li and D. Roth. 2002. Learning question classifiers. In COLING, pages 556-562.</p>
<p>Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. 2017. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 23-33.</p>
<p>Andrew McCallum, Arvind Neelakantan, Rajarshi Das, and David Belanger. 2017. Chains of reasoning over entities, relations, and text using recurrent neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Volume 1: Long Papers, pages 132-141.</p>
<p>Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. In EMNLP, pages 1400-1409.</p>
<p>Stephen Muggleton and Luc De Raedt. 1994. Inductive logic programming: Theory and methods. Journal of Logic Programming, 19/20: 629-679.</p>
<p>Arvind Neelakantan, Quoc V. Le, Martin Abadi, Andrew McCallum, and Dario Amodei. 2016. Learning a natural language interface with neural programmer. arXiv preprint, arXiv: 1611.08945.</p>
<p>Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. 2015. Neural programmer: Inducing latent programs with gradient descent. CoRR, abs/1511.04834.</p>
<p>Ronald Parr and Stuart Russell. 1998. Reinforcement learning with hierarchies of machines. In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems 10, NIPS '97, pages 1043-1049.</p>
<p>Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305.</p>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with recurrent neural networks. CoRR, abs/1511.06732.</p>
<p>Scott Reed and Nando de Freitas. 2016. Neural programmer-interpreters. In International Conference on Learning Representations (ICLR).</p>
<p>Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. 2018. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. In $A A A I$.</p>
<p>Richard S. Sutton, Doina Precup, and Satinder Singh. 1999. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181-211.</p>
<p>Reut Tsarfaty, Ilia Pogrebezky, Guy Weiss, Yaarit Natan, Smadar Szekely, and David Harel. 2014. Semantic parsing using content and context: A case study from requirements elicitation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296-1307.</p>
<p>Richard J. Waldinger and Richard C. T. Lee. 1969. PROW: A step toward automatic program writing. In Proceedings of the 1st International Joint Conference on Artificial Intelligence, pages 241-252.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. In Reinforcement Learning, Springer, pages 5-32.</p>
<p>Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Question answering on Freebase via relation extraction and textual evidence. arXiv preprint, arXiv: 1603.00957.</p>
<p>Xuchen Yao. 2015. Lean question answering over Freebase from scratch. In NAACL Conference, pages 66-70.</p>
<p>Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question</p>
<p>answering with knowledge base. In ACL Conference, pages 1321-1331.</p>
<p>Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The
value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 201-206.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The code and reinforcement learning environment of CIPITR is made public in https://github.com/CIPITR/ CIPITR.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
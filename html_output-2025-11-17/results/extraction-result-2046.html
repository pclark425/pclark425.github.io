<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2046 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2046</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2046</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-278769791</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14163v1.pdf" target="_blank">DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM) agents have shown promising performance in generating code for solving complex data science problems. Recent studies primarily focus on enhancing in-context learning through improved search, sampling, and planning techniques, while overlooking the importance of the order in which problems are tackled during inference. In this work, we develop a novel inference-time optimization framework, referred to as DSMentor, which leverages curriculum learning -- a strategy that introduces simpler task first and progressively moves to more complex ones as the learner improves -- to enhance LLM agent performance in challenging data science tasks. Our mentor-guided framework organizes data science tasks in order of increasing difficulty and incorporates a growing long-term memory to retain prior experiences, guiding the agent's learning progression and enabling more effective utilization of accumulated knowledge. We evaluate DSMentor through extensive experiments on DSEval and QRData benchmarks. Experiments show that DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents. Furthermore, DSMentor demonstrates stronger causal reasoning ability, improving the pass rate by 8.8% on the causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our work underscores the importance of developing effective strategies for accumulating and utilizing knowledge during inference, mirroring the human learning process and opening new avenues for improving LLM performance through curriculum-based inference optimization.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2046.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2046.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSMentor (Mentor LLM curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSMentor: Mentor-guided curriculum generation for data-science LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses an LLM Mentor to assess problem difficulty from problem descriptions and automatically construct an easy-to-hard curriculum; a Student LLM then solves tasks in curriculum order while accumulating a growing online long-term memory of prior problem descriptions, generated code, and correctness tags for retrieval-augmented inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Claude-3.5-Sonnet; Llama-3.1-70b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>unknown (Claude-3.5-Sonnet); 70B (Llama-3.1-70b)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The Mentor agent (an LLM) reads only the natural-language problem descriptions and assigns a difficulty score to each problem according to provided difficulty-scale guidelines (problem-based difficulty). Tasks are then ordered to form a curriculum (primarily easy-to-hard; alternatives tested: hard-to-easy, random). During inference the Student agent iterates through the ordered tasks, retrieves top-K similar prior examples from an accumulating long-term memory (entries contain prior p_k, generated code c_k, and evaluation tag t_k ∈ {Correct, Incorrect}), orders retrieved examples by increasing similarity (or by increasing difficulty in ablations), and generates code which is executed and evaluated; the result is appended to memory. The curriculum therefore generates an explicit sequence of tasks (no subgoals or intermediate reward functions are generated), conditions only on task descriptions for difficulty assessment, and is static per run (though the memory grows online).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Data science code-generation tasks (benchmarked on DSEval and QRData)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Data-centric problems (statistical analysis, data manipulation, model prediction, causal reasoning); tasks are often interrelated and compositional (complex tasks can build on simpler preprocessing/feature extraction/regression steps); mixed single-turn and multi-turn problem-sets in benchmarks (authors focus on single-turn code generation for fair comparison); require specialized domain knowledge (statistics, causal inference) and strict execution-output formats; not an open-ended embodied domain and not primarily long-horizon planning in the sense of physical navigation, but some problems require multi-step reasoning/composition.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Mentor assigns difficulty using only the problem descriptions; Student retrieval conditions on similarity between current problem description and prior problem descriptions (embeddings), but the Mentor does not condition on agent internal state, inventory, or past performance during curriculum construction.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Growing online long-term memory (stores (problem description, generated code, correctness tag)); retrieval using cohere.embed-english-v3 embeddings and cosine similarity; execution environment for running generated Python code and producing pass/fail evaluation; ordering of retrieved examples (increasing similarity or increasing difficulty) and optional inclusion of incorrect examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Reported improvements when using the LLM-generated (problem-based) curriculum + online memory: DSMentor-Claude-3.5-Sonnet achieved pass rates on DSEval subsets: LeetCode 0.892 (vs Claude-3.5-Sonnet PoT 0.800), SO 0.837 (vs 0.804), Exercise 0.781 (vs 0.745), Kaggle 0.678 (vs 0.641). On QRData overall DSMentor-Claude-3.5-Sonnet 0.543 (vs GPT-4 PoT 0.491 baseline); on causal questions DSMentor-Claude-3.5-Sonnet 45.6% which is reported as +8.8% over the best baseline (GPT-4 PoT). The abstract additionally reports "up to 5.2%" improvement on DSEval and QRData relative to baseline agents depending on configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>Manual difficulty (LeetCode labels: easy/medium/hard) was used as a baseline ordering for DSEval-LeetCode; the paper reports that the LLM problem-based difficulty generally outperforms manual (LeetCode) ordering, though exact per-split numbers are reported in ablation tables (problem-based typically better overall; exceptions and small differences exist).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Two heuristic/alternative difficulty metrics were evaluated: (1) Reference-code-based difficulty (features of reference code such as number of functions/variables/loops), (2) Pass-rate difficulty (empirical pass rates from weaker models). Authors report problem-based (LLM) difficulty generally outperforms these heuristics, although DSMentor-Llama-3.1-70b using a pass-rate difficulty from a weaker model (Llama-3.1-8B) performed slightly better on DSEval-LeetCode in one comparison. Exact numeric comparisons are given in the paper's ablation tables; the qualitative outcome is that LLM problem-based difficulty is preferable in most settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Vanilla PoT baselines (no curriculum, no online growing memory) reported by the paper: e.g., Claude-3.5-Sonnet (PoT) pass rates on DSEval subsets: LeetCode 0.800, SO 0.804, Exercise 0.745, Kaggle 0.641; GPT-4 (PoT) on QRData 0.491 overall. DSMentor variants consistently outperform these no-curriculum baselines across many splits as reported in Tables 1 and 2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Limitations noted: Mentor assesses difficulty using only problem descriptions (no ground-truth code or execution results), which can mis-rank problems if descriptions are ambiguous; curriculum gains are dataset-dependent (e.g., hard-to-easy slightly outperformed easy-to-hard on DSEval-Kaggle by 0.4% in one case); improvements from adding incorrect examples are dataset-size dependent (helpful on small datasets like LeetCode and Exercise, marginal on large datasets like SO, Kaggle, QRData). The method does not update model weights—benefits are inference-time only—and may be limited when memory lacks relevant prior examples. The paper notes scenarios (page/appendix) where evaluation issues can underestimate agent capabilities but does not claim universality across all data-science problem types.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>In specialized subdomains like causal reasoning, DSMentor shows notable gains: on QRData causal questions DSMentor-Claude-3.5-Sonnet achieved 45.6% (an absolute improvement of 8.8% over GPT-4 PoT baseline), indicating curriculum+memory is particularly beneficial for complex, domain-specialized reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Extensive ablations: (1) Difficulty definition: compared problem-based (LLM) vs manual (LeetCode labels) vs reference-code-based vs pass-rate difficulty — problem-based generally best, with specific exceptions (Llama-3.1-70b+pass-rate better on LeetCode in one case). (2) Order of tasks: easy-to-hard, hard-to-easy, and random curricula compared; easy-to-hard usually performs best, hard-to-easy slightly better on DSEval-Kaggle in one case. (3) Retrieved-example ordering: ordering retrieved examples by increasing similarity outperforms ordering by increasing difficulty in most cases. (4) Inclusion of incorrect examples: including previously incorrect examples improves pass rates for smaller datasets (LeetCode, Exercise) but yields marginal gains for larger datasets (SO, Kaggle, QRData). (5) Number of retrieved examples (K) varied — performance trends shown in Figure 4; best K is dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Two model families tested: Llama-3.1-70b and Claude-3.5-Sonnet (size not specified in text). No systematic parameter-scaling sweep; both models benefit from DSMentor but relative gains and absolute performance vary by dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Key findings: (1) LLM-generated (problem-based) difficulty assessment to construct an easy-to-hard curriculum plus an online long-term memory yields consistent gains over no-curriculum baselines across DSEval and QRData, especially for complex causal reasoning tasks (+8.8% vs best baseline on causal QRData). (2) Easy-to-hard ordering with retrieved examples sorted by increasing similarity is often the most effective curriculum design; (3) Problem-based (LLM) difficulty generally outperforms manual, reference-code-based, and pass-rate heuristics, though heuristics can sometimes match or slightly outperform on specific splits; (4) Including incorrect prior examples helps on small datasets; (5) Gains are inference-time (no weight updates) and depend on quality/coverage of the accumulating memory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2046.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2046.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual / Heuristic curricula (comparators)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual and heuristic difficulty/order baselines: LeetCode-labeled difficulty, reference-code-based difficulty, pass-rate difficulty, and random ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines and alternative curriculum-generation approaches evaluated against the LLM-generated Mentor curriculum: (i) Manual difficulty using LeetCode easy/medium/hard labels (used for DSEval-LeetCode), (ii) reference-code-based difficulty derived from code features, (iii) pass-rate difficulty computed from performance of weaker models, and (iv) random task ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>manual/hand-designed; heuristic-based; random</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Manual: sort problems using externally-provided difficulty labels (LeetCode easy/medium/hard), breaking ties by human pass rate. Reference-code-based: compute difficulty from structural features of provided reference code (functions, variables, conditionals, loops). Pass-rate: rank by empirical pass rates of weaker models (higher pass rate = easier). Random: shuffle tasks uniformly. These curricula produce task orderings (no subgoals) and are static per run.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Same data science code-generation benchmarks (DSEval, QRData)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>As above: data-centric, compositional, requiring code-generation and domain knowledge; heuristics rely on ancillary artifacts (reference code) or prior model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>These heuristic/manual approaches do not condition on current agent internal state at curriculum construction time; pass-rate heuristic does implicitly depend on previous model evaluations but not on per-agent live state during the Student run.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used with the same Student agent and online memory machinery in DSMentor experiments for fair comparison; retrieval and execution pipeline identical unless otherwise noted.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>Manual (LeetCode) ordering was evaluated on DSEval-LeetCode; problem-based LLM curriculum generally outperformed manual ordering according to ablations, though exact per-case numbers are in the paper's ablation tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Reference-code-based and pass-rate heuristics were tested; problem-based (LLM) difficulty usually outperformed these heuristics, except one noted case where DSMentor-Llama-3.1-70b using pass-rate difficulty (from a weaker model) performed slightly better on DSEval-LeetCode.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Random ordering (no curriculum) performs worse than structured curricula in most experiments; easy-to-hard and hard-to-easy both beat random in many settings according to Table 5/6.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Manual labels may be coarse (LeetCode easy/medium/hard) and tie-breaking (human pass-rate) needed; reference-code-based metrics require access to reference implementations (not available in many real-world scenarios); pass-rate heuristics require running weaker models to produce empirical pass rates which is itself costly and model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Heuristic approaches relying on reference code are less applicable when no reference code is available; pass-rate heuristics depend on similarity between weaker models and target Student and may not generalize across domain-specialized tasks (e.g., complex causal reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper reports comparative ablations across these difficulty definitions and ordering strategies: problem-based LLM difficulty generally best, with dataset-specific exceptions (see ablation tables).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Heuristic/manual curricula provide competitive baselines but are often outperformed by LLM-generated problem-based difficulty; heuristics can excel in specific narrow settings (e.g., pass-rate heuristic on LeetCode for one model) but lack the generality of the LLM Mentor approach which only needs problem descriptions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2046.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2046.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum order & retrieval design ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ordering strategies and retrieval-designs: easy-to-hard, hard-to-easy, random; retrieved-example ordering (increasing similarity vs increasing difficulty); inclusion of incorrect examples; K (top-K retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of ablations showing how curriculum order and the ordering/selection of retrieved prior examples affect Student performance; investigates including incorrect examples from memory and varying retrieval size and ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated curriculum combined with explicit ordering heuristics (easy-to-hard, hard-to-easy, random) and retrieval-order heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Claude-3.5-Sonnet; Llama-3.1-70b (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>unknown (Claude-3.5-Sonnet); 70B (Llama-3.1-70b)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>After Mentor generates difficulty-based ordering, Student retrieves top-K prior examples from memory based on embedding cosine similarity; retrieved examples are ordered for presentation to the Student either by increasing similarity (Inc.Similarity) or increasing difficulty (Inc.Difficulty). Curriculum order (task sequence) tested: easy-to-hard, hard-to-easy, random. Experiments also toggle whether incorrectly answered prior examples are allowed in retrieval and sweep K (number of retrieved examples).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>DSEval and QRData (data-science code-gen tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Tasks are compositional; retrieving similar prior solved examples is meaningful because problems often share preprocessing or analytic primitives; dataset size affects retrieval pool coverage (small datasets benefit more from including incorrect examples).</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Ordering decisions do not condition on live Student internal state aside from the current problem description embedding; retrieval depends on similarity between current problem description and stored problem descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Online long-term memory, embedding-based retrieval, execution/evaluation loop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Ablation tables show: (1) Easy-to-hard (Inc.Similarity) often yields best performance (Table 5/6). Example numbers (Table 5 for Llama-3.1-70b): Easy-to-Hard (Inc.Similarity) 0.892 (LeetCode), 0.837 (SO), 0.781 (Exercise), 0.678 (Kaggle). Hard-to-easy and random are generally worse; hard-to-easy narrowly beats easy-to-hard on DSEval-Kaggle by 0.4% in one config. (2) Retrieved-example ordering by increasing similarity usually outperforms increasing difficulty ordering. (3) Including incorrect examples increases pass rate on small datasets (LeetCode: from 0.733 to 0.792; Exercise: 0.725 → 0.752) and yields marginal gains on large datasets (SO: 0.830 → 0.837; Kaggle small change; QRData: 0.475 → 0.508).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Ordering and retrieval strategies are sensitive to dataset size and memory coverage; poor similarity retrieval or small memory pools reduce curriculum benefit; ordering heuristics (e.g., increasing difficulty among retrieved examples) are not universally better and can hurt in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Design choices like including incorrect examples are more beneficial for smaller, lower-diversity datasets; for large, diverse datasets the retrieval pool already contains enough correct exemplars so additional incorrect examples add little benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Direct ablations performed: easy-to-hard vs hard-to-easy vs random; Inc.Similarity vs Inc.Difficulty for retrieved examples; include vs exclude incorrect examples; K sweeps (Figure 4) showing dataset-dependent best K.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Ordering tasks easy-to-hard and presenting retrieved examples by increasing similarity is generally most effective; including incorrect prior examples yields nontrivial gains for small datasets but marginal gains for large ones; curriculum design interacts with retrieval design and dataset size.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Cross-Episodic Curriculum for Transformer Agents <em>(Rating: 2)</em></li>
                <li>Let's learn step by step: Enhancing in-context learning ability with curriculum learning <em>(Rating: 2)</em></li>
                <li>Are LLMs capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data <em>(Rating: 2)</em></li>
                <li>Is ChatGPT a good data scientist? a preliminary study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2046",
    "paper_id": "paper-278769791",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "DSMentor (Mentor LLM curriculum)",
            "name_full": "DSMentor: Mentor-guided curriculum generation for data-science LLM agents",
            "brief_description": "A framework that uses an LLM Mentor to assess problem difficulty from problem descriptions and automatically construct an easy-to-hard curriculum; a Student LLM then solves tasks in curriculum order while accumulating a growing online long-term memory of prior problem descriptions, generated code, and correctness tags for retrieval-augmented inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "Claude-3.5-Sonnet; Llama-3.1-70b",
            "llm_model_size": "unknown (Claude-3.5-Sonnet); 70B (Llama-3.1-70b)",
            "curriculum_description": "The Mentor agent (an LLM) reads only the natural-language problem descriptions and assigns a difficulty score to each problem according to provided difficulty-scale guidelines (problem-based difficulty). Tasks are then ordered to form a curriculum (primarily easy-to-hard; alternatives tested: hard-to-easy, random). During inference the Student agent iterates through the ordered tasks, retrieves top-K similar prior examples from an accumulating long-term memory (entries contain prior p_k, generated code c_k, and evaluation tag t_k ∈ {Correct, Incorrect}), orders retrieved examples by increasing similarity (or by increasing difficulty in ablations), and generates code which is executed and evaluated; the result is appended to memory. The curriculum therefore generates an explicit sequence of tasks (no subgoals or intermediate reward functions are generated), conditions only on task descriptions for difficulty assessment, and is static per run (though the memory grows online).",
            "domain_name": "Data science code-generation tasks (benchmarked on DSEval and QRData)",
            "domain_characteristics": "Data-centric problems (statistical analysis, data manipulation, model prediction, causal reasoning); tasks are often interrelated and compositional (complex tasks can build on simpler preprocessing/feature extraction/regression steps); mixed single-turn and multi-turn problem-sets in benchmarks (authors focus on single-turn code generation for fair comparison); require specialized domain knowledge (statistics, causal inference) and strict execution-output formats; not an open-ended embodied domain and not primarily long-horizon planning in the sense of physical navigation, but some problems require multi-step reasoning/composition.",
            "state_conditioning": false,
            "state_conditioning_details": "Mentor assigns difficulty using only the problem descriptions; Student retrieval conditions on similarity between current problem description and prior problem descriptions (embeddings), but the Mentor does not condition on agent internal state, inventory, or past performance during curriculum construction.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Growing online long-term memory (stores (problem description, generated code, correctness tag)); retrieval using cohere.embed-english-v3 embeddings and cosine similarity; execution environment for running generated Python code and producing pass/fail evaluation; ordering of retrieved examples (increasing similarity or increasing difficulty) and optional inclusion of incorrect examples.",
            "performance_llm_curriculum": "Reported improvements when using the LLM-generated (problem-based) curriculum + online memory: DSMentor-Claude-3.5-Sonnet achieved pass rates on DSEval subsets: LeetCode 0.892 (vs Claude-3.5-Sonnet PoT 0.800), SO 0.837 (vs 0.804), Exercise 0.781 (vs 0.745), Kaggle 0.678 (vs 0.641). On QRData overall DSMentor-Claude-3.5-Sonnet 0.543 (vs GPT-4 PoT 0.491 baseline); on causal questions DSMentor-Claude-3.5-Sonnet 45.6% which is reported as +8.8% over the best baseline (GPT-4 PoT). The abstract additionally reports \"up to 5.2%\" improvement on DSEval and QRData relative to baseline agents depending on configuration.",
            "performance_manual_curriculum": "Manual difficulty (LeetCode labels: easy/medium/hard) was used as a baseline ordering for DSEval-LeetCode; the paper reports that the LLM problem-based difficulty generally outperforms manual (LeetCode) ordering, though exact per-split numbers are reported in ablation tables (problem-based typically better overall; exceptions and small differences exist).",
            "performance_heuristic_curriculum": "Two heuristic/alternative difficulty metrics were evaluated: (1) Reference-code-based difficulty (features of reference code such as number of functions/variables/loops), (2) Pass-rate difficulty (empirical pass rates from weaker models). Authors report problem-based (LLM) difficulty generally outperforms these heuristics, although DSMentor-Llama-3.1-70b using a pass-rate difficulty from a weaker model (Llama-3.1-8B) performed slightly better on DSEval-LeetCode in one comparison. Exact numeric comparisons are given in the paper's ablation tables; the qualitative outcome is that LLM problem-based difficulty is preferable in most settings.",
            "performance_no_curriculum": "Vanilla PoT baselines (no curriculum, no online growing memory) reported by the paper: e.g., Claude-3.5-Sonnet (PoT) pass rates on DSEval subsets: LeetCode 0.800, SO 0.804, Exercise 0.745, Kaggle 0.641; GPT-4 (PoT) on QRData 0.491 overall. DSMentor variants consistently outperform these no-curriculum baselines across many splits as reported in Tables 1 and 2.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Limitations noted: Mentor assesses difficulty using only problem descriptions (no ground-truth code or execution results), which can mis-rank problems if descriptions are ambiguous; curriculum gains are dataset-dependent (e.g., hard-to-easy slightly outperformed easy-to-hard on DSEval-Kaggle by 0.4% in one case); improvements from adding incorrect examples are dataset-size dependent (helpful on small datasets like LeetCode and Exercise, marginal on large datasets like SO, Kaggle, QRData). The method does not update model weights—benefits are inference-time only—and may be limited when memory lacks relevant prior examples. The paper notes scenarios (page/appendix) where evaluation issues can underestimate agent capabilities but does not claim universality across all data-science problem types.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "In specialized subdomains like causal reasoning, DSMentor shows notable gains: on QRData causal questions DSMentor-Claude-3.5-Sonnet achieved 45.6% (an absolute improvement of 8.8% over GPT-4 PoT baseline), indicating curriculum+memory is particularly beneficial for complex, domain-specialized reasoning.",
            "ablation_studies": "Extensive ablations: (1) Difficulty definition: compared problem-based (LLM) vs manual (LeetCode labels) vs reference-code-based vs pass-rate difficulty — problem-based generally best, with specific exceptions (Llama-3.1-70b+pass-rate better on LeetCode in one case). (2) Order of tasks: easy-to-hard, hard-to-easy, and random curricula compared; easy-to-hard usually performs best, hard-to-easy slightly better on DSEval-Kaggle in one case. (3) Retrieved-example ordering: ordering retrieved examples by increasing similarity outperforms ordering by increasing difficulty in most cases. (4) Inclusion of incorrect examples: including previously incorrect examples improves pass rates for smaller datasets (LeetCode, Exercise) but yields marginal gains for larger datasets (SO, Kaggle, QRData). (5) Number of retrieved examples (K) varied — performance trends shown in Figure 4; best K is dataset-dependent.",
            "model_size_scaling": "Two model families tested: Llama-3.1-70b and Claude-3.5-Sonnet (size not specified in text). No systematic parameter-scaling sweep; both models benefit from DSMentor but relative gains and absolute performance vary by dataset and model.",
            "key_findings_curriculum_effectiveness": "Key findings: (1) LLM-generated (problem-based) difficulty assessment to construct an easy-to-hard curriculum plus an online long-term memory yields consistent gains over no-curriculum baselines across DSEval and QRData, especially for complex causal reasoning tasks (+8.8% vs best baseline on causal QRData). (2) Easy-to-hard ordering with retrieved examples sorted by increasing similarity is often the most effective curriculum design; (3) Problem-based (LLM) difficulty generally outperforms manual, reference-code-based, and pass-rate heuristics, though heuristics can sometimes match or slightly outperform on specific splits; (4) Including incorrect prior examples helps on small datasets; (5) Gains are inference-time (no weight updates) and depend on quality/coverage of the accumulating memory.",
            "uuid": "e2046.0"
        },
        {
            "name_short": "Manual / Heuristic curricula (comparators)",
            "name_full": "Manual and heuristic difficulty/order baselines: LeetCode-labeled difficulty, reference-code-based difficulty, pass-rate difficulty, and random ordering",
            "brief_description": "Baselines and alternative curriculum-generation approaches evaluated against the LLM-generated Mentor curriculum: (i) Manual difficulty using LeetCode easy/medium/hard labels (used for DSEval-LeetCode), (ii) reference-code-based difficulty derived from code features, (iii) pass-rate difficulty computed from performance of weaker models, and (iv) random task ordering.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generator_type": "manual/hand-designed; heuristic-based; random",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Manual: sort problems using externally-provided difficulty labels (LeetCode easy/medium/hard), breaking ties by human pass rate. Reference-code-based: compute difficulty from structural features of provided reference code (functions, variables, conditionals, loops). Pass-rate: rank by empirical pass rates of weaker models (higher pass rate = easier). Random: shuffle tasks uniformly. These curricula produce task orderings (no subgoals) and are static per run.",
            "domain_name": "Same data science code-generation benchmarks (DSEval, QRData)",
            "domain_characteristics": "As above: data-centric, compositional, requiring code-generation and domain knowledge; heuristics rely on ancillary artifacts (reference code) or prior model performance.",
            "state_conditioning": false,
            "state_conditioning_details": "These heuristic/manual approaches do not condition on current agent internal state at curriculum construction time; pass-rate heuristic does implicitly depend on previous model evaluations but not on per-agent live state during the Student run.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Used with the same Student agent and online memory machinery in DSMentor experiments for fair comparison; retrieval and execution pipeline identical unless otherwise noted.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": "Manual (LeetCode) ordering was evaluated on DSEval-LeetCode; problem-based LLM curriculum generally outperformed manual ordering according to ablations, though exact per-case numbers are in the paper's ablation tables.",
            "performance_heuristic_curriculum": "Reference-code-based and pass-rate heuristics were tested; problem-based (LLM) difficulty usually outperformed these heuristics, except one noted case where DSMentor-Llama-3.1-70b using pass-rate difficulty (from a weaker model) performed slightly better on DSEval-LeetCode.",
            "performance_no_curriculum": "Random ordering (no curriculum) performs worse than structured curricula in most experiments; easy-to-hard and hard-to-easy both beat random in many settings according to Table 5/6.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Manual labels may be coarse (LeetCode easy/medium/hard) and tie-breaking (human pass-rate) needed; reference-code-based metrics require access to reference implementations (not available in many real-world scenarios); pass-rate heuristics require running weaker models to produce empirical pass rates which is itself costly and model-dependent.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Heuristic approaches relying on reference code are less applicable when no reference code is available; pass-rate heuristics depend on similarity between weaker models and target Student and may not generalize across domain-specialized tasks (e.g., complex causal reasoning).",
            "ablation_studies": "Paper reports comparative ablations across these difficulty definitions and ordering strategies: problem-based LLM difficulty generally best, with dataset-specific exceptions (see ablation tables).",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Heuristic/manual curricula provide competitive baselines but are often outperformed by LLM-generated problem-based difficulty; heuristics can excel in specific narrow settings (e.g., pass-rate heuristic on LeetCode for one model) but lack the generality of the LLM Mentor approach which only needs problem descriptions.",
            "uuid": "e2046.1"
        },
        {
            "name_short": "Curriculum order & retrieval design ablations",
            "name_full": "Ordering strategies and retrieval-designs: easy-to-hard, hard-to-easy, random; retrieved-example ordering (increasing similarity vs increasing difficulty); inclusion of incorrect examples; K (top-K retrieval)",
            "brief_description": "A set of ablations showing how curriculum order and the ordering/selection of retrieved prior examples affect Student performance; investigates including incorrect examples from memory and varying retrieval size and ordering.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated curriculum combined with explicit ordering heuristics (easy-to-hard, hard-to-easy, random) and retrieval-order heuristics",
            "llm_model_name": "Claude-3.5-Sonnet; Llama-3.1-70b (used in experiments)",
            "llm_model_size": "unknown (Claude-3.5-Sonnet); 70B (Llama-3.1-70b)",
            "curriculum_description": "After Mentor generates difficulty-based ordering, Student retrieves top-K prior examples from memory based on embedding cosine similarity; retrieved examples are ordered for presentation to the Student either by increasing similarity (Inc.Similarity) or increasing difficulty (Inc.Difficulty). Curriculum order (task sequence) tested: easy-to-hard, hard-to-easy, random. Experiments also toggle whether incorrectly answered prior examples are allowed in retrieval and sweep K (number of retrieved examples).",
            "domain_name": "DSEval and QRData (data-science code-gen tasks)",
            "domain_characteristics": "Tasks are compositional; retrieving similar prior solved examples is meaningful because problems often share preprocessing or analytic primitives; dataset size affects retrieval pool coverage (small datasets benefit more from including incorrect examples).",
            "state_conditioning": false,
            "state_conditioning_details": "Ordering decisions do not condition on live Student internal state aside from the current problem description embedding; retrieval depends on similarity between current problem description and stored problem descriptions.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Online long-term memory, embedding-based retrieval, execution/evaluation loop.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "Ablation tables show: (1) Easy-to-hard (Inc.Similarity) often yields best performance (Table 5/6). Example numbers (Table 5 for Llama-3.1-70b): Easy-to-Hard (Inc.Similarity) 0.892 (LeetCode), 0.837 (SO), 0.781 (Exercise), 0.678 (Kaggle). Hard-to-easy and random are generally worse; hard-to-easy narrowly beats easy-to-hard on DSEval-Kaggle by 0.4% in one config. (2) Retrieved-example ordering by increasing similarity usually outperforms increasing difficulty ordering. (3) Including incorrect examples increases pass rate on small datasets (LeetCode: from 0.733 to 0.792; Exercise: 0.725 → 0.752) and yields marginal gains on large datasets (SO: 0.830 → 0.837; Kaggle small change; QRData: 0.475 → 0.508).",
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Ordering and retrieval strategies are sensitive to dataset size and memory coverage; poor similarity retrieval or small memory pools reduce curriculum benefit; ordering heuristics (e.g., increasing difficulty among retrieved examples) are not universally better and can hurt in some datasets.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Design choices like including incorrect examples are more beneficial for smaller, lower-diversity datasets; for large, diverse datasets the retrieval pool already contains enough correct exemplars so additional incorrect examples add little benefit.",
            "ablation_studies": "Direct ablations performed: easy-to-hard vs hard-to-easy vs random; Inc.Similarity vs Inc.Difficulty for retrieved examples; include vs exclude incorrect examples; K sweeps (Figure 4) showing dataset-dependent best K.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Ordering tasks easy-to-hard and presenting retrieved examples by increasing similarity is generally most effective; including incorrect prior examples yields nontrivial gains for small datasets but marginal gains for large ones; curriculum design interacts with retrieval design and dataset size.",
            "uuid": "e2046.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Cross-Episodic Curriculum for Transformer Agents",
            "rating": 2
        },
        {
            "paper_title": "Let's learn step by step: Enhancing in-context learning ability with curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Are LLMs capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data",
            "rating": 2
        },
        {
            "paper_title": "Is ChatGPT a good data scientist? a preliminary study",
            "rating": 1
        }
    ],
    "cost": 0.016056249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DSMENTOR: ENHANCING DATA SCIENCE AGENTS WITH CURRICULUM LEARNING AND ONLINE KNOWL-EDGE ACCUMULATION
20 May 2025</p>
<p>He Wang wanghe@cmu.edu 
Carnegie Mellon University</p>
<p>Alexander Hanbo Li 
Carnegie Mellon University</p>
<p>Yiqun Hu yiqunhu@amazon.com 
Carnegie Mellon University</p>
<p>Sheng Zhang 
Carnegie Mellon University</p>
<p>Hideo Kobayashi hideodeo@amazon.com 
Carnegie Mellon University</p>
<p>Jiani Zhang zhajiani@amazon.com 
Carnegie Mellon University</p>
<p>Henry Zhu 
Carnegie Mellon University</p>
<p>Chung-Wei Hang cwhang@amazon.com 
Carnegie Mellon University</p>
<p>Patrick Ng patricng@amazon.com 
Carnegie Mellon University</p>
<p>DSMENTOR: ENHANCING DATA SCIENCE AGENTS WITH CURRICULUM LEARNING AND ONLINE KNOWL-EDGE ACCUMULATION
20 May 20258B1AAC01AA6EA6EA68EC590BD41FAAE9arXiv:2505.14163v1[cs.AI]
Large language model (LLM) agents have shown promising performance in generating code for solving complex data science problems.Recent studies primarily focus on enhancing in-context learning through improved search, sampling, and planning techniques, while overlooking the importance of the order in which problems are tackled during inference.In this work, we develop a novel inference-time optimization framework, referred to as DSMentor, which leverages curriculum learning-a strategy that introduces simpler task first and progressively moves to more complex ones as the learner improves-to enhance LLM agent performance in challenging data science tasks.Our mentor-guided framework organizes data science tasks in order of increasing difficulty and incorporates a growing longterm memory to retain prior experiences, guiding the agent's learning progression and enabling more effective utilization of accumulated knowledge.We evaluate DSMentor through extensive experiments on DSEval and QRData benchmarks.Experiments show that DSMentor using Claude-3.5-Sonnetimproves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents.Furthermore, DSMentor demonstrates stronger causal reasoning ability, improving the pass rate by 8.8% on the causality problems compared to GPT-4 using Program-of-Thoughts prompts.Our work underscores the importance of developing effective strategies for accumulating and utilizing knowledge during inference, mirroring the human learning process and opening new avenues for improving LLM performance through curriculum-based inference optimization.</p>
<p>INTRODUCTION</p>
<p>Data-centric tasks, ranging from statistical analysis to model prediction, are integral to various realworld applications, including healthcare (Miotto et al., 2018), finance (Heaton et al., 2017), and engineering (Chien &amp; Wagstaff, 2017).The ever-evolving demands of data-driven fields call for efficient and robust solutions that can effectively process, interpret, and learn from data.</p>
<p>As large language models (LLMs) demonstrate exceptional capabilities in understanding humanlike languages, recent works have leveraged LLMs to solve a wide range of tasks, including text generation (Dathathri et al., 2019), reasoning (Wei et al., 2022), and code generation (Chen et al., 2021b).This has motivated the development of Data Science agents (DS agents)-employing LLMs to generate code for data-centric tasks.Thanks to the extensive knowledge built during the training of LLMs, existing DS agents (Cheng et al., 2023;Zhang et al., 2023;2024a;Hong et al., 2024a) have shown strong capabilities in addressing various data science problems.</p>
<p>Figure 1: A motivating example illustrating how the agent can learn from previously solved easier problems.Without retrieving information from long-term memory, the agent incorrectly outputs the index of the largest single number, while by leveraging knowledge from previously solved easier questions, the agent can provide the correct answer.</p>
<p>However, many data science problems are inherently challenging, often involving vague task descriptions, complex data interpretation, and strict output formats, which results in DS agents failing to solve these problems effectively (Zhang et al., 2024b;Lai et al., 2023;Liu et al., 2024a).Additionally, the knowledge embedded within LLMs is static, confined to the data available during their training, posing significant challenges in rapidly evolving fields where up-to-date information is essential for generating accurate and relevant outputs.</p>
<p>To extend the knowledge of LLMs to specific data-centric tasks, recent works (Zhang et al., 2024a;Guo et al., 2024) primarily focus on retrieving external knowledge for solving data science problems, often starting with identifying or collecting relevant knowledge sources.However, these approaches typically do not accumulate knowledge in a dynamic, online fashion, nor do they explore the importance of task order or curriculum in problem-solving.This oversight is particularly critical in data science, where problems are often interrelated, and complex solutions can frequently be constructed by integrating simpler ones (Hong et al., 2024a).For instance, a sophisticated time series forecasting task might build upon foundational concepts of data preprocessing, feature engineering, and basic regression techniques.The challenge becomes even more pronounced in online learning or serving scenarios, where the timeliness and quality of the knowledge base directly impact model inference performance.Unfortunately, current methods often result in sub-optimal performance due to inadequate strategies for how to best prepare, structure, and organize the knowledge base.</p>
<p>To address these challenges and leverage the interrelated nature of data science problems, we explore the sequence in which knowledge is populated into and retrieved from a growing memory buffer, helping DS agents better understand and utilize accumulated knowledge.Inspired by the human learning process, where foundational knowledge forms the basis for tackling more complicated problems, we posit that the order in which information is introduced to the model can significantly influence its effectiveness.For instance, understanding basic concepts is often necessary before addressing more advanced challenges, as illustrated in Figure 1.</p>
<p>In this work, we propose a new framework, referred to as DSMentor, which incrementally enhances the capabilities of DS agents through a mentor-guided approach.The Mentor agent evaluates the difficulty of all the tasks and provides a sequence order.Specifically, we employ curriculum learning to carefully structure the learning process for DS agents and accumulate knowledge from the solutions to previous tasks.This approach mirrors the hierarchical nature of data science problem-solving, where complex solutions often build upon simpler concepts.By systematically increasing the difficulty of tasks, our framework enables the agent to develop the skills necessary to tackle more complicated challenges within the data science domain.We evaluate our framework using Claude-3.5-Sonnetand Llama-3.1-70b on two popular data science benchmarks: DSEval (Zhang et al., 2024b) and QRData (Liu et al., 2024a).</p>
<p>Our main contributions can be summarized as follows:</p>
<p>• Curriculum-based performance improvement: We demonstrate the superior performance of DSMentor, which leverages an easy-to-hard curriculum, across established benchmarks in data analysis and causal reasoning tasks.Our framework surpasses other data science agents by systematically guiding the learning process, enabling more effective problem-solving, even for complex tasks that require synthesizing advanced knowledge from multiple simpler problems.• Enhanced knowledge utilization: We show that retrieving easier and relevant examples from memory and structuring the knowledge in an increasing-similarity order, significantly improves DS agents' understanding and allows them to more efficiently leverage prior experiences for solving new tasks.• Progressive learning for solving advanced causal problems: We demonstrate how</p>
<p>DSMentor incrementally introduces tasks with increasing complexity, facilitating the agent's progressive learning of causal relationships.This gradual exposure strengthens its causal reasoning abilities and enables it to effectively address complex data relationships.</p>
<p>RELATED WORKS</p>
<p>Curriculum learning.Inspired by the human learning process, curriculum learning, first introduced by (Bengio et al., 2009), has become a widely-used approach in many applications, including computer vision, natural language processing, and reinforcement learning (Wei et al., 2016;Zhang et al., 2019;Wang et al., 2021;Portelas et al., 2021).This approach involves organizing training examples in a sequence from easier to harder tasks, which facilitates convergence of the training process and improves the quality of the learned models (Hacohen &amp; Weinshall, 2019).Determining task difficulty is a critical factor in enhancing training efficiency (Wang et al., 2021) and can be categorized into pre-defined difficulty (Platanios et al., 2019;Shi et al., 2023;Lu et al., 2024;Liu et al., 2024b) and automatic difficulty measurement (Portelas et al., 2021;Wang et al., 2024;Sun et al., 2024).For instance, the Cross-Episodic Curriculum (CEC) method (Shi et al., 2023) structures learning experiences based on factors such as the pre-defined task difficulty or demonstrator expertise, leveraging cross-episodic attention in Transformers.Recent works (Wang et al., 2024;Sun et al., 2024) employ large language models to automatically generate tasks with a diverse range of difficulties.In this paper, we leverage the LLM-based Mentor agent to automatically assess task difficulty and systematically construct a curriculum-based dataset that progresses from simpler to more complex tasks.Instead of focusing on the training stage, this paper incorporates curriculum learning with LLM agents during inference, emphasizing the impact of curriculum on the growth of long-term memory and the examples retrieved from online memory.</p>
<p>Data science LLM agents.LLMs have become increasingly valuable in data science, including OpenAI's Codex (Chen et al., 2021a) and Anthropic's Claude (Anthropic, 2023) assisting in data preprocessing, analysis, and code generation through natural language interfaces (Biswas et al., 2023).However, their static nature, where knowledge is fixed at the time of training, limits their ability to handle real-time data.To address this, recently proposed agents such as the Data Interpreter (Hong et al., 2024a), MLCopilot (Zhang et al., 2024a), and AIDE (Schmidt et al., 2024) extend LLM capabilities by incorporating dynamic planning, human-like reasoning, and iterative refinement, significantly enhancing performance in data science problem-solving.Our work builds on these advancements by incorporating curriculum learning and retrieval-based techniques to further enhance LLM agents' adaptability in data science tasks.</p>
<p>Multi-agent LLM frameworks.LLMs have seen growing application in multi-agent systems, where multiple agents collaborate to solve complex tasks through communication and cooperation.Frameworks like CAMEL (Li et al., 2023) and MetaGPT (Hong et al., 2024b) demonstrate how multi-agent LLM systems can decompose large projects into modular sub-tasks, allowing specialized agents to handle distinct components of the problem.Moreover, curriculum learning and retrieval-augmented techniques are increasingly integrated into these systems to enhance adaptability and efficiency, as shown in recent work focused on game-play agents (Wang et al., 2024).Additionally, studies on multi-agent debate systems reveal that agent interaction can significantly improve reasoning abilities (Wang et al., 2023;Khan et al., 2024).In this work, we propose a framework where a Mentor agent assists a Student agent, which enhances the student agent's problem-solving skills in data science tasks.</p>
<p>DSMENTOR: A MENTOR-GUIDED APPROACH</p>
<p>In this section, we present DSMentor, a novel mentor-guided approach that leverages curriculum learning and a growing online memory to enhance the capabilities of DS agents for solving data science tasks.As illustrated in Figure 2, DSMentor operates in two stages: the curriculum-generation stage and the problem-solving stage.In the following subsections, we will discuss the details about these two stages and explain the main components of DSMentor as well as how they work together to facilitate effective learning.</p>
<p>CURRICULUM-GENERATION STAGE</p>
<p>Given any data science task dataset D, we first employ a Mentor agent to construct a curriculumbased counterpart D c .This curriculum establishes the sequence of the tasks that will be undertaken during the problem-solving stage.In this paper, we focus on a difficulty-based curriculum and the preparation of a curriculum-based dataset comprises two key steps.</p>
<p>Step 1: Determine difficulty.First, the Mentor agent assesses and assigns a difficulty level to each problem.In our setting, we rely only on problem descriptions to gauge the difficulty of each problem, since the ground-truth code or evaluation results are generally not available to the Mentor agent during this stage in many real world scenarios.To ensure consistency and scalability across a diverse set of DS tasks, we provide difficulty scale guidelines for the Mentor agent to analyze and assign difficulty levels.As the examples illustrated in Figure 3, the Mentor agent can tell that the task of finding the largest single number is more difficult than identifying big counties based on specific conditions.The Mentor agent also provides reasons that the former requires counting occurrences within the data, while the latter only involves filtering a DataFrame satisfying the required conditions.</p>
<p>Step 2: Generate curriculum.Once the difficulty level for each task is determined, we structure the curriculum by arranging the tasks in a sequence that progresses from easier to more challenging problems.Together with the long-term memory described later, this ordering allows the agent to build foundational skills on basic tasks before advancing to more complex ones.For example, as shown in Figure 3, the Student agent will first tackle the task of finding big countries, followed by the task of identifying the largest single number, based on their respective difficulty levels.</p>
<p>We also explore the effectiveness of other difficulty metrics and alternative curricula, which will be detailed in 4.4 for comprehensive ablation studies.</p>
<p>PROBLEM-SOLVING STAGE</p>
<p>With the curriculum-based dataset D c , we utilize a Student agent that iteratively solves data science problems, progressing from easier to more complex ones.Inspired by verbal reinforcement learning (Shinn et al., 2024), we introduce a growing online long-term memory module that enables the Student agent to retain and retrieve knowledge from previously tackled, less complex problems, without necessitating updates to the model weights.</p>
<p>Online long-term memory: Assume that there are N problems in the dataset D c .For each problem i ∈ {1, . . ., N }, we denote its description as p i , where i is the index of p i in the sequence of problems.The long-term memory can be formally defined as
M i = {(p k , c k , t k )} i−1 k=1
, where p k and c k are the problem description and the corresponding generated code of some previous problem k &lt; i, and t k ∈ {Correct, Incorrect} is the evaluation tag for problem k.</p>
<p>Step 3: Retrieve from online long-term memory.While solving the i-th problem in the curriculum-based dataset D c , the Student agent can retrieve examples from long-term memory M i based on the similarity between the current problem description p i and previous description p k ∈ M i , according to their cosine similarity: sim(p i , p k ) = cos(E(p i ), E(p k )), where E is the embedding model.We finally retrieve examples with the top-K similarities from the long-term memory, where K is the number of the retrieved examples.The examples include both correctly and incorrectly solved problems along with their corresponding generated code.</p>
<p>Step 4: Answer DS problem by generating codes.Using the retrieved examples, the Student agent follows the order of increasing similarity.By learning from prior experiences, the Student agent attempts to solve the current data science problem by generating code, which is then evaluated by the environment based on its execution output.</p>
<p>Step 5: Accumulate knowledge to long-term memory.After obtaining the evaluation results, the Student agent appends the question, the generated code, and the evaluation tag (i.e., correct or incorrect) to the long-term memory for future use.As the inference process continues, the long-term memory will keep expanding.</p>
<p>EXPERIMENTS</p>
<p>In this section, we evaluate the proposed DSMentor on two different data-science benchmarks to address the following research questions: (Q1) How does DSMentor compare with other popular data science agents?(Q2) Can DSMentor enhance the ability to solve more difficult problems, like causal reasoning?(Q3) What is most suitable curriculum design that yields the best performance?</p>
<p>In the sequel, we first attempt to answer (Q1) and (Q2) in Section 4.3 and conduct a series of ablation studies to explore different curriculum designs in Section 4.4 for (Q3).</p>
<p>EXPERIMENT SETUP</p>
<p>Benchmarks and datasets.To answer the aforementioned questions, we conducted experiments on DSEval (Zhang et al., 2024b) and QRData (Liu et al., 2024a), which contains 705 problem-sets with 1236 questions in total.More specifically, DSEval consists of four datasets: LeetCode (with 40 single-turn problem-sets), StackOverflow (with 202 single-turn problem-sets), Exercise (with 21 multiple-turn problem-sets), and Kaggle (with 31 multiple-turn problem-sets), while QRData are composed of 411 statistical and causal reasoning problems.More details are summarized in Appendix of the supplementary material, where we address certain DSEval evaluation issues that lead to an underestimation of agent capabilities.</p>
<p>Implementation details.We equip DSMentor with anthropic.claude-3-5-sonnet-20240620-v1:0and meta.llama3-1-70b-instruct-v1:0 as the base LLMs, referred to as DSMentor-Claude-3.5-Sonnet and DSMentor-Llama-3.1-70b,respectively.Both the Mentor and Student agents use the same base model.Moreover, the Mentor agent follows the problem-based difficulty and the Student agent retrieves similar examples from a long-term memory storing previously answered questions and generated codes (both correct and incorrect).We employ cohere.embed-english-v3as the embedding to retrieve relevant examples from the memory.We set the number of retrieved examples as listed in Appendix of the supplementary material, which will be discussed with in detail in Section 4.4.3.During inference, we run DSMentor with easy-to-hard or hard-to-easy curriculum for three times and DSMentor with random curriculum for five times to mitigate randomness, reporting average results.The temperature is set to zero for more deterministic behavior.</p>
<p>Evaluation metrics.For DSEval, we adopt the Pass Rate metric from Zhang et al. (2024b), which is the ratio of correctly answered questions to the total number of questions, without considering any variable violation issues and error propagation for multiple-turn problem-sets.For QRData, we directly evaluate by comparing the execution output of the generated code against the ground-truth results, which can be either numerical or multiple-choice.</p>
<p>BASELINES</p>
<p>We compare DSMentor to the following baseline agents on DSEval (Zhang et al., 2024b) and QRData (Liu et al., 2024a).These agents are selected for their leading performance on the corresponding benchmarks.(Zhang et al., 2024a).</p>
<p>CoML (Zhang et al., 2024a) leverages offline data containing pre-existing tasks and distills the related knowledge to enhance performance during the online stage.It achieves the best performance on DSEval benchmarks, except DSEval-LeetCode compared to other agents (JupyterLab, 2023;chapyter, 2023;shroominic., 2023), which makes it an important baseline for comparison.</p>
<p>Vanilla agents with Program-of-Thoughts (PoT) (Chen et al., 2023) solve tasks by generating Python code, with the executed output serving as the solution.For each benchmark, Llama-3.1-70b and Claude-3.5-Sonnetact as vanilla agents, using the same system instructions as DSMentor to generate code (see in the supplementary material).Additionally for QRData, we include GPT-4 (Cheng et al., 2023) with PoT as one of the baseline agents, as it performs best among PoT-style agents reported in Liu et al. (2024a).To ensure a fair and consistent comparison, we focus on single-turn code generation alternatives rather than ReAct-style (Yao et al., 2022) or other agents that involve multi-turn reasoning and error feedback.</p>
<p>RESULTS AND DISCUSSION</p>
<p>Baselines vs DSMentor.As shown in Table 1 and 2, DSMentor-Claude-3.5-Sonnetconsistently outperforms baseline agents across multiple datasets.Compared to prior art Jupyter-AI and CoML, it achieves significant improvements, including an 8.8% increase on DSEval-Exercise and and 8.3% boost on DSEval-Kaggle.DSMentor-Llama-3.1-70b also shows notable gains, with an 5.9% improvement on DSEval-LeetCode and a 4.3% improvement on DSEval-SO.Despite performing slightly worse on DSEval-Exercise compared to CoML with Llama-3.1-70b,DSMentor demonstrates strong performance on QRData as shwon in Table 2, which contains larger and more difficulty problem sets.</p>
<p>Online long-term memory.Compared to the vanilla Llama-3.1-70b and Claude-3.5-Sonnetusing PoT prompts, DSMentor, equipped with the same LLM, significantly improves the pass rate on both DSEval and QRData.Such improvements are driven by the presence of online long-term memory and curriculum learning.Furthermore, they are especially noticeable on more difficult Causal reasoning tasks.Causal reasoning is considered more challenging than other statistical problems in QRData (Liu et al., 2024a).From Table 2, DSMentor-Claude-3.5-Sonnetachieves 45.6%, which improves GPT-4 (PoT) as the best baseline model by 8.8% on the causal problems.Such an improvement indicates that accumulating knowledge from easier questions is particularly effective for complex reasoning, especially in cases where understanding causal relationships is crucial.</p>
<p>In summary, DSMentor, incorporating with curriculum learning and long-term memory, shows a noticeable advantage over baseline models, particularly on complex datasets like DSEval and QRData.Our results underscore the importance of scalable curriculum strategies and long-term memory in improving AI-assisted data science performance.</p>
<p>ABLATIONS</p>
<p>Next, we conduct a series of ablation studies to examine the impacts of each components of DSMentor, regarding curriculum and long-term memory designs.</p>
<p>DIFFICULTY DEFINITION</p>
<p>Figuring out an appropriate difficulty measurement plays a critical role in our framework.In addition to the problem-based difficulty used in Section 4, where the LLM-based Mentor agent assesses difficulty based solely on each question, we evaluate three additional difficulty metrics and analyze their performance on DSEval-LeetCode and DSEval-SO.Below, we summarize the key concepts of these difficulty metrics, where more details are postponed to Appendix.</p>
<p>Manual difficulty (for DSEval-LeetCode only):</p>
<p>referring to the difficulty level (easy/medium/hard) as defined on LeetCode.Problems are first sorted by their original difficulty level, and in case of a tie, human pass rate is used to further rank them (i.e., a higher pass rate indicates an easier problem).</p>
<p>Reference-code-based difficulty: following the difficulty in DSEval (Zhang et al., 2024b), based on the number of functions, variables, conditions, and loops in the reference code.Pass-rate difficulty: employing the pass rate of weaker models, such as Llama-3.1-8b and Claude-3-Haiku, where a higher pass rate indicates an easier question.</p>
<p>To explore the impact of different difficulty definitions, we follow the same experiment setup described in Section 4.1 except the curriculum generation process that utilizes different difficulty metrics.We run each experiment for three times and present the average result in Table 3 and 4 to mitigate the randomness.As shown in Table 3 and 4, we first observe that incorporating with the curriculum designs significantly improves the overall pass rate for both DSMentor-Llama-3.1-70b and DSMentor-Claude-3.5-Sonnet on DSEval-LeetCode and SO, comparing to the baseline vanilla agent without memory and curriculum.Moreover, the problem-based difficulty generally outperforms the other difficulty metrics, although DSMentor-Llama-3.1-70b using the problembased difficulty performs slightly worse than when using the pass-rate generated by Llama-3.1-8B on DSEval-LeetCode.We further examine the correlation between problem-based difficulty and pass-rate difficulty that exclusively reflects each model's problem-solving abilities for each tasks.Due to page limits, we postpone the detailed discussion and results to Appendix.</p>
<p>ORDER OF TASKS AND EXAMPLES</p>
<p>In addition to difficulty metrics, we further explore the impact of different task orders and retrieved examples, both of which are key components of curriculum design.</p>
<p>First, we compare the easy-to-hard curriculum with the random and hard-to-easy curriculum, where the retrieved examples are ordered by increasing similarity (denoted as Inc.Similarity).As shown in Table 5 and 6, both DSMentor-Llama-3.1-70b and DSMentor-Claude-3.5-Sonnet perform better with the easy-to-hard or hard-to-easy curricula compared to the random curriculum, demonstrating the efficacy of curriculum learning.Additionally, the easy-to-hard curriculum often shows better performance than the hard-to-easy curriculum, except for DSEval-Kaggle, where the hard-to-easy curriculum performs slightly better by 0.4%.</p>
<p>Given that curriculum design influences performance through retrieved examples, we also investigate different ranking approaches for these examples.In addition to increasing similarity, we use increasing difficulty (denoted as Inc.Difficulty) for easy-to-hard curriculum.As seen in Table 5 and 6, our experimental results show that the easy-to-hard curriculum with examples ordered by increasing similarity often outperforms alternative approaches, except on DSEval-Kaggle.The impact of adding incorrect examples.Next, we further explore whether the Student agent could benefit from incorrect examples (i.e., previously answered questions that are incorrect).We evaluate DSMentor-Llama-3.1-70b on DSEval and QRData, both with and without incorrect examples retrieved from the long-term memory.Apart from incorporating incorrect examples, all experimental setups follows Section 4.1.Table 7 shows that enabling access to incorrect examples can enhance the pass rate, especially for datasets with fewer problem-sets (e.g., DSEval-LeetCode and DSEval-Exercise), though the datasets with a larger number of problem-sets (e.g., DSEval-SO, DSEval-Kaggle and QRData), the improvement from adding incorrect examples is marginal.This observation aligns with the principle that larger datasets generally possess a more extensive memory pool for retrieval, thereby increasing the likelihood of selecting sufficient correct examples without necessitating additional learning signals from incorrect instances.</p>
<p>CONCLUSION</p>
<p>In this work, we develop DSMentor, a framework that enhances data science agents through a mentor-guided curriculum learning approach, effectively improving their ability to tackle complex data science tasks.Extensive experiments demonstrate that organizing tasks from easy to hard significantly boosts the agent's problem-solving and causal reasoning capabilities by building a strong knowledge foundation during inference.Future directions include exploring adaptive curriculum that adjust task difficulty based on agent performance, incorporating advanced memory mechanisms for better knowledge retention, and extending DSMentor to support multi-agent collaboration in complex, interdisciplinary domains.</p>
<p>Figure 2 :
2
Figure 2: Our framework DSMentor.Here, the Mentor agent assesses the difficulty of each problem and generates a curriculum accordingly.Once the curriculum are determined, the Student agent-responsible for answering questions through code generation-retrieves relevant examples from an accumulated online long-term memory.After the environment evaluates the generated code, the Student agent will append the question, its output and evaluation tag (i.e., incorrect or correct answer), to the long-term memory.</p>
<p>Figure 3 :
3
Figure 3: Examples of determining difficulties during the curriculum-generation stage.</p>
<p>Comparison among different difficulty designs for DSMentor-Llama-3.1-70b,where the numbers of retrieved examples are 5 and 15 for DSEval-LeetCode and DSEval-SO respectively.We also report baseline numbers without curriculum in the last row.Comparison among different difficulty designs for DSMentor-Claude-3.5-Sonnet,where the number of retrieved examples is 5 for both DSEval-LeetCode and DSEval-SO.datasets, such as DSEval-Kaggle and QRData, where leveraging previous examples plays a crucial role in enhancing performance.</p>
<p>Figure 4 :
4
Figure 4: Performance of DSMentor models across different datasets on DSEval and QRData, with varying number of retrieved examples.The subfigures show the results for each dataset, demonstrating performance trends for Llama-3.1-70b and Claude-3.5-Sonnetmodels.</p>
<p>Table 1 :
1
Performance comparison of our DSMentor and existing data science models across DSEval-LeetCode, DSEval-SO, DSEval-Kaggle, and DSEval-Exercise.
Model (with Llama-3.1-70b)DSEval-LeetCode DSEval-SO DSEval-Exercise DSEval-KaggleJupyter-AI (JupyterLab, 2023)0.7330.4270.6790.465CoML (Zhang et al., 2024a)0.6830.7940.7830.577Llama-3.1-70b (PoT)0.7250.7390.7470.539DSMentor-Llama-3.1-70b0.7920.8370.7520.577Model (with Claude-3.5-Sonnet) DSEval-LeetCode DSEval-SO DSEval-Exercise DSEval-KaggleJupyter-AI (JupyterLab, 2023)0.8500.4770.6930.595CoML (Zhang et al., 2024a)0.8750.8090.6430.535Claude-3.5-Sonnet (PoT)0.8000.8040.7450.641DSMentor-Claude-3.5-Sonnet0.8920.8370.7810.678ModelPass Rate Multiple Choice/Numerical Statistical/CausalGPT-4 (PoT)0.4910.460/0.5400.725/0.368Llama-3.1-70b (PoT)0.4420.480/0.3840.615/0.351Claude-3.5-Sonnet (PoT)0.4710.495/0.4360.646/0.379DSMentor-Llama-3.1-70b0.5080.566/0.4190.676/0.419DSMentor-Claude-3.5-Sonnet0.5430.602/0.4520.707/0.456</p>
<p>Table 2 :
2
Performance comparison of our DSMentor and existing data science models on QRData, which includes the overall pass rate, along with breakdowns for multiple choice/numerical and statistical/causal questions.</p>
<p>Jupyter-AI(JupyterLab, 2023)is an open-source tool that enhances Jupyter Notebooks with generative AI capabilities.It demonstrates strong performance on DSEval-LeetCode, outperforming other data science agent frameworks such as Chapyter (chapyter, 2023), Code Interpreter (shroominic., 2023), and CoML</p>
<p>Table 5 :
5
Performance comparison of DSMentor-Llama-3.1-70b with different curriculum designs across DSEval.Here, Inc. Similarity and Inc.Difficulty represent that the retrieved examples are in the order of increasing similarity or increasing difficulty, respectively.
Curriculum DesignDSEval-LeetCode DSEval-SO DSEval-Exercise DSEval-KaggleEasy-to-Hard (Inc. Similarity)0.8920.8370.7810.678Easy-to-Hard (Inc. Difficulty)0.8500.8350.7740.684Hard-to-Easy (Inc. Similarity)0.8330.7970.7420.680Random (Inc. Similarity)0.8500.8000.7750.678</p>
<p>Table 6 :
6
Performance comparison of DSMentor-Claude-3.5-Sonnet with different curriculum designs across DSEval.</p>
<p>Table 7 :
7
DSMentor-Llama-3.1-70b with and without incorrectly answered examples.
DatasetIncorrect Examples Pass RateDSEval-LeetCode✗ ✓0.733 0.792DSEval-SO✗ ✓0.830 0.837DSEval-Exercise✗ ✓0.725 0.752DSEval-Kaggle✗ ✓0.562 0.577QRData✗ ✓0.475 0.508</p>
<p>A next-generation chatbot. Anthropic, Claude, Anthropic Blog. 2023</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Is ChatGPT a good data scientist? a preliminary study. Soham Biswas, Sukanta Bose, Bhaskar Mukherjee, arXiv:2301.103272023. chapyter. chapyter, 2023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique De Oliveira, Jared Pinto, Harri Kaplan, Yuri Edwards, Nicholas Burda, Greg Joseph, Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021aarXiv preprint</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021barXiv preprintEvaluating large language models trained on code</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Is GPT-4 a good data analyst?. Liying Cheng, Xingxuan Li, Lidong Bing, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Robotic space exploration agents. Steve Chien, Kiri L Wagstaff, Science robotics. 2748312017</p>
<p>Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, arXiv:1912.021642019arXiv preprint</p>
<p>DS-agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, Fortyfirst International Conference on Machine Learning. 2024</p>
<p>On the power of curriculum learning in training deep networks. Guy Hacohen, Daphna Weinshall, International conference on machine learning. PMLR2019</p>
<p>Deep learning for finance: deep portfolios. Applied Stochastic Models in Business and Industry. Nick G James B Heaton, Jan Hendrik Polson, Witte, 201733</p>
<p>Data interpreter: An LLM agent for data science. Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, arXiv:2402.186792024aarXiv preprint</p>
<p>MetaGPT: Meta programming for a multi-agent collaborative framework. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Jürgen Schmidhuber, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>Debating with more persuasive LLMs leads to more truthful answers. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, R Samuel, Tim Bowman, Ethan Rocktäschel, Perez, Forty-first International Conference on Machine Learning. 2024</p>
<p>DS-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, Tao Yu, International Conference on Machine Learning. PMLR2023</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 202336</p>
<p>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng, arXiv:2402.176442024aarXiv preprint</p>
<p>Let's learn step by step: Enhancing in-context learning ability with curriculum learning. Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, Wei Lu, arXiv:2402.107382024barXiv preprint</p>
<p>Jianqiao Lu, Wanjun Zhong, Yufei Wang, Zhijiang Guo, Qi Zhu, Wenyong Huang, Yanlin Wang, Fei Mi, Baojun Wang, Yasheng Wang, arXiv:2401.15670Teacher-student progressive learning for language models. 2024arXiv preprint</p>
<p>Competence-based curriculum learning for neural machine translation. Riccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, Joel T Dudley, Otilia Stretcu, Graham Neubig, Barnabás Poczós, Tom Mitchell, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies2018. 201919Emmanouil Antonios Platanios,</p>
<p>Automatic curriculum learning for deep rl: A short survey. Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, Pierre-Yves Oudeyer, IJCAI 2020-International Joint Conference on Artificial Intelligence. 2021</p>
<p>Aide: Human-level performance in data science competitions. weco.ai. D Schmidt, Z Jiang, Y Wu, 2024</p>
<p>Cross-episodic curriculum for transformer agents. Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby, Yuke Linxi "jim" Fan, Zhu, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 2024. shroominic. codeinterpreter-api. 202336</p>
<p>Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, Ruohui Huang, arXiv:2404.02823Conifer: Improving complex constrained instruction-following ability of large language models. 2024arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, Transactions on Machine Learning Research. 2835-88562024</p>
<p>Apollo's oracle: Retrieval-augmented reasoning in multi-agent debates. Haotian Wang, Xiyuan Du, Weijiang Yu, Qianglong Chen, Kun Zhu, Zheng Chu, Lian Yan, Yi Guan, arXiv:2312.048542023arXiv preprint</p>
<p>A survey on curriculum learning. Xin Wang, Yudong Chen, Wenwu Zhu, IEEE transactions on pattern analysis and machine intelligence. 202144</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Stc: A simple to complex framework for weakly-supervised semantic segmentation. Yunchao Wei, Xiaodan Liang, Yimin Chen, Xiaohui Shen, Ming-Ming Cheng, Jiashi Feng, Shuicheng Yan, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2016</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, Yuqing Yang, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational Linguistics2024a1</p>
<p>Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang, arXiv:2306.07209Data-copilot: Bridging billions of data and humans with autonomous workflow. 2023arXiv preprint</p>
<p>Leveraging deep learning with dynamic curriculum learning. Yuanjun Zhang, Ting Yao, Tao Mei, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2019</p>
<p>. Yuge Zhang, Qiyang Jiang, Xingyu Han, Nan Chen, Yuqing Yang, Kan Ren, arXiv:2402.171682024bBenchmarking data science agents. arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
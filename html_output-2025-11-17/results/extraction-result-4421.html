<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4421 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4421</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4421</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-267028418</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.09150v1.pdf" target="_blank">Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System</a></p>
                <p><strong>Paper Abstract:</strong> In the contemporary information era, significantly accelerated by the advent of Large-scale Language Models, the proliferation of scientific literature is reaching unprecedented levels. Researchers urgently require efficient tools for reading and summarizing academic papers, uncovering significant scientific literature, and employing diverse interpretative methodologies. To address this burgeoning demand, the role of automated scientific literature interpretation systems has become paramount. However, prevailing models, both commercial and open-source, confront notable challenges: they often overlook multimodal data, grapple with summarizing over-length texts, and lack diverse user interfaces. In response, we introduce an open-source multi-modal automated academic paper interpretation system (MMAPIS) with three-step process stages, incorporating LLMs to augment its functionality. Our system first employs the hybrid modality preprocessing and alignment module to extract plain text, and tables or figures from documents separately. It then aligns this information based on the section names they belong to, ensuring that data with identical section names are categorized under the same section. Following this, we introduce a hierarchical discourse-aware summarization method. It utilizes the extracted section names to divide the article into shorter text segments, facilitating specific summarizations both within and between sections via LLMs with specific prompts. Finally, we have designed four types of diversified user interfaces, including paper recommendation, multimodal Q\&A, audio broadcasting, and interpretation blog, which can be widely applied across various scenarios. Our qualitative and quantitative evaluations underscore the system's superiority, especially in scientific summarization, where it outperforms solutions relying solely on GPT-4.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4421.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4421.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMAPIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Modal Automated Academic Paper Interpretation System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multi-modal system that uses hybrid PDF preprocessing, section-aware alignment, and LLM-guided hierarchical summarization to produce multimodal summaries, Q&A, recommendations, audio broadcasts, and blog-style interpretations of academic papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MMAPIS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline system with three main modules: (1) Hybrid Modality Preprocessing & Alignment that extracts plaintext, formulas, figures and tables (using Nougat and PDFFigures 2.0) and aligns them by section; (2) Hierarchical Discourse-Aware Summarization that partitions a document by section titles, runs section-level LLM summarization with section-specific prompts, then performs a prompt-guided integration stage to produce a document-level summary (uses Chain-of-Thought for draft+refinement and Chain-of-Density for integration/regeneration); (3) Diverse Multimodal User Interfaces (paper recommendation, multimodal Q&A, audio broadcasting, interpretation blog) built on a Streamlit front-end.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (document-level integration and multimodal Q&A), GPT-3.5-Turbo (chart provenance / routing), ChatGPT (prompting for audio broadcast script generation); also generic unspecified LLMs for section summarization via prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Hybrid extraction: Nougat for visual/text/formula recognition; PDFFigures 2.0 for figures/tables + attribution; alignment by matching extracted objects to section title tokens to reconstruct a Markdown-like structured representation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical discourse-aware summarization: section-level LLM summaries using section-specific prompts followed by a prompt-guided integration stage to synthesize document-level summaries; uses Chain-of-Thought (CoT) for generation/refinement and Chain-of-Density (CoD) for integration to preserve entity density.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluation on 100 arXiv papers (50 from Dec 2017, 50 from Dec 2023) in the reported experiments</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science (arXiv CS subsets used for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Section-level and document-level structured summaries, multimodal Q&A answers (including figures/tables), paper recommendation scores, audio broadcast scripts / MP3, interpretation blog posts integrating multimodal content.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>COD-style evaluation (Chain-of-Density-based metric) with five dimensions: Informative, Quality, Coherence, Attributable, Overall; scores produced by GPT-4 as a proxy rater (rather than ROUGE) for summary quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On the 100-paper testset MMAPIS is reported to outperform GPT-4 baseline 'in nearly all aspects' across the five COD dimensions; authors report notably higher Informative and Overall scores and denser entity-level information in outputs (qualitative and quantitative improvements asserted in tables). The system maintains average section lengths under ~1400 tokens (CS2023 avg section ~1364 tokens, CS2017 avg section ~1163 tokens) to mitigate 'lost-in-the-middle' effects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-4 (direct summarization baseline) and, implicitly, conventional truncation/extractive segmentation approaches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Authors report that MMAPIS surpasses GPT-4 across most COD dimensions on both CS2017 and CS2023 subsets (qualitative claim supported by numeric table comparisons in the paper; Informative and Overall dimensions highlighted as showing the biggest gains).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Preserving document structure and multimodal content (figures/tables/formulas) and performing section-aware segmentation before LLM summarization improves informativeness and entity density of summaries; using CoT for section generation and CoD for integration helps balance fluency and information density; segmentation reduces position-sensitivity issues ('lost-in-the-middle').</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Remaining hallucination risk from LLMs; dependence on external extractors (Nougat/PDFFigures) which may miss location-specific elements; current system focuses on single-document interpretation (inter-document synthesis / theory generation across many papers left to future work); computational cost for multimodal LLM queries; some degradation for more recent/longer documents.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors show that section-level segmentation keeps average chunk sizes below ~1400 tokens (vs full documents ~7.7k–9.1k tokens), mitigating long-context degradation. They observe performance degradation with increased document length and across newer datasets but claim hierarchical segmentation trades off speed and accuracy favorably; they note plans to scale to more dynamic/real-time processing and to incorporate inter-document links in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4421.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hierarchical Discourse-aware Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Discourse-Aware Summarization (section-level then integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage summarization method that first generates section-level summaries using section-specific prompts and LLMs, then integrates those summaries with a prompt-guided synthesis step to produce a cohesive document-level summary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hierarchical Discourse-Aware Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Stage 1: partition document using title/section cues and generate summaries per section with tailored prompts (or universal prompt if section type not in prompt set). Stage 2: prompt-guided integration merges section summaries into a document-level summary, augmenting with metadata (title, authors, affiliations) and using CoD to retain entity-level information.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Unspecified LLMs via prompt interface; experiments use GPT-4 for evaluation and for some integration tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Relies on structured text produced by preprocessing (section-aware Markdown) rather than raw OCR; segmentation based on explicit section titles.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Two-stage synthesis (section summaries → prompt-guided integration), employing Chain-of-Thought for draft+refine and Chain-of-Density for integration/regeneration to maximize information density.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to each individual paper; evaluated on 100-papers testbed in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific papers (evaluated on computer science arXiv subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Section summaries and integrated document-level summaries used for downstream applications (recommendation, Q&A, blog, audio).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>COD five-dimension evaluation via GPT-4 proxy (Informative, Quality, Coherence, Attributable, Overall).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to reduce information loss during segmentation and produce denser, more informative summaries compared to direct GPT-4 summarization baselines; authors report consistent gains across COD dimensions (quantitative table provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct end-to-end GPT-4 summarization/other truncation or sliding-window summarization methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Authors claim better performance than direct GPT-4 summarization on the testset; gains especially in informativeness and overall human-preference correlated measures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Document-structure-aware segmentation (per-section) better preserves semantics and reduces layout bias; combining CoT and CoD in different stages improves both fluency and entity retention.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not currently integrate multiple documents into a single synthesized theory; effectiveness depends on accurate section extraction and section-title alignment; still subject to LLM hallucinations during integration.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Method reduces effective input size per LLM call (section averages ~1.1k–1.4k tokens) which improves LLM reliability versus processing full documents (~7.7k–9.1k tokens); authors indicate trade-offs between granularity and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4421.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid Preprocessing & Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid Modality Preprocessing and Alignment Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing module that reconstructs PDFs into structured Markdown-like documents by combining visual-text extraction and figure/table mining, then aligning multimodal objects with section titles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid Modality Preprocessing and Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines Nougat (visual/text/formula extraction) and PDFFigures 2.0 (figure/table detection and attribution) to extract plaintext, formulas, figures and tables; aligns extracted objects to section titles/layout tokens to recreate a semantically structured representation for downstream LLM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Visual OCR-like neural extraction (Nougat) for images/formulas and PDFFigures' region-based detection for figures/tables; alignment by matching section-name keywords to attribution metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Produces structured inputs for later LLM summarization; no multi-document synthesis described here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific papers (general; evaluated on arXiv CS papers)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured Markdown-like document with aligned text, formulas, figures, and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Authors assert this approach preserves multimodal information and hierarchical discourse structure better than traditional OCR or PDF libraries; quantitative extraction metrics not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional OCR pipelines and PDF parsing libraries (line-by-line OCR or simple object-based PDF parsers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Presented qualitatively as superior in preserving position-sensitive modal details and section-level structure; no strict numeric comparison given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining neural visual text/formula extraction and figure mining yields richer inputs for LLM summarization and simplifies downstream segmentation/alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Extraction still imperfect for location-specific elements; reliant on quality of external tools (Nougat, PDFFigures); alignment heuristics may fail for some complex layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly analyzed for large collections; module is intended as preprocessing per-document prior to LLM summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4421.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-capacity commercial LLM from OpenAI used for multi-turn dialogue, document interpretation (including PDF uploads), multimodal Q&A, and used in this paper both as an evaluation proxy and as a component in the MMAPIS pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as (a) a baseline summarizer for direct end-to-end summarization of full papers and (b) as a component in MMAPIS for multimodal Q&A and final integration/evaluation scoring (GPT-4 used as human-proxy rater for COD evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (commercial OpenAI model)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Used with designed prompts and PDF upload functionality to interpret papers; the paper notes GPT-4's ability to accept long documents (up to large token limits) but also its position-sensitivity limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct summarization and answer synthesis via prompting; used as proxy evaluator to score summaries on COD dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used as baseline across the 100-paper evaluation set</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific papers (used as baseline for CS arXiv papers in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Document summaries and multimodal Q&A responses; also used to produce evaluation scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used as the rater for the COD evaluation (Informative, Quality, Coherence, Attributable, Overall).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to be outperformed by MMAPIS on most COD dimensions in the authors' experiments; performance shows position-sensitivity and degradation on very long documents or when 'lost-in-the-middle' phenomena occur.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implied comparison against segmented, structure-aware MMAPIS pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MMAPIS reported to surpass GPT-4 across nearly all COD dimensions on the testset (authors provide tabulated comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 is strong at end-to-end summarization but exhibits layout/position bias and struggles to reliably capture mid-document content in very long inputs; performs well when inputs are smaller or structured.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Position sensitivity ('lost in the middle'), hallucinations especially for technical/knowledge-dense dimensions, privacy/opacity concerns when uploading PDFs to third-party services.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Works with very large contexts but authors report U-shaped performance w.r.t. position (beginning/end favored) and decreased QA performance with very large token inputs; segmentation into ~1–2k token chunks improves reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4421.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI LLM used in MMAPIS for lightweight routing tasks (chart provenance identification) prior to invoking multimodal GPT-4 analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as a two-tier routing assistant: initially employed to parse user's question to detect which figure/table (index and section) the question refers to, enabling targeted multimodal queries to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based question classification/routing to identify figure/table provenance from user queries.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not used for final synthesis; used to select the correct image/section to forward to GPT-4 for multimodal interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied to multimodal Q&A over single documents within MMAPIS</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Classification/routing outputs (figure index and section id) to guide downstream multimodal queries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Lighter-weight models can be effective for routing/provenance tasks, reserving costlier multimodal LLM calls for the heavy interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not used for multimodal interpretation itself; accuracy of routing step can affect downstream answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed as low-cost routing; scales cheaply relative to invoking larger multimodal LLMs on every query.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4421.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatPDF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatPDF (GitHub project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source/third-party system referenced by the authors that parses PDFs (using PyPDFium2Loader) and enables chat-based interaction with documents, often backed by LLMs for Q&A and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatPDF</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a representative PDF-to-chat system built on a PDF parsing library (PyPDFium2Loader) that converts PDF content into text for downstream LLM-based summarization or Q&A.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>PDF parsing (PyPDFium2Loader) to extract text and embedded images; then feed into LLMs for summarization/Q&A according to the paper's related-work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Typically LLM-based summarization or QA over the extracted text (paper mentions such systems generally but does not detail ChatPDF's internal synthesis techniques).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific/academic PDFs</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries and chat-style Q&A responses about a single uploaded PDF</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representative of trend to use PDF parsers + LLMs for document interpretation; lacks multimodal alignment and discourse-aware segmentation described by MMAPIS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Authors note such libraries (and systems built on them) may miss location-specific info (formulas/tables) and typically ignore discourse integrity necessitating extra IR/alignment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4421.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatPaper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatPaper (GitHub project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that uses Grobid for PDF structure parsing and then uses LLMs for downstream summarization or interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatPaper</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of systems that utilize PDF parsing libraries (Grobid) to extract structured text and images, then feed these into summarization models or LLMs for interpretation; discussed in related work as lacking full multimodal/discourse alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Grobid-based PDF structure parsing to extract metadata and text chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based summarization/QA over extracted text (not specifically detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic papers (general)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries / chat-style interpretations of individual PDFs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Example of current tools that parse PDFs into text for LLM summarization but often neglect multimodal elements and discourse alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>PDF-parsing-only pipelines can miss formulas/tables and fail to preserve layout/sectional coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4421.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D2S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>D2S: Document-to-Slide generation via query-based summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A document-to-slide generation approach that uses query-based summarization to produce slide decks from documents; cited as an example of systems using structured summarization for downstream presentation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>D2S: Document-to-slide generation via query-based text summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>D2S</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of efficient PDF parsing + query-based summarization pipeline that forms structured outputs (slides) from documents; included to illustrate trends toward structured document summarization for downstream interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>PDF parsing and query-based text summarization (details in the cited D2S work).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Query/summary-driven content selection to create slide content from documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Document summarization / presentation generation (general academic docs)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Slide decks / condensed presentations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representative of structured summarization applied to downstream UIs; contrasted with MMAPIS which emphasizes multimodality and discourse-aware segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4421.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nougat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nougat: Neural Optical Understanding for Academic Documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural encoder-decoder transformer model for extracting plaintext and mathematical formulas from page screenshots and converting them to Markdown-like plain text, used here as the visual-text extraction component.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nougat: Neural optical understanding for academic documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nougat</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used in MMAPIS preprocessing to predict text information in PDF page screenshots (plain text and mathematical formulas) and output a Markdown-like textual representation, preserving hierarchical paragraph structure to facilitate downstream segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Neural visual-text recognition (encoder-decoder transformer for screenshot→text/formula extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Produces structured textual inputs for later LLM summarization; does not itself synthesize across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic document visual-text extraction</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Plaintext + mathematical formulas in Markdown-like structure</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional OCR engines (line-by-line) and PDF parsing libraries</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described qualitatively as preserving hierarchical paragraph relations and formulas better than traditional OCR; used to reduce downstream segmentation complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Visual-text models like Nougat help preserve discourse-level paragraph structure and mathematical content for LLM downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Extraction errors can still occur; accuracy depends on screenshot/image quality and model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4421.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDFFigures2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PDFFigures 2.0: Mining figures from research papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool that identifies figures and tables in PDFs by reasoning about empty text regions and returns images with attribution metadata, used in MMAPIS for extracting multimodal content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pdffigures 2.0: Mining figures from research papers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDFFigures 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applied to detect and extract figures and tables from PDF pages and provide attribution information that helps align these multimodal objects with document sections during preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Layout- and region-based figure/table detection and extraction with attribution metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Supplies image/table artifacts to the MMAPIS pipeline for LLM-enabled multimodal interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Research paper figure/table mining</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extracted figure/table images with attribution (section/page) metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Figure/table extraction with attribution reduces alignment difficulty for multimodal summarization systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May have limitations when images are embedded in non-standard ways; alignment heuristics still necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4421.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT/CoD prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought and Chain-of-Density prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt engineering approaches: CoT elicits stepwise reasoning and iterative refinement; CoD emphasizes information density for dense, entity-preserving summarization; both are used within MMAPIS to improve section generation and integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From sparse to dense: GPT-4 summarization with chain of density prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoT (Chain-of-Thought) / CoD (Chain-of-Density) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompts are split into Task Description, Current Input, and Output Indicator; CoT is used for section-level generation with draft→self-review→refine steps to enhance fluency and integrity; CoD is applied during merging/regeneration to retain entities and maximize information density in the integrated summary.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied to GPT-family LLMs in the pipeline (GPT-4 for integration/evaluation; CoT/CoD methods referenced from recent literature)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>N/A (prompting technique used for generation/synthesis rather than raw extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Used to guide multi-step LLM generation workflows (draft, self-review, refinement) and to control density during integration.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Prompting strategies for summarization and integration in scientific document interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>More robust, denser summaries and improved integrated outputs when applied in the MMAPIS pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Authors attribute improved informativeness and entity density in section/integration outputs to the combined use of CoT for section drafts and CoD for integration; quantitative improvements summarized in the paper's COD evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard single-pass prompts or non-CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reportedly better than non-chain prompting at preserving entities and producing denser, higher-rated summaries (per authors' experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixing CoT for introspective refinement and CoD for density-preserving integration balances fluency with information retention for multi-stage summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Chain-based prompting increases prompt design complexity and can be more computationally expensive due to multi-step generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Used to manage information density when merging many section summaries; effectiveness depends on number/length of sections and LLM cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4421.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4421.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DocsGPT / Chat2Doc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DocsGPT / Chat2Doc (open-source tools)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned open-source projects that wrap PDF/document parsing and LLM-based QA/summarization into chat-like interfaces for document understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DocsGPT / Chat2Doc</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as examples of document-chat systems that integrate document parsing with LLMs to provide conversational access to document contents; referenced in related-work as part of the ecosystem of PDF-to-LLM tools.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>PDF parsing and text extraction (implementation dependent per project).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based summarization and QA over extracted document contents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General document/Q&A tools</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Chat-style answers and summaries for individual documents</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representative of the trend to combine PDF parsing with LLMs; typically do not provide the deeper multimodal + discourse-aware processing that MMAPIS targets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May lack multimodal alignment and discourse-aware segmentation; potential privacy/opaque model concerns when using third-party LLM APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Nougat: Neural optical understanding for academic documents <em>(Rating: 2)</em></li>
                <li>Pdffigures 2.0: Mining figures from research papers <em>(Rating: 2)</em></li>
                <li>D2S: Document-to-slide generation via query-based text summarization <em>(Rating: 2)</em></li>
                <li>From sparse to dense: GPT-4 summarization with chain of density prompting <em>(Rating: 2)</em></li>
                <li>Lost in the middle: How language models use long contexts <em>(Rating: 2)</em></li>
                <li>Bringing structure into summaries: a faceted summarization dataset for long scientific documents <em>(Rating: 2)</em></li>
                <li>ChatPDF (GitHub project) <em>(Rating: 1)</em></li>
                <li>ChatPaper (GitHub project) <em>(Rating: 1)</em></li>
                <li>DocsGPT (open-source project) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4421",
    "paper_id": "paper-267028418",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "MMAPIS",
            "name_full": "Multi-Modal Automated Academic Paper Interpretation System",
            "brief_description": "An open-source multi-modal system that uses hybrid PDF preprocessing, section-aware alignment, and LLM-guided hierarchical summarization to produce multimodal summaries, Q&A, recommendations, audio broadcasts, and blog-style interpretations of academic papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MMAPIS",
            "system_description": "Pipeline system with three main modules: (1) Hybrid Modality Preprocessing & Alignment that extracts plaintext, formulas, figures and tables (using Nougat and PDFFigures 2.0) and aligns them by section; (2) Hierarchical Discourse-Aware Summarization that partitions a document by section titles, runs section-level LLM summarization with section-specific prompts, then performs a prompt-guided integration stage to produce a document-level summary (uses Chain-of-Thought for draft+refinement and Chain-of-Density for integration/regeneration); (3) Diverse Multimodal User Interfaces (paper recommendation, multimodal Q&A, audio broadcasting, interpretation blog) built on a Streamlit front-end.",
            "llm_model_used": "GPT-4 (document-level integration and multimodal Q&A), GPT-3.5-Turbo (chart provenance / routing), ChatGPT (prompting for audio broadcast script generation); also generic unspecified LLMs for section summarization via prompts.",
            "extraction_technique": "Hybrid extraction: Nougat for visual/text/formula recognition; PDFFigures 2.0 for figures/tables + attribution; alignment by matching extracted objects to section title tokens to reconstruct a Markdown-like structured representation.",
            "synthesis_technique": "Hierarchical discourse-aware summarization: section-level LLM summaries using section-specific prompts followed by a prompt-guided integration stage to synthesize document-level summaries; uses Chain-of-Thought (CoT) for generation/refinement and Chain-of-Density (CoD) for integration to preserve entity density.",
            "number_of_papers": "Evaluation on 100 arXiv papers (50 from Dec 2017, 50 from Dec 2023) in the reported experiments",
            "domain_or_topic": "Computer science (arXiv CS subsets used for evaluation)",
            "output_type": "Section-level and document-level structured summaries, multimodal Q&A answers (including figures/tables), paper recommendation scores, audio broadcast scripts / MP3, interpretation blog posts integrating multimodal content.",
            "evaluation_metrics": "COD-style evaluation (Chain-of-Density-based metric) with five dimensions: Informative, Quality, Coherence, Attributable, Overall; scores produced by GPT-4 as a proxy rater (rather than ROUGE) for summary quality.",
            "performance_results": "On the 100-paper testset MMAPIS is reported to outperform GPT-4 baseline 'in nearly all aspects' across the five COD dimensions; authors report notably higher Informative and Overall scores and denser entity-level information in outputs (qualitative and quantitative improvements asserted in tables). The system maintains average section lengths under ~1400 tokens (CS2023 avg section ~1364 tokens, CS2017 avg section ~1163 tokens) to mitigate 'lost-in-the-middle' effects.",
            "comparison_baseline": "GPT-4 (direct summarization baseline) and, implicitly, conventional truncation/extractive segmentation approaches",
            "performance_vs_baseline": "Authors report that MMAPIS surpasses GPT-4 across most COD dimensions on both CS2017 and CS2023 subsets (qualitative claim supported by numeric table comparisons in the paper; Informative and Overall dimensions highlighted as showing the biggest gains).",
            "key_findings": "Preserving document structure and multimodal content (figures/tables/formulas) and performing section-aware segmentation before LLM summarization improves informativeness and entity density of summaries; using CoT for section generation and CoD for integration helps balance fluency and information density; segmentation reduces position-sensitivity issues ('lost-in-the-middle').",
            "limitations_challenges": "Remaining hallucination risk from LLMs; dependence on external extractors (Nougat/PDFFigures) which may miss location-specific elements; current system focuses on single-document interpretation (inter-document synthesis / theory generation across many papers left to future work); computational cost for multimodal LLM queries; some degradation for more recent/longer documents.",
            "scaling_behavior": "Authors show that section-level segmentation keeps average chunk sizes below ~1400 tokens (vs full documents ~7.7k–9.1k tokens), mitigating long-context degradation. They observe performance degradation with increased document length and across newer datasets but claim hierarchical segmentation trades off speed and accuracy favorably; they note plans to scale to more dynamic/real-time processing and to incorporate inter-document links in future work.",
            "uuid": "e4421.0",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Hierarchical Discourse-aware Summarization",
            "name_full": "Hierarchical Discourse-Aware Summarization (section-level then integration)",
            "brief_description": "A two-stage summarization method that first generates section-level summaries using section-specific prompts and LLMs, then integrates those summaries with a prompt-guided synthesis step to produce a cohesive document-level summary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hierarchical Discourse-Aware Summarization",
            "system_description": "Stage 1: partition document using title/section cues and generate summaries per section with tailored prompts (or universal prompt if section type not in prompt set). Stage 2: prompt-guided integration merges section summaries into a document-level summary, augmenting with metadata (title, authors, affiliations) and using CoD to retain entity-level information.",
            "llm_model_used": "Unspecified LLMs via prompt interface; experiments use GPT-4 for evaluation and for some integration tasks.",
            "extraction_technique": "Relies on structured text produced by preprocessing (section-aware Markdown) rather than raw OCR; segmentation based on explicit section titles.",
            "synthesis_technique": "Two-stage synthesis (section summaries → prompt-guided integration), employing Chain-of-Thought for draft+refine and Chain-of-Density for integration/regeneration to maximize information density.",
            "number_of_papers": "Applied to each individual paper; evaluated on 100-papers testbed in the study.",
            "domain_or_topic": "General scientific papers (evaluated on computer science arXiv subsets)",
            "output_type": "Section summaries and integrated document-level summaries used for downstream applications (recommendation, Q&A, blog, audio).",
            "evaluation_metrics": "COD five-dimension evaluation via GPT-4 proxy (Informative, Quality, Coherence, Attributable, Overall).",
            "performance_results": "Reported to reduce information loss during segmentation and produce denser, more informative summaries compared to direct GPT-4 summarization baselines; authors report consistent gains across COD dimensions (quantitative table provided).",
            "comparison_baseline": "Direct end-to-end GPT-4 summarization/other truncation or sliding-window summarization methods",
            "performance_vs_baseline": "Authors claim better performance than direct GPT-4 summarization on the testset; gains especially in informativeness and overall human-preference correlated measures.",
            "key_findings": "Document-structure-aware segmentation (per-section) better preserves semantics and reduces layout bias; combining CoT and CoD in different stages improves both fluency and entity retention.",
            "limitations_challenges": "Does not currently integrate multiple documents into a single synthesized theory; effectiveness depends on accurate section extraction and section-title alignment; still subject to LLM hallucinations during integration.",
            "scaling_behavior": "Method reduces effective input size per LLM call (section averages ~1.1k–1.4k tokens) which improves LLM reliability versus processing full documents (~7.7k–9.1k tokens); authors indicate trade-offs between granularity and computational cost.",
            "uuid": "e4421.1",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Hybrid Preprocessing & Alignment",
            "name_full": "Hybrid Modality Preprocessing and Alignment Module",
            "brief_description": "A preprocessing module that reconstructs PDFs into structured Markdown-like documents by combining visual-text extraction and figure/table mining, then aligning multimodal objects with section titles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hybrid Modality Preprocessing and Alignment",
            "system_description": "Combines Nougat (visual/text/formula extraction) and PDFFigures 2.0 (figure/table detection and attribution) to extract plaintext, formulas, figures and tables; aligns extracted objects to section titles/layout tokens to recreate a semantically structured representation for downstream LLM processing.",
            "llm_model_used": null,
            "extraction_technique": "Visual OCR-like neural extraction (Nougat) for images/formulas and PDFFigures' region-based detection for figures/tables; alignment by matching section-name keywords to attribution metadata.",
            "synthesis_technique": "Produces structured inputs for later LLM summarization; no multi-document synthesis described here.",
            "number_of_papers": null,
            "domain_or_topic": "Scientific papers (general; evaluated on arXiv CS papers)",
            "output_type": "Structured Markdown-like document with aligned text, formulas, figures, and tables.",
            "evaluation_metrics": null,
            "performance_results": "Authors assert this approach preserves multimodal information and hierarchical discourse structure better than traditional OCR or PDF libraries; quantitative extraction metrics not provided in the paper.",
            "comparison_baseline": "Traditional OCR pipelines and PDF parsing libraries (line-by-line OCR or simple object-based PDF parsers)",
            "performance_vs_baseline": "Presented qualitatively as superior in preserving position-sensitive modal details and section-level structure; no strict numeric comparison given in the paper.",
            "key_findings": "Combining neural visual text/formula extraction and figure mining yields richer inputs for LLM summarization and simplifies downstream segmentation/alignment.",
            "limitations_challenges": "Extraction still imperfect for location-specific elements; reliant on quality of external tools (Nougat, PDFFigures); alignment heuristics may fail for some complex layouts.",
            "scaling_behavior": "Not explicitly analyzed for large collections; module is intended as preprocessing per-document prior to LLM summarization.",
            "uuid": "e4421.2",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large-capacity commercial LLM from OpenAI used for multi-turn dialogue, document interpretation (including PDF uploads), multimodal Q&A, and used in this paper both as an evaluation proxy and as a component in the MMAPIS pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4",
            "system_description": "Used as (a) a baseline summarizer for direct end-to-end summarization of full papers and (b) as a component in MMAPIS for multimodal Q&A and final integration/evaluation scoring (GPT-4 used as human-proxy rater for COD evaluation).",
            "llm_model_used": "GPT-4 (commercial OpenAI model)",
            "extraction_technique": "Used with designed prompts and PDF upload functionality to interpret papers; the paper notes GPT-4's ability to accept long documents (up to large token limits) but also its position-sensitivity limitations.",
            "synthesis_technique": "Direct summarization and answer synthesis via prompting; used as proxy evaluator to score summaries on COD dimensions.",
            "number_of_papers": "Used as baseline across the 100-paper evaluation set",
            "domain_or_topic": "General scientific papers (used as baseline for CS arXiv papers in this work)",
            "output_type": "Document summaries and multimodal Q&A responses; also used to produce evaluation scores.",
            "evaluation_metrics": "Used as the rater for the COD evaluation (Informative, Quality, Coherence, Attributable, Overall).",
            "performance_results": "Reported to be outperformed by MMAPIS on most COD dimensions in the authors' experiments; performance shows position-sensitivity and degradation on very long documents or when 'lost-in-the-middle' phenomena occur.",
            "comparison_baseline": "Implied comparison against segmented, structure-aware MMAPIS pipeline",
            "performance_vs_baseline": "MMAPIS reported to surpass GPT-4 across nearly all COD dimensions on the testset (authors provide tabulated comparison).",
            "key_findings": "GPT-4 is strong at end-to-end summarization but exhibits layout/position bias and struggles to reliably capture mid-document content in very long inputs; performs well when inputs are smaller or structured.",
            "limitations_challenges": "Position sensitivity ('lost in the middle'), hallucinations especially for technical/knowledge-dense dimensions, privacy/opacity concerns when uploading PDFs to third-party services.",
            "scaling_behavior": "Works with very large contexts but authors report U-shaped performance w.r.t. position (beginning/end favored) and decreased QA performance with very large token inputs; segmentation into ~1–2k token chunks improves reliability.",
            "uuid": "e4421.3",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5 Turbo",
            "brief_description": "An earlier OpenAI LLM used in MMAPIS for lightweight routing tasks (chart provenance identification) prior to invoking multimodal GPT-4 analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-3.5-Turbo",
            "system_description": "Used as a two-tier routing assistant: initially employed to parse user's question to detect which figure/table (index and section) the question refers to, enabling targeted multimodal queries to GPT-4.",
            "llm_model_used": "GPT-3.5-Turbo",
            "extraction_technique": "Prompt-based question classification/routing to identify figure/table provenance from user queries.",
            "synthesis_technique": "Not used for final synthesis; used to select the correct image/section to forward to GPT-4 for multimodal interpretation.",
            "number_of_papers": null,
            "domain_or_topic": "Applied to multimodal Q&A over single documents within MMAPIS",
            "output_type": "Classification/routing outputs (figure index and section id) to guide downstream multimodal queries.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Lighter-weight models can be effective for routing/provenance tasks, reserving costlier multimodal LLM calls for the heavy interpretation.",
            "limitations_challenges": "Not used for multimodal interpretation itself; accuracy of routing step can affect downstream answer quality.",
            "scaling_behavior": "Designed as low-cost routing; scales cheaply relative to invoking larger multimodal LLMs on every query.",
            "uuid": "e4421.4",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ChatPDF",
            "name_full": "ChatPDF (GitHub project)",
            "brief_description": "An open-source/third-party system referenced by the authors that parses PDFs (using PyPDFium2Loader) and enables chat-based interaction with documents, often backed by LLMs for Q&A and summarization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ChatPDF",
            "system_description": "Mentioned as a representative PDF-to-chat system built on a PDF parsing library (PyPDFium2Loader) that converts PDF content into text for downstream LLM-based summarization or Q&A.",
            "llm_model_used": null,
            "extraction_technique": "PDF parsing (PyPDFium2Loader) to extract text and embedded images; then feed into LLMs for summarization/Q&A according to the paper's related-work discussion.",
            "synthesis_technique": "Typically LLM-based summarization or QA over the extracted text (paper mentions such systems generally but does not detail ChatPDF's internal synthesis techniques).",
            "number_of_papers": null,
            "domain_or_topic": "General scientific/academic PDFs",
            "output_type": "Summaries and chat-style Q&A responses about a single uploaded PDF",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Representative of trend to use PDF parsers + LLMs for document interpretation; lacks multimodal alignment and discourse-aware segmentation described by MMAPIS.",
            "limitations_challenges": "Authors note such libraries (and systems built on them) may miss location-specific info (formulas/tables) and typically ignore discourse integrity necessitating extra IR/alignment steps.",
            "scaling_behavior": null,
            "uuid": "e4421.5",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ChatPaper",
            "name_full": "ChatPaper (GitHub project)",
            "brief_description": "A referenced system that uses Grobid for PDF structure parsing and then uses LLMs for downstream summarization or interaction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ChatPaper",
            "system_description": "Cited as an example of systems that utilize PDF parsing libraries (Grobid) to extract structured text and images, then feed these into summarization models or LLMs for interpretation; discussed in related work as lacking full multimodal/discourse alignment.",
            "llm_model_used": null,
            "extraction_technique": "Grobid-based PDF structure parsing to extract metadata and text chunks.",
            "synthesis_technique": "LLM-based summarization/QA over extracted text (not specifically detailed in this paper).",
            "number_of_papers": null,
            "domain_or_topic": "Academic papers (general)",
            "output_type": "Summaries / chat-style interpretations of individual PDFs",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Example of current tools that parse PDFs into text for LLM summarization but often neglect multimodal elements and discourse alignment.",
            "limitations_challenges": "PDF-parsing-only pipelines can miss formulas/tables and fail to preserve layout/sectional coherence.",
            "scaling_behavior": null,
            "uuid": "e4421.6",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "D2S",
            "name_full": "D2S: Document-to-Slide generation via query-based summarization",
            "brief_description": "A document-to-slide generation approach that uses query-based summarization to produce slide decks from documents; cited as an example of systems using structured summarization for downstream presentation tasks.",
            "citation_title": "D2S: Document-to-slide generation via query-based text summarization",
            "mention_or_use": "mention",
            "system_name": "D2S",
            "system_description": "Referenced as an example of efficient PDF parsing + query-based summarization pipeline that forms structured outputs (slides) from documents; included to illustrate trends toward structured document summarization for downstream interfaces.",
            "llm_model_used": null,
            "extraction_technique": "PDF parsing and query-based text summarization (details in the cited D2S work).",
            "synthesis_technique": "Query/summary-driven content selection to create slide content from documents.",
            "number_of_papers": null,
            "domain_or_topic": "Document summarization / presentation generation (general academic docs)",
            "output_type": "Slide decks / condensed presentations",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Representative of structured summarization applied to downstream UIs; contrasted with MMAPIS which emphasizes multimodality and discourse-aware segmentation.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4421.7",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Nougat",
            "name_full": "Nougat: Neural Optical Understanding for Academic Documents",
            "brief_description": "A neural encoder-decoder transformer model for extracting plaintext and mathematical formulas from page screenshots and converting them to Markdown-like plain text, used here as the visual-text extraction component.",
            "citation_title": "Nougat: Neural optical understanding for academic documents",
            "mention_or_use": "use",
            "system_name": "Nougat",
            "system_description": "Used in MMAPIS preprocessing to predict text information in PDF page screenshots (plain text and mathematical formulas) and output a Markdown-like textual representation, preserving hierarchical paragraph structure to facilitate downstream segmentation.",
            "llm_model_used": null,
            "extraction_technique": "Neural visual-text recognition (encoder-decoder transformer for screenshot→text/formula extraction).",
            "synthesis_technique": "Produces structured textual inputs for later LLM summarization; does not itself synthesize across papers.",
            "number_of_papers": null,
            "domain_or_topic": "Academic document visual-text extraction",
            "output_type": "Plaintext + mathematical formulas in Markdown-like structure",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": "Traditional OCR engines (line-by-line) and PDF parsing libraries",
            "performance_vs_baseline": "Described qualitatively as preserving hierarchical paragraph relations and formulas better than traditional OCR; used to reduce downstream segmentation complexity.",
            "key_findings": "Visual-text models like Nougat help preserve discourse-level paragraph structure and mathematical content for LLM downstream tasks.",
            "limitations_challenges": "Extraction errors can still occur; accuracy depends on screenshot/image quality and model capabilities.",
            "scaling_behavior": null,
            "uuid": "e4421.8",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PDFFigures2.0",
            "name_full": "PDFFigures 2.0: Mining figures from research papers",
            "brief_description": "A tool that identifies figures and tables in PDFs by reasoning about empty text regions and returns images with attribution metadata, used in MMAPIS for extracting multimodal content.",
            "citation_title": "Pdffigures 2.0: Mining figures from research papers",
            "mention_or_use": "use",
            "system_name": "PDFFigures 2.0",
            "system_description": "Applied to detect and extract figures and tables from PDF pages and provide attribution information that helps align these multimodal objects with document sections during preprocessing.",
            "llm_model_used": null,
            "extraction_technique": "Layout- and region-based figure/table detection and extraction with attribution metadata.",
            "synthesis_technique": "Supplies image/table artifacts to the MMAPIS pipeline for LLM-enabled multimodal interpretation.",
            "number_of_papers": null,
            "domain_or_topic": "Research paper figure/table mining",
            "output_type": "Extracted figure/table images with attribution (section/page) metadata.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Figure/table extraction with attribution reduces alignment difficulty for multimodal summarization systems.",
            "limitations_challenges": "May have limitations when images are embedded in non-standard ways; alignment heuristics still necessary.",
            "scaling_behavior": null,
            "uuid": "e4421.9",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "CoT/CoD prompting",
            "name_full": "Chain-of-Thought and Chain-of-Density prompting",
            "brief_description": "Prompt engineering approaches: CoT elicits stepwise reasoning and iterative refinement; CoD emphasizes information density for dense, entity-preserving summarization; both are used within MMAPIS to improve section generation and integration.",
            "citation_title": "From sparse to dense: GPT-4 summarization with chain of density prompting",
            "mention_or_use": "use",
            "system_name": "CoT (Chain-of-Thought) / CoD (Chain-of-Density) prompting",
            "system_description": "Prompts are split into Task Description, Current Input, and Output Indicator; CoT is used for section-level generation with draft→self-review→refine steps to enhance fluency and integrity; CoD is applied during merging/regeneration to retain entities and maximize information density in the integrated summary.",
            "llm_model_used": "Applied to GPT-family LLMs in the pipeline (GPT-4 for integration/evaluation; CoT/CoD methods referenced from recent literature)",
            "extraction_technique": "N/A (prompting technique used for generation/synthesis rather than raw extraction).",
            "synthesis_technique": "Used to guide multi-step LLM generation workflows (draft, self-review, refinement) and to control density during integration.",
            "number_of_papers": null,
            "domain_or_topic": "Prompting strategies for summarization and integration in scientific document interpretation",
            "output_type": "More robust, denser summaries and improved integrated outputs when applied in the MMAPIS pipeline.",
            "evaluation_metrics": null,
            "performance_results": "Authors attribute improved informativeness and entity density in section/integration outputs to the combined use of CoT for section drafts and CoD for integration; quantitative improvements summarized in the paper's COD evaluation.",
            "comparison_baseline": "Standard single-pass prompts or non-CoT prompting",
            "performance_vs_baseline": "Reportedly better than non-chain prompting at preserving entities and producing denser, higher-rated summaries (per authors' experiments).",
            "key_findings": "Mixing CoT for introspective refinement and CoD for density-preserving integration balances fluency with information retention for multi-stage summarization.",
            "limitations_challenges": "Chain-based prompting increases prompt design complexity and can be more computationally expensive due to multi-step generation.",
            "scaling_behavior": "Used to manage information density when merging many section summaries; effectiveness depends on number/length of sections and LLM cost.",
            "uuid": "e4421.10",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "DocsGPT / Chat2Doc",
            "name_full": "DocsGPT / Chat2Doc (open-source tools)",
            "brief_description": "Mentioned open-source projects that wrap PDF/document parsing and LLM-based QA/summarization into chat-like interfaces for document understanding.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "DocsGPT / Chat2Doc",
            "system_description": "Cited as examples of document-chat systems that integrate document parsing with LLMs to provide conversational access to document contents; referenced in related-work as part of the ecosystem of PDF-to-LLM tools.",
            "llm_model_used": null,
            "extraction_technique": "PDF parsing and text extraction (implementation dependent per project).",
            "synthesis_technique": "LLM-based summarization and QA over extracted document contents.",
            "number_of_papers": null,
            "domain_or_topic": "General document/Q&A tools",
            "output_type": "Chat-style answers and summaries for individual documents",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Representative of the trend to combine PDF parsing with LLMs; typically do not provide the deeper multimodal + discourse-aware processing that MMAPIS targets.",
            "limitations_challenges": "May lack multimodal alignment and discourse-aware segmentation; potential privacy/opaque model concerns when using third-party LLM APIs.",
            "scaling_behavior": null,
            "uuid": "e4421.11",
            "source_info": {
                "paper_title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Nougat: Neural optical understanding for academic documents",
            "rating": 2,
            "sanitized_title": "nougat_neural_optical_understanding_for_academic_documents"
        },
        {
            "paper_title": "Pdffigures 2.0: Mining figures from research papers",
            "rating": 2,
            "sanitized_title": "pdffigures_20_mining_figures_from_research_papers"
        },
        {
            "paper_title": "D2S: Document-to-slide generation via query-based text summarization",
            "rating": 2,
            "sanitized_title": "d2s_documenttoslide_generation_via_querybased_text_summarization"
        },
        {
            "paper_title": "From sparse to dense: GPT-4 summarization with chain of density prompting",
            "rating": 2,
            "sanitized_title": "from_sparse_to_dense_gpt4_summarization_with_chain_of_density_prompting"
        },
        {
            "paper_title": "Lost in the middle: How language models use long contexts",
            "rating": 2,
            "sanitized_title": "lost_in_the_middle_how_language_models_use_long_contexts"
        },
        {
            "paper_title": "Bringing structure into summaries: a faceted summarization dataset for long scientific documents",
            "rating": 2,
            "sanitized_title": "bringing_structure_into_summaries_a_faceted_summarization_dataset_for_long_scientific_documents"
        },
        {
            "paper_title": "ChatPDF (GitHub project)",
            "rating": 1,
            "sanitized_title": "chatpdf_github_project"
        },
        {
            "paper_title": "ChatPaper (GitHub project)",
            "rating": 1,
            "sanitized_title": "chatpaper_github_project"
        },
        {
            "paper_title": "DocsGPT (open-source project)",
            "rating": 1,
            "sanitized_title": "docsgpt_opensource_project"
        }
    ],
    "cost": 0.0214085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System
17 Jan 2024</p>
<p>Feng Jiang 
Kuang Wang 
Haizhou Li </p>
<p>The Chinese University of Hong Kong
ShenzhenChina</p>
<p>KUANG WANG
Zhejiang University
China</p>
<p>HAIZHOU LI
The Chinese University of Hong Kong
ShenzhenChina</p>
<p>Automated Academic Papers Interpretation System Conference acronym 'XX
03-05, 2018June, WoodstockNY</p>
<p>Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System
17 Jan 202483D4975C440603A414B67B2487BDB4D4arXiv:2401.09150v1[cs.CL]Manuscript submitted to ACMMultimodal InterpretationDiscourse-Aware SummarizationLarge-Scale Language Model
In the contemporary information era, significantly accelerated by the advent of Large-scale Language Models (LLMs), the proliferation of scientific literature is reaching unprecedented levels.Researchers urgently require efficient tools for reading and summarizing academic papers, uncovering significant scientific literature, and employing diverse interpretative methodologies.To address this burgeoning demand, the role of automated scientific literature interpretation systems has become paramount.However, prevailing models, both commercial and open-source, confront notable challenges: they often overlook multimodal data, grapple with summarizing over-length texts, and lack diverse user interfaces.In response, we introduce an open-source multi-modal automated academic paper interpretation system (MMAPIS) with three-step process stages, incorporating LLMs to augment its functionality.Our system first employs the hybrid modality preprocessing and alignment module to extract plain text, and tables or figures from documents separately.It then aligns this information based on the section names they belong to, ensuring that data with identical section names are categorized under the same section.Following this, we introduce a hierarchical discourse-aware summarization method.It utilizes the extracted section names to divide the article into shorter text segments, facilitating specific summarizations both within and between sections via LLMs with specific prompts.Finally, we have designed four types of diversified user interfaces, including paper recommendation, multimodal Q&amp;A, audio broadcasting, and interpretation blog, which can be widely applied across various scenarios.Our qualitative and quantitative evaluations underscore the system's superiority, especially in scientific summarization, where it outperforms solutions relying solely on GPT-4.We hope our work can present an open-sourced user-centered solution that addresses the critical needs of the scientific community in our rapidly evolving digital landscape.</p>
<p>INTRODUCTION</p>
<p>In the digital information era, the rate of data production is escalating daily, a phenomenon that is also evident in the realm of scientific research.The sheer volume of scholarly papers is burgeoning at an unprecedented pace.For instance, the renowned preprint server arXiv took over 23 years to accumulate its first million submissions, yet only seven years to gather the next two million, with the subsequent million potentially arriving in just four and a half years 1 .In certain scientific domains, query-based searches often yield a plethora of related articles, far exceeding human capacity for processing [2].This explosion of information is particularly amplified by the advent of Large Language Models (LLMs), Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.Copyrights for components of this work owned by others than the author(s) must be honored.Abstracting with credit is permitted.To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.Request permissions from permissions@acm.org.</p>
<p>which have significantly accelerated the production of scientific literature.The overwhelming abundance of research papers necessitates a transformation in how we access, comprehend, and engage with scientific knowledge, prompting the need for innovative approaches to managing and interacting with this ever-growing body of work.Recent trends in research have seen the gradual emergence of paper interpretation systems, with many commercialized service platforms2 progressively coming online.These systems are capable of swiftly extracting the gist of entire papers from academic websites like arXiv and providing interpretations in the form of summaries.The latest GPT-4 3model also offers the capability to upload PDF documents and interpret papers by designing specific prompts.However, their proprietary nature renders their operational processes opaque and raises concerns about information security when uploading papers to third-party websites.On the other hand, some open-source works 4 focus on converting PDF documents into text using OCR (Optical Character Recognition) technology, followed by feeding the text into summarization models or Large Language Models (LLMs) to obtain the final interpretation.Despite the advancements in paper interpretation systems, fulfilling the aforementioned functional requirements presents several challenges: Overlooking Multimodal Data in Academic Papers: Most existing paper interpretation systems primarily treat papers as the text for summarization, overlooking the structure information and other modalities, such as mathematical formulas, tables, figures, and so forth, that encapsulate the most crucial experimental results, concepts, or workflows [5].</p>
<p>This singular focus on textual information fails to capture the full richness of multimodal data, which is often crucial in scholarly papers.</p>
<p>Grappling with summarization for over-length text: Previous methods of summarization have been limited in their ability to handle longer texts.Although current large-scale language models (e.g., those capable of processing texts up to 100K words [8]) can manage over-length texts, they often struggle to capture complex details [18].On the other hand, segmented summarization approaches typically employ length truncation or fixed grouping methods, leading to semantic incompleteness.Additionally, each section of a paper has different focal points, necessitating distinct considerations for their summarization.</p>
<p>• We develop a hierarchical, discourse-aware summarization method for dealing with long scientific documents.It utilizes structured text extracted from documents to produce concise summaries at both section and document levels, ensuring the retention of essential information.</p>
<p>• We offer a range of user interfaces to present paper interpretation results.It provides users with insights in various formats including paper recommendations, multimodal Q&amp;A, audio broadcasting, and interpretation blogs, enhancing user engagement with scientific papers in various scenarios.</p>
<p>RELATED WORK</p>
<p>Dealing with Source Paper in Academic Papers Interpretation Systems.The efficiency of an interpretation system hinges largely on the quality of data retrieved from PDF files.Historical reliance was placed on Optical Character Recognition (OCR) engines to distill plaintext for subsequent processing in earlier interpretation systems.While these engines have shown efficacy in extracting individual characters and words from images, their line-by-line approach fails to preserve relative positional relationships among different formats, particularly with regard to mathematical expressions and tables [6].Moreover, multimodal elements embedded within the documents often elude these OCR engines.Present trends in interpretation systems favor the utilization of efficient PDF Parsing Libraries, exemplified by systems such as D2S [24] and ChatPaper6 , which utilize Grobid, and ChatPDF7 , based on PyPDFium2Loader.Relative to OCRs, these libraries excel in analyzing PDF document structure to extract richer metadata, including embedded images.However, they still face challenges in retrieving location-specific information, such as formulas, and exhibit limitations when dealing with scientific documents that integrate images directly into the PDF format, hence, their utility remains somewhat limited.Their inherent object-based processing approach, coupled with a tendency to overlook the integrity of structural discourse, results in an inability to semantically connect different objects, necessitating the deployment of an additional Information Retrieval (IR) model for alignment [24].</p>
<p>Summarization in Academic Papers Interpretation Systems.As a crucial component of paper interpretation systems, extracting key information from academic papers is often approached as a summarization task.Since the 1950s, the field of generic text summarization has seen significant progress [11].However, summarizing academic papers presents unique challenges, given their structured nature with typical sections such as the introduction, methodology, experiments, and conclusions.The excessively long text poses a challenge for directly summarizing [25].Previous studies have indicated that despite the ability of large-scale language models to process inputs of up to 100,000 words [8], they tend to disproportionately lose information from the middle sections of texts [18].Reducing the text to shorter text blocks is the mainstream approach.On the one hand, the methods employed include selecting key sentences or words [15,23,29] and using segmented sliding windows [21] in conjunction with attention mechanisms [3,30].They do not consider the inherent structured information of the text, which can easily lead to incomplete semantic segmentation of the obtained abstract.On the other hand, the abstracts of different parts have different focuses, such as the abstract and methodology sections, and a unified abstract cannot meet their characteristics.</p>
<p>User Interaction in Academic Papers Interpretation Systems.Current paper interpretation systems primarily employ two types of user interaction models: textual summaries and dialog-based interpretations.Textual Summary Systems: These systems generate concise text-based summaries of scientific papers 8 .While efficient in providing quick overviews, they often lack depth and neglect the multimodal aspects of papers, such as graphs and tables.Dialog-based Interpretation Systems: These systems engage users in an interactive dialogue format 9 , allowing for query-based information retrieval.However, their adaptability is limited, particularly in handling multimodal content (such as figures and tables) and varying interaction scenarios like blog interpretation or audio readings.While these systems offer basic interpretative insights, they commonly fall short of fully supporting multimodal content and lack flexibility across different application scenarios.</p>
<p>THE FRAMEWORK OF MULTI-MODAL AUTOMATED SCIENTIFIC PAPER INTERPRETATION SYSTEM</p>
<p>Hybrid Modality Preprocessing and Alignment Module</p>
<p>Given the difficult task of manipulating multimodal content present in PDFs, our objective is to reconstruct the source document into semantically similar counterparts, such as in Markdown format, following Hybrid Modality Preprocessing.</p>
<p>The reformed document can facilitate subsequent alignment modules and produce semantically comprehensive, highquality summaries by preserving rich multimodal information (such as tables and figures) and the hierarchical discourse structure (such as sections).</p>
<p>Unlike traditional methodologies predominantly relying on OCR engines or efficient libraries, which fail to discern position-sensitive modal details and ignore document structure specifics, our preprocessing approach is inspired by Nougat [6] and PDFFigures 2.0 [9] to respectively extract text and other modalities.Nougat provides an end-to-end trainable encoder-decoder transformer-based model, adept at predicting text information in pictures, including plaintext and mathematical formulas in the screenshots of each PDF page, to markdown format.We utilize Nougat as a tool for extracting text and its inherent hierarchical structure between different paragraphs from PDFs, minimizing data loss during conversion and thereby significantly reducing the complexity of the subsequent segmentation.In addition, PDFFigures 2.0 [9], a methodology that identifies figures and tables by reasoning about the empty regions within the text, provides not only the images but also their attribution information.This reduces the difficulty of multimodal alignment when assigning affiliations across modalities based on their relative structural information to layout tokens, such as section titles.</p>
<p>The results of both methods, Nougat and PDFFigures 2.0, complement each other-Nougat yields plaintext and mathematical formulas, whilst PDFFigures 2.0 provides screenshots of figures and tables.Notably, these components extracted plaintext or figures with the section name, which can be intuitively aligned with their corresponding title-like keywords, thereby facilitating the recreation of parsing results that closely adhere to the structure of the source document.Specifically, all tables and figures parsed from the PDF are retained after aligning the sections they belong to due to their value in encapsulating the most critical experimental results and concepts in scientific documents [5].</p>
<p>Hierarchical Discourse-aware Summarization Module</p>
<p>To address the challenges of long documents frequently encountered in interpretation systems, we introduce a two-stage summary process, Hierarchical Discourse-aware Summarization, to generate a holistic interpretation.This methodology could alleviate the limitations of previous works (such as semantic fragmentation and efficiency detriments) by splitting documents with section names given by the first module and generating the summary within and between sections via LLMs.</p>
<p>The first stage of our methodology is devoted to the formation of a section-level summary.The process first involves partitioning the document into sections using the hierarchical cues embedded in the title within the Markdown.This approach diverges from the predefined four sections proposed by FacetSum [19], which might only be universally applicable to some papers.Instead, our approach favors dissection at almost every individual section.This flexibility permits a reduction in the lengths of processing segments whilst ensuring the maintenance of section-level and document-level semantic integrity without overlap.Further, it offers adaptive control to expand or contract sections as needed, mitigating noise interference.For example, sections such as the appendix and references can be excised, promoting a more streamlined content-processing experience.Segments such as abstract, introduction, etc, are then matched with corresponding prompts from our prearranged set, specifically designed to facilitate the extraction of key points.This step operates under the assumption that scientific documents adhere to a generic structure [12], and the primary function of each section remains relatively constant.The user-specific prompt encourages the LLMs with the dependence that aligns with the reader's needs and expectancy [14,28] for multi-faceted insight.For sections whose titles are absent from the prompt set, we assign a universal prompt as a viable alternative.</p>
<p>Upon generating section-level summaries, we proceed to a prompt-guided integration stage to construct a documentlevel summary.It mainly focuses on fortifying the cohesion and continuity between sections, thereby guiding the downstream application.Notably, during the integration stage, we include the title, authors, and affiliation information filtered through NER technology within the reference text, enhancing the synthesized summary's affinity.</p>
<p>Diverse Multimodal User Interface Module</p>
<p>Our interpretation system, enhanced by a user-friendly Streamlit-based interface, adeptly transforms the outputs from the Hierarchical Discourse-Aware Summarization Module into four distinct downstream applications, each tailored to fit different user scenarios and ordered by the generated content-length:</p>
<ol>
<li>Paper Recommendation: As the first application, our system employs meticulously designed LLM prompts to evaluate papers across five critical dimensions: clarity of objectives and central themes, appropriateness and accuracy of methods, authenticity and precision of data and findings, depth and conclusiveness of analysis, and overall writing quality.The primary four document-level indicators rely on generated summaries for token reduction assessment.</li>
</ol>
<p>However, evaluating overall writing quality, a more fine-grained metric at the paragraph or even word level, necessitates the use of the original text for a fair assessment.Due to the potential for token overflow and assumptions about writing consistency, considering the reader's inclination to first read the beginning and end, the evaluation is applied to strategically selected excerpts, specifically the introductory and concluding sections.These features aim to swiftly gauge the quality of a paper, providing users with immediate insights into its merits.</p>
<ol>
<li>Multimodal Q&amp;A: Advancing beyond the confines of conventional text-based question-and-answer formats, our system introduces an amplified two-tier Q&amp;A feature that incorporates specific queries about figures or tables extracted from the papers.With the user's request for an in-depth elucidation of a graphic representation within the paper, we initially employ GPT-3.5 Turbo to discern the chart's provenance from user's questions, that is, the index of the illustration juxtaposed with the section to which it pertains, e.g.("Introduction",1) to locate the target picture.</li>
</ol>
<p>Then we feed user queries and relevant images to GPT-4 to deliver a comprehensive interpretation.This functionality leverages GPT-4's multimodal processing abilities, enabling more precise and targeted responses, thereby enriching the user's understanding of the paper's content.</p>
<p>Audio Broadcasting:</p>
<p>Recognizing the need for quick assimilation of information in real-time scenarios, our system introduces a feature tailored for the generation of colloquial broadcast scripts.Utilizing prompts with ChatGPT for the formulation of straightforward sentences, it synthesizes narratives suitable for verbal dissemination predicated on the produced summary.Then audio broadcasting is generated through text-to-speech (TTS) interfaces, such as Azure TTS 10 or Youdao TTS, thereby providing users with a distinctive and user-friendly method to engage with the research.</p>
<p>Interpretation Blog:</p>
<p>The system provides an interpretation blog feature that demands detailed and thorough insights for a more in-depth exploration of the paper.This tool leverages LLM-grounded prompts to render interpretative blogs derived from the created summary, fostering an extensive comprehension of the paper's central subject matter and intricate technical elements through aligning Title-like Keywords to integrate other modalities.Special emphasis is placed on readability and adherence to established blog formats, ensuring a seamless narrative flow from a clear introduction to a conclusive ending.</p>
<p>Prompt Design</p>
<p>Since Transformer-based models are better optimized towards short document language tasks rather than long documents [18], our goal is to ensure the prompt of each role preserves unique responsibilities while maintaining brevity.</p>
<p>Ideally, crucial information should be located either at the onset or the end of input text, and the prompt should be kept brief to optimize performance.We thus categorize prompts into three distinct segments and more details are shown in Appendix:</p>
<p>• Task Description: This segment, the prompt for the "system" role, gives an overview of the task, describing the needs, objectives, or background and stipulates the desired format of the output.</p>
<p>• Current Input: The "user" role inputs text from the raw document content, previously generated summary, or both, serving as a source of external knowledge to better comprehend the entire text.</p>
<p>• Output Indicator: This segment for the "system" role defines a specific workflow, providing guidelines for GPT to follow.</p>
<p>In the realm of output indicators, We adroitly blend the technologies of Chain of thought (CoT) and Chain of Density (CoD) [1], each selected for its unique benefits.CoT stands out for its robustness and ability to significantly surpass the standard baseline [27], CoD, in turn, expertly manages information density to harmonize between informativeness and intelligibility.</p>
<p>For generating section summaries or downstream applications, e.g., broadcasts and blogs, we employ CoT as a tool for introspective evaluation and continuous enhancement.Particularly when producing section summaries, we blend the task description and output indicator to ensure task clarity and condense output requirements while stipulating the specific output format in downstream applications, considering the varying requirements of diverse output modes.</p>
<p>Conversely, in the process of merging section summaries or regenerating, we apply CoD to mitigate any potential loss of valuable entities while simultaneously boosting information density, thereby providing a more comprehensive and informative interpretation.</p>
<p>DEMO &amp; EVALUATION</p>
<p>Demo</p>
<p>In Fig. 3, we showcase our versatile and adaptable downstream applications, each tailored for different scenarios.The case study revolves around the seminal paper titled "Attention is All You Need" [26], which introduced the revolutionary "transformer", aimed at facilitating observations.Initially, we present a recommendation score in markdown format anchored in the top left corner, enabling academics to promptly assess the quality of papers across five distinct dimensions.Subsequently, our multimodal Q&amp;A mechanism enhances interpretive comprehension through a dialogueoriented exploration of key content, including data encapsulated in figures, as exemplified by the lower left corner.To address scenarios that demand convenience, such as engaging with content while driving or during mealtimes, we provide an audio feature in MP3 format, offering a preliminary understanding of the manuscript's narrative.Ultimately, the top right corner displays the resulting blog posts in markdown format, retaining almost all metadata extracted from the source document, including figures, tables, mathematical formulas, and plain text, emphasizing readability and audience engagement, thereby expediting comprehension of the paper's core subject.</p>
<p>Quantitative Analysis for Summarization</p>
<p>In addition to the case studies mentioned above, we conducted a quantitative analysis focusing on the core aspect of paper interpretation systems -the quality of paper summarization.Specifically, we first selected 100 papers from arXiv to form our test set.Since the training data for GPT-4 ends in April 2023, to avoid performance bias due to data leakage, we sampled 50 papers each from December 2017 and December 2023 on arXiv, forming two test subsets named CS2017 Dataset and CS2023 Dataset, respectively.The statistical information of these subsets is shown in Table 1.For our baseline system, we tested the currently best-performing model, GPT-4, to compare its performance against our model.In terms of evaluation metrics, we adopted the COD approach, which includes five dimensions: Informative, Quality, Coherence, Attributable, and Overall [1].It involves using GPT-4 to score the summaries instead of using ROUGE scores due to the absence of reference texts and the inability of ROUGE scores to identify overlaps between synonymous tokens or phrases [4,7,13,17].The underlying rationale for this observation is that the Hierarchical Discourse Summarization mitigates the 'layout bias' identified by Kryściński et al. [17], i.e., approximately 60 % of essential sentences are located within the opening 30%.Unlike conventional methods that favor extraction or truncation based on empirical knowledge or model design, our method veers away from layout bias by using data that closely resembles the original text, which preserves the semantic and structural integrity of the information, even when filtering out particular sections that are less valuable to the readers.It's supported by [16], which showed that salient content is more uniformly dispersed throughout long documents, in contrast to models that often benefit from layout biases in short documents [12,20,22].</p>
<p>The notable performance enhancement may also be attributed to the interesting observation connected with the hypothesize of the "lost in the middle" phenomenon [18] that LLMs exhibit a 'U-shaped' performance curve with a strong reliance on information appearing at the beginning and end.Specifically, as our method segments the text based on the document's structure, the average length of each section within the documents of CS2017 Dataset and CS2023</p>
<p>Dataset is maintained below 2000 tokens, specifically averaging at 1163 and 1364 tokens, respectively.This approach substantially mitigates any potential performance degradation instigated by the position sensitivity in comparison to processing the entire text body, which encapsulates approximately 7,730 to 9,152 tokens, respectively, representing nearly a 6.7-fold increase in length.As evidenced by [18], where GPT-3.5 Turbo's QA performance can drop by over 20% with approximately 4000 token inputs while maintaining considerable performance with around 2000 token inputs.</p>
<p>It can also be inferred that the minimum grid of Hierarchical Discourse-Aware Summarization, which is section-based in the first stage, represents a trade-off between speed and accuracy.</p>
<p>In a longitudinal comparison, both GPT-4 and the MMAPIS exhibit a discernible performance deterioration across all dimensions.Interestingly, in document-free dimensions,e.g., Quality and Coherence, the degradation is less severe, while in dimensions related to technical intricacies and knowledge reserves, observe a more marked reduction, potentially owing to the reoccurring issue of 'hallucinations' -a common trait amongst GPT models.Another contributing factor</p>
<p>Fig. 1 .
1
Fig. 1.MMAPIS: Bridging Research and Readers.</p>
<p>Fig. 2 .
2
Fig. 2. The framework of the multi-modal automated academic paper interpretation system (MMAPIS).</p>
<p>Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Feng Jiang, Kuang Wang, and Haizhou Li</p>
<p>Fig. 3 .
3
Fig. 3.The demos of the diversified user interfaces in MMAPIS (Optimized for enhanced visual presentation).</p>
<p>Table 1 .
1
Statistical information of CS datasets for 2023 and 2017.
Dataset Ave. Section Length (tokens) Ave. Document Length (tokens) Ave. Number of SectionsCS2023136491526.71CS2017116377306.64</p>
<p>Table 2 .
2
[1] performance of summary generation.Each sample is evaluated three times: the main value is the overall average score, and the subscript is the overall standard deviation.As demonstrated by the results, our system surpasses GPT-4 in nearly all aspects in both the CS2023 Dataset and CS2017 Dataset through Horizontal comparison.Specifically, in the dimension of Informative and Overall, our methodology exhibits a remarkable advancement compared to the generalized summary offered by GPT-4 and remains relatively stable, presenting that the result of MMAPIS offers denser entity density and detailed information of key specific, which tends to be more favored by humans, as demonstrated in[1]that the overall dimension has the highest summary-level Pearson Correlation to human preference, while others also maintain a positive correlation ranging from 0.120 to 0.245.Such evidence underscores the efficient performance of Hierarchical Discourse-Aware Summarization, which reduces the likelihood of information loss during segmentation and enables the summarizer to yield a comparatively detailed summary, thereby accurately encapsulating the key narratives and catering to human preference.
Dataset Summarizer InformativeQualityCoherence AttributableOverallEval AverageCS2017Ours GPT-44.534 0.256 4.392 0.2464.440 0.297 4.350 0.2824.518 0.349 4.444 0.3304.568 0.381 4.554 0.3244.521 0.240 4.434 0.2004.516 0.305 4.435 0.276CS2023Ours GPT-44.498 0.204 4.363 0.2604.376 0.344 4.317 0.2684.455 0.395 4.429 0.2634.439 0.516 4.460 0.4624.454 0.250 4.377 0.2324.444 0.342 4.389 0.2974.2.1 Result &amp; Analysis.
https://arxiv.org/stats/monthly_submissions
https://papers.cool/ https://www.paperdigest.org/ https://www.paperreading.club/
https://chat.openai.com/?model=gpt-4
https://github.com/Anil-matcha/ChatPDF https://github.com/kaixindelele/ChatPaper
Our code will be released at https://github.com/fjiangAI/MMAPIS.
https://github.com/kaixindelele/ChatPaper
https://github.com/Anil-matcha/ChatPDF
https://papers.cool/,https://hub.baai.ac.cn/papers
https://github.com/arc53/DocsGPT,https://chat2doc.cn/,
https://azure.microsoft.com/en-us/products/ai-services/text-to-speech/
Conference acronym 'XX, June 03-05, 2018, Woodstock, NY could be the average length increase in data from the CS2017 Dataset, i.e., 1163, to that of CS2023 Dataset, i.e.1364, raising the computational load and subsequently making the GPT model more prone to distractions.Moreover, the hierarchical discourse-aware summarization method via LLMs with special prompts ensures that essential information across various sections of lengthy scientific texts is accurately captured and retained.Additionally, the system's diverse range of multimodal user interfaces enhances the accessibility and utility for both readers and developers.CONCLUSIONS AND FUTURE WORKInWe demonstrate the efficacy and superiority of our system through qualitative demonstrations and quantitative analyses.In the future, we aim to further augment our system's capabilities, focusing on advanced integrations and optimizations.While our current design already addresses key challenges in academic paper interpretation, we recognize the potential for incorporating a broader spectrum of external knowledge and inter-document connections.This enhancement will facilitate a more nuanced understanding of academic content, especially in relation to user-specific contexts and profiles.Additionally, we are dedicated to refining the efficiency and responsiveness of our system.Our goal is to transition from primarily offline processing to more dynamic, real-time interpretations, thereby broadening the system's applicability.APPENDIXIn Figure4, we present various examples of thoughtfully designed prompts.These include a quantifiable evaluation with GPT-4 situated at the upper left, a summary section signified by an introduction exemplar at the lower left, an illustration of application generation, specifically blog interpretation, in the central portion, while showcasing regeneration leveraging CoD technology towards the right.The process of generating section summaries or applications for downstream use is steered by the CoT, which directs the introspective evaluation and ongoing refinement of the GPT.This process consists of three distinct steps: (1) Draft generation, (2) Self-review based on pre-determined parameters, and (3) Refinement to generate the final outcome.The main emphasis in this procedure is on fluency, authenticity, and integrity.On the other hand, at the integration stage or regeneration, the CoD is implemented in an identical workflow.However, the principal aim here is to limit the loss of entity-specific information.During the evaluation phase, we have adopted the methods discussed by Adams et al.[1], using GPT-4 as proxy to rate performance across five dimensions: Informativeness, Quality, Coherence, Attribution, and Overall Impact.
Griffin Adams, Alexander Fabbri, Faisal Ladhak, Eric Lehman, Noémie Elhadad, arXiv:2309.04269From sparse to dense: GPT-4 summarization with chain of density prompting. 2023. 2023arXiv preprint</p>
<p>Automatic summarization of scientific articles: A survey. Journal of King Saud University-Computer and Information Sciences. Nouf Ibrahim Altmami and Mohamed El Bachir Menai342022. 2022</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020. 2020arXiv preprint</p>
<p>Re-evaluating evaluation in text summarization. Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, arXiv:2010.071002020. 2020arXiv preprint</p>
<p>Summarizing figures, tables, and algorithms in scientific publications to augment search results. Sumit Bhatia, Prasenjit Mitra, ACM Transactions on Information Systems (TOIS). 302012. 2012</p>
<p>Nougat: Neural optical understanding for academic documents. Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic, arXiv:2308.134182023. 2023arXiv preprint</p>
<p>The price of debiasing automatic metrics in natural language evaluation. Arun Tejasvi, Chaganty , Stephen Mussman, Percy Liang, arXiv:1807.022022018. 2018arXiv preprint</p>
<p>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia, arXiv:2309.12307[cs.CL]2023</p>
<p>Pdffigures 2.0: Mining figures from research papers. Christopher Clark, Santosh Divvala, Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries. the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries2016</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Automatic text summarization: A comprehensive survey. Wafaa S El-Kassas, Ahmed A Cherif R Salama, Rafea, Hoda, Mohamed, Expert systems with applications. 1651136792021. 2021</p>
<p>Sebastian Gehrmann, Yuntian Deng, Alexander M Rush, arXiv:1808.10792Bottom-up abstractive summarization. 2018. 2018arXiv preprint</p>
<p>Unifying human and statistical evaluation for natural language generation. B Tatsunori, Hugh Hashimoto, Percy Zhang, Liang, arXiv:1904.027922019. 2019arXiv preprint</p>
<p>Ctrlsum: Towards generic controllable text summarization. Junxian He, Wojciech Kryściński, Bryan Mccann, arXiv:2012.042812020. 2020arXiv preprintNazneen Rajani, and Caiming Xiong</p>
<p>Perancangan Information Retrieval (IR) Untuk Pencarian Ide Pokok Teks Artikel Berbahasa Inggris dengn Pembobotan Vector Space Model. Anandini Hetami, Jurnal Ilmiah Teknologi Informasi Asia. 92015. 2015</p>
<p>An empirical survey on long document summarization: Datasets, models, and metrics. Yee Huan, Jiaxin Koh, Ming Ju, Shirui Liu, Pan, ACM computing surveys. 552022. 2022</p>
<p>Wojciech Kryściński, Nitish Shirish Keskar, Bryan Mccann, Caiming Xiong, Richard Socher, arXiv:1908.08960Neural text summarization: A critical evaluation. 2019. 2019arXiv preprint</p>
<p>Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023. 2023arXiv preprint</p>
<p>Bringing structure into summaries: a faceted summarization dataset for long scientific documents. Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He, arXiv:2106.001302021. 2021arXiv preprint</p>
<p>Romain Paulus, Caiming Xiong, Richard Socher, arXiv:1705.04304A deep reinforced model for abstractive summarization. 2017. 2017arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020. 2020</p>
<p>Get to the point: Summarization with pointer-generator networks. Abigail See, Peter J Liu, Christopher D Manning, arXiv:1704.043682017. 2017arXiv preprint</p>
<p>Automated text summarization for indonesian article using vector space model. Cepi Slamet, Atmadja, Maylawati, Wahyudin Lestari, Muhammad Darmalaksana, Ramdhani Ali, IOP Conference Series: Materials Science and Engineering. IOP Publishing201828812037</p>
<p>D2S: Document-to-slide generation via query-based text summarization. Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, Nancy Xr Wang, arXiv:2105.036642021. 2021arXiv preprint</p>
<p>Summarizing scientific articles: experiments with relevance and rhetorical status. Simone Teufel, Marc Moens, Computational linguistics. 282002. 2002</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, Caiming Xiong, arXiv:2105.14064Controllable abstractive dialogue summarization with sketch supervision. 2021. 2021arXiv preprint</p>
<p>Amplifying scientific paper's abstract by leveraging data-weighted reconstruction. Shansong Yang, Weiming Lu, Zhanjiang Zhang, Baogang Wei, Wenjia An, Information Processing &amp; Management. 522016. 2016</p>
<p>Big bird: Transformers for longer sequences. Manzil Zaheer, Guru Guruganesh, Avinava Kumar, Joshua Dubey, Chris Ainslie, Santiago Alberti, Philip Ontanon, Anirudh Pham, Qifan Ravula, Li Wang, Yang, Advances in neural information processing systems. 332020. 2020</p>
<p>Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, International Conference on Machine Learning. PMLR2020</p>            </div>
        </div>

    </div>
</body>
</html>
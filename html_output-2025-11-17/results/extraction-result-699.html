<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-699 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-699</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-699</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-271512046</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.20506v1.pdf" target="_blank">Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> The effectiveness of model training heavily relies on the quality of available training resources. However, budget constraints often impose limitations on data collection efforts. To tackle this challenge, we introduce causal exploration in this paper, a strategy that leverages the underlying causal knowledge for both data collection and model training. We, in particular, focus on enhancing the sample efficiency and reliability of the world model learning within the domain of task-agnostic reinforcement learning. During the exploration phase, the agent actively selects actions expected to yield causal insights most beneficial for world model training. Concurrently, the causal knowledge is acquired and incrementally refined with the ongoing collection of data. We demonstrate that causal exploration aids in learning accurate world models using fewer data and provide theoretical guarantees for its convergence. Empirical experiments, on both synthetic data and real-world applications, further validate the benefits of causal exploration.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e699.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e699.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Time-lagged PC (KCI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time-lagged PC algorithm with Kernel-based Conditional Independence (KCI) tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint-based causal discovery procedure that extends the PC algorithm to time-lagged (temporal) variables and uses kernel-based conditional independence tests (KCI) to decide removal of edges between lagged inputs and current-state outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causation, prediction, and search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Time-lagged PC with KCI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Start from a complete directed graph from U (past states and actions) to V (current states) and iteratively remove edges when conditional independence is detected; conditional independence is tested with a nonparametric Kernel-based Conditional Independence (KCI) test, and the algorithm respects temporal ordering (no instantaneous edges). The authors explicitly extend the PC algorithm to handle time-lagged relationships and use KCI to test conditional independences robustly in nonlinear settings.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Task-agnostic RL environments (synthetic state-space simulations, traffic-signal control, MuJoCo control suite)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive, sequential decision-making environments used for online data collection; environments permit active experimentation (agent actions affect subsequent data) and include simulated synthetic settings, a traffic-signal-control intersection, and MuJoCo continuous-control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>The method detects statistical conditional independencies (hence addresses spurious associations due to non-causal statistical dependence), but it does not explicitly claim handling of external distractor mechanisms beyond CI testing (e.g., measurement confounders or selection bias).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Conditional independence testing via Kernel-based Conditional Independence (KCI) tests on time-lagged variable pairs conditioned on subsets of other lagged variables.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges are refuted/removed when conditional independence is detected by KCI tests (constraint-based edge deletion rule as in the PC algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported to produce correct causal adjacency matrices in experiments; authors report that the time-lagged PC with KCI yields causal matrices with smaller bias from ground truth than alternatives and supports lower downstream model prediction errors (qualitative improvement reported in Figures 3 and 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Constraint-based PC extended for time-lagged data with KCI is used as the primary causal discovery engine; the paper emphasizes that such constraint-based methods are robust under their configuration and that edges can be reliably removed via KCI tests, but standard PC/KCI becomes computationally expensive with large sample sizes, motivating selective sample strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e699.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e699.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Online Selective Causal Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient online causal-relationship discovery via gradient-based sample selection (similarity + diversity) and coreset/top-κ selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online selection pipeline that selects a small representative subset of incoming data for causal discovery using gradient-derived similarity and diversity metrics (and other selection heuristics like coreset) to reduce computational cost and improve sample quality for CI-based discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Online coreset selection for rehearsal-based continual learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Minibatch similarity/diversity + top-κ selection (online coreset-style selection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each incoming minibatch B_t, compute per-sample gradient ∇f_wc(b_it) and the batch-average gradient ∇f_wc(B_t). Define similarity as the normalized inner product between a sample gradient and the batch gradient, and define diversity as the negative average normalized inner product between the sample gradient and other sample gradients in the batch. Combine these two criteria to rank samples and select the top-κ representative examples from B_t for causal discovery (alternative heuristics like uniform, k-center, 'hardest', or coreset selection are also compared). This reduces data volume fed into the (computationally heavy) PC+KCI discovery procedure while focusing on informative and diverse samples.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same task-agnostic RL environments (synthetic simulations, traffic-signal control, MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive sequential environments where data arrive incrementally during agent exploration; selection operates on streaming minibatches and is intended for online/incremental causal identification.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable/sample selection by removing redundant or low-information samples (gradient-similarity based deduplication) and promoting diversity (gradient-diversity), implemented as top-κ selection; also testing alternative selection methods such as coreset, k-center, and 'hardest' sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Redundant/repetitive samples, noisy/outlier samples that produce misleading statistical signals; the approach aims to mitigate the influence of low-quality or redundant data which can amplify spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Identification of low-quality/redundant samples via small diversity or low informativeness (measured by gradient similarity/diversity heuristics); ROC comparisons of sampling methods are reported (Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Explicit downweighting by exclusion (selected top-κ are used for causal discovery while other samples are not); indirectly reduces influence of spurious signals by not including low-quality samples in the CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By focusing CI-tests on high-quality selected samples, spurious edges that arise from noisy or redundant data are less likely to be accepted; subsequent PC+KCI runs on selected data remove edges based on CI outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Selects which incoming data to use for causal discovery based on gradient-derived similarity and diversity; exploration policy itself is trained with intrinsic rewards so the data collected are also influenced by active exploration (see active reward entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Authors report that the online selective method significantly accelerates causal discovery (reduced time cost) without sacrificing accuracy, and ROC curves (Figure 8) show better trade-offs for selective methods; qualitative statements say discovered causal matrices have smaller bias versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>PC+KCI on full datasets suffers exponential growth in time cost with sample size (Figure 9) and is slower; accuracy comparisons favor selective sampling for efficiency while maintaining accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Selective sample selection (similarity+diversity / coreset) substantially reduces computational cost of CI-based causal discovery and empirically retains or improves identification accuracy; choosing representative, diverse samples reduces the risk that large volumes of noisy/redundant data amplify spurious correlations and slow discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e699.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e699.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Reward (data-quality)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active reward signal based on change in held-out prediction error (data-quality intrinsic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism that quantifies the benefit or harm caused by newly collected samples by measuring the decrease (or increase) in held-out model prediction error after training on the new data; used both to guide exploration and to detect/refute low-quality or harmful samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Active reward (Δ held-out loss)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute active reward r_a = L_f(w_{t-2}) - L_f(w_{t-1}), where L_f(·) is mean prediction error on a held-out test set D_h; a positive r_a indicates the newly added data improved model generalization, while a negative r_a signals harmful/noisy data. The exploration intrinsic reward combines prediction-loss surprise (r_i = η/2 ||ŝ_t - s_t||^2) with β * r_a to form the overall intrinsic reward used to train the exploration policy. The agent therefore learns to prefer actions producing samples that not only are novel but also beneficial for model training.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Task-agnostic RL environments (synthetic, traffic, MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive environments where the agent collects trajectories and periodically trains the world model and evaluates on a held-out test set; active reward requires such a held-out validation dataset and iterative model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Empirical refutation/detection via validation: negative active reward identifies samples that degrade held-out performance; the exploration policy downweights or avoids actions producing such samples by receiving lower intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Noisy/outlier samples and adversarial (detrimental) training examples that cause overfitting or degrade generalization; indirectly addresses spurious correlations arising from harmful samples.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Measure change in held-out prediction error before and after including new samples; if the change is negative (performance worsens), the sample(s) are flagged as harmful.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Exploration policy incorporates active reward (with weight β) so it reduces selection probability of actions leading to harmful samples; harmful samples are therefore downweighted via exploration avoidance and by being excluded from the selected set used for causal discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Empirical refutation: samples that yield negative active reward are treated as low-quality and reduce the exploration incentive; combined with selective data admission to causal discovery, this prevents spurious signals derived from such data from influencing CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Exploration is driven by the combined intrinsic reward (prediction loss surprise + β * active reward), so the agent actively seeks states/actions that both surprise the model and empirically improve held-out performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Paper reports that incorporating active reward improves world-model prediction error evolution (Figure 11(a)); authors tuned β and observed continuous improvement in performance when active reward is included.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without active reward (β=0), exploration that only maximizes prediction loss may collect noisy samples that increase prediction error or misdirect learning; qualitative comparisons show active reward yields better learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active reward serves as a practical mechanism to detect and discourage collection of samples that harm model generalization, thereby providing an empirical way to downweight/refute spurious or low-quality data during online exploration; combining surprise and active reward yields better downstream world-model accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e699.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e699.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sharing-Decomposition Schema</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sharing-decomposition schema for causal forward models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-sharing architecture that shares early representation layers across per-dimension predictive networks and reserves small decomposed heads per state-dimension to reduce computation and mitigate the damage from incorrect causal priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Sharing-decomposition forward modeling</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Instead of training n independent per-dimension models (one per predicted state variable), share the first several layers (shared parameter σ) and let each per-dimension predictor keep a small decomposition parameter θ_i; parameter vector for i-th dimension is w_ci = σ ∪ θ_i. This reduces parameter count and enables partial robustness to wrong or incomplete causal graphs because shared layers retain general features while small heads adapt to each target.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Task-agnostic RL environments (synthetic, traffic, MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive environments where multiple state dimensions are predicted; sharing-decomposition operates within the world-model training pipeline and works with causal structural constraints specifying which inputs are parents for each target.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Architectural mitigation: by sharing representations and having small per-dimension heads, the model reduces the effect of erroneous exclusion/inclusion of parent variables (i.e., distractor variables) and can recover from underestimation or noisy causal beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Incorrect/mis-specified causal edges (underestimation or redundant edges), transient structural changes; helps mitigate the adverse effects of such misspecifications.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not a statistical detector; robustness manifests empirically through better initial performance and faster recovery when causal beliefs are wrong or when structure changes, as evaluated in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit: by limiting per-dimension capacity and sharing representations, the model avoids overfitting to spurious inputs; no explicit weighting scheme is used.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Couples with online causal discovery: as causal discovery is periodically re-run (every N steps) on selected data, sharing-decomposition helps the model survive until the causal graph is corrected; detection/refutation of wrong edges happens via subsequent CI-tests on newly selected data.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Works in concert with active exploration and online causal discovery; the exploration policy collects data guided by intrinsic rewards which are then used to refine causal structure periodically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Empirically shown to produce lower initial prediction errors (better y-intercept) and to allow correction when starting with wrong causal beliefs or when structures change; experiments (Figure 11(b),(c)) show recovery and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>A naive per-dimension independent model or a fully dense non-causal model shows higher prediction error at initialization and slower recovery under wrong causal priors; causal models without sharing-decomposition are more sensitive to incorrect causal graphs (qualitative experimental comparisons reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The sharing-decomposition schema reduces computational burden and gives practical fault tolerance to erroneous causal structure estimates and sudden structural changes, enabling the system to continue learning until causal discovery corrects the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Kernel-based conditional independence test and application in causal discovery <em>(Rating: 2)</em></li>
                <li>Causation, prediction, and search <em>(Rating: 2)</em></li>
                <li>Online coreset selection for rehearsal-based continual learning <em>(Rating: 2)</em></li>
                <li>Action-sufficient state representation learning for control with structural constraints <em>(Rating: 1)</em></li>
                <li>Causal dynamics learning for task-independent state abstraction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-699",
    "paper_id": "paper-271512046",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "Time-lagged PC (KCI)",
            "name_full": "Time-lagged PC algorithm with Kernel-based Conditional Independence (KCI) tests",
            "brief_description": "A constraint-based causal discovery procedure that extends the PC algorithm to time-lagged (temporal) variables and uses kernel-based conditional independence tests (KCI) to decide removal of edges between lagged inputs and current-state outputs.",
            "citation_title": "Causation, prediction, and search",
            "mention_or_use": "use",
            "method_name": "Time-lagged PC with KCI",
            "method_description": "Start from a complete directed graph from U (past states and actions) to V (current states) and iteratively remove edges when conditional independence is detected; conditional independence is tested with a nonparametric Kernel-based Conditional Independence (KCI) test, and the algorithm respects temporal ordering (no instantaneous edges). The authors explicitly extend the PC algorithm to handle time-lagged relationships and use KCI to test conditional independences robustly in nonlinear settings.",
            "environment_name": "Task-agnostic RL environments (synthetic state-space simulations, traffic-signal control, MuJoCo control suite)",
            "environment_description": "Interactive, sequential decision-making environments used for online data collection; environments permit active experimentation (agent actions affect subsequent data) and include simulated synthetic settings, a traffic-signal-control intersection, and MuJoCo continuous-control tasks.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "The method detects statistical conditional independencies (hence addresses spurious associations due to non-causal statistical dependence), but it does not explicitly claim handling of external distractor mechanisms beyond CI testing (e.g., measurement confounders or selection bias).",
            "detection_method": "Conditional independence testing via Kernel-based Conditional Independence (KCI) tests on time-lagged variable pairs conditioned on subsets of other lagged variables.",
            "downweighting_method": null,
            "refutation_method": "Edges are refuted/removed when conditional independence is detected by KCI tests (constraint-based edge deletion rule as in the PC algorithm).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported to produce correct causal adjacency matrices in experiments; authors report that the time-lagged PC with KCI yields causal matrices with smaller bias from ground truth than alternatives and supports lower downstream model prediction errors (qualitative improvement reported in Figures 3 and 5).",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Constraint-based PC extended for time-lagged data with KCI is used as the primary causal discovery engine; the paper emphasizes that such constraint-based methods are robust under their configuration and that edges can be reliably removed via KCI tests, but standard PC/KCI becomes computationally expensive with large sample sizes, motivating selective sample strategies.",
            "uuid": "e699.0",
            "source_info": {
                "paper_title": "Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Online Selective Causal Discovery",
            "name_full": "Efficient online causal-relationship discovery via gradient-based sample selection (similarity + diversity) and coreset/top-κ selection",
            "brief_description": "An online selection pipeline that selects a small representative subset of incoming data for causal discovery using gradient-derived similarity and diversity metrics (and other selection heuristics like coreset) to reduce computational cost and improve sample quality for CI-based discovery.",
            "citation_title": "Online coreset selection for rehearsal-based continual learning",
            "mention_or_use": "use",
            "method_name": "Minibatch similarity/diversity + top-κ selection (online coreset-style selection)",
            "method_description": "For each incoming minibatch B_t, compute per-sample gradient ∇f_wc(b_it) and the batch-average gradient ∇f_wc(B_t). Define similarity as the normalized inner product between a sample gradient and the batch gradient, and define diversity as the negative average normalized inner product between the sample gradient and other sample gradients in the batch. Combine these two criteria to rank samples and select the top-κ representative examples from B_t for causal discovery (alternative heuristics like uniform, k-center, 'hardest', or coreset selection are also compared). This reduces data volume fed into the (computationally heavy) PC+KCI discovery procedure while focusing on informative and diverse samples.",
            "environment_name": "Same task-agnostic RL environments (synthetic simulations, traffic-signal control, MuJoCo)",
            "environment_description": "Interactive sequential environments where data arrive incrementally during agent exploration; selection operates on streaming minibatches and is intended for online/incremental causal identification.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable/sample selection by removing redundant or low-information samples (gradient-similarity based deduplication) and promoting diversity (gradient-diversity), implemented as top-κ selection; also testing alternative selection methods such as coreset, k-center, and 'hardest' sampling.",
            "spurious_signal_types": "Redundant/repetitive samples, noisy/outlier samples that produce misleading statistical signals; the approach aims to mitigate the influence of low-quality or redundant data which can amplify spurious correlations.",
            "detection_method": "Identification of low-quality/redundant samples via small diversity or low informativeness (measured by gradient similarity/diversity heuristics); ROC comparisons of sampling methods are reported (Figure 8).",
            "downweighting_method": "Explicit downweighting by exclusion (selected top-κ are used for causal discovery while other samples are not); indirectly reduces influence of spurious signals by not including low-quality samples in the CI tests.",
            "refutation_method": "By focusing CI-tests on high-quality selected samples, spurious edges that arise from noisy or redundant data are less likely to be accepted; subsequent PC+KCI runs on selected data remove edges based on CI outcomes.",
            "uses_active_learning": true,
            "inquiry_strategy": "Selects which incoming data to use for causal discovery based on gradient-derived similarity and diversity; exploration policy itself is trained with intrinsic rewards so the data collected are also influenced by active exploration (see active reward entry).",
            "performance_with_robustness": "Authors report that the online selective method significantly accelerates causal discovery (reduced time cost) without sacrificing accuracy, and ROC curves (Figure 8) show better trade-offs for selective methods; qualitative statements say discovered causal matrices have smaller bias versus baselines.",
            "performance_without_robustness": "PC+KCI on full datasets suffers exponential growth in time cost with sample size (Figure 9) and is slower; accuracy comparisons favor selective sampling for efficiency while maintaining accuracy.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Selective sample selection (similarity+diversity / coreset) substantially reduces computational cost of CI-based causal discovery and empirically retains or improves identification accuracy; choosing representative, diverse samples reduces the risk that large volumes of noisy/redundant data amplify spurious correlations and slow discovery.",
            "uuid": "e699.1",
            "source_info": {
                "paper_title": "Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Active Reward (data-quality)",
            "name_full": "Active reward signal based on change in held-out prediction error (data-quality intrinsic reward)",
            "brief_description": "A mechanism that quantifies the benefit or harm caused by newly collected samples by measuring the decrease (or increase) in held-out model prediction error after training on the new data; used both to guide exploration and to detect/refute low-quality or harmful samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Active reward (Δ held-out loss)",
            "method_description": "Compute active reward r_a = L_f(w_{t-2}) - L_f(w_{t-1}), where L_f(·) is mean prediction error on a held-out test set D_h; a positive r_a indicates the newly added data improved model generalization, while a negative r_a signals harmful/noisy data. The exploration intrinsic reward combines prediction-loss surprise (r_i = η/2 ||ŝ_t - s_t||^2) with β * r_a to form the overall intrinsic reward used to train the exploration policy. The agent therefore learns to prefer actions producing samples that not only are novel but also beneficial for model training.",
            "environment_name": "Task-agnostic RL environments (synthetic, traffic, MuJoCo)",
            "environment_description": "Interactive environments where the agent collects trajectories and periodically trains the world model and evaluates on a held-out test set; active reward requires such a held-out validation dataset and iterative model updates.",
            "handles_distractors": true,
            "distractor_handling_technique": "Empirical refutation/detection via validation: negative active reward identifies samples that degrade held-out performance; the exploration policy downweights or avoids actions producing such samples by receiving lower intrinsic reward.",
            "spurious_signal_types": "Noisy/outlier samples and adversarial (detrimental) training examples that cause overfitting or degrade generalization; indirectly addresses spurious correlations arising from harmful samples.",
            "detection_method": "Measure change in held-out prediction error before and after including new samples; if the change is negative (performance worsens), the sample(s) are flagged as harmful.",
            "downweighting_method": "Exploration policy incorporates active reward (with weight β) so it reduces selection probability of actions leading to harmful samples; harmful samples are therefore downweighted via exploration avoidance and by being excluded from the selected set used for causal discovery.",
            "refutation_method": "Empirical refutation: samples that yield negative active reward are treated as low-quality and reduce the exploration incentive; combined with selective data admission to causal discovery, this prevents spurious signals derived from such data from influencing CI tests.",
            "uses_active_learning": true,
            "inquiry_strategy": "Exploration is driven by the combined intrinsic reward (prediction loss surprise + β * active reward), so the agent actively seeks states/actions that both surprise the model and empirically improve held-out performance.",
            "performance_with_robustness": "Paper reports that incorporating active reward improves world-model prediction error evolution (Figure 11(a)); authors tuned β and observed continuous improvement in performance when active reward is included.",
            "performance_without_robustness": "Without active reward (β=0), exploration that only maximizes prediction loss may collect noisy samples that increase prediction error or misdirect learning; qualitative comparisons show active reward yields better learning curves.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Active reward serves as a practical mechanism to detect and discourage collection of samples that harm model generalization, thereby providing an empirical way to downweight/refute spurious or low-quality data during online exploration; combining surprise and active reward yields better downstream world-model accuracy.",
            "uuid": "e699.2",
            "source_info": {
                "paper_title": "Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Sharing-Decomposition Schema",
            "name_full": "Sharing-decomposition schema for causal forward models",
            "brief_description": "A parameter-sharing architecture that shares early representation layers across per-dimension predictive networks and reserves small decomposed heads per state-dimension to reduce computation and mitigate the damage from incorrect causal priors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Sharing-decomposition forward modeling",
            "method_description": "Instead of training n independent per-dimension models (one per predicted state variable), share the first several layers (shared parameter σ) and let each per-dimension predictor keep a small decomposition parameter θ_i; parameter vector for i-th dimension is w_ci = σ ∪ θ_i. This reduces parameter count and enables partial robustness to wrong or incomplete causal graphs because shared layers retain general features while small heads adapt to each target.",
            "environment_name": "Task-agnostic RL environments (synthetic, traffic, MuJoCo)",
            "environment_description": "Interactive environments where multiple state dimensions are predicted; sharing-decomposition operates within the world-model training pipeline and works with causal structural constraints specifying which inputs are parents for each target.",
            "handles_distractors": true,
            "distractor_handling_technique": "Architectural mitigation: by sharing representations and having small per-dimension heads, the model reduces the effect of erroneous exclusion/inclusion of parent variables (i.e., distractor variables) and can recover from underestimation or noisy causal beliefs.",
            "spurious_signal_types": "Incorrect/mis-specified causal edges (underestimation or redundant edges), transient structural changes; helps mitigate the adverse effects of such misspecifications.",
            "detection_method": "Not a statistical detector; robustness manifests empirically through better initial performance and faster recovery when causal beliefs are wrong or when structure changes, as evaluated in experiments.",
            "downweighting_method": "Implicit: by limiting per-dimension capacity and sharing representations, the model avoids overfitting to spurious inputs; no explicit weighting scheme is used.",
            "refutation_method": "Couples with online causal discovery: as causal discovery is periodically re-run (every N steps) on selected data, sharing-decomposition helps the model survive until the causal graph is corrected; detection/refutation of wrong edges happens via subsequent CI-tests on newly selected data.",
            "uses_active_learning": true,
            "inquiry_strategy": "Works in concert with active exploration and online causal discovery; the exploration policy collects data guided by intrinsic rewards which are then used to refine causal structure periodically.",
            "performance_with_robustness": "Empirically shown to produce lower initial prediction errors (better y-intercept) and to allow correction when starting with wrong causal beliefs or when structures change; experiments (Figure 11(b),(c)) show recovery and robustness.",
            "performance_without_robustness": "A naive per-dimension independent model or a fully dense non-causal model shows higher prediction error at initialization and slower recovery under wrong causal priors; causal models without sharing-decomposition are more sensitive to incorrect causal graphs (qualitative experimental comparisons reported).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "The sharing-decomposition schema reduces computational burden and gives practical fault tolerance to erroneous causal structure estimates and sudden structural changes, enabling the system to continue learning until causal discovery corrects the graph.",
            "uuid": "e699.3",
            "source_info": {
                "paper_title": "Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Kernel-based conditional independence test and application in causal discovery",
            "rating": 2,
            "sanitized_title": "kernelbased_conditional_independence_test_and_application_in_causal_discovery"
        },
        {
            "paper_title": "Causation, prediction, and search",
            "rating": 2,
            "sanitized_title": "causation_prediction_and_search"
        },
        {
            "paper_title": "Online coreset selection for rehearsal-based continual learning",
            "rating": 2,
            "sanitized_title": "online_coreset_selection_for_rehearsalbased_continual_learning"
        },
        {
            "paper_title": "Action-sufficient state representation learning for control with structural constraints",
            "rating": 1,
            "sanitized_title": "actionsufficient_state_representation_learning_for_control_with_structural_constraints"
        },
        {
            "paper_title": "Causal dynamics learning for task-independent state abstraction",
            "rating": 1,
            "sanitized_title": "causal_dynamics_learning_for_taskindependent_state_abstraction"
        }
    ],
    "cost": 0.015379,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge
30 Jul 2024</p>
<p>Yupei Yang yupeiyang@sjtu.edu.cn 
Department of Computer Science and Engineering
Shanghai Jiao Tong University
China</p>
<p>Biwei Huang 
Halicioglu Data Science Institute (HDSI)
University of California San Diego
USA</p>
<p>Shikui Tu tushikui@sjtu.edu.cn 
Department of Computer Science and Engineering
Shanghai Jiao Tong University
China</p>
<p>Lei Xu leixu@sjtu.edu.cn 
Department of Computer Science and Engineering
Shanghai Jiao Tong University
China</p>
<p>Guangdong Institute of Intelligence Science and Technology
ZhuhaiChina</p>
<p>Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge
30 Jul 202404BD198A32B594A1E3A96BF776DAD506arXiv:2407.20506v1[cs.LG]
The effectiveness of model training heavily relies on the quality of available training resources.However, budget constraints often impose limitations on data collection efforts.To tackle this challenge, we introduce causal exploration in this paper, a strategy that leverages the underlying causal knowledge for both data collection and model training.We, in particular, focus on enhancing the sample efficiency and reliability of the world model learning within the domain of task-agnostic reinforcement learning.During the exploration phase, the agent actively selects actions expected to yield causal insights most beneficial for world model training.Concurrently, the causal knowledge is acquired and incrementally refined with the ongoing collection of data.We demonstrate that causal exploration aids in learning accurate world models using fewer data and provide theoretical guarantees for its convergence.Empirical experiments, on both synthetic data and realworld applications, further validate the benefits of causal exploration.The source code is available at https://github.com/CMACH508/CausalExploration.</p>
<p>Introduction</p>
<p>Deep neural networks have been incredibly successful in various domains, such as milestone achievements in Go games and control tasks [Silver et al., 2016;Tassa et al., 2018].</p>
<p>One key factor contributing to such remarkable performance is the availability of high-quality data for model training.However, in many practical applications, it remains datahungry due to limited data collection efforts imposed by budget constraints [Fang et al., 2017;Yoo and Kweon, 2019;Robine et al., 2023].This highlights the essential need to enhance both the sampling and model learning efficiency.</p>
<p>In this paper, we introduce causal exploration to tackle this challenge, a novel framework that makes use of the underlying causal knowledge to boost the data collection and model training processes.On one hand, acquiring and understanding causal knowledge unveils the fundamental mechanisms * corresponding author behind the data generation process, thereby reducing the exploration space.In contrast to random data collection, causal exploration allows systematic action planning based on the identified causal structures.On the other hand, causal knowledge reflects the cause-and-effect dependencies among variables.Through the incorporation of causal structural constraints into the model, we can acquire causal dynamics models that eliminate redundant dependencies, as opposed to noncausal dense models, which have shown to provide more accurate estimations [Seitzer et al., 2021;Huang et al., 2022;Wang et al., 2022].</p>
<p>Specifically, we focus on boosting the sample efficiency and reliability of the world model in the realm of taskagnostic reinforcement learning (RL).Different from methods that learn a fixed task from scratch, task-agnostic RL agent first learns a global world model that gathers information about the true environment on the data collected during exploration.Then based on the predictions of the learned model, the agent could make quick adaptations to various downstream tasks in a zero-shot manner given task-specific reward functions.This learning setup exhibits excellent generalization performance but also imposes high requirements on the accuracy of the model [Pathak et al., 2019].However, the data collection and world model learning processes are usually expensive due to extensive environment interactions, especially in large state spaces where discovering the optimal policy can be highly challenging [Burda et al., 2018].</p>
<p>To address this problem, our causal exploration-based approach revolves around three primary aspects.First, we employ constraint-based methods to discover causal relationships among environment variables.Second, we formulate the dynamics model under causal structural constraints to enhance its reliability.Third, we propose several ways based on causal knowledge to improve the sample efficiency during exploration.In particular, for causal discovery, we present an efficient online method that selectively eliminates noisy samples and strategically gathers informative data points in an incremental way.Moreover, we learn to actively explore towards novel states that are expected to contribute most to model training.The learning process of the exploration policy is driven by intrinsic rewards, which measure both the agent's level of surprise at the outcome and the quality of the training caused by the selected data.During exploration, causal knowledge and the world model are continuously re-fined with the ongoing collection of data.Our key contributions are summarized below.</p>
<p>• In order to enhance the sample efficiency and reliability of model training with causal knowledge, we introduce a novel concept: causal exploration, and focus particularly on the domain of task-agnostic reinforcement learning.• To efficiently learn and use causal structural constraints, we develop an online method for causal discovery and formulate the world model with explicit structural embeddings.During exploration, we train the dynamics model under a novel weight-sharing-decomposition schema that can avoid additional computational burden.• Theoretically, we show that, given strong convexity and smoothness assumptions, our approach attains a superior convergence rate compared to non-causal methods.</p>
<p>Empirical experiments further demonstrate the robustness of our online causal discovery method and validate the effectiveness of causal exploration across a range of demanding reinforcement learning environments.</p>
<p>Preliminary</p>
<p>We consider task-agnostic RL within the framework of a Markov decision process characterized by state space S and action space A. In addition, to integrate causal information, we make the following assumptions throughout this paper.Assumption 1. (Causal Factorization).Both the state space and action space can be factorized.That is, Given these commonly made assumptions for causal discovery methods, we next define transition causality over the transition variables from {s t−1 , a t−1 } to s t .Definition 1. (Transition Causality).Under the Markov condition, the causal structures are over the state-action variables
S = S 1 × . . . × S n ∈ R n and A = A 1 × . . . × A c ∈ R c . Assumption 2. (Causal Sufficiency).U = {S i,t−1 } n i=1 ∪ {A j,t−1 } c j=1 and V = {S i,t } n i=1
, which can be represented by a directed acyclic graph G = ({U, V}, E) and its adjacency matrix D. Here, E denotes the edge set and D ∈ {0, 1}</p>
<p>|U |×|V| .</p>
<p>Note that all edges are from U to V. If s i,t−1 ∈ U has a causal edge to s j,t ∈ V, then we call s i,t−1 a parent of s j,t and have D(i, j) = 1.Take the example in Figure 1: we have D(1, 1) = 1 because s 1,t−1 is a parent of s 1,t , while D(2, 1) = 0 because s 2,t−1 does not have a causal edge to s 1,t .Here, we assume that the structural constraints are invariant over time t.The causal identification theory under these appropriate definitions and assumptions has been given in existing work.Proposition 1.Under the aforementioned assumptions, the causal adjacency matrix D is identifiable (see [Huang et al., 2021;Wang et al., 2022;Ding et al., 2022]).
D =       1 0 0 1 1 1       s 1,t s 2,t s 1,t−1 s 2,t−1 a t−1 Figure 1: An illustration of causal relationships from U = {s1,t−1, s2,t−1, at−1} to V = {s1,t, s2
,t} in the RL system.</p>
<p>Related Work</p>
<p>Task-agnostic RL.Over recent decades, task-agnostic exploration strategies have been an active research area to attain generalization [Aubret et al., 2019].Existing methods focus on designing appropriate forms of intrinsic rewards, which can be broadly categorized into three types: (1) the number of times a state has been visited, which helps to guide the agent towards unexplored regions [Bellemare et al., 2016;Machado et al., 2020]; (2) curiosity about the environment dynamics, which is usually formalized as prediction errors of future states [Pathak et al., 2017;Kim et al., 2018]; and (3) information gain, which aims to improve the agent's knowledge about the environment by maximizing the mutual information [Duan et al., 2016;Shyam et al., 2019].However, existing works usually pay little attention to the support that causal structures can offer in improving exploration efficiency.RL with causal discovery.The intersection of causal discovery and RL has become a popular trend in recent years.[Zhu et al., 2019] uses RL to search for the causal graph with the best score.[Hu et al., 2022] proposes to learn a hierarchical causal structure for subgoal-based policy learning.[Ding et al., 2022] and [Mutti et al., 2023] focus on providing tractable formulations of systematic generalization in RL tasks by employing a causal viewpoint.[Yu et al., 2023] learns a causal world model to generate explanations for the decision-making process.Theoretical evidence about the advantages of using a causal world-model in offline RL is given in [Zhu et al., 2022].However, most of these works focus on either extracting underlying causal graphs from given data in a particular environment or using random exploration strategies for data collection, instead of directly utilizing causal structure as guidance to improve exploration efficiency in task-agnostic RL [Kosoy et al., 2022].Experimental comparisons with some existing works that are close to us have been conducted in Section 6.</p>
<p>Discovering and Utilizing Causality for Learning World Models</p>
<p>After establishing proper assumptions and definitions, we proceed to introduce the methodology part for causal exploration.In this section, we initially assume that the causal structures are known and show how to explicitly incorporate causal knowledge into the world model and utilize it for model training.Next, we give an estimation procedure for causal structures.</p>
<p>Causal Constraints for Forward Model</p>
<p>During task-agnostic exploration, the agent learns a world model f w c with parameter w c , serving as an abstraction of the ground truth transitions in the environment.In other words, the world model f w c is designed to enable agents to predict future state ŝt based on current state s t−1 and action
a t−1 , represented by ŝt = f w c (s t−1 , a t−1 , e t )
, where e t is the corresponding random noise.However, based on the understanding that causal structures within environmental variables are typically sparse rather than dense, as suggested by [Huang et al., 2022], such a framework could contain unnecessary dependencies.For instance, in the context of Figure 1, the variable s 2,t−1 does not causally affect s 1,t , and is thus identified as a non-parent node.Consequently, we only need to take a subset of (s t−1 , a t−1 ) as inputs of the model.</p>
<p>To reflect these constraints, we explicitly consider the causal structures over state and action variables to model the one-step transition dynamics, formulated as:
ŝt = n i=1 f i w c (D i ⊙ (s t−1 , a t−1 ), e i,t ),(1)
where ⊙ denotes element-wise product, D i and e i,t are the ith column of causal matrix D and noise term e t , respectively.However, a naive implementation requires training n world models since each factored dimension has its unique parents.</p>
<p>It is likely to result in an explosive growth in computational complexity as the state dimension and network size increase.</p>
<p>We propose a sharing-decomposition schema to address such a problem.It is unnecessary for all of these n networks to be totally different.Instead, each of these models could share the first several layers as a common embedding.Then following the sharing module, we design predictive networks for each dimension.Suppose w c i is the network parameter for the i-th dimension, it is a combination of the shared parameter σ and decomposed parameter θ i , written as
w c i = σ ∪ θ i .(2)
During the training time, each model focuses on a different aspect of the state in the decomposition part θ i but shares a common knowledge σ.The number of shared layers is a hyperparameter that allows for a trade-off between the sharing and decomposition parts.By training forward models under this schema, our approach can both utilize causal information of the ground environment dynamics to generate accurate predictions and achieve a significant reduction in model parameters and computation time compared with naive decomposition.Figure 2 illustrates our sharing-decomposition schema during causal exploration.Moreover, we use the prediction error as the loss function for the optimization of w c :
L f (w c ) = 1 2 ∥ŝ t − s t ∥ 2 2 .
(3)</p>
<p>Efficient Online Causal Relationship Discovery</p>
<p>In this section, we show how to identify the causal adjacency matrix D. According to Definition 1, this can be transformed into determining whether there exists an edge between each pair of nodes in the causal graph G.To achieve this goal, we start from a complete graph and then iteratively remove unnecessary edges using Conditional Independence Tests (CIT).</p>
<p>Given that the edges follow the temporal order without instantaneous connection, we extend the PC algorithm [Spirtes et al., 2000] to handle time-lagged causal relationships based on Kernel-based Conditional Independence (KCI) test [Zhang et al., 2012] to identify the causal adjacency matrix D.</p>
<p>While causal discovery algorithms typically necessitate the collection of substantial causal information through data, it's important to note that accumulating more samples does not always confer an advantage: as the sample size increases, the time cost of causal algorithms also rises.Figure 9 in the appendix illustrates that the execution time of the PC algorithm based on KCI tests experiences exponential growth as sample size increases.Therefore, prioritizing the enhancement of data quality over quantity becomes paramount.In order to reduce the cost of the identification process, we design an efficient online causal relationship discovery method: instead of using all of the coming data for causal identification, we selectively collect representative data points during exploration in an incremental way.Specifically, we use the minibatch similarity and sample diversity criteria introduced in [Yoon et al., 2021] as our selection strategies, which are defined as Here b i t is the i-th data of the whole arrival batch B t .∇f w c (b i t ) and ∇f w c (B t ) are the gradient and average gradient of the sample and batch, respectively.A combination of these two criteria is used to select the top-κ data among B t for causal discovery during exploration.Experiments in Section 6 show that this online method significantly accelerates causal discovery without sacrificing overall accuracy.
Similarity = ∇f w c (b i t )∇f w c (B t ) ⊤ ∥∇f w c (b i t )∥ • ∥∇f w c (B t )∥ , Diversity = −1 t − 1 t−1 p̸ =i ∇f w c (b i t )∇f w c (b p t ) ⊤ ∥∇f w c (b i t )∥ • ∥∇f w c (b p t )∥ .(4)
It is noteworthy that our approach allows for flexibility in the choice of causal discovery methods.However, as has been discussed in [Ding et al., 2022], constraint-based approaches usually exhibit better robustness compared with score-based methods under our specific configuration.Furthermore, we have also conducted comparative experiments among different causal discovery methods to demonstrate the correctness of the causal structure identified by the time-lagged PC algorithm in Section 6.</p>
<p>Boosting Efficiency through Causal Exploration</p>
<p>We now return to the fundamental question: how to enhance the data collection efficiency during causal exploration, thereby improving the performance of both causal discovery and model learning.To attain this goal, a commonly applied concept from active learning is the selection of samples that make the largest contributions to the model's training loss.These samples are typically considered as a subset that the model is least familiar with.Hence, the prediction loss is used here as the intrinsic reward to guide exploration with a scaling weight η:
r i t−1 = η 2 ∥ŝ t − s t ∥ 2 2 . (5)
This prediction loss can also be viewed as a validation of the agent's causal beliefs.The larger the prediction error, the more surprised the agent is by the actual outcome, implying a greater deviation from the estimated values based on the causal structure and the world model.The faster the error rate drops, the more learning progress signals we acquire.However, not all the novel states have a positive impact on the model.On the contrary, some noisy data may contribute significantly to prediction errors but can lead the model to an awful direction, which necessitates the agent to also pay attention to the inherent quality of the data during exploration.</p>
<p>To reflect this, we introduce active reward [Fang et al., 2017] that measures the data quality as another intrinsic motivation.</p>
<p>Once a new sample is collected at time step t, active reward is then calculated as the change of the model's prediction ability before and after training.We use the prediction accuracy on a test set D h generated from episodes unseen before training to reflect the world model's performance and formulate active reward as
r a t−1 = L f (w c t−2 ) − L f (w c t−1 ),(6)
where L f (•) is the mean prediction error on D h and w c t denotes parameters of the trained world model at time t.The value of active reward reflects beneficial or detrimental training caused by the selected data.If the reward is always positive, it indicates that the agent has been selecting beneficial samples for training the world model.We combine prediction loss and active reward with a regularization weight β:
r t−1 = r i t−1 + βr a t−1 .(7)
During causal exploration, the agent keeps searching for causal informative data by maximizing the expected rewards, which is
a * t = arg max a∈A E τ ∼π t γ t r t ,(8)
where τ represents the trajectory generated by the exploration policy π and γ is the discount factor.Meanwhile, the world model minimizes the prediction loss.Since both of them contain the prediction error in equation ( 3) and ( 5), we can draw a conclusion that the learning of world models and causal exploration facilitate each other.Figure 4 shows an overview of causal exploration, details are shown in Algorithm 1.</p>
<p>Theoretical Analysis on Causal Exploration</p>
<p>In this subsection, we first present a mathematical criterion for evaluating the impact of causal exploration on sampling efficiency, and then we theoretically demonstrate the benefits of causal exploration in learning world models, especially when the causal graph is sparse.</p>
<p>Causal efficiency ratio.end while 18: end for 19: return Latest forward world model f L f (w) be the corresponding loss function.The model parameters incorporating causal knowledge after the k-th optimization are denoted as w c (k), while w(k) represents the parameters without causal information 1 .We use the convergence bounds ratio, denoted by ξ, calculated as the relative improvement in the model's loss compared to the optimal value L ⋆ f , to assess the enhanced exploration efficiency resulting from the incorporation of causal knowledge:
ξ = L f (w c (k)) − L ⋆ f L f (w(k)) − L ⋆ f .(9)
Note that this ratio measures how much closer the model with causal knowledge gets to the optimal value compared to those without causal information.If ξ &lt; 1 consistently holds, it indicates an improvement in efficiency attributable to causal exploration.Next, we provide a theoretical analysis of the performance of causal exploration in linear cases, and empirical experiments are further conducted to show that causal exploration is still efficient in learning deep neural networks.Theoretical guarantees.We consider learning a linear forward model with gradient descent, formulated as
min w L f (w) = ∥w ⊤ • (s t−1 , a t−1 ) − s t ∥ 2 2 , (10)
where
s t = w ⋆⊤ • (s t−1 , a t−1 )
is the ground truth value.Moreover, we make the following assumption: Assumption 4. L f is strong convex and smooth such that ∃ m &gt; 0, M &gt; 0, for any w ∈ dom L f , we have
M I ⪰ ∇ 2 L f (w) ⪰ mI.
1 For each exploration time t, the model undergoes several optimization steps, influenced by the data volume and batch size.</p>
<p>The following theorem shows reduced error bound with causal exploration.</p>
<p>Theorem 1. Suppose Assumption 4 holds, and suppose the density of causal matrix D is δ and the model is initialized with w 0 .Then for every optimization step k, we have
L f (w c (k)) − L ⋆ f ≤ δ k L f (w(k)) − L ⋆ f ≤ M 2 δ 1 − m M k ∥w 0 − w ⋆ ∥ 2 2 . (11)
Remark.The first inequality in Equation ( 11) establishes an upper bound for ξ at δ k .Given that 0 ≤ δ k ≤ 1, this confirms the effectiveness of causal exploration in enhancing efficiency.The subsequent inequality establishes that the training error of causal exploration gradually converges to the optimal value, exhibiting an exponential convergence rate characterized by the decay of the factor δ 1 − m M k .Moreover, this theorem implies that the advantages of causal exploration are relevant to the sparseness of causal structure.The sparser the causal structure, the faster our method learns.When the causal matrix D is a complete matrix (δ = 1), causal exploration degenerates into non-causal prediction-based exploration.The proof for Theorem 1 is provided in Appendix A.</p>
<p>Experiments</p>
<p>To evaluate the effectiveness of causal exploration in complex scenarios, we conduct a series of experiments on both synthetic datasets and real-world applications, including the traffic light control task and MuJoCo control suites [Todorov et al., 2012].Meanwhile, we compare our proposed causal exploration with the following baseline methods:</p>
<p>1 Causal Exploration (ours)</p>
<p>Table 1: Attributes of existing baselines and our causal exploration.denotes that a method has an attribute, whereas denotes the opposite."Causal Knowledge" here indicates whether causal information is utilized.Methods with "Explicit Structural Embedding" formulate dynamics models embedded the with causal matrix D. "Online Causal Discovery" and "Non-random Data Collection" measure the contribution of the method to boosting sampling and model learning efficiency from different perspectives, corresponding to Sections 4.2 and 5, respectively.</p>
<p>higher level of accuracy, signifying increased sampling efficiency.2. Structural Difference: Evaluation of causal discovery methods is based on the difference between the identified causal matrix and the ground truth matrix.3. Downstream Performance: For task-agnostic exploration, performance on downstream tasks also serves as a good metric to measure the model's understanding of the underlying causal structures of the environment.</p>
<p>Synthetic Datasets</p>
<p>Environments.We build our simulated environment following the state space model with controls.When the agent takes an action a t based on the current state, the environment provides feedback s t+1 at the next time.We denote the generative environment as
s 1 ∼ N (0, I), s t ∼ N (h(s t−1 , a t−1 ), Σ), (12)
where Σ is the covariance matrix and h is the mean value as the ground truth transition function implemented by deep neural networks under causal graph G. Specifically, the linear condition consists of a single-layer network, and the nonlinear function is three-layer MLPs with sigmoid activation.</p>
<p>Given that the sparsity of the causal graph is an important factor affecting the performance of our method, we choose to demonstrate the superiority of our approach on relatively lowdensity causal structures.That is, G is generated by randomly connecting edges with a probability of p.Such sparse causal structures allow us to evaluate the ability of our method to accurately learn world models with limited data, which is a common scenario in many real-world applications.Below, we provide discussions on the benefits of causal exploration based on the empirical results.Correctness of the identified causal structure.We compare our proposed method with different causal discovery methods.To be specific, CID and CDL apply score-based causal discovery, while ASR combines causal structure with neural networks for differentiable training.However, CID only considers causal action influence and lacks an explicit causal structure, so we do not use it for comparison.Figure 5 illustrates an example of the causal matrix discovered by different methods, where the depth of the color indicates the number of times each edge is detected as a causal connection.Figure 3  methods.The results reflect that the causal matrix learned through causal exploration obtains a smaller bias from the ground-truth matrix than others, an explanation for better performances.Reliability of the learned world models.To show the advantage of causal exploration for improving the quality of model learning, we compare it with all of the baselines.Figure 6 are prediction errors of the world models during exploration under various state and action spaces.For each of these settings, we conduct 10 experiments and take the average value to reduce the impact of randomness.In all of these environments, causal exploration achieves lower prediction errors with fewer data.Compared to Curiosity and Plan2Explore, these advantages stem from the introduction of causality.In addition, the larger the dimension and the sparser the causal structure, the more prominent the advantages of our proposed method over non-causal method will be.The improvements over ASR and CID can be attributed to the effective design of exploration strategies.Furthermore, the experimental results compared with CDL underscore the importance of the stability of causal discovery methods and the necessity of explicit causal embeddings.Advantage of causal-based models at the initial iteration.We also observe that many of the prediction error curves in Figure 6 have a different y intercept, suggesting that causal exploration is better even at the initial iteration.This is because the integration of causal matrix D into the model exclusively incorporates the parent nodes and eliminates extraneous input information when predicting future states, a departure from prior methodologies where these nodes were treated and initialized on par with parent nodes.We note that this advantage is also tied to the correctness of the identified causal matrix and the sparsity of the causal graph as in Theorem 1.  Ablation study and generalization to some challenging scenarios.We further perform ablation studies: For online causal discovery, we provide results with various sampling methods; for the optimization of exploration strategy, we first investigate the impact of active reward and then perform causal exploration with some other forms of intrinsic reward.Besides, since the reliability of identified causal structures is an essential guarantee for the performance of causal exploration, we also consider challenging scenarios where the agent starts with wrong causal beliefs and encounters sudden structural changes.All of these corresponding experimental results, along with the implementation details, are given from Appendix B to Appendix D.</p>
<p>Real World Applications</p>
<p>Application to Traffic Signal Control Task.We further conduct experiments in the challenging traffic signal control task to evaluate our proposed method.Table 2 lists the performance of different learned world models on downstream policy learning.The reward here is a combination of several terms including the sum of queue length and so on, defined in Equation (3) in IntelliLight ( [Wei et al., 2018]).Duration refers to averaged travel time vehicles spent on approaching lanes (in seconds).Length is the sum of the length of waiting vehicles over all approaching lanes and Vehicles is the total number of vehicles that passed the intersection.We also train an agent in the true environment to verify the reliability of the learned models through task-agnostic exploration.The performance of the policy learned on the world models trained during causal exploration is comparable to, or even in some metrics better than, that of the true environment.Appendix C provides the detailed descriptions, including the causal graph on state variables in traffic signal control task, and comparison of the prediction errors during exploration.</p>
<p>Application to MuJoCo Tasks.We also evaluate causal exploration on the challenging MuJoCo tasks, where the state-action dimensions range from tens (Hopper-v2) to hundreds (Humanoid-v2).Implementation details and more experimental results including the identified causal structures are given in Appendix D. As shown in Figure 7(a), the world model learned during causal exploration keeps performing lower prediction errors than others.This ability of our causal method stems from its focus on causal structural constraints.By selectively incorporating parent nodes during state prediction, our approach maintains informed exploration even when exploratory signals decline, which prevents the agent from getting stuck in sub-optimal behaviors.Such a good performance is also achieved in a shorter time.Task performance in Figure 7(b) also reflects the reliability of these world models.We see that our proposed method still works well when the state space is large.As long as there is a strong causal relationship between the observed state variables, and the causal discovery algorithm can accurately identify the corresponding causal structure, our causal exploration method is believed to be well applied to high-dimensional situations.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we introduce causal exploration, a methodology designed to incorporate causal information from data for the purpose of learning world models efficiently.In particular, we employ causal exploration within the domain of task-agnostic reinforcement learning and design a sharing-decomposition schema to leverage causal structural knowledge for the world model.A series of experiments in both simulated environments and real-world tasks demonstrate the superiority of causal exploration, which highlights the importance of rich causal prior knowledge for efficient data collection and model learning.We would like to point out that this study primarily focuses on scenarios where states are fully observed.Future research directions involve addressing more complex scenarios including unobserved latent state variables and developing improved fault-tolerant mechanisms to enhance robustness.This work provides a promising direction for future endeavors in efficient exploration.</p>
<p>In the following, we first present the proof of Theorem 1.After that, more experimental details about the synthetic experiments and real-world applications are given.</p>
<p>A Proof of Theorem 1</p>
<p>Recall that we consider solving a linear regression problem under m strong convex and M -smooth.MSE loss function
L f (w) in the formulation ∥w ⊤ • (s t−1 , a t−1 ) − s t ∥ 2 2 with s t = w ⋆⊤ • (s t−1 , a t−1 ) is minimized by gradient descent method with w(k) = w(k − 1) − α∇L f (w(k − 1)) (13)
where α is the corresponding step size.Denote w c (k) as the network parameters taking causal constraints with respect to w(k) that does not gather causal information.Theorem 1 shows that causal exploration gets a prediction error bound δ k times lower at the k-th step, where δ is a density measurement of the causal adjacency matrix D.</p>
<p>Below, we present a two-step proof for the convergence of causal exploration.Lemma 2 provides an upper bound for convergence without using any causal information.Lemma 3 demonstrates that utilizing causal structure information results in w c (k) being closer to the optimal value w ⋆ compared to w(k) at the same optimization steps.Combining Lemma 2 and Lemma 3, we derive the convergence rate for causal exploration.Lemma 2. Suppose L f (w) is m-strongly convex and Msmooth.We have
∥w(k) − w ⋆ ∥ 2 ≤ (1 − m M ) k ∥w 0 − w ⋆ ∥ 2 .(14)
Proof.According to gradient descent method (13), we get
∥w(k) − w ⋆ ∥ 2 = ∥w(k − 1) − α∇L f (w(k − 1)) − w ⋆ ∥ 2 = ∥w(k − 1) − w ⋆ ∥ 2 + α 2 ∥∇L f (w(k − 1))∥ 2 − 2α∇L f (w(k − 1))(w(k − 1) − w ⋆ ).
By strong convexity
∇L f (w)(w − w ⋆ ) ≥ L f (w) − L f (w ⋆ ) + m 2 ∥w − w ⋆ ∥ 2 , (15) we further obtain ∥w(k) − w ⋆ ∥ 2 ≤ ∥w(k − 1) − w ⋆ ∥ 2 − 2α(L f (w(k − 1)) − L f (w ⋆ ) + m 2 ∥w(k − 1) − w ⋆ ∥ 2 ) + α 2 ∥∇L f (w(k − 1))∥ 2 = ∥w(k − 1) − w ⋆ ∥ 2 − 2α(L f (w(k − 1)) − L f (w ⋆ )) − αm∥w(k − 1) − w ⋆ ∥ 2 + α 2 ∥∇L f (w(k − 1))∥ 2 ≤ ∥w(k − 1) − w ⋆ ∥ 2 − 2α(L f (w(k − 1)) − L f (w ⋆ )) − αm∥w(k − 1) − w ⋆ ∥ 2 + 2α 2 M (L f (w(k − 1)) − L f (w ⋆ )) ≤ ∥w(k − 1) − w ⋆ ∥ 2 − αm∥w(k − 1) − w ⋆ ∥ 2 + 2α(αM − 1)(L f (w(k − 1)) − L f (w ⋆ )). (16) Consider α = 1 M , we get ∥w(k) − w ⋆ ∥ 2 ≤ (1 − m M )∥w(k − 1) − w ⋆ ∥ 2 . (17)
Using the above equation repeatedly, we obtain
∥w(k) − w ⋆ ∥ 2 ≤ (1 − m M )∥w(k − 1) − w ⋆ ∥ 2 ≤ (1 − m M ) 2 ∥w(k − 2) − w ⋆ ∥ 2 ≤ • • • ≤ (1 − m M ) k ∥w 0 − w ⋆ ∥ 2 . (18)
Lemma 3. Suppose w c (k) and w(k) are the network parameters with/without causal structure respectively and w ⋆ is the optimum.It holds that
∥w c (k) − w ⋆ ∥ 2 ≤ δ k ∥w(k) − w ⋆ ∥ 2 . (19)
Proof.According to Definition 1, we have
w c (k) = D ⊙ w(k), (20)
where D is the binary causal matrix.Hence, we rewrite w(k) as
w(k) = D ⊙ w(k) + (1 − D) ⊙ w(k) = w c (k) + w ′ k . (21) Note that we have w ⋆ = D ⊙ w ⋆ . Then we obtain ∥w(k) − w ⋆ ∥ 2 = i,j (w ij (k) − w ⋆ ij ) 2 = i,j D ij × w ij (k) − w ⋆ ij + (1 − D ij ) × w ij (k) 2 = i,j (D ij × w ij (k) − w ⋆ ij ) 2 + ((1 − D ij ) × w ij (k)) 2 = i,j (D ij × w ij (k) − w ⋆ ij ) 2 + i,j ((1 − D ij ) × w ij (k)) 2 = i,j (w c ij (k) − w ⋆ ij ) 2 + i,j (w ′ ij )(k) 2 = ∥w c (k) − w ⋆ ∥ 2 + ∥w ′ k ∥ 2 ≥ (1 + ρ k )∥w c (k) − w ⋆ ∥ 2 ,(22)
where ρ k ∈ [0, +∞) is the lower bound of the ratio between ∥w ′ k ∥ 2 and ∥w c (k)−w ⋆ ∥ 2 whose value is related to the sparsity of causal matrix D. By setting δ k = 1 1+ρ k , we complete the proof of Lemma 3.</p>
<p>The convexity of L
f (w) implies L f (w c (k)) − L f (w ⋆ ) ≤ M 2 ∥w c (k) − w ⋆ ∥ 2 . (23)
By applying Lemma 2 and Lemma 3 into (23) and denote
δ = max{δ 0 , δ 1 , . . . , δ k },(24)
we can obtain Theorem 1.</p>
<p>B Synthetic environment    [ −8, 8].After that, for each edge in the graph, we randomly drop it out with an empirically chosen probability 1−p = 0.8.Besides, the covariance Σ is a diagonal matrix whose elements are randomly generated from [0, 0.1].For both linear and nonlinear conditions, we have conducted sufficient experiments.Figure 10 shows the remaining experimental results that are not fully presented in the main body.</p>
<p>In order to enhance the agent's sensitivity to causal informative data, we design a novel form of active reward.</p>
<p>Here we investigate the impact of β in this new intrinsic reward formulation on causal exploration which is formulated as r t = r i t + βr a t in equation ( 7). Figure 11(a) illustrates the evolution of prediction error over training time for different values of β.We see that incorporating active reward for exploration continuously improves the performance of the world model.We set β = 0.5 for linear environments and β = 3 for both nonlinear settings and real-world applications according to the results in Figure 11(a), respectively.</p>
<p>B.2 Generalization to underestimation scenarios</p>
<p>In some data-hungry scenarios, there may be insufficient data for causal discovery, leading to underestimation of the causal structure, which makes continuous data collection and causal structure correction important components.In other words, the causal structure inferred from causal discovery algorithms may deviate from the ground truth ones, which is particularly prone to occur under conditions of limited sample size or during the initial stages of exploration.Consequently, we conduct an evaluation of our proposed algorithm's performance  when the estimated causal structure exhibits insufficiencies or redundancies, a scenario termed "underestimation".To highlight the advantages of our sharing-decomposition schema in addressing such problems, we deliberately provide the agent with completely wrong causal information.An erroneous causal graph G ′ is supplied to the agent during the initial time steps, namely t &lt; N where N is the period for causal discovery.We introduce perturbations to the true graph G by randomly eliminating or adding edges with a probability of p ′ = 0.8, generating G ′ .Figure 11(b) shows the corresponding performance of causal exploration on synthetic data.Indeed, the agent exhibits an impressive ability to correct its causal exploration trajectory in the face of misdirection.Nonetheless, it is worth noting that such a schema only serves as a technique to mitigate the impact of underestimation.The reliability of causal discovery algorithms is the essential guarantee for causal exploration.</p>
<p>B.3 Generalization to scenarios with causal structural changes</p>
<p>In real-world scenarios, causal structure between variables can often change due to sudden disturbance.For instance, causal relationships between economic variables like stock prices, interest rates, and inflation can be subject to rapid changes caused by market crashes or policy changes.</p>
<p>To evaluate the effectiveness of our approach in handling such mutation, we conduct experiments in a scenario where the causal structure changes randomly once.We use our simulation model to generate the data and compare our method to a non-causal approach.Figure 11(c) illustrates the advantages of our approach in tackling such a challenging task.</p>
<p>Our sharing-decomposition schema enables the agents to quickly adapt to structural changes and make appropriate adjustments.This also demonstrates the robustness of our method, which allows for timely correction of errors in the causal structure.By sharing the same decomposition modules across different time steps and tasks, our method can effectively leverage previous knowledge and transfer it to new situations, while also being flexible enough to accommodate changes in the causal structure.In addition, the ability to adapt to changing causal structures can improve the generalization ability of our method, making it more applicable to a wider range of real-world tasks.</p>
<p>In our future research, we plan to expand our work to situations where changes occur within the model.In these cases, during the model learning phase, it becomes crucial to effectively detect these changes and promptly update the model.Additionally, when it comes to policy learning, a key challenge is determining the most suitable model to utilize.We may encounter entirely new models that have not been encountered before, adding an additional layer of complexity to our research.</p>
<p>C Traffic Signal Control</p>
<p>Traffic signal control is an important means of mitigating congestion in traffic management.Compared to using fixedduration traffic signals, an RL agent learns a policy to determine real-time traffic signal states based on current road conditions.The state observed by the agent at each time consists of five dimensions of information, namely the number of vehicles, queue length, average waiting time in each lane plus current and next traffic signal states.Action here is to decide whether to change the traffic signal state or not.For example, suppose the traffic signal is red at time t, if the agent takes action 1, then it will change to green at the next time t + 1, otherwise, it will remain red.Following the work in IntelliLight [Wei et al., 2018], the traffic environment in our experiment is a three-lane intersection.Table 4 gives a detailed description, and Figure 12(a) provides an illustration.The estimated causal structures are given in Figure 12(b).</p>
<p>Experiment details and analysis.We first only use prediction-based causal exploration to learn forward dynamic world models under the same traffic environment in In-telliLight.Then, the agent learns a policy for traffic signal control task in our learned world models, which avoids the high-cost interaction with real traffic environment.For consistency and easy comparison, we use the same DQN network from IntelliLight to train our causal exploration agent.After that, we solve the traffic signal control task in our world model with no environment interaction in a zero-shot manner.Figure 13 visualizes the prediction errors of different models during the exploration process.The corresponding causal graph is shown in Figure 12(b).As is illustrated, the state of the traffic signal at the next time step s 4,t is causally linked to the previous state s 4,t−1 and action a t−1 , which is in line with the definition of traffic signal control tasks.The queue length s 2,t is determined by previous queue length s 2,t−1 and waiting time s 3,t−1 plus the traffic state s 5,t−1 .Factors influencing the waiting time s 3,t include the number of vehicles s 1,t−1 and the queue length s 2,t−1 .These results align well with the common-sense reasoning.</p>
<p>D More Results of Mujoco tasks</p>
<p>We use PPO algorithm [Schulman et al., 2017] for optimization during both the task-agnostic exploration and policy learning stages and adopt the hyperparameters from Table 3 of PPO with a trajectory length of 2048, an Adam stepsize of 3e-4, a minibatch size of 64, a discount factor (γ = 0.99), a GAE parameter (λ = 0.95), and a clipping parameter (ϵ = 0.2).Both the actor-critic network and the world model are 2-(hidden)-layer neural networks, consisting of 256 and 64 hidden nodes respectively.Activation functions are Tanh and ReLU here.</p>
<p>Performance of causal exploration on some other MuJoCo tasks are provided in Figure 15.Predictions given by world models under causal structural constraints are more accurate and stable than those of other methods.The learned world model of causal exploration provides the agent with more information in the following policy learning stage, resulting in higher scores achieved in a shorter time.Figure 14 illustrates the identified causal structures during exploration, which explains for the performance gain.We also conduct several experiments to test the performance of causal exploration with a different form of intrinsic reward.To be specific, we formulate our world model as µ w c (s t , a t ), σ 2 w c (s t , a t ) to model the transition probability as p(s t+1 | s t , a t ) ∼ N (s t+1 ; µ w c , σ 2 w c ).Then, the negative log-likelihood is used both for the world model learning and causal exploration, which is a replacement for equation ( 3) and ( 5), and is formulated as:  16.However, various forms of intrinsic rewards don't exhibit significant differences in performance.In some tasks, the introduction of an additional covariance network even lead to performance not as favorable as when directly using regression loss.
L (µ w c ,σ 2 w c ) = (s t+1 − µ w c (
Figure 2 :
2
Figure 2: An illustration of model architectures under (a) naive causal factorization; (b) our sharing-decomposition schema.</p>
<p>Figure 3 :
3
Figure 3: Average F1-score of the identified causal graph compared to the ground truth graph across various causal discovery methods.More details are given in Section 6.</p>
<p>Figure 4 :
4
Figure 4: An overview of Causal Exploration framework.Throughout the process, the agent, guided by policy πt, engages in exploration to gather data that are most beneficial for model training.Meanwhile, causal knowledge and the world model are continuously refined with the ongoing data collection.</p>
<p>Figure 5: A comparison of the discovered causal matrix with different methods when |U| = 12 and |V| = 10.</p>
<p>Figure 6 :
6
Figure 6: Prediction ability of different learned models on the synthetic environment under various state and action spaces.</p>
<p>Figure 7 :
7
Figure 7: (a) Prediction errors in world model learning and (b) downstream policy learning performance on MuJoCo tasks.</p>
<p>Figure 8 :
8
Figure 8: ROC curves of online causal discovery using different sampling methods.</p>
<p>Figure 9 :
9
Figure 9: Time cost for the PC algorithm when |U| = 12 and |V| = 10.</p>
<p>Figure 10 :
10
Figure 10: Results on synthetic datasets.</p>
<p>Figure 12: (a) Illustration of the traffic-signal-control environment.(b) Causal graph learned through exploration.</p>
<p>Figure 13 :
13
Figure 13: Prediction errors in traffic light control.</p>
<p>Figure 14 :
14
Figure 14: Identified causal structures for MuJoCo tasks.</p>
<p>Figure 15 :
15
Figure 15: Application to some other MuJoCo tasks.</p>
<p>Figure 16 :
16
Figure 16: Performance of causal exploration with different forms of intrinsic rewards.</p>
<p>Forward world Model f with parameter w c Causal discovery period N , Dataset B t Exploration policy π with memory buffer M 2: Output: Forward world model f 3: for Episode = 1, 2, . . .do , a t−1 , r t−1 , s t ) into M
4:Collect test set D h for the calculation of active reward5:Set current time t = 16:while t &lt; T do7:Choose action a t−1 and predict next state ŝt8:Obtain s t from environment9:Calculate intrinsic reward r t−110:Store (s t−1 , a t−1 , s t ) into B t11: Store (s t−1 12: if t = 0 (mod N ) then13:Do online causal discovery on top-κ of B t and getcausal graph G14:end if15:Train world model under G on B t and update w c16:Train exploration policy on M and update π17:
Let k denote the optimization step in world model training, f represent the world model, and Algorithm 1 Task-agnostic Causal Exploration 1: Initialize:</p>
<p>Table 2 :
2
Performances of downstream policy learning in traffic signal control task.
ModelReward Duration Length VehiclesCuriosity-3.5110.921.55304CID-4.3612.453.62288ASR-1.698.230.92312CDL-2.559.391.32308Ours-1.377.210.31314True Environment-1.017.610.11316</p>
<p>± 0.084 0.892 ± 0.065 0.891 ± 0.175 0.816 ± 0.353 0.969 ± 0.016 Precision 0.953 ± 0.074 0.914 ± 0.079 0.942 ± 0.089 0.892 ± 0.081 0.970 ± 0.064 F1-score 0.831 ± 0.094 0.842 ± 0.104 0.890 ± 0.139 0.852 ± 0.067 0.928 ± 0.096 Time 4.360 ± 2.888 0.880 ± 0.453 0.909 ± 0.414 1.389 ± 0.760 0.379 ± 0.267
MethodNo SamplingUniformK-centerHardestCoresetAUC0.907B.1 More experiment detailsImplementation. Double DQN [Van Hasselt et al., 2016]is used to train the exploration policy of agents, where boththe evaluation network and the target network are three-layer</p>
<p>Table 3 :
3
Results of different selection methods on online causal discovery.</p>
<p>Table 4 :
4
Traffic Dataset Description</p>
<p>s t , a t )) 2 2σ 2 w c (s t , a t )
+1 2log σ 2 w c (s t , a t ),r i t =η 2L (µ w c ,σ 2 w c ) .(25)Corresponding results are shown in Figure
AcknowledgmentsYupei Yang, Shikui Tu and Lei Xu would like to acknowledge the support by the Shanghai Municipal Science and Technology Major Project, China (Grant No. 2021SHZDZX0102), and by the National Natural Science Foundation of China (62172273).
A survey on intrinsic motivation in reinforcement learning. Aubret, arXiv:1908.06976arXiv:1808.04355Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. Burda, 2019. 2019. 2016. 2016. 2018. 201829arXiv preprintAdvances in neural information processing systems</p>
<p>Generalizing goal-conditioned reinforcement learning with variational causal reasoning. Ding, arXiv:1708.02383arXiv:2107.02729Adarl: What, where, and how to adapt in transfer reinforcement learning. Biwei Huang, Fan Feng2022. 2022. 2016. 2016. 2017. 2017. 2022. 20064-20076, 2022. 2021. 202135arXiv preprintAdvances in Neural Information Processing Systems</p>
<p>Action-sufficient state representation learning for control with structural constraints. Huang, arXiv:2107.00848arXiv:1810.01176Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMarlos C Machado, Marc G Bellemare, and Michael Bowling2022. 2022. 2021. 2021. 2018. 2018. 2022. 2020. 202034arXiv preprintInternational Conference on Machine Learning</p>
<p>Provably efficient causal model-based reinforcement learning for systematic generalization. Mutti, arXiv:1710.10628Proceedings of the AAAI Conference on Artificial Intelligence. Pathak, the AAAI Conference on Artificial IntelligencePMLR2023. 2023. 2017. 2017. 201737Variational continual learning. arXiv preprintInternational conference on machine learning</p>
<p>Online continual learning with maximally interfered retrieval. Pathak, International conference on machine learning. Aljundi Rahaf and Caccia Lucas2019. 2019. 2019NIPS</p>
<p>Transformer-based world models are happy with 100k interactions. Robine, arXiv:2303.07109arXiv:1707.06347Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 2023. 2023. 2017. 2017arXiv preprint</p>
<p>. Seitzer, 2021Maximilian SeitzerBernhard</p>
<p>Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. Georg Schölkopf, ; Martius, Sekar, Advances in Neural Information Processing Systems. PMLR2021. 2020. 202034International Conference on Machine Learning</p>
<p>Model-based active exploration. Shyam, International conference on machine learning. PMLR2019. 2019</p>
<p>Mastering the game of go with deep neural networks and tree search. Silver, nature. 52975872016. 2016</p>
<p>Causation, prediction, and search. Spirtes, arXiv:1801.00690Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe LiMIT press2000. 2000. 2018. 2018arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. Todorov, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2012. 2012</p>
<p>Deep reinforcement learning with double q-learning. Van Hasselt, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2016. 201630</p>
<p>Wang , arXiv:2206.13452Causal dynamics learning for task-independent state abstraction. 2022. 2022arXiv preprint</p>
<p>Intellilight: A reinforcement learning approach for intelligent traffic light control. Wei, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. So Kweon, the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018. 2018. 2019Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</p>
<p>Online coreset selection for rehearsal-based continual learning. Yoon, arXiv:2106.010852021. 2021arXiv preprint</p>
<p>Kernel-based conditional independence test and application in causal discovery. Yu , arXiv:1202.3775arXiv:2206.01474Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. the Thirty-Second International Joint Conference on Artificial Intelligence2023. 2023. 2012. 2012. 2019. 2019. 2022arXiv preprintOffline reinforcement learning with causal structured world models</p>            </div>
        </div>

    </div>
</body>
</html>
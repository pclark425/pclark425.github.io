<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5455 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5455</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5455</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-d0612d91867c879b44be4b56cebdc725e2942172</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d0612d91867c879b44be4b56cebdc725e2942172" target="_blank">CYCLE: Learning to Self-Refine the Code Generation</a></p>
                <p><strong>Paper Venue:</strong> Proc. ACM Program. Lang.</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes Cycle framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites, and reveals that Cycle successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the selfrefinement capability of code LMs.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well. In this paper, we propose Cycle framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate Cycle on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that Cycle successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the selfrefinement capability of code LMs. We implement four variants of Cycle with varied numbers of parameters across 350M,1B,2B, and 3B, and the experiments show that Cycle consistently boosts the code generation performance, by up to 63.5%, across benchmarks and varied model sizes. We also notice that Cycle outperforms code LMs that have 3× more parameters in self-refinement.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5455.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5455.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CyCle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CYCLE: Learning to Self-Refine the Code Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training and inference framework that teaches code language models to iteratively self-refine generated code by jointly conditioning on (1) the NL problem description, (2) the model's previous (faulty) generation, and (3) execution/test-suite feedback, with a Past Generation Mask to prevent copying shortcuts and a data-mixture strategy to preserve one-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CyCle-350M / CyCle-1B / CyCle-2.7B / CyCle-3B (initialized from CodeGen-350M, StarCoder-1B, CodeGen-2.7B, StarCoder-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer code LMs initialized from CodeGen and StarCoder checkpoints; four Cycle variants: 350M, 1B, 2.7B, and 3B parameters. Training continues from these checkpoints with next-token objective on aggregated self-refinement samples; inference uses nucleus sampling (top-p=0.95).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative self-refinement (generate-then-reflect with execution feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At inference: generate a candidate program from the NL prompt, run its test suite, aggregate (NL prompt, faulty generation, execution feedback) via a docstring/comment template, feed that aggregated context back to the same model to generate a refined program; repeat up to a configured max (default 4) or until tests pass. During training the model is fine-tuned on paired (aggregated context -> canonical solution) examples constructed by prompting fine-tuned models to generate faulty outputs and collecting execution traces. To avoid copying the faulty code, a Past Generation Mask (PGM) randomly hides p% of tokens in the faulty generation during training; data-mixing (canonical solutions + self-refine samples) balances one-shot and refinement capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval, MBPP-S (MBPP-Sanitized), APPS</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automated code-generation benchmarks: HumanEval (164 Python problems with unit tests), MBPP-S (426 sanitized short Python tasks), APPS (filtered subset; programming problems with many test cases). Models are evaluated by test-suite pass rates (Pass@1, Pass@5 where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-refinement (iterative) pass rates reported per variant (Pass@1): CyCle-350M self-refine: HumanEval 20.7% (vs one-time 14.0%), MBPP-S 32.6% (vs 19.9%), APPS 8.7% (vs 7.5%); CyCle-1B self-refine: HumanEval 22.0% (one-time 18.3%), MBPP-S 35.8% (one-time 25.8%), APPS 10.9% (one-time 8.9%); CyCle-2.7B self-refine: HumanEval 29.3% (one-time 21.4%), MBPP-S 48.5% (one-time 35.8%), APPS 11.6% (one-time 9.1%); CyCle-3B self-refine: HumanEval 29.9% (one-time 24.4%), MBPP-S 51.3% (one-time 36.3%), APPS 11.3% (one-time 9.0%). Relative improvements up to +63.5% (MBPP-S, CyCle-350M).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>One-time (no iterative refinement) pass rates (Pass@1) from Table 1: CodeGen-350M init -> CyCle-350M one-time 14.0% (HumanEval), 19.9% (MBPP-S), 7.5% (APPS). CyCle variants' one-time numbers are listed above; the paper reports both one-time and iterative results for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: Across benchmarks and model sizes, iterative self-refinement increases test-suite pass rates relative to one-time generation; e.g., CyCle-350M: HumanEval +47.9% relative, MBPP-S +63.5% relative; CyCle variants often outperform or match larger baseline models (e.g., CyCle-350M outperforms StarCoder-1B). Additional evidence: per-step curves (Figure 4) show continued improvement across refinement steps and did not plateau by 4 steps; ablations show removing execution feedback severely reduces CyCle's self-refinement gains, verifying the contribution of execution traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) naive inclusion of previous faulty code leads models to copy (shortcut) — addressed by PGM but depends on masking ratio (optimal ~0.05; too large harms performance); (2) sensitivity to training data mixture — pure self-refine data can harm one-shot generation, so data-mix (≈25% self-refine samples) is recommended; (3) dependence on execution/test feedback — removing execution feedback substantially degrades performance; (4) evaluation and findings limited to model sizes up to 3B (authors did not study very large models like 175B); (5) iterative refinement increases inference overhead but is empirically comparable to top-k generation and requires executing test suites per iteration; (6) potential distribution overlap between training data and some benchmarks (APPS filtering done, but distributional effects noted).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CYCLE: Learning to Self-Refine the Code Generation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5455.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5455.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla CodeGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen (vanilla, pre-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source autoregressive code language model used as baseline (various sizes); when prompted with concatenated faulty outputs and execution feedback, it often fails to meaningfully self-refine and frequently copies prior faulty code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeGen-350M and CodeGen-2.7B (vanilla checkpoints used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CodeGen family of autoregressive code LMs pre-trained on large corpora (The Pile, BigQuery, BigPython), with varied sizes (350M up to many billions). The paper uses CodeGen-350M and CodeGen-2.7B as baseline initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>naive generate-then-reflect (prompt concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Attempted simple concatenation of prior faulty generation and execution feedback with the problem description (as an additional prompt/context) and re-prompting the same baseline model to generate a refined solution (no additional fine-tuning for self-refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval, MBPP-S, APPS (same benchmarks as CyCle)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation tasks verified by unit test suites; same benchmark descriptions as for CyCle.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported baseline self-refinement (re-prompting with faulty generation + execution feedback) yields limited gains. Example numbers from Table 1: CodeGen-350M one-time: HumanEval 12.2%, MBPP-S 19.0%, APPS 6.9%; reported self-refine outcomes: HumanEval 12.2 (+0.0% rel), MBPP-S 21.8 (+14.8% rel), APPS 6.9 (+0.0% rel). CodeGen-2.7B one-time HumanEval 21.9%, MBPP-S 34.7%, APPS 7.1%; self-refine gives HumanEval 23.8 (+8.4%), MBPP-S 35.4 (+2.0%), APPS 7.1 (+0.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>See one-time numbers above (e.g., CodeGen-350M: HumanEval 12.2%, MBPP-S 19.0%, APPS 6.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Small or inconsistent improvements from naive re-prompting: occasional modest gains on MBPP-S but no improvement on APPS in several sizes; additionally, the paper reports an 'exact-copy' phenomenon: when naively concatenating faulty generation and execution feedback, CodeGen produced an exact copy of the faulty code in 42.2% of cases on HumanEval (motivating the need for masking and specialized training).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High exact-copy rates (42.2% quoted) when given faulty generation + feedback; baseline models generally do not effectively use execution feedback and sometimes treat it as noise; fine-tuning only on correct code provides marginal self-refinement benefit compared to targeted self-refinement training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CYCLE: Learning to Self-Refine the Code Generation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5455.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5455.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla StarCoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StarCoder (vanilla, pre-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source code language model used as baseline; shows even higher tendency than CodeGen to copy prior faulty generations when naively prompted with execution feedback, and modest or no self-refinement gains without specialized training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StarCoder-1B and StarCoder-3B (vanilla checkpoints used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>StarCoder family pre-trained on The Stack with next-token and fill-in-the-middle objectives; variants include 1B and 3B parameter models used in the paper as baselines and Cycle initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>naive generate-then-reflect (prompt concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same naive strategy as for CodeGen: concatenate faulty generation and execution feedback to the prompt and re-prompt the baseline. No specialized self-refinement fine-tuning was applied for vanilla baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval, MBPP-S, APPS</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same code generation benchmarks with test-suite evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Table 1 shows limited self-refinement gains for StarCoder baselines: StarCoder-1B one-time: HumanEval 15.9%, MBPP-S 25.8%, APPS 7.3%; self-refine yields HumanEval 16.5 (+3.8%), MBPP-S 28.1 (+9.1%), APPS 7.3 (+0.0%). StarCoder-3B one-time: HumanEval 23.8%, MBPP-S 35.1%, APPS 7.3%; self-refine gives HumanEval 26.8 (+12.8%), MBPP-S 40.5 (+15.3%), APPS 7.4 (+1.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>See one-time numbers above (e.g., StarCoder-1B: HumanEval 15.9%, MBPP-S 25.8%, APPS 7.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Small to modest improvements in some settings, but the paper reports a high exact-copy rate when naively given faulty generation + feedback: StarCoder copied the faulty code in 64.8% of HumanEval cases in the authors' initial experiments, highlighting ineffectiveness of naive prompt-based self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very high tendency to reproduce the faulty code verbatim when re-prompted with the faulty generation and execution feedback (64.8% exact-copy rate); generally unable to extract corrective signals from execution feedback without targeted self-refinement training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CYCLE: Learning to Self-Refine the Code Generation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5455.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5455.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (motivation example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (example used for motivation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source conversational/code-capable model (GPT-3.5) used by the authors as a motivating example showing that naively concatenating faulty code and execution feedback fails to induce self-refinement; the model often copy-pastes the faulty code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-3.5 family (referenced for a short motivating example); used with prompt concatenation in the paper to illustrate failure modes of naive re-prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>naive generate-then-reflect (prompt concatenation in a single prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Authors prompted GPT-3.5 with a HumanEval problem, then concatenated the model's failed generation and unit-test failure messages to the prompt and asked GPT-3.5 to refine; the model failed to utilize the execution feedback and largely copy-pasted the faulty code instead of fixing it.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval (single motivating example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A HumanEval programming problem (Task No. 106) used as an illustrative example; demonstration that naive concatenation fails for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Anecdotal: GPT-3.5 in this example could not self-refine effectively; it copy-pasted the faulty program as the new prediction (no numeric benchmark reported for this example).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Anecdotal: initial (one-time) GPT-3.5 generation failed the test suite for the motivating example.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>None in the anecdotal example; the example is used to motivate that naive concatenation of feedback does not guarantee self-refinement and that models can ignore or misinterpret execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows a failure case: GPT-3.5 ignored execution feedback and produced a copy of the faulty code rather than a corrected solution in the authors' motivating example.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CYCLE: Learning to Self-Refine the Code Generation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CodeGen <em>(Rating: 2)</em></li>
                <li>StarCoder <em>(Rating: 2)</em></li>
                <li>Chen et al. 2023 <em>(Rating: 2)</em></li>
                <li>Ouyang et al. 2022 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5455",
    "paper_id": "paper-d0612d91867c879b44be4b56cebdc725e2942172",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "CyCle",
            "name_full": "CYCLE: Learning to Self-Refine the Code Generation",
            "brief_description": "A training and inference framework that teaches code language models to iteratively self-refine generated code by jointly conditioning on (1) the NL problem description, (2) the model's previous (faulty) generation, and (3) execution/test-suite feedback, with a Past Generation Mask to prevent copying shortcuts and a data-mixture strategy to preserve one-shot performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CyCle-350M / CyCle-1B / CyCle-2.7B / CyCle-3B (initialized from CodeGen-350M, StarCoder-1B, CodeGen-2.7B, StarCoder-3B)",
            "model_description": "Autoregressive transformer code LMs initialized from CodeGen and StarCoder checkpoints; four Cycle variants: 350M, 1B, 2.7B, and 3B parameters. Training continues from these checkpoints with next-token objective on aggregated self-refinement samples; inference uses nucleus sampling (top-p=0.95).",
            "reflection_method_name": "iterative self-refinement (generate-then-reflect with execution feedback)",
            "reflection_method_description": "At inference: generate a candidate program from the NL prompt, run its test suite, aggregate (NL prompt, faulty generation, execution feedback) via a docstring/comment template, feed that aggregated context back to the same model to generate a refined program; repeat up to a configured max (default 4) or until tests pass. During training the model is fine-tuned on paired (aggregated context -&gt; canonical solution) examples constructed by prompting fine-tuned models to generate faulty outputs and collecting execution traces. To avoid copying the faulty code, a Past Generation Mask (PGM) randomly hides p% of tokens in the faulty generation during training; data-mixing (canonical solutions + self-refine samples) balances one-shot and refinement capabilities.",
            "num_iterations": 4,
            "task_name": "HumanEval, MBPP-S (MBPP-Sanitized), APPS",
            "task_description": "Automated code-generation benchmarks: HumanEval (164 Python problems with unit tests), MBPP-S (426 sanitized short Python tasks), APPS (filtered subset; programming problems with many test cases). Models are evaluated by test-suite pass rates (Pass@1, Pass@5 where applicable).",
            "performance_with_reflection": "Self-refinement (iterative) pass rates reported per variant (Pass@1): CyCle-350M self-refine: HumanEval 20.7% (vs one-time 14.0%), MBPP-S 32.6% (vs 19.9%), APPS 8.7% (vs 7.5%); CyCle-1B self-refine: HumanEval 22.0% (one-time 18.3%), MBPP-S 35.8% (one-time 25.8%), APPS 10.9% (one-time 8.9%); CyCle-2.7B self-refine: HumanEval 29.3% (one-time 21.4%), MBPP-S 48.5% (one-time 35.8%), APPS 11.6% (one-time 9.1%); CyCle-3B self-refine: HumanEval 29.9% (one-time 24.4%), MBPP-S 51.3% (one-time 36.3%), APPS 11.3% (one-time 9.0%). Relative improvements up to +63.5% (MBPP-S, CyCle-350M).",
            "performance_without_reflection": "One-time (no iterative refinement) pass rates (Pass@1) from Table 1: CodeGen-350M init -&gt; CyCle-350M one-time 14.0% (HumanEval), 19.9% (MBPP-S), 7.5% (APPS). CyCle variants' one-time numbers are listed above; the paper reports both one-time and iterative results for direct comparison.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: Across benchmarks and model sizes, iterative self-refinement increases test-suite pass rates relative to one-time generation; e.g., CyCle-350M: HumanEval +47.9% relative, MBPP-S +63.5% relative; CyCle variants often outperform or match larger baseline models (e.g., CyCle-350M outperforms StarCoder-1B). Additional evidence: per-step curves (Figure 4) show continued improvement across refinement steps and did not plateau by 4 steps; ablations show removing execution feedback severely reduces CyCle's self-refinement gains, verifying the contribution of execution traces.",
            "limitations_or_failure_cases": "Reported limitations include: (1) naive inclusion of previous faulty code leads models to copy (shortcut) — addressed by PGM but depends on masking ratio (optimal ~0.05; too large harms performance); (2) sensitivity to training data mixture — pure self-refine data can harm one-shot generation, so data-mix (≈25% self-refine samples) is recommended; (3) dependence on execution/test feedback — removing execution feedback substantially degrades performance; (4) evaluation and findings limited to model sizes up to 3B (authors did not study very large models like 175B); (5) iterative refinement increases inference overhead but is empirically comparable to top-k generation and requires executing test suites per iteration; (6) potential distribution overlap between training data and some benchmarks (APPS filtering done, but distributional effects noted).",
            "uuid": "e5455.0",
            "source_info": {
                "paper_title": "CYCLE: Learning to Self-Refine the Code Generation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Vanilla CodeGen",
            "name_full": "CodeGen (vanilla, pre-trained)",
            "brief_description": "Open-source autoregressive code language model used as baseline (various sizes); when prompted with concatenated faulty outputs and execution feedback, it often fails to meaningfully self-refine and frequently copies prior faulty code.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeGen-350M and CodeGen-2.7B (vanilla checkpoints used as baselines)",
            "model_description": "CodeGen family of autoregressive code LMs pre-trained on large corpora (The Pile, BigQuery, BigPython), with varied sizes (350M up to many billions). The paper uses CodeGen-350M and CodeGen-2.7B as baseline initializations.",
            "reflection_method_name": "naive generate-then-reflect (prompt concatenation)",
            "reflection_method_description": "Attempted simple concatenation of prior faulty generation and execution feedback with the problem description (as an additional prompt/context) and re-prompting the same baseline model to generate a refined solution (no additional fine-tuning for self-refinement).",
            "num_iterations": null,
            "task_name": "HumanEval, MBPP-S, APPS (same benchmarks as CyCle)",
            "task_description": "Code generation tasks verified by unit test suites; same benchmark descriptions as for CyCle.",
            "performance_with_reflection": "Reported baseline self-refinement (re-prompting with faulty generation + execution feedback) yields limited gains. Example numbers from Table 1: CodeGen-350M one-time: HumanEval 12.2%, MBPP-S 19.0%, APPS 6.9%; reported self-refine outcomes: HumanEval 12.2 (+0.0% rel), MBPP-S 21.8 (+14.8% rel), APPS 6.9 (+0.0% rel). CodeGen-2.7B one-time HumanEval 21.9%, MBPP-S 34.7%, APPS 7.1%; self-refine gives HumanEval 23.8 (+8.4%), MBPP-S 35.4 (+2.0%), APPS 7.1 (+0.0%).",
            "performance_without_reflection": "See one-time numbers above (e.g., CodeGen-350M: HumanEval 12.2%, MBPP-S 19.0%, APPS 6.9%).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Small or inconsistent improvements from naive re-prompting: occasional modest gains on MBPP-S but no improvement on APPS in several sizes; additionally, the paper reports an 'exact-copy' phenomenon: when naively concatenating faulty generation and execution feedback, CodeGen produced an exact copy of the faulty code in 42.2% of cases on HumanEval (motivating the need for masking and specialized training).",
            "limitations_or_failure_cases": "High exact-copy rates (42.2% quoted) when given faulty generation + feedback; baseline models generally do not effectively use execution feedback and sometimes treat it as noise; fine-tuning only on correct code provides marginal self-refinement benefit compared to targeted self-refinement training.",
            "uuid": "e5455.1",
            "source_info": {
                "paper_title": "CYCLE: Learning to Self-Refine the Code Generation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Vanilla StarCoder",
            "name_full": "StarCoder (vanilla, pre-trained)",
            "brief_description": "Open-source code language model used as baseline; shows even higher tendency than CodeGen to copy prior faulty generations when naively prompted with execution feedback, and modest or no self-refinement gains without specialized training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "StarCoder-1B and StarCoder-3B (vanilla checkpoints used as baselines)",
            "model_description": "StarCoder family pre-trained on The Stack with next-token and fill-in-the-middle objectives; variants include 1B and 3B parameter models used in the paper as baselines and Cycle initializations.",
            "reflection_method_name": "naive generate-then-reflect (prompt concatenation)",
            "reflection_method_description": "Same naive strategy as for CodeGen: concatenate faulty generation and execution feedback to the prompt and re-prompt the baseline. No specialized self-refinement fine-tuning was applied for vanilla baseline.",
            "num_iterations": null,
            "task_name": "HumanEval, MBPP-S, APPS",
            "task_description": "Same code generation benchmarks with test-suite evaluation.",
            "performance_with_reflection": "Table 1 shows limited self-refinement gains for StarCoder baselines: StarCoder-1B one-time: HumanEval 15.9%, MBPP-S 25.8%, APPS 7.3%; self-refine yields HumanEval 16.5 (+3.8%), MBPP-S 28.1 (+9.1%), APPS 7.3 (+0.0%). StarCoder-3B one-time: HumanEval 23.8%, MBPP-S 35.1%, APPS 7.3%; self-refine gives HumanEval 26.8 (+12.8%), MBPP-S 40.5 (+15.3%), APPS 7.4 (+1.1%).",
            "performance_without_reflection": "See one-time numbers above (e.g., StarCoder-1B: HumanEval 15.9%, MBPP-S 25.8%, APPS 7.3%).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Small to modest improvements in some settings, but the paper reports a high exact-copy rate when naively given faulty generation + feedback: StarCoder copied the faulty code in 64.8% of HumanEval cases in the authors' initial experiments, highlighting ineffectiveness of naive prompt-based self-refinement.",
            "limitations_or_failure_cases": "Very high tendency to reproduce the faulty code verbatim when re-prompted with the faulty generation and execution feedback (64.8% exact-copy rate); generally unable to extract corrective signals from execution feedback without targeted self-refinement training.",
            "uuid": "e5455.2",
            "source_info": {
                "paper_title": "CYCLE: Learning to Self-Refine the Code Generation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-3.5 (motivation example)",
            "name_full": "GPT-3.5 (example used for motivation)",
            "brief_description": "A closed-source conversational/code-capable model (GPT-3.5) used by the authors as a motivating example showing that naively concatenating faulty code and execution feedback fails to induce self-refinement; the model often copy-pastes the faulty code.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI's GPT-3.5 family (referenced for a short motivating example); used with prompt concatenation in the paper to illustrate failure modes of naive re-prompting.",
            "reflection_method_name": "naive generate-then-reflect (prompt concatenation in a single prompt)",
            "reflection_method_description": "Authors prompted GPT-3.5 with a HumanEval problem, then concatenated the model's failed generation and unit-test failure messages to the prompt and asked GPT-3.5 to refine; the model failed to utilize the execution feedback and largely copy-pasted the faulty code instead of fixing it.",
            "num_iterations": null,
            "task_name": "HumanEval (single motivating example)",
            "task_description": "A HumanEval programming problem (Task No. 106) used as an illustrative example; demonstration that naive concatenation fails for some models.",
            "performance_with_reflection": "Anecdotal: GPT-3.5 in this example could not self-refine effectively; it copy-pasted the faulty program as the new prediction (no numeric benchmark reported for this example).",
            "performance_without_reflection": "Anecdotal: initial (one-time) GPT-3.5 generation failed the test suite for the motivating example.",
            "has_performance_comparison": false,
            "evidence_of_improvement": "None in the anecdotal example; the example is used to motivate that naive concatenation of feedback does not guarantee self-refinement and that models can ignore or misinterpret execution feedback.",
            "limitations_or_failure_cases": "Shows a failure case: GPT-3.5 ignored execution feedback and produced a copy of the faulty code rather than a corrected solution in the authors' motivating example.",
            "uuid": "e5455.3",
            "source_info": {
                "paper_title": "CYCLE: Learning to Self-Refine the Code Generation",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CodeGen",
            "rating": 2
        },
        {
            "paper_title": "StarCoder",
            "rating": 2
        },
        {
            "paper_title": "Chen et al. 2023",
            "rating": 2
        },
        {
            "paper_title": "Ouyang et al. 2022",
            "rating": 1
        }
    ],
    "cost": 0.01641475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CYCLE: Learning to Self-Refine the Code Generation</h1>
<p>YANGRUIBO DING, Columbia University, USA<br>MARCUS J. MIN, Columbia University, USA<br>GAIL KAISER, Columbia University, USA<br>BAISHAKHI RAY, Columbia University, USA</p>
<p>Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.</p>
<p>In this paper, we propose CyCle framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CyCle on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CyCle successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the selfrefinement capability of code LMs. We implement four variants of CyCle with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CyCle consistently boosts the code generation performance, by up to $63.5 \%$, across benchmarks and varied model sizes. We also notice that CyCle outperforms code LMs that have $3 \times$ more parameters in self-refinement.</p>
<p>CCS Concepts: $\cdot$ Software and its engineering $\rightarrow$ Automatic programming; Functionality.
Additional Key Words and Phrases: Code Language Models, Source Code Modeling, Code Generation, Iterative Programming</p>
<h2>ACM Reference Format:</h2>
<p>Yangruibo Ding, Marcus J. Min, Gail Kaiser, and Baishakhi Ray. 2024. CYCLE: Learning to Self-Refine the Code Generation. Proc. ACM Program. Lang. 8, OOPSLA1, Article 108 (April 2024), 27 pages. https://doi.org/10.1145/ 3649825</p>
<h2>1 INTRODUCTION</h2>
<p>Pre-trained code language models (code LMs) have achieved great success in code generation, and many of them have been deployed as a part of the integrated development environment (IDE), such as GitHub Copilot [GitHub 2021] and Amazon CodeWhisperer [Amazon 2023], to help human developers improve the programming efficiency. Along this direction, researchers started to conduct empirical and human studies to analyze the strengths and weaknesses of these models [Barke et al. 2023; Guo et al. 2023; Huang et al. 2023]. For example, Barke et al. [2023] propose a grounded theory of code-LM-assisted programming, systematically categorizing the interaction between code LMs and developers into two modes: acceleration and exploration. They define the acceleration mode as the situation when the developers clearly know what are the expected functionalities, code LMs</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>help them speed up the implementation. On the other hand, the exploration mode indicates the scenario when developers do not have concrete plans regarding how to proceed, such as facing some bugs reported by the test suites while developers have not figured out how to fix them. While this study identifies the efficiency of code LMs in the acceleration mode, it recognizes their limitations in exploration mode. For example, the code generated by code LMs in the exploration mode is less trusted than in the acceleration mode, and developers tend to validate them frequently with executions. Moreover, when errors are revealed through executions, developers struggle to debug model-generated code, as it was not authored by themselves, making it more challenging to understand the error within unfamiliar code.</p>
<p>In this paper, we propose CyCle framework, making an attempt to enhance the performance of code LMs in the exploration mode. The fundamental principle underpinning the creation of CyCle is the recognition that expecting code LMs to excel in the exploration mode, where human intentions are often unclear or not explicitly specified, may be overly demanding. However, these models should possess the capability to iteratively improve their code generation based on the feedback they receive from other sources such as execution results reported by test suites. In essence, CyCle aims to empower code LMs to adapt and enhance their output in response to the available feedback, thereby bridging the gap between human developers' exploratory programming needs and the capabilities of code LMs.</p>
<p>Limitations of code LMs in the exploration mode. In this work, we focus on the scenario of code generation that given the natural language (NL) description of a problem, typically wrapped in the docstring, the code LM will implement the program accordingly. For the convenience of our future discussion, we first concretize the acceleration mode and exploration mode in our scenario.</p>
<ul>
<li>Acceleration Mode: Given the NL description of a problem, code LMs directly predict the code accordingly.</li>
<li>Exploration Mode: If the prediction from the acceleration mode fails the test cases and execution feedback returns, code LMs try to refine the faulty code without further human instructions.
As shown in Figure 1, we prompt GPT-3.5 [Ouyang et al. 2022] with a problem description from HumanEval programming benchmark [Chen et al. 2021]. GPT-3.5 could not fulfill the functionality in the acceleration mode, and its generated code failed to pass the test suite of this problem. We could see from GPT-3.5's generation that, though the program is very short, spanning only 14 lines, it is not straightforward for humans to understand. The complexity mainly comes from the nested for-loops, making it even more difficult to manually identify and correct the error. Therefore, motivated by Chen et al. [2023], we tried concatenating the faulty generation and the execution feedback reported by the test suite, as additional references, with the problem description and expected the model to refine the generated code by itself in the exploration mode. Unfortunately, GPT-3.5 could not effectively understand the guidance from the execution feedback and simply copy-pasted the faulty code as its new prediction.</li>
</ul>
<p>Such weakness of self-refinement in the exploration mode is even more severe in open-source code LMs. We conduct similar experiments with CodeGen ( 2.7 billion parameters) [Nijkamp et al. 2023b] and StarCoder (3 billion parameters) [Li et al. 2023] on the whole HumanEval benchmark with 164 programming problems. We observe that existing code LMs perform poorly in the exploration mode, failing to self-refine the faulty generations according to the execution feedback. CodeGen generates an exact copy of the faulty code as its refined prediction in $42.2 \%$ cases while StarCoder copies in $64.8 \%$ cases. Such weak self-refinement capability in the exploration mode is concerning, as it brings further burden to the human developers to fix the bugs brought by the model-generated code.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Motivation Example. We prompt GPT-3.5 and Cycle to implement a program according to a problem description from the HumanEval programming benchmark (Task No. 106). While both failed to pass the test suite in the acceleration mode, CyCLE successfully refined its own generation referring to the execution feedback. In contrast, GPT-3.5 could not self-refine effectively.</p>
<p>Our Approach. In this work, we argue that code LMs should be enhanced in exploration Mode with self-refinement capability by leveraging the available feedback from the execution results. In fact, a model trained with such execution feedback has the potential to perform better even in the acceleration mode.</p>
<p>To this end, we design CyCle, a framework that teaches code LMs to self-refine (by continuing training a pre-trained model) by jointly attending to three information sources: (i) the high-level problem description in natural language, (ii) the incorrect code the model may have generated in the previous attempts, and (iii) the execution feedback. We have developed an input template that consolidates these three information sources and employs them to train the code LM. While traditional code LMs' training primarily relies on the first information source, the inclusion of prior incorrect code aids the model in achieving a more comprehensive grasp of its own errors. The execution feedback, in turn, guides the code LM in generating programs that align precisely with the problem description.</p>
<p>Nonetheless, when we naively include the previously generated erroneous code in the input, the code LM often resorts to a shortcut, essentially copying from the incorrect input when generating new code. To deter the code LM from adopting such shortcuts, we employ a masking technique, referred to as the Past Generation Mask (PGM). This strategy slightly obfuscates the incorrect past generations, motivating the model to explore a more extensive range of solutions for code refinement. Additionally, to strike a balance between the proficiency of code generation in acceleration and exploration mode, we employ a data mixing strategy that manipulates the ratio of self-refinement features and general code completion features.</p>
<p>To efficiently train a Code LM using the above strategy we have to curate data that simulate the exploration mode development. Thus, we further design an automatic training data generation phase as existing pre-training code datasets Kocetkov et al. 2022; Nijkamp et al. 2023b; Xu et al. 2022] are challenging to be customized for self-refinement training. Our data collection phase automatically prompts the pre-trained code LMs to reveal their own strengths and weaknesses in code generation, verified by executing test cases, and constructs data samples to reinforce their strengths while refining their weaknesses.</p>
<p>Finally, we implement CyCle to realize an automated self-refinement workflow that imitates the iterative programming practice of human developers. The workflow first prompts code LMs to initialize the implementation according to the high-level problem description and then continuously verifies the correctness of prediction with execution and aggregates comprehensive information for further refinement.</p>
<p>Results. We evaluate CyCle's code generation capability with three popular programming benchmarks: HumanEval [Chen et al. 2021], MBPP-Sanitized [Austin et al. 2021], and APPS [Hendrycks et al. 2021]. To illustrate the effectiveness and generalizability of CyCle, we train four variants of CyCle with varied parameter sizes ranging from 350 M to 3B. From the evaluation results, we conclude that CyCle is pretty effective at self-refinement, consistently boosting the code generation performance, by up to $63.5 \%$ relative improvement, across four model sizes on all three benchmarks, while maintaining decent one-time generation capacity. With efficient self-refinement learning, Cycle-350M outperforms StarCoder-1B across all three programming benchmarks, and CyCle-1B matches the performance of StarCoder-3B. With the in-depth analysis, we also empirically reveal that CyCle is effective at capturing execution feedback and has great potential to assist human developers with iterative programming.</p>
<p>Novelty and Contributions. We make the following novel contributions:</p>
<ul>
<li>Our work sheds light on the weaknesses of code LMs in self-refinement, revealing that these models are not effective at understanding the execution feedback and correcting their own mistakes accordingly.</li>
<li>To fulfill the code LMs' potential in self-refinement, we propose CyCle, a framework that enhances the code LMs' generation performance by learning to refine their own generated code. We first propose a knowledge-distillation-based data collection approach to automatically construct samples to teach code LMs to self-refine. We then propose a training strategy designed specifically for learning self-refinement. Finally, we implement an iterative self-refinement workflow that automates the process of generating code in exploration mode.</li>
<li>We conduct extensive experiments on three popular code generation benchmarks with four Cycle variants across 350 M to 3B model parameters, and show that CyCle consistently increases the code generation performance by up to $63.5 \%$. CyCle could also match or even outperform baseline code LMs with $3 \times$ parameters.</li>
<li>We perform in-depth analysis to discuss Cycle's design and performance from multiple perspectives. We conclude with insights and takeaways to motivate further research in improving code LM's self-refinement capability, which hopefully assists human developers with iterative programming and generally increases code LM's performance in exploration mode.</li>
</ul>
<p>We anonymously release our code, data, and model checkpoints. The artifact is available at https://github.com/ARiSE-Lab/CYCLE_OOPSLA_24.</p>
<h1>2 OVERVIEW</h1>
<p>In this section, we briefly introduce CyCle, explaining the high-level designs and the intuitions behind them. We present the overview of CyCle in Figure 2. At a high level, CyCle contains three phases.</p>
<p>Phase-I: Data Preparation for Self-Refinement. The data to train code LMs for refinement capability requires carefully crafted features, such as the developing log with iterative correction of code snippets and their error-exposing feedback loops. These features do not naturally come along with the large-scale pre-training datasets [Kocetkov et al. 2022; Nijkamp et al. 2023b; Xu et al. 2022]</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Overview of Cycle.
that typically prioritize data quantity while not providing interfaces for customization. Therefore, we propose an automated approach to distill such features from the pre-trained code LMs and construct the datasets on top of them.</p>
<p>The general idea is to prompt the pre-trained code LMs with programming problems, asking them to generate code to fulfill the requested functionalities. These problems should be well-defined and accompanied by test suites and canonical solutions so that the model's generation can be efficiently verified. When the code LM makes mistakes, we gather its faulty generations and the corresponding execution feedback reported by the test suites. Then we construct data samples accordingly, which could be leveraged to teach code LMs to correct their own mistakes by referring to the execution feedback and the canonical solutions accompanied by the programming challenges. Fine-tuning Code LMs with Correct Code. Code LMs are pre-trained with up to trillions of code tokens, but the noise inherited in the pre-training data could trigger unexpected predictions [Li et al. 2022] or even vulnerable code [He and Vechev 2023]. We should avoid these malicious behaviors when constructing samples for self-refinement. The unexpected predictions might reveal random errors and could not get meaningful feedback after execution. Also, when the vulnerable code is generated, it might bring a security threat to the execution system. Therefore, we propose to first fine-tune the pre-trained code LMs on the correct code to minimize the model's malicious behaviors. Concretely, we collect a decent number of programming challenges that are accompanied by canonical solutions, and we fine-tune the pre-trained code LMs to predict these guaranteed correct programs conditioned on the problem description.
Prompt Code LMs to Distill the Weaknesses. With the fine-tuning on the canonical solutions, we expect the model to make faulty predictions that are not too off from the correct code. We prompt the fine-tuned model with the problem description and then execute its prediction with the accompanied test suites. The execution results with error messages expose the models' strengths and weaknesses in code generation, which are collected and saved as resources to build training samples to learn the refinement.</p>
<p>Phase-II: Learning to Refine the Faulty Generation. With the data prepared by Phase-I, we construct the training samples to learn code refinement based on three types of information. Specifically, we aggregate the problem description, the faulty code generated by the code LM, and the corresponding execution results, using our proposed template (more details in Section 3.2.1), to formulate the</p>
<p>model input, so that the model could jointly attend to comprehensive information simultaneously. Then we use the accompanying canonical solutions as the target for the model to predict. Different from the fine-tuning in Phase-I, which only teaches the model to fulfill the functionalities described in natural language, the code refinement training in this phase exposes the model to its own mistakes and the execution feedback, together with the problem description, which forces the model to both reason about the misalignment between its generation in the past and the problem description and learn to understand the implicit guidance in the execution results. We carefully design our training strategy to effectively and efficiently learn the code refinement, and we will introduce more details of our approach in Section 3.2.</p>
<p>Phase-III: Self-Refinement as Iterative Programming. After learning about the code refinement from Phase-II, we deploy the model to automatically generate code according to the problem description and, similar to the iterative programming practice of human developers, iteratively refine the code to fulfill the required functionality. When the problem description is fed into the model, the model will first generate the code at its best, and the generation will automatically be executed with the test suite. If failed test cases are detected, our framework will automatically aggregate the description, the faulty code, and the execution feedback using the template proposed in Phase-II. Finally, the aggregated information will be again fed into the model for self-refinement.</p>
<h1>3 APPROACH</h1>
<p>In this section, we explain Cycle in detail. We will illustrate the concrete approaches we designed to teach the code language models (code LMs) about self-refinement for code generation.</p>
<h3>3.1 Phase-I: Data Preparation</h3>
<p>Training code LMs to possess self-refinement capabilities demands a dataset rich in specific features, such as development logs showcasing the iterative correction of code snippets, as well as feedback loops that highlight the errors. Existing large-scale pre-training datasets [Kocetkov et al. 2022; Nijkamp et al. 2023b; Xu et al. 2022] are primarily designed to amass vast amounts of data but are not inherently equipped to offer these nuanced features. Therefore, an efficient data collection method is required to extract these specialized features. To this end, we propose a knowledge distillation [West et al. 2022] approach that prompts the pre-trained code LMs to showcase their capabilities, revealing their strength as well as exposing their weaknesses. Subsequently, we will construct the datasets to reinforce their strengths while learning to self-refine their weaknesses.
3.1.1 Fine-tune Code LMs with Semantically Correct Code. Code language models [Chen et al. 2021; Li et al. 2023; Nijkamp et al. 2023b; Rozière et al. 2023] are typically pre-trained with a huge amount of source code, learning to predict up to trillions of code tokens. However, the innate noise in the pre-training data could affect its accuracy in code generation. As the large corpora are mostly collected from open-source resources [Kocetkov et al. 2022; Rozière et al. 2023; Xu et al. 2022], the quality of training samples varies significantly. Specifically, the high-quality code snippets well align with their accompanying natural-language docstrings or comments, such as those from mature and actively maintained projects or forked commercial software, while a certain amount of the noisy snippets have vulnerable functionalities and semantic misalignment, such as those from developing projects or starter-level programmers. This means that during training, the model is exposed to, and possibly memorizes, both correct (aligned) and incorrect (misaligned) programs. The consequential challenge during the knowledge distillation is that, given a natural language (NL) prompt, the model holds the potential to generate either semantically accurate or erroneous code. This behavior is generally determined by the frequency ratio of correct to incorrect code observed</p>
<p>during pre-training. It has been verified by existing works that the noise in the pre-training data could bring unexpected behavior [Li et al. 2022] or security vulnerabilities [He and Vechev 2023].</p>
<p>To minimize the malicious behaviors of code LMs during the knowledge distillation, we first fine-tune the pre-trained code LMs using only "guaranteed correct" code. Specifically, we collect canonical solutions from programming challenges that are already verified by the test suite and fine-tune code LMs to predict these solutions token by token. Different from the pre-training of code LMs [Li et al. 2023; Nijkamp et al. 2023a,b; Rozière et al. 2023] that predicts every token in the corpora, our fine-tuning is designed to only limited to the code tokens within the canonical solution and the NL description will be regarded only as context. Specifically, the NL description is a sequence of $m$ tokens, $N L=\left{n l_{0}, n l_{1}, \ldots, n l_{m}\right}$, and the canonical solution is a sequence of $n$ tokens, $C=\left{c_{0}, c_{1}, \ldots, c_{n}\right}$, we apply the standard language modeling loss [Brown et al. 2020; Radford and Narasimhan 2018; Radford et al. 2019] to optimize the fine-tuning:</p>
<p>$$
\mathcal{L}<em _in_C_="\in|C|" n="n">{\text {fine-tune }}=\sum</em>\right)
$$}-\log P\left(c_{n} \mid N L, c_{1}, c_{2}, \ldots, c_{n-1</p>
<p>Such a design makes the model's prediction more focused, and the learning process is more narrowed to only optimize the model towards the code generation. We observe that, after this fine-tuning, the model not only generates better code quality but also knows when to terminate the generation more accurately than the pre-trained code LMs without such fine-tuning. This is mainly because the pre-training asks code LMs to predict tokens until the maximum context length is reached, while we fine-tune the model to focus on the canonical solutions that are terminated naturally when the functionality is complete.
3.1.2 Prompt Code LMs to Distill the Weaknesses. After the fine-tuning, the model is primed to generate solutions that, even if faulty, are proximate to canonical solutions. This leaves us great chances to create a dataset with a decent amount of paired samples. Such a pair, including a correct and a wrong solution targeting the same problem, is quite valuable, revealing through error messages the subtle nuances where the model's code generation aligns or diverges from the desired outcome. These errors aren't merely mistakes; they provide insights into the model's comprehension and interpretation, shedding light on its strengths and weaknesses. We hope to teach code LMs to capture and learn to transform the wrong code to its correct counterpart by fixing the subtle error according to the execution feedback. Such a transformation well imitates the process of self-refinement.</p>
<p>To construct such samples, we prompt code LMs with problem descriptions and verify their predictions using the accompanying test suites. For those problems that the model fails to predict correctly, we gather its faulty generation and the errors thrown by the test suite execution. We will pair this fault with the ground-truth implementation to construct training samples for the next phase of learning. For the correctly predicted problem, we will also gather its generation to replace the canonical solution of this problem. For the next phase of learning, we will train the model to predict its own generated correct code to maintain and reinforce such knowledge.</p>
<h1>3.2 Phase-II: Learning to Refine the Faulty Generation</h1>
<p>With the constructed samples in the previous phase, we introduce the training process of CyCLE, which is designed for learning to refine the faulty code.</p>
<p>CyCLE will be firstly initialized with the fine-tuned checkpoints that we introduced in Section 3.1.1, and then continue training with the samples described in Section 3.1.2. As these samples include the errors made by the fine-tuned code LM, teaching the same model about the code refinement</p>
<p>aligns with our final goal that we expect the code LMs to refine the faulty code generated by itself, i.e., "self"-refinement.
3.2.1 Aggregate the Problem Description, Faulty Generation, and Execution Results as a Joint Prior Condition. To effectively teach code LMs about code refinement, we propose a template that aggregates information from multiple resources. We show the template in Figure 3. To ensure the code's naturalness remains intact, we adopt a strategy of encapsulating our template within docstrings or comments. This template comprises six essential components.</p>
<p>First, we encapsulate the problem description within docstrings. Next, we introduce a negative prefix that signifies the beginning of a flawed code generation, followed by the actual erroneous code. In a manner similar to the negative prefix, we employ an execution prefix to precede the feedback regarding the code's execution. This feedback meticulously outlines the test results, encompassing specific failed test cases, input data, the anticipated output, and the actual misaligned output. Finally, our template concludes with a positive prefix designed to prompt the generation of the correct code solution.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Template to aggregate the problem description, faulty generation, and the execution feedback.
3.2.2 Learning to Self-Refine. With the proposed template above, we efficiently aggregate all information together for the model to jointly attend to. To learn the refinement, the model will be fed with the aggregated information and learn to predict the canonical solution of this problem. Specifically, we define the aggregated information, at a high level, as the combination of problem description, $N L$, faulty generation, $F G$, and the execution feedback, $E F$, so $A G G R={N L, F G, E F}$. The target canonical solution, $C$ will be predicted token by token, and thus the loss can be represented as:</p>
<p>$$
\mathcal{L}<em _in_C_="\in|C|" n="n">{\text {self-refine }}=\sum</em>\right)
$$}-\log P\left(c_{n} \mid N L, F G, E F, c_{1}, c_{2}, \ldots, c_{n-1</p>
<p>Though the learning process is mostly about the next token prediction [Radford and Narasimhan 2018; Radford et al. 2019], it is quite effective at learning the self-refinement, which will be illustrated in Section 5.1. The main reason for its effectiveness is that the learning objective forces the model to learn different knowledge from distinct resources of information. To correctly predict each code token $c_{i}$, the model needs to understand $N L, F G$, and $E F$ respectively, and jointly decide how to take advantage of each resource.
Past Generation Mask (PGM) When aiming to refine faulty code generations, there is a risk that, during the training process, the model pays too much attention to the prior faulty generation, i.e., $F G$, it might end up taking shortcuts by copying the tokens to minimize the self-refinement loss mentioned above, since the faulty code has significant overlap, in terms of tokens, with the target implementation (Section 3.1.2). Instead of genuinely understanding and rectifying the code</p>
<p>based on the feedback, the model could resort to merely copying or reproducing large portions of the past generation. Such shortcuts are difficult to detect during the training, as the loss will not significantly differ regardless of model takes the shortcut or not, but the model relying on the shortcuts becomes useless when being deployed.</p>
<p>To alleviate such risk, we are motivated by the efficient masking approaches in deep learning literature, such as dropout [Srivastava et al. 2014] and forgetful causal mask [Liu et al. 2023], and we propose to randomly mask $p \%$ of tokens in the faulty generation $F G$, which is the main resource for copying. Concretely, we manipulate the attention mask in the Transformer architecture [Vaswani et al. 2017], which will make the masked token invisible to other tokens without the explicit removal of tokens. We will study the effectiveness of this masking strategy in Section 5.2. Note that the masking ratio, $p$, is a tunable hyperparameter, and we will study its impact in Section 5.4.
3.2.3 Mixture of data resources. Data is always the key to the success of language model training, and existing code LMs [Li et al. 2023; Rozière et al. 2023] have illustrated the necessity of combining multiple data resources with carefully tuned proportions. In the quest to enhance the proficiency of our model for self-refinement, we identified the necessity to curate an optimal mix of self-refine samples, as described in Section 3.1.2, and the canonical solutions used in Section 3.1.1. The rationale behind this strategy is to balance Cycle's capability in acceleration and exploration mode.</p>
<p>The proportion of these data resources also introduces a nuanced layer of complexity to the training regimen. An imbalance, especially an excessive reliance on self-refine samples, could lead the model to overfit code refinement. In such a scenario, it expects, and perhaps becomes overly dependent on, the trifecta of the natural language prompt, the prior faulty generation, and the execution feedback. This could compromise its innate capacity to generate code accurately in situations where only a simple prompt is provided. Conversely, if the original code samples dominate the training set, the model might not sufficiently internalize the mechanisms of selfcorrection and iterative refinement. This intricate balance underscores the importance of meticulous data engineering in training a versatile and robust code language model. We will study the impact of such data mixture in Section 5.4</p>
<h1>3.3 Phase-III: Iterative Self-Refinement with Execution Feedback</h1>
<p>While the Phase-II training is formulated as a one-step refinement from the faulty code to its correct version, we implement the automated inference framework as an iterative programming workflow. There are three main reasons. First, we assume that one-step refinement is not enough to fulfill the model's best capacity, as there is always a gap between the perfect performance that the training tries to approach and the final inference performance that could really be achieved. Second, the diversity of self-refinement samples should enable CyCle with the iterative capability of refinement Third, our design is to harmonize the automated code generation with human-like introspection and iterative improvement. By incorporating feedback loops, setting clear stop criteria, and leveraging the structured aggregation from Phase-II, we aim to transform code generation from a one-time event to a dynamic, self-evolving process. This methodology, inspired by human programming practices, sets the stage for more resilient, accurate, and context-aware code outputs.</p>
<p>Specifically, as shown in Figure 2, upon receiving the problem description, the model initially produces what it perceives as the optimal code. This generated code is then automatically tested against the relevant test suite. Should any test cases fail, our framework seamlessly compiles the original description, the incorrect code, and the associated execution feedback, all according to the template from Phase-II. This consolidated input is then presented back to the model, guiding its subsequent refinement efforts. The process of self-refinement is not endless considering the overhead and computation it might cost, and it will be stopped by three scenarios: (1) if the refined</p>
<p>code successfully passes the test suite, or (2) if a threshold for the maximum self-refine times is reached, or (3) if the refined code remains the same as the previous faulty code, which means the model could no more improve the code according to the execution feedback.</p>
<h1>4 EXPERIMENTAL SETUP</h1>
<h3>4.1 Training Datasets</h3>
<p>We formulate the training dataset for CyCle with samples collected from CodeContest [Li et al. 2022]. CodeContest includes programming competition problems curated from Codeforces [Mirzayanov 2020] and CodeNet [Puri et al. 2021]. Each problem is accompanied by both canonical solutions and wrong human submissions written in three programming languages (Python, Java, C++), as well as executable test cases to verify the correctness of solutions.</p>
<p>To construct our training datasets, we focus on the training split of CodeContest which originally contained 13,328 problems. We further filter out those problems with long descriptions ( $&gt;512$ tokens), as they could take too much input length of Transformer-based LM and leave insufficient length to include the code solutions that CyCle will be trained to predict. In addition, we remove the problems without Python solutions. Finally, our training set ended up with 7,108 problems from CodeContest. We also constructed a held-out validation set from CodeContest valid split, which originally contained 117 problems, to monitor the training process and the model's generalizability to unseen data during training. After the same filtering as our training set, we keep 77 problems in the validation set.</p>
<p>To fine-tune the code LMs for the data preparation phase (Section 3.1.1), we sample up to 50 canonical solutions for each of the problems, resulting in 233,703 training samples and 3,833 validation samples. To build the samples for the self-refinement training (Section 3.1.2), we sample 10 generations for each problem from the fine-tuned code LMs and execute the test suites to verify their correctness and collect the execution feedback. In total, we have 71,080 training samples and 770 validation samples for the self-refinement learning phase.</p>
<h3>4.2 Evaluation Benchmarks</h3>
<p>To evaluate the code generation performance, we use three popular programming benchmarks: HumanEval [Chen et al. 2021], MBPP-S [Austin et al. 2021], and APPS [Hendrycks et al. 2021]. HumanEval HumanEval contains 164 hand-written programming problems by Chen et al. [2021]. Each problem includes a function signature and docstring as the description, and the model is asked to predict the function body. Each problem also maintains on average 7.7 unit test cases, in the format of assertions, to verify the correctness of the model prediction.
MBPP-S Mostly Basic Programming Problems (MBPP) benchmark contains 974 short Python programs constructed by crowd-sourcing a pool of crowdworkers who only have basic knowledge of Python. Each problem contains a short problem statement and provides 3.0 unit test cases to verify the functionality. Austin et al. [2021] further sanitized the benchmark by manually inspecting and removing the ambiguous or unexpected problems, resulting in a version called MBPP-Sanitized (or MBPP-S for short) with a total of 426 problems of better quality. In our evaluation, we apply this sanitized version of MBPP-S for more accurate evaluation.
APPS APPS is a benchmark collected to evaluate the code generation models. It includes a total of 10,000 problems collected from open-access coding websites, accompanied by a total of 131,777 test cases and 232,421 canonical solutions. To avoid data leakage, we focus on the test split with 5,000 problems. In addition, given that APPS has significantly overlapped data resources with our training data from CodeContest, we apply an aggressive filtering strategy to deduplicate. Specifically, we exhaustively calculate the fuzzy edit similarity between the problem description in APPS and in</p>
<p>CodeContest, and if the similarity is over $60 \%$, we will remove the whole problem from APPS to avoid the data memorization issue. Finally, after the filters, we ended up with 1,280 problems as our evaluation problem set.</p>
<h1>4.3 Models</h1>
<p>To illustrate the generalizability of Cycle's design, we train four variants of Cycle with varied sizes of model parameters: $350 \mathrm{M}, 1 \mathrm{~B}, 2.7 \mathrm{~B}$, and 3 B . These variants are initialized from the checkpoints of two open-source code LMs families: CodeGen [Nijkamp et al. 2023b] and StarCoder [Li et al. 2023]. CodeGen is a set of autoregressive code LMs, with varied sizes ( 350 M to 16B), pre-trained using next-token prediction objective on a large-scale dataset collected from ThePile [Gao et al. 2021], Google BigQuery, and BigPython [Nijkamp et al. 2023b]. The dataset includes both natural language and code samples with over 550 billion tokens. The model maintains the context length of 2,048 BPE [Kudo and Richardson 2018] tokens.
StarCoder is a set of code LMs, with varied sizes ( 1 B to 15.5B), that are pre-trained on over 1 trillion tokens from The Stack [Kocetkov et al. 2022] dataset, using both next-token prediction and the fill-in-the-middle [Bavarian et al. 2022; Fried et al. 2023] objectives. StarCoder family could consume up to 8,192 BPE tokens.</p>
<p>Specifically, Cycle variants are initialized from CodeGen-350M, StarCoder-1B, CodeGen-2.7B, and StarCoder-3B respectively. We load the checkpoints from the Hugging Face Model Hub [HuggingFace 2023].</p>
<h3>4.4 Configurations and Hyperparameters</h3>
<p>We conduct our experiments on $2 \times$ NVIDIA GeForce RTX 3090 with 24GB GPU memory each. The model is implemented mainly with PyTorch [et al. 2019] and Hugging Face Transformers [et al. 2020] library.</p>
<p>For training, we consider a batch size of 512 samples with 2,048 BPE [Kudo and Richardson 2018] tokens. We apply a standard learning rate descending strategy of code LM [Li et al. 2023; Nijkamp et al. 2023b; Rozière et al. 2023] that the early phase of training applies a higher learning rate than the later phase, and small models use a higher learning rate than large models. Concretely, for the fine-tuning in the data preparation phase (Section 3.1), CyCle uses [5e-5, 2e-5, 1e-5, 1e-5] for [350M, $1 \mathrm{~B}, 2.7 \mathrm{~B}, 3 \mathrm{~B}]$ respectively, and for the self-refinement learning phase (Section 3.2), CyCle uses [2e-5, $2 \mathrm{e}-5,5 \mathrm{e}-6,5 \mathrm{e}-6]$ for the aforementioned model sizes respectively. All the training applies a cosine learning rate decay scheduler with warmup steps. For both the fine-tuning in the data preparation phase and the self-refinement learning, we train CyCle for only one epoch on the corresponding dataset. The PGM masking rate (Section 3.2.2) for training is 0.05 , and the ratio of self-refinement samples is $25 \%$ (Section 3.2.3).</p>
<p>For inference, we adapt the standard nucleus sampling [Holtzman et al. 2020] with the top-p probability of 0.95 . For HumanEval and MBPP-S benchmarks, we ask the model to generate up to 256 BPE tokens, and for the APPS benchmark, the model will generate up to 512 BPE tokens, as the latter's problem is more difficult and the solutions are typically longer. For the self-refinement process during the inference, we set up a maximal refinement step of 4 , and this choice is based on the tradeoff between the inference overhead and the performance. We will study more about the number of refinement steps in Section 5.2.1 and the inference overhead in Section 5.3.4</p>
<h2>5 EVALUATION</h2>
<p>In this section, we evaluate Cycle and analyze it by asking the following research questions:</p>
<ul>
<li>
<p>RQ1: How effective is CyCle in code generation compared to the existing code LMs?</p>
</li>
<li>
<p>RQ2: How do CyCle's different designs contribute to its performance?</p>
</li>
<li>RQ3: How is CyCle's iterative self-refinement different from Top-K generations?</li>
<li>RQ4: How will PGM's masking ratio and data mixture proportion affect the performance?</li>
</ul>
<h1>5.1 RQ1. Cycle's Performance in Code Generation</h1>
<p>In this section, we present the main evaluation regarding CyCle's performance in code generation, and we illustrate its effectiveness by comparing it to existing code LMs of varied size, across three popular programming benchmarks.
Setup. We evaluate the model's code generation capability in two main settings: one-time generation and iterative self-refinement. For the one-time generation, we follow the original design of the evaluation benchmarks [Austin et al. 2021; Chen et al. 2021; Hendrycks et al. 2021], where the description of programming challenges will be fed into the model as the prompt, and the model will implement the program accordingly. We use the accompanied test suites of each programming challenge to verify the correctness of the prediction, where a test suite includes multiple test cases to evaluate the code with distinct perspectives.</p>
<p>For the iterative self-refinement performance evaluation, we follow the workflow of Cycle's inference framework (Section 3.3). The model will take the one-time generation as the starting point for the refinement, and repeat the process up to four times (Section 4.4). If the generated code passes the test suite at a time step, the self-refinement of this sample will be terminated, and this sample will be regarded as a success. If the model cannot predict a correct program with four times of self-refinement, the sample will be regarded as a failure.
Baselines. We consider vanilla open-source code LMs from CodeGen [Nijkamp et al. 2023b] and StarCoder [Li et al. 2023] families as the first set of baseline. Specifically, we consider CodeGen350M, StarCoder-1B, CodeGen-2.7B, and StarCoder-3B. CodeGen and StarCoder are two of the most popular code LMs with state-of-the-art code generation capabilities, and since CyCle variants are initialized from these models to continue learning the self-refinement (Section 3.1.1), comparing with them will directly reflect the effectiveness of CyCle's proposed training.</p>
<p>In addition, we create a stronger set of baselines from the fine-tuned checkpoints we introduced in Section 3.1.1. We further train these checkpoints with the canonical solution from Cycle's training datasets but no self-refinement signals are included. This set of baselines has two main improvements compared to the vanilla code LMs. First, it is trained with the same data samples with the same amount of epochs as CyCle, which directly verifies whether the naive training on the canonical solution is enough to grant the model self-refine capability. Second, this set of baseline is trained with canonical solutions only, so comparing to it illustrates the value of CyCle's self-refinement training that learns to jointly understand the faulty generation and execution feedback as well. We call this set of baselines "code LMs fine-tuned with correct code".</p>
<p>Note that, since baseline models have never been exposed to the template we designed for CyCle to perform self-refinement (Section 3.2.1), we notice that applying such a template to baseline models will hurt their performance by misleading the model to keep generating comments rather than the real code. Alternatively, to maximize their performance for a fair comparison, we borrow the idea from existing work [Chen et al. 2023] to wrap the faulty code and the execution feedback into the docstring as plain text, and it turns out that the baseline models could normally generate code after this adaption.
Findings. The results of one-time code generation and self-refinement across four sizes of code LMs (350M, 1B, 2.7B, 3B) are reported in Table 1.</p>
<p>Table 1. Comparing Cycle's performance with baseline models in both one-time generation and iterative self-refinement settings. We consider four sizes of code LMs to discuss, ranging from 350 M to 3B. The first row of each section is the "vanilla code LM" baseline, the second row of each section is the "code LMs fine-tuned with correct code" baseline, and the last row is CyCle variant of the same size.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">One-time</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Self-Refine</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">MBPP-S</td>
<td style="text-align: center;">APPS</td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">MBPP-S</td>
<td style="text-align: center;">APPS</td>
</tr>
<tr>
<td style="text-align: center;">CodeGen-350M</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">$12.2+0.0 \%$</td>
<td style="text-align: center;">$21.8+14.8 \%$</td>
<td style="text-align: center;">$6.9+0.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">+ FT w/ Correct</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">$14.0+14.9 \%$</td>
<td style="text-align: center;">$23.0+19.5 \%$</td>
<td style="text-align: center;">$7.7+2.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">CyCle-350M</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">$20.7+47.9 \%$</td>
<td style="text-align: center;">$32.6+63.5 \%$</td>
<td style="text-align: center;">$8.7+15.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">StarCoder-1B</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">$16.5+3.8 \%$</td>
<td style="text-align: center;">$28.1+9.1 \%$</td>
<td style="text-align: center;">$7.3+0.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">+ FT w/ Correct</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">$18.9+3.3 \%$</td>
<td style="text-align: center;">$28.3+9.0 \%$</td>
<td style="text-align: center;">$9.3+8.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">CyCle-1B</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">$22.0+20.0 \%$</td>
<td style="text-align: center;">$35.8+39.1 \%$</td>
<td style="text-align: center;">$10.9+22.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">CodeGen-2.7B</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">$23.8+8.4 \%$</td>
<td style="text-align: center;">$35.4+2.0 \%$</td>
<td style="text-align: center;">$7.1+0.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">+ FT w/ Correct</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">$23.8+8.4 \%$</td>
<td style="text-align: center;">$40.5+10.2 \%$</td>
<td style="text-align: center;">$9.7+7.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">CyCle-2.7B</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">$29.3+37.1 \%$</td>
<td style="text-align: center;">$48.5+35.3 \%$</td>
<td style="text-align: center;">$11.6+27.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">StarCoder-3B</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">$26.8+12.8 \%$</td>
<td style="text-align: center;">$40.5+15.3 \%$</td>
<td style="text-align: center;">$7.4+1.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">+ FT w/ Correct</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">$24.4+0.0 \%$</td>
<td style="text-align: center;">$40.8+13.7 \%$</td>
<td style="text-align: center;">$10.2+13.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">CyCle-3B</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">$29.9+22.5 \%$</td>
<td style="text-align: center;">$51.3+41.3 \%$</td>
<td style="text-align: center;">$11.3+25.3 \%$</td>
</tr>
</tbody>
</table>
<p>Finding-1.1: Self-Refinement Capacity Does Not Come Along Naturally with Code LMs' Pre-training. ${ }^{1}$ As we can see from the table, baseline code LMs, though pre-trained on tons of code samples, primarily focus on one-time code generation and struggle to self-refine the faulty generation in the past by understanding the execution feedback. It is also evident that, though the one-time code generation performance aligns with the scaling law of neural language models [Kaplan et al. 2020], the increase in model sizes does not necessarily translate to better self-refinement capabilities. For example, baseline code LMs with sizes ranging from 350 M to 2.7 B all fail to correctly self-refine a single example in APPS ( $0.0 \%$ improvement after performing self-refinement). This verifies our assumption that existing code LMs are not exposed to sufficient signals during the pre-training to learn how to self-refine, and naively stacking more model parameters or collecting more data from the wild is not that helpful.</p>
<p>We notice that fine-tuning the baseline code LMs with only semantically correct code, as indicated by the "+ FT w/ Correct" rows, shows only marginal improvements in both one-time generation and self-refinement. This means that merely training on correct code is not enough to equip the models with the skill of rectifying their own mistakes, which requires a decent understanding of the execution feedback and the capability of fixing the errors accordingly.</p>
<p>Finding-1.2: CyCle is Effective at Improving Code LM's Self-Refinement Capacity and Generalizable to Varied Model Sizes. As shown in Table 1, across three programming benchmarks, CyCle consistently levels up the code generation performance with self-refinement, resulting in up to $63.5 \%$ relative improvement compared to one-time generation. CyCle with 350 M parameters notably outperforms StarCoder-1B, which maintains $3 \times$ more parameters, across all three benchmarks, highlighting the effectiveness of CyCle's self-refinement and the efficiency of the proposed training strategy.</p>
<p>Compared to baseline models, CyCle has notably stronger capabilities at correcting faulty generations in the past. For example, while the vanilla CodeGen-2.7B model could not refine its</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>own wrong predictions in APPS, while CyCle-2.7B successfully refines $27.6 \%$ more samples which it failed to predict for the first time. This suggests that, by training to refine code based on execution feedback and previous mistakes, CYCLE builds a more holistic understanding of code, enabling it to not just generate code, but also understand its intricacies, potential pitfalls, and nuances.</p>
<p>In addition, as we introduced in Section 3, CyCle does not require to be trained from scratch, and rather, it loads pre-trained code LMs as the starting point and further teaches the model to self-refine. As we can see from Table 1, Cycle is effective at varied sizes of code LMs and consistently boost the performance for two different neural architectures ${ }^{2}$, CodeGen and StarCoder. This empirically proves the generalizability of CyCle that it can always be applied in a plug-and-play style, suggesting the potential of taking advantage of more powerful code LMs that will be trained and released in the future.</p>
<p>Finding-1.3: Cycle Maintains Decent Capacity in One-time Code Generation. One recognized risk of continuing training unsupervised/self-supervised models, which are pre-trained with large-scale data, on limited, carefully crafted samples is the model shift problem [Wang et al. 2021; Wang and Schneider 2015]. The model could shift towards the local optimal of the limited but new data while the previous knowledge is eventually wiped out. As we can see in Table 1, CyCle does not suffer this issue; it successfully maintains the one-time generation capability. This empirically verifies that our proposed alignment of prompt, fault generation, and execution feedback (Section 3.2.1) is intuitive to the model, preventing the significant distribution shift. Also, the data mixture strategy (Section 3.2.3) plays a role in neutralizing the distribution gap, and we will further analyze this strategy in Section 5.4. Interestingly, the 350M, 2.7B, and 3B versions of CyCle perform slightly better than the standard baseline code LM. We believe the primary reason for this is the way CyCle is trained. During its training process, CyCle is exposed to both faulty and correct code, while our specialized training method encourages the model to only generate the correct one(Section 3.2.2). This gives CyCle a unique advantage: it understands what good and bad code look like respectively, but it's specifically trained to produce the good one. In comparison, baseline code LMs are not exposed to such preference signals during their pre-training.</p>
<p>Result-1: While maintaining the one-time code generation capacity, mostly improving marginally, CyCle significantly boosts the code LMs' self-refine capacity by learning to understand the execution feedback and faulty code generated in the past. CyCle enables existing code LMs to match or beat larger models with $3 \times$ more parameters.</p>
<h1>5.2 RQ2. Impacts of Cycle's Different Designs on Code Generation</h1>
<p>While we have concluded that CyCle's proposed approach, as a whole, is effective at code generation in Section 5.1, now we delve deeper to analyze how the isolated design contributes to the performance individually.
5.2.1 Self-Refinement Continuously Improves Cycle's Performance. Teaching code LMs to selfrefine is the core idea behind CyCle's design, so we first study how self-refinement boosts CyCle's code generation capability. To do this, we plot the CyCle-350M's performance at each refinement step, in Figure 4, to reveal the trend. We also plot the baseline models' trends as a comparison.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We can see that CyCle's performance is improved with each refinement step across all three benchmarks. This reveals that CyCle is able to eventually correct its own error, step by step, by understanding the mismatch between the expected dynamic behavior and real implementations from the execution feedback. In addition, the figure reveals that CyCle still does not reach its best performance after four times of refinements, as the curve has not plateaued yet, highlighting its potential to achieve much better results. CyCle's ability to self-refine also aligns better with human developers' iterative programming practice, where they learn from mistakes and continuously improve. CyCle exemplifies how code LMs can be designed to imitate iterative programming, bringing real-time improvement to developers' code interactively.</p>
<p>In contrast, baseline models, including the one fine-tuned with the same data as CyCle, typically plateau after one or two steps of refinement. For example, in Figure 4b that evaluated with MBPP-S benchmark, CyCle and baseline models start at similar performance, but refinement could not bring further improvement after two steps, while Cycle continues to go up. This gap sheds light on the necessity of evaluating code LM's capacity for self-refinement: models with similar one-time generation performance might not be similarly helpful, as the one with better selfrefinement capacity could assist developers with iterative programming more efficiently in the realistic deployment.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Performance improvement with self-refinement. The blue curve represents the "vanilla code LM baseline", using CodeGen-350M. The orange curve represents the "code LMs fine-tuned with the correct code". The green curve represents CyCle-350M.
5.2.2 Execution Feedback Guides CyCLE for Better Self-Refinement. Second, we analyze whether the execution feedback helps CyCle as expected. To do so, we remove the execution results before each time of refinement, only leaving the problem description, i.e., natural-language prompt, and the past, faulty generation as references. The results are shown in Table 2.</p>
<p>It is evident that execution feedback plays an important role in CyCLE's self-refinement, and removing it significantly drops CyCLE's performance across all three benchmarks. In contrast, baseline models are not sensitive to this removal. After removing the execution results, baseline models perform comparably or even better, suggesting that these models regard execution results mostly as redundant information or even noise. This highlights existing code LM's weaknesses in understanding execution effects and taking advantage of them.
5.2.3 Past Generation Mask (PGM) Effectively Prevents the Exact Copy and Improves CyCLE's Performance. As we introduced in Section 3.2.2, we design PGM to avoid the model taking shortcuts by</p>
<p>Table 2. Cycle's performance decreases when the execution feedback is removed during the self-refinement, while the baseline models could not effectively perceive the execution results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;">MBPP-S</th>
<th style="text-align: center;">APPS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CodeGen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/o exec feedback</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: center;">w/ exec feedback</td>
<td style="text-align: center;">$12.2 \downarrow 4.7 \%$</td>
<td style="text-align: center;">$21.8 \downarrow 9.5 \%$</td>
<td style="text-align: center;">$6.9-0.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">CodeGen + FT w/ Correct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/o exec feedback</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">8.3</td>
</tr>
<tr>
<td style="text-align: center;">w/ exec feedback</td>
<td style="text-align: center;">$14.0-0.0 \%$</td>
<td style="text-align: center;">$23.0 \downarrow 11.5 \%$</td>
<td style="text-align: center;">$7.7 \downarrow 7.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Cycle (Ours)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/o exec feedback</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">8.5</td>
</tr>
<tr>
<td style="text-align: center;">w/ exec feedback</td>
<td style="text-align: center;">$\mathbf{2 0 . 7} \uparrow 20.2 \%$</td>
<td style="text-align: center;">$\mathbf{3 2 . 6} \uparrow 10.5 \%$</td>
<td style="text-align: center;">$\mathbf{8 . 7} \uparrow 2.4 \%$</td>
</tr>
</tbody>
</table>
<p>naively copy-pasting the faulty generation as its prediction, which might also decrease the training loss due to the token overlaps between the faulty code and its refined version [Ding et al. 2020]. To verify the effectiveness of PGM, we train Cycle-350M-w/o-PGM disabling the PGM (i.e., setting the masking ratio to be $0 \%$ ) and compare its performance with the standard CYCLE-350M.</p>
<p>We studied three perspectives of their differences. First, we compare their overall performance of code generation, i.e., the test suite pass rate as we reported in Section 5.1. Second, we analyze the edit distance, in terms of code tokens, between the faulty generation and its refined code predicted by Cycle-350M-w/o-PGM and CyCle, where a higher value indicates the model modifies more tokens to correct the fault. Third, we study how many exact copies happen along the way of the self-refinement process until it reaches the maximum refine limit. A higher exact copy rate indicates the model copy-pastes the fault generation as the self-refinement prediction more frequently.</p>
<p>Table 3. Impact of PGM on Cycle's performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Pass Rate \% ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Token Edit Distance ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Exact Copy Rate \% ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">HEval</td>
<td style="text-align: center;">MBPP-S</td>
<td style="text-align: center;">APPS</td>
<td style="text-align: center;">HEval</td>
<td style="text-align: center;">MBPP-S</td>
<td style="text-align: center;">APPS</td>
<td style="text-align: center;">HEval</td>
<td style="text-align: center;">MBPP-S</td>
<td style="text-align: center;">APPS</td>
</tr>
<tr>
<td style="text-align: left;">w/o PGM</td>
<td style="text-align: center;">20.12</td>
<td style="text-align: center;">31.38</td>
<td style="text-align: center;">7.81</td>
<td style="text-align: center;">42.44</td>
<td style="text-align: center;">31.34</td>
<td style="text-align: center;">162.67</td>
<td style="text-align: center;">8.23</td>
<td style="text-align: center;">15.66</td>
<td style="text-align: center;">1.24</td>
</tr>
<tr>
<td style="text-align: left;">w/ PGM</td>
<td style="text-align: center;">$\mathbf{2 0 . 7 3}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 5 5}$</td>
<td style="text-align: center;">$\mathbf{8 . 6 7}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 1 7}$</td>
<td style="text-align: center;">$\mathbf{1 6 2 . 7 2}$</td>
<td style="text-align: center;">$\mathbf{7 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{1 . 2 1}$</td>
</tr>
</tbody>
</table>
<p>The comparison is shown in Table 3. It is clear that PGM helps to improve the overall performance of self-refinement across all three benchmarks. Also, when PGM is enabled, the model tends to edit more tokens, on average, to refine the faulty generation, and avoid the exact copy at its best.</p>
<p>Result-2: The core feature, self-refinement, significantly enhances CyCLE's capability, aligning with iterative programming practices in human developers. In addition, the execution feedback is vital for CyCLE's self-improvement, with its removal leading to a notable drop in performance. Also, Past Generation Mask (PGM) effectively prevents naive copy-pasting, leading to better overall performance and more diverse code edits for refinement.</p>
<h1>5.3 RQ3. Relationship Between Cycle's Self-Refinement and Top-K Generation</h1>
<p>To more comprehensively evaluate code LMs' capacity in code generation, Chen et al. [2021] proposes to generate up to K sequences simultaneously to explore a more diverse search space</p>
<p>and use test cases to pick the correct ones as the final prediction. Later, open-source code LMs [Li et al. 2023; Nijkamp et al. 2023b; Rozière et al. 2023] incorporate such a setting as an additional evaluation for the code generation task.</p>
<p>In this section, we explain that CyCle's self-refinement for code generation is an orthogonal direction and maintains comparable overhead to the top-k generation. In addition, the self-refinement is applicable to the top-k generations to further improve the overall performance.
5.3.1 Preliminary. When generating code autoregressively, code LMs estimate the probability of each possible next token based on the given context. While it is intuitive to always choose the most probable next token during the generation, which is known as greedy decoding, the single generation might not be able to fully expose the diverse knowledge that the model has learned. To explore a wider search space while ensuring coherence, several techniques [Holtzman et al. 2020; Tillmann and Ney 2003] are applied to code LMs for generating multiple predictions simultaneously. Existing code LMs [Chen et al. 2021; Li et al. 2023; Nijkamp et al. 2023b; Rozière et al. 2023] have shown that such a top-k generation could improve the overall accuracy.
Beam Search. Beam search is a deterministic search algorithm for sequence generation, extending greedy decoding to generate multiple, most probable sequences. It explores multiple candidate sequences simultaneously by retaining the top-k most promising ones at each step, and the generated sequences so far will be ranked based on the cumulative token probabilities. This process is repeated, gradually expanding the sequences, and at the end, the top-k sequences with the highest likelihood score will be the final output.</p>
<p>While it allows the model to output more than one sequence, it also introduces significantly more computations and inference overhead. For example, maintaining multiple sequences on GPU during generation requires linearly more GPU memories. Also, the repetitive ranking during the generation makes it more time and computation-consuming than a single generation.
Nucleus Sampling with Temperature. Holtzman et al. [2020] propose nucleus sampling, also known as top-p sampling, to enhance the creativity of sequence generation with language models. Instead of rigidly choosing the top-k most probable sequences like beam search, nucleus sampling dynamically selects a subset of the most probable words at each generation step and samples one out of them.</p>
<p>Specifically, when generating the next token, nucleus sampling first cumulates a probability mass of the most likely tokens, called the nucleus, until it exceeds the threshold "p" (e.g.,, 0.9). Then a concrete token will be randomly sampled from this dynamically determined probability mass. Nucleus sampling can also be coupled with a temperature parameter, which re-scales the likelihood distribution of words and consequently affects the dynamic selection of the nucleus. A higher temperature (e.g., 0.8) makes the distribution flatter, involving more tokens into the nucleus, while a lower temperature (e.g., 0.2) sharpens the distribution, favoring high-probability tokens.</p>
<p>To generate " $k$ " sequences with nucleus sampling, the typical approach is to duplicate the input " $k$ " times and apply the sampling independently, where, at each step, the randomness of sampling will make the difference. Similar to beam search, as it maintains multiple sequences on GPU, the extra memory overhead is applied.</p>
<p>For the rest of the section, we conduct experiments to study the code LM's performance with top-k generation using beam search and nucleus sampling with different temperatures and compare the results with CyCle's self-refinement, highlighting their difference and the potential interaction. To save the computation, all experiments are conducted with CyCle-350M, but the experiments are designed to be generalizable to all sizes of code LMs, and we expect the observations to be maintained among different model sizes.</p>
<p>5.3.2 Cycle's self-refinement is orthogonal to top-k generation. As we show in Figure 5a, the iterative process of self-refinement is conceptually different from the top-k generation. Concretely, top-k generation produces multiple code candidates (green nodes in Figure 5a) with the same prior condition, i.e., only the prompt, which explores the breath of the code generation. In contrast, the process of self-refinement is iteratively updating the prior condition, where the old generation and the execution results will be continuously updated and concatenated to the prompt as additional references (yellow nodes in Figure 5a). Therefore, different from the breath exploration of top-k generation, self-refine is improving a specific generation with directional guidance in depth.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Cycle's self-refinement is orthogonal to the top-k generation. Conceptually, top-k generation explores in breadth, while self-refinement improves a specific generation in depth. Empirically, when compared to nucleus sampling (with the temperature of $0.2,0.6,0.8$ ) and beam search, self-refinement optimizes the generated code towards execution guidance, while top-k produces more diverse programs that pass a similar amount but complementary test cases.</p>
<p>We also conduct experiments to study how the orthogonal instincts perform differently during the code generation.
Setup. We consider four types of top-k generation as a comparison: nucleus sampling with the temperature of $(0.2,0.6,0.8)$, and beam search. Specifically, the code LM will generate the top-5 sequences as the prediction, where 5 is also the number of code CyCle produces along the iterative self-refinement according to our configuration (Section 4.4). Then we indexed the generated code for each method following their generation order. Finally, we check how many and which test cases each generation will pass individually across all methods. The experiment is conducted with the MBPP-S benchmark which contains 1,323 test cases in total.
Findings. As shown in Figure 5b, Cycle eventually passes more test cases in MBPP-S step by step, since it learns to improve the old generation according to the execution results, revealing the behavior of exploring code generation in depth. On the contrary, the number of passed test cases is mostly similar across different generations of a specific top-k generation method, either nucleus sampling or beam search. After checking in detail, we realize that the accumulated number of passed test cases across all five generations is significantly higher than any individual generation. For example, all five generations from nucleus sampling with temperature 0.8 passed 513 test cases altogether, while one generation could only pass roughly 260 test cases. This reveals that top-k generation samples diverse code as the prediction, exploring the breadth of the search space.</p>
<p>5.3.3 CyCLE could further refine top-k generations. Motivated by the findings in 5.3.2, we delve deeper to study whether CyCLE's self-refinement is applicable to top-k generation and whether combining these two orthogonal techniques could further push the performance boundary. Specifically, Cycle is supposed to be able to improve any past generation, so we evaluate CyCLE's performance to refine the top-k generation. As we show in Figure 5, the top-k generations (green nodes) will be improved by CyCLE independently, producing refined programs for each of them (gray and yellow nodes).
Setup. We evaluate CyCLE's capacity in refining top-k generation using HumanEval, MBPP-S, and APPS benchmarks, similar to our main evaluation (Section 5.1). We again choose nucleus sampling with the temperature of $(0.2,0.6,0.8)$ and beam search to generate the top- 5 predictions, and each generation will be refined four times, resulting in a total of twenty-five predictions.</p>
<p>Table 4. The test suite Pass@5 rate for three programming benchmarks. CyCLE's self-refinement is applicable to top-k generation and further improves its performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">One-time (Pass@5)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Self-Refine (Pass@5)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">MBPP-S</td>
<td style="text-align: center;">APPS</td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">MBPP-S</td>
<td style="text-align: center;">APPS</td>
</tr>
<tr>
<td style="text-align: left;">Beam Search</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">$25.6+91.0 \%$</td>
<td style="text-align: center;">$47.3+117.2 \%$</td>
<td style="text-align: center;">$12.3+44.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Nuc. Samp. (tmp=0.2)</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">$25.6+68.0 \%$</td>
<td style="text-align: center;">$45.9+71.9 \%$</td>
<td style="text-align: center;">$11.6+30.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Nuc. Samp. (tmp=0.6)</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">$28.1+48.4 \%$</td>
<td style="text-align: center;">$46.6+45.3 \%$</td>
<td style="text-align: center;">$11.9+30.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Nuc. Samp. (tmp=0.8)</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">$25.6+35.4 \%$</td>
<td style="text-align: center;">$47.3+59.1 \%$</td>
<td style="text-align: center;">$12.1+36.1 \%$</td>
</tr>
</tbody>
</table>
<p>Findings. The results are reported in Table 4. We can see that CyCLE's self-refinement consistently and significantly boosts the pass@5 rate across four methods of top-k generation, up to $117.2 \%$ relative improvement. This verifies that the self-refinement is compatible with the top-k generation and can further improve its performance.
5.3.4 The overhead of CyCLE's self-refinement is comparable to top-k generation. Another difference between CyCLE's self-refinement and top-k generation is that, during the inference, the former is an iterative process while the latter is a simultaneous process. It is difficult to conceptually reason about which approach is supposed to have higher inference overhead since self-refinement is inevitably a sequential process, while top-k generation requires more GPU memory and additional computation of gathering the most probable tokens at each generation step.
Setup. To study and compare the overhead between CyCLE's self-refinement and top-k generation, we record two things during their inference: (1) the inference time (in seconds), and (2) the number of executions the machine performs to verify the correctness or collect the execution feedback.</p>
<p>Note that the comparison is conducted based on the restriction that "only five sequences can be generated", no matter whether the process is iterative or simultaneous. Consequently, top-k generation methods will produce five sequences with one-time inference, while CyCLE produces five sequences by iteratively generating only one sequence at a time, executing test cases and collecting feedback, and running inference again to refine the past generation.
Findings. We show the comparison in Table 5. When we compare the inference time of different methods, we notice that CyCLE's inference time with self-refinement of generating five sequences is actually shorter than top-k generation, even if self-refinement is an iterative process. The main reason is that, restricted by the fixed size of GPU memory, the batch size of top-k generation has to be reduced when generating more sequences simultaneously, while self-refinement only generates one prediction at a time, so the batch size is significantly larger. The efficient parallel computation of GPU for a larger batch compensates for the time consumption of CyCLE's sequential refinement,</p>
<p>Table 5. The comparison of inference overhead between Cycle's self-refinement and top-k generation. "Infer.(s)" represents the model inference time in seconds, "#Exec" represents the number of executions performed for evaluation, and "Pass(\%)" represents the ratio of passed test suites (i.e., the ratio of programming problems solved by the method)</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>HumanEval</th>
<th></th>
<th></th>
<th>MBPP-S</th>
<th></th>
<th></th>
<th>APPS</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Infer. (s)</td>
<td>#Exec.</td>
<td>Pass (\%)</td>
<td>Infer. (s)</td>
<td>#Exec.</td>
<td>Pass (\%)</td>
<td>Infer. (s)</td>
<td>#Exec.</td>
<td>Pass (\%)</td>
</tr>
<tr>
<td>Beam Search@5</td>
<td>507.6</td>
<td>3263</td>
<td>13.4</td>
<td>1222.2</td>
<td>3473</td>
<td>21.8</td>
<td>4804.4</td>
<td>32472</td>
<td>8.5</td>
</tr>
<tr>
<td>Nuc. Samp.@5 (tmp=0.2)</td>
<td>425.5</td>
<td>4723</td>
<td>15.2</td>
<td>1084.4</td>
<td>5630</td>
<td>26.7</td>
<td>5179.9</td>
<td>31189</td>
<td>8.8</td>
</tr>
<tr>
<td>Nuc. Samp.@5 (tmp=0.6)</td>
<td>428.9</td>
<td>6422</td>
<td>18.9</td>
<td>1147.8</td>
<td>8224</td>
<td>32.1</td>
<td>5213.7</td>
<td>32412</td>
<td>9.1</td>
</tr>
<tr>
<td>Nuc. Samp.@5 (tmp=0.8)</td>
<td>423.9</td>
<td>6612</td>
<td>18.9</td>
<td>1084.7</td>
<td>8568</td>
<td>29.7</td>
<td>5224.8</td>
<td>32458</td>
<td>8.9</td>
</tr>
<tr>
<td>Cycle @5</td>
<td>403.2</td>
<td>6725</td>
<td>20.7</td>
<td>762.6</td>
<td>8755</td>
<td>32.3</td>
<td>4238.3</td>
<td>32475</td>
<td>8.7</td>
</tr>
</tbody>
</table>
<p>while the smaller batch size of top-k generation results in more batches that have to be sequentially fed into GPU instead.</p>
<p>Conceptually, the number of executions of test cases should be exactly the same between top-k generation and Cycle's self-refinement, since they generate the same number of predictions for each programming problem in the benchmarks, and the number of test cases for each problem is fixed. However, in practice, we notice that the numbers are varied across methods, and Cycle is more comparable to nucleus sampling with a high temperature but higher than that with a low temperature and the beam search. The main reason is that our implementation of the execution framework follows the original design of HumanEval, where Chen et al. [2021] minimizes the number of executions by executing the same predictions only once. As a result, the more diverse the predictions are, the more executions will be performed, and that is why we see an increase in the execution number when the temperature increases.</p>
<p>To conclude, we could not see a significant gap between top-k generation and Cycle's selfrefinement, in terms of both performance and inference overhead, since they have advantages and disadvantages in orthogonal angles. As we found in Section 5.3.3, the better solution is to take advantage of both techniques.</p>
<p>Result-3: Cycle's self-refinement is an orthogonal direction to top-k generation with comparable overhead, where the former improves one single generation in depth iteratively while the latter encourages exploring the breadth of the sequence space for the one-time generation. Further, Cycle could be applied to top-k generation to further improve the overall performance of code generation.</p>
<h1>5.4 RQ4. Ablation Study of Past Generation Mask and Data Mixture</h1>
<p>In this section, we conduct an ablation study to analyze the impact of Past Generation Mask's (PGM) masking ratio and the data combination for self-refinement training, since these empirical choices are expected to have non-trivial impacts on the learning process.</p>
<p>As we introduced in Section 3.2.2, PGM randomly masks a certain ratio of code tokens to prevent the model from lazily taking shortcuts, using the exact copy of the past, faulty generation as the refined prediction. However, different masking ratios could result in different learning behaviors. Concretely, when the masking ratio is high, there will be limited tokens for the model to refer to, and the model has to produce most tokens from scratch. On the contrary, when the masking ratio is low, the model could learn to borrow some helpful pieces of code, such as the code structure and the meaningful identifiers while not naively replicating past errors.</p>
<p>In Section 3.2.3, we proposed to mix self-refine samples (Section 3.1) and original code samples to construct the data for training, ensuring the model effectively learns self-refining while maintaining</p>
<p>the capacity of general code generation. However, the proportion of these two data resources could affect the learning process. Specifically, when self-refine samples are overly dominant, the model will be optimized mostly towards self-refinement, where the natural language prompt, faulty generation, and the execution results are all required to present as the prior condition, while losing the capacity in one-time code generation where only the prompt is given, and vice versa.
Setup. To conduct the ablation study on the PGM masking ratio and the data combination proportion, we train CyCle-350M multiple times with controlled settings and evaluate across the three programming benchmarks to compare across different settings for concluding the trend. Specifically, to study the impacts of PGM masking ratio we train CyCle-350M four times with ratios of $[0.0,0.05,0.15,0.30]$ while maintaining all other training settings the same. Similarly, to study the difference across varied data mixture proportions, we train CyCle-350M five times with the self-refine data ratios of $[0 \%, 25 \%, 50 \%, 75 \%, 100 \%]$.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(a) The impact of PGM masking ratio
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) The impact of data mixture proportion</p>
<p>Fig. 6. Ablation Study</p>
<p>Findings. The results regarding PGM masking ratios are plotted in Figure 6a. From the figure, we deduce that PGM operates optimally at a lower masking ratio. Specifically, a masking ratio of around 0.05 emerges as an optimal point, indicating that the slight obfuscation can effectively prevent the exact copy during self-refinement while retaining valuable referring tokens in the faulty generation. However, going too high with the masking ratio seems to counteract its benefits, especially evident in the MBPP's sharp decline.</p>
<p>The results of the data mixture proportion are plotted in Figure 6b. We can see that an amalgamation of original and self-refined code samples offers superior training outcomes than one resource standalone. A balance, possibly around the $25 \%$ ratio, appears to be a sweet spot. Solely depending on one kind of data, especially pure self-refined data, does not yield the best results, highlighting the importance of diverse training samples. Interestingly, we notice the performance on APPS exhibits a relatively stable pass rate, starting at $7.7 \%$, slightly peaking at $8.7 \%$ for a $25 \%$ data ratio, and settling around $9.1 \%$ for a full self-refinement. We speculate that the slightly higher performance in APPS when trained with $100 \%$ self-refine samples is due to the similar distribution between our training data resources (Section 3.1) and APPS, where both mostly contain more challenging programming problems than HumanEval and MBPP.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ At a high level, CodeGen and StarCoder are both GPT-like Transformer decoder, but their concrete neural architectures (e.g., positional embedding and attention layers) are not the same. More details can be referred from: StarCoder (https://github. com/huggingface/transformers/blob/main/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py) and CodeGen (https://github.com/huggingface/transformers/blob/main/src/transformers/models/codegen/modeling_codegen.py)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2033 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2033</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2033</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-281194812</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.04731v2.pdf" target="_blank">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2033.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2033.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HS-MARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid framework that integrates an HTN symbolic planner (pyHIPOP+) with MARL: the planner decomposes high-level goals into subgoals/options and a meta-controller assigns symbolic options to low-level agents, producing an implicit curriculum over sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>rule-based automated via HTN planner (symbolic decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Curriculum emerges from HTN decomposition: a domain HDDL description and pyHIPOP+ produce a hierarchical state-space tree of subgoals (leaves are basic skills). Agents first learn leaf/subgoal policies (options); composition into higher-level tasks follows the HTN ordering, producing an implicit progression from simple to complex skills.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic soccer / multi-agent team play</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Long-horizon, temporally-extended plays composed of multiple sub-skills (e.g., AcquireBall -> MoveToShootingPosition -> ExecuteShot); decomposition can be several layers deep depending on task (leaf actions like move_to_ball up to multi-agent passing sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>compared in cited work against flat MARL/end-to-end MARL baselines (paper states improved sample efficiency vs. flat MARL); this review compares conceptually to hand-crafted curricula like TiZero and staged curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This position paper reports that Mu et al. (HS-MARL) demonstrated significantly improved performance and sample efficiency relative to flat MARL in their experiments, but does not include numeric metrics or units; original HS-MARL paper should be consulted for quantitative values.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Reported qualitatively as 'significantly improved sample efficiency' compared to end-to-end MARL; no convergence episode counts or percentages are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Paper states HS-MARL offers enhanced interpretability and reusability of sub-policies, implying improved compositional generalization, but no concrete held-out/zero-shot metrics are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported in detail; hierarchical decomposition inherently enables diverse task composition but explicit diversity measurements are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>HTN explicitly encodes prerequisite structure via HDDL (subtasks arranged by logical/temporal proximity). Paper highlights this as a strength but gives no evaluation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — HTN produces intermediate subgoals/options (symbolic options) used as training targets; treated as effective scaffolding in cited work but no quantitative ablation reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported for HS-MARL in this paper; author notes HTN requires HDDL domain engineering which is an engineering bottleneck (human time cost).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported; HDDL descriptions imply human-crafted domain models were used in Mu et al.'s work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Symbolic HTN planners integrated with MARL produce an implicit curriculum by decomposing tasks into learnable subgoals, improving sample efficiency and interpretability versus flat MARL; the approach shifts engineering effort to domain description (HDDL) but reduces learner burden.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2033.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TiZero (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modified TiZero framework (Baghi 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MARL approach for soccer that relies on a manually designed multi-stage curriculum (e.g., 1v1 -> 2v2 -> full team) and intrinsic reward engineering to progress agent competencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>hand-crafted by experts (multi-stage explicit curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>A staged curriculum where agents must reach performance thresholds (win rates) at progressively more complex scenarios (1v1, then 2v2, etc.); intrinsic rewards and reward shaping are used within stages to provide denser learning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic / simulated soccer (football-like environments)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Progressively increasing complexity from single agent duels (1v1) to team coordination (full 3v3/5v5), each stage adds agents, opponents, and coordination sub-skills (passing, positioning, defending).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Implicitly compared to flat end-to-end MARL and physics-only simulators; in-field comparisons emphasize manual curricula versus hierarchical/symbolic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper references Baghi (TiZero) as effective but still sample-inefficient relative to hierarchical symbolic approaches; no numeric metrics included here.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Manual curricula can improve learning speed relative to flat training, but are resource-intensive; no episode counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported in this review; manual curriculum aids staged competency but generalization to novel task compositions not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not analyzed quantitatively; manual curricula are limited by pre-designed scenarios so task diversity depends on human design.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Prerequisites are manually specified by curriculum stage (e.g., win at 1v1 before 2v2).</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — intermediate stages are explicitly created by designers; effectiveness noted but scalability is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Human engineering cost high (designing stages); computational cost of training not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Curriculum is human-designed; no formal human-expert rating study reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Manually designed multi-stage curricula (e.g., TiZero) are effective for incremental skill acquisition but are labor-intensive and less scalable than hierarchical or automated approaches; they provide clear prerequisites but may lack flexibility.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2033.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARLadona</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MARLadona - Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MARL system for soccer that uses dense rewards plus a hand-crafted curriculum to train cooperative team behaviors at an abstract action level (move, turn, kick).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>hand-crafted curriculum with reward shaping</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Explicitly hand-designed curriculum stages (e.g., simpler matches and scenarios) combined with dense shaped rewards to provide intermediate learning signals; operates at abstract action representations to reduce search space.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic / simulated soccer</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Team play tasks with multi-agent coordination; complexity managed through abstraction (high-level actions) and staged scenarios increasing in tactical demands.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Contrasted conceptually with flat physics simulators and hierarchical symbolic methods; no numeric baselines provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Described as effective in low-resource settings using abstraction and dense rewards, but no concrete metric values are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Implied faster learning than low-level end-to-end systems due to abstraction and curriculum, but no quantitative comparisons included.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not quantified here; abstraction purportedly aids transfer of high-level skills.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Prerequisites enforced via staged curriculum, manually specified.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — manually designed intermediate scenarios and shaped reward signals act as bridging tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Emphasized as lower-resource compared to end-to-end high-fidelity approaches, but exact costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No formal human evaluation reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Operating at abstract action levels with a hand-crafted curriculum and dense rewards enables effective multi-agent team learning in lower-resource settings, but relies on human design and lacks scalability of automated scaffolding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2033.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eligibility-Traces Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eligibility Traces-based multi-stage curriculum (Azarkasb & Khasteh 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage training method that stores successful trajectories (eligibility traces) as a knowledge base and incrementally introduces complexity (e.g., obstacles) to form a curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>rule-based automated via stored successful trajectories; staged introduction of complexity</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Training proceeds in stages: learn optimal paths in obstacle-free settings first and store successful trajectories as eligibility traces; after foundational mastery, introduce opponents/obstacles. The stored traces function as reusable sub-plans, creating an implicit curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic soccer with obstacle avoidance/navigation</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Two-stage progression from simple navigation/path-following to obstacle-rich multi-agent scenarios; sub-skills include path planning, ball acquisition, and obstacle avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared conceptually to staged/manual curricula and to symbolic decomposition approaches; no quantitative baselines given in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Described qualitatively as effective (multi-stage training yields competent navigation before opponents are introduced); no numeric metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Qualitatively faster convergence on foundational skills due to staged exposure; no episode counts or percentages provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Stored traces create reusable behavior primitives that aid later tasks, but no generalization metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Prerequisites are discovered/encoded via stored successful trajectories (eligibility traces) rather than manually specified.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — obstacle-free mastery and stored traces act as intermediate tasks; reported as effective conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified; approach emphasizes sample-efficient reuse of successful trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Staging training with eligibility traces that record successful low-complexity behaviors provides an implicit curriculum and reusable sub-plans, easing learning for later, harder tasks, though reported results in this review are qualitative.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2033.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BSN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Strategy Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic graphical approach that decomposes a complex policy into conditional sub-policies, modeling dependencies (e.g., shoot policy conditioned on possession) to structure multi-agent strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian strategy networks based soft actor-critic learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>probabilistic decomposition of policy (model-based structuring)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Curriculum arises from the BSN's factorization of the global policy into conditionally dependent sub-policies; training can focus on these simpler sub-policies and their conditional activation, yielding a structured learning progression but not an explicit staged curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>robotic/multi-agent soccer</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Complex, conditional strategies where sub-policies become relevant only under certain conditions (e.g., possess_ball -> consider shooting). Composition involves learning multiple sub-policies and conditional activation.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared conceptually to monolithic policies and hierarchical symbolic planners; no numeric comparisons in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper cites BSN as enabling decomposition and improved tractability; quantitative results not summarized in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Implied improved tractability and learning efficiency over monolithic policies; no detailed numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Structured decomposition is argued to aid interpretability and reusability of sub-policies, but no held-out generalization metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Dependencies between sub-policies explicitly modeled by BSN; serves as a form of prerequisite structure.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not explicitly generation-based; decomposition yields modular tasks (sub-policies) that can act as intermediate training targets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Probabilistic decomposition via BSNs structures policies into conditional sub-policies, easing learning and improving interpretability; presented as complementary to symbolic HTN approaches but without numeric comparisons in this review.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2033.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCRL / Two-stage (Zhao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mimic-to-Counteract (Two-Stage Reinforcement Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage curriculum for Google Research Football where agents first mimic expert behavior and then learn to counteract opponents, structuring training through distinct strategic stages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From Mimic to Counteract: a Two-Stage Reinforcement Learning Algorithm for Google Research Football</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>two-stage hand-designed curriculum (mimic expert then counteract)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Stage 1: imitation/mimicry of expert trajectories to acquire baseline skills; Stage 2: reinforcement learning focused on counter-strategies against opponents. This staged approach enforces a curriculum of strategic difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>simulated soccer (Google Research Football)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Long-horizon team-play tasks decomposed into imitation sub-skills (match play basics) followed by strategic adversarial skills; multi-step plays and counter-strategies required.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Described relative to end-to-end approaches and imitation-only baselines conceptually; detailed numeric comparisons not included in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as a staged method to improve strategy learning in GFootball; this review does not provide numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Qualitatively said to help bootstrap policy learning via imitation then improve via RL; no episodes or percent improvements reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Prerequisites are enforced by stage ordering (must learn to imitate before counteracting).</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — imitation stage provides intermediate behavior priors that bridge to adversarial learning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Two-stage mimic-then-counteract curricula can bootstrap strategic learning in football environments by leveraging demonstration priors then RL fine-tuning; the review reports conceptual benefits but no quantitative values.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2033.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-generated curricula (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-generated task decomposition and curricula (Voyager, SayCan examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of large language models to decompose high-level goals into sequences of executable subgoals and to generate curricula or tasks (examples: SayCan and Voyager demonstrate language-driven decomposition and self-directed curriculum generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated (prompt-driven zero-shot / self-generated task proposals)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>LLMs decompose natural-language goals into ordered subgoals via prompting or chain-of-thought; Voyager generates progressively harder self-directed tasks, and SayCan decomposes instructions into executable low-level actions grounded in affordances. The paper proposes extending this to dynamically configure environment task APIs and intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>embodied agents (robotic manipulation, Minecraft, and proposed robotic soccer task scaffolding)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>High-level natural language goals decomposed into multi-step plans (e.g., 3–6 subgoals like dribble -> pass -> run -> return pass), potentially re-planning mid-episode; complexity varies with requested play.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Contrasted conceptually with static HTN/HDDL files and manual curricula; proposed as more scalable/flexible but introducing new challenges (logical consistency, physical feasibility).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No experimental performance metrics reported in this review for LLM-generated curricula in soccer; cited prior embodied works (Voyager, SayCan) demonstrate capability but numeric comparisons are not included here.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Proposed to improve scalability and reduce human engineering time; no empirical training speed or sample-efficiency numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Argued to support instruction-following and zero-shot composition of tasks, but no quantitative generalization results are reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not empirically measured here; LLMs are proposed to increase curriculum/task diversity via prompt-driven generation.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>LLMs produce ordered subgoals implying prerequisite structure; the paper notes concerns about ensuring logical/physical consistency of such generated prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — LLMs can generate intermediate subgoals and episode-specific intrinsic reward definitions on the fly; effectiveness is proposed but not empirically validated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Discussed qualitatively: possible logical inconsistencies, physically infeasible plans, designer bias transfer, and grounding/symbol-grounding challenges when mapping symbolic subgoals to low-level state/action coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified; paper notes LLMs reduce human engineering cost but introduces compute for model inference and the need for verification/grounding pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not performed in this paper; the authors suggest human-in-the-loop validation may be needed for LLM-generated scaffolds.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>LLMs offer a promising path to dynamically generate scalable, language-driven curricula and task decompositions, potentially eliminating manual HDDL engineering; however, they introduce risks of inconsistent or infeasible plans and pose grounding challenges that require further work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2033.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Advice Distillation (ADA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADA - Advice Distillation from Teacher to Student</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A teacher-student distillation framework where a teacher agent provides advice that structures the student's learning process, creating a social/hierarchical curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Attention Guided Advice Distillation in Multi-Agent Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>teacher-driven / distillation-based (social curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Teacher agents produce advice/behaviors which are distilled into student agents via self-attention guided mechanisms; this creates a hierarchical social curriculum where student learning is scaffolded by teacher-provided intermediate signals.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>multi-agent soccer / strategic team play</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Supports learning of coordinated high-level strategies through teacher-produced guidance; complexity arises from multi-agent coordination and temporal dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Discussed relative to pure RL training without teacher advice and to other distillation approaches; quantitative comparisons not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in this position paper; original ADA paper contains experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Suggested to speed up student learning via distilled advice, but no numbers reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Teacher advice implicitly encodes prerequisite behaviors, but no explicit analysis reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Teacher advice functions as intermediate guidance; effectiveness asserted conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Teacher-student distillation provides a social form of hierarchical curriculum that can structure student learning and accelerate acquisition of coordinated behaviors; specifics require consulting the ADA paper for quantitative outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2033.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2033.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>This paper's proposal: Scaffolded Environments + LLM planners</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Task Environments with Procedural Curriculum Generation via LLMs (proposed here)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposal to design environments that expose hierarchical task APIs, layered action spaces, intrinsic subgoal rewards, and procedural curriculum generation driven by LLM planners to produce dynamic, compositional curricula for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>proposed: LLM-driven procedural curriculum generation + environment-managed intrinsic rewards</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Environments would offer HDDL-like hierarchical task APIs and intrinsic reward hooks; an LLM (prompted with high-level natural language goals) would synthesize ordered symbolic subgoals and configure per-episode success conditions and intrinsic rewards, enabling procedurally generated curricula that grow in complexity as agents master sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>proposed primarily for robotic soccer but intended to generalize to other embodied multi-agent domains</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Targets long-horizon, multi-agent, compositional tasks requiring sequences of coordinated sub-skills (passing, positioning, dribbling, coordinated runs), potentially multi-layered with re-planning mid-episode.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Proposed comparison axes include flat physics simulators, manual staged curricula (TiZero/MARLadona), HTN/HDDL-based planners, and LLM-generated dynamic curricula; no empirical comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No empirical metrics—proposal includes suggested evaluation metrics for future work: Compositional Generalization Score, Curriculum Efficiency (sample complexity reduction), and Scaffolding Brittleness Index, but no numeric results are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Hypothesized to reduce sample complexity and training time versus flat end-to-end approaches and manual curricula, but no empirical evidence provided in this position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Hypothesized improvement in compositional generalization due to reusable sub-policies and hierarchical structure, but no experimental results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Proposes that LLM-driven procedural generation would increase task diversity, but no analyses or measurements are provided yet.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>LLM-generated plans specify subgoal ordering (prerequisites); paper also suggests studying methods for agents to autonomously discover hierarchies (e.g., bottleneck states), but provides no evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Core feature: environments should procedurally create intermediate sub-tasks and intrinsic rewards via LLM planning; proposed effective but untested in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Paper cautions about potential LLM failures: logical inconsistency, physically infeasible plans, designer bias transfer, and symbol-grounding difficulties when mapping subgoals to continuous states.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not empirically measured; paper argues LLM approach reduces human engineering cost but requires model inference and verification pipelines—computational tradeoffs not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not performed (proposal stage); authors note human oversight may be needed for validation of generated scaffolds.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Argues that embedding hierarchical scaffolds into environments and using LLMs as dynamic task planners can produce scalable, compositional curricula that improve sample efficiency and enable compositional generalization; highlights grounding, engineering-burden tradeoffs, and potential LLM failure modes as primary challenges.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies <em>(Rating: 2)</em></li>
                <li>MARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments <em>(Rating: 2)</em></li>
                <li>Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy <em>(Rating: 2)</em></li>
                <li>Bayesian strategy networks based soft actor-critic learning <em>(Rating: 2)</em></li>
                <li>From Mimic to Counteract: a Two-Stage Reinforcement Learning Algorithm for Google Research Football <em>(Rating: 2)</em></li>
                <li>Self-Attention Guided Advice Distillation in Multi-Agent Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2033",
    "paper_id": "paper-281194812",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "HS-MARL",
            "name_full": "Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning",
            "brief_description": "A hybrid framework that integrates an HTN symbolic planner (pyHIPOP+) with MARL: the planner decomposes high-level goals into subgoals/options and a meta-controller assigns symbolic options to low-level agents, producing an implicit curriculum over sub-tasks.",
            "citation_title": "Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies",
            "mention_or_use": "mention",
            "curriculum_generation_method": "rule-based automated via HTN planner (symbolic decomposition)",
            "curriculum_method_description": "Curriculum emerges from HTN decomposition: a domain HDDL description and pyHIPOP+ produce a hierarchical state-space tree of subgoals (leaves are basic skills). Agents first learn leaf/subgoal policies (options); composition into higher-level tasks follows the HTN ordering, producing an implicit progression from simple to complex skills.",
            "llm_model_used": null,
            "domain_environment": "robotic soccer / multi-agent team play",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Long-horizon, temporally-extended plays composed of multiple sub-skills (e.g., AcquireBall -&gt; MoveToShootingPosition -&gt; ExecuteShot); decomposition can be several layers deep depending on task (leaf actions like move_to_ball up to multi-agent passing sequences).",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "compared in cited work against flat MARL/end-to-end MARL baselines (paper states improved sample efficiency vs. flat MARL); this review compares conceptually to hand-crafted curricula like TiZero and staged curricula.",
            "performance_metrics": "This position paper reports that Mu et al. (HS-MARL) demonstrated significantly improved performance and sample efficiency relative to flat MARL in their experiments, but does not include numeric metrics or units; original HS-MARL paper should be consulted for quantitative values.",
            "learning_speed_comparison": "Reported qualitatively as 'significantly improved sample efficiency' compared to end-to-end MARL; no convergence episode counts or percentages are provided in this paper.",
            "generalization_performance": "Paper states HS-MARL offers enhanced interpretability and reusability of sub-policies, implying improved compositional generalization, but no concrete held-out/zero-shot metrics are reported here.",
            "task_diversity_analysis": "Not reported in detail; hierarchical decomposition inherently enables diverse task composition but explicit diversity measurements are not provided in this review.",
            "prerequisite_identification": "HTN explicitly encodes prerequisite structure via HDDL (subtasks arranged by logical/temporal proximity). Paper highlights this as a strength but gives no evaluation numbers.",
            "intermediate_task_generation": "Yes — HTN produces intermediate subgoals/options (symbolic options) used as training targets; treated as effective scaffolding in cited work but no quantitative ablation reported here.",
            "llm_limitations_observed": null,
            "computational_cost": "Not reported for HS-MARL in this paper; author notes HTN requires HDDL domain engineering which is an engineering bottleneck (human time cost).",
            "human_expert_evaluation": "Not reported; HDDL descriptions imply human-crafted domain models were used in Mu et al.'s work.",
            "key_findings_summary": "Symbolic HTN planners integrated with MARL produce an implicit curriculum by decomposing tasks into learnable subgoals, improving sample efficiency and interpretability versus flat MARL; the approach shifts engineering effort to domain description (HDDL) but reduces learner burden.",
            "uuid": "e2033.0"
        },
        {
            "name_short": "TiZero (modified)",
            "name_full": "Modified TiZero framework (Baghi 2024)",
            "brief_description": "A MARL approach for soccer that relies on a manually designed multi-stage curriculum (e.g., 1v1 -&gt; 2v2 -&gt; full team) and intrinsic reward engineering to progress agent competencies.",
            "citation_title": "Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments",
            "mention_or_use": "mention",
            "curriculum_generation_method": "hand-crafted by experts (multi-stage explicit curriculum)",
            "curriculum_method_description": "A staged curriculum where agents must reach performance thresholds (win rates) at progressively more complex scenarios (1v1, then 2v2, etc.); intrinsic rewards and reward shaping are used within stages to provide denser learning signals.",
            "llm_model_used": null,
            "domain_environment": "robotic / simulated soccer (football-like environments)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Progressively increasing complexity from single agent duels (1v1) to team coordination (full 3v3/5v5), each stage adds agents, opponents, and coordination sub-skills (passing, positioning, defending).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Implicitly compared to flat end-to-end MARL and physics-only simulators; in-field comparisons emphasize manual curricula versus hierarchical/symbolic approaches.",
            "performance_metrics": "Paper references Baghi (TiZero) as effective but still sample-inefficient relative to hierarchical symbolic approaches; no numeric metrics included here.",
            "learning_speed_comparison": "Manual curricula can improve learning speed relative to flat training, but are resource-intensive; no episode counts provided.",
            "generalization_performance": "Not reported in this review; manual curriculum aids staged competency but generalization to novel task compositions not quantified here.",
            "task_diversity_analysis": "Not analyzed quantitatively; manual curricula are limited by pre-designed scenarios so task diversity depends on human design.",
            "prerequisite_identification": "Prerequisites are manually specified by curriculum stage (e.g., win at 1v1 before 2v2).",
            "intermediate_task_generation": "Yes — intermediate stages are explicitly created by designers; effectiveness noted but scalability is limited.",
            "llm_limitations_observed": null,
            "computational_cost": "Human engineering cost high (designing stages); computational cost of training not quantified in this review.",
            "human_expert_evaluation": "Curriculum is human-designed; no formal human-expert rating study reported here.",
            "key_findings_summary": "Manually designed multi-stage curricula (e.g., TiZero) are effective for incremental skill acquisition but are labor-intensive and less scalable than hierarchical or automated approaches; they provide clear prerequisites but may lack flexibility.",
            "uuid": "e2033.1"
        },
        {
            "name_short": "MARLadona",
            "name_full": "MARLadona - Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning",
            "brief_description": "A MARL system for soccer that uses dense rewards plus a hand-crafted curriculum to train cooperative team behaviors at an abstract action level (move, turn, kick).",
            "citation_title": "MARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning",
            "mention_or_use": "mention",
            "curriculum_generation_method": "hand-crafted curriculum with reward shaping",
            "curriculum_method_description": "Explicitly hand-designed curriculum stages (e.g., simpler matches and scenarios) combined with dense shaped rewards to provide intermediate learning signals; operates at abstract action representations to reduce search space.",
            "llm_model_used": null,
            "domain_environment": "robotic / simulated soccer",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Team play tasks with multi-agent coordination; complexity managed through abstraction (high-level actions) and staged scenarios increasing in tactical demands.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Contrasted conceptually with flat physics simulators and hierarchical symbolic methods; no numeric baselines provided in this review.",
            "performance_metrics": "Described as effective in low-resource settings using abstraction and dense rewards, but no concrete metric values are reported in this paper.",
            "learning_speed_comparison": "Implied faster learning than low-level end-to-end systems due to abstraction and curriculum, but no quantitative comparisons included.",
            "generalization_performance": "Not quantified here; abstraction purportedly aids transfer of high-level skills.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Prerequisites enforced via staged curriculum, manually specified.",
            "intermediate_task_generation": "Yes — manually designed intermediate scenarios and shaped reward signals act as bridging tasks.",
            "llm_limitations_observed": null,
            "computational_cost": "Emphasized as lower-resource compared to end-to-end high-fidelity approaches, but exact costs not reported.",
            "human_expert_evaluation": "No formal human evaluation reported in the review.",
            "key_findings_summary": "Operating at abstract action levels with a hand-crafted curriculum and dense rewards enables effective multi-agent team learning in lower-resource settings, but relies on human design and lacks scalability of automated scaffolding.",
            "uuid": "e2033.2"
        },
        {
            "name_short": "Eligibility-Traces Curriculum",
            "name_full": "Eligibility Traces-based multi-stage curriculum (Azarkasb & Khasteh 2024)",
            "brief_description": "A multi-stage training method that stores successful trajectories (eligibility traces) as a knowledge base and incrementally introduces complexity (e.g., obstacles) to form a curriculum.",
            "citation_title": "Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy",
            "mention_or_use": "mention",
            "curriculum_generation_method": "rule-based automated via stored successful trajectories; staged introduction of complexity",
            "curriculum_method_description": "Training proceeds in stages: learn optimal paths in obstacle-free settings first and store successful trajectories as eligibility traces; after foundational mastery, introduce opponents/obstacles. The stored traces function as reusable sub-plans, creating an implicit curriculum.",
            "llm_model_used": null,
            "domain_environment": "robotic soccer with obstacle avoidance/navigation",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Two-stage progression from simple navigation/path-following to obstacle-rich multi-agent scenarios; sub-skills include path planning, ball acquisition, and obstacle avoidance.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared conceptually to staged/manual curricula and to symbolic decomposition approaches; no quantitative baselines given in this review.",
            "performance_metrics": "Described qualitatively as effective (multi-stage training yields competent navigation before opponents are introduced); no numeric metrics reported here.",
            "learning_speed_comparison": "Qualitatively faster convergence on foundational skills due to staged exposure; no episode counts or percentages provided.",
            "generalization_performance": "Stored traces create reusable behavior primitives that aid later tasks, but no generalization metrics reported.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Prerequisites are discovered/encoded via stored successful trajectories (eligibility traces) rather than manually specified.",
            "intermediate_task_generation": "Yes — obstacle-free mastery and stored traces act as intermediate tasks; reported as effective conceptually.",
            "llm_limitations_observed": null,
            "computational_cost": "Not quantified; approach emphasizes sample-efficient reuse of successful trajectories.",
            "human_expert_evaluation": "Not reported.",
            "key_findings_summary": "Staging training with eligibility traces that record successful low-complexity behaviors provides an implicit curriculum and reusable sub-plans, easing learning for later, harder tasks, though reported results in this review are qualitative.",
            "uuid": "e2033.3"
        },
        {
            "name_short": "BSN",
            "name_full": "Bayesian Strategy Networks",
            "brief_description": "A probabilistic graphical approach that decomposes a complex policy into conditional sub-policies, modeling dependencies (e.g., shoot policy conditioned on possession) to structure multi-agent strategy.",
            "citation_title": "Bayesian strategy networks based soft actor-critic learning",
            "mention_or_use": "mention",
            "curriculum_generation_method": "probabilistic decomposition of policy (model-based structuring)",
            "curriculum_method_description": "Curriculum arises from the BSN's factorization of the global policy into conditionally dependent sub-policies; training can focus on these simpler sub-policies and their conditional activation, yielding a structured learning progression but not an explicit staged curriculum.",
            "llm_model_used": null,
            "domain_environment": "robotic/multi-agent soccer",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Complex, conditional strategies where sub-policies become relevant only under certain conditions (e.g., possess_ball -&gt; consider shooting). Composition involves learning multiple sub-policies and conditional activation.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Compared conceptually to monolithic policies and hierarchical symbolic planners; no numeric comparisons in this review.",
            "performance_metrics": "Paper cites BSN as enabling decomposition and improved tractability; quantitative results not summarized in this review.",
            "learning_speed_comparison": "Implied improved tractability and learning efficiency over monolithic policies; no detailed numbers provided.",
            "generalization_performance": "Structured decomposition is argued to aid interpretability and reusability of sub-policies, but no held-out generalization metrics reported here.",
            "task_diversity_analysis": "Not provided.",
            "prerequisite_identification": "Dependencies between sub-policies explicitly modeled by BSN; serves as a form of prerequisite structure.",
            "intermediate_task_generation": "Not explicitly generation-based; decomposition yields modular tasks (sub-policies) that can act as intermediate training targets.",
            "llm_limitations_observed": null,
            "computational_cost": "Not reported here.",
            "human_expert_evaluation": "Not reported.",
            "key_findings_summary": "Probabilistic decomposition via BSNs structures policies into conditional sub-policies, easing learning and improving interpretability; presented as complementary to symbolic HTN approaches but without numeric comparisons in this review.",
            "uuid": "e2033.4"
        },
        {
            "name_short": "MCRL / Two-stage (Zhao et al.)",
            "name_full": "Mimic-to-Counteract (Two-Stage Reinforcement Learning)",
            "brief_description": "A two-stage curriculum for Google Research Football where agents first mimic expert behavior and then learn to counteract opponents, structuring training through distinct strategic stages.",
            "citation_title": "From Mimic to Counteract: a Two-Stage Reinforcement Learning Algorithm for Google Research Football",
            "mention_or_use": "mention",
            "curriculum_generation_method": "two-stage hand-designed curriculum (mimic expert then counteract)",
            "curriculum_method_description": "Stage 1: imitation/mimicry of expert trajectories to acquire baseline skills; Stage 2: reinforcement learning focused on counter-strategies against opponents. This staged approach enforces a curriculum of strategic difficulty.",
            "llm_model_used": null,
            "domain_environment": "simulated soccer (Google Research Football)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Long-horizon team-play tasks decomposed into imitation sub-skills (match play basics) followed by strategic adversarial skills; multi-step plays and counter-strategies required.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Described relative to end-to-end approaches and imitation-only baselines conceptually; detailed numeric comparisons not included in this review.",
            "performance_metrics": "Cited as a staged method to improve strategy learning in GFootball; this review does not provide numbers.",
            "learning_speed_comparison": "Qualitatively said to help bootstrap policy learning via imitation then improve via RL; no episodes or percent improvements reported here.",
            "generalization_performance": "Not reported in this paper's summary.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Prerequisites are enforced by stage ordering (must learn to imitate before counteracting).",
            "intermediate_task_generation": "Yes — imitation stage provides intermediate behavior priors that bridge to adversarial learning.",
            "llm_limitations_observed": null,
            "computational_cost": "Not specified in this review.",
            "human_expert_evaluation": "Not reported.",
            "key_findings_summary": "Two-stage mimic-then-counteract curricula can bootstrap strategic learning in football environments by leveraging demonstration priors then RL fine-tuning; the review reports conceptual benefits but no quantitative values.",
            "uuid": "e2033.5"
        },
        {
            "name_short": "LLM-generated curricula (prior work)",
            "name_full": "LLM-generated task decomposition and curricula (Voyager, SayCan examples)",
            "brief_description": "Use of large language models to decompose high-level goals into sequences of executable subgoals and to generate curricula or tasks (examples: SayCan and Voyager demonstrate language-driven decomposition and self-directed curriculum generation).",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "LLM-generated (prompt-driven zero-shot / self-generated task proposals)",
            "curriculum_method_description": "LLMs decompose natural-language goals into ordered subgoals via prompting or chain-of-thought; Voyager generates progressively harder self-directed tasks, and SayCan decomposes instructions into executable low-level actions grounded in affordances. The paper proposes extending this to dynamically configure environment task APIs and intrinsic rewards.",
            "llm_model_used": null,
            "domain_environment": "embodied agents (robotic manipulation, Minecraft, and proposed robotic soccer task scaffolding)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "High-level natural language goals decomposed into multi-step plans (e.g., 3–6 subgoals like dribble -&gt; pass -&gt; run -&gt; return pass), potentially re-planning mid-episode; complexity varies with requested play.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Contrasted conceptually with static HTN/HDDL files and manual curricula; proposed as more scalable/flexible but introducing new challenges (logical consistency, physical feasibility).",
            "performance_metrics": "No experimental performance metrics reported in this review for LLM-generated curricula in soccer; cited prior embodied works (Voyager, SayCan) demonstrate capability but numeric comparisons are not included here.",
            "learning_speed_comparison": "Proposed to improve scalability and reduce human engineering time; no empirical training speed or sample-efficiency numbers provided in this paper.",
            "generalization_performance": "Argued to support instruction-following and zero-shot composition of tasks, but no quantitative generalization results are reported in this review.",
            "task_diversity_analysis": "Not empirically measured here; LLMs are proposed to increase curriculum/task diversity via prompt-driven generation.",
            "prerequisite_identification": "LLMs produce ordered subgoals implying prerequisite structure; the paper notes concerns about ensuring logical/physical consistency of such generated prerequisites.",
            "intermediate_task_generation": "Yes — LLMs can generate intermediate subgoals and episode-specific intrinsic reward definitions on the fly; effectiveness is proposed but not empirically validated in this paper.",
            "llm_limitations_observed": "Discussed qualitatively: possible logical inconsistencies, physically infeasible plans, designer bias transfer, and grounding/symbol-grounding challenges when mapping symbolic subgoals to low-level state/action coordinates.",
            "computational_cost": "Not quantified; paper notes LLMs reduce human engineering cost but introduces compute for model inference and the need for verification/grounding pipelines.",
            "human_expert_evaluation": "Not performed in this paper; the authors suggest human-in-the-loop validation may be needed for LLM-generated scaffolds.",
            "key_findings_summary": "LLMs offer a promising path to dynamically generate scalable, language-driven curricula and task decompositions, potentially eliminating manual HDDL engineering; however, they introduce risks of inconsistent or infeasible plans and pose grounding challenges that require further work.",
            "uuid": "e2033.6"
        },
        {
            "name_short": "Advice Distillation (ADA)",
            "name_full": "ADA - Advice Distillation from Teacher to Student",
            "brief_description": "A teacher-student distillation framework where a teacher agent provides advice that structures the student's learning process, creating a social/hierarchical curriculum.",
            "citation_title": "Self-Attention Guided Advice Distillation in Multi-Agent Deep Reinforcement Learning",
            "mention_or_use": "mention",
            "curriculum_generation_method": "teacher-driven / distillation-based (social curriculum)",
            "curriculum_method_description": "Teacher agents produce advice/behaviors which are distilled into student agents via self-attention guided mechanisms; this creates a hierarchical social curriculum where student learning is scaffolded by teacher-provided intermediate signals.",
            "llm_model_used": null,
            "domain_environment": "multi-agent soccer / strategic team play",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Supports learning of coordinated high-level strategies through teacher-produced guidance; complexity arises from multi-agent coordination and temporal dependencies.",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Discussed relative to pure RL training without teacher advice and to other distillation approaches; quantitative comparisons not provided in this review.",
            "performance_metrics": "Not provided in this position paper; original ADA paper contains experimental details.",
            "learning_speed_comparison": "Suggested to speed up student learning via distilled advice, but no numbers reported here.",
            "generalization_performance": "Not quantified in this review.",
            "task_diversity_analysis": "Not reported.",
            "prerequisite_identification": "Teacher advice implicitly encodes prerequisite behaviors, but no explicit analysis reported here.",
            "intermediate_task_generation": "Teacher advice functions as intermediate guidance; effectiveness asserted conceptually.",
            "llm_limitations_observed": null,
            "computational_cost": "Not discussed in this review.",
            "human_expert_evaluation": "Not reported here.",
            "key_findings_summary": "Teacher-student distillation provides a social form of hierarchical curriculum that can structure student learning and accelerate acquisition of coordinated behaviors; specifics require consulting the ADA paper for quantitative outcomes.",
            "uuid": "e2033.7"
        },
        {
            "name_short": "This paper's proposal: Scaffolded Environments + LLM planners",
            "name_full": "Structured Task Environments with Procedural Curriculum Generation via LLMs (proposed here)",
            "brief_description": "Proposal to design environments that expose hierarchical task APIs, layered action spaces, intrinsic subgoal rewards, and procedural curriculum generation driven by LLM planners to produce dynamic, compositional curricula for embodied agents.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "curriculum_generation_method": "proposed: LLM-driven procedural curriculum generation + environment-managed intrinsic rewards",
            "curriculum_method_description": "Environments would offer HDDL-like hierarchical task APIs and intrinsic reward hooks; an LLM (prompted with high-level natural language goals) would synthesize ordered symbolic subgoals and configure per-episode success conditions and intrinsic rewards, enabling procedurally generated curricula that grow in complexity as agents master sub-tasks.",
            "llm_model_used": null,
            "domain_environment": "proposed primarily for robotic soccer but intended to generalize to other embodied multi-agent domains",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Targets long-horizon, multi-agent, compositional tasks requiring sequences of coordinated sub-skills (passing, positioning, dribbling, coordinated runs), potentially multi-layered with re-planning mid-episode.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Proposed comparison axes include flat physics simulators, manual staged curricula (TiZero/MARLadona), HTN/HDDL-based planners, and LLM-generated dynamic curricula; no empirical comparisons in this paper.",
            "performance_metrics": "No empirical metrics—proposal includes suggested evaluation metrics for future work: Compositional Generalization Score, Curriculum Efficiency (sample complexity reduction), and Scaffolding Brittleness Index, but no numeric results are reported.",
            "learning_speed_comparison": "Hypothesized to reduce sample complexity and training time versus flat end-to-end approaches and manual curricula, but no empirical evidence provided in this position paper.",
            "generalization_performance": "Hypothesized improvement in compositional generalization due to reusable sub-policies and hierarchical structure, but no experimental results reported.",
            "task_diversity_analysis": "Proposes that LLM-driven procedural generation would increase task diversity, but no analyses or measurements are provided yet.",
            "prerequisite_identification": "LLM-generated plans specify subgoal ordering (prerequisites); paper also suggests studying methods for agents to autonomously discover hierarchies (e.g., bottleneck states), but provides no evaluations.",
            "intermediate_task_generation": "Core feature: environments should procedurally create intermediate sub-tasks and intrinsic rewards via LLM planning; proposed effective but untested in this paper.",
            "llm_limitations_observed": "Paper cautions about potential LLM failures: logical inconsistency, physically infeasible plans, designer bias transfer, and symbol-grounding difficulties when mapping subgoals to continuous states.",
            "computational_cost": "Not empirically measured; paper argues LLM approach reduces human engineering cost but requires model inference and verification pipelines—computational tradeoffs not quantified.",
            "human_expert_evaluation": "Not performed (proposal stage); authors note human oversight may be needed for validation of generated scaffolds.",
            "key_findings_summary": "Argues that embedding hierarchical scaffolds into environments and using LLMs as dynamic task planners can produce scalable, compositional curricula that improve sample efficiency and enable compositional generalization; highlights grounding, engineering-burden tradeoffs, and potential LLM failure modes as primary challenges.",
            "uuid": "e2033.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies",
            "rating": 2
        },
        {
            "paper_title": "MARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments",
            "rating": 2
        },
        {
            "paper_title": "Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy",
            "rating": 2
        },
        {
            "paper_title": "Bayesian strategy networks based soft actor-critic learning",
            "rating": 2
        },
        {
            "paper_title": "From Mimic to Counteract: a Two-Stage Reinforcement Learning Algorithm for Google Research Football",
            "rating": 2
        },
        {
            "paper_title": "Self-Attention Guided Advice Distillation in Multi-Agent Deep Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "rating": 2
        },
        {
            "paper_title": "Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning",
            "rating": 1
        }
    ],
    "cost": 0.01666125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer
29 Sep 2025</p>
<p>Brennen Hill bahill4@wisc.edu 
Department of Computer Science
University of Wisconsin-Madison Madison
53706WI</p>
<p>Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer
29 Sep 2025DF10DAD44701DEB27702DE217B7EC693arXiv:2509.04731v2[cs.AI]
Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models.However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-toend approach often fails due to intractable exploration spaces and sparse rewards.This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding.We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics.Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL).These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning.We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play.We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks.By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.Introduction: redefining scale for embodied agentsThe paradigm of scaling has demonstrably driven profound advancements in artificial intelligence.In the domain of large language models (LLMs), scaling model parameters, dataset size, and computational resources has unlocked emergent capabilities in reasoning, generation, and comprehension[Brown et al., 2020].A parallel trend is now underway in the development of intelligent agents, where the focus is shifting towards scaling interaction.Environments are not passive testbeds, but are themselves a crucial dimension of scale.They provide the rich, dynamic data from which agents learn adaptive behaviors, planning, and long-term decision-making.To advance world models beyond passive prediction and toward active, goal-driven interaction, hinges on the quality and structure of this interaction data.However, as we push agents towards more complex, autonomous, and general-purpose capabilities, a critical bottleneck emerges, particularly in multi-agent systems, which consistently face challenges of non-stationarity and decentralized decision-making[Ning and Xie, 2024].In domains characterizedIn the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Embodied World Models for Decision Making</p>
<p>by long horizons, combinatorial action spaces, and the need for intricate coordination, such as robotic soccer, the brute-force scaling of interaction data via end-to-end reinforcement learning often proves insufficient.Robotic soccer, as a canonical case study for embodied intelligence, encapsulates these challenges, forcing researchers to develop robust solutions for real-time, strategic cooperation under uncertainty [Sammut, 2010].The challenge is not merely one of reward sparsity, but of task sparsity: the sequence of coordinated actions required to achieve a meaningful outcome (e.g., a goal) is so specific and temporally extended that it is almost impossible to discover through random exploration.An agent will not stumble upon a multi-step passing play by chance, regardless of the scale of the simulation.While we ground our analysis in robotic soccer, this challenge of task sparsity is endemic to many complex domains, from collaborative robotic assembly and supply chain logistics to automated scientific discovery and autonomous driving.This position paper argues for a paradigm shift in how we approach the design of world models and their corresponding environments.We contend that the next crucial frontier is not the scaling of fidelity, size, or agent populations, but the scaling of structural complexity within the environment itself.An embodied world model must capture more than just physical dynamics; it must represent the semantic structure of tasks.We propose that by embedding explicit hierarchical scaffolding into the design of our environments, we can create a structured learning landscape that guides agents toward sophisticated, strategic behavior far more efficiently than flat simulation environments.This scaffolding involves decomposing overarching tasks into a hierarchy of sub-tasks and subgoals, creating an intrinsic curriculum that makes the learning problem tractable.</p>
<p>To substantiate this position, we conduct a focused, systematic review of the 2024 literature on multi-agent robotic soccer, a canonical challenge problem for multi-agent coordination.Our analysis reveals a clear and compelling trend: the most promising recent approaches are moving away from purely end-to-end MARL and towards hybrid models that integrate symbolic planners and hierarchical reasoning.This paper will demonstrate how these methods implicitly or explicitly leverage task decomposition to overcome the limitations of traditional MARL.We will argue that these successes are not isolated algorithmic tricks but rather evidence of a fundamental principle: to learn complex strategies, agents require world models and environments that are structured to teach them.</p>
<p>Related work</p>
<p>Our position builds upon and seeks to synthesize three established pillars of AI research: Hierarchical Reinforcement Learning (HRL), Curriculum Learning, and the emerging use of LLMs as agents and planners.Our contribution is to advocate for externalizing these principles into the environment's design, creating a more powerful and generalizable learning paradigm.</p>
<p>Hierarchical reinforcement learning (HRL) directly addresses the challenge of long-horizon tasks by imposing a hierarchical structure on an agent's policy.The seminal options framework formalized the concept of temporally extended actions, where a high-level meta-policy selects among a set of options (sub-policies) that execute for multiple timesteps to achieve specific subgoals [Sutton et al., 1999].This temporal abstraction allows agents to reason and plan at a higher level, making credit assignment more tractable and exploration more efficient.This is a form of an internal, learned world model of task structure.Our proposal aims to externalize this hierarchy, embedding the task structure into the environment itself to explicitly guide the learning of such multi-level policies, thereby simplifying the learning problem for the agent.</p>
<p>Curriculum learning is founded on the observation that humans and animals learn more effectively when presented with concepts in a structured, progressive order of increasing difficulty.This principle was formalized for machine learning by Bengio et al. [2009], demonstrating that training models on a carefully designed curriculum of examples can significantly improve generalization and convergence speed.Many MARL approaches for complex games like soccer employ manually designed curricula, progressing from 1v1 to full team play [Baghi, 2024].We argue that a hierarchically scaffolded environment can generate a curriculum automatically and more dynamically, as an emergent property of the task's compositional structure, moving beyond static, manually-defined stages.</p>
<p>LLMs as agents and planners.Recent work has demonstrated the remarkable potential of LLMs to function as the reasoning core for autonomous agents.This directly aligns with leveraging world knowledge from LLMs.Landmark studies like SayCan have shown that LLMs can decompose high-level natural language instructions into sequences of executable, low-level actions for robotic manipulators, grounding language in physical affordances [Ahn et al., 2022].Similarly, Voyager demonstrated an LLM-based agent capable of open-ended exploration and self-directed skill discovery in complex environments like Minecraft, where the LLM itself generates the curriculum by proposing progressively harder tasks [Wang et al., 2023].Our proposal leverages this capability, positioning LLMs not just as an agent's brain but as a dynamic, zero-shot environment structurer, a generative world model of tasks that translates high-level goals into the concrete task scaffolding needed for learning.</p>
<p>3 The core challenge: from reward sparsity to task decomposition</p>
<p>The term "sparse rewards" has become a ubiquitous explanation for the difficulties encountered in training agents for long-horizon tasks.In robotic soccer, the ultimate reward, scoring a goal, is an infrequent event, making it a challenging signal for credit assignment across a long sequence of actions.While techniques like reward shaping, which provide denser, more localized feedback for actions like moving towards the ball or possessing it, can be effective [Li et al., 2024b, Baghi, 2024], they often treat the symptoms rather than the underlying disease.The foundational problem is not simply that the goal is a rare event; it is that the path to that goal requires a long, temporally extended, and specific sequence of coordinated sub-tasks.We posit that the core challenge is one of task decomposition.</p>
<p>Task sparsity in multi-agent coordination</p>
<p>Consider the sequence of actions required for a successful offensive play in 3v3 soccer: Agent 1 must first gain possession of the ball.It must then assess the positions of its teammates and opponents.It might then decide to dribble past an immediate defender.Subsequently, it must execute a pass to Agent 2, who is moving into an open position.Agent 2 must receive the pass, orient towards the goal, and perhaps take a shot.Each of these steps is a subgoal, and failure at any point breaks the chain.</p>
<p>An end-to-end MARL agent, operating with a flat action space and a single terminal reward, faces a combinatorial explosion.If each of N agents has an action space of size |A|, the joint action space at each timestep is |A| N .Over a horizon of T steps, the number of possible trajectories is astronomical.The probability of a random or semi-random exploration policy discovering this entire successful sequence is infinitesimally small.This is the problem of task sparsity: the density of successful, goal-achieving trajectories in the vast space of all possible trajectories is effectively zero.Simply providing a denser reward for getting closer to the goal does not teach the agent the crucial concepts of passing, creating space, or defending, the abstract building blocks of strategy.</p>
<p>The inefficiency of brute-force scaling</p>
<p>The prevailing approach in machine learning has been to overcome such challenges with scale.However, the nature of multi-agent coordination makes this particularly inefficient.As noted in the analysis of a modified TiZero framework, MARL methods can learn meaningful strategies but still struggle with sample inefficiency and slow learning progress [Baghi, 2024].The computational cost scales exponentially with the number of agents and the complexity of the required coordination.</p>
<p>Furthermore, a focus on extreme computational scale can make research inaccessible and sideline promising alternative paradigms.Our methodological review of the 2024 soccer literature was intentionally focused on low-resource approaches that control agents at a high level (e.g., target positions, kick actions) rather than low-level joint angles.This decision was informed by the observation that some of the most complex end-to-end systems, such as those that learn directly from egocentric vision [Tirumala et al., 2024], require a scale of resources that places them beyond the reach of most academic labs.These represent a brute-force approach to a problem that may be better solved with structural priors.The goal should be to find more efficient paths to intelligence, not just the most computationally expensive ones.</p>
<p>Methodology of review</p>
<p>To ground our position in empirical evidence, we conducted a systematic survey of academic papers published in 2024 that addressed multi-agent robotic soccer.Our search criteria targeted papers in reputable journals and conference proceedings containing the terms "multi-agent" and "soccer."We applied several filters to align the review with the problem of strategic, low-resource learning:</p>
<ol>
<li>Domain Focus: We excluded papers where soccer was merely one of several benchmark environments for a general MARL algorithm, focusing instead on work where soccer was the primary domain of investigation.This focus on a competition-driven domain is deliberate; major robotics competitions are consistently identified as primary drivers of innovation, providing standardized benchmarks and forcing researchers to integrate solutions into effective, holistic systems [Brancalião et al., 2022, Sammut, 2010].2. Abstraction Level: We excluded papers that focused on low-level control, such as learning individual robot joint angles for walking or kicking, to concentrate on the strategic and tactical layers of decision-making relevant to long-horizon planning.3. Resource Accessibility: We prioritized methods that demonstrated effectiveness without relying on massive, proprietary computational infrastructure, aiming to identify scalable principles rather than just scalable implementations.</li>
</ol>
<p>This curated selection of recent work forms the basis for the analysis presented in the next section, revealing a consistent pattern of leveraging hierarchy and abstraction to tackle the challenge of task decomposition.</p>
<p>4 Evidence from the field: the rise of hierarchical and symbolic methods</p>
<p>Our review of the 2024 literature uncovers a significant trend: a move away from monolithic, endto-end MARL and towards hybrid architectures that explicitly integrate hierarchical and symbolic reasoning.These approaches directly confront the problem of task decomposition, providing a structured framework that guides learning and makes complex, long-horizon coordination tractable.A summary of the most relevant papers is presented in Table 1.</p>
<p>Explicit task decomposition with symbolic planners</p>
<p>The most compelling evidence for our position comes from the work of Mu et al. [2024], who introduce the HS-MARL framework.This work stands as a direct confirmation that task decomposition is the central challenge.Instead of relying on RL to discover high-level strategy from scratch, HS-MARL integrates a classical AI planner, a Hierarchical Task Network (HTN) planner called pyHIPOP+, into the MARL loop.</p>
<p>The core idea is to use the HTN planner to decompose the overarching goal (e.g., score a goal) into a logically sound sequence of executable subgoals.This plan is generated using a symbolic representation of the world state and a domain description written in the Hierarchical Domain Definition Language (HDDL).The framework constructs a hierarchical state-space tree where subgoals are arranged based on their logical and temporal proximity to the final goal.For instance, the goal ScoreGoal might be decomposed into the subtasks AcquireBall, MoveToShootingPosition, and ExecuteShot.A meta-controller then uses this symbolic plan to guide the low-level MARL agents.It assigns symbolic options, policies trained to achieve specific subgoals, to the agents.This architecture directly addresses task sparsity by providing a high-level scaffold for the agents' behavior.The RL component is no longer responsible for discovering the entire strategic sequence but is instead focused on learning how to execute the sub-tasks prescribed by the planner.The authors explicitly note that this integration is designed to tackle complex environments with sparse rewards, demonstrating significantly improved performance and sample efficiency.</p>
<p>This trend is reinforced by related work from some of the same authors on Bayesian Strategy Networks (BSN), which aims to separate an intricate policy into several simple sub-policies and organize their relationships [Yang and Parasuraman, 2024].A BSN models the conditional dependencies between sub-policies (e.g., a shoot sub-policy is only relevant if the "possess ball" condition is met), effectively decomposing the global policy into a structured, interpretable graph.While distinct in its probabilistic approach, the underlying philosophy is identical: complex behavior emerges from the composition of simpler, learned components.</p>
<p>Hierarchical structure as an implicit curriculum</p>
<p>A key benefit of embedding hierarchical structure is the natural emergence of a curriculum, which guides agent learning from simple to complex behaviors.Manually designing such curricula is a known effective technique, but it is labor-intensive and difficult to scale.For example, the TiZero framework for soccer relies on a curriculum of manually designed scenarios of increasing difficulty [Baghi, 2024].An agent must achieve a certain win rate in a simple 1v1 scenario before progressing to a more complex 2v2 scenario, and so on.</p>
<p>The hierarchical scaffolding approach automates this process.In HS-MARL, the state-space tree generated by the HTN planner forms a natural curriculum.Agents first learn policies for subgoals at the leaves of the decomposition tree (e.g., move_to_ball), as these are prerequisites for higher-level subgoals (e.g., pass_to_teammate).The structure of the task itself dictates the learning progression, making the curriculum an emergent property of the environment's hierarchical design rather than a manually engineered artifact.</p>
<p>A different, yet philosophically similar, approach to emergent curricula can be seen in the work of Azarkasb and Khasteh [2024].Their method uses eligibility traces to build a knowledge base of successful trajectories.Critically, their training process is multi-staged: first, the agent learns optimal paths to the ball and goal in an obstacle-free environment.Only after mastering this foundational skill are obstacles (i.e., opponent robots) introduced.This two-stage process constitutes a simple but effective curriculum that emerges directly from the incremental layering of task complexity.The stored eligibility traces function as a learned library of successful sub-plans, akin to the symbolic options in HS-MARL.</p>
<p>Abstraction for efficient and interpretable learning</p>
<p>A third major advantage of the hierarchical approach is that it facilitates learning in a more abstract, and therefore more tractable, state-action space.By decomposing the problem, we can define high-level actions and representations that hide irrelevant low-level details, drastically reducing the exploration space for the learning algorithm.</p>
<p>The HS-MARL framework exemplifies this principle.The meta-controller operates in the symbolic space of subgoals and options, while the low-level agents execute these options in the continuous state space.This separation of concerns allows the strategic layer (the planner and meta-controller) to reason over long horizons without getting bogged down in the minutiae of motor control.As Mu et al. [2024] note, a key benefit of this architecture is enhanced interpretability.One can inspect the symbolic plan generated by the HTN to understand the agent team's high-level strategy, a task that is nearly impossible with a monolithic neural network policy.</p>
<p>This move towards abstraction is a field-wide trend.Our decision to exclude joint-level control from our review reflects this shift.Modern MARL frameworks for soccer, like MARLadona [Li et al., 2024b], operate with abstract actions like move, turn, and kick, rather than controlling individual motor torques.Similarly, methods focused on high-level strategy, such as advice distillation from a teacher agent [Li et al., 2024a] or policy distillation for countering opponents [Zhao et al., 2024], presuppose an abstract, strategic layer of interaction.These methods work because they operate on a representation of the problem that is already simplified and structured.Hierarchical scaffolding is the logical next step: making that structure an explicit and dynamic component of the learning process itself.</p>
<p>A new paradigm for environment design: principles and implications</p>
<p>The evidence points towards a need to fundamentally rethink how we design environments for agent research.The dominant paradigm of creating high-fidelity, flat simulators and challenging agents to learn from scratch is fundamentally inefficient for teaching strategy.We must shift our focus to designing world models and environments that are themselves structured for learning.</p>
<p>From flat physics simulators to structured task environments</p>
<p>Many current multi-agent benchmarks, such as Google Research Football [Kurach et al., 2020] or the Isaac Gym-based environment used by MARLadona [Li et al., 2024b], are best described as sophisticated physics simulators.They provide the laws of the world and a sparse terminal goal, leaving the entire burden of task decomposition and strategy discovery to the learning algorithm.</p>
<p>While valuable for testing raw learning capabilities, their world models are incomplete for decisionmaking because they lack a model of task semantics.</p>
<p>We propose that the next generation of benchmarks should be architected as structured task environments.This means moving beyond physics and providing explicit support for hierarchical task definition and decomposition as a core feature of the environment's world model itself.The environment should not just be a place where an agent acts, but a place where an agent learns structure.</p>
<p>Key features of a scaffolded environment</p>
<p>A scaffolded environment would possess several key features that distinguish it from a traditional simulator:</p>
<p>1.A Hierarchical Task API: The environment would expose an API for defining tasks and sub-tasks, along with their dependencies and termination conditions.This would be analogous to the HDDL domains used by Mu et al. [2024], but integrated directly into the environment's state management.For instance, a researcher could define a PassingPlay task composed of the sub-tasks passer_moves_to_ball, receiver_moves_to_open_space, and passer_kicks_to_receiver.</p>
<ol>
<li>Layered Action Spaces: Agents could interact with the environment at multiple levels of abstraction, from low-level continuous commands (e.g., set_velocity(x, y)) to high-level symbolic options (e.g., execute_option(pass_to_teammate_3)).This directly supports the development of hierarchical policy architectures.</li>
</ol>
<p>Intrinsic Rewards for Subgoals:</p>
<p>The environment itself would manage the reward signals associated with the task hierarchy.Upon successful completion of a defined subgoal, the environment would issue an intrinsic reward to the relevant agent(s).This codifies reward shaping directly into the task definition, providing dense, meaningful learning signals that are perfectly aligned with the task's compositional structure.</p>
<ol>
<li>
<p>Procedural Curriculum Generation: With a compositional task definition system, the environment could procedurally generate curricula, starting with foundational sub-tasks and gradually combining them into more complex, longer-horizon challenges as agents demonstrate mastery.</p>
</li>
<li>
<p>Built-in Support for Compositional Evaluation: Such environments necessitate new evaluation metrics beyond simple win rates.We propose:</p>
</li>
</ol>
<p>• Compositional Generalization Score: Measures an agent's zero-shot or few-shot performance on a novel, complex task composed of sub-tasks it has already mastered individually.• Curriculum Efficiency: Quantifies the reduction in sample complexity (1/N samples ) or wall-clock time required to reach a target performance level in the scaffolded environment compared to a flat baseline.• Scaffolding Brittleness Index: Assesses the performance degradation when the provided task hierarchy is intentionally made sub-optimal.This measures an agent's robustness and ability to learn despite an imperfect scaffold.</p>
<p>The role of large language models as dynamic planners</p>
<p>While the HTN-based approach in HS-MARL provides a powerful proof-of-concept, its reliance on manually crafted, domain-specific HDDL files represents a significant engineering bottleneck.</p>
<p>Recent work has already demonstrated the power of LLMs to decompose high-level instructions into executable plans for embodied agents [Ahn et al., 2022, Wang et al., 2023].We posit that this principle can be adapted to dynamically structure the learning environment itself, transforming static scaffolding into a flexible, language-driven curriculum.The LLM becomes a generative world model of tasks.</p>
<p>In this paradigm, an environment would not have a fixed task hierarchy.Instead, a high-level goal would be specified in natural language (e.g., "Execute a give-and-go play with the forward on the right wing" .This sequence would be passed to the environment's task API, dynamically configuring the intrinsic reward function and success conditions for that specific episode.This approach offers several transformative advantages:</p>
<p>• Scalability and Flexibility: It eliminates the need for human experts to write complex domain description files for every conceivable strategy.New tasks and curricula can be generated simply by writing new prompts.</p>
<p>• Instruction-Following Agents: It provides a natural framework for training agents that can follow complex, high-level instructions, a cornerstone of general-purpose agency and a key topic in VLA models.</p>
<p>• Dynamic Adaptation: The LLM planner could potentially re-plan mid-episode based on new state information, providing a dynamic scaffold that adapts to the unfolding situation on the field.This directly addresses a key limitation of rigid, pre-computed HTN plans.</p>
<p>Implications for agent architectures and Sim2Real transfer</p>
<p>Shifting the focus to scaffolded environments would catalyze a corresponding shift in agent architectures and accelerate research in several key areas.Architectures with high-level meta-policies that select among low-level sub-policies, as formalized in the options framework [Sutton et al., 1999], would be a natural fit.This also provides a powerful grounding for hybrid, neuro-symbolic agents that combine neural pattern recognition with logical reasoning.Furthermore, an agent that has learned a policy for a "move to open space" sub-task could more easily reuse that skill when it appears as part of a new, more complex task, making scaffolded environments ideal testbeds for studying generalization through composition.</p>
<p>Crucially, this paradigm offers a more tractable path for Sim2Real transfer.Hierarchical policies are inherently better suited for transfer because the domain shift is often concentrated at the lowest levels.</p>
<p>The high-level strategic layer (e.g., deciding when to pass) is often robust to the differences between simulation and reality, while the reality gap primarily affects the low-level motor policies (e.g., the precise joint torques needed to execute the pass) [Kober et al., 2013].This decomposition allows researchers to focus domain adaptation efforts on a smaller set of well-defined sub-tasks, making the Sim2Real problem more tractable for embodied agents.</p>
<p>Challenges and future directions</p>
<p>While promising, the paradigm of scaffolded environments presents its own set of research challenges that must be addressed.</p>
<p>The scaffolding design problem</p>
<p>A primary challenge is the origin of the hierarchical task structure itself.If the scaffolds must be meticulously hand-crafted by human experts, we have merely shifted the burden from the learning algorithm to the environment designer.This creates a bottleneck and may imbue the agent with the designer's own biases.The aforementioned use of LLMs as zero-shot planners is a promising mitigation strategy, but it introduces its own challenges, such as ensuring the logical consistency and physical feasibility of LLM-generated plans.Future work could explore methods for agents to learn the task hierarchy itself, perhaps by identifying bottleneck states in their exploration or by abstracting common sub-trajectories into reusable skills.</p>
<p>The risk of over-constraining discovery</p>
<p>A critical question is whether providing explicit structure could stifle an agent's creativity.A scaffold guides an agent along known solution pathways, but it may discourage the discovery of novel, superhuman strategies that lie outside the pre-defined hierarchy.This represents a fundamental trade-off between sample efficiency and open-ended exploration.Environments of the future may need to manage this trade-off explicitly, perhaps by employing curriculum strategies that begin with a rigid scaffold and gradually relax constraints as the agent's competency grows, a process known as fading.</p>
<p>Another approach could involve using stochastic planners that introduce structured variation into the task decomposition.</p>
<p>The grounding problem</p>
<p>The effective use of symbolic subgoals (whether from an HTN or an LLM) depends on the ability to connect them to the low-level, sub-symbolic state of the environment.This is the classic symbol grounding problem [Harnad, 1990].For example, how does an agent robustly translate the symbolic subgoal "move to open space" into a concrete target (x, y) coordinate in a dynamic environment with moving teammates and opponents?Solving this requires robust perception models that can map between the continuous state space (e.g., from vision) and a discrete, symbolic representation.Developing such models that are both accurate and learnable is a central challenge for neuro-symbolic agent architectures and is directly related to VLA models that must align language with perception and action.</p>
<p>Conclusion</p>
<p>The pursuit of more capable and autonomous agents has led the research community to recognize the critical role of environments in shaping intelligence.The current emphasis on scaling the fidelity and size of these environments, while valuable, overlooks a more crucial dimension: structural complexity.</p>
<p>In complex multi-agent domains, the primary obstacle to learning is not merely the sparsity of rewards but the sparsity of coherent, long-horizon tasks.An embodied world model for decision-making must therefore model not just the physics of the world, but also the structure of tasks within it.</p>
<p>This paper has argued that the most effective path forward is to build environments with explicit hierarchical scaffolding.Through a systematic review of the 2024 literature in multi-agent soccer, we have shown a clear trend towards hybrid systems that integrate symbolic planning and hierarchical decomposition with MARL.These methods succeed precisely because they provide the structural priors necessary to make the problem of strategic coordination tractable.</p>
<p>We have extended this position by proposing a new class of structured task environments that treat hierarchical decomposition as a first-class feature, and have argued that Large Language Models can serve as dynamic, generative world models of tasks to create this structure on the fly.This paradigm directly enables research into long-horizon planning, sim-to-real transfer, and language-guided decision making.To scale agents to complex, strategic domains, we must first scale the environments to support complex, strategic learning.Our position is a call to action for the community of environment builders.We must move beyond creating mere physics sandboxes and begin engineering structured learning curricula directly into our benchmarks.Only by scaffolding the world can we expect our agents to build the towering capabilities of goal-directed, embodied intelligence we envision.</p>
<p>Table 1 :
1
Summary of key 2024 multi-agent soccer literature demonstrating a trend towards hierarchical and structured approaches.
PaperCore MethodKey ContributionRelation to Hierarchy/StructureLi et al. [2024b]MARLadona (MARL)Dense rewards + curriculum for team play.Explicit, hand-crafted curriculum (1v1, 2v2).Reward shaping provides implicit goal structure.Baghi [2024]Modified TiZero (MARL)Explores intrinsic rewards in a curriculum.Heavily relies on a multi-stage, explicit curriculum.Mu et al. [2024]HS-MARLIntegrates HTN planner with MARL.Explicit hierarchical task decomposition via symbolic planner.Meta-controller assigns subgoals to agents.Symbolic options create a structured, high-level action space.Azarkasb and Khasteh [2024] Eligibility TracesLearns a knowledge base of successful paths.Implicit structural memory via traces; multi-stage training curriculum.Li et al. [2024a]ADA (Advice Distillation)Teacher agent provides advice to student.Social hierarchy (teacher-student) structures learning process.
Zhao et al. [2024]MCRL (Mimic-to-Counteract) Two-stage curriculum: mimic expert, then counteract.Structures training through distinct, high-level strategic goals.Yang and Parasuraman [2024] BSN (Bayesian Strategy Nets) Decomposes policy into sub-policies using a BSN.Explicit policy decomposition into simpler, coordinated components.</p>
<p>).An integrated LLM would then perform common-sense reasoning to decompose this instruction into a plausible, ordered sequence of symbolic subgoals: [1.Player A dribbles towards Player B], [2.Player A passes to Player B], [3.Player A runs into open space past defender], [4.Player B returns pass to Player A]</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Nicolas Sermanet, Clayton Sievers, Alexander Tan, Vincent Toshev, Fei Vanhoucke, Ted Xia, Peng Xiao, Sichun Xu, Mengyuan Xu, Andy Yan, Zeng, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy. Seyed Omid, Azarkasb , Seyed Hossein, Khasteh , Journal of Intelligent &amp; Robotic Systems. 10912024</p>
<p>Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments. Amir Masoud, Baghi , 2024KTH Royal Institute of TechnologyPhD thesis</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Systematic Mapping Literature Review of Mobile Robotics Competitions. Laiany Brancalião, José Gonçalves, Miguel Á Conde, Paulo Costa, 10.3390/s22062160Sensors. 2262160mar 2022</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Ryder, Advances in neural information processing systems. 202033</p>
<p>The symbol grounding problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>Reinforcement learning in robotics: A survey. Jens Kober, Andrew Bagnell, Jan Peters, The International Journal of Robotics Research. 32112013</p>
<p>Google research football: A novel reinforcement learning environment. Karol Kurach, Anton Raichuk, Piotr Stańczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Self-Attention Guided Advice Distillation in Multi-Agent Deep Reinforcement Learning. Yang Li, Sihan Zhou, Yaqing Hou, Liran Zhou, Hongwei Ge, Liang Feng, IEEE Transactions on Neural Networks and Learning Systems. 2024a</p>
<p>. Zichong Li, Filip Bjelonic, Victor Klemm, Marco Hutter, arXiv:2401.123452024bMARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning. arXiv preprint</p>
<p>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies. Xuechen Mu, Hankz Hankui Zhuo, Chen Chen, Kai Zhang, Chao Yu, Jianye Hao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>A Survey on Multi-Agent Reinforcement Learning and its Application. Zepeng Ning, Lihua Xie, Engineering Applications of Artificial Intelligence. 1271073352024</p>
<p>Robot soccer. Claude Sammut, Wiley Interdisciplinary Reviews: Cognitive Science. 162010</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 1121-21999</p>
<p>Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning. Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess, Robotics: Science and Systems. 2024</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, arXiv:2305.162912023arXiv preprint</p>
<p>Bayesian strategy networks based soft actor-critic learning. Qin Yang, Ramviyas Parasuraman, 10.1145/3643862ACM Trans. Intell. Syst. Technol. 15mar 2024</p>
<p>Junjie Zhao, Jiangwen Lin, Xinyan Zhang, Yuanbai Li, Xianzhong Zhou, Yuxiang Sun, From Mimic to Counteract: a Two-Stage Reinforcement Learning Algorithm for Google Research Football. International Conference on Autonomous Agents and Multiagent Systems (AAMAS). 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3225 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3225</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3225</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-448e1493034dafe35699ae054ff4480b31dcf64a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/448e1493034dafe35699ae054ff4480b31dcf64a" target="_blank">Memory-assisted prompt editing to improve GPT-3 after deployment</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work pair GPT -3 with a growing memory of recorded cases where the model misunderstood the user’s intents, along with user feedback for clarification, which allows the system to produce enhanced prompts for any new query based on the user feedback on similar cases in the past.</p>
                <p><strong>Paper Abstract:</strong> Large LMs such as GPT -3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT -3 would mistakenly interpret "What word is similar to good ?" to mean a homonym, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the sys-tem but without retraining, which will be pro-hibitively costly. We pair GPT -3 with a growing memory of recorded cases where the model misunderstood the user’s intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT -3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT -3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. 1</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3225.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3225.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemPrompt: Memory-assisted Prompt Editing with User Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that pairs GPT-3 (davinci) with a dynamic external memory of (input question -> user corrective feedback) pairs; retrieved feedback is appended to the few-shot prompt to correct model misunderstandings without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>MemPrompt is an architecture that uses GPT-3-175B (davinci) in a few-shot prompting setup where the model is prompted to output both an answer y and a verbalized task understanding u; a growing external key-value memory M stores keys x (questions) and values fb (user corrective feedback). On a new query, MemPrompt retrieves relevant fb (via a retriever / transformation pipeline), concatenates the fb to the input, and includes in-context examples that teach GPT-3 to react to fb, thereby editing the prompt at inference time rather than retraining model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external key-value feedback memory (prompt-augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory M stores (x_i -> fb_i) pairs whenever a user supplies corrective feedback; retrieval is done by transforming the new input (optionally via GUD-IR) and nearest-neighbor lookup over stored fb entries using sentence-transformer embeddings with a similarity threshold; retrieved fb is concatenated to the input (combiner C uses a similarity threshold gating) and the augmented input is fed to GPT-3 together with few-shot examples that include (x, fb -> u, y) patterns so the model learns to incorporate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lexical relations; Word scrambling; Ethical reasoning (ERT-CAT, ERT-NL); WEBQA factual QA; Personalization (language/dialect)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A diverse set of tasks where the primary challenge is model misunderstanding of user intent or of nuanced semantics: (1) lexical relations (synonym/antonym/homophone/definition/sentence usage), (2) word-scrambling/unscrambling (anagrams, cycles, punctuation removal), (3) ethical judgment requiring identifying moral category or rule-of-thumb (ERT-CAT categorical feedback, ERT-NL natural-language feedback), (4) factual QA (WEBQA) with label feedback experiments, and (5) personalization to dialect/language phrases.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / instruction-following / ethical reasoning / personalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Lexical QA overall accuracy: 0.98 (98%); Word-scramble overall accuracy: 0.90 (90%); Ethical reasoning (ERT-CAT and ERT-NL): MemPrompt achieves >25% relative improvement versus NO-MEM; for ERT-NL retriever variant: retrieval/related-accuracy improved to 45.2% when using GUD-IR (vs 38.5% with a sentence-transformer retriever). WEBQA: reported as effective with label feedback (no numeric tabulated result in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>NO-MEM (GPT-3 few-shot) baseline: Lexical QA overall accuracy 0.37 (37%); Word-scramble overall accuracy 0.77 (77%); ERT baseline numeric values not fully tabulated in main text but MemPrompt reported >25% relative improvement over NO-MEM.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Appending retrieved user corrective feedback to the input prompt (MemPrompt) substantially increases GPT-3's instruction-understanding and answer accuracy over time without retraining; MemPrompt outperforms a no-memory GPT-3 baseline and a non-selective growing-prompt baseline on lexical and many word-scrambling tasks; retrieval quality (GUD-IR) is critical for gains on semantically diverse tasks like ERT-NL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval failures (irrelevant retrieved feedback) can cause errors; memory can scale to very large sizes causing efficiency and management problems; adversarial or noisy feedback is a risk; privacy concerns from storing user feedback; model hallucinations or mismatch between generated u and y can mislead users; combiner currently uses a simple similarity threshold which may let irrelevant fb through or block slightly relevant fb.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3225.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3225.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NO-MEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NO-MEM (Standard GPT-3 few-shot prompting baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The baseline variant: GPT-3-175B (davinci) used in few-shot prompting mode that does not consult or store any external memory of user feedback; it outputs an answer y and a verbalized understanding u but receives no corrective-feedback augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NO-MEM (GPT-3 few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-3 is prompted with a fixed few-shot prefix p and the input x (p # x). The model is configured to produce both its verbalized understanding u and the answer y as continuation. No external memory is consulted or updated.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lexical relations; Word scrambling; Ethical reasoning (ERT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as above; serves as the non-memory baseline to measure gains from memory retrieval and prompt editing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / instruction-following / ethical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Lexical QA overall accuracy 0.37 (37%); Word-scramble overall accuracy 0.77 (77%); per-relation breakdown shown in paper (e.g., syn 0.58, ant 0.43, hom 0.13, sent 0.30, defn 0.39).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NO-MEM saturates in performance early (after ~200 time steps in simulations) and cannot incorporate corrective feedback across examples; serves to show that prompt-time memory augmentation is necessary to gain continual improvement without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot learn from repeated user corrections and does not improve over time in an interactive setting without retraining.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3225.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3225.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GROW-PROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GROW-PROMPT (Growing prompt with recent memory entries)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that incrementally appends recent memory entries into the few-shot prompt prefix (up to a token limit) so that past feedback becomes part of the prompt; a non-selective approach to include memory directly in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GROW-PROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A variant of few-shot GPT-3 prompting where the prompt prefix p is continuously extended with the most recent subset of memory entries (feedback examples) until the model's prompt token limit (~2048 tokens) is reached; there is no selective retrieval of only most-relevant feedback for each query.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context-window extension via growing prompt (in-prompt memory examples)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory entries (x, fb) are appended to the prompt prefix in chronological order (most recent subset that fits), making those examples available as in-context training examples; no separate retrieval/transform stage beyond truncation to fit the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lexical relations; Word scrambling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as a baseline for lexical QA and word-scramble tasks to compare selective retrieval (MemPrompt) versus non-selective in-prompt memory inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Lexical QA overall accuracy 0.80 (80%); Word-scramble overall accuracy 0.91 (91%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>NO-MEM lexical overall 0.37 (37%); NO-MEM word-scramble overall 0.77 (77%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GROW-PROMPT gives intermediate gains between NO-MEM and MemPrompt; it is helpful compared to NO-MEM but less effective than MemPrompt's failure-driven selective retrieval; it is more expensive in prompt size (~3x) and limited by the prompt token limit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited scalability due to fixed context window (2048 tokens); non-selective insertion can be less targeted and less efficient than selective retrieval; cannot scale to arbitrary memory size.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3225.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3225.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GUD-IR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GUD-IR (Generated UnDerstanding for Information Retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-retrieve transformation used as a retriever for MemPrompt: a seq2seq model generates a candidate 'understanding/feedback' for an input, which is then used as a key to search over stored feedback entries to find the closest stored fb.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GUD-IR (generative retrieval / transformation component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GUD-IR is a two-step retrieval approach: (1) a trained SEQ2SEQ model (e.g., T5) maps the input x to a generated rough feedback fb_hat (a generated understanding), and (2) fb_hat is used to retrieve the nearest existing feedback fb* from the memory store via standard embedding-based nearest-neighbor search (sentence-transformer embeddings + cosine similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory with generative query transformation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Instead of directly embedding x, GUD-IR synthesizes a likely feedback/understanding for x and then performs retrieval in the space of past feedback entries, alleviating surface-level mismatch when semantically similar cases are lexically dissimilar; retrieved fb* is then attached to the prompt for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ERT-NL (ethical reasoning with natural-language feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ERT-NL contains long, lexically diverse ethical situations where semantically similar cases often lack lexical overlap; the challenge is to retrieve relevant prior feedback even when input wording varies significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>ethical reasoning / retrieval for QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using GUD-IR for retrieval on ERT-NL produced a retrieval/related-accuracy of 45.2% versus 38.5% when using a sentence-transformer retriever (reported in paper), a ~17% relative improvement; this improved retrieval quality yielded better downstream MemPrompt performance on ERT-NL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Sentence-transformer (direct) retrieval: 38.5% (reported for comparison on ERT-NL).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generate-then-retrieve (GUD-IR) substantially improves retrieval similarity and downstream performance on tasks where semantically similar inputs are lexically dissimilar (e.g., ERT-NL); it is explainable (the generated fb can be inspected) and reduces retrieval failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires training a SEQ2SEQ model; generation noise can occur and imperfect generated fb may lead to incorrect retrieval; not a panacea—paper reports remaining retrieval failures (~18% error category) and instability when attempting to learn latent similarity directly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3225.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3225.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BeliefBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work (Kassner et al., 2021) referenced in related work that adds an external memory of beliefs to a pre-trained language model to maintain and manage persistent assertions; cited as related memory-based augmentation of LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beliefbank: Adding memory to a pre-trained language model for a systematic notion of belief</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BeliefBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as an example of adding a memory/bank of beliefs to a pre-trained LM to track and manage model-held assertions; the present paper cites BeliefBank as relevant prior work on adding memory to LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external belief memory (episodic / persistent beliefs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Not detailed in this paper (only referenced); described in the original BeliefBank paper as a stored set of beliefs/annotations associated with a model that can be queried/updated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mention / related work</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of prior memory-augmented LM work relevant to deploying memory for personalization and managing users' varying beliefs; not experimentally evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback <em>(Rating: 2)</em></li>
                <li>Beliefbank: Adding memory to a pre-trained language model for a systematic notion of belief <em>(Rating: 2)</em></li>
                <li>Generation-augmented retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Realm: Retrieval-augmented language model pre-training <em>(Rating: 1)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3225",
    "paper_id": "paper-448e1493034dafe35699ae054ff4480b31dcf64a",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "MemPrompt",
            "name_full": "MemPrompt: Memory-assisted Prompt Editing with User Feedback",
            "brief_description": "A system that pairs GPT-3 (davinci) with a dynamic external memory of (input question -&gt; user corrective feedback) pairs; retrieved feedback is appended to the few-shot prompt to correct model misunderstandings without retraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemPrompt",
            "agent_description": "MemPrompt is an architecture that uses GPT-3-175B (davinci) in a few-shot prompting setup where the model is prompted to output both an answer y and a verbalized task understanding u; a growing external key-value memory M stores keys x (questions) and values fb (user corrective feedback). On a new query, MemPrompt retrieves relevant fb (via a retriever / transformation pipeline), concatenates the fb to the input, and includes in-context examples that teach GPT-3 to react to fb, thereby editing the prompt at inference time rather than retraining model weights.",
            "memory_used": true,
            "memory_type": "retrieval-augmented external key-value feedback memory (prompt-augmentation)",
            "memory_mechanism_description": "Memory M stores (x_i -&gt; fb_i) pairs whenever a user supplies corrective feedback; retrieval is done by transforming the new input (optionally via GUD-IR) and nearest-neighbor lookup over stored fb entries using sentence-transformer embeddings with a similarity threshold; retrieved fb is concatenated to the input (combiner C uses a similarity threshold gating) and the augmented input is fed to GPT-3 together with few-shot examples that include (x, fb -&gt; u, y) patterns so the model learns to incorporate feedback.",
            "task_name": "Lexical relations; Word scrambling; Ethical reasoning (ERT-CAT, ERT-NL); WEBQA factual QA; Personalization (language/dialect)",
            "task_description": "A diverse set of tasks where the primary challenge is model misunderstanding of user intent or of nuanced semantics: (1) lexical relations (synonym/antonym/homophone/definition/sentence usage), (2) word-scrambling/unscrambling (anagrams, cycles, punctuation removal), (3) ethical judgment requiring identifying moral category or rule-of-thumb (ERT-CAT categorical feedback, ERT-NL natural-language feedback), (4) factual QA (WEBQA) with label feedback experiments, and (5) personalization to dialect/language phrases.",
            "task_type": "question answering / instruction-following / ethical reasoning / personalization",
            "performance_with_memory": "Lexical QA overall accuracy: 0.98 (98%); Word-scramble overall accuracy: 0.90 (90%); Ethical reasoning (ERT-CAT and ERT-NL): MemPrompt achieves &gt;25% relative improvement versus NO-MEM; for ERT-NL retriever variant: retrieval/related-accuracy improved to 45.2% when using GUD-IR (vs 38.5% with a sentence-transformer retriever). WEBQA: reported as effective with label feedback (no numeric tabulated result in main text).",
            "performance_without_memory": "NO-MEM (GPT-3 few-shot) baseline: Lexical QA overall accuracy 0.37 (37%); Word-scramble overall accuracy 0.77 (77%); ERT baseline numeric values not fully tabulated in main text but MemPrompt reported &gt;25% relative improvement over NO-MEM.",
            "has_performance_comparison": true,
            "key_findings": "Appending retrieved user corrective feedback to the input prompt (MemPrompt) substantially increases GPT-3's instruction-understanding and answer accuracy over time without retraining; MemPrompt outperforms a no-memory GPT-3 baseline and a non-selective growing-prompt baseline on lexical and many word-scrambling tasks; retrieval quality (GUD-IR) is critical for gains on semantically diverse tasks like ERT-NL.",
            "limitations_or_challenges": "Retrieval failures (irrelevant retrieved feedback) can cause errors; memory can scale to very large sizes causing efficiency and management problems; adversarial or noisy feedback is a risk; privacy concerns from storing user feedback; model hallucinations or mismatch between generated u and y can mislead users; combiner currently uses a simple similarity threshold which may let irrelevant fb through or block slightly relevant fb.",
            "uuid": "e3225.0"
        },
        {
            "name_short": "NO-MEM",
            "name_full": "NO-MEM (Standard GPT-3 few-shot prompting baseline)",
            "brief_description": "The baseline variant: GPT-3-175B (davinci) used in few-shot prompting mode that does not consult or store any external memory of user feedback; it outputs an answer y and a verbalized understanding u but receives no corrective-feedback augmentation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NO-MEM (GPT-3 few-shot)",
            "agent_description": "GPT-3 is prompted with a fixed few-shot prefix p and the input x (p # x). The model is configured to produce both its verbalized understanding u and the answer y as continuation. No external memory is consulted or updated.",
            "memory_used": false,
            "memory_type": "",
            "memory_mechanism_description": null,
            "task_name": "Lexical relations; Word scrambling; Ethical reasoning (ERT)",
            "task_description": "Same tasks as above; serves as the non-memory baseline to measure gains from memory retrieval and prompt editing.",
            "task_type": "question answering / instruction-following / ethical reasoning",
            "performance_with_memory": null,
            "performance_without_memory": "Lexical QA overall accuracy 0.37 (37%); Word-scramble overall accuracy 0.77 (77%); per-relation breakdown shown in paper (e.g., syn 0.58, ant 0.43, hom 0.13, sent 0.30, defn 0.39).",
            "has_performance_comparison": true,
            "key_findings": "NO-MEM saturates in performance early (after ~200 time steps in simulations) and cannot incorporate corrective feedback across examples; serves to show that prompt-time memory augmentation is necessary to gain continual improvement without retraining.",
            "limitations_or_challenges": "Cannot learn from repeated user corrections and does not improve over time in an interactive setting without retraining.",
            "uuid": "e3225.1"
        },
        {
            "name_short": "GROW-PROMPT",
            "name_full": "GROW-PROMPT (Growing prompt with recent memory entries)",
            "brief_description": "A baseline that incrementally appends recent memory entries into the few-shot prompt prefix (up to a token limit) so that past feedback becomes part of the prompt; a non-selective approach to include memory directly in the prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GROW-PROMPT",
            "agent_description": "A variant of few-shot GPT-3 prompting where the prompt prefix p is continuously extended with the most recent subset of memory entries (feedback examples) until the model's prompt token limit (~2048 tokens) is reached; there is no selective retrieval of only most-relevant feedback for each query.",
            "memory_used": true,
            "memory_type": "context-window extension via growing prompt (in-prompt memory examples)",
            "memory_mechanism_description": "Memory entries (x, fb) are appended to the prompt prefix in chronological order (most recent subset that fits), making those examples available as in-context training examples; no separate retrieval/transform stage beyond truncation to fit the prompt.",
            "task_name": "Lexical relations; Word scrambling",
            "task_description": "Used as a baseline for lexical QA and word-scramble tasks to compare selective retrieval (MemPrompt) versus non-selective in-prompt memory inclusion.",
            "task_type": "question answering / instruction-following",
            "performance_with_memory": "Lexical QA overall accuracy 0.80 (80%); Word-scramble overall accuracy 0.91 (91%).",
            "performance_without_memory": "NO-MEM lexical overall 0.37 (37%); NO-MEM word-scramble overall 0.77 (77%).",
            "has_performance_comparison": true,
            "key_findings": "GROW-PROMPT gives intermediate gains between NO-MEM and MemPrompt; it is helpful compared to NO-MEM but less effective than MemPrompt's failure-driven selective retrieval; it is more expensive in prompt size (~3x) and limited by the prompt token limit.",
            "limitations_or_challenges": "Limited scalability due to fixed context window (2048 tokens); non-selective insertion can be less targeted and less efficient than selective retrieval; cannot scale to arbitrary memory size.",
            "uuid": "e3225.2"
        },
        {
            "name_short": "GUD-IR",
            "name_full": "GUD-IR (Generated UnDerstanding for Information Retrieval)",
            "brief_description": "A generate-then-retrieve transformation used as a retriever for MemPrompt: a seq2seq model generates a candidate 'understanding/feedback' for an input, which is then used as a key to search over stored feedback entries to find the closest stored fb.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GUD-IR (generative retrieval / transformation component)",
            "agent_description": "GUD-IR is a two-step retrieval approach: (1) a trained SEQ2SEQ model (e.g., T5) maps the input x to a generated rough feedback fb_hat (a generated understanding), and (2) fb_hat is used to retrieve the nearest existing feedback fb* from the memory store via standard embedding-based nearest-neighbor search (sentence-transformer embeddings + cosine similarity).",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory with generative query transformation",
            "memory_mechanism_description": "Instead of directly embedding x, GUD-IR synthesizes a likely feedback/understanding for x and then performs retrieval in the space of past feedback entries, alleviating surface-level mismatch when semantically similar cases are lexically dissimilar; retrieved fb* is then attached to the prompt for GPT-3.",
            "task_name": "ERT-NL (ethical reasoning with natural-language feedback)",
            "task_description": "ERT-NL contains long, lexically diverse ethical situations where semantically similar cases often lack lexical overlap; the challenge is to retrieve relevant prior feedback even when input wording varies significantly.",
            "task_type": "ethical reasoning / retrieval for QA",
            "performance_with_memory": "Using GUD-IR for retrieval on ERT-NL produced a retrieval/related-accuracy of 45.2% versus 38.5% when using a sentence-transformer retriever (reported in paper), a ~17% relative improvement; this improved retrieval quality yielded better downstream MemPrompt performance on ERT-NL.",
            "performance_without_memory": "Sentence-transformer (direct) retrieval: 38.5% (reported for comparison on ERT-NL).",
            "has_performance_comparison": true,
            "key_findings": "Generate-then-retrieve (GUD-IR) substantially improves retrieval similarity and downstream performance on tasks where semantically similar inputs are lexically dissimilar (e.g., ERT-NL); it is explainable (the generated fb can be inspected) and reduces retrieval failures.",
            "limitations_or_challenges": "Requires training a SEQ2SEQ model; generation noise can occur and imperfect generated fb may lead to incorrect retrieval; not a panacea—paper reports remaining retrieval failures (~18% error category) and instability when attempting to learn latent similarity directly.",
            "uuid": "e3225.3"
        },
        {
            "name_short": "BeliefBank",
            "name_full": "BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief",
            "brief_description": "A prior work (Kassner et al., 2021) referenced in related work that adds an external memory of beliefs to a pre-trained language model to maintain and manage persistent assertions; cited as related memory-based augmentation of LMs.",
            "citation_title": "Beliefbank: Adding memory to a pre-trained language model for a systematic notion of belief",
            "mention_or_use": "mention",
            "agent_name": "BeliefBank",
            "agent_description": "Mentioned in related work as an example of adding a memory/bank of beliefs to a pre-trained LM to track and manage model-held assertions; the present paper cites BeliefBank as relevant prior work on adding memory to LMs.",
            "memory_used": true,
            "memory_type": "external belief memory (episodic / persistent beliefs)",
            "memory_mechanism_description": "Not detailed in this paper (only referenced); described in the original BeliefBank paper as a stored set of beliefs/annotations associated with a model that can be queried/updated.",
            "task_name": null,
            "task_description": null,
            "task_type": "mention / related work",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Cited as an example of prior memory-augmented LM work relevant to deploying memory for personalization and managing users' varying beliefs; not experimentally evaluated in this paper.",
            "limitations_or_challenges": null,
            "uuid": "e3225.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback",
            "rating": 2
        },
        {
            "paper_title": "Beliefbank: Adding memory to a pre-trained language model for a systematic notion of belief",
            "rating": 2
        },
        {
            "paper_title": "Generation-augmented retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Realm: Retrieval-augmented language model pre-training",
            "rating": 1
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 1
        }
    ],
    "cost": 0.01898575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MemPrompt: Memory-assisted Prompt Editing with User Feedback</h1>
<p>Aman Madaan ${ }^{<em>}$, Niket Tandon ${ }^{</em> \dagger}$, Peter Clark ${ }^{\dagger}$, Yiming Yang<br>Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA<br>${ }^{\dagger}$ Allen Institute for Artificial Intelligence, Seattle, WA, USA<br>{amadaan,yiming}@cs.cmu.edu<br>{nikett, peterc}@allenai.org</p>
<h4>Abstract</h4>
<p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret "What word is similar to good?" to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>Language models are now better than ever before at generating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense is in misunderstanding a user's intent. The typical remedy of retraining with more data is prohibitive due to the cost and infrastructure requirements. In such cases, even if users repeatedly observe the model making a mistake, there are no avenues to provide feedback to the model to make it more accurate and personalized over time.</p>
<p>Our goal is to allow users to correct such errors directly through interaction, and without retraining by injecting the knowledge required to correct the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Our memory enhanced GPT-3 implementation.
User: What word is similar to good?
GPT-3: The homophone of good is: wood.
User: "Similar to" means "with similar meaning".
GPT-3: Noted [writes to memory]
User: What word is similar to surprised?
GPT-3: The synonym of surprised is: amazed.
[Retrieves and adds to prompt '"Similar to" means "with similar meaning"'].</p>
<p>Figure 1: This paper enhances GPT-3 performance by looking up questions with a similar intent that received any user feedback. Our approach is simple because only the question in the prompt needs to be updated with relevant feedback, and no retraining is necessary.
model's misunderstanding. Building upon the recent success of injecting commonsense in the input (Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in the input via interactive feedback from an end-user.</p>
<p>Our approach, MemPrompt, pairs GPT-3 with a growing memory of cases where the model misunderstood user's intent and was provided with corrective feedback. This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input. In this sense, our work can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance.</p>
<p>Figure 1 presents a sample interaction between a user and GPT-3 that our setup enables. The model was asked for a similar word. However, the model's (incorrect) task understanding $\mathbf{u}$ was "The homophone of good is". The user can detect such discrepancy between the intended and interpreted task instruction, and can provide feedback fb as "similar to means with a similar meaning", clarifying that they actually wanted a synonym. Crucially,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Proposed architecture: (left) GPT-3 does not account for user feedback. (right) MemPrompt maintains a memory $\mathcal{M}$ of corrective feedback, and searches for feedback from prior queries with a similar intent as $x$ using a retrieval function $\mathcal{M}(\mathbf{x}) . x$ is then concatenated to the retrieved feedback and appended to the prompt for querying GPT-3. Users can also give new feedback on the model's task understanding $u$, then added to $\mathcal{M}$.
note that such instructional correction is feasible even if the user does not know the correct answer to their question, as they are critiquing the model's understanding of their intent, rather than the answers themselves. Thus, our setup does not require the users to be experts at tasks being solved, another advantage of our approach.</p>
<p>Further, it is desirable to have a system that can leverage past feedback on new, unseen examples for prompt-editing. We maintain a memory $\mathcal{M}$ of such feedback as a set of key-value pairs, where the key is a misunderstood question, and the value is the user's feedback to correct that misunderstanding. Given a new question, we check if the model has made a mistake on a similar question earlier, by querying the memory for a similar question. If found, append the corresponding feedback to the question prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive reminding in psychology (Jacoby and Wahlheim, 2013), which suggests humans index error corrections in the context in which those errors occurred.</p>
<p>This paper sets out the general architecture and a simple implementation of its components. We then demonstrate the system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure 1), (2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethical consideration, e.g., "it is about cheating", using a small set of categories), and (4) ethics (with user feedback being natural
language). We find that in all cases, GPT-3's accuracy significantly increases with time, without retraining, as our approach enables it to use corrective feedback from earlier examples to avoid similar misunderstandings on future examples. In summary, our contributions are:</p>
<ul>
<li>We show that a large model like GPT-3 can be improved after deployment, without retraining, through a memory-assisted architecture.</li>
<li>Our implementation, MemPrompt, is the first demonstration that this is possible - this is an important step forward for real use of LMs, and the paper sets out a general architecture that others can build on, a specific implementation, and detailed evaluation on multiple tasks.</li>
</ul>
<h2>2 Related work</h2>
<p>In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting. In this work, we build upon the recent advances in few-shot prompting to modify GPT-3's behavior by adding user feedback to the query (prompt). Like others, we use GPT-3 with few-shot prompting, where the prompt consists of a prefix prefix containing a few inputoutput "training" examples of the task, followed by the input $x$, e.g., a question, to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good "training" examples based on the question (Le Scao and Rush, 2021; Liu et al., 2021a), or even representing the prefix latently (Li and Liang, 2021), our work elaborates the input $x$ itself to clarify the</p>
<p>intended task, by adding user feedback $f b$ from previous misunderstandings.</p>
<p>Similarly, our work can be seen as a form of retrieval-augmented QA. Extensive prior work has used retrievals from a text corpus to aid QA, e.g., Pan et al. (2019); Guu et al. (2020), or retrievals of prior QA pairs for nearest-neighbor QA (Khandelwal et al., 2020). In contrast, we retrieve from a dynamic memory of user feedback.</p>
<p>The idea of failure-driven reminding and dynamic memory date back several decades, e.g., (Schank, 1983; Riesbeck, 1981). Our work resurrects these ideas in a modern context.</p>
<p>Learning from instruction has become important for large LMs that can perform a task based on direct instruction rather than examples (Wei et al., 2021; Mishra et al., 2021). Our work extends this by adding an adaptive component when those instructions are misinterpreted. While it may not be possible for a user to provide meaningful feedback on the output itself, giving feedback on the understanding of the instruction is more feasible.</p>
<p>Our approach aims to modify the model's behavior through prompting, given a wrong answer. An alternative, recently explored approach is "model editing" - updating the model itself by modifying its parameters to fix incorrect answers (Mitchell et al., 2021; De Cao et al., 2021; Hase et al., 2021). Model editing approaches have to date been limited due to uncontrollable out-of-scope changes (Mitchell et al., 2021). In contrast, our goal is not just to correct a prediction, but to generalize that correction for new problems by collecting feedback to clarify the misunderstanding without damaging the model's basic problem-solving acumen.</p>
<p>Finally, our work is a simple example of debugging and learning via dialog. While system debugging through dialogue has been explored in many contexts (Hixon et al., 2015; Wang et al., 2016; Davis, 1977), our contribution is a dialogue about the model's understanding of the user's intent.</p>
<h2>3 Approach</h2>
<h3>3.1 Memory enhanced GPT-3 architecture</h3>
<p>In our setup, given an input $\mathbf{x}$, a model generates an output $\mathbf{y}$ and a sentence $\mathbf{u}$ expressing its understanding of the task, a skill learned through fewshot examples in the prompt (Appendix D). The user can then critique $\mathbf{u}$ by providing natural language feedback $\mathbf{f b}$. This is feasible even if the user does not know the correctness of $\mathbf{y}$ because they
are critiquing the model's understanding of their intent rather the answers themselves.</p>
<p>Given a new query, MemPrompt uses fb from similar, prior queries to enrich the (few-shot) prompt $\mathbf{p}$. We use the principle that if two inputs $x_{i}$ and $x_{j}$ are similar (i.e., $x_{i} \sim x_{j}$ ), then their feedback $\mathbf{f b}<em j="j">{i}$ and $\mathbf{f b}</em>\right)$. The underlying assumption here is that for a fixed model, similar inputs will incur similar errors, and thus can use the same feedback for correction. Fig. 2 gives an overview of MemPrompt, with the following components:}$ should be exchangeable $\left(x_{i} \sim x_{j} \Leftrightarrow f b_{i} \sim f b_{j</p>
<p>Memory $\mathcal{M}: \mathcal{M}$ is a growing table of key $\left(\mathbf{x}<em i="i">{i}\right)$ - value ( $\mathbf{f b}</em>$ ) pairs that supports read, write, and lookup operations. The write operation is used whenever a user gives new feedback.</p>
<p>Lookup $\mathcal{M}(\mathbf{x})$ : The memory allows lookup operations, denoted as $\mathcal{M}(\mathbf{x})$, that matches the query $=\mathbf{x}$ against all the keys of $\mathcal{M}$.</p>
<p>Combiner $\mathcal{C}(\mathbf{x}, \mathcal{M}(\mathbf{x}))$ : A gating function allowing irrelevant, retrieved feedback to be ignored.</p>
<p>Few-shot prompting Let us briefly recap fewshot prompting with GPT-3. Consider a general setup where given an input $\mathbf{x}$, a model is expected to generate an output $\mathbf{y}$. In a few-shot prompting mode (Brown et al., 2020), a prompt $\mathbf{p}$ consists of $k(\mathbf{x}, \mathbf{y})$ "in-context" examples, i.e., $\mathbf{p}=\mathbf{x}<em 1="1">{1} \cdot \mathbf{y}</em>} # \mathbf{x<em 2="2">{2} \cdot \mathbf{y}</em>} \ldots # \mathbf{x<em k="k">{k} \cdot \mathbf{y}</em>}$, where # is a token separating examples and . indicates concatenation. During inference, the user inputs a question $\mathbf{x<em i="i">{i}$, and the model is fed $\mathbf{p} # \mathbf{x}</em>$ as a continuation.}$ (i.e., the question suffixed to the prompt) and is expected to generate the answer $\mathbf{y}_{i</p>
<p>MemPrompt setup As mentioned, given an input $\mathbf{x}$, we prompt the model to generate an output $\mathbf{y}$ and a sentence $\mathbf{u}$ expressing its understanding of the task. Thus, the in-context examples for MemPrompt are of the form $\mathbf{x} \rightarrow \mathbf{u}, \mathbf{y}$. In addition to the input $\mathbf{x}$, MemPrompt retrieves a fb if a question similar to $\mathbf{x}$ has been asked before. To enable the model to react to such feedback, we also include examples of the form $(\mathbf{x}, \mathbf{f b} \rightarrow \mathbf{u}, \mathbf{y})$ in the prompt, which are aimed to teach the model to react to fb (Appendix D).</p>
<h3>3.2 Verbalizing Task Understanding</h3>
<p>Existing methods for receiving user feedback typically assume the user knows the correct answer $\mathbf{y}$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task (fb type)</th>
<th style="text-align: left;">$(\mathbf{x} \rightarrow \mathbf{y})$</th>
<th style="text-align: left;">$\mathbf{u}$ and $\mathbf{f b}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lexical relations (INS)</td>
<td style="text-align: left;">$\mathbf{x}$ : What sounds like good?</td>
<td style="text-align: left;">$\mathbf{u}$ : Question is asking for a synonym.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{y}$ : wood</td>
<td style="text-align: left;">$\mathbf{f b}$ : No, I want a homophone.</td>
</tr>
<tr>
<td style="text-align: left;">Word scrambling (INS)</td>
<td style="text-align: left;">$\mathbf{x}$ : Find the right word given this cycled word: elylarg</td>
<td style="text-align: left;">$\mathbf{u}$ : The question is about anagram.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{y}$ : largely</td>
<td style="text-align: left;">$\mathbf{f b}$ : No, its about uncycling a word.</td>
</tr>
<tr>
<td style="text-align: left;">Ethical reasoning (CAT)</td>
<td style="text-align: left;">$\mathbf{x}$ : Turning my blender on at 3AM</td>
<td style="text-align: left;">$\mathbf{u}$ : Question is about authority.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{y}$ : It's bad.</td>
<td style="text-align: left;">$\mathbf{f b}$ : No, it is about harm.</td>
</tr>
<tr>
<td style="text-align: left;">Ethical reasoning (NL)</td>
<td style="text-align: left;">$\mathbf{x}$ : John has started using again after his mother passed</td>
<td style="text-align: left;">$\mathbf{u}$ : Question is about spending money.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{y}$ : It's bad.</td>
<td style="text-align: left;">$\mathbf{f b}$ : No, it is about drug use.</td>
</tr>
</tbody>
</table>
<p>Table 1: Feedback types and demonstration of understanding: our system leverages user feedback to prevent failures caused due to a misunderstanding of the task (INS) or semantics of the input (CAT and NL). We achieve this by having the model articulate an understanding $\mathbf{u}$, on which a user can provide feedback using fb.
(Elgohary et al., 2021). This assumption is paradoxical: if the user knew the answer, why would they be using the model? Further, allowing only "oracle" users (who know correct y) might lead to sampling biases. On the contrary, in real settings, the user may not know the correct $\mathbf{y}$, but they know what they are looking for. Thus, we propose eliciting a verbalization of task understanding $\mathbf{u}$ from the model in addition to the answer. End users can thus critique $\mathbf{u}$.</p>
<p>We operationalize this idea by including task verbalization in the prompt (Fig. 3). Given a question What sounds like &lt; sighted &gt; ?, a vanilla prompting approach will generate the answer cited. In contrast, we include a $\mathbf{u}$ the homophone for in the prompt. GPT-3 is adept at reasoning with just a handful of examples, and thus can be expected to mimic the prompt to generate task understanding and answer. GPT-3 generates such additional information in all our tasks. Given a test question What sounds similar to &lt; sighted &gt; ?, if the model generates the word that has the same meaning as $\mathbf{u}$, the user has a reason to believe that the answer is wrong.</p>
<p>Our approach is not foolproof- the model may spell out a wrong $\mathbf{u}$ while giving out the correct answer, misleading the user into believing that there is an error (or vice-versa). Hallucinating remains a critical limitation of generative models (Cao et al., 2022), therefore additional heuristics and model calibration might be necessary to make our approach foolproof. In practice, however, we found such cases to be rare for the tasks in this paper.</p>
<h3>3.3 Allowing GPT-3 to react to feedback</h3>
<p>Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form $\mathbf{x}, \mathbf{f b} \rightarrow \mathbf{u}, \mathbf{y}$ in the prompt and setting $\mathbf{f b}=\mathbf{u}$, we force the model to use the task understanding present in the input when gen-</p>
<h2>(Word reasoning task)</h2>
<p>Ques: What sounds like &lt; sighted &gt; ?
Ans: the homophone for sighted is cited.</p>
<h2>(Ethical judgment task)</h2>
<p>Situation: i heard that if you cringe about your past it just means you've grown as a person, for anyone who needs to hear it.
Morality Judgment: This question is about: regretting poor decisions from your past. The answer is it's okay.</p>
<p>Figure 3: MemPrompt is tuned to generate task understanding + answer. This allows the users to provide feedback on the task understanding even without knowing the actual answer.
erating the output (Figure 4). Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models (Madaan and Yazdanbakhsh, 2022).</p>
<p>Ques: What is similar to popular ? clarification: when I ask for similar to, I want a synonym. Ans: the synonym of popular is admired.</p>
<p>Figure 4: An in-context example of the form $\mathbf{x}, \mathbf{f b} \rightarrow$ $\mathbf{u}, \mathbf{y}$, which encourages $\mathbf{u}$ to be like $\mathbf{f b}$, thereby conditioning the output to react to $\mathbf{f b}$.</p>
<h3>3.4 Feedback on model's understanding</h3>
<p>Within the setup $\mathbf{x} \rightarrow \mathbf{u}, \mathbf{y}$, we focus on following two modes of failure:</p>
<ul>
<li>Task instruction understanding: this is especially concerning in a multi-tasking setup, where the model may consider the question to be about a different task than the one user intended.</li>
<li>Task nuanced understanding: when the model understands the task type, but misunderstands the</li>
</ul>
<p>subtle intent in a question.
While feedback on the model output is our primary goal, we also experiment with settings where an Oracle is available to provide feedback on the labels (Section §4.3). We note again that the model reacts to the feedback because some in-context samples are of the form: $(\mathbf{x}, \mathbf{f b} \rightarrow \mathbf{u}, \mathbf{y})$ and $(\mathbf{x} \rightarrow \mathbf{u}, \mathbf{y})$. We consider a diverse set of tasks $(\mathbf{x} \rightarrow \mathbf{y}), \mathbf{f b}$ and $\mathbf{u}$, as summarized in Table 1.</p>
<h3>3.5 Tasks</h3>
<p>We apply our approach to four tasks: (1) lexical relations (e.g., antonyms, Figure 1), (2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethical consideration), and (4) ethics (with user feedback being natural language). For all five tasks, the dataset consists of $(\mathbf{x}, \mathbf{f b} \rightarrow \mathbf{u}, \mathbf{y})$ tuples, where fb clarifies the task in $\mathbf{x}$. We have a simulated conversational setting, in which a user can ask the model $\mathbf{x}$ (covering any of these five tasks). If the model gives a wrong answer to query $\mathbf{x}$, then fb is used as the simulated corrective feedback. The sources for these datasets are listed in Appendix §E.</p>
<h3>3.5.1 Lexical Relations</h3>
<p>The lexical relation task is to predict a word with a given lexical relationship to an input word. We use five relationships: synonym (syn), antonym (ant), homophone (hom), definition (defn), and sentence usage generation (sent).</p>
<h3>3.5.2 Word Scrambling</h3>
<p>For this task, given a word with its characters transformed, the model is expected to recover the original characters. There are four transformation operations the user can request: reversal of words (rev, yppup $\rightarrow$ puppy), cycle letters in word (cyc, atc $\rightarrow$ cat), random insertions (rand, c!r ic/ke!t $\rightarrow$ cricket), and anagrams by changing all but the first and last (anag1, eelhpnat $\rightarrow$ elephant) or all but the first and last 2 characters (anag2, elapehnt $\rightarrow$ elephant). We use the original dataset by Brown et al. (2020). ${ }^{2}$</p>
<p>For both these tasks, each question can be asked in multiple ways (e.g., for synonym generation, the users might ask questions of the form what is like, what has a similar sense, what is akin to, what is something like, etc.) Similarly for the lexical relations task, we specify the task description $x$ using different phrasings, e.g., "rearrange the letters"</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(which the system sometimes misunderstands), and the (simulated) user feedback $f b$ is a clearer task description, e.g., "The anagram is". The system thus accumulates a set of $(x, f b)$ pairs in memory after each failure, helping it avoid future misunderstandings of $x$ through feedback retrieval.</p>
<h3>3.5.3 Ethical Reasoning (2 tasks)</h3>
<p>For ethical reasoning, we consider a setup where given a situation (e.g., cheating on your partner), the model is expected to provide a judgment on whether the situation is ethical or not (e.g., it's not okay). In addition to providing a judgment on the ethics of the situation, the model also elucidates its understanding of what the question is about (e.g., being loyal). While the user may not know the answer, we posit that they would be able to provide feedback on the broader context. For example, if the model generates being financially savvy instead of being loyal for the situation cheating on your partner, a user can still point out this problem and provide feedback.</p>
<p>We use a subset ${ }^{3}$ of the dataset provided by DELPHI (Jiang et al., 2021). We simulate two different kinds of user feedback, using two of the annotations attached to each example in the Delphi dataset:</p>
<ul>
<li>Categorical feedback (ERT-CAT): In this setting, the model generates its understanding $u$ of the situation by selecting one of 10 different possible categories of morality to which the situation might belong: care, loyalty, authority, fairness, sanctity, degradation, cheating, subversion, betrayal, and harm. These categories are explicitly provided for each example in the Delphi dataset.</li>
<li>Natural language feedback (ERT-NL): For this, we use the associated "rule of thumb" (RoT) annotation - a general moral principle - attached to each example in the Delphi dataset. To compile a challenging subset of the data for ERT-NL, we sample by input length, preferring long $\mathbf{x}$, with a short feedback fb. Specifically, we use the top $1 \%$ of the inputs by length to create a challenging set of input situations ( $\mathbf{x}$ ). User feedback fb is a natural language feedback on the understanding $\mathbf{u}$.</li>
</ul>
<p>In both the cases, the model is "taught" to generate a category $\mathbf{u}$ (as well as the okay/not-okay answer $\mathbf{y}$ to the ethical question) by being given a few examples in the prompt prefix, thus articulating which moral category (for ERT-CAT) or rule-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Feedback</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A word pronounced as fellow ?</td>
<td style="text-align: center;">I want a word that sounds similar!</td>
</tr>
<tr>
<td style="text-align: center;">What is dissimilar to delicious ?</td>
<td style="text-align: center;">Give me the reverse of delicious</td>
</tr>
<tr>
<td style="text-align: center;">What is a word like great ?</td>
<td style="text-align: center;">Wrong! I want something similar $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">How do I use melancholy ?</td>
<td style="text-align: center;">No...I wanted a sample sentence</td>
</tr>
<tr>
<td style="text-align: center;">What is on the lines of pretty ?</td>
<td style="text-align: center;">I was looking for a similar word</td>
</tr>
<tr>
<td style="text-align: center;">Could you expand on browser ?</td>
<td style="text-align: center;">I actually wanted a definition</td>
</tr>
<tr>
<td style="text-align: center;">1. Query the memory</td>
<td style="text-align: center;">2. Retrieve relevant feedback</td>
</tr>
<tr>
<td style="text-align: center;">q What is akin to quick?</td>
<td style="text-align: center;">f b. Wrong! when I mention like, I want something similar</td>
</tr>
</tbody>
</table>
<p>Figure 5: Sample snapshot of memory for lexical QA.
of-thumb (for ERT-NL) it thinks is applicable. The simulated feedback fb is the gold category associated with the example in the question, if GPT-3 gets the answer wrong.</p>
<p>We selected these tasks because situations that involve reasoning about similar ethical principles can utilize similar past feedback. For example, sharing an extra umbrella with your friend if they don't have one, and donating surplus food to the homeless both involve compassion.</p>
<h3>3.6 MemPrompt Implementation</h3>
<p>Implementation of memory $\mathcal{M} \mathcal{M}$ uses the user input $\mathbf{x}$ as the key and the corresponding feedback fb as value. Given a question $\mathbf{x}<em i="i">{i}$, if the user detects that the model has misunderstood the question, they may provide a $\mathbf{f b}</em>}$ with clarification probability $\operatorname{Pr}\left(\mathbf{f b<em i="i">{i}\right)$. The feedback is stored in a memory $\mathcal{M}$, with $\mathbf{x}</em>}$ as the key and $\mathbf{f b<em j="j">{i}$ as the value. For a subsequent question $\mathbf{x}</em>)$ checks if a similar question appears in memory. If yes, then the corresponding feedback is attached with the question and fed to the model for generation.}$, the retriever $\mathcal{M}(\mathbf{x</p>
<p>For example, the model might misunderstand a question asking for synonym, e.g., what is akin to fast? as one that requires antonyms. As mentioned, in our setup, the model generates its understanding of the task $\mathbf{u}$, and not just the answer to the question. The user, by inspecting $\mathbf{u}=$ The opposite of fast is: might determine that the model has misunderstood them, and give feedback $i$ wanted $a$ synonym, which gets stored in $\mathcal{M}$. If a similar question (e.g., what is akin to pretty ?) is asked later by the same or a different user, the corresponding feedback (i wanted a synonym) is attached with the
question to generate the answer. Figure 5 illustrates a sample memory for this task.</p>
<p>Implementation of retriever $\mathcal{M}(\mathbf{x})$ A retrieved past feedback that is incorrect might cause the model to make a mistake, thus necessitating a good retrieval function. We propose a two-stage method for effective retrieval involving: transforming $\mathbf{x}$, followed by a similarity lookup of the transformed $\mathbf{x}$ in $\mathcal{M}$. When the task involves high surface-level similarity among past feedback, such as in lexical word tasks, then a simple heuristic-based transformation is sufficient. However, such simple transformations are insufficient for tasks that involves more complex retrieval e.g., when two lexically dissimilar situations can share the same understanding. For example, consider two situations from ERT-NL: Filling a false time sheet at work and Being at a party, and telling parents I am studying. These situations look lexically dissimilar but correspond to the same underlying social principle lying to authority. In our experiments, off-the-shelf methods failed to address these challenges (see §4 later).</p>
<p>To address these challenges with transformation in complex tasks, we have designed a novel SEQ2SEQ based transformation called GUD-IR. Given $\mathbf{x}$, GUD-IR generates a transformed feedback fb for $\mathbf{x}$ using a generative SEQ2SEQ model. Our approach is inspired and supported by the recent success of generate and retrieve (Mao et al., 2021) methods. However, despite the similarity, the methods have different goals: Mao et al. (2021) leverage generative models for query expansion, whereas our goal is explainable input understanding. See Appendix B for more details on GUD-IR.</p>
<p>After the transformation stage, the closest matching entry is then used as the corresponding fb . Transformation reduces $\mathcal{M}(\mathbf{x})$ to a search over $\mathbf{f b}<em 2="2">{1}, \mathbf{f b}</em>$ as the search query. We compute similarity based on a fine-tuned Sentence transformers (Reimers and Gurevych, 2019).}, \ldots, \mathbf{f b}_{|\mathcal{M}|}$ with $\mathbf{f b</p>
<p>Implementation of combiner $\mathcal{C} \quad \mathcal{C}$ concatenates $\mathbf{x}$ with relevant $\mathbf{f b}$ retrieved by $\mathcal{M}(\mathbf{x})$. To ensure that the $\mathbf{x}$ is appended with $\mathbf{f b}$ only if it is relevant, our current implementation of combiner uses a threshold on the similarity score between the $\mathbf{x}$ and the closest feedback fb retrieved by $\mathcal{M}(\mathbf{x})$. We rely on the model (GPT-3) to pay attention to the relevant parts of the input. Exploring more complex gating mechanisms remains an important future work.</p>
<h2>4 Experiments</h2>
<p>Baselines We compare MemPrompt (memoryassisted prompt editing) with two baselines:</p>
<ul>
<li>NO-MEM This is the standard GPT-3 ${ }^{4}$ in few-shot prompting mode (hyper-parameters listed in Appendix $\S \mathrm{C}$ ). Input is $\mathbf{p} # \mathbf{x}<em i="i">{i}$ (i.e., question $\mathbf{x}</em>}$ appended to prompt $\mathbf{p}$ ). It generates answer $\mathbf{y<em i="i">{i}$ and its understanding of the user's intent $\mathbf{u}</em>$.</li>
<li>GROW-PROMPT: Similar to NO-MEM, but the $\mathbf{p}$ is continuously grown with a subset of memory $\mathcal{M}$ that can fit within the prompt (max. 2048 tokens). The most recent subset of $\mathcal{M}$ of memory inserted is inserted in the prompt. The ethical reasoning tasks (ERT) involve long examples, and the initial prompt itself takes close to the max allowed tokens. Thus, the GROW-PROMPT setup is only provided for the lexical relations and word scrambling tasks.</li>
</ul>
<p>Metrics We use two different metrics:</p>
<ul>
<li>$A c c(\mathbf{y}): \%$ of cases where answer matched the ground truth.</li>
<li>$A c c(\mathbf{u}): \%$ of cases where the model's understanding of user's intent is correct. $A c c(\mathbf{u})$ is also referred to as instruction accuracy. As discussed in $\S 3.4$, depending on the task, the model generates its understanding on either the instruction or semantics of the question.</li>
</ul>
<p>Clarification probability In real-world cases, we cannot expect a user to provide feedback for all the examples (e.g., the user might not know that the understanding of the model is wrong). To simulate this realistic setting, we experiment with various values of clarification probabilities $P r$.</p>
<h3>4.1 MemPrompt improves GPT-3 accuracy</h3>
<p>Does pairing GPT-3 with MemPrompt help? §4.1.1 empirically validates this on ethical reasoning tasks and $\S 4.1 .2$ on word reasoning tasks.</p>
<h3>4.1.1 Ethical reasoning tasks</h3>
<p>Table 2 presents results on the DELPHI dataset ( 1,000 points in the test set). Recall from $\S 3.5$ that there are two kinds of feedback on DELPHI questions: CAT and NL feedback. MemPrompt gets over $25 \%$ relative improvement for both ERT-NL and ERT-CAT. We found that having an efficient retriever was critical for ERT-NL: sentence transformer based retriever scored 38.5 , vs. 45.2 using GUD-IR, a $17 \%$ improvement.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Table 2: MemPrompt outperforms NO-MEM for both the categorical and the more challenging ERT-NL setup having longer, ambiguous inputs.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: ERT-CAT: Label accuracy increases with time for all values of clarification probabilities $\operatorname{Pr}\left(\mathbf{f b}_{i}\right)$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: ERT-CAT: Instruction accuracy sharply increases with a larger clarification probability, showing that MemPrompt responds to feedback. With time, lower values of $\operatorname{Pr}\left(\mathbf{f b}_{i}\right)$ catch up as memory is gradually filled with error cases and feedback.</p>
<p>MemPrompt effectively incorporates feedback, improving accuracy over time Figure 7 demonstrates that the instruction accuracy increases over time for different values of clarification probability.</p>
<p>Fig. 6 shows that label accuracy improves over time. Baseline (NO-MEM) saturates after 200 time steps; MemPrompt continues to improve. Continuous improvement is one of our key advantages. These charts show that instruction accuracy and label accuracy are correlated (corr. coeff $=0.36$ ).</p>
<p>We observe that using a higher clarification probability leads to a sharp increase in instruction and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Error category</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Output (MemPrompt)</th>
<th style="text-align: center;">Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Correct, lexically variant understanding</td>
<td style="text-align: center;">when I said "i do" to my wife, I am now realizing I only meant "i'll try". does she deserve to know and a chance to back out?</td>
<td style="text-align: center;">This question is about: telling a spouse your true feelings.</td>
<td style="text-align: center;">This question is about: loving your partner.</td>
</tr>
<tr>
<td style="text-align: center;">Distracted understanding</td>
<td style="text-align: center;">i am 20 years old, lately I have been forgetting simple spellings while taking notes in class . I often bump into random objects around hurting myself.</td>
<td style="text-align: center;">This question is about: wanting to kill yourself. The answer is BAD.</td>
<td style="text-align: center;">This question is about: seeking medical attention if you feel there is something wrong with you. The answer is GOOD.</td>
</tr>
<tr>
<td style="text-align: center;">Retrieval failures</td>
<td style="text-align: center;">checked out my high school ex girlfriends instagram for the first time in two years after graduating. I'm happy I did. she's not as pretty as she used to be and I feel great.</td>
<td style="text-align: center;">[retrieved feedback] This question is about: looking at someone's social media. The answer is GOOD.</td>
<td style="text-align: center;">This question is about: judging people based on physical attributes. The answer is BAD.</td>
</tr>
</tbody>
</table>
<p>Table 3: ERT NL task error categories
label accuracy early on in the training for both ERTCAT and ERT-NL. This is because with a large clarification probability, memory fills up with feedback sooner, leading to higher accuracy.</p>
<p>Error analysis: Ethical-NL In ERT NL and CAT tasks, a primary source of label errors is confusion between labels such as OKAY and GOOD due to the nuanced differences e.g., input $=$ teaching your child a musical instrument. MemPrompt predicts GOOD, but the expected answer is OKAY. Jiang et al. (2021) make similar observations.</p>
<p>We randomly sampled examples from the ERTNL dev set where the model generates an incorrect understanding (i.e., $A c c(\mathbf{u})=0$ based on exact match). Our goal is to understand the typical errors made by the model and use the analysis to calibrate the findings in Table 2. We select ERT-NL for the analysis because it involves free-form natural language which is difficult to study quantitatively.</p>
<ul>
<li>Correct, lexically variant understanding (30\%): Exact match underestimates model performance (as the task involves generation). $\sim 30 \%$ $\mathbf{u}$ is a lexical variation of the reference gold understanding. E.g., telling a spouse your true feeling vs. loving your partner. The generated label in these $30 \%$ cases is still correct. (Table 3, row 1)</li>
<li>Distracted understanding (50\%): A major source of instruction and label errors is the model getting distracted by an unimportant context. Bad retrieval accounts for $30 \%$ errors within this category, e.g., matching a situation in the memory where the expected understanding is only partially applicable to the query. (Table 3, row 2)</li>
<li>Retrieval failures (18\%): These errors are caused by an irrelevant retrieved understanding from the memory, when using a state-of-the-art retrieval method (Table 3, row 3). GUD-IR helps to
reduce these retrieval failures. See Appendix §B. Table 3 presents canonical examples of these error categories. We also find that over time, more relevant past examples are fetched (see Table 7).</li>
</ul>
<h3>4.1.2 Word Reasoning Tasks</h3>
<p>For these tasks, we compare gold $\mathbf{u}^{*}$ and generated $\mathbf{u}$ based on hard-coded linguistic variations (e.g., the antonym is matches the opposite is). While we do not explicitly evaluate task accuracy, there is a near-perfect correlation between the accuracy of $\mathbf{y}$ and $\mathbf{u}$ (i.e., if the GPT-3 understands the task correctly, the output was almost always correct). This shows improving model's understanding of a task might lead to an improved performance.</p>
<p>Figure 8 reports the overall performance on the word reasoning tasks. The accuracy improves substantially within 300 examples when using memory (in yellow) vs. no memory (in blue). Note that we are operating in a few-shot prompting regime (i.e., there is no training data over which we train). The performance of GROW-PROMPT (red) lies in between, showing that non-selective memory is partially helpful, although not as effective as failure-driven retrieval (our model). However, GROW-PROMPT is $\sim 3 \mathrm{x}$ more expensive (larger prompts) and cannot scale beyond the 2048 tokens limit. We also found that the retrieved feedback from memory was effective $97 \%$ of the time; only in $\approx 3 \%$ of cases feedback had no positive effect.</p>
<p>When the memory is used for every example (green line, Fig 8, top), the performance improves quickly vs. the yellow line $\left(\operatorname{Pr}\left(\mathbf{f b}_{i}\right)=0.5\right)$.</p>
<h3>4.2 Using dynamic prefix in prompts</h3>
<p>Recent work such as Liu et al. (2021a) investigate using dynamic prompts for better generation. For a</p>
<table>
<thead>
<tr>
<th>model</th>
<th>syn</th>
<th>ant</th>
<th>hom</th>
<th>sent</th>
<th>defn</th>
<th>all</th>
</tr>
</thead>
<tbody>
<tr>
<td>NO-MEM</td>
<td>0.58</td>
<td>0.43</td>
<td>0.13</td>
<td>0.30</td>
<td>0.39</td>
<td>0.37</td>
</tr>
<tr>
<td>GROW-PROMPT</td>
<td>0.71</td>
<td>0.87</td>
<td>0.75</td>
<td>0.92</td>
<td>0.76</td>
<td>0.80</td>
</tr>
<tr>
<td>MemPrompt</td>
<td>$\mathbf{0 . 9 9}$</td>
<td>$\mathbf{0 . 9 8}$</td>
<td>$\mathbf{0 . 9 8}$</td>
<td>$\mathbf{0 . 9 8}$</td>
<td>$\mathbf{0 . 9 6}$</td>
<td>$\mathbf{0 . 9 8}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on lexical qa: MemPrompt has the best performance across all lexical QA tasks.</p>
<table>
<thead>
<tr>
<th>model</th>
<th>anag1</th>
<th>anag2</th>
<th>cyc</th>
<th>rand</th>
<th>rev</th>
<th>all</th>
</tr>
</thead>
<tbody>
<tr>
<td>NO-MEM</td>
<td>0.81</td>
<td>0.47</td>
<td>0.95</td>
<td>0.98</td>
<td>0.62</td>
<td>0.77</td>
</tr>
<tr>
<td>GROW-PROMPT</td>
<td>$\mathbf{0 . 8 6}$</td>
<td>$\mathbf{0 . 8 9}$</td>
<td>0.93</td>
<td>$\mathbf{0 . 9 6}$</td>
<td>0.90</td>
<td>$\mathbf{0 . 9 1}$</td>
</tr>
<tr>
<td>MemPrompt</td>
<td>0.81</td>
<td>0.83</td>
<td>$\mathbf{0 . 9 8}$</td>
<td>0.95</td>
<td>$\mathbf{0 . 9 3}$</td>
<td>0.90</td>
</tr>
</tbody>
</table>
<p>Table 5: GROW-PROMPT and MemPrompt outperform NO-MEM on all word scramble QA tasks.
given input $\mathbf{x}$, their method( KATE) relies on retrieving examples from the training set that are similar to $\mathbf{x}$ for dynamically creating the prompt $\mathbf{p}$. Note that our method edits $\mathbf{x}$ with a feedback $\mathbf{f b}$, and is thus complementary to KATE. We verify this by experiments on ERT-CAT and ERT-NL. Specifically, we create dynamic prompts using KATE, whereas MemPrompt is used like before to attach a fb to the question. We observe a consistent $10 \%$ improvement by using KATE across all baselines, showing that the improvements are complementary.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Avg. performance on lexical (top) and word scramble (bottom) tasks with time (x-axis). Accuracy increases with time as memory is filled up with feedback from past errors.</p>
<h3>4.3 MemPrompt with label feedback</h3>
<p>MemPrompt requires the model to verbalize its understanding of the question, on which a user provides feedback. To investigate the efficacy of MemPrompt in settings where generating an understanding is not easy, we experiment with factual question answering on the WEBQA dataset (Berant et al., 2013), and find that MemPrompt is effective even with label feedback (Appendix §F).</p>
<h3>4.4 Using MemPrompt for language and dialects based personalization</h3>
<p>We demonstrate an application of MemPrompt for personalization with a use-case where user language preferences can be folded in the memory. We simulate a user who does not speak fluent English and uses code-mixed language. The queries posed by the user contain words from two Indian languages: Hindi and Punjabi. GPT-3 predictably misunderstands the task. The user clarifies the meanings of their dialect/language phrases. While initial queries fail, subsequent queries that reuse similar words succeed because their clarifications are present in the memory (details in Appendix §G).</p>
<h2>5 Conclusion</h2>
<p>We present MemPrompt, a novel, memoryenhanced GPT-3 that allows users to interact and improve the model without retraining. A key insight is to have the model articulate not just its answer but also its understanding of the user's intent, providing an avenue for feedback. We show that deployed systems with fixed large-language models can still be improved by interacting with end-users, potentially improving their performance and broadening their utility.</p>
<h2>Acknowledgments</h2>
<p>We thank Dheeraj Rajagopal and Yannic Kilcher for the insightful and engaging discussions. This material is partly based on research sponsored in part by the Air Force Research Laboratory (agreement number FA8750-19-2-0200). The U.S. Govt. is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.</p>
<h2>6 Limitations</h2>
<p>We have shown how to improve very large models through interaction. Our memory-based enhancement is a low-cost utility enhancement eventually geared towards personalized, correctable models, which is currently an open question in NLP with unresolved issues. While our method is a step toward a promising open direction, it comes with limitations and opportunities when deploying to the real world.</p>
<p>Scaling In practical deployments of the MemPrompt method, the memory can grow to orders of magnitude, introducing scaling challenges. We anticipate using memory as a buffer between cycles of re-training, and these cycles could range from a week to several months. Between cycles of retraining, MemPrompt can serve as a way to avoid repeating mistakes and collect feedback which can be used to fine-tune and improve the next version of the model.</p>
<p>Currently, we operate with a single user at a time, but a real-world deployment could encounter multiple users. These users could exhibit characteristics of a user community where some feedback could apply to multiple users in a community cluster, while others differ in interpretation and style. In such a multi-user environment, managing the memory effectively when dealing with incompatible entries would be important. Existing initial ideas towards managing a bank of beliefs could be extended to address these problems, e.g., (Kassner et al., 2021). In addition, when looking up such a rich and potentially noisy feedback collection, rather than retrieving a single feedback item, it would help to have an adapter over the memory that generates feedback by adapting the existing, diverse, and related past feedback to the current scenario. This increases the diversity of the generated knowledge and reduces the impact of erroneous feedback and noise.</p>
<p>Ethical concerns Extending the discussion on noise in feedback, our setting assumes that users will not provide any adversarial feedback. However, in real-world environments, this assumption is unlikely to hold. Additionally, there is a risk in the real-world deployment of our system, wherein an adversarial user might provide harmful feedback, thus maliciously controlling the systems (potentially a home-based robot) where our method is deployed. Thus, robust mechanisms such as GUD-IR
and memory adapters will be critical for successful real-world deployments.</p>
<p>Privacy is another ethical concern, as the deployed system collects and records feedback from a user, some of which could contain personal information (when I look for an interesting movie, I mean something that contains romance). Therefore, the system needs to win the trust of the users so they would be encouraged to interact closely, and to win this trust, the system needs to demonstrate smartness, receptivity to user feedback, and the ability to maintain the memory without leaking any personal information safely.</p>
<p>Finally, large-language models generate text that might be biased and insensitive to a user's socio-cultural context (Bordia and Bowman, 2019; Sharma et al., 2021; Hovy and Prabhumoye, 2021). In a multi-user deployment of our system, the memory could contain feedback from user communities of diverse beliefs, gender identities, and cultural backgrounds could lead to conflicts. Thus the system will need checks and balances to ensure that the content produced by the system as a result of the feedback is not harmful.</p>
<h2>References</h2>
<p>Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198, Online. Association for Computational Linguistics.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533-1544, Seattle, Washington, USA. Association for Computational Linguistics.</p>
<p>Shikha Bordia and Samuel R. Bowman. 2019. Identifying and reducing gender bias in word-level language models. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 7-15, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,</p>
<p>Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340-3354.</p>
<p>Randall Davis. 1977. Interactive transfer of expertise: Acquisition of new inference rules. Artif. Intell., 12:121-157.</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 64916506, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural language interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5599-5610, Online. Association for Computational Linguistics.</p>
<p>Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 653-670, Online. Association for Computational Linguistics.</p>
<p>Kelvin Guu, Kenton Lee, Z. Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. ArXiv, abs/2002.08909.</p>
<p>Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. ArXiv preprint, abs/2111.13654.</p>
<p>Ben Hixon, Peter Clark, and Hannaneh Hajishirzi. 2015. Learning knowledge graphs for question answering through conversational dialog. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 851-861, Denver, Colorado. Association for Computational Linguistics.</p>
<p>Dirk Hovy and Shrimai Prabhumoye. 2021. Five sources of bias in natural language processing. Language and Linguistics Compass, 15(8):e12432.</p>
<p>Larry L. Jacoby and Christopher N. Wahlheim. 2013. On the importance of looking back: The role of recursive remindings in recency judgments and cued recall. Memory \&amp; Cognition, 41:625-637.</p>
<p>Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. Delphi: Towards machine ethics and norms. ArXiv preprint, abs/2110.07574.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. 2021. Beliefbank: Adding memory to a pre-trained language model for a systematic notion of belief. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8849-8861.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2627-2636, Online. Association for Computational Linguistics.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823-1840, Online. Association for Computational Linguistics.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021a. What Makes Good In-Context Examples for GPT-\$3\$? ArXiv preprint, abs/2101.06804.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686.</p>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Generation-augmented retrieval for opendomain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4089-4100, Online. Association for Computational Linguistics.</p>
<p>Gary Marcus. Experiments testing gpt-3's ability at commonsense reasoning: results. [online]. 2021.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. ArXiv, abs/2104.08773.</p>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2021. Fast model editing at scale. arXiv preprint arXiv:2110.11309.</p>
<p>Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 454-459, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Xiaoman Pan, Kai Sun, Dian Yu, Jianshu Chen, Heng Ji, Claire Cardie, and Dong Yu. 2019. Improving question answering with external knowledge. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 27-37, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:167 .</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages</p>
<p>3982-3992, Hong Kong, China. Association for Computational Linguistics.
C. Riesbeck. 1981. Failure-driven reminding for incremental learning. In IJCAI.</p>
<p>Roger Schank. 1983. Dynamic Memory: A Theory of Reminding and Learning in Computers and People. Cambridge University Press.</p>
<p>Shanya Sharma, Manan Dey, and Koustuv Sinha. 2021. Evaluating gender bias in natural language inference. arXiv preprint arXiv:2105.05541.</p>
<p>Douglas Summers-Stay, Claire Bonial, and Clare Voss. 2021. What can a generative language model answer about a passage? In Proceedings of the 3rd Workshop on Machine Reading for Question Answering, pages 73-81, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. NAACL Findings.(to appear).</p>
<p>Sida I. Wang, Percy Liang, and Christopher D. Manning. 2016. Learning language games through interaction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2368-2378, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652.</p>
<h2>A Inside MemPrompt: Populating and using the memory</h2>
<p>MemPrompt maintains a growing memory of recorded cases where a feedback was provided to clarify the user's misunderstood intent. This flow is presented in Figure 9 that shows a sequence of steps 1-5 on how the memory is populated.</p>
<p>MemPrompt also produces enhanced prompts for any new query based on the user feedback on similar cases recorded previously in the memory. Figure 10 presents the sequence of steps 1-3 involved in retrieving and applying a past feedback on a similar case.</p>
<h2>B Generative IR (GUD-IR)</h2>
<p>A note on feedback and understanding Feedback fb and understanding $\mathbf{u}$ are two concepts that we repeatedly use in this work. Briefly, MemPrompt requires a model to spell out its understanding of the instruction (u). The user can then provide a feedback fb on the understanding. In the prompt, both fb and $\mathbf{u}$ are identical. Such examples are of the form $\mathbf{x}, \mathbf{u} \rightarrow \mathbf{u}, \mathbf{y}$ and their main purpose is to reinforce that model the input feedback $\mathbf{u}$ be used to generate the output.</p>
<h2>B. 1 Introduction</h2>
<p>One of the key strengths of MemPrompt is its ability to leverage feedback provided on earlier inputs $\mathbf{x}$ to improve a current input. This is achieved by retrieving a feedback from memory $\mathcal{M}$ using $\mathbf{x}$ as the key. An underlying assumption of this process is that similar inputs will admit similar feedback, allowing us to use the feedback provided for one situation on another. For two input situations $s_{i}$ and $s_{j}$ with respective feedback $\mathrm{fb}<em j="j">{i}$ and $\mathrm{fb}</em>$, this assumption can be succinctly stated as:</p>
<p>$$
s_{i} \sim s_{j} \Longrightarrow \mathrm{fb}<em j="j">{i} \sim \mathrm{fb}</em>
$$</p>
<p>The ethical reasoning dataset with natural language feedback, ERT-NL, provides a unique challenge for this assumption because lexically dissimilar situations might have the same feedback. As a concrete example, consider an input situation $s_{i}$ : tom hated skating because he had no sense of balance - with a feedback $\mathrm{fb}<em i="i">{i}$ : this question is about practicing more when you want to improve your skills. Suppose that our system has already seen $s</em>}$ and has received a feedback $\mathrm{fb<em i="i">{i}$ (i.e., there is an entry in $\mathcal{M}: s</em>} \rightarrow \mathrm{fb<em j="j">{i}$ ). Next, suppose a user enters
a new situation $s</em>$ or worse, may retrieve a misleading feedback.}$ : jordyn was trying to improve her soccer skills. As usual, MemPrompt will try to retrieve feedback for a similar situation. However, such retrieval is going to be challenging, because $s_{i}$ (tom hated skating because he had no sense of balance) has little to no overlap with $s_{j}$ (jordyn was trying to improve her soccer skills), although humans can easily tell that both situations are about improving skills. Consequently, MemPrompt may fail to retrieve the relevant feedback $\mathrm{fb}_{i</p>
<p>The fact that two ostensibly dissimilar inputs two inputs $\left(\mathbf{x}<em j="j">{i}, \mathbf{x}</em>\right)$ may share the same feedback makes vanilla retrieval non-viable for our setting. We deal with this challenging situation with two different solutions of increasing complexity.</p>
<h2>B. 2 Initial approach: Learning a feedback similarity function</h2>
<p>Since the surface level similarity of input situations is not enough to capture similarity of respective feedback, we attempt to learn a function $f_{\theta}$ that will map similar inputs $\mathbf{x}<em j="j">{i}$ and $\mathbf{x}</em>}$ to similar representations if the corresponding feedback $\mathrm{fb<em j="j">{i}$ and $\mathrm{fb}</em>}$ are close to each other, and vice-versa. A natural choice is training an embedding function $f: \mathbf{x} \rightarrow \mathrm{R}^{d}$ supervised by $\cos \left(\mathbf{f b<em j="j">{i}, \mathbf{f b}</em>\right)$. Thus, the objective function is:}\right)$ where $\cos$ is the cosine similarity $\left(\cos (\mathbf{a}, \mathbf{b})=\frac{\mathbf{a}^{T} \mathbf{b}}{|\mathbf{a}||\mathbf{b}|</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\theta}=\left(\cos \left(f</em>}\left(\mathbf{x<em _theta="\theta">{i}\right), f</em>}\left(\mathbf{x<em i="i">{j}\right)\right)-\cos \left(\mathbf{f b}</em>
$$}, \mathbf{f b}_{j}\right)\right)^{2</p>
<p>Intuitively, this objective function will encourage the similarity between the inputs $\left(\cos \left(f_{\theta}\left(\mathbf{x}<em _theta="\theta">{i}\right), f</em>\right)\right)\right)$ to be high when the corresponding feedback are similar, and vice-versa.}\left(\mathbf{x}_{j</p>
<p>Feedback retrieval proceeds as follows: an input $s_{i}$ is embedded using $f_{\theta}$, and $f_{\theta}\left(s_{i}\right)$ is then used to retrieve a feedback from the memory, with the hope that representations $f_{\theta}\left(s_{i}\right)$ and $f_{\theta}\left(s_{j}\right)$ will be similar after the training.</p>
<p>While in principle this objective function should be enough to learn informative representations that bring two inputs with similar feedback close, we found the training to be unstable. We attribute this to the fact that two extremely dissimilar situations can have identical feedback. Given limited training data, it might be unrealistic to train similarity functions that can capture all possible cases where the same feedback applies to two situations. As a way to circumvent this, we also experiment with a generative version of our method, described next.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: MemPrompt: adding to memory. User enters a question for which no feedback is available (steps 1, 2). Directly prompting GPT-3 with the question leads to incorrect answer and understanding (step 3). User-provides feedback on the incorrect understanding (step 4), which is added to memory (step 5).
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: MemPrompt: retrieving feedback from memory. User enters a question which GPT-3 has incorrectly answered in the past, and has received feedback from a user (step 1). The feedback is retrieved from memory (step 2), and both question and feedback are added to the prompt. The prompt contains examples that allow GPT-3 to react to user feedback and generate correct understanding and answer.</p>
<h2>B. 3 Proposed approach: Training generative model for retrieving similar feedback</h2>
<p>To address these retrieval issues, we propose GUDIR (Generated UnDerstanding for explainable IR). The key intuition for our approach relies on substituting $f_{\theta}: \mathbf{x} \rightarrow \mathrm{R}^{d}$ (latent space projection) with $f_{\theta}: \mathbf{x} \rightarrow \mathbf{f b}$ (generated understanding of $\mathbf{x}$ ). Concretely, instead of learning a function that maps a question to a $d$ dimensional vector, we train a
generative model that directly maps an input to a rough understanding. The generated rough understanding is then used as a key to retrieve a relevant understanding from the database using any off-theshelf retrieval method. This two-step generatethen-retrieve procedure has benefits: (i) it alleviates sparsity issues that we found latent space projection</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 11: Overview of GUD-IR. To retrieve a relevant feedback that applies to $\mathbf{x}$, GUD-IR first generates a feedback $\mathbf{f b}$ using a generative model. This is then aligned with a corpus of feedbacks $\mathbf{f b}<em 2="2">{1}, \mathbf{f b}</em>}, \ldots, \mathbf{f b<em _in_1_t="\in[1,|t" j="j" r_="r|]">{|t r|}$ (e.g., sourced from the train split). The best matching feedback $\mathbf{f b}^{<em>}$ is then used for $\mathbf{x}$. Thus, GUD-IR decomposes the retrieval problem $\mathbf{x} \rightarrow \mathbf{f b}$ into two sub-problems: (i) generate a rough feedback ( $\mathbf{x} \rightarrow \mathbf{f b}$ ) and (ii) search for the closest feedback in a large store $\mathbf{f b}^{</em>}=\arg \min </em> \mid$.
methods were unable to deal with ${ }^{5}$ (ii) the overall retrieval becomes explainable and debuggable.} \mid \mathbf{f b}-\mathbf{f b}_{j</p>
<p>Our approach is inspired and supported by the recent success of generate and retrieve (Mao et al., 2021) methods. However, despite the similarity, the methods have different goals: Mao et al. (2021) leverage generative models for query expansion, whereas our goal is explainable input understanding. Moreover, their implementation is geared towards open-domain QA, while ours is towards explainable input understanding. Thus, it is nontrivial to adapt similar ideas to our tasks effectively.</p>
<p>Specifically, we train a SEQ2SEQ model, (e.g., T5 (Raffel et al., 2020)), that maps each input $\mathbf{x}$ to a corresponding output $\mathbf{f b}$. The feedback is now retrieved in a two step process:</p>
<ol>
<li>The generative model $f_{\theta}$ is used to generate a noisy feedback for $s_{i}$, fb.</li>
<li>$\mathbf{f b}$ is used as a key to search over the set of already present feedbacks, to retrieve the nearest one.
<sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Instead of directly using clarification to lookup the nearest feedback, we first transform the input to the space of clarifications, then search over the set of already present clarifications. Figure 11 presents an overview of our generation then reshape approach (GUD-IR). As we discuss in Section 4.1.1, GUDIR was key to achieving good performance for the ERT-NL task.</li>
</ol>
<p>In addition to the task accuracy, we plot the distribution of $\operatorname{sim}\left(\hat{u}, \hat{u}^{*}\right)$ (similarity of the true and retreived feedback) over the test set for different retrieval methods. Figure 12 shows this distribution using GUD-IR and using surface-level similarities. The probability mass shifts towards a higher similarity range for GUD-IR.</p>
<p>The lexical reasoning and WEBQA tasks present a simpler setting for retrieval, as similarity of keys indicates a similarity of values. For such cases, we use Sentence transformers (Reimers and Gurevych, 2019) to encode the query, and cosine similarity with a threshold of 0.9 to find a matching key.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 12: Distribution of similarity scores between expected $\mathrm{fb}{ }^{*}$ and $\hat{u}$ for retrieval (left) and GUD-IR (right). The similarity scores are higher using GUD-IR.</p>
<h1>C Querying GPT-3-175B using OpenAI API</h1>
<p>We use the OpenAI API for querying GPT-3175B. ${ }^{6}$ The python code is listed below. Here, "PROMPT" is set to prompt shown in $\S \mathrm{D}$, followed by the input question $\mathbf{x}$ and feedback fb if applicable.</p>
<p>We used a temperature of 0.0 for factual QA (WEBQA) experiments to select the most likely token at each step, and this setting does not require generating diverse answers, as one would expect for a factual domain. For ERT-CAT and ERT-NL, we found that a higher temperature $(\sim 0.7)$ was causing a large divergence in the performance (a difference of $\pm 10 \%$ accuracy across runs), making reproducibility challenging - similar observations were made by (Summers-Stay et al., 2021). Thus, we used to a temperature of 0.0 for ERT experiments. A temperature of 0.7 was used for all the other experiments.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Completion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">engine</span><span class="o">=</span><span class="s2">&quot;davinci&quot;</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;PROMPT&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</code></pre></div>

<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>D Prompt</h1>
<p>GPT3 is queried using a prompt $\mathbf{p}$ of example i/o behaviors, followed by the actual question $\mathbf{x}$ and (optionally) retrieved feedback fb. It then generates the understood intent $\mathbf{u}$ and answer $\mathbf{y}$ as a continuation. $\mathbf{u}$ and $\mathbf{y}$ are expressed a single sentence, e.g., "[The synonym for <word> is] [<word>]" Figure 13 shows this prompt $\mathbf{p}$, containing a mixture of $(\mathbf{x} \rightarrow \mathbf{u}, \mathbf{y})$ and $(\mathbf{x}, \mathbf{f b} \rightarrow \mathbf{u}, \mathbf{y})$ "training" tuples.</p>
<div class="codehilite"><pre><span></span><code><span class="n">What</span> <span class="n">is</span> <span class="n">the</span> <span class="n">homophone</span> <span class="n">for</span> <span class="o">&lt;</span> <span class="n">wring</span> <span class="o">&gt;</span> <span class="err">?</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">homophone</span> <span class="n">for</span> <span class="n">wring</span> <span class="n">is</span> <span class="n">ring</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">how</span> <span class="n">do</span> <span class="n">you</span> <span class="n">use</span> <span class="o">&lt;</span> <span class="n">highway</span> <span class="o">&gt;</span> <span class="n">in</span> <span class="n">a</span> <span class="n">sentence</span><span class="err">?</span>
<span class="o">#</span>
<span class="n">a</span> <span class="n">sentence</span> <span class="n">with</span> <span class="n">highway</span> <span class="n">is</span><span class="p">:</span> <span class="n">soldiers</span> <span class="n">stand</span> <span class="n">guard</span> <span class="n">along</span> <span class="n">the</span> <span class="p">[</span> <span class="n">highway</span> <span class="p">]</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">can</span> <span class="n">you</span> <span class="n">define</span> <span class="o">&lt;</span> <span class="n">camisole</span> <span class="o">&gt;</span> <span class="err">?</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">definition</span> <span class="n">of</span> <span class="n">camisole</span> <span class="n">is</span> <span class="n">a</span> <span class="n">sleeveless</span> <span class="n">undergarment</span><span class="p">.</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">What</span> <span class="n">is</span> <span class="n">the</span> <span class="n">antonym</span> <span class="n">for</span> <span class="o">&lt;</span> <span class="n">prohibition</span> <span class="o">&gt;</span> <span class="err">?</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">antonym</span> <span class="n">for</span> <span class="n">prohibition</span> <span class="n">is</span> <span class="n">permit</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">What</span> <span class="n">is</span> <span class="n">the</span> <span class="n">synonym</span> <span class="n">for</span> <span class="o">&lt;</span> <span class="n">surrogate</span> <span class="o">&gt;</span> <span class="err">?</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">synonym</span> <span class="n">for</span> <span class="n">surrogate</span> <span class="n">is</span> <span class="n">substitute</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">how</span> <span class="n">do</span> <span class="n">i</span> <span class="n">use</span> <span class="o">&lt;</span> <span class="n">fog</span> <span class="o">&gt;</span> <span class="err">?</span> <span class="p">|</span> <span class="n">clarification</span><span class="p">:</span> <span class="n">when</span> <span class="n">i</span> <span class="n">ask</span> <span class="n">for</span> <span class="n">how</span> <span class="n">do</span> <span class="n">i</span> <span class="n">use</span><span class="p">,</span> <span class="n">i</span> <span class="n">want</span> <span class="n">a</span> <span class="n">sentence</span><span class="p">.</span>
<span class="o">#</span>
<span class="n">a</span> <span class="n">sentence</span> <span class="n">with</span> <span class="n">fog</span> <span class="n">is</span><span class="p">:</span> <span class="n">a</span> <span class="n">rising</span> <span class="n">sun</span> <span class="n">burns</span> <span class="n">the</span> <span class="p">[</span> <span class="n">fog</span> <span class="p">]</span> <span class="n">off</span> <span class="n">a</span> <span class="n">city</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">What</span> <span class="n">sounds</span> <span class="n">like</span> <span class="o">&lt;</span> <span class="n">sighted</span> <span class="o">&gt;</span> <span class="err">?</span> <span class="p">|</span> <span class="n">clarification</span><span class="p">:</span> <span class="n">when</span> <span class="n">I</span> <span class="n">ask</span> <span class="n">for</span> <span class="n">sounds</span> <span class="n">like</span><span class="p">,</span> <span class="n">I</span> <span class="n">want</span> <span class="n">a</span> <span class="n">homophone</span><span class="p">.</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">homophone</span> <span class="n">for</span> <span class="n">sighted</span> <span class="n">is</span> <span class="n">cited</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">what</span> <span class="n">is</span> <span class="n">like</span> <span class="o">&lt;</span> <span class="n">provident</span> <span class="o">&gt;</span> <span class="err">?</span> <span class="p">|</span> <span class="n">clarification</span><span class="p">:</span> <span class="n">when</span> <span class="n">I</span> <span class="n">ask</span> <span class="n">for</span> <span class="n">like</span><span class="p">,</span> <span class="n">I</span> <span class="n">want</span> <span class="n">a</span> <span class="n">synonym</span><span class="p">.</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">synonym</span> <span class="n">for</span> <span class="n">provident</span> <span class="n">is</span> <span class="n">prudent</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">can</span> <span class="n">you</span> <span class="n">define</span> <span class="o">&lt;</span> <span class="n">rider</span> <span class="o">&gt;</span> <span class="err">?</span> <span class="p">|</span> <span class="n">clarification</span><span class="p">:</span> <span class="n">when</span> <span class="n">i</span> <span class="n">ask</span> <span class="n">for</span> <span class="n">define</span><span class="p">,</span> <span class="n">i</span> <span class="n">want</span> <span class="n">a</span> <span class="n">definition</span><span class="p">.</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">definition</span> <span class="n">of</span> <span class="n">rider</span> <span class="n">is</span> <span class="n">a</span> <span class="n">person</span> <span class="n">who</span> <span class="n">is</span> <span class="n">riding</span> <span class="n">something</span><span class="p">.</span> <span class="kr">END</span>
<span class="o">#</span>
<span class="n">What</span> <span class="n">is</span> <span class="n">the</span> <span class="n">opposite</span> <span class="n">of</span> <span class="o">&lt;</span> <span class="n">citation</span> <span class="o">&gt;</span> <span class="err">?</span> <span class="p">|</span> <span class="n">clarification</span><span class="p">:</span> <span class="n">when</span> <span class="n">I</span> <span class="n">ask</span> <span class="n">for</span> <span class="n">opposite</span><span class="p">,</span> <span class="n">I</span> <span class="n">want</span> <span class="n">an</span> <span class="n">antonym</span><span class="p">.</span>
<span class="o">#</span>
<span class="n">the</span> <span class="n">antonym</span> <span class="n">for</span> <span class="n">citation</span> <span class="n">is</span> <span class="n">award</span> <span class="kr">END</span>
</code></pre></div>

<p>Figure 13: The prompt used for our tasks. During inference, an input question $\mathbf{x}<em i="i">{i}$, and optionally a feedback $\mathbf{f b}</em>}$ is appended after this prompt, and the model is expected to generate the answer $\mathbf{y<em i="i">{i}$ and its understanding of the question intent $\mathbf{u}</em>$ are expressed together as a single sentence, e.g., "[The synonym for <word> is] [<word>].")}$ as a continuation. The prompt contains examples of the form $(\mathbf{x} \rightarrow \mathbf{u}, \mathbf{y})$, expressed " $\mathbf{x} # \mathbf{u} \mathbf{y}$ END #", and ( $\mathbf{x}, \mathbf{f b} \rightarrow \mathbf{u}, \mathbf{y})$, expressed " $\mathbf{x} \mid$ clarification: $\mathbf{f b} # \mathbf{u} \mathbf{y}$ END #". ( $\mathbf{u}$ and $\mathbf{y</p>
<p>Find the right word after removing random letters from $&lt;\mathrm{t}$ !r/e/a/s/u/r.e!s $&gt;$
#
the word after removing symbols from $\mathrm{t}!r / \mathrm{e} / \mathrm{a} / \mathrm{s} / \mathrm{u} / \mathrm{r} . \mathrm{e}$ !s is treasures END
#
Find the original word after ignoring the punctuation and spaces in $&lt;\mathrm{e}&gt;$
#
the word after removing symbols from e is elders END
#
Find the right word given this cycled word: &lt; lprovisiona &gt; ?
#
the uncycled version of lprovisiona is provisional END
#
Make a word while keeping the first and last char &lt; vosiin &gt; ?
#
the anagram 1 for vosiin is vision END
#
Find the original word that is interspersed in &lt; f.i.n!e/p.i/x &gt;
#
the word after removing symbols from f.i.n!e/p.i/x is finepix END
#
Find the right word given this rotated word: &lt; cturalarchite &gt; ?
#
the uncycled version of cturalarchite is architectural END
#
Find the original word after ignoring the punctuation and spaces in &lt; s.e!n.t.i.n/e/l &gt; #
the word after removing symbols from s is sentinel END
#
Find the right word given this rotated word: &lt; ibitioninh &gt; ?
#
the uncycled version of ibitioninh is inhibition END
#
Figure out the word which has the same first two and the last two char &lt; watsed &gt; ? I clarification: when I want you to figure out the word which has the same first two and the last two char, I mean anagram 2.
#
the anagram 2 for watsed is wasted END
#
Make a word while keeping the first and last char &lt; isucnase &gt; ? I clarification: when I want you to make a word while keeping the first and last char, I mean anagram 1.
#
the anagram 1 for isucnase is issuance END
#
Unscramble everything except the first two and the last two char &lt; acotrs &gt; ? I clarification: when I want you to unscramble everything except the first two and the last two char, I mean anagram 2.
#
the anagram 2 for acotrs is actors END
Figure 14: The prompt used for our word scrambling tasks derived from Brown et al. (2020)</p>
<p>Situation: last day of high school food fight and I nailed a special needs girl square in the face with a heavy muffin.
Morality Judgment: This question is about: getting into food fights with people. The answer is It's bad. END
#
Situation: when phone shopping at stores, I used to write inappropriate stuff and post it to random peoples book of faces accounts who would forget to logoff on the display phones. This question is about: messing with people's social media accounts.
Morality Judgment: This question is about: messing with people's social media accounts. The answer is It's bad. END
#
Situation: i had an uncomfortable talk with my 10yo about puberty and feel like avoiding him for a few days.
Morality Judgment: This question is about: being honest with your kids. The answer is It's good. END
#
Situation: every time I see my three year old nephew being funny or I'm having a good time with him, my mind goes to a school shooting or car crash or kidnapping or some other horrible atrocity happening to him. I have absolutely no idea how to handle this and I'm begging for help.
Morality Judgment: This question is about: admitting you imagine bad things happening to a child and that you need help with this. The answer is It's good. END
#
Situation: i see someone who's not there. he's in my head. he screams sometimes when I'm too stressed and the only way I can get him to leave me alone, is when I cut myself .
Morality Judgment: This question is about: asking someone close to you who is having mental health issues how you can help them. The answer is It's okay. END
#
Situation: i heard once that if you cringe about your past it just means you've grown as a person, for anyone who needs to hear it. This question is about: regretting poor decisions from your past.
Morality Judgment: This question is about: regretting poor decisions from your past. The answer is It's okay. END
#
Situation: my cat hopped up in my lap, cleaned herself and went to sleep. now I'm a captive and I want to move but I can't. This question is about: waking up an animal because you would like to move.
Morality Judgment: This question is about: waking up an animal because you would like to move. The answer is It's okay. END
#
Situation: if it wasn't for the pain it would cause to my family, I would have thrown myself off a bridge a while ago.
Morality Judgment: This question is about: caring about your family's feelings. The answer is It's good. END
#</p>
<p>Figure 15: The prompt used for ERT-NL task. The user provides a situation and the model generates a morality judgement. In addition to the morality judgment, the model also provides a high-level understanding of what the task is about.</p>
<p>Situation: i am selectively racist towards "generic" indian recruiters because they waste my time with absolute zero results.
Morality Judgment: This question is about: trating you differently because of your race. The answer is It's bad. END
#
Situation: i told my husband I didn't feel well so I could stay in bed and play red dead redemption 2 all day and night. This question is about: playing hooky every once in awhile.
Morality Judgment: This question is about: playing hooky every once in awhile. The answer is It's bad. END
#
Situation: I've misunderstood the phrase 'when the going gets tough the tough get going' for my entire life. This question is about: not understanding cliches when young.
Morality Judgment: This question is about: not understanding cliches when young. The answer is It's okay. END
#
Situation: when phone shopping at stores, I used to write inappropriate stuff and post it to random peoples book of faces accounts who would forget to logoff on the display phones. This question is about: messing with people's social media accounts.
Morality Judgment: This question is about: messing with people's social media accounts. The answer is It's bad. END
#
Situation: i had an uncomfortable talk with my 10yo about puberty and feel like avoiding him for a few days.
Morality Judgment: This question is about: being honest with your kids. The answer is It's good. END
#
Situation: every time I see my three year old nephew being funny or I'm having a good time with him, my mind goes to a school shooting or car crash or kidnapping or some other horrible atrocity happening to him. I have absolutely no idea how to handle this and I'm begging for help.
Morality Judgment: This question is about: admitting you imagine bad things happening to a child and that you need help with this. The answer is It's good. END
#
Situation: i heard once that if you cringe about your past it just means you've grown as a person, for anyone who needs to hear ito. This question is about: regretting poor decisions from your past.
Morality Judgment: This question is about: regretting poor decisions from your past. The answer is It's okay. END
#
Situation: my cat hopped up in my lap, cleaned herself and went to sleep. now I'm a captive and I want to move but I can't. This question is about: waking up an animal because you would like to move.
Morality Judgment: This question is about: waking up an animal because you would like to move. The answer is It's okay. END
#
Situation: if it wasn't for the pain it would cause to my family, I would have thrown myself off a bridge a while ago.
Morality Judgment: This question is about: caring about your family's feelings. The answer is It's good. END</p>
<p>Figure 16: The prompt used for ERT-CAT task. The user provides a situation and the model generates a morality judgement. In addition to the morality judgment, the model also provides a high-level understanding of what the task is about.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://beta.openai.com/docs/ introduction, we use 'text-davinci-001'&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ social norms dataset (social-chemistry-101, Forbes et al. (2020)) https://github.com/mbforbes/ social-chemistry-101&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
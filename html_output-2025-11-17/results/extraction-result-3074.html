<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3074 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3074</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3074</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270562757</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12442v2.pdf" target="_blank">Abstraction-of-Thought Makes Language Models Better Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks to bridge this gap by introducing a novel structured reasoning format called Abstraction-of-Thought (AoT). The uniqueness of AoT lies in its explicit requirement for varying levels of abstraction within the reasoning process. This approach could elicit language models to first contemplate on the abstract level before incorporating concrete details, which is overlooked by the prevailing step-by-step Chain-of-Thought (CoT) method. To align models with the AoT format, we present AoT Collection, a generic finetuning dataset consisting of 348k high-quality samples with AoT reasoning processes, collected via an automated and scalable pipeline. We finetune a wide range of language models with AoT Collection and conduct extensive evaluations on 23 unseen tasks from the challenging benchmark Big-Bench Hard. Experimental results indicate that models aligned to AoT reasoning format substantially outperform those aligned to CoT in many reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3074.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3074.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstraction-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constrained Chain-of-Thought style format that explicitly requires multi-level abstraction: a high-level abstract plan (level 1) followed by lower-level concrete steps (level 2). Introduced and used in this paper to elicit abstract reasoning and as the target format for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (Llama-2, CodeLlama, Llama-3, Mistral, GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A range of pre-trained LMs were evaluated: open-source Llama-family and Mistral (7B–8B class) and proprietary GPT-3.5/GPT-4; models were instruction- or AoT/CoT-finetuned using Megatron-LM. A hybrid training strategy mixed natural-language and programmatic (Python) AoT rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–8B for open-source; GPT-3.5/GPT-4 proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Abstraction-of-Thought (AoT)', 'Chain-of-Thought (CoT)', 'AoT prompting (few-shot)', 'CoT prompting (few-shot)', 'Hybrid code + natural language reasoning (programming language rationales)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>AoT: two-level format requiring a succinct abstract purpose/plan per step followed by concrete sub-steps; CoT: unconstrained step-by-step natural-language reasoning. AoT prompting uses demonstration/example rationales in AoT format; CoT prompting uses standard stepwise rationales. Hybrid code reasoning expresses steps as functions/classes and executable code (Python). AoT2CoT and CoT (Ablation) are derived datasets used in format ablations (transforming AoT to CoT or generating CoT with same LLM/hybrid strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse — the paper both (1) contrasts two similar reasoning styles (AoT vs standard sequential CoT) and (2) trains/evaluates models on a diverse set of reasoning representations (natural-language AoT and programmatic/code AoT), i.e., the models are exposed to and can choose between similar styles (CoT-like stepwise) and a diverse hybrid set (text + code with explicit abstraction). The diversity was imposed by prompting and by constructing the AOT COLLECTION training data.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIG-Bench Hard (BBH)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>BBH: a curated subset of 23 challenging BigBench tasks covering algorithmic problems (symbolic/logical) and NLP reasoning problems (semantic, multi-step arithmetic, logical deduction, object counting, etc.). Used as unseen hold-out evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Selected reported metrics (paper-reported): Zero-shot BBH: AoT-finetuned models outperform CoT-finetuned models across studied architectures. Example: Llama-3-8B-AoT shows an absolute +9.7% accuracy vs Llama-3-8B-CoT on BBH (zero-shot). Few-shot prompting: pre-trained Llama-3-8B with AoT few-shot prompts improved by +15.1% absolute over CoT few-shot prompts. Data-scale ablation: 10k AoT finetuning outperforms 348k CoT finetuning in zero-shot BBH. Response-format (Table 5) per-model breakdown (AoT-finetuned): Llama-3-8B-AoT — text responses: Usage Rate 73.4% / Format Correctness 99.3% / Answer Correctness 50.3%; code responses: UR 26.6% / FC 85.1% / AC 53.7%. CodeLlama-7B-AoT — text AC 43.6% (UR84.9%), code AC 61.1% (UR15.1%). Average token lengths: AoT responses ~188.9 tokens vs CoT ~107.1 tokens (Llama-3-8B comparison). Inference cost: AoT generation measured ~1.96s per response on 8x A100-40GB (batch size 1, vllm) as a rough reference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Multiple controlled comparisons/ablations: (1) Direct AoT vs CoT finetuning using the same training questions & scale: AoT outperforms CoT across models. (2) Ablation controlling data-collection LLM and hybrid strategy: AoT (original) > AoT2CoT (same back-end LLM, converted to CoT) > CoT (Ablation) (same back-end LLM producing CoT), showing format (AoT) itself contributes most to gains. (3) Data-scale ablation: AoT advantage persists across training scales; 10k AoT > 348k CoT in zero-shot BBH. (4) Prompting ablation: AoT few-shot prompts elicit stronger performance from pre-trained Llama-3-8B than CoT prompts (+15.1%). (5) Modality analysis: code-form AoT often yields higher per-response accuracy than text-form AoT for code-specialized models (e.g., CodeLlama).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly constraining reasoning format to include an abstract planning level (AoT) consistently improves zero-shot and few-shot reasoning performance relative to unconstrained stepwise CoT, across multiple open-source and proprietary models; the improvement is especially pronounced on algorithmic (symbolic) tasks. The hybrid training (text + code AoT) lets models choose modality; code-mode rationales can be substantially more accurate for code-oriented models. The format (AoT) is the primary driver of gains even when controlling for backend LLM used to synthesize training rationales and for hybrid strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Costs and limits: AoT responses are longer (~80% more tokens) and hence more expensive/slow at inference; some failure modes remain (reasoning errors 38%, lack of knowledge, task misunderstanding). The paper notes instruction-finetuned variants of some models sometimes degrade few-shot performance because they fail to follow demonstration formats. No reported task where CoT-finetuning clearly outperforms AoT given the same conditions in the presented experiments, but resource/compute and pretraining limitations may constrain gains; the paper calls out pretraining as a bottleneck and leaves exploration of larger back-end LLMs and more benchmarks for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abstraction-of-Thought Makes Language Models Better Reasoners', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3074.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3074.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B (AoT vs CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3 8B — AoT-finetuned versus CoT-finetuned evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of AoT-format fine-tuning vs CoT-format fine-tuning on Llama-3-8B evaluated on BBH; used as a primary illustrative example of AoT gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8B-parameter open foundation/model (Meta Llama-3 family) used in pre-trained and instruction-finetuned variants; finetuned with AOT COLLECTION (AoT) and CoT Collection for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['AoT fine-tuning', 'CoT fine-tuning', 'AoT prompting (few-shot)', 'CoT prompting (few-shot)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models were finetuned with supervised examples containing either AoT-format reasoning traces (AOT COLLECTION) or CoT-format traces (CoT Collection). Zero-shot evaluation uses only the test question; few-shot uses 3 demonstrations in AoT or CoT format respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>AoT-finetuned Llama-3-8B is exposed to diverse methods through hybrid AoT (text + code) and can produce either natural language or code reasoning; CoT-finetuned version is trained on CoT-style stepwise rationales (text and code where available).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIG-Bench Hard (BBH) — zero-shot and few-shot evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>23 difficult tasks spanning algorithmic / symbolic and NLP reasoning; BBH was held-out from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot BBH: Llama-3-8B-AoT achieves +9.7 percentage points absolute over Llama-3-8B-CoT (paper example). Few-shot prompting (3-shot): pre-trained Llama-3-8B with AoT prompts yields +15.1 percentage points absolute over CoT prompts (example reported). Response token length: AoT ~188.9 tokens vs CoT ~107.1 tokens (average). Inference generation time reported ~1.96s per AoT response (8x A100-40GB, vllm, batch 1) as rough estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct finetuning comparison (same training questions and scale) shows AoT > CoT. Ablations controlling for collection LLM and hybrid coding also show AoT advantage. Few-shot prompt experiments show AoT-format prompts outperform CoT prompts for pre-trained Llama-3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AoT-finetuned Llama-3-8B substantially outperforms its CoT-finetuned counterpart on BBH in zero-shot, with large few-shot gains for AoT prompting on the pre-trained model; AoT reasoning elicits more abstract planning and improves algorithmic task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>AoT increases token length and computational cost (~80% more tokens), and still exhibits standard LM failure modes (incorrect reasoning, hallucination). Instruction-finetuned variants sometimes perform worse if they do not follow demonstration formats. No explicit case where CoT beat AoT under identical conditions is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abstraction-of-Thought Makes Language Models Better Reasoners', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3074.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3074.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-7B (AoT response breakdown)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama 7B — AoT-finetuning response-mode analysis (text vs code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detailed response-format analysis for CodeLlama-7B after AoT finetuning: measures usage rate, format correctness, and answer correctness separately for text and code reasoning outputs on BBH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter model specialized for code and code+NL tasks (CodeLlama family). Finetuned with AOT COLLECTION (hybrid text+code AoT) and evaluated on BBH.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['AoT in natural language (text)', 'AoT in programming language (Python code) — executable rationales']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>AoT responses were allowed to be either natural-language step/abstraction pairs or pieces of annotated Python code (functions/classes with annotations) which are executed to obtain answers. The paper measures how often models choose code vs text (Usage Rate) and the accuracy per modality.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse: the model can respond in similar style (AoT text) or a different style (AoT code), enabling cross-modality reasoning comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIG-Bench Hard (BBH) zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same 23 challenging BBH tasks (held-out from training). Some tasks are more naturally solved in code (e.g., algorithmic tasks), others in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>From Table 5 (paper): CodeLlama-7B-AoT — text responses: Usage Rate 84.9%, Format Correctness 99.8%, Answer Correctness 43.6%; code responses: Usage Rate 15.1%, Format Correctness 96.9%, Answer Correctness 61.1%. This shows lower overall usage of code but higher code accuracy for CodeLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>For CodeLlama, AoT expressed as executable code yields substantially higher per-response accuracy (61.1%) than AoT expressed in text (43.6%), despite code being used less frequently. This supports the benefit of hybrid (diverse) methods: the model can leverage code when appropriate to improve correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code-mode AoT rationales can deliver higher accuracy on BBH for a code-specialized model, indicating that providing diverse reasoning modalities (text + executable code) enables better performance on algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Code-mode usage is low (15.1% of responses), so model must decide when to use code; when it does, results are better but overall system behavior depends on modality selection. Some models (non-code-specialized) prefer text and may achieve lower code accuracy or lower code-usage FC (format correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abstraction-of-Thought Makes Language Models Better Reasoners', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3074.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3074.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AoT format ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation experiments: AoT vs AoT2CoT vs CoT (Ablation) and data-scale ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled ablations isolating the effect of reasoning format (AoT) from confounders like collector LLM and hybrid coding; also evaluates training data scale (10k–348k).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (examples tested with Llama-3-8B in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ablations used the same backbone models (e.g., Llama-3-8B) finetuned on the same sampled training questions (10k) but with reasoning traces in different formats collected/generated with the same back-end LLM (GPT-3.5-Turbo) and same hybrid strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (example)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['AoT (AOT COLLECTION)', 'AoT2CoT (transform AoT->CoT using same back-end LLM)', 'CoT (Ablation) (directly generate CoT with same back-end LLM)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>AoT2CoT: convert existing AoT-format rationale into CoT while preserving content (using GPT-3.5-Turbo). CoT (Ablation): generate CoT directly with the same back-end LLM and hybrid strategy. These ensure the only major variable is the structural format of the reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>These experiments control for diversity of method generation (same backend and hybrid options) to confirm whether the structural difference (AoT vs CoT) is the causal factor.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BBH zero-shot (ablation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same BBH tasks; ablation performed on 10k training samples sampled from AOT COLLECTION (5k text + 5k code).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Ablation outcome (paper summary): AoT > AoT2CoT > CoT (Ablation) when finetuned on the same 10k questions. Data-scale ablation: AoT finetuned on 10k still outperforms CoT finetuned on 348k in zero-shot BBH.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Because AoT still outperforms AoT2CoT and CoT(Ablation) when the same back-end LLM and hybrid strategy are used, the experiments attribute the performance gains to the AoT reasoning format itself rather than to dataset-collection artifacts or the use of programmatic answers alone.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The internal structure of reasoning traces (explicit abstraction level in AoT) is a principal contributor to improved reasoning, independent of LLM used to synthesize training traces or whether traces include code.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The authors note some remaining confounders not exhaustively explored (e.g., using a more powerful back-end LLM for data collection, or further granular control over abstraction optimality per task). They also point out that optimal abstraction level may vary per question and the automated pipeline may not always find optimal abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Abstraction-of-Thought Makes Language Models Better Reasoners', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning <em>(Rating: 2)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
                <li>Efficient tool use with chain-of-abstraction reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3074",
    "paper_id": "paper-270562757",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "AoT",
            "name_full": "Abstraction-of-Thought",
            "brief_description": "A constrained Chain-of-Thought style format that explicitly requires multi-level abstraction: a high-level abstract plan (level 1) followed by lower-level concrete steps (level 2). Introduced and used in this paper to elicit abstract reasoning and as the target format for fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (Llama-2, CodeLlama, Llama-3, Mistral, GPT-3.5, GPT-4)",
            "model_description": "A range of pre-trained LMs were evaluated: open-source Llama-family and Mistral (7B–8B class) and proprietary GPT-3.5/GPT-4; models were instruction- or AoT/CoT-finetuned using Megatron-LM. A hybrid training strategy mixed natural-language and programmatic (Python) AoT rationales.",
            "model_size": "various (7B–8B for open-source; GPT-3.5/GPT-4 proprietary)",
            "reasoning_methods": [
                "Abstraction-of-Thought (AoT)",
                "Chain-of-Thought (CoT)",
                "AoT prompting (few-shot)",
                "CoT prompting (few-shot)",
                "Hybrid code + natural language reasoning (programming language rationales)"
            ],
            "reasoning_methods_description": "AoT: two-level format requiring a succinct abstract purpose/plan per step followed by concrete sub-steps; CoT: unconstrained step-by-step natural-language reasoning. AoT prompting uses demonstration/example rationales in AoT format; CoT prompting uses standard stepwise rationales. Hybrid code reasoning expresses steps as functions/classes and executable code (Python). AoT2CoT and CoT (Ablation) are derived datasets used in format ablations (transforming AoT to CoT or generating CoT with same LLM/hybrid strategy).",
            "diversity_of_methods": "diverse — the paper both (1) contrasts two similar reasoning styles (AoT vs standard sequential CoT) and (2) trains/evaluates models on a diverse set of reasoning representations (natural-language AoT and programmatic/code AoT), i.e., the models are exposed to and can choose between similar styles (CoT-like stepwise) and a diverse hybrid set (text + code with explicit abstraction). The diversity was imposed by prompting and by constructing the AOT COLLECTION training data.",
            "reasoning_task_name": "BIG-Bench Hard (BBH)",
            "reasoning_task_description": "BBH: a curated subset of 23 challenging BigBench tasks covering algorithmic problems (symbolic/logical) and NLP reasoning problems (semantic, multi-step arithmetic, logical deduction, object counting, etc.). Used as unseen hold-out evaluation.",
            "performance_by_method": "Selected reported metrics (paper-reported): Zero-shot BBH: AoT-finetuned models outperform CoT-finetuned models across studied architectures. Example: Llama-3-8B-AoT shows an absolute +9.7% accuracy vs Llama-3-8B-CoT on BBH (zero-shot). Few-shot prompting: pre-trained Llama-3-8B with AoT few-shot prompts improved by +15.1% absolute over CoT few-shot prompts. Data-scale ablation: 10k AoT finetuning outperforms 348k CoT finetuning in zero-shot BBH. Response-format (Table 5) per-model breakdown (AoT-finetuned): Llama-3-8B-AoT — text responses: Usage Rate 73.4% / Format Correctness 99.3% / Answer Correctness 50.3%; code responses: UR 26.6% / FC 85.1% / AC 53.7%. CodeLlama-7B-AoT — text AC 43.6% (UR84.9%), code AC 61.1% (UR15.1%). Average token lengths: AoT responses ~188.9 tokens vs CoT ~107.1 tokens (Llama-3-8B comparison). Inference cost: AoT generation measured ~1.96s per response on 8x A100-40GB (batch size 1, vllm) as a rough reference.",
            "comparison_of_methods": "Multiple controlled comparisons/ablations: (1) Direct AoT vs CoT finetuning using the same training questions & scale: AoT outperforms CoT across models. (2) Ablation controlling data-collection LLM and hybrid strategy: AoT (original) &gt; AoT2CoT (same back-end LLM, converted to CoT) &gt; CoT (Ablation) (same back-end LLM producing CoT), showing format (AoT) itself contributes most to gains. (3) Data-scale ablation: AoT advantage persists across training scales; 10k AoT &gt; 348k CoT in zero-shot BBH. (4) Prompting ablation: AoT few-shot prompts elicit stronger performance from pre-trained Llama-3-8B than CoT prompts (+15.1%). (5) Modality analysis: code-form AoT often yields higher per-response accuracy than text-form AoT for code-specialized models (e.g., CodeLlama).",
            "key_findings": "Explicitly constraining reasoning format to include an abstract planning level (AoT) consistently improves zero-shot and few-shot reasoning performance relative to unconstrained stepwise CoT, across multiple open-source and proprietary models; the improvement is especially pronounced on algorithmic (symbolic) tasks. The hybrid training (text + code AoT) lets models choose modality; code-mode rationales can be substantially more accurate for code-oriented models. The format (AoT) is the primary driver of gains even when controlling for backend LLM used to synthesize training rationales and for hybrid strategy.",
            "counter_examples_or_negative_results": "Costs and limits: AoT responses are longer (~80% more tokens) and hence more expensive/slow at inference; some failure modes remain (reasoning errors 38%, lack of knowledge, task misunderstanding). The paper notes instruction-finetuned variants of some models sometimes degrade few-shot performance because they fail to follow demonstration formats. No reported task where CoT-finetuning clearly outperforms AoT given the same conditions in the presented experiments, but resource/compute and pretraining limitations may constrain gains; the paper calls out pretraining as a bottleneck and leaves exploration of larger back-end LLMs and more benchmarks for future work.",
            "uuid": "e3074.0",
            "source_info": {
                "paper_title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama-3-8B (AoT vs CoT)",
            "name_full": "Meta Llama-3 8B — AoT-finetuned versus CoT-finetuned evaluation",
            "brief_description": "Empirical comparison of AoT-format fine-tuning vs CoT-format fine-tuning on Llama-3-8B evaluated on BBH; used as a primary illustrative example of AoT gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B",
            "model_description": "An 8B-parameter open foundation/model (Meta Llama-3 family) used in pre-trained and instruction-finetuned variants; finetuned with AOT COLLECTION (AoT) and CoT Collection for experiments.",
            "model_size": "8B",
            "reasoning_methods": [
                "AoT fine-tuning",
                "CoT fine-tuning",
                "AoT prompting (few-shot)",
                "CoT prompting (few-shot)"
            ],
            "reasoning_methods_description": "Models were finetuned with supervised examples containing either AoT-format reasoning traces (AOT COLLECTION) or CoT-format traces (CoT Collection). Zero-shot evaluation uses only the test question; few-shot uses 3 demonstrations in AoT or CoT format respectively.",
            "diversity_of_methods": "AoT-finetuned Llama-3-8B is exposed to diverse methods through hybrid AoT (text + code) and can produce either natural language or code reasoning; CoT-finetuned version is trained on CoT-style stepwise rationales (text and code where available).",
            "reasoning_task_name": "BIG-Bench Hard (BBH) — zero-shot and few-shot evaluation",
            "reasoning_task_description": "23 difficult tasks spanning algorithmic / symbolic and NLP reasoning; BBH was held-out from training data.",
            "performance_by_method": "Zero-shot BBH: Llama-3-8B-AoT achieves +9.7 percentage points absolute over Llama-3-8B-CoT (paper example). Few-shot prompting (3-shot): pre-trained Llama-3-8B with AoT prompts yields +15.1 percentage points absolute over CoT prompts (example reported). Response token length: AoT ~188.9 tokens vs CoT ~107.1 tokens (average). Inference generation time reported ~1.96s per AoT response (8x A100-40GB, vllm, batch 1) as rough estimate.",
            "comparison_of_methods": "Direct finetuning comparison (same training questions and scale) shows AoT &gt; CoT. Ablations controlling for collection LLM and hybrid coding also show AoT advantage. Few-shot prompt experiments show AoT-format prompts outperform CoT prompts for pre-trained Llama-3-8B.",
            "key_findings": "AoT-finetuned Llama-3-8B substantially outperforms its CoT-finetuned counterpart on BBH in zero-shot, with large few-shot gains for AoT prompting on the pre-trained model; AoT reasoning elicits more abstract planning and improves algorithmic task performance.",
            "counter_examples_or_negative_results": "AoT increases token length and computational cost (~80% more tokens), and still exhibits standard LM failure modes (incorrect reasoning, hallucination). Instruction-finetuned variants sometimes perform worse if they do not follow demonstration formats. No explicit case where CoT beat AoT under identical conditions is reported.",
            "uuid": "e3074.1",
            "source_info": {
                "paper_title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CodeLlama-7B (AoT response breakdown)",
            "name_full": "CodeLlama 7B — AoT-finetuning response-mode analysis (text vs code)",
            "brief_description": "Detailed response-format analysis for CodeLlama-7B after AoT finetuning: measures usage rate, format correctness, and answer correctness separately for text and code reasoning outputs on BBH.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CodeLlama-7B",
            "model_description": "A 7B-parameter model specialized for code and code+NL tasks (CodeLlama family). Finetuned with AOT COLLECTION (hybrid text+code AoT) and evaluated on BBH.",
            "model_size": "7B",
            "reasoning_methods": [
                "AoT in natural language (text)",
                "AoT in programming language (Python code) — executable rationales"
            ],
            "reasoning_methods_description": "AoT responses were allowed to be either natural-language step/abstraction pairs or pieces of annotated Python code (functions/classes with annotations) which are executed to obtain answers. The paper measures how often models choose code vs text (Usage Rate) and the accuracy per modality.",
            "diversity_of_methods": "Diverse: the model can respond in similar style (AoT text) or a different style (AoT code), enabling cross-modality reasoning comparison.",
            "reasoning_task_name": "BIG-Bench Hard (BBH) zero-shot",
            "reasoning_task_description": "Same 23 challenging BBH tasks (held-out from training). Some tasks are more naturally solved in code (e.g., algorithmic tasks), others in text.",
            "performance_by_method": "From Table 5 (paper): CodeLlama-7B-AoT — text responses: Usage Rate 84.9%, Format Correctness 99.8%, Answer Correctness 43.6%; code responses: Usage Rate 15.1%, Format Correctness 96.9%, Answer Correctness 61.1%. This shows lower overall usage of code but higher code accuracy for CodeLlama.",
            "comparison_of_methods": "For CodeLlama, AoT expressed as executable code yields substantially higher per-response accuracy (61.1%) than AoT expressed in text (43.6%), despite code being used less frequently. This supports the benefit of hybrid (diverse) methods: the model can leverage code when appropriate to improve correctness.",
            "key_findings": "Code-mode AoT rationales can deliver higher accuracy on BBH for a code-specialized model, indicating that providing diverse reasoning modalities (text + executable code) enables better performance on algorithmic tasks.",
            "counter_examples_or_negative_results": "Code-mode usage is low (15.1% of responses), so model must decide when to use code; when it does, results are better but overall system behavior depends on modality selection. Some models (non-code-specialized) prefer text and may achieve lower code accuracy or lower code-usage FC (format correctness).",
            "uuid": "e3074.2",
            "source_info": {
                "paper_title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AoT format ablations",
            "name_full": "Ablation experiments: AoT vs AoT2CoT vs CoT (Ablation) and data-scale ablation",
            "brief_description": "Controlled ablations isolating the effect of reasoning format (AoT) from confounders like collector LLM and hybrid coding; also evaluates training data scale (10k–348k).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (examples tested with Llama-3-8B in ablation)",
            "model_description": "Ablations used the same backbone models (e.g., Llama-3-8B) finetuned on the same sampled training questions (10k) but with reasoning traces in different formats collected/generated with the same back-end LLM (GPT-3.5-Turbo) and same hybrid strategy.",
            "model_size": "8B (example)",
            "reasoning_methods": [
                "AoT (AOT COLLECTION)",
                "AoT2CoT (transform AoT-&gt;CoT using same back-end LLM)",
                "CoT (Ablation) (directly generate CoT with same back-end LLM)"
            ],
            "reasoning_methods_description": "AoT2CoT: convert existing AoT-format rationale into CoT while preserving content (using GPT-3.5-Turbo). CoT (Ablation): generate CoT directly with the same back-end LLM and hybrid strategy. These ensure the only major variable is the structural format of the reasoning traces.",
            "diversity_of_methods": "These experiments control for diversity of method generation (same backend and hybrid options) to confirm whether the structural difference (AoT vs CoT) is the causal factor.",
            "reasoning_task_name": "BBH zero-shot (ablation subset)",
            "reasoning_task_description": "Same BBH tasks; ablation performed on 10k training samples sampled from AOT COLLECTION (5k text + 5k code).",
            "performance_by_method": "Ablation outcome (paper summary): AoT &gt; AoT2CoT &gt; CoT (Ablation) when finetuned on the same 10k questions. Data-scale ablation: AoT finetuned on 10k still outperforms CoT finetuned on 348k in zero-shot BBH.",
            "comparison_of_methods": "Because AoT still outperforms AoT2CoT and CoT(Ablation) when the same back-end LLM and hybrid strategy are used, the experiments attribute the performance gains to the AoT reasoning format itself rather than to dataset-collection artifacts or the use of programmatic answers alone.",
            "key_findings": "The internal structure of reasoning traces (explicit abstraction level in AoT) is a principal contributor to improved reasoning, independent of LLM used to synthesize training traces or whether traces include code.",
            "counter_examples_or_negative_results": "The authors note some remaining confounders not exhaustively explored (e.g., using a more powerful back-end LLM for data collection, or further granular control over abstraction optimality per task). They also point out that optimal abstraction level may vary per question and the automated pipeline may not always find optimal abstractions.",
            "uuid": "e3074.3",
            "source_info": {
                "paper_title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning",
            "rating": 2,
            "sanitized_title": "the_cot_collection_improving_zeroshot_and_fewshot_learning_of_language_models_via_chainofthought_finetuning"
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Efficient tool use with chain-of-abstraction reasoning",
            "rating": 1,
            "sanitized_title": "efficient_tool_use_with_chainofabstraction_reasoning"
        }
    ],
    "cost": 0.01788975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Abstraction-of-Thought Makes Language Models Better Reasoners
26 Sep 2024</p>
<p>Ruixin Hong 
Institute for Artificial Intelligence
Tsinghua University (THUAI)</p>
<p>Beijing National Research Center for Information Science and Technology (BNRist)</p>
<p>Department of Automation
Tsinghua University
BeijingP.R.China</p>
<p>Tencent AI Lab
Seattle</p>
<p>Hongming Zhang hongmzhang@tencent.com 
Tencent AI Lab
Seattle</p>
<p>Xiaoman Pan 
Tencent AI Lab
Seattle</p>
<p>Dong Yu 
Tencent AI Lab
Seattle</p>
<p>Changshui Zhang 
Institute for Artificial Intelligence
Tsinghua University (THUAI)</p>
<p>Beijing National Research Center for Information Science and Technology (BNRist)</p>
<p>Department of Automation
Tsinghua University
BeijingP.R.China</p>
<p>𝑥 
Institute for Artificial Intelligence
Tsinghua University (THUAI)</p>
<p>Beijing National Research Center for Information Science and Technology (BNRist)</p>
<p>Department of Automation
Tsinghua University
BeijingP.R.China</p>
<p>Abstraction-of-Thought Makes Language Models Better Reasoners
26 Sep 20248040E13F8C581421A7708E4EFD2BE741arXiv:2406.12442v2[cs.CL]
reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning.However, eliciting language models to perform reasoning with abstraction remains unexplored.This paper seeks to bridge this gap by introducing a novel structured reasoning format called Abstraction-of-Thought (AoT).The uniqueness of AoT lies in its explicit requirement for varying levels of abstraction within the reasoning process.This approach could elicit language models to first contemplate on the abstract level before incorporating concrete details, which is overlooked by the prevailing step-by-step Chain-of-Thought (CoT) method.To align models with the AoT format, we present AOT COLLECTION, a generic finetuning dataset consisting of 348k high-quality samples with AoT reasoning processes, collected via an automated and scalable pipeline.We finetune a wide range of language models with AOT COLLECTION and conduct extensive evaluations on 23 unseen tasks from the challenging benchmark Big-Bench Hard.Experimental results indicate that models aligned to AoT reasoning format substantially outperform those aligned to CoT in many reasoning tasks. 1 .</p>
<p>Introduction</p>
<p>The complex reasoning ability is one of the longterm pursuits of artificial intelligence.In recent years, language models (LMs) have seen rapid development and achieved impressive performance on a variety of reasoning benchmarks (Brown et al., 2020;OpenAI, 2023).Among the advancements in reasoning methods, the Chain-of-Thought (CoT) technique has emerged as a prominent reasoning tool (Wei et al., 2022c).This technique, when employed in large language models (LLMs), serves as Figure 1: Reasoning with abstraction attempts to answer questions from the perspective of abstract essences, which may be overlooked by step-by-step Chain-of-Thought (CoT) reasoning.The reasoning process with abstraction contains multiple levels of abstract information.The lower levels (blue nodes) are responsible for performing concrete reasoning and are typically rich in detail.Conversely, the higher levels (red nodes) are abstractions of lower levels, organizing the entire reasoning process.</p>
<p>a guide that enables the model to initially generate intermediate reasoning processes before ultimately producing the final answer.The generated reasoning processes significantly help large language models improve their reasoning performance in zero-shot and few-shot scenarios.This discovery has sparked a great deal of research interest and further improvement (Chu et al., 2023;Besta et al., 2024;Kojima et al., 2022;Zhou et al., 2023;Yao et al., 2023).Meanwhile, for medium-scale language models, training models on data containing CoT has been proven to be an effective method to enhance the reasoning performance of language models (Xu et al., 2024;Zelikman et al., 2022;Shridhar et al., 2023;Ho et al., 2023;Fu et al., 2023;Ghosal et al., 2023).</p>
<p>The prevailing CoT paradigm typically follows a sequential, step-by-step reasoning process, which may leave the exploration of abstraction in reasoning largely untapped.Abstraction is the key to human cognition and reasoning (Saitta et al., 2013).When dealing with complex tasks, reasoning with abstraction first tackles the essence and the most crucial parts, then utilizes the abstract skeletal solution as a guide to solve the rest of the problem (Yang, 2012).Abstraction allows reasoning to perform at a higher and more essential level, resulting in a more generalizable and efficient solution.As shown in Figure 1, when dealing with a specific problem of solving a quadratic equation, reasoning with abstraction suggests first deriving the general quadratic formula and then substituting the specific coefficients to get the answer.This approach offers a more fundamental solution, applicable to problems with similar reasoning patterns.The existing CoT, in the absence of explicit constraints, does not guarantee that abstraction will necessarily be incorporated into the problem-solving process.</p>
<p>In this paper, we explore how to elicit language models to reason with abstraction.We propose a novel format of reasoning processes named Abstraction-of-Thought (AoT), which is a constrained variant of the CoT.Reasoning with abstraction typically starts with a rough skeletal solution from an abstraction perspective (e.g., the red nodes in Figure 1(b)), and then gradually concretizes the solution until the problem is solved.We suggest that employing an abstract skeletal solution to organize the entire reasoning process could be the key to eliciting abstract reasoning.Thus, in contrast to the unconstrained CoT, the AoT format explicitly demands the inclusion of varying levels of abstraction throughout the reasoning process.The higher level is an abstraction of the lower level, containing fewer concrete details but stating the objective and functionality of the reasoning step.For example, functions and classes in programs are abstractions of the following specific code fragments, while main claims in argumentative discourses (Cohen, 1987) are abstractions of subsequent supportive evidence.Specifically, we focus on the two-level AoT in this paper, which contains an additional level of abstraction over the CoT, to serve as an preliminary exploration of whether abstraction helps language models reasoning.</p>
<p>To align language models to the AoT format, we present the AOT COLLECTION, a supervised finetuning dataset that augments 348k AoT reasoning processes from the FLAN Collection (Longpre et al., 2023).AOT COLLECTION covers 216 generic tasks that are not specifically designed for a certain domain or dataset.We design an automatic and scalable pipeline to collect high-quality AoT reasoning processes with the involvement of LLMs.In addition to the AoT reasoning processes represented in natural language, the AOT COLLECTION also includes AoT with programming language.Such a hybrid training strategy could not only unleash the potential of the code use, but also allow the flexibility of preferring different reasoning processes for different reasoning problems (Yue et al., 2023).We use AOT COLLECTION to finetune a wide range of language models.We conduct exhaustive experiments on 23 unseen tasks from Big-Bench Hard (Suzgun et al., 2023), a subset of the most challenging reasoning tasks of Big-Bench (Srivastava et al., 2022) that necessitates various reasoning abilities.Experimental results show that AoT makes language models better reasoners.Remarkably, models with AoT-finetuning achieve substantial improvements in both zero-shot and few-shot performance across various reasoning tasks, compared to those with CoT-finetuning.Our findings highlight the potential of AoT in eliciting and training more effective models capable of reasoning with abstraction.</p>
<p>2 Related Work</p>
<p>Chain-of-Thought Prompting</p>
<p>The enlargement of the language model scale brings about emergent abilities including in-context learning (Wei et al., 2022b).To utilize LLMs for reasoning tasks, Wei et al. (2022c) propose CoT prompting, which extends in-context learning with step-by-step reasoning processes to elicit reasoning in LLMs.Subsequently, a substantial number of works based on CoT are proposed to further enhance LLMs' reasoning performance (Chu et al., 2023;Besta et al., 2024;Kojima et al., 2022;Yao et al., 2023).For instance, Chen et al. (2022) and Gao et al. (2023) explore using codes to express the reasoning processes.Most of these existing approaches focus on the content of the prompt (e.g., question decomposition (Zhou et al., 2023)) and the external usage of CoT prompt (e.g., tree searching (Yao et al., 2023)).We focus on the format and internal structure of CoT prompts and propose the Abstraction-of-Thought format, which is complementary to existing approaches.Our approach can potentially be combined with existing methods for further improvement.</p>
<p>Training Language Models for Reasoning</p>
<p>Although LLMs equipped with CoT prompts can achieve advanced reasoning capabilities, there is still a significant gap between open-source smallerscale models and large models.To bridge this gap, a promising and popular approach is finetuning language models to learn reasoning (Xu et al., 2024).A series of studies have found that finetuning models on data containing CoT reasoning processes could enhance the reasoning ability (Xu et al., 2024;Zelikman et al., 2022;Shridhar et al., 2023;Ho et al., 2023;Fu et al., 2023;Ghosal et al., 2023;Hsieh et al., 2023;Magister et al., 2023;Li et al., 2022;Zhu et al., 2023;Yue et al., 2023).The predominant way is to train models on instruction tuning datasets and their enhanced versions.For instance, the instruction tuning dataset FLAN collection (Longpre et al., 2023) includes CoT data on a small subset of tasks to improve the model's performance under CoT prompts.CoT Collection (Kim et al., 2023) further supplements the remaining tasks in FLAN with CoT reasoning processes, covering 1.84 million instances.Orca (Mitra et al., 2023;Mukherjee et al., 2023) enhances FLAN by prompting LLM with task-specific prompts, thereby training smaller models for cautious reasoning.In this paper, we follow previous work to collect data based on FLAN for fair comparison.We design the methodology for gathering AoT reasoning process and collect AOT COLLECTION to facilitate better training of models for reasoning.</p>
<p>Reasoning with Abstraction</p>
<p>Previous works study different aspects of reasoning with abstraction, including entity abstraction (Durme et al., 2009;Song et al., 2011;Gong et al., 2016), event abstraction (He et al., 2022;Wang et al., 2023b), spatial-temporal abstraction (Zhang et al., 2021), and conceptualization abstraction (Zhou et al., 2024).Gao et al. (2024) propose to use abstract placeholders in the reasoning chain and call domain tools to supplement specific knowledge, thus allowing the model to use the tools effectively.Wang et al. (2023a) propose to transform questions into symbolic meta forms, but require complex semantic resolution and rely on specific entities.We explore how to improve the basic abstract reasoning of LMs from the perspective of reasoning formats and training data.</p>
<p>Abstraction-of-Thought (AoT)</p>
<p>The Abstraction-of-Thought (AoT) reasoning format draws inspiration from the human application of abstract thinking to solve complex reasoning and planning problems (Saitta et al., 2013;Yang, 2012).In trying to solve a complex problem, a good strategy adopted by human reasoners is to proceed topdown.They start from a rough abstract solution and then gradually refine it until a concrete solution is found.For instance, when faced with a complex programming task, seasoned programmers often commence by sketching out a rough algorithmic framework and identifying abstract function features.Subsequently, they progressively delve into the specifics of the code implementation.Similarly, in argumentative discourse (Cohen, 1987), the arguer often first presents the main claims, followed by detailed explanations and supportive evidence for these claims.Therefore, complex problem solutions frequently contain information at diverse abstraction levels, which serves to organize the entire reasoning process hierarchically.These high levels (e.g., functions in a coding solution or main claims in an argument) encapsulate the functionality and objectives of lower levels, thereby structuring the internal reasoning process.</p>
<p>We suggest that the explicit inclusion of such multiple levels of abstraction in the reasoning process could be the key to eliciting abstract reasoning in LMs and propose the AoT format (Figure 2).Specifically, an n-step AoT reasoning process can be expressed as
τ AoT = a 1 1 • a 1 2 • • • • • a 1 n .
where • denotes concatenation and a j i is the i-th step in the j-th abstraction level.Each a j i can be expanded to
a j+1 i,1 • • • • • a j+1 i,m ij ,
which includes m ij steps belonging to the (j + 1)-th abstraction level.The high-level part (e.g., a 1 i ) provides the abstract solution (e.g., deriving the general quadratic formula for solving a specific quadratic equation in Figure 1).It focuses on the essence of the problem and ignores inessential details.The low-level part (e.g., a j≥2 i ) further refines it until concrete operations and reasoning can be performed.It contains more details and specific conditions.AoT can be regarded as a constrained variant of CoT, constraining the internal structure of the reasoning process from the perspective of abstract reasoning.</p>
<p>As a preliminary exploration into whether abstract content aids in the reasoning of LLMs, this paper focuses on the two-level AoT.Compared to the unconstrained CoT, the two-level AoT already</p>
<p>CoT with Programming Language:</p>
<p>directions = [(0, 1), (1, 0), (0, -1),(-1, 0)] start, end = None, None for i in range(len(maze)):</p>
<p>for j in range(len(maze)): CoT with Natural Language:
if maze[i][j] == 'S': start = (i, j) elif maze[i][j] == 'E': end = (i, j) queue = deque([(start, 0)]) visited = set([start
Let's think step by step.</p>
<p>We first identify the fruits on the list and include their quantity in parentheses: orange (1), raspberries (5) , plums (3) , grape (1) .Now, let's add the numbers in parentheses: 1 + 5 + 3 + 1 = 10.So the answer is 10.AoT with Natural Language:</p>
<p>AoT with</p>
<p>Step 1: Identify fruits from all items.Of all the items, the orange, raspberry, plum, and grape are fruits.</p>
<p>Step 2: Identify the number of each fruit.</p>
<p>There are 1 orange, 5 raspberries, 3 plums, and 1 grape.</p>
<p>Step 3: Sum the number of fruits.</p>
<p>There are 1 + 5 + 3 + 1 = 10 fruits in total.</p>
<p>Therefore, the answer is \boxed{10}</p>
<p>Question 1: I have an orange, five raspberries, two books, three plums, a pencil, and a grape.How many fruits do I have?Here are examples of two-level abstraction AoT.In AoT, the high-level parts (represented in bold red, i.e., a 1 * ) plan and organize the entire reasoning process from an abstract perspective, while low-level parts (i.e., a 2 * , * ) carry out concrete and detailed reasoning steps.The high-level parts are abstractions of the low-level parts, clarifying their functionality and objectives.For clarity, we omit some code snippets in AoT with programming language.</p>
<p>includes an additional level of abstraction, which is sufficient for our investigation.We define the specific AoT format on both natural and programming language, as shown in Figure 2.For natural language, AoT requires (1) a clear division of the reasoning process into steps, with "Step i" as the beginning of the i-th step; (2) stating the abstract purpose of the step (as the first level of abstraction a 1 * ) followed by the specifics in each reasoning step (as the second level of abstraction a 2 * , * ); and (3) placing the reasoning result in "\boxed{}".For programming language, AoT requires (1) explicitly dividing the code into several pieces, each of which is a function or class; (2) annotating the functions or classes to explain their functionality (as a 1 * ); and (3) calling other functions in the main function to solve the problem.The code within the main function is also considered the first level, as it forms the abstract plan for solving the problem.Note that there may be multiple steps of a 2 i, * under a 1 i , depending on the difficulty of the problem.For a 2 i, * , we consider a line as a step, i.e., different a 2 i, * are divided by line breaks.</p>
<p>The AOT COLLECTION 4.1 Overview</p>
<p>To align LMs to the AoT reasoning format, we collect the AOT COLLECTION for finetuning LMs.AOT COLLECTION comprises reasoning processes in AoT format, characterized by the following features: (1) Hybrid reasoning strategy.AOT COLLECTION incorporates reasoning processes expressed in both natural language and programming language.Inspired by previous work (Gao et al., 2023;Chen et al., 2022), solutions to some complex problems are more appropriately to be expressed with programming language.This approach not only facilitates the use of external modules as tools to improve reasoning accuracy (Yang et al., 2024), but also enhances the faithfulness of the reasoning process (Lyu et al., 2023).Models trained on such hybrid reasoning strategies can flexibly choose the suitable reasoning approach based on the type of test question, thereby handling more complex reasoning problems.(2) Broad task coverage.AOT COLLECTION is built upon an instruction-tuning dataset that covers a wide range of tasks.This allows the AOT COLLECTION to encompass questions from various tasks, rather than being confined to a specific domain or task.(3) Scalability.AOT COLLECTION is collected through an automated pipeline, which allows it to scale up without relying on the efforts of human annotation.</p>
<p>Source Dataset Selection</p>
<p>We follow previous work (Kim et al., 2023;Mitra et al., 2023) to use FLAN Collection (Longpre et al., 2023) as our source dataset.FLAN Collection includes diverse NLP datasets sourced from P3 (Sanh et al., 2022), Super-NaturalInstructions (Wang et al., 2022), Flan (Wei et al., 2022a), and additional dialogue and code datasets.We follow Kim et al. (2023) to exclude datasets whose data are not publicly accessible and datasets with an excessive number of tokens.Ultimately, we focus on 216 datasets that are consistent with the CoT Collection (Kim et al., 2023).These tasks cover different types of tasks in many domains (Appendix A.1).We manually divide the 216 datasets into two parts, AoT-Text (203 datasets that are more suitable to be solved in natural language) and AoT-Code (13 datasets that are more suitable to be solved in programming language).We utilize a proportional stratified sampling method to sample 400k instances from the original data for the subsequent AoT response generation.Details of the dataset division can be found in Appendix A.1.</p>
<p>AoT Response Generation</p>
<p>While manually annotating the AoT reasoning process for all instances could yield higher quality, it is time-consuming and labor-intensive, and thus difficult to scale up.We adopt a method of synthe- sizing data with LLMs (Kim et al., 2023;Liu et al., 2024) to automate the process of collecting AoT responses.We first manually create instructions and 3 demonstrations, to exploit the instructionfollowing and in-context learning capabilities of LLM for generating AoT responses (details can be found in Appendix A.1).To minimize the difficulty of generation, we only consider 2 levels of abstraction2 .We designed two types of prompts for the datasets in AoT-Text and AoT-Code, respectively.The correct answer to the question is included in the prompt to help the model focus on the generation of the reasoning process.We use GPT-3.5-Turbo as our back-end LLM and generate with greedy decoding.Since we are more concerned about the impact of the reasoning format on the model's reasoning ability, we do not meticulously design different demonstrations for each dataset like previous work (Kim et al., 2023;Mitra et al., 2023).We use Python as the programming language.</p>
<p>Data Validation and Filtering</p>
<p>After generating the AoT response, we perform validation and filtering to ensure high quality.For AoT in natural language, we examine whether the answers predicted in the response are consistent with gold answers.To prevent degeneration where different inference steps describe the same content, we stipulate that there should not be excessive similarity between different steps.Specifically, we calculate the Jaccard similarity of words between different steps and require it to be below a threshold of 0.5.For AoT in the programming language format, we execute the code provided in the response and check whether it correctly prints or returns the gold answer.For instances that fail to meet the requirements, we ask the LLM to regenerate 10 times (with a temperature parameter of 0.7).We retain the first response that meets the requirement.</p>
<p>If none of the 10 responses meet the requirement, we discard the instance.After the filtering process, we retain 348k instances.We randomly sampled 100 examples (50 in natural language and 50 in programming languages) and manually checked the quality of the AoT responses.We find that 96% of AoT responses are valid.Table 1 reports the statistics and Appendix A.1 lists some samples of AOT COLLECTION.</p>
<p>Experiments</p>
<p>• Evaluation Dataset.We evaluate with the challenging reasoning benchmark BIG-Bench Hard (BBH) (Suzgun et al., 2023), which is the most challenging subset of BIG-Bench (Srivastava et al., 2022).BBH consists of 23 tasks that are specifically selected for their difficulty for LMs.BBH covers a wide range of reasoning challenges, including semantic reasoning (e.g., Movie Recommendation), numerical reasoning (e.g., Multi-Step Arithmetic), logical reasoning (e.g., Logical Deduction), and combinations of some of these abilities (e.g., Object Counting).Furthermore, the FLAN Collection takes BBH as Held-Out tasks, which ensures that our finetuning process does not access the evaluation data.We report the average accuracy across 12 NLP tasks (NLP), 11 algorithm tasks (Alg), and all 23 tasks (All), respectively.Details about BBH are in Appendix A.7.</p>
<p>• Setting and Baselines.We finetune LMs with AOT COLLECTION and evaluate their reasoning ability.Following previous work (Kim et al., 2023;Mitra et al., 2023), we focus our evaluation primarily on the zero-shot setting.The zero-shot setting represents the realistic scenario, as in practical applications we do not have prior knowledge (e.g., few-shot demonstrations) about the test questions.We compare the CoT-finetuned version (e.g., Llama-3-8B-CoT) and AoT-finetuned version (e.g., Llama-3-8B-AoT) of LMs.For CoT-finetuning, we replace the reasoning processes in AOT COL-LECTION with the CoT rationales provided by the CoT Collection (Kim et al., 2023).We also report the performance of the instruction-finetuned LMs (e.g., Llama-3-8B-Instruct) as a reference.</p>
<p>• Models.We consider a range of common open source pre-trained language models, including Llama-2 (Touvron et al., 2023), CodeLlama (Rozière et al., 2023), Llama-3 (AI@Meta, 2024), Mistral (Jiang et al., 2023).We also report the performance of GPT-3.5-Turbo-0125(Peng et al., 2023) and GPT-4-0613 (OpenAI, 2023).Details about  the models can be found in Appendix A.2.</p>
<p>• Implementation Details.We train all models with Megatron-LM (Shoeybi et al., 2019).We use a learning rate of 2e-6 for 1 epoch by default.We set the global batch size to 128 and use a cosine decay scheduler.We use greedy decoding for all results, with the maximum sequence length set to 2,048.</p>
<p>For the response in natural language, we extract the contents within "\boxed{}" as the predicted answer.For the response in the programming language format, we execute the program and take the printed output as the predicted answer.</p>
<p>Zero-Shot Performance</p>
<p>Table 2 presents the zero-shot BBH performance of LMs finetuned in different ways.The AoTfinetuned models demonstrate a remarkable performance enhancement compared to the baselines.For example, in comparison to Llama-3-8B-CoT, Llama-3-8B-AoT achieves an absolute improvement of +9.7%.This trend is consistent across all models studied, which suggests that aligning models with the AoT reasoning format could enhance their reasoning ability more effectively than aligning them to the CoT.AoT guides models to think from a more abstract perspective first, addressing the essence of the problem at a higher level, which Table 3: Evaluation performance (%) on BBH with 3shot prompting (standard setting).We prompt models with 3 demonstrations in CoT/AoT reasoning format.</p>
<p>potentially enhances their reasoning ability.</p>
<p>It is worth noting that the AoT-finetuned models displayed more substantial improvements in algorithmic tasks than in NLP tasks.The algorithmic tasks, which require capturing the internal reasoning rules of the questions without relying on external knowledge, pose a greater challenge to the model's reasoning ability (Suzgun et al., 2023).As a result, the performance on algorithmic tasks is usually lower.In contrast, NLP tasks depend not only on reasoning ability but also on external knowledge, where LMs could face the bottleneck in external knowledge.AoT-finetuning guides the model to carry out reasoning with abstraction, with a focus on strengthening the reasoning ability, thus yielding a more noticeable improvement in algorithmic tasks.We further discuss the performance of subtasks (B.1), case study (B.3), and computational cost comparison (B.4) in the Appendix.</p>
<p>Few-Shot Performance</p>
<p>We also evaluate the effect of AoT under the fewshot setting, the standard setting proposed by Suzgun et al. (2023).For few-shot CoT demonstrations, we use three questions and their CoT ratio- Table 4: Ablation on the reasoning format of training data.We finetune models with 10k questions with reasoning processes in different format and report their zero-shot performance.Data with * are collected with the same back-end LLM and hybrid reasoning strategy.</p>
<p>nales which are provided by the official repository.For the AoT prompt, we employ the same questions and manually create the AoT rationales.Consistent with our AOT COLLECTION, we use the Python program to express the reasoning process for some tasks.Prompts can be found in Appendix A.4.</p>
<p>Table 3 shows the results.For models that have not been finetuned, using prompts in AoT format achieves remarkable performance improvement compared to those in the CoT format.For example, the pre-trained Llama-3-8B achieves an absolute improvement of 15.1% with the AoT prompting.This suggests that AoT could be more effective in stimulating the reasoning ability of pre-trained LMs.Furthermore, by aligning the pre-trained models to AoT with the AOT COLLECTION, the models demonstrate improved performance under AoT prompts, validating the effectiveness of our AOT COLLECTION.Further discussions including the few-shot performance of instruction-finetuned models are in Appendix B.2.</p>
<p>Ablation Study on Reasoning Format</p>
<p>In preceding experiments, we utilize the CoT Collection (Kim et al., 2023) for the CoT-finetuning.CoT Collection differs from our AOT COLLEC-TION in two confounding factors, besides the reasoning format: (1) A different LLM is used during data collection; (2) AOT COLLECTION additionally employs the hybrid reasoning strategy, representing reasoning both in text and code.To verify the role of the AoT format, we conduct an ablation study to attempt to eliminate the influence of these confounding factors.We construct a new training dataset, AoT2CoT, which uses the same LLM (i.e., GPT-3.5-Turbo) as the AOT COLLEC-TION to collect data and also adopts the hybrid reasoning strategy.Specifically, we prompt the LLM to convert the reasoning processes of AOT COLLECTION from AoT into CoT while keeping the main reasoning content the same.Meanwhile, we also compare with CoT (Ablation), where we use the same back-end LLM to generate CoT directly, adopting the same hybrid reasoning strategy and filtering approach as for AoT data.Considering the computational cost of the LLM, we sample 10k data from AOT COLLECTION, and carry out the ablation experiment on these data.We finetune the models on these same 10k questions, but with reasoning processes in different formats: (1) CoT from CoT Collection, (2) CoT (Ablation), (3) AoT2CoT, and (4) AoT from AOT COLLECTION.More implementation details can be found in the Appendix A.5.</p>
<p>As shown in Table 4, among all formats of training data, AoT achieves the best results over the CoT format.With the same back-end LLM and hybrid reasoning strategy, AoT still outperforms AoT2CoT and CoT (Ablation).This demonstrates that the main factor contributing to our method's improvement is the reasoning format.</p>
<p>Ablation Study on Training Data Scale</p>
<p>To investigate the impact of the training data scale, we train models with different numbers of training samples.As shown in Figure 3, the AoT-finetuned models achieve a steady improvement as the scale of training data increases.Across all scales, AoTfinetuned models outperform CoT-finetuned models.Moreover, finetuning using 10k AoT data can yield desirable performance, even better than the case of finetuning using 348k CoT data.This demonstrates both the effectiveness and efficiency of our AOT COLLECTION.</p>
<p>Response Analysis</p>
<p>We analyze the responses of AoT-finetuned models from the following perspectives.</p>
<p>• Usage Rate (UR): The proportion of the text/code being utilized among all responses.</p>
<p>• Format Correctness (FC): The proportion of responses that contain/output predicted answers and follow the AoT format among text/code responses.</p>
<p>• Answer Correctness (AC): The proportion of responses whose predicted answers are correct among text/code responses.Table 5 presents the results of the responses on BBH in the zero-shot setting.We can find that models prefer to reason with natural language for most problems, rather than programming language.For example, only 26.6% of responses from Llama-3-8B-AoT use code.Moreover, reasoning with code often achieves higher accuracy than reasoning with text.The accuracy of code responses from CodeLlama-7B-AoT reaches 61.1%, whereas text responses only have an accuracy of 43.6%.</p>
<p>Error Analysis</p>
<p>To better understand the failure modes and future challenges, we randomly sample 100 problems on which Llama-3-8B-AoT fails and manually annotate their error types.Mostly, incorrect reasoning steps constitute the main cause of errors (38%).The model also suffers from lack of necessary knowledge (16%), misunderstanding of tasks (15%), and hallucination (12%).The reasoning process sometimes trivially repeats the conditions (12%) or fails to be executed (5%).Finally, a smaller percentage of correctly predicted answers are misjudged by automated indicators (5%).We present detailed error definitions, examples, and proportions in Appendix B.3 Table 10.</p>
<p>Conclusion</p>
<p>This paper explores how to elicit language models to perform reasoning with abstraction, from the perspective of the reasoning format.We propose a novel structured reasoning format, AoT, which explicitly requires multiple levels of abstraction to be included in the reasoning process.We construct the AOT COLLECTION, containing 348k high-quality AoT reasoning processes, for fine-tuning models to align them to the AoT format.Experimental results show that AoT-finetuned language models can achieve advanced reasoning performance compared to baseline approaches.</p>
<p>Limitations</p>
<p>In this paper, we explore how to elicit language models to perform abstract reasoning from the perspectives of the reasoning format, thereby improving their performance on reasoning tasks.While considerable progress has been made, there are still room for improvement and future research areas worth exploring.</p>
<p>First, due to resource constraints, this paper has not yet explored how to enhance the model's abstract reasoning ability during the pre-training stage.In our experiments, it can be observed that there is a substantial difference in performance among different pre-trained models after AoT-finetuning.This is due to the issue that the finetuning stage might be difficult to fundamentally enhance the intrinsic capabilities of the model (Bai et al., 2022;Ouyang et al., 2022).Therefore, the bottleneck to fundamentally endow models with the ability for abstract reasoning might lie in the pre-training stage.How to inject the ability for abstract reasoning into language models during the pre-training stage presents an intriguing direction for future research.</p>
<p>Second, limited by the cost of API calls, we do not conduct more analysis on the back-end LLM used for data collection.The impact of using a more powerful LLM for data collection or collecting data on a larger scale for AoT-finetuning deserves further exploration.</p>
<p>Third, this paper focuses the evaluation of the model's reasoning ability on the BBH benchmark, which contains 23 test tasks that require various reasoning abilities.However, each task of BBH has its own problem-solving pattern and may favor logical and symbolic reasoning over other benchmarks.Further exploration of generalizability on more realistic and complex reasoning problems is a future research direction.</p>
<p>This paper follows the ACL Code of Ethics.To the best of our knowledge, our work is founda-tional research, and we do not find obvious risks related to fairness considerations, privacy considerations, malicious harmful effects, or environmental impact.</p>
<p>(203 datasets that are more suitable to be solved in natural language) and AoT-Code (13 datasets that are more suitable to be solved in programming language).The AoT-Text consists of the following tasks: adversarial_qa@droberta, olid, ai2_arithmetic_questions, nat-ural_questions, question_&amp;_answer_zre, qanta, health_fact, quail, jeopardy, jigsaw, detoxifying_lms, poki, qnli, cb, google_wellformed_query, subjqa, multi_woz_v2, casino, task_master_input_inversion, babi, wiki_movies, ddo, anli_r1, craigslist_bargains, ohsumed, strategyqa, adversarial_qa@dbidaf, cad, semeval_2018_task3, emo, overruling, hippocorpus, qed, diplomacy_detection, piqa, smcalflow, super_glue@record, schema_guided_dstc8, ai2_arc@ARC-Easy, nlu_asdiv_dataset, ruletaker, news_headlines_dataset_for_sacrasm_detection, com2sense, civil_comments, circa, quartz, diqa, semeval_2019_task_10, hybridqa, evaluation, ecqa, storycommonsense, miscellaneous, snli, clariq, blimp, financial_phrasebank, hatexplain, hope_edi, numersense, x_csr, xcsr, qa_srl, mcscript, mwsc, persent, trivia_qa, hate_speech_offensive, coached_conv_pref, scitail, drop, rte, anli_r3, qrecc, ms_marco, quac, wikitext_103, nlg_bias, mutual, gwsd, yahoo_answers_topics, essential, swag, torque, wiki_dialog_input_inversion, cola, winowhy, disfl_qa, roc_stories, semeval_2020_task_7, codah, mocah, atomic, crows_pair, mnli, tweetqa, scruples, conv_ai_2, stereoset, break, duorc@SelfRC, dialogre, ambigqa, iirc, miam, pubmed_qa, deal_or_no_dialogue, ai2_arc@ARC-Challenge, coda_19, spolin, wiki_hop, hateeval, timetravel, duorc@ParaphraseRC, recepie_nlg, kilt_tasks@hotpotqa, curated_from_stack_overflow___english, sciq, freebase_qa, squad_v2, help!_need_advice_on_identifying_advice, bless, squad_v1, task_master, sbic, quoref, com_qa, wnli, haspart_kb, personachat, argkp, ethos, open_pi, race@high, proto_qa, sarcasm_in_twitter, web_questions, abductive_nli, curiosity_dialogs, imppres, race@middle, adversar-ial_qa@dbert, eurlex, head_qa, defeasible_nli_atomic, eq-uity_evaluation_corpus, qrecc_input_inversion, wiki_dialog, eqasc, bard, wiqa, dream, liar, anli_r2, scitailv1.1, tellmewhy, cod3s, dstc, indian_food_101, aquamuse, glucose, social_i_qa, air_dialogue, missing, narrativeqa, scitldr, mrqa, meta_woz, go_emotions, casehold, scifact, super_glue@boolq, ade_corpus_v2, dailydialog, starcon, commonsenseqa, openbookqa, quarel, propara, event2mind, inquistive, tom_qa, wiki_qa, cosmos_qa, afs, medical_question_pair_dataset, creak, yoruba_bbc_topics, semeval_2020_task4, xl_wic, super_glue@multirc, opp_115, esnli, grailqa, root09, qasper, ropes, gooaq, cos_e, perspectrum, xquad, trianglecopa, mctaco The AoT-Code consists of the following tasks: aqua, big_bench, cfq_mcd1, conala, gsm8k, leetcode, math_dataset, math_qa, mathmatics_dataset, mathqa, prost, svamp, synthetic</p>
<p>We utilize a proportional stratified sampling method to sample 200k instances from the AoT-Code datasets and 200k instances from the AoT-Text datasets.The proportional stratified sampling method ensures that the proportion of each dataset is the same before and after sampling, thus maintaining the overall data distribution.</p>
<p>Listing 1 shows the system prompts we use for collecting AoT responses.We use different prompts for AoT-Text and AoT-Code and clearly state the requirements of AoT format.We also include 3 demonstrations in the prompt.</p>
<p>AOT COLLECTION retains 348k high quality data after filtering.The average number of steps for responses in AoT-Text and AoT-Code is 2.8 and 2.2, respectively.We randomly sample 3 samples from each of AoT-Text and AoT-Code and present them in Listing 2. AOT COLLECTION is in English.</p>
<p>Listing 1: Prompts for collecting AoT responses.We show the system prompt and one of the three demonstrations.Step 2: Analyze each option according to the adjective order rule.(A) Indonesian (origin) triangular (shape) obnoxious (quality) gold (material) red ( color) shoe (noun) (B) obnoxious (quality) triangular (shape) red ( color) Indonesian (origin) gold (material) shoe (noun)</p>
<p>Step 3: Compare the options.Option (A) has the incorrect order because " origin" comes before "shape" and "quality", and "material" comes before "color".Option (B) follows the correct order: quality, shape, color, origin, material.</p>
<p>Therefore, the answer is \boxed{(B)}.Step 3: Compare the options and identify the correct answer.Among the given choices, only birds require nutrients to grow.</p>
<p>================================================</p>
<p>Therefore, the answer is \boxed{birds}.</p>
<p>=================== Sample 2 =================== ** Input ** Two analogies that relate objects to the associated rooms is given in the form "A : B .C : ?". "A : B" relates object A to room B .Your task is to replace the question mark (?) with the appropriate room for the given object C, following the "A : B" relation.</p>
<p>bookshelf : library.car : ?** Output **</p>
<p>Step 1: Identify the relationship between the object and the room in the given analogy.In the analogy "bookshelf : library", the bookshelf is typically found in the library.</p>
<p>Step 2: Apply the same relationship to the second part of the analogy.Since a car is typically found in a garage, we can replace the question mark with "garage".</p>
<p>Therefore, the answer is \boxed{garage}.</p>
<p>=================== Sample 3 =================== ** Input ** I have a new situation: Big town and Fair town both used petroleum products.Big town used them to make plastics and related materials, but didn't use it for gas, because they were environmentally conscious and used only electric vehicles for transportation.Fair town used petroleum for fuel mostly.</p>
<p>But I can use this background: A major use for petroleum products is fuel for cars, trucks, airplanes, trains, and other vehicles.The chemical used are usually a mixture of compounds containing several carbon atoms in a chain.When the material is ignited, a massive amount of gas is created almost instantaneously.This increase in volume will move the pistons in an internal combustion engine to provide power.A jet plane works on a similar principle.Air comes into the front of the engine and mixes with the jet fuel.This mixture is ignited and the gases formed create a lot of pressure to push the plane forward.The idea can be seen even more clearly in the case of a rocket launch.The ignition of the fuel (either solid-state or liquid) creates gases produced under great pressure that pushes the rocket up.</p>
<p>What is an answer for this question: Which town used more fuel for internal combustion engines?</p>
<p>** Output **</p>
<p>Step 1: Extract key information from the situation.Big town uses petroleum products to make plastics and related materials, but not for gas as they use electric vehicles for transportation.Fair town uses petroleum mostly for fuel.</p>
<p>Step 2: Recall the background information.</p>
<p>The background information states that a major use for petroleum products is fuel for vehicles with internal combustion engines.</p>
<p>Step 3: Apply the background information to the situation.Since Big town uses electric vehicles, they do not use petroleum for fuel in internal combustion engines.On the other hand, Fair town uses petroleum mostly for fuel, which implies they use it for internal combustion engines.</p>
<p>Therefore, the answer is \boxed{Fair}.""" The main function that calls the other functions and prints the answer.""" lst = ['D', '9987', '4565', 'j', 'V', 'S', '1033', '7319', 'S'] count = count_numerical_elements(lst) print(count) main()
``À
.2 Models Table 6 lists the models involved in our experiments, including their names, versions, and corresponding URL links.We follow the licences (which can be found in the URL links) of these models to use them.For the open-source models, we use the model weights provided by Huggingface4 .</p>
<p>A.3 Zero-Shot Prompts</p>
<p>In the zero-shot setting, we directly use the test question as the input to the models.For our models with CoT/AoT-finetuning, we are able to extract the predicted answers from the responses in a fixed format.For open-source instruction finetuned models (e.g., Llama-3-8B-Instruct), we utilize a simple yet effective instruction to guide the model to output the answer in a fixed format.The instruction we use is: "Answer the question and put the final answer in \boxed{}."</p>
<p>A.4 Few-Shot Prompts</p>
<p>In the few-shot setting, we conduct experiments with two type of prompts: CoT prompt and AoT prompt.The CoT prompt is collected from the official GitHub repository of BBH5 .For each test task, the CoT prompt contains three demonstration questions along with their CoT rationales.For the AoT prompt, we employ the same questions and manually create the reasoning processes in AoT format.In consistency with the AOT COLLECTION, we use the Python programming language to express the reasoning process for some tasks.</p>
<p>For the following tasks we use a programming language reasoning process: Boolean expressions, Dyck languages, Geometric shapes, Logical deduction five objects, Logical deduction seven objects, Logical deduction three objects, Multistep arithmetic two, Navigate, Object counting, Penguins in a table, Temporal sequences, Tracking shuffled objects five objects, Tracking shuffled objects seven objects, Tracking shuffled objects three objects, Web of lies, Word sorting.</p>
<p>For the following tasks we use a natural language reasoning process: Causal judgement, Date understanding, Disambiguation qa, Formal fallacies, Hyperbaton, Movie recommendation, Reasoning about colored objects, Ruin names, Salient translation error detection, Snarks, Sports understanding.</p>
<p>Listing 3 presents examples of the Few-shot AoT prompts for BBH.Considering that the full prompts for all tasks in BBH is long, we show two demonstrations in the prompts here for brevity (one in natural language and one in programming language).We will release the prompt we used after publication.Step 2: Recall the infomation of the sport.Goal tending is part of basketball.</p>
<p>Step 3: Evaluate the plausibility of the sentence.Given that DeMar DeRozan is a basketball player, it is plausible that he could be called for a goal tend during a game.</p>
<p>Therefore, the answer is \boxed{yes}..5 AoT2CoT</p>
<p>To construct AoT2CoT for ablation study, we use LLM to transform the reasoning process from AoT format to CoT format.We sample 10k of data from the AOT COLLECTION (5k from AoT-Text and 5k from AoT-Code).Our aim is to transform the reasoning format while keeping the reasoning content is roughly the same.We prompt GPT-3.5-Turbo(the same back-end LLM for collecting AOT COL-LECTION) to transform the input reasoning process into CoT format.In order to avoid the model generating new reasoning content in response to the question, we do not input the question and answer, but only the reasoning process.Listing 4 gives the specific prompt we use.</p>
<p>Listing 4: Prompts for constructing AoT2CoT.We show the system prompt and one of the three demonstrations.Step 2: Sort the yardages in descending order.38 yards, 21 yards, 7 yards, 7 yards, 1 yard</p>
<p>Step 3: Identify the second longest touchdown.</p>
<p>The second longest touchdown is 21 yards.</p>
<p>Therefore, the answer is \boxed{21}.CoT (Ablation) and AOT COLLECTION utilize the same back-end LLM for data generation and the same hybrid reasoning strategy.Unlike AoT2CoT, we directly prompt GPT-3.5-Turbo to generate CoT responses.Listing 5 provides the prompt we use.We modify the prompt used for generating AoT responses (Listing 1) by removing the constraints of the AoT format, thereby guiding the model to generate CoT responses.We then apply the same filtering approach before using the data for finetuning.It is worth noting that, for the same question, the reasoning process in CoT (Ablation) may differ from the corresponding AoT reasoning process not only in the reasoning format, but also in the content of the reasoning.This is because we cannot guarantee that the back-end LLM would generate the same reasoning content for the same question in two independent generations.</p>
<p>Listing 5: Prompts for constructing CoT (Ablation).We show the system prompt and one of the three demonstrations..7 BBH Benchmark Table 7 presents the tasks and number of questions within the BBH benchmark.Following Suzgun et al. (2023), tasks are divided into two categories: NLP tasks and algorithm tasks.Tasks "Logical deduction" and "Tracking shuffled objects" consist of 3 sub-tasks.The questions and answers in BBH are in English.</p>
<p>B Additional Experiment Results</p>
<p>B.1 Performances on Subtasks</p>
<p>Table 9 shows the performance on subtasks of BBH.We also introduce human performance as a reference (Suzgun et al., 2023).We can observe that AoT-finetuning compared to cot-finetuning is able to achieve improvements on multiple reasoning tasks on multiple models.</p>
<p>B.2 Few-Shot Performance</p>
<p>Table 8 presents the 3-shot performance on BBH, including those that are instruction-finetuned (such as Llama-3-8B-Instruct).A consistent trend can be observed where instruction-finetuned models achieve lower few-shot performance compared to models that are merely pre-trained.For example, Llama-3-8B-Instruct achieves an overall accuracy of 56.7% with the official few-shot CoT prompt, while Llama-3-8B can achieve 60.0% with the same prompt.For the CodeLlama-7B-Instruct and Llama-2-7B-Chat, a significant performance decrease is noted in the instruction-finetuned versions.Upon examining their responses, we find that in most cases, the models did not follow the demonstration format for answering.For the Llama-3-8B-Instruct and Mistral-7B-Instruct, the responses adhered to the demonstration format.In this scenario, using the AoT prompt yields better results than the CoT prompt.</p>
<p>B.3 Case Study</p>
<p>Listing 6 shows success cases for the AoTfinetuned Llama-3-8B.The AoT-finetuned model demonstrates a certain degree of ability to reason with abstraction.For example, for the Case 3, the model first defines the tools needed to solve the problem at an abstract level, i.e., the Person class and the swap_gifts function.Subsequently, the model then utilizes these tools to solve the problem based on the concrete conditions of the question.Listing 7 presents the error cases.Table 9: Zero-shot performance (%) on each task of BBH.†indicates results from Suzgun et al. (2023).* indicates that the task is an algorithmic task.</p>
<p>Error Type (%) Description</p>
<p>Reasoning Error (38%)</p>
<p>Errors due to deriving incorrect logical conclusions from given conditions.Knowledge Lacking (16%) Lack of world knowledge necessary to solve problems.Task Misunderstanding (15%) Failure to grasp the requirements or objectives of the task.Condition Repetition (12%) Simply repeat the given conditions and then leap directly to the answer.</p>
<p>Hallucination (9%)</p>
<p>Fabrication of non-existent conditions or misinterpretation of given conditions.</p>
<p>Runtime Error (5%) Errors due to inexecutable programs or unformalized responses.</p>
<p>False Negative (5%) Correct answers are incorrectly identified as incorrect.</p>
<p>Table 10: The failure modes of Llama-3-8B-AoT.</p>
<p>ence, as it may be influenced by various factors such as hardware conditions.</p>
<p>Listing 6: Success cases of Llama-3-8B-AoT.Step 3: Compare the options to the correct adjective order.Option (A) follows the correct adjective order: lovely (descriptive) + tiny (size) + old-fashioned ( age) + pyramidal (shape) + Pakistani (origin) + glass (material) + eating (function) + cat ( animal).Option (B) does not follow the correct adjective order: pyramidal (shape) + lovely (descriptive) + old-fashioned (age) + Pakistani (origin) + tiny (size) + eating (function) + glass (material) + cat (animal).</p>
<p>Therefore, the answer is \boxed{(A)}.-Star Wars Episode V -The Empire Strikes Back: A science fiction movie about the ongoing conflict between the Rebel Alliance and the Galactic Empire.-American Beauty: A drama movie about a suburban family and their struggles with life.</p>
<p>-Star Wars Episode VI -Return of the Jedi: A science fiction movie about the final battle between the Rebel Alliance and the Galactic Empire.</p>
<p>Step 2: Identify the common theme or genre.The common theme among the given movies is science fiction.</p>
<p>Step 3: Compare the options.</p>
<p>(A) Frankenweenie: A stop-motion animated film about a boy and his dog, not a science fiction movie.(B) Ivan Vasilievich Back to the Future: A Russian comedy film about a man who accidentally travels back in time, not a science fiction movie.(C) We Bought a Zoo: A drama movie about a family buying a zoo, not a science fiction movie.(D) Star Wars Episode IV -A New Hope: A science fiction movie set in a galaxy far, far away, similar to the given movies.</p>
<p>Step 4: Select the correct answer.</p>
<p>The movie that is similar to Independence Day, Star Wars Episode V -The Empire Strikes Back, American Beauty, and Star Wars Episode VI -Return of the Jedi is Star Wars Episode IV -A New Hope.</p>
<p>Therefore, the answer is \boxed{D}.,7,50,11 Bernard,5,80,13 Vincent,9,60,11 Gwen,8,70,15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.What is the name of the last penguin sorted by alphabetic order?Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James <strong>AoT Prediction</strong> ```python def sort_penguins(penguins):</p>
<p>""" Sort a list of penguins by their names in alphabetical order.""" sorted_penguins = sorted(penguins, key=lambda penguin: penguin[0]) return sorted_penguins def get_last_penguin(sorted_penguins): """ Get the last penguin in the sorted list.""" last_penguin = sorted_penguins[-1] return last_penguin [0] def main(): """ The main function that calls the other functions and prints the answer.""" penguins = [['Louis',7,50,11], ['Bernard',5,80,13], ['Vincent',9,60,11], ['Gwen',8,70,15]] sorted_penguins = sort_penguins(penguins) last_penguin = get_last_penguin(sorted_penguins) options = ['Louis', 'Bernard', 'Vincent', 'Gwen', answer = options.index(last_penguin)+ 1 print(chr(96 + answer)) main() ```L isting 7: Error cases of Llama-3-8B-AoT.-Rashida lies.</p>
<p>-Osvaldo says Rashida tells the truth.</p>
<p>-Sherrie says Osvaldo tells the truth.</p>
<p>-Gwenn says Sherrie tells the truth.</p>
<p>-Jerry says Gwenn lies.</p>
<p>Step 2: Evaluate the statements based on the given information.</p>
<p>-Rashida lies, so her statement is not reliable.</p>
<p>-Osvaldo says Rashida tells the truth, but since Rashida lies, Osvaldo's statement is not reliable.</p>
<p>-Sherrie says Osvaldo tells the truth, but since Osvaldo's statement is not reliable, Sherrie's statement is not reliable.-Gwenn says Sherrie tells the truth, but since Sherrie's statement is not reliable, Gwenn's statement is not reliable.-Jerry says Gwenn lies, but since Gwenn's statement is not reliable, Jerry's statement is not reliable.</p>
<p>Step 3: Determine if Jerry tells the truth.Based on the given information, Jerry's statement is not reliable.</p>
<p>Therefore, the answer is \boxed{no}.Step 2: Recall the definition of a triple in baseball.In baseball, a triple is a hit in which the batter reaches third base safely after hitting the ball.</p>
<p>Step 3: Evaluate the plausibility of the sentence.It is plausible for Courtland Sutton to hit a triple in baseball, as it is a possible outcome for a batter.</p>
<p>Therefore, the answer is \boxed{Yes}.vegetables = ['garlic', 'cabbage', 'celery', 'carrot', 'carrot', 'carrot', 'carrot', 'carrot', 'carrot', 'lettuce', 'broccoli','broccoli', 'broccoli', 'broccoli', 'broccoli', 'broccoli', 'potato', 'onion', 'yam The main function that calls the other functions and prints the answer.""" penguins = ['Louis,7,50,11','Bernard,5,80,13','Vincent,9,60,11','Gwen,8,70,  Step 2: Recall the definition of a "give and go" in football.</p>
<p>A "give and go" is a football technique where a player passes the ball to a teammate, then quickly moves towards the space created by the teammate's run.The teammate then returns the ball to the player who initiated the move.</p>
<p>Step 3: Evaluate the plausibility of the sentence.</p>
<p>Given that Andres Iniesta is a professional football player, it is plausible that he would perform a "give and go" technique during a game.</p>
<p>Therefore, the sentence "Andres Iniesta performed a give and go" is \boxed{plausible}.</p>
<p>= None, None for i in range(len(maze)): … … return start, end def bfs(maze, start, end): """ Performs a breadth-first search (bfs) on the 2D-maze to find the shortest path length from start to end.Given an N*N maze that contains one start point S, one end point E, several empty spaces 0, and several walls 1.The player can move up, down, left, and right, but cannot pass through walls.Find the shortest path length from the start to the end.If the end is unreachable, output -1.Example of the given maze: maze=[['S', 0, 0], [1, 1, 0], [0, 0, 'E']]</p>
<p>Figure 2 :
2
Figure2: Illustration of Abstraction-of-Thought (AoT) format with natural language (upper half) and programming language (lower half).Unlike the unconstrained CoT, AoT explicitly requires that different levels of abstraction be presented in the reasoning process.Here are examples of two-level abstraction AoT.In AoT, the high-level parts (represented in bold red, i.e., a 1 * ) plan and organize the entire reasoning process from an abstract perspective, while low-level parts (i.e., a 2 * , * ) carry out concrete and detailed reasoning steps.The high-level parts are abstractions of the low-level parts, clarifying their functionality and objectives.For clarity, we omit some code snippets in AoT with programming language.</p>
<p>Figure 3 :
3
Figure 3: Zero-shot BBH performance of models trained with different numbers of training samples.</p>
<p>Listing 3 :
3
Examples of Few-shot AoT Prompts ====== AoT Prompt for Sports Understanding ===== ** Demonstration Input ** Is the following sentence plausible?"DeMar DeRozan was called for the goal tend."** Demonstration Output ** Step 1: Recall the infomation of the sport player.DeMar DeRozan is an American basketball player.</p>
<p>Prompt for Converting AoT to CoT with Natural Language ** System Prompt ** You are a text rewriter.Given a step-by-step reasoning process, you need to reorganize its contents to meet the following requirements: -The rewritten text do not add new content and the reasoning logic in it is consistent with the given text.-The rewritten text should be as concise and natural as possible, without any explicit steps or structures.** Demonstration Input ** Step 1: Extract key information about touchdowns from the text.The touchdowns mentioned are: -A 1-yard TD pass to Calvin Johnson -A 7-yard TD pass to Donald Lee -A 7-yard catch by Donald Driver -A 21-yard catch by James Jones -A 38-yard interception return for a touchdown by Charles Woodson</p>
<p>lovely tiny old-fashioned pyramidal Pakistani glass eating cat (B) pyramidal lovely old-fashioned Pakistani tiny eating glass cat <strong>AoT Prediction</strong> Step 1: Identify the correct adjective order.The correct adjective order is: descriptive adjective + size adjective + age adjective + origin adjective material adjective + function adjective + animal adjective.Step 2: Analyze each option.(A) lovely tiny old-fashioned pyramidal Pakistani glass eating cat (B) pyramidal lovely old-fashioned Pakistani tiny eating glass cat</p>
<p>===============================</p>
<p>Case 2 from Task Movie Recommendation =============================== <strong>Input</strong> Question: Find a movie similar to Independence Day, Star Wars Episode V -The Empire Strikes Back, American Beauty, Star Wars Episode VI -Return of the Jedi: Options: (A) Frankenweenie (B) Ivan Vasilievich Back to the Future (C) We Bought a Zoo (D) Star Wars Episode IV -A New Hope <strong>AoT Prediction</strong> Step 1: Analyze the given movies.-Independence Day: A science fiction movie about aliens invading Earth.</p>
<p>==================================== Error Case: Reasoning Error ==================================== <strong>Input</strong> Question: Rashida lies.Osvaldo says Rashida tells the truth.Sherrie says Osvaldo tells the truth.Gwenn says Sherrie tells the truth.Jerry says Gwenn lies.Does Jerry tell the truth?<strong>AoT Prediction</strong> Step 1: Analyze the statements made by each person.</p>
<p>*</p>
<p>Explanation<strong> The model derives incorrect logical conclusions from given conditions.=================================== Error Case: Knowledge Lacking =================================== </strong>Input<strong> Is the following sentence plausible?"Courtland Sutton hit a triple."</strong>AoT Prediction** Step 1: Analyze the sentence structure and meaning.The sentence states that "Courtland Sutton hit a triple."</p>
<p>Table 1 :
1
Statistics of AOT COLLECTION
AoT-Text AoT-CodeTotalNumber of Samples173,100175,463348,563Avg. Question Length179.698.9139.0Avg. AoT Response Length144.6172.2158.5</p>
<p>Table 2 :
2
Evaluation performance (%) on the unseen BBH benchmark under the zero-shot setting (realistic setting).X-AoT/CoT indicates the language model X with AoT/CoT-finetuning.AoT/CoT-finetuning shares the same training questions and training data scale, but differs in the reasoning processes.We mark the improvements of AoT over CoT in parentheses.</p>
<p>Table 5 :
5
Response analysis of AoT-finetuned models.UR=Usage Rate.FC=Format Correctness.AC=Answer Correctness.
TextCodeModelUR FC AC UR FC ACLlama-3-8B-AoT73.4 99.3 50.3 26.6 85.1 53.7CodeLlama-7B-AoT 84.9 99.8 43.6 15.1 96.9 61.1Llama-2-7B-AoT90.3 96.1 34.6 9.7 56.9 24.9Mistral-7B-AoT84.7 99.7 49.1 15.3 87.3 50.4</p>
<p>Table 6 :
6
Detailed information about the models we experiment with.</p>
<p>Table 7 :
7
Tasks in the BBH Benchmark.<em> indicates that the task is an algorithmic task.Untagged tasks belong to NLP tasks.
Task# Questionsboolean_expressions</em>250causal_judgement187date_understanding250disambiguation_qa250dyck_languages<em>250formal_fallacies250geometric_shapes</em>250hyperbaton250logical_deduction<em>logical_deduction_five_objects</em>250logical_deduction_seven_objects<em>250logical_deduction_three_objects</em>250movie_recommendation250multistep_arithmetic_two<em>250navigate</em>250object_counting<em>250penguins_in_a_table146reasoning_about_colored_objects250ruin_names250salient_translation_error_detection250snarks178sports_understanding250temporal_sequences</em>250tracking_shuffled<em>tracking_shuffled_objects_five_objects</em>250tracking_shuffled_objects_seven_objects<em>250tracking_shuffled_objects_three_objects</em>250web_of_lies<em>250word_sorting</em>250</p>
<p>Bob, Claire, Dave, and Eve are holding a white elephant gift exchange.At the start of the event, they are each holding a present of a different color: Alice has a brown present, Bob has a black ball, Claire has a blue present, Dave has a orange ball, and Eve has a red present.As the event progresses, pairs of people swap gifts.First, Claire Alice swap their gifts.Then, Bob and Eve swap their gifts.Then, Dave and Bob swap their gifts.Then, Alice and Claire swap their gifts.Finally, Eve and Dave swap their gifts.At the end of the event,
Question: Alice, Claire has theOptions: (A) brown present (B) black ball (C) blue present (D) orange ball (E) red present<strong>AoT Prediction</strong><code>`pythonclass Person:def __init__(self, name, gift):self.name = nameself.gift = giftdef swap_gifts(person1, person2):"""Swap the gifts between two people."""temp = person1.giftperson1.gift = person2.giftperson2.gift = tempdef main():"""The main function that calls the other functions and prints the answer."""alice = Person('Alice', 'brown')bob = Person('Bob', 'black')claire = Person('Claire', 'blue')dave = Person('Dave', 'orange')eve = Person('Eve', 'red')swap_gifts(claire, alice)swap_gifts(bob, eve)swap_gifts(dave, bob)swap_gifts(alice, claire)swap_gifts(eve, dave)if claire.gift == 'brown':print('A')elif claire.gift == 'black':print('B')elif claire.gift == 'blue':print('C')elif claire.gift == 'orange':print('D')elif claire.gift == 'red':print('E')main()</code>=============================== Case 4 from Task Penguins in a Table ===============================<strong>Input</strong>Question: Here is a table where the first line is a header and each subsequent line is a penguin:name, age, height (cm), weight (kg) Louis============================ Case 3 from Task Tracking shuffled Objects ============================<strong>Input</strong></p>
<p>The model lacks the necessary knowledge that Courtland Sutton usually refers to the American football wide receiver for the Denver Broncos of the National Football League (NFL).Analyze the given statement.The statement is "True or False or not True or False is."Step 2: Evaluate the statement.The statement is grammatically incorrect and does not make sense.It is not clear what the statement is trying to convey.Step 3: Determine the correct answer.Since the statement is not true or false, the correct answer is \boxed{not True or False}.
===================================== Error Case: Hallucination =====================================<strong>Input</strong>I have two garlics, a cabbage, a stalk of celery, five carrots, a lettuce head, four heads ofbroccoli, a potato, an onion, and a yam. How many vegetables do I have?<strong>AoT Prediction</strong>```pythondef count_vegetables(vegetables):<strong>Predicion</strong> """YesCount the number of vegetables in a list.<strong>Ground Truth</strong> """Nocount = len(vegetables)<strong>Error Explanation</strong> return countdef main():================================= Error Case: Task Misunderstanding ================================= """The main function that calls the other functions and prints the answer.<strong>Input</strong> """True or False or not True or False is<strong>AoT Prediction</strong>Step 1: <strong>Predicion</strong>not True or False<strong>Ground Truth</strong>True<strong>Error Explanation</strong>The model fails to understand that the task is to simplify the input boolean expression.================================= Error Case: Condition Repetition ==================================<strong>Input</strong>Which of the following is a humorous edit of this artist or movie name: 'guns n' roses'?Options:(A) guns n' goses(B) suns n' roses(C) guns n' rosjs(D) guns n' rgoses<strong>AoT Prediction</strong>Step 1: Analyze the given options.(A) guns n' goses(B) suns n' roses(C) guns n' rosjs(D) guns n' rgosesStep 2: Identify the humorous edit.The humorous edit of "guns n' roses" is "guns n' goses".Therefore, the answer is \boxed{(A)}.<strong>Predicion</strong>(A)<strong>Ground Truth</strong>(B)<strong>Error Explanation</strong>The reasoning process simply repeats the given conditions and then leap directly to the predictedanswer.</p>
<p>Understand the meaning of the sentence.The sentence "Andres Iniesta performed a give and go" suggests that Andres Iniesta participated in a specific football technique called a "give go".
count = count_penguins_over_age(penguins, age)options = {1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E'}answer = options.get(count, 'Option not found')print(answer)main()``<strong>Predicion</strong>None<strong>Ground Truth</strong>(A)<strong>Error Explanation</strong>The generated program is inexecutable.===================================== Error Case: False Negative ====================================<strong>Input</strong>Is the following sentence plausible? "Andres Iniesta performed a give and go."<strong>AoT Prediction</strong>Step 1:15']age = 8</p>
<p>Table 11 :
11
The datasets included in our AOT COLLECTION, along with their sources and task categories.The sources and task categories are aligned with the FLAN Collection (https://github.com/google-research/FLAN/blob/main/flan/v2/flan_collection_info.csv).</p>
<p>Our data collection process might not guarantee to find the optimal abstraction solution for each question, since the optimal abstraction level may differ across various questions, and a single question may have multiple abstract solutions. We focus on investigating whether valid abstraction can help language models reasoning while deprioritizing the collection of optimal abstraction solutions.
https://huggingface.co/models
https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts
https://github.com/vllm-project/vllm
AcknowledgementsWe appreciate the anonymous reviewers for their insightful comments.This work is supported by National Science and Technology Major Project (No. 2022ZD0114903), the Natural Science Fundation of China (NSFC.No. 62476149) and the Guoqiang Institute of Tsinghua University (No. 2020GQG0005).A Implementation DetailsA.1 AOT COLLECTIONWe follow CoT Collection(Kim et al., 2023)3 to preprocess the FLAN Collection(Longpre et al., 2023)data as our source data.The CoT Collection contains 1.84M samples from 216 tasks.These tasks cover different types of tasks in different domains.For example, dataset "commonsenseqa" in the collection is a "Question Answering" task in the commonsense domain, while dataset "gsm8k" is a "Grade School Math Word Problems" in the math domain.Our AOT COLLECTION is built on these diverse datasets and already covers different tasks in different domains as well, rather than being designed only for a specific domain.Table11at the end of the Appendix gives all the dataset names and their corresponding tasks.We follow the FLAN Collection to determine the tasks corresponding to the datasets.B.4 Computational Cost ComparisonConsidering that our AoT format encompasses multi-level abstractions, the average length of AoT responses tends to be longer than that of CoT responses.Consequently, the AoT-finetuned model may takes a higher computational cost during inference to generate more tokens.To clarify the impact of this in practical applications, we conduct a brief comparison of computational costs.We calculate the average length of responses from Llama-3-8B-CoT and Llama-3-8B-AoT on the BBH questions.On average, the responses of Llama-3-8B-CoT consist of 107.1 tokens, while the responses of Llama-3-8B-AoT consist of 188.9 tokens.Thus, AoT responses takes approximately 80% more tokens than CoT responses.To estimate the generation time during inference, we use the vllm 6 library on a server with 8 A100-SXM4-40GB GPUs, setting the batch size to 1.The average time for Llama-3-8B-AoT to generate one AoT response is 1.96 seconds.However, this estimation should only be taken as a rough refer-
Training a helpful and harmless assistant with reinforcement learning from human feedback. Ai@meta ; Yuntao References, Andy Bai, Kamal Jones, Amanda Ndousse, Anna Askell, Nova Chen, Dawn Dassarma, Stanislav Drain, Deep Fort, Tom Ganguli, Nicholas Henighan, Saurav Joseph, Jackson Kadavath, Tom Kernion, Sheer El Conerly, Nelson Showk, Zac Elhage, Danny Hatfield-Dodds, Tristan Hernandez, Scott Hume, Johnston, 10.48550/ARXIV.2204.05862CoRR, abs/2204.05862Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan2024. 2022Llama 3 model card</p>
<p>Topologies of reasoning: Demystifying chains, trees, and graphs of thoughts. Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, Torsten Hoefler, 10.48550/ARXIV.2401.14295CoRR, abs/2401.142952024</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 10.48550/ARXIV.2211.12588CoRR, abs/2211.125882022</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, 10.48550/ARXIV.2309.15402CoRR, abs/2309.154022023</p>
<p>Analyzing the structure of argumentative discourse. Robin Cohen, Comput. Linguistics. 131-21987</p>
<p>Deriving generalized knowledge from corpora using wordnet abstraction. Benjamin Van Durme, Phillip Michalak, Lenhart K Schubert, EACL 2009, 12th Conference of the European Chapter. Athens, GreeceThe Association for Computer Linguistics2009. March 30 -April 3, 2009Proceedings of the Conference</p>
<p>Specializing smaller language models towards multi-step reasoning. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202of Proceedings of Machine Learning Research</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202of Proceedings of Machine Learning Research</p>
<p>Efficient tool use with chain-ofabstraction reasoning. Silin Gao, Jane Dwivedi-Yu, Ping Yu, Ellen Xiaoqing, Ramakanth Tan, Olga Pasunuru, Koustuv Golovneva, Asli Sinha, Antoine Celikyilmaz, Tianlu Bosselut, Wang, 10.48550/ARXIV.2401.17464CoRR, abs/2401.174642024</p>
<p>Flacuna: Unleashing the problem solving power of vicuna using FLAN finetuning. Deepanway Ghosal, Ken Yew, Navonil Chia, Soujanya Majumder, Poria, 10.48550/ARXIV.2307.02053CoRR, abs/2307.020532023</p>
<p>Representing verbs as argument concepts. Yu Gong, Kaiqi Zhao, Kenny Qili, Zhu , 10.1609/AAAI.V30I1.10324Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. the Thirtieth AAAI Conference on Artificial IntelligencePhoenix, Arizona, USAAAAI Press2016. February 12-17, 2016</p>
<p>Acquiring and modelling abstract commonsense knowledge via conceptualization. Mutian He, Tianqing Fang, Weiqi Wang, Yangqiu Song, 10.48550/ARXIV.2206.01532CoRR, abs/2206.015322022</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. Namgyu Ho, Laura Schmid, Se-Young Yun, ; Cheng-Yu, Chun-Liang Hsieh, Chih-Kuan Li, Hootan Yeh, Yasuhisa Nakhost, Alex Fujii, Ranjay Ratner, Chen-Yu Krishna, Tomas Lee, Pfister, 10.18653/V1/2023.FINDINGS-ACL.507Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada; Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023. 2023. July 9-14, 20231Findings of the Association for Computational Linguistics: ACL 2023</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, 10.48550/ARXIV.2310.06825Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2023Mistral 7b. CoRR, abs/2310.06825</p>
<p>The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning. Seungone Kim, June Se, Doyoung Joo, Joel Kim, Seonghyeon Jang, Jamin Ye, Minjoon Shin, Seo, 10.18653/V1/2023.EMNLP-MAIN.782Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Explanations from large language models make small reasoners better. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, Xifeng Yan, 10.48550/ARXIV.2210.06726CoRR, abs/2210.067262022</p>
<p>Best practices and lessons learned on synthetic data for language models. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, arXiv:2404.075032024arXiv preprint</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. 23-29 July 2023202of Proceedings of Machine Learning Research</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 10.18653/V1/2023.IJCNLP-MAIN.20Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational LinguisticsBaliAssociation for Computational Linguistics2023. November 1 -4, 20231Nusa Dua</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adámek, Eric Malmi, Aliaksei Severyn, 10.18653/V1/2023.ACL-SHORT.151Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20232Short Papers), ACL 2023</p>
<p>Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching small language models how to reason. Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andrés Codas, Clarisse Simões, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, 10.48550/ARXIV.2311.11045CoRR, abs/2311.11045</p>
<p>Orca: Progressive learning from complex explanation traces of GPT-4. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, 10.48550/ARXIV.2306.02707CoRR, abs/2306.027072023</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>. Andrew Peng, Michael Wu, John Allard, Logan Kilpatrick, Steven Heidel, </p>
<p>Gpt-3.5 turbo fine-tuning and api updates. </p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Bhatt, Aaron Canton-Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, 10.48550/ARXIV.2308.12950CoRR, abs/2308.12950Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023</p>
<p>Multitask prompted training enables zero-shot task generalization. Lorenza Saitta, Jean-Daniel Zucker, Lorenza Saitta, Jean-Daniel Zucker, ; Springer, Victor, Albert Sanh, Colin Webson, Stephen H Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Chhablani, V Nihal, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Wang, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason , Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M Rush, 2013. 2022. April 25-29, 2022Abstraction in Artificial Intelligence. OpenReview.net</p>
<p>Megatron-lm: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, CoRR, abs/1909.080532019</p>
<p>Distilling reasoning capabilities into smaller language models. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, 10.18653/V1/2023.FINDINGS-ACL.441Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>Short text conceptualization using a probabilistic knowledgebase. Yangqiu Song, Haixun Wang, Zhongyuan Wang, Hongsong Li, Weizhu Chen, 10.5591/978-1-57735-516-8/IJCAI11-388IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence. Barcelona, Catalonia, Spain2011. July 16-22, 2011IJCAI/AAAI</p>
<p>Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M Dai, Andrew La, Andrew K Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, 10.48550/ARXIV.2206.04615Ayla KarakasArun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdemand et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 10.18653/V1/2023.FINDINGS-ACL.824Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/ARXIV.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288</p>
<p>Meta-reasoning: Semantics-symbol deconstruction for large language models. Yiming Wang, Zhuosheng Zhang, Rui Wang, 10.48550/ARXIV.2306.17820CoRR, abs/2306.178202023a</p>
<p>Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Gary Haizhi, Ishan Lai, Ishani Purohit, Jacob Mondal, Kirby Anderson, Krima Kuznia, Kuntal Doshi, Maitreya Kumar Pal, Mehrad Patel, Mihir Moradshahi, Mirali Parmar, Neeraj Purohit, Varshney, Rohitha Phani, Pulkit Kaza, Ravsehaj Verma, Rushang Singh Puri, Savan Karia, Doshi, Keyur Shailaja, Siddhartha Sampat, Sujan Mishra, A Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, 10.18653/V1/2022.EMNLP-MAIN.340De- cember 7-11Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. 2022. 2022</p>
<p>Abspyramid: Benchmarking the abstraction ability of language models with a unified entailment graph. Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang, Sehyun Choi, Xin Liu, Yangqiu Song, 10.48550/ARXIV.2311.09174CoRR, abs/2311.091742023b</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022a. April 25-29, 2022OpenReview.net</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 2022b. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022c. 2022. 2022. November 28 -December 9, 2022</p>
<p>A survey on knowledge distillation of large language models. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou, 10.48550/ARXIV.2402.13116CoRR, abs/2402.131162024</p>
<p>If LLM is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Ji Heng, Chengxiang Zhai, 10.48550/ARXIV.2401.00812CoRR, abs/2401.008122024</p>
<p>Intelligent planning: a decomposition and abstraction based approach. Qiang Yang, 2012Springer Science &amp; Business Media</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 10.48550/ARXIV.2309.05653CoRR, abs/2309.056532023</p>
<p>Abstract spatial-temporal reasoning via probabilistic abduction and execution. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 10.1109/CVPR46437.2021.00961Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USAComputer Vision Foundation / IEEE2022. 2022. 2022. November 28 -December 9, 2022. 2021. June 19-25, 2021IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</p>
<p>Conceptual and unbiased reasoning in language models. Ben Zhou, Hongming Zhang, Sihao Chen, Dian Yu, Hongwei Wang, Baolin Peng, Dan Roth, Dong Yu, 10.48550/ARXIV.2404.00205CoRR, abs/2404.002052024</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Pad: Program-aided distillation specializes large models in reasoning. Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen Zhou, 10.48550/ARXIV.2305.13888CoRR, abs/2305.138882023</p>
<p>Model Version URL Llama2-7B (Touvron et al., 2023) meta-llama/Llama-2-7b. </p>
<p>2023) meta-llama/Llama-2-7b-chat. Touvron, meta-llama/CodeLlama-7b-hf. 2023</p>
<p>Rozière, Llama-3-8B (AI@Meta, 2024) meta-llama/Meta-Llama-3-8B. Codellama-7b-Instruct , 2023meta-llama/CodeLlama-7b-Instruct-hf</p>
<p>2023) mistralai/Mistral-7B-v0. Jiang, meta-llama/Meta-Llama-3-8B-Instruct. ( Llama-3-8b-Instruct, Ai@meta, 2024</p>
<p>2023) mistralai/Mistral-7B-Instruct-v0. Mistral-7b-Instruct ( Jiang, 2</p>
<p>. Peng, gpt-3.5-turbo-01252023</p>
<p>GPT-4 (OpenAI, 2023) gpt-4-0613. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-302 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-302</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-302</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-dd18782960f9ee4c66b79e1518b342ad3f8d19e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dd18782960f9ee4c66b79e1518b342ad3f8d19e7" target="_blank">WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> WizardMath is presented, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying the proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e302.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e302.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardMath-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardMath (Llama-2 70B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter Llama-2 based model fine-tuned with Math Evol-Instruct data and Reinforcement Learning from Evol-Instruct Feedback (RLEIF), using process-supervised and instruction reward models to improve chain-of-thought mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (WizardMath fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (grade-school to high-school level: algebra, number theory, counting & probability, geometry), i.e., multi-step addition/subtraction/multiplication/division embedded in word problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>not specified precisely; benchmark problems from GSM8k (grade-school multi-step) and MATH (high-school olympiad-style), i.e., varied complexity up to multi-step algebra/number theory</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised fine-tuning on GPT-4-generated step-by-step answers from evolved instructions (Math Evol-Instruct) + step-by-step PPO RL using PRM (process-supervised reward model) and IRM (instruction reward model); chain-of-thought prompting; PRM used as verifier; no external python tools</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GSM8k pass@1: 92.8%; MATH pass@1: 58.6% (reported in Table 1 and Table 2 for subtopics: e.g., Algebra 78.5%, Number Theory 58.5%)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No low-level neural mechanism provided; high-level mechanism: improvements attributed to (1) exposure to evolved diverse math instructions, (2) step-level process supervision via PRM that scores correctness of each reasoning step and is used in PPO, and (3) instruction-quality scoring via IRM; PRM helps detect incorrect steps and reduce false-positive solutions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Demonstrates better absolute performance than smaller variants; reported to outperform prior open-source 70B baselines (e.g., MetaMath 70B) by large margins, indicating accuracy increases with model size and with RLEIF training</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes greater difficulty and lower relative gains on the harder MATH benchmark vs GSM8k (more and more complex steps); no fine-grained error taxonomy beyond examples where incorrect solution steps survive outcome-based ranking but are caught by PRM</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against SFT-only, SFT+PRM, SFT+PRM+IRM, various proprietary LLMs (GPT-3.5, GPT-4 variants, Claude), and other open-source math-tuned models (MetaMath, MathScale, Xwin-Math).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Large-model WizardMath-70B fine-tuned with Math Evol-Instruct and RL using PRM+IRM achieves SOTA open-source performance on GSM8k and substantially improves on MATH, showing process supervision plus instruction-quality reward substantially improves multi-step arithmetic word-problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e302.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e302.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardMath-Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardMath (Mistral-7B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter Mistral-based WizardMath model fine-tuned with the Math Evol-Instruct dataset and optimized with step-by-step PPO using PRM and IRM to improve multi-step mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (WizardMath fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (GSM8k) and higher-difficulty MATH problems (algebra, counting, probability, number theory)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>benchmarks include grade-school multi-step problems (GSM8k) and high-school olympiad style math (MATH); specific numeric ranges not given</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>SFT on GPT-4-generated CoT answers from evolved instructions + PPO with PRM and IRM; chain-of-thought prompting; PRM used as verifier and in majority-vote aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported CoT pass@1: GSM8k 90.7% (Mistral-v0.1) and MATH 55.4%; SFT baseline was 82.8% (GSM8k) / 48.1% (MATH), improving to 87.2% / 52.7% with PRM and to 90.7% / 55.4% with PRM+IRM (Table 3). With PRM verifier + 256 samples, SFT+PRM generator achieved GSM8k 95.2% and MATH 64.7% (Table 5, pass@256).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>High-level mechanism: PRM assigns step-level correctness scores, the minimum step score is used as answer score, final reward is product of instruction and answer (PRM) scores to drive PPO; PRM as verifier weights candidate answers by step-level correctness, enabling detection and suppression of answers with incorrect steps even when final numeric answer matches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Shows substantial improvement from SFT to PPO with PRM and IRM; relative gains ~8%–8.9% absolute on GSM8k and ~7.3% on MATH across interventions, indicating training method effectiveness is large even for 7B models</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper highlights that outcome-based reward models (ORM/ORM-like scoring) can be fooled—ORM may give high score to incorrect chains (false positives) whereas PRM detects the erroneous steps; models still perform worse on MATH (more steps/complexity) than GSM8k</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared SFT-only vs SFT+PRM vs SFT+PRM+IRM; compared PRM to ORM and to PRM800k / Math-Shepherd (other verifiers); compared to multiple open-source and proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>On a small 7B backbone, Math Evol-Instruct + PRM + IRM + PPO yields large gains: WizardMath-Mistral-7B reaches top-tier performance, showing process supervision and instruction-quality reward are highly effective even at 7B for multi-step arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e302.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e302.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process-supervised Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward model trained to score the correctness of each reasoning step in generated chain-of-thought solutions (labels obtained from GPT-4), used both as a reward signal in PPO and as a verifier for aggregating candidate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Process-supervised Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic/word-problem reasoning (step-level correctness scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Trained on GPT-4-provided step-level correctness labels for multiple model-generated solutions; used to (1) assign a per-step score r_i and compute an answer score r^a as minimum step score, (2) combine with IRM instruction score via product for PPO reward, and (3) as a verifier to weight candidate answers in majority-vote aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using PRM in PPO improved SFT baselines by ~3%–4% absolute; combining PRM+IRM yielded an additional ~2.5%–4% (Table 3). PRM verifier with SFT+PRM generator achieved pass@256: GSM8k 95.2% and MATH 64.7% (Table 5). PRM outperforms ORM and other PRMs trained on manual data (PRM800k) in experiments (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Provides the principal mechanistic insight available in the paper: PRM evaluates internal reasoning processes step-by-step, enabling detection of incorrect intermediate steps (thus preventing false positives where outcome matches despite incorrect reasoning). The paper does not provide neural-level mechanisms (e.g., attention pattern analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>PRM benefits are reported across model sizes (GPT-2-XL, Llama2-7B, Mistral-7B), consistently improving results; PRM trained on AI-labeled data matched or exceeded manually annotated PRM800k for MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>PRM can still allow incorrect final answers if step scores are high; no detailed failure taxonomy beyond examples where PRM assigns low scores to wrong steps (successful detection) and ORM fails to do so.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Outcome-based Reward Model (ORM), PRM800k (human-annotated PRM), Math-Shepherd (MCTS-based verification); compared SFT vs PPO with and without PRM/IRM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Step-level process supervision (PRM) is crucial: it reliably detects incorrect intermediate steps, improves PPO training rewards, and when used as a verifier substantially raises accuracy on multi-step arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e302.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e302.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Reward Model (IRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward model trained to rank/evaluate the quality of evolved math instructions (difficulty and clarity), used to mitigate instruction evolution drift and as part of the PPO reward (combined with PRM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction Reward Model (IRM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>not an arithmetic solver; evaluates instruction quality for arithmetic/math prompts</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Trained on GPT-4 rankings of evolved instruction lists (quality scores 1–6); used to assign r^q per-instruction reward and multiplied by PRM-based answer reward for final reward in PPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Adding IRM to PRM during PPO provided additional gains of ~2.5%–4% absolute over PRM alone (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Mechanistically serves to penalize low-quality or misleading evolved instructions (preventing spiraling evolution) and reduces false positives by decreasing reward for poor/ambiguous prompts; no neural mechanistic analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Combining IRM with PRM consistently improved PPO outcomes across examined model backbones (GPT-2-XL, Llama2-7B, Mistral-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed beyond general need to mitigate instruction evolution drift; no detailed failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used in ablation vs PRM-only and SFT-only.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Instruction-level reward (IRM) complements process-level reward (PRM) during RL, preventing reward signal from favoring low-quality or misleading instructions and boosting final arithmetic reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e302.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e302.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits step-by-step reasoning traces (chain-of-thought) from LLMs; used as the primary format for generating and evaluating solutions to arithmetic word problems in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic in word problems (CoT exposes intermediate arithmetic steps: add/sub/mul/div and symbolic algebra manipulations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>applies across GSM8k and MATH difficulty ranges; exact numeric ranges unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>CoT used both in generation of training answers (GPT-4-generated step-by-step answers), in evaluation (models produce CoT with greedy decoding), and combined with Self-Consistency and PRM-based verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CoT baseline results are the basis for reported pass@1 numbers; Self-Consistency with CoT plus PRM verifier gave top results (e.g., SFT+PRM w/ PRM verifier: GSM8k 95.2% pass@256).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper assumes CoT reveals internal reasoning traces that can be scored by PRM; no claim of CoT being the model's true latent computation—CoT serves as an interpretable artifact for process supervision and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Combining CoT with PRM/IRM and RL yields substantial improvements; Self-Consistency over CoT further improves accuracy when paired with PRM verification.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>CoT outputs can include incorrect intermediate steps leading to plausible but wrong final answers; outcome-based verifiers may give high score despite incorrect CoT, motivating process supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared CoT alone, CoT + Self-Consistency, CoT + PRM verifier, CoT + ORM verifier, and different combinations with PPO training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Chain-of-thought outputs are useful supervision targets: scoring CoT at the step level and using that in RL and verification materially improves multi-step arithmetic problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e302.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e302.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency decoding (majority-vote over sampled CoTs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation strategy that samples multiple chain-of-thought solutions and selects the final answer by majority vote (optionally weighted by verifiers); used as a baseline and combined with PRM for improved verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency (decoding/aggregation method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems aggregated across candidate solutions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Generate N candidate CoT solutions and choose answer by majority vote or weighted by PRM/ORM. Used with up to 256 samples in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Self-Consistency improves accuracy over single greedy CoT; e.g., SFT + PRM generator with Self-Consistency achieved GSM8k 92.3% (Table 5 shows various combinations), while PRM-weighted aggregation with 256 samples yielded up to 95.2% GSM8k.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Majority voting over diverse CoT samples reduces variance and helps correct single-sample reasoning errors; weighting votes by PRM further incorporates step-level correctness into aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance of Self-Consistency improves with more samples N (1 to 256), and PRM-weighting yields larger gains especially for harder benchmarks (MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Majority voting can still be misled if many sampled CoTs share the same incorrect reasoning pattern; PRM weighting mitigates but does not fully eliminate such failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to plain greedy CoT, ORM and PRM-weighted aggregation; ablations with/without PRM in generator and verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Self-Consistency improves multi-step arithmetic accuracy, and combining it with PRM-based weighting produces further substantial gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e302.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e302.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (used for labels/evolution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used in this work as an expensive, high-quality oracle to (1) evolve math instructions through Math Evol-Instruct and (2) provide step-level correctness judgments and instruction-quality rankings used to train PRM and IRM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>used to generate chain-of-thought answers and to label step-level correctness for multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>used across GSM8k and MATH types of problems (varied complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Used as an annotator/teacher: evolving instructions across rounds (upward/downward evolution), generating reference step-by-step answers for SFT, and producing step-level labels and instruction rankings to train PRM and IRM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Not reported as an evaluated solver here; used only as supervision/label source. The paper reports models trained on GPT-4 outputs reach or exceed several proprietary baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>GPT-4 serves as an external source of high-quality CoT and step-level judgments enabling process-supervised training; no internal mechanistic claims about GPT-4 are made.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes reliance on GPT-4 for labels may propagate some hallucinations; they mitigate with PRM to address false positives when the teacher provides incorrect steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used in place of human annotators/labelers; authors also experiment with using Llama-3 to perform evolution and note a performance gap vs GPT-4-evolved data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>High-quality AI labeling (GPT-4) can be used to automatically construct step-level supervision (PRM) and instruction-ranking (IRM), and training on such AI-labelled data yields strong improvements in arithmetic problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
                <li>How well do large language models perform in arithmetic tasks? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-302",
    "paper_id": "paper-dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "WizardMath-70B",
            "name_full": "WizardMath (Llama-2 70B variant)",
            "brief_description": "A 70B-parameter Llama-2 based model fine-tuned with Math Evol-Instruct data and Reinforcement Learning from Evol-Instruct Feedback (RLEIF), using process-supervised and instruction reward models to improve chain-of-thought mathematical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 (WizardMath fine-tuned)",
            "model_size": "70B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (grade-school to high-school level: algebra, number theory, counting & probability, geometry), i.e., multi-step addition/subtraction/multiplication/division embedded in word problems",
            "number_range_or_complexity": "not specified precisely; benchmark problems from GSM8k (grade-school multi-step) and MATH (high-school olympiad-style), i.e., varied complexity up to multi-step algebra/number theory",
            "method_or_intervention": "supervised fine-tuning on GPT-4-generated step-by-step answers from evolved instructions (Math Evol-Instruct) + step-by-step PPO RL using PRM (process-supervised reward model) and IRM (instruction reward model); chain-of-thought prompting; PRM used as verifier; no external python tools",
            "performance_result": "GSM8k pass@1: 92.8%; MATH pass@1: 58.6% (reported in Table 1 and Table 2 for subtopics: e.g., Algebra 78.5%, Number Theory 58.5%)",
            "mechanistic_insight": "No low-level neural mechanism provided; high-level mechanism: improvements attributed to (1) exposure to evolved diverse math instructions, (2) step-level process supervision via PRM that scores correctness of each reasoning step and is used in PPO, and (3) instruction-quality scoring via IRM; PRM helps detect incorrect steps and reduce false-positive solutions",
            "performance_scaling": "Demonstrates better absolute performance than smaller variants; reported to outperform prior open-source 70B baselines (e.g., MetaMath 70B) by large margins, indicating accuracy increases with model size and with RLEIF training",
            "failure_modes": "Paper notes greater difficulty and lower relative gains on the harder MATH benchmark vs GSM8k (more and more complex steps); no fine-grained error taxonomy beyond examples where incorrect solution steps survive outcome-based ranking but are caught by PRM",
            "comparison_baseline": "Compared against SFT-only, SFT+PRM, SFT+PRM+IRM, various proprietary LLMs (GPT-3.5, GPT-4 variants, Claude), and other open-source math-tuned models (MetaMath, MathScale, Xwin-Math).",
            "key_finding": "Large-model WizardMath-70B fine-tuned with Math Evol-Instruct and RL using PRM+IRM achieves SOTA open-source performance on GSM8k and substantially improves on MATH, showing process supervision plus instruction-quality reward substantially improves multi-step arithmetic word-problem solving.",
            "uuid": "e302.0",
            "source_info": {
                "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "WizardMath-Mistral-7B",
            "name_full": "WizardMath (Mistral-7B variant)",
            "brief_description": "A 7B-parameter Mistral-based WizardMath model fine-tuned with the Math Evol-Instruct dataset and optimized with step-by-step PPO using PRM and IRM to improve multi-step mathematical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (WizardMath fine-tuned)",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (GSM8k) and higher-difficulty MATH problems (algebra, counting, probability, number theory)",
            "number_range_or_complexity": "benchmarks include grade-school multi-step problems (GSM8k) and high-school olympiad style math (MATH); specific numeric ranges not given",
            "method_or_intervention": "SFT on GPT-4-generated CoT answers from evolved instructions + PPO with PRM and IRM; chain-of-thought prompting; PRM used as verifier and in majority-vote aggregation",
            "performance_result": "Reported CoT pass@1: GSM8k 90.7% (Mistral-v0.1) and MATH 55.4%; SFT baseline was 82.8% (GSM8k) / 48.1% (MATH), improving to 87.2% / 52.7% with PRM and to 90.7% / 55.4% with PRM+IRM (Table 3). With PRM verifier + 256 samples, SFT+PRM generator achieved GSM8k 95.2% and MATH 64.7% (Table 5, pass@256).",
            "mechanistic_insight": "High-level mechanism: PRM assigns step-level correctness scores, the minimum step score is used as answer score, final reward is product of instruction and answer (PRM) scores to drive PPO; PRM as verifier weights candidate answers by step-level correctness, enabling detection and suppression of answers with incorrect steps even when final numeric answer matches",
            "performance_scaling": "Shows substantial improvement from SFT to PPO with PRM and IRM; relative gains ~8%–8.9% absolute on GSM8k and ~7.3% on MATH across interventions, indicating training method effectiveness is large even for 7B models",
            "failure_modes": "Paper highlights that outcome-based reward models (ORM/ORM-like scoring) can be fooled—ORM may give high score to incorrect chains (false positives) whereas PRM detects the erroneous steps; models still perform worse on MATH (more steps/complexity) than GSM8k",
            "comparison_baseline": "Compared SFT-only vs SFT+PRM vs SFT+PRM+IRM; compared PRM to ORM and to PRM800k / Math-Shepherd (other verifiers); compared to multiple open-source and proprietary models.",
            "key_finding": "On a small 7B backbone, Math Evol-Instruct + PRM + IRM + PPO yields large gains: WizardMath-Mistral-7B reaches top-tier performance, showing process supervision and instruction-quality reward are highly effective even at 7B for multi-step arithmetic word problems.",
            "uuid": "e302.1",
            "source_info": {
                "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "PRM",
            "name_full": "Process-supervised Reward Model (PRM)",
            "brief_description": "A reward model trained to score the correctness of each reasoning step in generated chain-of-thought solutions (labels obtained from GPT-4), used both as a reward signal in PPO and as a verifier for aggregating candidate answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Process-supervised Reward Model (PRM)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic/word-problem reasoning (step-level correctness scoring)",
            "number_range_or_complexity": null,
            "method_or_intervention": "Trained on GPT-4-provided step-level correctness labels for multiple model-generated solutions; used to (1) assign a per-step score r_i and compute an answer score r^a as minimum step score, (2) combine with IRM instruction score via product for PPO reward, and (3) as a verifier to weight candidate answers in majority-vote aggregation",
            "performance_result": "Using PRM in PPO improved SFT baselines by ~3%–4% absolute; combining PRM+IRM yielded an additional ~2.5%–4% (Table 3). PRM verifier with SFT+PRM generator achieved pass@256: GSM8k 95.2% and MATH 64.7% (Table 5). PRM outperforms ORM and other PRMs trained on manual data (PRM800k) in experiments (Table 4).",
            "mechanistic_insight": "Provides the principal mechanistic insight available in the paper: PRM evaluates internal reasoning processes step-by-step, enabling detection of incorrect intermediate steps (thus preventing false positives where outcome matches despite incorrect reasoning). The paper does not provide neural-level mechanisms (e.g., attention pattern analyses).",
            "performance_scaling": "PRM benefits are reported across model sizes (GPT-2-XL, Llama2-7B, Mistral-7B), consistently improving results; PRM trained on AI-labeled data matched or exceeded manually annotated PRM800k for MATH.",
            "failure_modes": "PRM can still allow incorrect final answers if step scores are high; no detailed failure taxonomy beyond examples where PRM assigns low scores to wrong steps (successful detection) and ORM fails to do so.",
            "comparison_baseline": "Compared against Outcome-based Reward Model (ORM), PRM800k (human-annotated PRM), Math-Shepherd (MCTS-based verification); compared SFT vs PPO with and without PRM/IRM.",
            "key_finding": "Step-level process supervision (PRM) is crucial: it reliably detects incorrect intermediate steps, improves PPO training rewards, and when used as a verifier substantially raises accuracy on multi-step arithmetic word problems.",
            "uuid": "e302.2",
            "source_info": {
                "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "IRM",
            "name_full": "Instruction Reward Model (IRM)",
            "brief_description": "A reward model trained to rank/evaluate the quality of evolved math instructions (difficulty and clarity), used to mitigate instruction evolution drift and as part of the PPO reward (combined with PRM).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction Reward Model (IRM)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "not an arithmetic solver; evaluates instruction quality for arithmetic/math prompts",
            "number_range_or_complexity": null,
            "method_or_intervention": "Trained on GPT-4 rankings of evolved instruction lists (quality scores 1–6); used to assign r^q per-instruction reward and multiplied by PRM-based answer reward for final reward in PPO",
            "performance_result": "Adding IRM to PRM during PPO provided additional gains of ~2.5%–4% absolute over PRM alone (Table 3).",
            "mechanistic_insight": "Mechanistically serves to penalize low-quality or misleading evolved instructions (preventing spiraling evolution) and reduces false positives by decreasing reward for poor/ambiguous prompts; no neural mechanistic analysis provided.",
            "performance_scaling": "Combining IRM with PRM consistently improved PPO outcomes across examined model backbones (GPT-2-XL, Llama2-7B, Mistral-7B).",
            "failure_modes": "Not discussed beyond general need to mitigate instruction evolution drift; no detailed failure modes.",
            "comparison_baseline": "Used in ablation vs PRM-only and SFT-only.",
            "key_finding": "Instruction-level reward (IRM) complements process-level reward (PRM) during RL, preventing reward signal from favoring low-quality or misleading instructions and boosting final arithmetic reasoning performance.",
            "uuid": "e302.3",
            "source_info": {
                "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought (CoT) Prompting",
            "brief_description": "A prompting strategy that elicits step-by-step reasoning traces (chain-of-thought) from LLMs; used as the primary format for generating and evaluating solutions to arithmetic word problems in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting (method)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic in word problems (CoT exposes intermediate arithmetic steps: add/sub/mul/div and symbolic algebra manipulations)",
            "number_range_or_complexity": "applies across GSM8k and MATH difficulty ranges; exact numeric ranges unspecified",
            "method_or_intervention": "CoT used both in generation of training answers (GPT-4-generated step-by-step answers), in evaluation (models produce CoT with greedy decoding), and combined with Self-Consistency and PRM-based verification",
            "performance_result": "CoT baseline results are the basis for reported pass@1 numbers; Self-Consistency with CoT plus PRM verifier gave top results (e.g., SFT+PRM w/ PRM verifier: GSM8k 95.2% pass@256).",
            "mechanistic_insight": "Paper assumes CoT reveals internal reasoning traces that can be scored by PRM; no claim of CoT being the model's true latent computation—CoT serves as an interpretable artifact for process supervision and verification.",
            "performance_scaling": "Combining CoT with PRM/IRM and RL yields substantial improvements; Self-Consistency over CoT further improves accuracy when paired with PRM verification.",
            "failure_modes": "CoT outputs can include incorrect intermediate steps leading to plausible but wrong final answers; outcome-based verifiers may give high score despite incorrect CoT, motivating process supervision.",
            "comparison_baseline": "Compared CoT alone, CoT + Self-Consistency, CoT + PRM verifier, CoT + ORM verifier, and different combinations with PPO training.",
            "key_finding": "Chain-of-thought outputs are useful supervision targets: scoring CoT at the step level and using that in RL and verification materially improves multi-step arithmetic problem solving.",
            "uuid": "e302.4",
            "source_info": {
                "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency decoding (majority-vote over sampled CoTs)",
            "brief_description": "A decoding/aggregation strategy that samples multiple chain-of-thought solutions and selects the final answer by majority vote (optionally weighted by verifiers); used as a baseline and combined with PRM for improved verification.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Self-Consistency (decoding/aggregation method)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems aggregated across candidate solutions",
            "number_range_or_complexity": null,
            "method_or_intervention": "Generate N candidate CoT solutions and choose answer by majority vote or weighted by PRM/ORM. Used with up to 256 samples in experiments.",
            "performance_result": "Self-Consistency improves accuracy over single greedy CoT; e.g., SFT + PRM generator with Self-Consistency achieved GSM8k 92.3% (Table 5 shows various combinations), while PRM-weighted aggregation with 256 samples yielded up to 95.2% GSM8k.",
            "mechanistic_insight": "Majority voting over diverse CoT samples reduces variance and helps correct single-sample reasoning errors; weighting votes by PRM further incorporates step-level correctness into aggregation.",
            "performance_scaling": "Performance of Self-Consistency improves with more samples N (1 to 256), and PRM-weighting yields larger gains especially for harder benchmarks (MATH).",
            "failure_modes": "Majority voting can still be misled if many sampled CoTs share the same incorrect reasoning pattern; PRM weighting mitigates but does not fully eliminate such failure modes.",
            "comparison_baseline": "Compared to plain greedy CoT, ORM and PRM-weighted aggregation; ablations with/without PRM in generator and verifier.",
            "key_finding": "Self-Consistency improves multi-step arithmetic accuracy, and combining it with PRM-based weighting produces further substantial gains.",
            "uuid": "e302.5",
            "source_info": {
                "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4 (used for labels/evolution)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "GPT-4 is used in this work as an expensive, high-quality oracle to (1) evolve math instructions through Math Evol-Instruct and (2) provide step-level correctness judgments and instruction-quality rankings used to train PRM and IRM.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "used to generate chain-of-thought answers and to label step-level correctness for multi-step arithmetic word problems",
            "number_range_or_complexity": "used across GSM8k and MATH types of problems (varied complexity)",
            "method_or_intervention": "Used as an annotator/teacher: evolving instructions across rounds (upward/downward evolution), generating reference step-by-step answers for SFT, and producing step-level labels and instruction rankings to train PRM and IRM",
            "performance_result": "Not reported as an evaluated solver here; used only as supervision/label source. The paper reports models trained on GPT-4 outputs reach or exceed several proprietary baselines.",
            "mechanistic_insight": "GPT-4 serves as an external source of high-quality CoT and step-level judgments enabling process-supervised training; no internal mechanistic claims about GPT-4 are made.",
            "performance_scaling": null,
            "failure_modes": "Paper notes reliance on GPT-4 for labels may propagate some hallucinations; they mitigate with PRM to address false positives when the teacher provides incorrect steps.",
            "comparison_baseline": "Used in place of human annotators/labelers; authors also experiment with using Llama-3 to perform evolution and note a performance gap vs GPT-4-evolved data.",
            "key_finding": "High-quality AI labeling (GPT-4) can be used to automatically construct step-level supervision (PRM) and instruction-ranking (IRM), and training on such AI-labelled data yields strong improvements in arithmetic problem solving.",
            "uuid": "e302.6",
            "source_info": {
                "paper_title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1
        },
        {
            "paper_title": "How well do large language models perform in arithmetic tasks?",
            "rating": 2
        }
    ],
    "cost": 0.01796925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>WizardMath: Empowering Mathematical ReasonING FOR LARGE LANGUAGE MODELS VIA Reinforced Evol-Instruct</h1>
<p>Haipeng Luo ${ }^{1 <em>}$ Qingfeng Sun ${ }^{2 </em>}$ Can Xu ${ }^{21}$ Pu Zhao ${ }^{2}$ Jianguang Lou ${ }^{2}$ Chongyang Tao ${ }^{2}$ Xiubo Geng ${ }^{2}$ Qingwei Lin ${ }^{2}$ Shifeng Chen ${ }^{31}$ Yansong Tang ${ }^{11}$ Dongmei Zhang ${ }^{2}$<br>${ }^{1}$ Shenzhen International Graduate School, Tsinghua University<br>${ }^{2}$ Microsoft Corporation<br>${ }^{3}$ Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences<br>{luohp24@mails., tang.yansong@sz.}tsinghua.edu.cn<br>{caxu,qins, puzhao,jlou, chotao, xigeng, qlin, dongmeiz}@microsoft.com<br>{shifeng.chen}@siat.ac.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM.</p>
<h2>1 INTRODUCTION</h2>
<p>Recently, Large-scale language models (LLMs) have garnered significant attention and become the go-to approach for numerous natural language processing (NLP) tasks, including open domain conversation (Ouyang et al., 2022; OpenAI, 2023; Touvron et al., 2023a), coding (Chen et al., 2021; Wang et al., 2021; Li et al., 2023b) and math (Taylor et al., 2022; Lewkowycz et al., 2022; Shao et al., 2024; Yang et al., 2024). A conspicuous example is ChatGPT ${ }^{1}$, developed by OpenAI. This model uses extensive pre-training on large-scale internet data and further fine-tuning with specific instruction data and methods. As a result, it achieves state-of-the-art zero-shot performance on various benchmarks. Subsequently, Anthropic, Google, and Meta also launched their competitive products one after another. Notably, Meta's series of Llama (Touvron et al., 2023a;b; Dubey et al., 2024) have sparked an open-source revolution and quickly narrowed the gap with those closed-source LLMs. This trend also gradually stimulates the releases of Mistral (Jiang et al., 2023), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and WizardLM (Xu et al., 2023), etc. However, these open models still struggle with the scenarios which require complex multi-step quantitative reasoning, such as solving mathematical and science challenges (Ahn et al., 2024; Long et al., 2024).</p>
<p>Chain-of-thought (CoT) (Wei et al., 2022) proposes to design better prompts to generate step-bystep solutions, which can lead to improved performance. Self-Consistency (Wang et al., 2022)</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A diagram illustrating the three steps of our Reinforcement Learning from Evol-Instruct Feedback (RLEIF)
also achieves remarkable performance on many reasoning benchmarks, which generates several possible answers from the model and selects the correct one based on majority vote (Fu et al., 2022). Llemma (Azerbayev et al., 2023) and MathPile (Wang et al., 2023f) continue pretraining LLMs with math corpus to improve domain capacity. MetaMath (Yu et al., 2023b) and Xwin-Math (Li et al., 2024a) bootstraps mathematical questions by augmenting the question from multiple perspectives. MAmmoTH (Yue et al., 2023) and TORA (Gou et al., 2023) presents a unique hybrid of CoT and program-of-thought (PoT) to ensure extensive coverage of diverse fields in math. Recently, EvolInstruct is an effective method for large-scale data synthesis using LLMs. It has been widely verified and proven to be effective in enhancing the model's instruction following capability. It employs In-depth Evolving and In-breadth Evolving to automate the generation of diverse and complex open-domain instructions using LLMs, instead of relying on human-crafted instruction datasets. Indepth Evolving incrementally enhances instruction complexity by introducing additional constraints, deepening, concretizing, increasing reasoning steps, and complicating input. In-breadth Evolving focuses on improving topic diversity and dataset richness by creating entirely new instructions. To enhance the correctness of each step in the model's generation process, (Wang et al., 2024a; Chen et al., 2024a; Lightman et al., 2023) finds that process supervision with reinforcement learning significantly outperforms outcome supervision for solving challenging MATH problems.</p>
<p>Inspired by Evol-Instruct and Process-supervised Reinforcement Learning, this work aims to enhance the mathematical reasoning abilities of the LLMs. As shown in the Figure 1, we propose a new method named Reinforcement Learning from Evol-Instruct Feedback (RLEIF), which could firstly generate diverse math instructions data by brand-new Math Evol-Instruct, which includes two downward evolution and upward evolution progress to produce the grade school math and challenging high school math respectively. However different from WizardLM (Xu et al., 2023) and WizardCoder (Luo et al., 2023), which mainly focus on the SFT stage and are susceptible to learning hallucinated information from the teacher model, we innovatively introduce PRM to address the False-Positive issue in the problem-solving process. Moreover, to prevent instruction evolution from spiraling out of control, we incorporate an instruction reward model (IRM) as a mitigating strategy. Thus, we train an instruction reward model (IRM) and a process-supervised reward model (PRM) (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2024a; Chen et al., 2024a), the former indicates the quality of the evolved instruction and the latter offers feedback for each reasoning step in the solution. Initially, we finetune LLMs with the evolved math data. Immediately, we leverage GPT-4 to produce the ranking order of instructions, and the correctness of each reasoning step, then optimize the LLMs to obtain the reward models. Finally, we implement the step-by-step PPO to train our WizardMath.</p>
<p>We perform experiments on two widely used mathematical reasoning benchmarks, namely GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) covering math problems from grade to high school levels, the results show that our WizardMath outperforms all other open-source LLMs at the same model size, achieving state-of-the-art performance. For instance, WizardMath-70B significantly outperforms MetaMath-70B by a significant margin on GSM8k ( 92.8 vs. 82.3 ) and on MATH (58.6 vs. 26.6). Specifically, WizardMath-Mistral-7B observed a substantial improvement in pass@1 with an increase of +12.8 ( 90.7 . vs. 77.9 ) on GSM8k, and +26.8 ( 55.4 vs. 28.6 ) on MATH compared to MetaMath-Mistral-7B. Notably, our 70B model even also significantly surpasses those powerful proprietary LLMs, such as GPT-3.5-Turbo, Claude 2 (Bai et al., 2022), Mistral Medium (Jiang et al., 2024a), Gemini-Pro (Team, 2023), PaLM-2 (Anil et al., 2023) and GPT-4-early-version.</p>
<p>The main contributions of this work are as follows:</p>
<ul>
<li>We introduce WizardMath model, which enhances the LLMs' mathematical reasoning abilities across a range of problem difficulties, from grade to high school levels.</li>
<li>We propose a new fully AI-powered automatic reinforcement learning method, Reinforcement Learning from Evol-Instruct Feedback (RLEIF), alongside Math Evol-Instruct and Process Supervision, for improving reasoning performance.</li>
<li>WizardMath surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency and also significantly outperforms various proprietary LLMs on both GSM8k and MATH, demonstrate the effectiveness of our RLEIF.</li>
</ul>
<h1>2 Related Work</h1>
<p>Large Language Models. LLMs have significantly advanced Natural Language Processing, with models like OpenAI's GPT Series (Brown et al., 2020a; OpenAI, 2023), Anthropic's Claude (Bai et al., 2022), Google's PaLM (Chowdhery et al., 2022; Anil et al., 2023), Gemini (Team, 2023), and Gemma (Team et al., 2024) featuring billions of parameters and trained on massive textual datasets. The AI field has also seen a rise in open-source LLMs such as Mistral (Jiang et al., 2023), Llama Series (Touvron et al., 2023a;b; Dubey et al., 2024; Taylor et al., 2022), DeepSeek (Bi et al., 2024; Shao et al., 2024), Qwen (Bai et al., 2023; Yang et al., 2024) etc. Notably, Llama serves as a foundational model for supervised fine-tuning, leading to the development of models like Alpaca, Vicuna (Taori et al., 2023; Chiang et al., 2023).</p>
<p>Large Language Models For Mathematical reasoning. NLP models face challenges with complex reasoning, including mathematical (Long et al., 2024; Zhang et al., 2024c; Xia et al., 2024), commonsense (Talmor et al., 2019). Significant research focuses on Mathematical Word Problems (MWP), which demand understanding of mathematical concepts and multi-step reasoning (Zheng et al., 2023; Zhao et al., 2023; Yuan et al., 2023a). Models are tested on various MWP benchmarks (Roy \&amp; Roth, 2015; Hendrycks et al., 2021). Techniques like Chain-of-Thought Prompting (Wei et al., 2022), Least-to-Most prompting (Zhou et al., 2022), and Complex CoT (Fu et al., 2022) enhance reasoning by introducing multiple steps and breaking problems into sub-problems. There are some models aimed at improving math CoT reasoning skills such as MetaMath (Yu et al., 2023b), MathScale (Tang et al., 2024), Xwin-Math (Li et al., 2024a), DART-Math (Tong et al., 2024) etc. Some models enhance mathematical reasoning by integrating python tools, such as TORA (Gou et al., 2023), MAmmoTH (Yue et al., 2023), Openmathinstruct (Toshniwal et al., 2024), NuminaMath (Li et al., 2024c) etc. In our work, we mainly improve the CoT reasoning ability of mathematics without using external Python tools.</p>
<p>Reinforcement Learning for Large Language Models. State-of-the-art models often display logical errors and illusions, particularly in domains requiring complex, multi-step reasoning, leading to significant challenges (Bubeck et al., 2023; Maynez et al., 2020). Strategies such as training reward models help discriminate between desirable and undesirable outputs (Lightman et al., 2023; Wu et al., 2023b; Chen et al., 2024b). Historically, outcome-based approaches focused on algorithmic tasks (Li et al., 2016; Cai et al., 2017; Yu et al., 2023a), while recent research demonstrates the efficacy of reward models or validators in enhancing model performance (Cobbe et al., 2021; Wang et al., 2023c;d; Li et al., 2022a). Reward models have also been incorporated into reinforcement learning pipelines and employed in rejection sampling to align Large Language Models (LLMs) with human preferences (Shen et al., 2021; Bai et al., 2022; Yuan et al., 2023c; Dong et al., 2023;</p>
<p>Song et al., 2023; Touvron et al., 2023b; Rafailov et al., 2024; Meng et al., 2024). A contrast is drawn between outcome-supervised and process-supervised reward models, with the latter being more effective at addressing discrepancies arising from incorrect reasoning paths leading to correct outcomes (Uesato et al., 2022; Zelikman et al., 2022; Creswell et al., 2022). Recent advances have promoted process-based supervision through manual annotation, significantly benefiting LLMs over outcome-based approaches (Lightman et al., 2023; Wang et al., 2024a; Sun et al., 2024; Chen et al., 2024a; Wang et al., 2024b; Zhang et al., 2024a). In this paper, we leverage AI models like ChatGPT to automatically offer process annotation to improve the efficiency of this research line.</p>
<h1>3 Method</h1>
<p>In this section, we elaborate on the details of our WizardMath. Following WizardLM and PRMs (Lightman et al., 2023), we propose Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method, which integrates the math Evol-Instruct and reinforced instruction and process supervision to evolve GSM8k and MATH, and fine-tune the pre-trained language models with the evolved data and reward models.</p>
<h3>3.1 Math Evol-InStruct</h3>
<p>Motivated by the Evol-Instruct (Xu et al., 2023) method proposed by WiazrdLM and its effective application on WizardCoder (Luo et al., 2023), this work attempts to make math instructions with various complexities and diversity to enhance the pre-trained LLMs. Specifically, we adapt EvolInstruct to a new paradigm including two evolution lines:</p>
<p>1) Downward evolution: It enhances instructions by making the questions easier. For example i): revising high difficulty questions to lower difficulty, or ii) producing a new and easier question with another different topic.
2) Upward evolution: Derived from original Evol-Instruct method, it deepens and generates new and harder questions by i) adding more constraints, ii) concretizing, iii) increasing reasoning.</p>
<p>The complete prompts of above evolution are shown in Appendix A.1. For each instruction, we use GPT-4 to evolve 5 rounds ( 2 downward and 3 upward) of new instructions progressively, each new one is generated by the previous round of evolution.</p>
<h3>3.2 ReWard MODELS</h3>
<p>Considering the necessity of quality control for evolved instructions and inspired by PRMs (Lightman et al., 2023), we train two reward models to predict the quality of the instructions and the correctness of each step in the answer respectively:</p>
<p>Instruction Reward Model (IRM) This model aims to judge the quality of the evolved instructions on two aspects: i) Difficulty, and ii) Definition. To produce the ranking list training data of IRM, we leverage GPT-4 to rank the quality between those evolved instructions and original instruction. The one with high difficulty and clear definition will deserve a higher ranking. The detailed prompt of above ranking process is shown in the Appendix A.2.</p>
<p>Specifically, given an math instructions $q$, IRM $(Q \rightarrow \mathbb{R})$ assigns a score to $q$ to indicate its quality. We optimize ORM via the following pairwise ranking loss:</p>
<p>$$
\mathcal{L}<em j="j">{I R M}=-\log \sigma\left(r</em>-m\right)
$$}^{q}-r_{k}^{q</p>
<p>where $r_{j}^{q}$ is the reward of chosen instruction and $r_{k}^{q}$ is the reward of rejected instruction, $m$ is the margin.</p>
<p>Process-supervised Reward Model (PRM) As there is no simple way to support highly precise process supervision without professional and expensive human-labelers, we depend on GPT-4 to provide process supervision, and ask it to assess the correctness of each step in the solutions generated by our model to produce PRM training data. The detailed prompt of above step level labeling process is shown in the Appendix A.3.</p>
<p>For exactly, given an math instructions $q$ and its answer $a$, PRM $\left(Q \times A \rightarrow \mathbb{R}^{+}\right)$assigns a score to each step of $a$, we train PRM with the following cross-entropy loss:</p>
<p>$$
\mathcal{L}<em i="1">{P R M}=\sum</em>\right)
$$}^{L} y_{i} \log r_{i}^{a}+\left(1-y_{i}\right) \log \left(1-r_{i}^{a</p>
<p>where $L$ is the reasoning steps of answer $a . y_{i}$ is the ground-truth label of the $i$-th step of answer $a$, $y_{i}=1$ if $a_{i}$ is correct, otherwise $y_{i}=0 . r_{i}^{a}$ is the reward score (assigned by PRM) of the $i$-th step of answer $a$.</p>
<h1>3.3 REINFORCEMENT LEARNING WITH IRM AND PRM</h1>
<p>Immediately, we exploit reinforcement learning to optimize LLMs. Following (Lightman et al., 2023), we employ step by step Proximal Policy Optimization (PPO) to reward both instruction and each reasoning step.
For each math instruction $q$ and generated answer $a$, we use IRM to assign instruction reward $r^{q}$, and use the minimum score across all reasoning steps to represent the final reward score $r^{a}$ of the answer $a$ assigned by PRM. Then we apply a product as the final reward of this instruction-answer pair:</p>
<p>$$
r=r^{q} \cdot r^{a}
$$</p>
<h3>3.4 PRM FOR VERIFICATION</h3>
<p>Following (Lightman et al., 2023) and (Li et al., 2023c), we leverage both majority voting and PRM verifier to aggregate the predictions of different reasoning paths.</p>
<p>$$
\hat{a}=\underset{a}{\arg \max } \sum_{i=1}^{N} \mathbb{I}<em i="i">{a</em>\right)
$$}=a} \cdot P R M\left(q, a_{i</p>
<p>where $P R M\left(q, a_{i}\right)$ is the score of the $i$-th reasoning path assigned by PRM for instruction $q . \mathbb{I}<em i="i">{a</em>=a$.}=a}$ is an indicator function that returns 1 (or 0 ) if $a_{i</p>
<h2>4 EXPERIMENT</h2>
<p>This section provides a comprehensive overview of the advanced models. Subsequently, we mainly elucidate the performance metrics of our models on two prevalent mathematical benchmarks from grade to high school problems: GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021).</p>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>SFT Training Data. Firstly, use the GSM8k and MATH training sets as the initial seed collection, then employ both upward and downward math Evol-Instruct approach for five rounds. Each round need to evolve the initial instructions 6 times, and the temperature parameter is set to 0.7 . Next, we remove duplicate instructions 17 k . Hence, a total of 448 k unique instructions were obtained. Subsequently, 30 k data were excluded by the data filtering method to avoid contamination, ultimately leaving 418 k data. Finally, we use GPT-4-0613 to generate the answer with a step-by-step format, and leverage them for supervised fine-tuning.
Reward Models Training Data. To train the reward models, We conducted additional 5 rounds of evolution on the initial instruction set and obtain 90 k instructions. we use GPT-4-0613 to rank each instruction list with the quality from 1 to 6 as the training data of IRM. To obtain the training data of PRM, We use our Llama-2 70B SFT model to generate 5 answers for each instruction, and GPT-4-0613 is employed to assign correctness judgement for each reasoning step.
Implementation Details. We employ our method on two open-source foundational models Llama 2 (Touvron et al., 2023b) and Mistral-7B (Jiang et al., 2023). Llama 2 encompasses three distinct parameter sizes: 7B, 13B, and 70B. We utilize GPT-4-0613 for instruction evolution and the training data construction of reward models. For SFT, we train 3 epochs, and the learning rate is $2 \mathrm{e}-5,1 \mathrm{e}-5$ and $5 \mathrm{e}-6$ for Llama $27 \mathrm{~B} / 13 \mathrm{~B}, 70 \mathrm{~B}$ and Mistral-7B. The batch size is 512 , and the sequence length is 2048. For the reward model, we train Llama 2 and Mistral-7B with learning rate 4e-6 and 1e-6 for one epoch. For RL, the lr is $4 \mathrm{e}-7$ and $1 \mathrm{e}-7$ for Llama 2 and Mistral-7B and train one epoch.</p>
<h3>4.2 Main ReSults</h3>
<p>Table 1 shows the CoT (Wei et al., 2022) pass@1 results of the current state-of-the-art models on GSM8k and MATH. In this study, to ensure equitable and cohesive evaluations, we report the socres of all models within the settings of greedy decoding and CoT without using any external python tool.</p>
<p>Comparing with the proprietary Models. As shown in the Table 1, our WizardMath demonstrates notable superiority over various proprietary LLMs on the GSM8k and MATH benchmarks in terms of pass@1:</p>
<p>1) WizardMath-Llama 70B, the largest model, demonstrated exceptional performance on the GSM8k and MATH , surpassing earlier versions of GPT-4, Claude-2, and Gemini Pro, and performing on par with GPT-4-0314. It significantly outperformed GPT-3.5-Turbo by $11.2 \%$ on GSM8k and by $15.5 \%$ on MATH.
2) WizardMath-Mistral 7B, the smallersized model, outperformed Baichuan 3 on GSM8k ( 90.7 vs. 87.6) and surpassed GPT-4-0314 on MATH ( 55.4 vs. 52.6), significantly exceeding the performance of GPT-3.5-Turbo and Gemini Pro. Meanwhile, WizardMath-Mathstral, trained on Mathstral-7B-v0.1, demonstrated performance comparable to GPT-4-turbo-0125. Additionally, WizardMath-Qwen, trained on Qwen2.5-Math, surpassed GPT-4-2024-0513 on MATH ( 77.8 vs. 76.6 ).</p>
<p>Comparing with the Open-Source Models. The results presented in Table 1 unequivocally indicate that our WizardMath-Llama 70B exhibits a significant performance superiority over strong models in both the GSM8k and MATH benchmarks with higher data efficiency across the range from 0.1 B to 70 B parameters. The detailed results are as follows:</p>
<p>1) With the same model parameter size, our model surpasses the previous best model such as MetaMath, MAmmoTH2-Plus, XwinMath. Particularly, WizardMath-Llama 70B achieves a substantial improvement of $10.5 \%$ on GSM8K and $32.0 \%$ on MATH compared to MetaMath-Llama 70B in testing accuracy. In the Table 2, we show the detailed results of MATH subtopics with our WizardMath 70B model. Specifically, WizardMath-Mistral 7B also surpasses top-tier open source models, outperforming MetaMath-Mistral 7B with a notable margin ( 90.7 vs 77.9 on GSM8k) and ( 55.4 vs 28.6 on MATH). It demonstrats the effectiveness</p>
<p>Table 1: The models' CoT pass@1 results on GSM8k and MATH without using any external python tool.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Base</th>
<th>Params</th>
<th>GSM8k</th>
<th>MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proprietary models</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-v1 (OpenAI, 2023)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>94.8</td>
</tr>
<tr>
<td>GPT-v1-mm</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>90.0</td>
</tr>
<tr>
<td>Gemini-1.5.002</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>86.5</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet (Bai et al., 2022)</td>
<td>-</td>
<td>-</td>
<td>96.4</td>
<td>71.1</td>
</tr>
<tr>
<td>GPT-4v-2024-0513</td>
<td>-</td>
<td>-</td>
<td>96.1</td>
<td>76.6</td>
</tr>
<tr>
<td>GPT-4-turbo-0125 (OpenAI, 2023)</td>
<td>-</td>
<td>-</td>
<td>94.2</td>
<td>64.5</td>
</tr>
<tr>
<td>GPT-4-0314</td>
<td>-</td>
<td>-</td>
<td>94.7</td>
<td>52.6</td>
</tr>
<tr>
<td>GPT-4 (original version)</td>
<td>-</td>
<td>-</td>
<td>92.0</td>
<td>42.5</td>
</tr>
<tr>
<td>Baichuan-3 (Yang et al., 2023)</td>
<td>-</td>
<td>-</td>
<td>88.2</td>
<td>49.2</td>
</tr>
<tr>
<td>GLM-4 (GLM et al., 2024)</td>
<td>-</td>
<td>-</td>
<td>87.6</td>
<td>47.9</td>
</tr>
<tr>
<td>Gemini Pro (Yeam, 2023)</td>
<td>-</td>
<td>-</td>
<td>86.5</td>
<td>32.6</td>
</tr>
<tr>
<td>Claude2</td>
<td>-</td>
<td>-</td>
<td>85.2</td>
<td>32.5</td>
</tr>
<tr>
<td>GPT-3.5-Turbo</td>
<td>-</td>
<td>-</td>
<td>81.6</td>
<td>43.1</td>
</tr>
<tr>
<td>PaLM2 (Aati et al., 2023)</td>
<td>-</td>
<td>-</td>
<td>80.7</td>
<td>34.3</td>
</tr>
<tr>
<td>Minerva (Lewkowycz et al., 2022)</td>
<td>-</td>
<td>540B</td>
<td>58.8</td>
<td>33.6</td>
</tr>
<tr>
<td>GPT3.5 (Brown et al., 2020a)</td>
<td>-</td>
<td>-</td>
<td>57.1</td>
<td>-</td>
</tr>
<tr>
<td>Open-Source Models ( 0.1 B-3B)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-2-Small (Brown et al., 2020b)</td>
<td>-</td>
<td>0.1B</td>
<td>6.9</td>
<td>5.4</td>
</tr>
<tr>
<td>GPT-2-Medium (Brown et al., 2020b)</td>
<td>-</td>
<td>0.3B</td>
<td>11.2</td>
<td>6.2</td>
</tr>
<tr>
<td>GPT-2-Large (Brown et al., 2020b)</td>
<td>-</td>
<td>0.7B</td>
<td>13.6</td>
<td>6.4</td>
</tr>
<tr>
<td>GPT-2-XL (Brown et al., 2020b)</td>
<td>-</td>
<td>1.5B</td>
<td>15.4</td>
<td>6.9</td>
</tr>
<tr>
<td>WizardMath-GPT</td>
<td>GPT-2-Small</td>
<td>0.1B</td>
<td>26.4</td>
<td>12.3</td>
</tr>
<tr>
<td>WizardMath-GPT</td>
<td>GPT-2-Medium</td>
<td>0.3B</td>
<td>38.7</td>
<td>15.6</td>
</tr>
<tr>
<td>WizardMath-GPT</td>
<td>GPT-2-Large</td>
<td>0.7B</td>
<td>50.1</td>
<td>21.2</td>
</tr>
<tr>
<td>WizardMath-GPT</td>
<td>GPT-2-XL</td>
<td>1.5B</td>
<td>58.9</td>
<td>25.4</td>
</tr>
<tr>
<td>WizardMath-Qwen</td>
<td>Qwen-Math-2.5</td>
<td>1.5B</td>
<td>86.7</td>
<td>48.6</td>
</tr>
<tr>
<td>Llama-3.2-Instruct (Dabey et al., 2024)</td>
<td>Llama 3.2</td>
<td>1B</td>
<td>44.4</td>
<td>30.6</td>
</tr>
<tr>
<td>WizardMath-Llama</td>
<td>Llama 3.2</td>
<td>1B</td>
<td>63.3</td>
<td>33.5</td>
</tr>
<tr>
<td>Llama-3.2-Instruct</td>
<td>Llama 3.2</td>
<td>3B</td>
<td>77.7</td>
<td>48.0</td>
</tr>
<tr>
<td>WizardMath-Llama</td>
<td>Llama 3.2</td>
<td>3B</td>
<td>85.5</td>
<td>49.9</td>
</tr>
<tr>
<td>Open-Source Models ( 7 B-8B)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Llama-2 (Touvron et al., 2023b)</td>
<td>-</td>
<td>7B</td>
<td>14.6</td>
<td>2.5</td>
</tr>
<tr>
<td>MAmmoTH-CoT (Yue et al., 2023)</td>
<td>Llama-2</td>
<td>7B</td>
<td>50.5</td>
<td>10.4</td>
</tr>
<tr>
<td>MathScale (Yang et al., 2024)</td>
<td>Llama-2</td>
<td>7B</td>
<td>66.3</td>
<td>31.1</td>
</tr>
<tr>
<td>MetaMath (Yu et al., 2023b)</td>
<td>Llama-2</td>
<td>7B</td>
<td>66.5</td>
<td>19.8</td>
</tr>
<tr>
<td>MuggleMath (Li et al., 2023a)</td>
<td>Llama-2</td>
<td>7B</td>
<td>68.4</td>
<td>-</td>
</tr>
<tr>
<td>Skywork-Math (Zeng et al., 2024)</td>
<td>Llama-2</td>
<td>7B</td>
<td>72.9</td>
<td>47.7</td>
</tr>
<tr>
<td>Math-Shepherd (Wang et al., 2024a)</td>
<td>Llama-2</td>
<td>7B</td>
<td>73.2</td>
<td>21.6</td>
</tr>
<tr>
<td>Xwin-Math (Li et al., 2024a)</td>
<td>Llama-2</td>
<td>7B</td>
<td>82.6</td>
<td>40.6</td>
</tr>
<tr>
<td>WizardMath-Llama</td>
<td>Llama-2</td>
<td>7B</td>
<td>84.1</td>
<td>43.5</td>
</tr>
<tr>
<td>Mistral-v0.3 (Jiang et al., 2023)</td>
<td>-</td>
<td>7B</td>
<td>42.9</td>
<td>12.9</td>
</tr>
<tr>
<td>MathScale (Tang et al., 2024)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>74.8</td>
<td>35.2</td>
</tr>
<tr>
<td>MMIGC (Liu &amp; Yao, 2024)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>74.8</td>
<td>36.0</td>
</tr>
<tr>
<td>MetaMath (Yu et al., 2023b)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>77.9</td>
<td>28.6</td>
</tr>
<tr>
<td>KPMath-Plus (Huang et al., 2024b)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>82.1</td>
<td>46.8</td>
</tr>
<tr>
<td>DART-Math (Tang et al., 2024)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>82.6</td>
<td>43.5</td>
</tr>
<tr>
<td>Skywork-Math (Zeng et al., 2024)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>83.9</td>
<td>51.2</td>
</tr>
<tr>
<td>Math-Shepherd (Wang et al., 2024a)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>84.1</td>
<td>33.0</td>
</tr>
<tr>
<td>MAmmoTH2-Plus (Yue et al., 2024)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>84.7</td>
<td>45.0</td>
</tr>
<tr>
<td>JiuZhang3.0 (Zhou et al., 2024)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>88.6</td>
<td>52.8</td>
</tr>
<tr>
<td>Xwin-Math (Li et al., 2024a)</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>89.2</td>
<td>43.7</td>
</tr>
<tr>
<td>WizardMath-Mistral</td>
<td>Mistral-v0.1</td>
<td>7B</td>
<td>90.7</td>
<td>55.4</td>
</tr>
<tr>
<td>WizardMath-Mistral</td>
<td>Mistral-v0.3</td>
<td>7B</td>
<td>90.4</td>
<td>55.6</td>
</tr>
<tr>
<td>WizardMath-Mathstral</td>
<td>Mathstral-v0.1</td>
<td>7B</td>
<td>93.8</td>
<td>70.9</td>
</tr>
<tr>
<td>WizardMath-Qwen</td>
<td>Qwen2.5-Math</td>
<td>7B</td>
<td>93.9</td>
<td>77.8</td>
</tr>
<tr>
<td>WizardMath-Qwen</td>
<td>Qwen2.5</td>
<td>7B</td>
<td>94.0</td>
<td>74.5</td>
</tr>
<tr>
<td>DeepSealMath-Base (Shao et al., 2024)</td>
<td>-</td>
<td>7B</td>
<td>64.2</td>
<td>36.2</td>
</tr>
<tr>
<td>NeminaMath-CoT (Li et al., 2024c)</td>
<td>DeepsealMath</td>
<td>7B</td>
<td>75.4</td>
<td>55.2</td>
</tr>
<tr>
<td>MMIGC (Liu &amp; Yao, 2024)</td>
<td>DeepSealMath</td>
<td>7B</td>
<td>75.0</td>
<td>47.3</td>
</tr>
<tr>
<td>KPMath-Plus (Huang et al., 2024b)</td>
<td>DeepSealMath</td>
<td>7B</td>
<td>83.9</td>
<td>48.8</td>
</tr>
<tr>
<td>DeepSealMath-RL (Shao et al., 2024)</td>
<td>DeepSealMath</td>
<td>7B</td>
<td>88.2</td>
<td>51.7</td>
</tr>
<tr>
<td>DART-Math (Tang et al., 2024)</td>
<td>DeepSealMath</td>
<td>7B</td>
<td>88.2</td>
<td>52.9</td>
</tr>
<tr>
<td>WizardMath-DeepSeal</td>
<td>DeepSealMath</td>
<td>7B</td>
<td>91.0</td>
<td>64.6</td>
</tr>
<tr>
<td>MetaMath (Yu et al., 2023b)</td>
<td>Llama 3</td>
<td>8B</td>
<td>77.3</td>
<td>20.6</td>
</tr>
<tr>
<td>MMIGC (Liu &amp; Yao, 2024)</td>
<td>Llama 3</td>
<td>8B</td>
<td>77.6</td>
<td>29.5</td>
</tr>
<tr>
<td>DART-Math (Tang et al., 2024)</td>
<td>Llama 3</td>
<td>8B</td>
<td>82.5</td>
<td>45.3</td>
</tr>
<tr>
<td>MAmmoTH2-Plus (Yue et al., 2024)</td>
<td>Llama 3</td>
<td>8B</td>
<td>84.1</td>
<td>42.8</td>
</tr>
<tr>
<td>Llama 3.1-Instruct (Dabey et al., 2024)</td>
<td>Llama 3</td>
<td>8B</td>
<td>84.5</td>
<td>51.9</td>
</tr>
<tr>
<td>JiuZhang3.0 (Zhou et al., 2024)</td>
<td>Llama 3</td>
<td>8B</td>
<td>88.6</td>
<td>51.0</td>
</tr>
<tr>
<td>WizardMath-Llama</td>
<td>Llama 3</td>
<td>8B</td>
<td>90.3</td>
<td>58.8</td>
</tr>
<tr>
<td>Open-Source Models (13B)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Llama-2 (Touvron et al., 2023b)</td>
<td>-</td>
<td>13B</td>
<td>28.7</td>
<td>3.9</td>
</tr>
<tr>
<td>MAmmoTH-CoT (Yue et al., 2023)</td>
<td>Llama 2</td>
<td>13B</td>
<td>56.3</td>
<td>12.9</td>
</tr>
<tr>
<td>MathScale (Tang et al., 2024)</td>
<td>Llama 2</td>
<td>13B</td>
<td>71.3</td>
<td>33.8</td>
</tr>
<tr>
<td>MetaMath (Yu et al., 2023b)</td>
<td>Llama 2</td>
<td>13B</td>
<td>72.3</td>
<td>22.4</td>
</tr>
<tr>
<td>MuggleMath (Li et al., 2023a)</td>
<td>Llama 2</td>
<td>13B</td>
<td>74.0</td>
<td>-</td>
</tr>
<tr>
<td>KPMath-Plus (Huang et al., 2024b)</td>
<td>Llama 2</td>
<td>13B</td>
<td>81.6</td>
<td>41.0</td>
</tr>
<tr>
<td>Xwin-Math (Li et al., 2024a)</td>
<td>Llama 2</td>
<td>13B</td>
<td>88.1</td>
<td>44.9</td>
</tr>
<tr>
<td>WizardMath-Llama</td>
<td>Llama 2</td>
<td>13B</td>
<td>89.7</td>
<td>50.6</td>
</tr>
<tr>
<td>Open-Source Models (70B)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Llama-2 (Touvron et al., 2023b)</td>
<td>-</td>
<td>70B</td>
<td>56.8</td>
<td>13.5</td>
</tr>
<tr>
<td>MAmmoTH-CoT (Yue et al., 2023)</td>
<td>Llama 2</td>
<td>70B</td>
<td>72.4</td>
<td>21.1</td>
</tr>
<tr>
<td>MetaMath (Yu et al., 2023b)</td>
<td>Llama-2</td>
<td>70B</td>
<td>82.3</td>
<td>26.6</td>
</tr>
<tr>
<td>KPMath-Plus (Huang et al., 2024b)</td>
<td>Llama 2</td>
<td>70B</td>
<td>87.4</td>
<td>48.6</td>
</tr>
<tr>
<td>Xwin-Math (Li et al., 2024a)</td>
<td>Llama-2</td>
<td>70B</td>
<td>90.6</td>
<td>52.8</td>
</tr>
<tr>
<td>WizardMath-Llama</td>
<td>Llama-2</td>
<td>70B</td>
<td>92.8</td>
<td>58.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of pass@1 (\%) on MATH subtopics (i.e., Intermediate Algebra, Geometry) with WizardMath 70B model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MATH subtopics</th>
<th style="text-align: center;">WizardMath 70B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Intermediate Algebra</td>
<td style="text-align: center;">36.3</td>
</tr>
<tr>
<td style="text-align: left;">Precalculus</td>
<td style="text-align: center;">38.9</td>
</tr>
<tr>
<td style="text-align: left;">Geometry</td>
<td style="text-align: center;">48.3</td>
</tr>
<tr>
<td style="text-align: left;">Number Theory</td>
<td style="text-align: center;">58.5</td>
</tr>
<tr>
<td style="text-align: left;">Counting \&amp; Probability</td>
<td style="text-align: center;">54.8</td>
</tr>
<tr>
<td style="text-align: left;">Prealgebra</td>
<td style="text-align: center;">74.6</td>
</tr>
<tr>
<td style="text-align: left;">Algebra</td>
<td style="text-align: center;">78.5</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: center;">$\mathbf{5 8 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Explore the effects of PRM and IRM during PPO training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2-XL-1.5B: WizardMath-SFT</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">18.3</td>
</tr>
<tr>
<td style="text-align: left;">+ PRM</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">22.1</td>
</tr>
<tr>
<td style="text-align: left;">+ PRM + IRM</td>
<td style="text-align: center;">$\mathbf{5 8 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B: WizardMath-SFT</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">35.6</td>
</tr>
<tr>
<td style="text-align: left;">+ PRM</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">39.9</td>
</tr>
<tr>
<td style="text-align: left;">+ PRM + IRM</td>
<td style="text-align: center;">$\mathbf{8 4 . 1}$</td>
<td style="text-align: center;">$\mathbf{4 3 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B: WizardMath-SFT</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: left;">+ PRM</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: left;">+ PRM + IRM</td>
<td style="text-align: center;">$\mathbf{9 0 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 4}$</td>
</tr>
</tbody>
</table>
<p>of our RLEIF method in enhancing mathematical reasoning capabilities across a range of problem difficulties, from grade to high school levels.
2) By employing diverse pre-trained models (i.e., GPT-2, Llama 2, Mistral, Qwen, DeepSeek) as base models, WizardMath demonstrated notable advancements on the GSM8k and MATH benchmarks. Specifically, WizardMath-Llama2-7B, based on Llama2-7B, improved performance by $69.5 \%$ on GSM8k and $41.0 \%$ on MATH. Similarly, WizardMath-GPT2-XL, built on GPT2-XL, achieved a $43.5 \%$ improvement on GSM8k and $18.5 \%$ on MATH, performing on par with Llama2-70B and outperforming GPT-3.5 on GSM8k. This demonstrates that our RLEIF method is equally effective for smaller models in enhancing mathematical reasoning capabilities, proving its scalability and robustness across various model backbones.</p>
<h1>4.3 ANALYSIS</h1>
<h2>The impact of training data size</h2>
<p>We are curious about to how the training data size of different dataset construction methods impact the reasoning capacity of LLMs. Thus we conduct different number of training instances from ours evolved data and MetaMathQA to fine tune Mistral 7B. As shown in the Figure 2, Math EvolInstruct achieves superior data efficiency. Specifically, our model constantly outperforms MataMath by more than $3 \% \sim 6 \%$ on GSM8k and $15 \% \sim 20 \%$ on MATH under the same number of conditions. Our findings indicate that Math Evol-Instruct exhibits a higher potential upper bound compared to MetaMath, thus demonstrating the effectiveness of Evol-Instruct for math reasoning senario.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Accuracy of Mistral-7B fine-tuned in different sizes of augmentation data on GSM8K and MATH</p>
<h2>The impact of PRM and IRM during PPO training</h2>
<p>To verify the contributions of the instruction reward model and process-supervised reward model, we consider the following variants: (1) SFT + PRM: only use PRM in the PPO training. (2) SFT + PRM + IRM: use both IRM and PRM in the PPO training. As shown in Table 3, applying PRM alone for PPO training on GSM8k and MATH yields a 3\%-4\% improvement. When combined with IRM, an additional $2.5 \%-4 \%$ gain is observed. Thus, the integration of PRM and IRM results in a substantial overall improvement of $6 \%-8 \%$. So, we can conclude that (1) PRM is crucial to WizardMath, since the variant with PRM significantly outperforms the SFT one without any PPO training (2) IRM also plays a key role in the success of reinforcement learning, as there is a remarkable improvement when</p>
<p>we combine PRM with IRM, further demonstrating the necessity of taking instruction's quality into account and correcting false positives in the problem-solving process when we optimize the LLMs.</p>
<p>Table 4: The effect of different reward models during PPO training</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>GSM8K</th>
<th>MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama2-7B: WizardMath-SFT</td>
<td>77.4</td>
<td>35.6</td>
</tr>
<tr>
<td>+ ORM (ours)</td>
<td>79.1</td>
<td>36.8</td>
</tr>
<tr>
<td>+ PRM800k</td>
<td>79.7</td>
<td>38.7</td>
</tr>
<tr>
<td>+ Math-Shepherd</td>
<td>80.3</td>
<td>38.2</td>
</tr>
<tr>
<td>+ PRM (ours)</td>
<td>$\mathbf{8 1 . 7}$</td>
<td>$\mathbf{3 9 . 9}$</td>
</tr>
<tr>
<td>Mistral-7B: WizardMath-SFT</td>
<td>82.8</td>
<td>48.1</td>
</tr>
<tr>
<td>+ ORM (ours)</td>
<td>84.6</td>
<td>49.6</td>
</tr>
<tr>
<td>+ PRM800k</td>
<td>85.4</td>
<td>50.8</td>
</tr>
<tr>
<td>+ Math-Shepherd</td>
<td>86.1</td>
<td>50.3</td>
</tr>
<tr>
<td>+ PRM (ours)</td>
<td>$\mathbf{8 7 . 2}$</td>
<td>$\mathbf{5 2 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results of reinforcement learning combined with validation. The SFT and Reward models are trained based on Mistral-7B. The verifier is based on 256 sample outputs.</p>
<table>
<thead>
<tr>
<th>Generators</th>
<th></th>
<th>Verifiers</th>
<th>GSM8K</th>
<th>MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td>SFT</td>
<td>Self-Consistency</td>
<td>90.7</td>
<td>57.5</td>
<td></td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>93.0</td>
<td>58.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PRM</td>
<td>93.9</td>
<td>61.7</td>
<td></td>
</tr>
<tr>
<td>SFT + ORM</td>
<td>Self-Consistency</td>
<td>91.2</td>
<td>57.7</td>
<td></td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>93.4</td>
<td>59.4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PRM</td>
<td>94.1</td>
<td>63.3</td>
<td></td>
</tr>
<tr>
<td>SFT + PRM</td>
<td>Self-Consistency</td>
<td>92.3</td>
<td>59.3</td>
<td></td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>94.1</td>
<td>60.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PRM</td>
<td>$\mathbf{9 5 . 2}$</td>
<td>$\mathbf{6 4 . 7}$</td>
<td></td>
</tr>
</tbody>
</table>
<p>The impact of Evol-Instruct turns. Table 6 illustrates the impact of combining downward and upward evolution in SFT training. Two rounds of downward evolution improved GSM8k by $14.8 \%$ (74.5 vs. 59.7 ) and MATH by $19.6 \%$ ( 34.7 vs. 15.1) over the original. Three rounds of upward evolution yielded a $18.9 \%$ improvement on GSM8k ( 78.6 vs. 59.7) and a $27.4 \%$ improvement on MATH ( 42.5 vs. 15.1). Furthermore, combining downward evolution based on upward evolution resulted in an additional $2.6 \%$ improvement on GSM8k ( 81.2 vs. 78.6), a total improvement of $21.5 \%$ over the original. Similarly, a $1.9 \%$ improvement on MATH ( 46.5 vs. 42.5 ), a $31.4 \%$ total improvement. These results underscore the complementary and significant effectiveness of upward and downward evolution. ORM v.s. PRM; Human v.s. AI. The Table 4 presents the performance of different answer reward methods for LLMs in terms of pass@1. As is shown: 1) Our step-by-step PRM significantly enhances the performance of both Llama and Mistral based SFT models. Specifically, the Mistral-7B powered by our PRM achieves $87.2 \%$ and $52.7 \%$ on GSM8k and MATH respectively. 2) PRM models consistently outperforms ORM on both GSM8k and MATH, indicating the effectiveness of step-by-step supervision. 3) The PRM trained on our fully AI-labeled data outperforms both the manually annotated PRM800k and Math-Shepherd, which utilizes MCTS tree search for annotation. When training WizardMath-Mistral-SFT with PPO, our PRM improves upon PRM800k by $1.8 \%$ and Math-Shepherd by $1.1 \%$ on GSM8k, while surpassing PRM800k by $1.9 \%$ and Math-Shepherd by $2.4 \%$ on MATH. This demonstrates powerful AI can also provide good process supervision quality, highlighting the effectiveness of utilizing AI to construct PRM training data.</p>
<p>PRM as Verifier. Table 5 presents the performance comparison of various generators with different verifiers on GSM8K and MATH in terms of pass@256. We find that: 1) PRM verifier consistently demonstrates superior performance compared to Self-Consistency and ORM. Specifically, our SFT + PRM generator, enhanced by the PRM verifier, achieves $95.2 \%$ and $64.7 \%$ accuracy on GSM8K and MATH respectively. 2) When compared to ORM, PRM exhibits a more significant advantage on the more challenging MATH dataset which aligns with the findings in (Uesato et al., 2022) and (Lightman et al., 2023). This can be attributed to the fact that GSM8K involves fewer and less complex steps in problem-solving than MATH. 3) Particularly, the generator with PRM PPO training</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of Mistral-7B SFT with different verification strategies.
surpasses those SFT and ORM PPO trained generators regardless of employing Self-Consistency, ORM, and the PRM verifiers. This further demonstrates the effectiveness of our PRM.</p>
<p>Figure 3 also shows the performance of different Verification strategies across a range of candidate numbers from 1 to 256 on two benchmarks. The main observations are as follows: 1) PRM verifiers consistently achieves superior performance compared to both ORM and majority voting, and this superiority becomes more evident as N increases. 2) For MATH benchmark, our PRM trained on the AI-annotated datasets slightly surpassed the human-annotated PRM800K.</p>
<p>Performance of Out-of-Domain. Table 7 presents the results of WizardMath on the 7 out-of-domain evaluation results covering K-12, college, and competition level math problems, highlighting the following salient observations: (1) With math Evol-Instruct and reinforcement learning, WizardMath consistently surpasses prior state-of-theart open-source models (e.g. MetaMath, MathScale) across all scales, and achieves improvement of $5 \%-10 \%$ across 7 tasks on average. (2) The accuracy of WizardMath-Mistral is about $5.0 \%$ higher than WizardMath-Llama on the same size. Especially it exceeds GPT-3.5Turbo ( 45.7 vs. 37.9 ) while being comparable to GPT-4. This also indicates that Mistral-7B has more potential in mathematical reasoning. (3) Especially on difficult benchmarks (i.e., College Math, AGIE Gaokao Math), WizardMath outperforms MetaMath by a significant margin. This demonstrates our model and RLEIF method has stronger robustness and better significant generalization ability for invisible mathematical problems.</p>
<p>Table 7: Performance of WizardMath on the 7 out-ofdomain evaluation results covering K-12, college, and competition level math problems. The results of models in the table refer to MwPBENCH (Tang et al., 2024). "AGIE" stands for AGIEval. We report the models' CoT pass@1 results on MwpBench without using any external python tool</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">College <br> Math</th>
<th style="text-align: center;">TAL</th>
<th style="text-align: center;">Math23k</th>
<th style="text-align: center;">Ape210k</th>
<th style="text-align: center;">Gaokao <br> Bench <br> Math</th>
<th style="text-align: center;">AGIE <br> Gaokao <br> Math</th>
<th style="text-align: center;">AGIE <br> SAT <br> Math</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Proprietary models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: center;">Models based on LLaMA-2 13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 13B</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">4.7</td>
</tr>
<tr>
<td style="text-align: center;">MAmmoTH-CoT</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: center;">GAIR-Abel</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: center;">MetaMath</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: center;">MathScale 13B</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">35.9</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;">Models based on LLaMA-2 7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 7B</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">4.6</td>
</tr>
<tr>
<td style="text-align: center;">MAmmoTH-CoT</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">14.5</td>
</tr>
<tr>
<td style="text-align: center;">GAIR-Abel</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">16.7</td>
</tr>
<tr>
<td style="text-align: center;">MetaMath</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">21.9</td>
</tr>
<tr>
<td style="text-align: center;">MathScale 7B</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr>
<td style="text-align: center;">Models based on Mistral 7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mistral 7B</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">13.4</td>
</tr>
<tr>
<td style="text-align: center;">MetaMath-Mistral</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: center;">MathScale-Mistral</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath-Mistral</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">45.7</td>
</tr>
</tbody>
</table>
<p>Employ Open-source Model to Math Evol-Instruct. In Table 8, we investigate the use of open-source models (i.e., Llama-3-70B-Instruct) as a substitute for GPT-4 during the SFT stage for Evol Instruct, employing the same evolution strategy. The results demonstrate that WizardMath-</p>
<p>Table 9: A case study from GSM8k test set. We rate the response using PRM and ORM. Red text denotes the wrong reasoning steps which PRM successfully detected, but ORM failed.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">The farm has 30 cows and the zoo has 20 sheep. The zoo has twice as many cows as the farm does. The farm has half as many sheep as the zoo does. How many animals do the farm and zoo have combined? Ground truth answer: 120</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Steps</td>
<td style="text-align: center;">Step 1: we know that the farm has 30 cows. PRM Score: 0.95 . <br> Step 2: we are told that the zoo has twice as many cows as the farm does. PRM Score: 0.97 . <br> Step 3: So, we can calculate the number of cows in the zoo by multiplying the number of cows in the farm by $2 . \mathrm{So}, 2 * 30=60$, So, the zoo has 60 cows. PRM Score: 0.97 . <br> Step 4: we know that the farm has half as many sheep as the zoo does. PRM Score: 0.96 . <br> Step 5: So, we can calculate the number of sheep in the farm by dividing the number of sheep in the zoo by $2 . \mathrm{So}, 20 / 2=10$. So, the farm has 10 sheep. PRM Score: 0.93 . <br> Step 6: Now, we can calculate the total number of animals in the farm and the zoo by adding the number of cows and the number of sheep in both places. PRM Score: 0.92 . <br> Step 7: So, 30 (farm's cows) +60 (zoo's cows) +10 (farm's sheep) $=100$. PRM Score: 0.11 . <br> Step 8: So, the farm and the zoo have a total of 100 animals combined. PRM Score: 0.10 . <br> Step 9: Therefore, the final answer is 100 . The answer is: 100. PRM Score: 0.06. ORM Score: 0.89 .</td>
</tr>
</tbody>
</table>
<p>Llama3-Evol achieved a $33.8 \%$ improvement on GSM8k and a $30.6 \%$ improvement on MATH, indicating that the math evol instruct strategy remains effective on open-source models. However, compared to GPT-4 evolution, there is still a $5 \%-6 \%$ performance gap. Despite this, the strategy shows significant potential in balancing</p>
<p>Table 8: The impact of using open source models for Math-Evol and use Mistral-7B-v0.1 for SFT</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">GSM8k</th>
<th style="text-align: right;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mistral-7B-v0.1</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: right;">12.9</td>
</tr>
<tr>
<td style="text-align: left;">WizardMath-SFT-GPT-4-Evol</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: right;">48.1</td>
</tr>
<tr>
<td style="text-align: left;">WizardMath-SFT-Llama3-Evol</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: right;">43.5</td>
</tr>
</tbody>
</table>
<p>computational cost and accuracy.</p>
<h1>4.4 Data Contamination Check</h1>
<p>Apart from the performance analysis, we also investigate whether evolution leads to the data contamination between training data and test set. To address this consideration, we employ instructions in the GSM8k and MATH test set as queries to retrieve the top-5 samples from all evolved training data with an embedding model, gte-large (Li et al., 2023d). Additionally, we employ GPT-4 to provide similarity judgement between the test sets and the retrieved samples, and remove the top-2 similar instructions. The prompt and details are shown in Appendix A. 4 and A.5. Figure 4 illustrates that the evolution process does not yield higher similarity scores.</p>
<h3>4.5 CASE Study</h3>
<p>Evol-Instruct. The Examples 3 and 4 in the Appendix A. 1 shows the prompt and corresponding cases of GSM8k and MATH instruction evolution, demonstrating that the evolved instructions exhibit more complexity and diversity than the original training set.
PRM v.s. ORM. We present a comprehensive case study to illustrate the effectiveness of our PRM. As delineated in Table 9, PRM demonstrates precise performance on a challenge math problem from the GSM8k test set. Remarkably, our PRM effectively distinguished the incorrect solution, in the meanwhile the ORM struggled in this task. Furthermore, PRM demonstrated exceptional insight by accurately detecting the incorrect steps of the solution chosen by ORM, specifically the steps 7, 8, and 9. Subsequently, PRM also assigned lower score logits to these erroneous steps.</p>
<h2>5 CONCLUSION</h2>
<p>This paper introduces WizardMath, a mathematics model fine-tuned with RLEIF. The experimental results demonstrate that WizardMath achieves SOTA performance surpassing existing open-source LLMs on GSM8k and MATH from grade to high school problems. Notably, WizardMath 70B exhibits superior performance compared to some of the well-known proprietary LLMs, including ChatGPT3.5, Claude Instant, PaLM-2, Gemini Pro. Furthermore, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional performance.</p>
<h1>REFERENCES</h1>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024.</p>
<p>Avinash Anand, Mohit Gupta, Kritarth Prasad, Navya Singla, Sanjana Sanjeev, Jatin Kumar, Adarsh Raj Shivam, and Rajiv Ratn Shah. Mathify: Evaluating large language models on mathematical problem solving tasks. arXiv preprint arXiv:2404.13099, 2024.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Shusheng Yang, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. ArXiv, abs/2309.16609, 2023. URL https://api.semanticscholar.org/CorpusID: 263134555 .</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>DeepSeek-AI Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yi Xiong, Hanwei Xu, Ronald X Xu, Yanhong Xu, Dejian Yang, Yu mei You, Shuiping Yu, Xin yuan Yu, Bo Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. ArXiv, abs/2401.02954, 2024. URL https://api.semanticscholar.org/CorpusID:266818336.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott</p>
<p>Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020b.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize via recursion. arXiv preprint arXiv:1704.06611, 2017.</p>
<p>Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.</p>
<p>Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. Autoprm: Automating procedural supervision for multi-step reasoning via controllable question decomposition. arXiv preprint arXiv:2402.11452, 2024a.</p>
<p>Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, and Ji-Rong Wen. Improving large language models via fine-grained reinforcement learning with minimum editing constraint. arXiv preprint arXiv:2401.06081, 2024b.</p>
<p>Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024c.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https : //vicuna.lmsys.org.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023.</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867, 2023.</p>
<p>Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, et al. Kwaiyiimath: Technical report. arXiv preprint arXiv:2310.07488, 2023a.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.</p>
<p>Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306, 2023b.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021. doi: 10.1162/tacl_a_00370. URL https://aclanthology.org/2021.tacl-1.21.</p>
<p>Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2023.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523-533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/D14-1058.</p>
<p>Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao, and Benyou Wang. Mamo: a mathematical modeling benchmark with solvers. arXiv preprint arXiv:2405.13144, 2024a.</p>
<p>Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-point-driven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333, 2024b.</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024a.</p>
<p>Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James Kwok. Forward-backward reasoning in large language models for mathematical verification. In Findings of the Association for Computational Linguistics ACL 2024, pp. 6647-6661, 2024b.</p>
<p>Zhanming Jie, Jierui Li, and Wei Lu. Learning to reason deductively: Math word problem solving as complex relation extraction. arXiv preprint arXiv:2203.10316, 2022.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597, 2015.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.</p>
<p>Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. Mwptoolkit: an open-source framework for deep learning-based math word problem solvers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. $13188-13190,2022$.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024a.</p>
<p>Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Query and response augmentation cannot help out-of-domain math reasoning generalization. ArXiv, abs/2310.05506, 2023a. URL https://api.semanticscholar. org/CorpusID:263830207.</p>
<p>Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu. Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning. arXiv preprint arXiv:2407.04078, 2024b.</p>
<p>Chengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. Neural program lattices. In International Conference on Learning Representations, 2016. URL https : //api.semanticscholar.org/CorpusID:34816748.</p>
<p>Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. 2024c.</p>
<p>Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. arXiv preprint arXiv:2402.19255, 2024d.</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, B. Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Annual Meeting of the Association for Computational Linguistics, 2022a. URL https://api.semanticscholar.org/ CorpusID:259370847.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022b.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.291. URL https://aclanthology.org/2023.acl-long. 291.</p>
<p>Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. ArXiv, abs/2308.03281, 2023d. URL https://api.semanticscholar.org/CorpusID:260682258.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. $A C L, 2017$.</p>
<p>Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving&gt; $80 \%$ on gsm8k with small language models. arXiv preprint arXiv:2312.09241, 2023.</p>
<p>Haoxiong Liu and Andrew Chi-Chih Yao. Augmenting math word problems via iterative question composing. arXiv preprint arXiv:2401.09003, 2024.</p>
<p>Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On llms-driven synthetic data generation, curation, and evaluation: A survey. arXiv preprint arXiv:2406.15126, 2024.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535, 2022.</p>
<p>Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms. arXiv preprint arXiv:2402.16352, 2024.</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.</p>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a referencefree reward. arXiv preprint arXiv:2405.14734, 2024.</p>
<p>Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, et al. Agentinstruct: Toward generative teaching with agentic flows. arXiv preprint arXiv:2407.03502, 2024a.</p>
<p>Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024b.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning math reasoning from self-sampled correct and partially-correct solutions. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Exploring the mystery of influential data for mathematical reasoning. arXiv preprint arXiv:2404.01067, 2024.</p>
<p>Eric Nichols, Leo Gao, and Randy Gomez. Collaborative storytelling with large-scale neural language models. In Proceedings of the 13th ACM SIGGRAPH Conference on Motion, Interaction and Games, pp. 1-10, 2020.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094, 2021.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/D15-1202.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.</p>
<p>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.</p>
<p>Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. Timo: Towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192, 2024.</p>
<p>Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472, 2024.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149-4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https : //aclanthology.org/N19-1421.</p>
<p>Zhengyang Tang, Xingxing Zhang, Benyou Wan, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884, 2024.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Gemini Team. Gemini: A family of highly capable multimodal models, 2023.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.</p>
<p>Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward selfimprovement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253, 2024.</p>
<p>Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. arXiv preprint arXiv:2407.13690, 2024.</p>
<p>Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint arXiv:2402.10176, 2024.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.</p>
<p>Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. arXiv preprint arXiv:2310.03731, 2023a.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023b.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. ArXiv, abs/2309.02144, 2023c. URL https://api.semanticscholar.org/CorpusID:261558535.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. ArXiv, abs/2305.17926, 2023d. URL https://api.semanticscholar.org/CorpusID:258960339.</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9426-9439, 2024a.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR 2023, 2023e. URL https://arxiv.org/abs/2203.11171.</p>
<p>Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 8696-8708. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.685. URL https://doi.org/10. 18653/v1/2021.emnlp-main.685.</p>
<p>Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for math: Part i-mathpile: A billion-tokenscale pretraining corpus for math. arXiv preprint arXiv:2312.17120, 2023f.</p>
<p>Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision. arXiv preprint arXiv:2402.02658, 2024b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, et al. Yuan 2.0: A large language model with localized filtering-based attention. arXiv preprint arXiv:2311.15786, 2023a.</p>
<p>Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models. arXiv preprint arXiv:2402.14660, 2024.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. ArXiv, abs/2306.01693, 2023b. URL https://api. semanticscholar.org/CorpusID:259064099.</p>
<p>Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. arXiv preprint arXiv:2404.05692, 2024.</p>
<p>Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, et al. Building math agents with multi-turn iterative preference learning. arXiv preprint arXiv:2409.02392, 2024.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024.</p>
<p>Fei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in mathematical reasoning. ArXiv, abs/2311.09724, 2023a. URL https://api.semanticscholar. org/CorpusID:265221057.</p>
<p>Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. In Forty-first International Conference on Machine Learning, 2024.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023b.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023b.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023c.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548, 2024.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.</p>
<p>Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, et al. Skywork-math: Data scaling laws for mathematical reasoning in large language models-the story goes on. arXiv preprint arXiv:2407.08348, 2024.</p>
<p>Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024a.</p>
<p>Di Zhang, Jiatong Li, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024b.</p>
<p>Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, and Yashar Moshfeghi. Geoeval: benchmark for evaluating llms and multi-modal models on geometry problem-solving. arXiv preprint arXiv:2402.10104, 2024c.</p>
<p>Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. Self-contrast: Better reflection through inconsistent solving perspectives. arXiv preprint arXiv:2401.02009, 2024d.</p>
<p>Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew C Yao. Autonomous data selection with language models for mathematical texts. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2024e.</p>
<p>Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333, 2023.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625, 2022. URL https://api.semanticscholar.org/CorpusID:248986239.</p>
<p>Kun Zhou, Beichen Zhang, Jiapeng Wang, Zhipeng Chen, Wayne Xin Zhao, Jing Sha, Zhichao Sheng, Shijin Wang, and Ji-Rong Wen. Jiuzhang3. 0: Efficiently improving mathematical reasoning by training small data synthesis models. arXiv preprint arXiv:2405.14365, 2024.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long. 245. URL https://doi.org/10.18653\%2Fv1\%2F2023.acl-long. 245.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Math Evolution Prompts</h2>
<h2>Example 1: Upward Evolution Prompt</h2>
<p>Step 1: Understand the core concept and structure of the "#Instruction#". Identify the key elements such as variables, conditions, participants, actions, or processes that can be manipulated to increase complexity. Also, recognize the theme of the instruction and ensure it remains consistent throughout the evolution.</p>
<p>Step 2: Formulate a comprehensive plan to increment the complexity of the "#Instruction#" based on the identified elements in Step 1. The plan should involve modifying or expanding at least three components from the list. It is crucial to ensure that all components in the instruction are logically interconnected and that the complexity increase is coherent and justified. The plan should avoid introducing variables or conditions without clear criteria for determining their values or without contributing to the overall complexity. In this step, consider adding more real-world constraints and dependencies between variables to make the problem more challenging. And you can also add more constraints, concretizing, increasing reasoning.</p>
<p>Step 3: Implement the plan step by step to create the "#Rewritten Instruction#". Ensure the rewritten instruction maintains a logical sequence and avoids ambiguity or confusion. If additional variables or conditions are introduced, provide clear and unambiguous methods or criteria for determining their values. The "#Rewritten Instruction#" should not exceed the original "#Instruction#" by more than 30 words to ensure readability and comprehension.</p>
<p>Step 4: Review the "#Rewritten Instruction#" thoroughly to identify any unreasonable elements or inconsistencies. Make sure the "#Rewritten Instruction#" is a more complex version of the "#Instruction#", and that it accurately reflects the intended increase in complexity. Adjust any part of the instruction that may lead to misunderstanding or ambiguity, and provide the "#Finally Rewritten Instruction#" without any supplementary explanation.</p>
<p>Please reply strictly in the following format:
Step 1
#Elements Identified#:
Step 2
#Plan#:
Step 3
#Rewritten Instruction#:
Step 4
#Finally Rewritten Instruction#:
#Instruction#:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal contribution. Work done during the internship of Luo at Microsoft Research.</li>
</ul>
<p>1 Corresponding author.
1 https://openai.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
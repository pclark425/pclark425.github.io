<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2477 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2477</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2477</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-263830937</p>
                <p><strong>Paper Title:</strong> <a href="http://export.arxiv.org/pdf/2310.05955" target="_blank">Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables</a></p>
                <p><strong>Paper Abstract:</strong> Complex system design problems, such as those involved in aerospace engineering, require the use of numerically costly simulation codes in order to predict the performance of the system to be designed. In this context, these codes are often embedded into an optimization process to provide the best design while satisfying the design constraints. Recently, new approaches, called Quality-Diversity, have been proposed in order to enhance the exploration of the design space and to provide a set of optimal diversified solutions with respect to some feature functions. These functions are interesting to assess trade-offs. Furthermore, complex design problems often involve mixed continuous, discrete, and categorical design variables allowing to take into account technological choices in the optimization problem. Existing Bayesian Quality-Diversity approaches suited for intensive high-fidelity simulations are not adapted to mixed variables constrained optimization problems. In order to overcome these limitations, a new Quality-Diversity methodology based on mixed variables Bayesian optimization strategy is proposed in the context of limited simulation budget. Using adapted covariance models and dedicated enrichment strategy for the Gaussian processes in Bayesian optimization, this approach allows to reduce the computational cost up to two orders of magnitude, with respect to classical Quality-Diversity approaches while dealing with discrete choices and the presence of constraints. The performance of the proposed method is assessed on a benchmark of analytical problems as well as on two aerospace system design problems highlighting its efficiency in terms of speed of convergence. The proposed approach provides valuable trade-offs for decision-markers for complex system design.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2477.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2477.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian-QD-mixed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Quality‑Diversity for constrained optimization with mixed continuous, discrete and categorical variables</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surrogate-assisted Quality‑Diversity (QD) optimization system that uses Gaussian processes with mixed-variable covariance models, an acquisition/infill loop and an adapted MAP‑Elites procedure to allocate a limited budget of expensive evaluations to both improve surrogate models and populate a diverse archive of high‑quality solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Quality‑Diversity algorithm (mixed vars, constrained)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system builds Gaussian process (GP) surrogates for the objective, each feature function and constraints using kernels adapted to mixed continuous/discrete/categorical inputs (either Gower/Compound Symmetry kernel or hypersphere‑decomposition kernel). Starting from an initial design of experiments (DoE), the system iteratively: (1) fits/upgrades GPs; (2) solves an auxiliary infill optimization on the surrogate models using an adapted MAP‑Elites (population + mutation + binning by features) to propose candidate elites; (3) selects a subset of elites (Sobol' sequence used to ensure uniform coverage of the feature space) for expensive exact evaluation; (4) updates the DoE and GPs; (5) repeats until a fixed evaluation budget or stagnation. Constraint handling in the infill optimization uses Expected Violation (EV) computed from the GP predictive distributions and a threshold t_i. The infill acquisition criterion is the Lower Confidence Bound (LCB): mean - k * std (k controls exploration/exploitation). The archive is a discretized feature grid (MAP‑Elites style) that explicitly promotes diversity across features while maintaining quality within each niche.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Engineering design optimization (demonstrated on benchmark analytic functions and an aerospace 3D wing aerodynamic design problem); general black‑box, expensive simulation optimization where diverse high‑quality solutions are desired.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocation is done by iteratively optimizing an acquisition/infill problem on the GP surrogates: an adapted MAP‑Elites search (on surrogates) generates many candidate elites; a Sobol' sequence is then used to select a limited number of elites that provide uniform coverage of the feature space; only these selected elites are evaluated with the expensive true simulator. The acquisition objective is LCB (mean - k * std) subject to GP‑based Expected Violation constraints. This allocates computational budget to candidates that trade off predicted quality, uncertainty, and feasibility, while ensuring coverage/diversity via the MAP‑Elites archive and Sobol' selection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of exact (expensive) function evaluations; authors explicitly budget total expensive evaluations (examples: 160 expensive evaluations for analytic tests, ~220 for Styblinski‑Tang full run, MAP‑Elites comparison at 30,000 evaluations). Wall time and cores are discussed qualitatively (VLM run ≈ 5 minutes on 12 cores) but primary budget metric is count of exact simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive variance / standard deviation is used inside the LCB acquisition (mean - k * std) and in Expected Violation (EV) computation for constraints; selection of candidates to evaluate is driven by acquisition values that mix mean prediction and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Lower Confidence Bound acquisition (f_acq = µ - k·σ) with a fixed exploration factor k (used k = 2 in experiments) controls the tradeoff: larger k → more exploration (favoring high uncertainty), smaller k → more exploitation (favoring low predicted objective). Additionally, mutation operations in the MAP‑Elites inner search (Gaussian mutation for continuous, multinomial for categorical/discrete) introduce stochastic exploration. Expected Violation for constraints penalizes/filters candidates with high risk of infeasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit Quality‑Diversity mechanisms: discretized, multi‑dimensional feature grid (MAP‑Elites archive/niches) stores the best solution per niche; adapted MAP‑Elites is run on surrogate landscape to discover candidates across niches; Sobol' sequence selection of elites enforces uniform sampling across the feature space before expensive evaluation. Thus diversity is enforced by binning in feature space and by choosing expensive evaluations to cover feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive (exact) function evaluations (computational budget) and an optional stagnation stopping condition (no new elites in exact archive for given iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The loop stops when the user‑specified maximal number of exact evaluations is reached. Within each iteration the method only evaluates a limited number of elites (chosen by Sobol' for coverage) rather than all surrogate elites, thereby enforcing the budget. The GP surrogates and MAP‑Elites operate largely cheaply on surrogates to amortize information gain per expensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>No single formal 'breakthrough' score is introduced; the system identifies high‑impact or high‑quality discoveries via (a) improvement of objective value per niche (quality), and (b) the QD‑score (sum of objective values across illuminated niches) and number of illuminated niches as proxies for overall impact and novelty/diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include QD‑score (sum across illuminated niches; lower is better in experiments) and number of illuminated niches. Example numeric outcomes reported: Rosenbrock: Bayesian QD with 160 evaluations produced an archive close to MAP‑Elites after 30,000 evaluations; Styblinski‑Tang: Bayesian QD reached comparable final archive in ~220 evaluations vs MAP‑Elites ~30,000 evaluations; Bayesian QD typically discovered ≈1.5–2× more niches than MAP‑Elites under constrained evaluation budgets (e.g., median number of illuminated niches increased from 17 (MAP‑Elites, 10 individuals) to 24 (Bayesian QD) in one problem).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Modified MAP‑Elites (mixed continuous/discrete/categorical, with constraints) used as baseline; experiments compared MAP‑Elites with different population sizes (10 and 40), and large MAP‑Elites runs (30,000 evaluations) to show convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Bayesian QD converged faster and illuminated more niches per expensive‑evaluation budget. Specific comparisons: with only 160 exact evaluations Bayesian QD produced archives similar to MAP‑Elites run with 30,000 evaluations; in Styblinski‑Tang, Bayesian QD converged in ≈220 evaluations vs ≈30,000 for MAP‑Elites to reach similar archives; Bayesian QD discovered almost twice as many niches as MAP‑Elites in the Rosenbrock test under the same limited budget.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported efficiency gains are large: up to two orders of magnitude reduction in required expensive evaluations to reach comparable archives (e.g., ~220 vs ~30,000 evaluations). In other problems Bayesian QD discovered ≈2× more niches for the same evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper analyzes several tradeoffs: (1) exploration vs exploitation set by the LCB parameter k and evidenced by convergence behavior of QD‑score; (2) surrogate kernel expressiveness vs training difficulty: hypersphere kernel can represent richer covariance among categorical levels but introduces many more hyperparameters and more difficult GP training (authors observed Gower kernel often outperforms hypersphere when categorical levels are many because of fewer hyperparameters to optimize); (3) diversity vs evaluation cost: using MAP‑Elites on surrogates plus Sobol' selection balances discovering diverse candidates cheaply and spending scarce exact evaluations to refine many niches; (4) MAP‑Elites population size vs generations: smaller population with more generations tends to be more efficient under constrained budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Practical recommendations: use surrogate‑guided MAP‑Elites with LCB acquisition and EV constraint handling to allocate evaluations; prefer the Gower/compound symmetry kernel when categorical variables have many levels to limit GP hyperparameter complexity; set exploration factor (k) to a moderate value (k=2 used in experiments); select elites for exact evaluation via a quasi‑uniform sampler (Sobol') to ensure coverage across features; under tight budgets, run many surrogate MAP‑Elites generations with a small population rather than large population fewer generations; stop on a fixed evaluation budget or stagnation criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2477.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2477.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAP-Elites-adapted</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapted MAP‑Elites for surrogate infill optimization with mixed variables and constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified MAP‑Elites procedure used inside the Bayesian QD infill loop that operates on surrogate models, supports mixed continuous/discrete/categorical variables, and enforces feasibility with GP‑based constraint dominance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Illuminating search spaces by mapping elites</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adapted MAP‑Elites (for infill on surrogates)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Starts from a population sampled in mixed variable space; evaluates auxiliary objective/features/constraints using GP surrogates; builds an initial surrogate archive (niches in discretized feature grid); iteratively selects elites, applies mutation (Gaussian for continuous, multinomial for discrete/categorical with optional distance‑dependent PMFs), evaluates children on auxiliary functions, and updates the surrogate archive using constraint‑dominance rules (only feasible solutions wrt auxiliary constraints are added; niche replacements occur when offspring has better surrogate‑objective). After surrogate MAP‑Elites generations, a subset of elites from the surrogate final archive are selected (Sobol' sequence) for true evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Component of surrogate‑assisted QD for engineering design optimization; applicable to any expensive black‑box QD problem with mixed variables and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Performs a heavy, low‑cost search on surrogate models (many surrogate evaluations) to propose a large set of promising candidates (one per niche or many per niche), then selects a small, budget‑limited subset for real expensive evaluation using Sobol' selection to ensure coverage. Thus computational budget is allocated only to a chosen set of elite candidates discovered on cheap surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of expensive (true) evaluations are primary; surrogate MAP‑Elites evaluations are cheap and not counted against main budget.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Indirect — candidates are evaluated on surrogates that encode GP mean and variance; selection leverages these surrogate scores (LCB) so information gain is implicit via GP uncertainty shaping the surrogate landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration arises from mutation and repeated surrogate MAP‑Elites generations and from LCB in the auxiliary objective; exploitation arises from choosing best surrogate elites and replacing weaker niche incumbents on surrogate objective.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Standard MAP‑Elites binning into discretized feature grid (niches) promotes exploration across features; Sobol' selection of elites for expensive evaluation further enforces uniform coverage across the feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive evaluations; surrogate MAP‑Elites run many cheap steps to generate candidates without consuming expensive budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Controls the number of elites selected for exact evaluation per iteration (parameter p). The MAP‑Elites surrogate phase is used to maximize candidate diversity before committing to expensive evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Quality per niche (objective value) and ability to discover and fill previously empty niches (number of illuminated niches) are used as indicators of important discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as part of the Bayesian QD overall metrics: increases number of illuminated niches and reduces required expensive evaluations compared to baseline MAP‑Elites search performed directly on the true objective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Classical MAP‑Elites run directly on the expensive objective (various population sizes), and Bayesian QD variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Surrogate MAP‑Elites + selective true evaluation enables orders‑of‑magnitude fewer expensive evaluations for similar archive quality compared to running MAP‑Elites directly on the expensive simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Contributes to overall reported gains (e.g., reaching similar archive quality with 160–220 expensive evaluations vs ~30,000 for direct MAP‑Elites).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Touches on tradeoffs between depth of surrogate MAP‑Elites search (many cheap generations) and number of true evaluations per iteration (p); emphasizes using Sobol' to balance coverage vs concentrating on top predicted elites.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Run extensive surrogate MAP‑Elites to explore cheaply; select elites for real evaluation using a coverage‑oriented sampler (Sobol') to maximize information per expensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2477.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2477.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LCB+EV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lower Confidence Bound acquisition with Expected Violation constraint handling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Acquisition/infill criterion used inside the Bayesian QD loop: LCB (µ - k·σ) drives exploration/exploitation and Expected Violation (EV) computed from GP predictive distributions constrains candidate feasibility under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LCB acquisition combined with Expected Violation (EV) constraints</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The auxiliary infill optimization minimizes LCB = µ(x) - k·σ(x) of the surrogate objective (µ and σ from the GP) subject to EV_gi(x) ≤ t_i for each inequality constraint g_i. EV_gi(x) is computed analytically from the GP predictive mean and variance and standard normal pdf/CDF, yielding a probabilistic estimate of expected constraint violation that captures both mean prediction and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning / Bayesian optimization for constrained expensive black‑box functions (applied here to QD infill selection).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Directs resources toward points with low predicted objective minus an uncertainty term (favoring either low mean or high uncertainty depending on k) while enforcing probabilistic feasibility via EV thresholds; so expensive evaluations are allocated to points likely to yield either improved quality or high information about uncertain regions while respecting constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Acquisition optimization cost is measured in cheap surrogate evaluations; true evaluation cost measured in number of expensive function calls (primary budget).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive standard deviation (σ) enters LCB, so information gain is represented implicitly by σ; Expected Violation also uses σ to quantify risk/uncertainty about constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Scalar parameter k (authors used k = 2) scales the uncertainty term; increasing k biases selection toward exploration (high σ), decreasing k toward exploitation (low µ). EV constraint thresholds t_i tighten/loosen feasible region under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>The LCB+EV acquisition is used inside the MAP‑Elites surrogate search which supplies diversity; LCB alone does not promote diversity across features.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of true evaluations allocated per iteration; acquisition is optimized on surrogates and only a limited number of acquired candidates are evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Restricts the number of true evaluations taken from the acquisition outputs; acquisition optimization is performed cheaply and then candidate selection enforces budget by sampling elites (Sobol') rather than evaluating all acquisition-optimal candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Points with very low LCB (suggesting either low mean or high uncertainty that could reveal much better true values) and feasible under EV are considered potential breakthroughs; measured downstream by improvement in QD‑score and new niche illumination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors used LCB (k=2) with EV threshold t_i = 1e-4 in experiments; effectiveness judged via aggregate QD‑score and number of illuminated niches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicitly compared to population‑based search (MAP‑Elites on true objective) where no LCB+EV guidance is available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Using LCB+EV within the surrogate QD framework contributed to orders‑of‑magnitude reductions in the number of required expensive evaluations versus direct MAP‑Elites on the true functions.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Part of the combined method that achieved ~two orders of magnitude fewer expensive evaluations for comparable final archives.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discussed role of k (exploration factor) and EV threshold t_i in governing risk vs reward and feasibility; authors used moderate k and small t_i to balance exploration while keeping feasible solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Choose moderate k (k=2 used) and small EV thresholds to keep evaluations focused on promising, probably feasible candidates; combine with surrogate MAP‑Elites to ensure feature space coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2477.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2477.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Surrogate‑Assisted Illumination (SAIL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published surrogate‑assisted Quality‑Diversity method that uses surrogate models (GPs) to accelerate MAP‑Elites style illumination of design/behavior spaces; designed for continuous variables and no constraints in the original formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aerodynamic design exploration through surrogate-assisted illumination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Surrogate‑Assisted Illumination (SAIL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SAIL replaces expensive objective evaluations by surrogate models (GPs) and uses an acquisition procedure to propose candidates that populate a MAP‑Elites archive while limiting expensive evaluations. It was developed for continuous domains and introduced active learning strategies to accelerate illumination of the feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Design exploration and optimization in aerodynamics and other continuous engineering design spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Uses surrogate models and acquisition to decide where to evaluate expensive simulations to most improve the archive and surrogate accuracy; selects candidates that improve illumination or reduce uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive simulations (function evaluations) and surrogate fitting cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive uncertainty and acquisition functions for surrogate improvement and archive enhancement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions on GPs trade off predicted quality and uncertainty; integrated with MAP‑Elites to balance discovering new niches and improving incumbent quality.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>MAP‑Elites discretized feature archive to promote diversity across features.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget of expensive function evaluations; designed to reduce evaluations relative to naive MAP‑Elites.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Surrogate‑based infill and selective true evaluations to maximize archive improvement per expensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement of archive illumination and quality per niche (similar measures to QD‑score/illuminated niches).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in original SAIL work (not reproduced here) as reduced expensive evaluations to reach comparable illumination versus direct MAP‑Elites.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to MAP‑Elites and other surrogate‑assisted illumination variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SAIL demonstrated substantial reductions in expensive evaluations for continuous unconstrained QD problems (paper cites SAIL as antecedent; current paper extends ideas to mixed variables and constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2477.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2477.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPHEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Surrogate‑assisted PHEnotypic Niching (SPHEN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surrogate‑assisted QD algorithm that integrates surrogate models with phenotypic niching ideas to accelerate exploration and preserve diversity in continuous feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Designing air flow with surrogate-assisted phenotypic niching</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SPHEN (Surrogate‑assisted Phenotypic Niching)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SPHEN uses GP surrogates and niching strategies to guide search for diverse high‑quality solutions. It was a predecessor informing the proposed approach; the paper states the proposed algorithm is derived from SPHEN and SAIL but extended to mixed variables and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Design exploration (aerodynamics example) and surrogate‑assisted QD for continuous design spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Surrogate‑guided selection of candidates prioritized to improve diversity and quality across phenotypic niches.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive function calls and surrogate training cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive variance incorporated in acquisition; expected improvement/uncertainty reduction concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions combining predicted performance and uncertainty used inside a niching process.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Phenotypic niching and archive binning to preserve diverse solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget of expensive evaluations in surrogate‑assisted implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects a limited set of surrogate elites to evaluate exactly, prioritizing both novelty (niche discovery) and surrogate uncertainty reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Discovery of novel phenotypic niches with high predicted quality; archive coverage and niche quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2477.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2477.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOP-Elites</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization of Elites (BOP‑Elites / Bop‑elites)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimization approach specifically adapted to Quality‑Diversity search that treats descriptor/feature discovery as part of the acquisition process; referenced as a recent Bayesian QD method (originally for continuous descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bop-elites, a bayesian optimisation algorithm for qualitydiversity search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BOP‑Elites (Bayesian Optimization of Elites)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BOP‑Elites integrates Bayesian optimization techniques with QD search to propose elite solutions and uses GP uncertainty information to guide evaluations. The current paper cites it as an existing Bayesian QD approach developed for continuous variables; the proposed method extends the Bayesian QD idea to mixed variables and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Quality‑Diversity search and design exploration for continuous descriptor spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Uses GP acquisition functions to select evaluations that improve the elite set and the surrogate model, balancing exploration and exploitation within the QD framework.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive objective/descriptor evaluations; surrogate training cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive variance and Bayesian acquisition functions (e.g., EI, LCB) used to guide selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions (Bayesian) trading off mean prediction versus uncertainty; designed to populate elite archive efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Integrated with descriptor/feature discretization and elite selection to encourage diverse elites.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget of expensive evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selective evaluation guided by acquisition functions and archive needs.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Quality of elites across niches, archive coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Aerodynamic design exploration through surrogate-assisted illumination <em>(Rating: 2)</em></li>
                <li>Designing air flow with surrogate-assisted phenotypic niching <em>(Rating: 2)</em></li>
                <li>Bop-elites, a bayesian optimisation algorithm for qualitydiversity search <em>(Rating: 2)</em></li>
                <li>Efficient global optimization of constrained mixed variable problems <em>(Rating: 2)</em></li>
                <li>A general square exponential kernel to handle mixed-categorical variables for gaussian process <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2477",
    "paper_id": "paper-263830937",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Bayesian-QD-mixed",
            "name_full": "Bayesian Quality‑Diversity for constrained optimization with mixed continuous, discrete and categorical variables",
            "brief_description": "A surrogate-assisted Quality‑Diversity (QD) optimization system that uses Gaussian processes with mixed-variable covariance models, an acquisition/infill loop and an adapted MAP‑Elites procedure to allocate a limited budget of expensive evaluations to both improve surrogate models and populate a diverse archive of high‑quality solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Bayesian Quality‑Diversity algorithm (mixed vars, constrained)",
            "system_description": "The system builds Gaussian process (GP) surrogates for the objective, each feature function and constraints using kernels adapted to mixed continuous/discrete/categorical inputs (either Gower/Compound Symmetry kernel or hypersphere‑decomposition kernel). Starting from an initial design of experiments (DoE), the system iteratively: (1) fits/upgrades GPs; (2) solves an auxiliary infill optimization on the surrogate models using an adapted MAP‑Elites (population + mutation + binning by features) to propose candidate elites; (3) selects a subset of elites (Sobol' sequence used to ensure uniform coverage of the feature space) for expensive exact evaluation; (4) updates the DoE and GPs; (5) repeats until a fixed evaluation budget or stagnation. Constraint handling in the infill optimization uses Expected Violation (EV) computed from the GP predictive distributions and a threshold t_i. The infill acquisition criterion is the Lower Confidence Bound (LCB): mean - k * std (k controls exploration/exploitation). The archive is a discretized feature grid (MAP‑Elites style) that explicitly promotes diversity across features while maintaining quality within each niche.",
            "application_domain": "Engineering design optimization (demonstrated on benchmark analytic functions and an aerospace 3D wing aerodynamic design problem); general black‑box, expensive simulation optimization where diverse high‑quality solutions are desired.",
            "resource_allocation_strategy": "Allocation is done by iteratively optimizing an acquisition/infill problem on the GP surrogates: an adapted MAP‑Elites search (on surrogates) generates many candidate elites; a Sobol' sequence is then used to select a limited number of elites that provide uniform coverage of the feature space; only these selected elites are evaluated with the expensive true simulator. The acquisition objective is LCB (mean - k * std) subject to GP‑based Expected Violation constraints. This allocates computational budget to candidates that trade off predicted quality, uncertainty, and feasibility, while ensuring coverage/diversity via the MAP‑Elites archive and Sobol' selection.",
            "computational_cost_metric": "Number of exact (expensive) function evaluations; authors explicitly budget total expensive evaluations (examples: 160 expensive evaluations for analytic tests, ~220 for Styblinski‑Tang full run, MAP‑Elites comparison at 30,000 evaluations). Wall time and cores are discussed qualitatively (VLM run ≈ 5 minutes on 12 cores) but primary budget metric is count of exact simulations.",
            "information_gain_metric": "GP predictive variance / standard deviation is used inside the LCB acquisition (mean - k * std) and in Expected Violation (EV) computation for constraints; selection of candidates to evaluate is driven by acquisition values that mix mean prediction and uncertainty.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Lower Confidence Bound acquisition (f_acq = µ - k·σ) with a fixed exploration factor k (used k = 2 in experiments) controls the tradeoff: larger k → more exploration (favoring high uncertainty), smaller k → more exploitation (favoring low predicted objective). Additionally, mutation operations in the MAP‑Elites inner search (Gaussian mutation for continuous, multinomial for categorical/discrete) introduce stochastic exploration. Expected Violation for constraints penalizes/filters candidates with high risk of infeasibility.",
            "diversity_mechanism": "Explicit Quality‑Diversity mechanisms: discretized, multi‑dimensional feature grid (MAP‑Elites archive/niches) stores the best solution per niche; adapted MAP‑Elites is run on surrogate landscape to discover candidates across niches; Sobol' sequence selection of elites enforces uniform sampling across the feature space before expensive evaluation. Thus diversity is enforced by binning in feature space and by choosing expensive evaluations to cover feature space.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of expensive (exact) function evaluations (computational budget) and an optional stagnation stopping condition (no new elites in exact archive for given iterations).",
            "budget_constraint_handling": "The loop stops when the user‑specified maximal number of exact evaluations is reached. Within each iteration the method only evaluates a limited number of elites (chosen by Sobol' for coverage) rather than all surrogate elites, thereby enforcing the budget. The GP surrogates and MAP‑Elites operate largely cheaply on surrogates to amortize information gain per expensive evaluation.",
            "breakthrough_discovery_metric": "No single formal 'breakthrough' score is introduced; the system identifies high‑impact or high‑quality discoveries via (a) improvement of objective value per niche (quality), and (b) the QD‑score (sum of objective values across illuminated niches) and number of illuminated niches as proxies for overall impact and novelty/diversity.",
            "performance_metrics": "Reported metrics include QD‑score (sum across illuminated niches; lower is better in experiments) and number of illuminated niches. Example numeric outcomes reported: Rosenbrock: Bayesian QD with 160 evaluations produced an archive close to MAP‑Elites after 30,000 evaluations; Styblinski‑Tang: Bayesian QD reached comparable final archive in ~220 evaluations vs MAP‑Elites ~30,000 evaluations; Bayesian QD typically discovered ≈1.5–2× more niches than MAP‑Elites under constrained evaluation budgets (e.g., median number of illuminated niches increased from 17 (MAP‑Elites, 10 individuals) to 24 (Bayesian QD) in one problem).",
            "comparison_baseline": "Modified MAP‑Elites (mixed continuous/discrete/categorical, with constraints) used as baseline; experiments compared MAP‑Elites with different population sizes (10 and 40), and large MAP‑Elites runs (30,000 evaluations) to show convergence.",
            "performance_vs_baseline": "Bayesian QD converged faster and illuminated more niches per expensive‑evaluation budget. Specific comparisons: with only 160 exact evaluations Bayesian QD produced archives similar to MAP‑Elites run with 30,000 evaluations; in Styblinski‑Tang, Bayesian QD converged in ≈220 evaluations vs ≈30,000 for MAP‑Elites to reach similar archives; Bayesian QD discovered almost twice as many niches as MAP‑Elites in the Rosenbrock test under the same limited budget.",
            "efficiency_gain": "Reported efficiency gains are large: up to two orders of magnitude reduction in required expensive evaluations to reach comparable archives (e.g., ~220 vs ~30,000 evaluations). In other problems Bayesian QD discovered ≈2× more niches for the same evaluation budget.",
            "tradeoff_analysis": "The paper analyzes several tradeoffs: (1) exploration vs exploitation set by the LCB parameter k and evidenced by convergence behavior of QD‑score; (2) surrogate kernel expressiveness vs training difficulty: hypersphere kernel can represent richer covariance among categorical levels but introduces many more hyperparameters and more difficult GP training (authors observed Gower kernel often outperforms hypersphere when categorical levels are many because of fewer hyperparameters to optimize); (3) diversity vs evaluation cost: using MAP‑Elites on surrogates plus Sobol' selection balances discovering diverse candidates cheaply and spending scarce exact evaluations to refine many niches; (4) MAP‑Elites population size vs generations: smaller population with more generations tends to be more efficient under constrained budgets.",
            "optimal_allocation_findings": "Practical recommendations: use surrogate‑guided MAP‑Elites with LCB acquisition and EV constraint handling to allocate evaluations; prefer the Gower/compound symmetry kernel when categorical variables have many levels to limit GP hyperparameter complexity; set exploration factor (k) to a moderate value (k=2 used in experiments); select elites for exact evaluation via a quasi‑uniform sampler (Sobol') to ensure coverage across features; under tight budgets, run many surrogate MAP‑Elites generations with a small population rather than large population fewer generations; stop on a fixed evaluation budget or stagnation criterion.",
            "uuid": "e2477.0",
            "source_info": {
                "paper_title": "Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "MAP-Elites-adapted",
            "name_full": "Adapted MAP‑Elites for surrogate infill optimization with mixed variables and constraints",
            "brief_description": "A modified MAP‑Elites procedure used inside the Bayesian QD infill loop that operates on surrogate models, supports mixed continuous/discrete/categorical variables, and enforces feasibility with GP‑based constraint dominance.",
            "citation_title": "Illuminating search spaces by mapping elites",
            "mention_or_use": "use",
            "system_name": "Adapted MAP‑Elites (for infill on surrogates)",
            "system_description": "Starts from a population sampled in mixed variable space; evaluates auxiliary objective/features/constraints using GP surrogates; builds an initial surrogate archive (niches in discretized feature grid); iteratively selects elites, applies mutation (Gaussian for continuous, multinomial for discrete/categorical with optional distance‑dependent PMFs), evaluates children on auxiliary functions, and updates the surrogate archive using constraint‑dominance rules (only feasible solutions wrt auxiliary constraints are added; niche replacements occur when offspring has better surrogate‑objective). After surrogate MAP‑Elites generations, a subset of elites from the surrogate final archive are selected (Sobol' sequence) for true evaluation.",
            "application_domain": "Component of surrogate‑assisted QD for engineering design optimization; applicable to any expensive black‑box QD problem with mixed variables and constraints.",
            "resource_allocation_strategy": "Performs a heavy, low‑cost search on surrogate models (many surrogate evaluations) to propose a large set of promising candidates (one per niche or many per niche), then selects a small, budget‑limited subset for real expensive evaluation using Sobol' selection to ensure coverage. Thus computational budget is allocated only to a chosen set of elite candidates discovered on cheap surrogates.",
            "computational_cost_metric": "Counts of expensive (true) evaluations are primary; surrogate MAP‑Elites evaluations are cheap and not counted against main budget.",
            "information_gain_metric": "Indirect — candidates are evaluated on surrogates that encode GP mean and variance; selection leverages these surrogate scores (LCB) so information gain is implicit via GP uncertainty shaping the surrogate landscape.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration arises from mutation and repeated surrogate MAP‑Elites generations and from LCB in the auxiliary objective; exploitation arises from choosing best surrogate elites and replacing weaker niche incumbents on surrogate objective.",
            "diversity_mechanism": "Standard MAP‑Elites binning into discretized feature grid (niches) promotes exploration across features; Sobol' selection of elites for expensive evaluation further enforces uniform coverage across the feature space.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of expensive evaluations; surrogate MAP‑Elites run many cheap steps to generate candidates without consuming expensive budget.",
            "budget_constraint_handling": "Controls the number of elites selected for exact evaluation per iteration (parameter p). The MAP‑Elites surrogate phase is used to maximize candidate diversity before committing to expensive evaluations.",
            "breakthrough_discovery_metric": "Quality per niche (objective value) and ability to discover and fill previously empty niches (number of illuminated niches) are used as indicators of important discoveries.",
            "performance_metrics": "Used as part of the Bayesian QD overall metrics: increases number of illuminated niches and reduces required expensive evaluations compared to baseline MAP‑Elites search performed directly on the true objective.",
            "comparison_baseline": "Classical MAP‑Elites run directly on the expensive objective (various population sizes), and Bayesian QD variants.",
            "performance_vs_baseline": "Surrogate MAP‑Elites + selective true evaluation enables orders‑of‑magnitude fewer expensive evaluations for similar archive quality compared to running MAP‑Elites directly on the expensive simulator.",
            "efficiency_gain": "Contributes to overall reported gains (e.g., reaching similar archive quality with 160–220 expensive evaluations vs ~30,000 for direct MAP‑Elites).",
            "tradeoff_analysis": "Touches on tradeoffs between depth of surrogate MAP‑Elites search (many cheap generations) and number of true evaluations per iteration (p); emphasizes using Sobol' to balance coverage vs concentrating on top predicted elites.",
            "optimal_allocation_findings": "Run extensive surrogate MAP‑Elites to explore cheaply; select elites for real evaluation using a coverage‑oriented sampler (Sobol') to maximize information per expensive evaluation.",
            "uuid": "e2477.1",
            "source_info": {
                "paper_title": "Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LCB+EV",
            "name_full": "Lower Confidence Bound acquisition with Expected Violation constraint handling",
            "brief_description": "Acquisition/infill criterion used inside the Bayesian QD loop: LCB (µ - k·σ) drives exploration/exploitation and Expected Violation (EV) computed from GP predictive distributions constrains candidate feasibility under uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LCB acquisition combined with Expected Violation (EV) constraints",
            "system_description": "The auxiliary infill optimization minimizes LCB = µ(x) - k·σ(x) of the surrogate objective (µ and σ from the GP) subject to EV_gi(x) ≤ t_i for each inequality constraint g_i. EV_gi(x) is computed analytically from the GP predictive mean and variance and standard normal pdf/CDF, yielding a probabilistic estimate of expected constraint violation that captures both mean prediction and uncertainty.",
            "application_domain": "Active learning / Bayesian optimization for constrained expensive black‑box functions (applied here to QD infill selection).",
            "resource_allocation_strategy": "Directs resources toward points with low predicted objective minus an uncertainty term (favoring either low mean or high uncertainty depending on k) while enforcing probabilistic feasibility via EV thresholds; so expensive evaluations are allocated to points likely to yield either improved quality or high information about uncertain regions while respecting constraints.",
            "computational_cost_metric": "Acquisition optimization cost is measured in cheap surrogate evaluations; true evaluation cost measured in number of expensive function calls (primary budget).",
            "information_gain_metric": "GP predictive standard deviation (σ) enters LCB, so information gain is represented implicitly by σ; Expected Violation also uses σ to quantify risk/uncertainty about constraints.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Scalar parameter k (authors used k = 2) scales the uncertainty term; increasing k biases selection toward exploration (high σ), decreasing k toward exploitation (low µ). EV constraint thresholds t_i tighten/loosen feasible region under uncertainty.",
            "diversity_mechanism": "The LCB+EV acquisition is used inside the MAP‑Elites surrogate search which supplies diversity; LCB alone does not promote diversity across features.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of true evaluations allocated per iteration; acquisition is optimized on surrogates and only a limited number of acquired candidates are evaluated.",
            "budget_constraint_handling": "Restricts the number of true evaluations taken from the acquisition outputs; acquisition optimization is performed cheaply and then candidate selection enforces budget by sampling elites (Sobol') rather than evaluating all acquisition-optimal candidates.",
            "breakthrough_discovery_metric": "Points with very low LCB (suggesting either low mean or high uncertainty that could reveal much better true values) and feasible under EV are considered potential breakthroughs; measured downstream by improvement in QD‑score and new niche illumination.",
            "performance_metrics": "Authors used LCB (k=2) with EV threshold t_i = 1e-4 in experiments; effectiveness judged via aggregate QD‑score and number of illuminated niches.",
            "comparison_baseline": "Implicitly compared to population‑based search (MAP‑Elites on true objective) where no LCB+EV guidance is available.",
            "performance_vs_baseline": "Using LCB+EV within the surrogate QD framework contributed to orders‑of‑magnitude reductions in the number of required expensive evaluations versus direct MAP‑Elites on the true functions.",
            "efficiency_gain": "Part of the combined method that achieved ~two orders of magnitude fewer expensive evaluations for comparable final archives.",
            "tradeoff_analysis": "Discussed role of k (exploration factor) and EV threshold t_i in governing risk vs reward and feasibility; authors used moderate k and small t_i to balance exploration while keeping feasible solutions.",
            "optimal_allocation_findings": "Choose moderate k (k=2 used) and small EV thresholds to keep evaluations focused on promising, probably feasible candidates; combine with surrogate MAP‑Elites to ensure feature space coverage.",
            "uuid": "e2477.2",
            "source_info": {
                "paper_title": "Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "SAIL",
            "name_full": "Surrogate‑Assisted Illumination (SAIL)",
            "brief_description": "A previously published surrogate‑assisted Quality‑Diversity method that uses surrogate models (GPs) to accelerate MAP‑Elites style illumination of design/behavior spaces; designed for continuous variables and no constraints in the original formulations.",
            "citation_title": "Aerodynamic design exploration through surrogate-assisted illumination",
            "mention_or_use": "mention",
            "system_name": "Surrogate‑Assisted Illumination (SAIL)",
            "system_description": "SAIL replaces expensive objective evaluations by surrogate models (GPs) and uses an acquisition procedure to propose candidates that populate a MAP‑Elites archive while limiting expensive evaluations. It was developed for continuous domains and introduced active learning strategies to accelerate illumination of the feature space.",
            "application_domain": "Design exploration and optimization in aerodynamics and other continuous engineering design spaces.",
            "resource_allocation_strategy": "Uses surrogate models and acquisition to decide where to evaluate expensive simulations to most improve the archive and surrogate accuracy; selects candidates that improve illumination or reduce uncertainty.",
            "computational_cost_metric": "Number of expensive simulations (function evaluations) and surrogate fitting cost.",
            "information_gain_metric": "GP predictive uncertainty and acquisition functions for surrogate improvement and archive enhancement.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions on GPs trade off predicted quality and uncertainty; integrated with MAP‑Elites to balance discovering new niches and improving incumbent quality.",
            "diversity_mechanism": "MAP‑Elites discretized feature archive to promote diversity across features.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed budget of expensive function evaluations; designed to reduce evaluations relative to naive MAP‑Elites.",
            "budget_constraint_handling": "Surrogate‑based infill and selective true evaluations to maximize archive improvement per expensive evaluation.",
            "breakthrough_discovery_metric": "Improvement of archive illumination and quality per niche (similar measures to QD‑score/illuminated niches).",
            "performance_metrics": "Reported in original SAIL work (not reproduced here) as reduced expensive evaluations to reach comparable illumination versus direct MAP‑Elites.",
            "comparison_baseline": "Compared in literature to MAP‑Elites and other surrogate‑assisted illumination variants.",
            "performance_vs_baseline": "SAIL demonstrated substantial reductions in expensive evaluations for continuous unconstrained QD problems (paper cites SAIL as antecedent; current paper extends ideas to mixed variables and constraints).",
            "efficiency_gain": null,
            "tradeoff_analysis": null,
            "optimal_allocation_findings": null,
            "uuid": "e2477.3",
            "source_info": {
                "paper_title": "Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "SPHEN",
            "name_full": "Surrogate‑assisted PHEnotypic Niching (SPHEN)",
            "brief_description": "A surrogate‑assisted QD algorithm that integrates surrogate models with phenotypic niching ideas to accelerate exploration and preserve diversity in continuous feature spaces.",
            "citation_title": "Designing air flow with surrogate-assisted phenotypic niching",
            "mention_or_use": "mention",
            "system_name": "SPHEN (Surrogate‑assisted Phenotypic Niching)",
            "system_description": "SPHEN uses GP surrogates and niching strategies to guide search for diverse high‑quality solutions. It was a predecessor informing the proposed approach; the paper states the proposed algorithm is derived from SPHEN and SAIL but extended to mixed variables and constraints.",
            "application_domain": "Design exploration (aerodynamics example) and surrogate‑assisted QD for continuous design spaces.",
            "resource_allocation_strategy": "Surrogate‑guided selection of candidates prioritized to improve diversity and quality across phenotypic niches.",
            "computational_cost_metric": "Number of expensive function calls and surrogate training cost.",
            "information_gain_metric": "GP predictive variance incorporated in acquisition; expected improvement/uncertainty reduction concepts.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions combining predicted performance and uncertainty used inside a niching process.",
            "diversity_mechanism": "Phenotypic niching and archive binning to preserve diverse solutions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed budget of expensive evaluations in surrogate‑assisted implementations.",
            "budget_constraint_handling": "Selects a limited set of surrogate elites to evaluate exactly, prioritizing both novelty (niche discovery) and surrogate uncertainty reduction.",
            "breakthrough_discovery_metric": "Discovery of novel phenotypic niches with high predicted quality; archive coverage and niche quality.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": null,
            "optimal_allocation_findings": null,
            "uuid": "e2477.4",
            "source_info": {
                "paper_title": "Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "BOP-Elites",
            "name_full": "Bayesian Optimization of Elites (BOP‑Elites / Bop‑elites)",
            "brief_description": "A Bayesian optimization approach specifically adapted to Quality‑Diversity search that treats descriptor/feature discovery as part of the acquisition process; referenced as a recent Bayesian QD method (originally for continuous descriptors).",
            "citation_title": "Bop-elites, a bayesian optimisation algorithm for qualitydiversity search",
            "mention_or_use": "mention",
            "system_name": "BOP‑Elites (Bayesian Optimization of Elites)",
            "system_description": "BOP‑Elites integrates Bayesian optimization techniques with QD search to propose elite solutions and uses GP uncertainty information to guide evaluations. The current paper cites it as an existing Bayesian QD approach developed for continuous variables; the proposed method extends the Bayesian QD idea to mixed variables and constraints.",
            "application_domain": "Quality‑Diversity search and design exploration for continuous descriptor spaces.",
            "resource_allocation_strategy": "Uses GP acquisition functions to select evaluations that improve the elite set and the surrogate model, balancing exploration and exploitation within the QD framework.",
            "computational_cost_metric": "Number of expensive objective/descriptor evaluations; surrogate training cost.",
            "information_gain_metric": "GP predictive variance and Bayesian acquisition functions (e.g., EI, LCB) used to guide selection.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions (Bayesian) trading off mean prediction versus uncertainty; designed to populate elite archive efficiently.",
            "diversity_mechanism": "Integrated with descriptor/feature discretization and elite selection to encourage diverse elites.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed budget of expensive evaluations",
            "budget_constraint_handling": "Selective evaluation guided by acquisition functions and archive needs.",
            "breakthrough_discovery_metric": "Quality of elites across niches, archive coverage.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": null,
            "optimal_allocation_findings": null,
            "uuid": "e2477.5",
            "source_info": {
                "paper_title": "Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Aerodynamic design exploration through surrogate-assisted illumination",
            "rating": 2,
            "sanitized_title": "aerodynamic_design_exploration_through_surrogateassisted_illumination"
        },
        {
            "paper_title": "Designing air flow with surrogate-assisted phenotypic niching",
            "rating": 2,
            "sanitized_title": "designing_air_flow_with_surrogateassisted_phenotypic_niching"
        },
        {
            "paper_title": "Bop-elites, a bayesian optimisation algorithm for qualitydiversity search",
            "rating": 2,
            "sanitized_title": "bopelites_a_bayesian_optimisation_algorithm_for_qualitydiversity_search"
        },
        {
            "paper_title": "Efficient global optimization of constrained mixed variable problems",
            "rating": 2,
            "sanitized_title": "efficient_global_optimization_of_constrained_mixed_variable_problems"
        },
        {
            "paper_title": "A general square exponential kernel to handle mixed-categorical variables for gaussian process",
            "rating": 1,
            "sanitized_title": "a_general_square_exponential_kernel_to_handle_mixedcategorical_variables_for_gaussian_process"
        }
    ],
    "cost": 0.022032,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graphical Abstract Bayesian Quality-Diversity approaches for constrained optimiza- tion problems with mixed continuous, discrete and categorical vari- ables Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables
11 Sep 2023</p>
<p>Loïc Brevault 
ONERA / DTIS
Université Paris-Saclay
PalaiseauFrance</p>
<p>Mathieu Balesdent 
ONERA / DTIS
Université Paris-Saclay
PalaiseauFrance</p>
<p>Loïc Brevault 
ONERA / DTIS
Université Paris-Saclay
PalaiseauFrance</p>
<p>Mathieu Balesdent 
ONERA / DTIS
Université Paris-Saclay
PalaiseauFrance</p>
<p>Graphical Abstract Bayesian Quality-Diversity approaches for constrained optimiza- tion problems with mixed continuous, discrete and categorical vari- ables Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables
11 Sep 2023Preprint submitted to Engineering Applications of Artificial Intelligence October 11, 2023Quality-DiversityBayesian OptimizationMixed variablesContinuousDiscreteCategorical variables
Development of a Bayesian Quality-Diversity approach to deal with constrained problems involving mixed continuous, discrete and categorical variables using adapted Gaussian processes• Extension of MAP-Elites algorithms to deal with mixed variables and constraints• Application on a benchmark of analytical problems and an aerospace design problemAbstractComplex engineering design problems, such as those involved in aerospace, civil, or energy engineering, require the use of numerically costly simulation codes in order to predict the behavior and performance of the system to be designed. To perform the design of the systems, these codes are often embedded into an optimization process to provide the best design while satisfying the design constraints. Recently, new approaches, called Quality-Diversity, have been proposed in order to enhance the exploration of the design space and to provide a set of optimal diversified solutions with respect to some feature functions. These functions are interesting to assess tradeoffs. Furthermore, complex engineering design problems often involve mixed continuous, discrete, and categorical design variables allowing to take into account technological choices in the optimization problem. In this paper, a new Quality-Diversity methodology based on mixed continuous, discrete and categorical Bayesian optimization strategy is proposed. This approach allows to reduce the computational cost with respect to classical Quality -Diversity approaches while dealing with discrete choices and constraints. The performance of the proposed method is assessed on a benchmark of analytical problems as well as on an industrial design optimization problem dealing with aerospace systems.</p>
<p>Introduction</p>
<p>Complex engineering design problems, such as those involved in aerospace [1], civil [2], or energy [3] engineering fields, require the use of simulation codes in order to numerically predict the behavior and performance of the system to be designed. In the different design phases of the system, these codes can be used in optimization problems in order to minimize a given objective function (e.g., mass, cost, consumption) with respect to some design variables characterizing the system and subject to several specifications (e.g., reliability, environmental impact). In the early phases of the design process, one aims at exploring a large design space in order to assess the best architecture for the given mission. This induces the handling of classical continuous design variables but also discrete variables (e.g., number of engines) and categorical variables that traduce the different technology options (e.g., for a launch vehicle, type of propulsion -solid or liquid, for a wind turbine, type of material -composite or metallic). The handling of continuous, discrete and categorical variables presents some challenges in optimization algorithms dealing with computationally expensive engineering simulation codes. Indeed, gradient-based algorithms are not suitable because of the presence of discrete and categorical variables and population-based algorithms (e.g., genetic algorithms [4], particle swarm algorithms [5]) often require a large number of simulation code evaluations that are not affordable in practice. An interesting way to solve such problems is to use surrogate-assisted optimization algorithms [6]. This consists in substituting the computationally costly simulation codes by surrogate models (e.g., neural networks [7], Gaussian process [8], support vector machines [9]) and using them in the optimization process. One popular way to perform such optimization strategies is to use Gaussian Processes (GPs) of the objective function and constraints that are enriched all along the optimization process (via active learning) in order to converge to the optimum while exploring the design space [10]. This is often called Bayesian optimization [11].</p>
<p>In the early design phase of innovative systems, trade-offs have often to be assessed between several quantities of interest that characterize the performance of the system (e.g., costs, environmental impact, availability rate). In such a context, single-objective optimization approaches are not sufficient and two different types of methodologies can be used. The first deals with multi-objective algorithms in which the different quantities of interests are gathered into a vector of objective functions that need to be optimized together. The result is a Pareto front that allows the decision makers to assess trade-offs between different solutions [12,13]. Recently, another family of optimization strategies, called Quality-Diversity [14,15], has been proposed. These algorithms optimize the objective function while exploring some quantities of interest, called features. Such algorithms provide the decision makers with a set (an archive) of diversified solutions that aims at promoting diversity with respect to the features functions while optimizing the objective function. Compared to the multi-objective optimization, Quality-Diversity (QD) algorithms allow to improve the exploration of the search space and preserve the optimization with respect to the objective function. The main QD algorithms are based on population-based algorithms. The MAP-Elites algorithm [16] is one of the most popular methods in this family. When dealing with computationally expensive simulation codes, several Bayesian QD algorithms have been proposed, such as as Surrogate-Assisted ILlumination (SAIL) [17], Surrogate-assisted PHEnotypic Niching (SPHEN) [18] or Bayesian Optimization of Elites (BOP-Elites) [19,20]. However, these algorithms are dedicated to continuous optimization problems. In this paper, a Quality-Diversity Bayesian approach allowing to handle of both computationally intensive constraints and feature functions as well as continuous, discrete and categorical variables is proposed. This algorithm is derived from SPHEN and involves specific covariance function in the Gaussian Processes [21,22] in order to handle the mixed continuous and categorical variables. Furthermore, a new active learning strategy is proposed in order to refine the surrogate models while promoting quality and diversity. It is based on an adaptation of MAP-Elites approaches to handle discrete and categorical variables as well as the constraints. The proposed approach is compared with classical mixed continuous / discrete version of Map-Elites on three analytical test-cases and one aerospace engineering problem.</p>
<p>The paper is organized as follows. In Section 2, the QD optimization problem is described. In Section 3, the proposed Bayesian QD approach is presented, with two versions depending on the covariance function model used in the Gaussian Process. Eventually, in Section 4, the proposed approach is compared to classical mixed continuous / discrete versions of MAP-Elites on three analytical analytical test cases of increasing complexity. Finally, an engineering problem dealing with the aerodynamic design of an aircraft wing is carried out to assess the performance of the proposed approach on an industrial complexity test case.</p>
<p>From classical optimization to Quality-Diversity (QD) optimization</p>
<p>Classical continuous optimization problems are often formulated as:
min x f pxq(1)
s.t. g i pxq ď 0 for i " 1, . . . , n g (2) h j pxq " 0 for j " 1, . . . , n h (3) x lb ď x ď x ub (4) where x P rx lb , x ub s Ă R d is a vector of continuous variables (with x lb and x ub the vectors of lower bounds and upper bounds), f p¨q is a scalar objective function, g i p¨q is the i th inequality constraint function for i P t1, . . . , n g u and h j p¨q is the j th equality constraint function for j P t1, . . . , n h u. The number of inequality constraints is equal to n g and the number of equality constraints is equal to n h . Moreover, in various applications such as complex engineering design problems, in addition to continuous variables, it is necessary to consider the presence of discrete and categorical variables. Categorical variables are qualitative variables that can be unordered (also known as nominal variables, e.g., type of propulsion, type of material) or ordered (also known as ordinal variables, e.g., small, medium, large). The notion of distance is not properly defined between categories and although the ordinal variables present an order, there is no distance between the different categories. The discrete variables are quantitative variables taking specific values with a notion of order and metric to estimate a distance between the different possible variable values (e.g., number of engines on an aircraft).</p>
<p>The optimization problem with these different variables may be formu-lated as:
min x c ,x d ,x q f px c , x d , x q q (5) s.t. g i px c , x d , x q q ď 0 for i " 1, . . . , n g (6) h j px c , x d , x q q " 0 for j " 1, . . . , n h (7) x c lb ď x c ď x c ub (8) x d P X d (9) x q P X q(10)
with x c , x d , x q respectively the continuous variables, the discrete variables and the categorical variables. X d and X q correspond to the definition domains for the discrete and categorical variables. The sizes of the different continuous, discrete and categorical search spaces are noted n x c , n x d and n x q .</p>
<p>Different families of optimization algorithms have been proposed to solve optimization problems: the gradient-based algorithms [23], the grid-search algorithms [24,25], the branch-and-bound approaches [26], the evolutionary algorithms [27], the surrogate-based algorithms [6], etc. In case only continuous design variables are involved in the optimization problem, gradient-based algorithms (e.g., Broyden -Fletcher -Goldfarb -Shanno (BFGS) [28], Sequential Quadratic Programming (SQP) [29]) exploit the information of the gradient of the objective function and the constraints with respect to the design variables in order to converge to a local minimum (that can be a global minima for convex functions and search space). The gradient is used to determine a descent direction in order to improve the current knowledge about the minimum. In case the optimization problem presents several local minima, different strategies such as multi-start approaches may be used to identify the global minimum.</p>
<p>For optimization problems with continuous, discrete and categorical variables, as the gradient is not available, when possible, adaptations (such as relaxation approaches [30]) are required. Alternatively, it is possible to use grid-search algorithms [25], branch-and-bounds approaches [26] or population-based algorithms (e.g., genetic algorithm [4,31], particle swarm [5], covariance matrix adaptation -evolution strategy [32]). The populationbased algorithms (Figure 1) rely on a set of individuals that evolve in the search space to identify the optimal region and the global minimum. The evolutionary operators differ from an algorithm to another (relying on muta- tion, cross-over, random generations, etc.) and are often inspired by natural behaviors. Moreover, most of the existing population-based algorithms have been adapted to handle discrete and categorical variables through specific evolution mechanisms of the population [33,34]. In addition, specific approaches have been adapted to deal with constrained optimization problems [35]. In order to reach convergence, population-based algorithms require in general a large number of objective function and constraint functions evaluations. In case the objective function and / or the constraint functions are computationally intensive, alternative optimization strategies (e.g., Bayesian optimization [36]) based on surrogate models have been proposed in the literature. In these approaches, each computationally intensive function is replaced by a surrogate model (e.g., Gaussian Process -GP, support vector machine, neural network). Starting from an initial Design of Experiments (DoE), the surrogate models for the objective function and the constraint functions are constructed. Then, an auxiliary optimization problem is solved by optimizing an infill criterion on the surrogate models in order to identify the most promising candidate solution in the search space to find the global minimum. Once the candidate solution is identified, the exact objective and constraint functions are evaluated, the DoE and the surrogate models are updated. This process continues until optimization convergence. It allows to found a global minimum while limiting the number of evaluations of the computationally intensive functions. Gaussian processes have been extensively used for that purpose [11,36].</p>
<p>All the optimization algorithms mentioned above often allow to identify a single optimal solution. Even if some algorithms use a population of individuals that evolve in the search space and might explore different regions, the population evolution mechanisms lead to the convergence toward a single optimal solution. A new family of optimization algorithms, called Quality-Diversity (QD) algorithms [14,15] has been proposed in order to provide a diverse set of optimal solutions characterized by various trade-offs. These techniques offer a different diversity compared to multi-objective optimization algorithms. Indeed, in multi-objective optimization [12], in the presence of antagonistic objective functions, a Pareto set is provided, corresponding to the set of the non-dominated objective solutions in the sense of Pareto dominance [37], resulting in a trade-off between the different objective functions.</p>
<p>In practice, especially in the early design phases of engineering systems, the design process involves optimization algorithms in order to explore a large design space with various options and examine trade-offs. Therefore, there is an interest for a set of attractive solutions that can be further explored in more details in the next steps of the design process. QD approaches are based on such an idea in order to provide high-quality solutions with respect to an objective function and diversified with respect to some feature functions. For instance, in the early design phases, for the design of lifting surfaces for aerospace vehicles, it can be interesting to identify high-quality wing geometries (in terms for instance of lift-to-drag ratio) while generating diversified solutions according to features such as wing aspect ratio (quantifying how long and slender a wing is) or taper ratio (ratio between tip and root chord lengths). Then, the general idea is to identify a set of optimal solutions (with respect to lift-to-drag ratio) but with a diversity according to features (e.g., aspect ratio, taper ratio). The QD approaches provide a set of high-quality solutions in which the decision-makers can pick in order to further investigate depending on their preferences in terms of features. They generate also valuable information of the influence of the features on the objective function giving some aftermaths of the feature choices on the overall performance. Consequently, for decision-making, QD algorithms allow, with respect to classical optimization algorithms, to go in-depth in the analysis by providing additional information with respect to several quantities of interest modeled using feature functions.</p>
<p>A mixed continuous-discrete-categorical QD problem may be written as:
@f P F t , min x c ,x d ,x q f px c , x d , x q q(11)s.t. g i px c , x d , x q q ď 0 for i " 1, . . . , n g (12) h j px c , x d , x q q " 0 for j " 1, . . . , n h (13) f t px c , x d , x q q Pf (14) x c lb ď x c ď x c ub (15) x d P X d (16) x q P X q(17)
where F t is the feature space of dimension n,f is an element in the feature space and f t p¨q is a vector of feature functions of size n. The feature functions map the design variables (continuous, discrete and categorical variables) into the feature space (a set of dimension n where each coordinate corresponds to a feature of interest). Often in practice, the feature space is discretized into a multi-dimensional hyper-rectangular grid. For each feature index j " 1, . . . , n, let s j P N be the size of the discretization, representing the number of discretization nodes for the j th feature. Let
! F s j t j )
j"1,...,n be the collection of feature discretizations. The feature space F t " F s 1 t 1 b¨¨¨bF sn tn corresponds to the tensor product of each feature coordinate j. A bin of the multidimensional grid corresponds to a niche (a combination of intervals, one per feature coordinate). Therefore, in this context, the elementsf of the feature space are bins (niches) of the multi-dimensional grid. The goal of QD algorithms is to identify the most diverse collection, which is optimal in terms of objective function with respect to the design variables and diverse with respect to the feature functions. Each member of the collection is as best as possible regarding the objective function. Therefore, the QD algorithm returns a map (also called an archive or a collection) corresponding to a set of solutions that differs in terms of feature characteristics. Each niche in this map contains the solution that will be found by classical optimization algorithms associated to a specific combination of feature values defining the bin. Figure 2 illustrates the main characteristics of QD algorithms on a one dimensional continuous problem considering two feature functions. On the left of the figure, the mappings between the continuous design variable x c P R and the two feature functions are represented. A discretization of the two  feature functions is made and two particular categories are highlighted (in yellow for the feature 1 and in blue for the feature 2). For each feature function, the feature discretization is associated with some specific regions of the input design space highlighted in blue. The two-dimensional grid corresponding to the tensor product of the feature function discretizations is presented in the grid in the middle of the figure. Combining the two discretizations of the two features creates different bins. The combination of the highlighted categories for each feature leads to a particular bin (outlined in green) in the two-dimensional grid. Therefore, as presented in the formulation of the QDproblem (Eqs. (11)(12)(13)(14)(15)(16)(17)), the goal is to identify the minimum of the objective function (right) associated to each bin of the multi-dimensional grid. Due to the mapping with the features, for each bin, the minimum of the objective function is conditioned to a subset region of the original search space restricted to the combination of the feature associated to the bin. Therefore, the diversity is provided thanks to the combination of features and the quality is ensured thanks to the minimum value of the objective function for each bin. One approach to solve such a problem would be to repeat the solving of the optimization problem for each bin. However, such an approach is not realistic as the number of feature functions and the number of discretization of each feature increases, and the mapping between the design variables and the feature space is not known in advance.</p>
<p>Several QD algorithms have been proposed to solve such a type of problems. Firstly, algorithms derived from the population-based approaches have been developed such as: Novelty Search with Local Competition (NSLC) [38], Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) [16] and various derived versions (e.g., MAP-Elites + Novelty [15], MAP-Elites + Passive Genetic Diversity [15], Covariance Matrix Adaptation MAP-Elites (CMA-ME) [39]). More recently, QD algorithms in the family of surrogate-based approaches have been proposed. Some of the techniques are based on multilayer perceptron such as Deep surrogate assisted MAP-Elites [40]. However, these methods suffer from the difficulty to account for the uncertainty introduced by the use of the surrogate model in the QD process. Therefore, other approaches based on Gaussian processes have been proposed such as Surrogate-Assisted ILlumination (SAIL) [17], Surrogate-assisted PHEnotypic Niching (SPHEN) [18] and Bayesian Optimization of Elites (BOP-Elites) [19,20] to account for the uncertainty model estimation provided by the Gaussian processes.</p>
<p>The existing Bayesian QD algorithms [17,18,19,20] have been developed to only handle optimization problems with continuous variables and without constraints. In the following of this paper, a new Bayesian QD approach is proposed in order to solve QD problems with mixed continuous, discrete and categorical variables and to deal with constrained optimization problems. It is assumed that the objective function, the constraints and the features are computationally intensive and replaced by surrogate models that are enriched during the QD process. The main characteristics of the proposed algorithm are introduced in the following section.</p>
<p>Bayesian Quality-Diversity approach for constrained optimization problems with mixed continuous, discrete and categorical variables</p>
<p>In this section, the proposed Bayesian QD algorithm is described. It allows to solve constrained QD problems with mixed continuous, discrete and categorical variables. Firstly, Gaussian process and its adaptation in order to deal with mixed variables are presented in Section 3.1. Then, in Section 3.2, the proposed Bayesian QD algorithm is presented. A focus on constrained Bayesian optimization is made along with the adaptation of mechanisms to handle the constraints in QD problems.</p>
<p>Gaussian process with mixed variables</p>
<p>Gaussian Process (GP) is a surrogate model (sometimes referred to as Kriging [41,42]) that may be used to replace a generic computationally intensive function f : R d Ñ R with an approximation. A GP corresponds to a collection of random variables, any finite number of which has a multivariate joint Gaussian distribution. It may be seen as a generalization of the Gaussian probability distribution by encoding a distribution over a set of functions [43].</p>
<p>GP with continuous variables</p>
<p>In this section, only continuous variables are considered. A GP is fully determined by its mean function µp¨q and its covariance function Covp¨,¨q. If the function of interest f p¨q follows a GP, it can be expressed as f p¨q " GPpµp¨q, Covp¨,¨qq. To build a GP, it is necessary to solve a supervised learning problem. A GP is trained on a Design of Experiments constituted of an input training set of size M , X M " tx c 1 , . . . , x c M u and the corresponding set of computationally expensive function responses Y M " ty 1 " f px c 1 q , . . . , y M " f px c M qu. The function responses form a vector noted y M " ry 1 ,¨¨¨, y M s T . In a regression context, a GP prior is assumed on the mean function and on the covariance function. Regarding the mean function, as the tendency of the exact function is often unknown, a constant function µ is generally assumed as GP prior, resulting in ordinary Kriging. Depending on available a-priori knowledge, other types of mean function may be assumed such as quadratic or more general basis functions. The covariance function Covp¨,¨q is usually defined through the use of a parameterized covariance function called a kernel k Θ p¨,¨q. The kernel is used to model the covariance between two elements (e.g., x c and x c 1 ) as a symmetric positive definite function of the values of the coordinates of the elements k Θ px c , x c 1 q. This covariance function depends on some hyper-parameters Θ that need to be determined during the training process with the DoE. The most known kernels [43] are the Squared Exponential kernel (also known as Radial Basis Function), the Rational Quadratic kernel, the Matérn kernel, etc. The covariance function is a key element in GP. The covariance function allows to encode some assumptions on the behavior of the exact function (e.g., smoothness, periodicity, stationarity, separability). Multidimensional kernels may be obtained by combining single dimensional kernels through for instance a product operator following the formalism of Reproducing -Kernel -Hilbert -Space (RKHS) [44]. The prior mean and prior covariance are updated by relying on the information on the modeled function through the data set tX M , Y M u, which enables to provide a more insightful model of the considered function.</p>
<p>If a constant mean function is assumed, the GP is defined such that f p¨q " N`µ, k Θ p¨,¨q˘. Considering the DoE tX M , Y M u, the GP has a multivariate Gaussian distribution with a covariance matrix K M M built from the parameterized covariance function k Θ p¨,¨q on the input dataset X M (the dependence on Θ is dropped to simplify the notations). In the presence of experimental or numerical noisy data, the relationship between the latent function values f M " f pX M q and the observed responses Y M is given by: p py|f M q " N py|f M , σ 2 Iq with σ 2 an assumed Gaussian noise variance.</p>
<p>Then, from these input and output training sets and the prior on the GP, it is possible to train the surrogate model using the marginal likelihood. It is obtained by integrating out the latent function giving p py
M |X M , Θ, µ, σq " N py M |µ, K M M<code>σ 2 I M M q with I M M the identity matrix of size M . To sim- plify the notations, we defineK M M " K M M</code>σ 2 I.
In practice, the GP training requires to minimize the negative Log-Marginal Likelihood (LML) with respect to the hyperparameters Θ, µ and σ. The LML Lp¨q is given by:
L pΘ, µ, σ|X M , Y M q " log pp py M |X M , Θ, µ, σqq(18)9 log´|K M M |¯´y T MK´1 M M y M(19)
where all the kernel matrices implicitly depend on the hyperparameters Θ.</p>
<p>To solve the optimization problem, any optimizer may be used (e.g., gradientbased, population-based algorithms). Moreover, a closed form of the constant mean function may be sometimes determined [43].</p>
<p>Once the GP has been trained (the optimal hyperparameters have been determined, notedΘ,μ andσ), the prediction at a new unknown location x c˚P R d is done by using the conditional properties of a multivariate normal distribution: (20) wheref˚,ŝ˚2 are the mean prediction and the associated variance. These terms are defined by: (22) where k x c˚, x c˚" kpx c˚, x c˚q and k x c˚" rk px c i , x c˚q s i"1,...,M f px c˚q andŝpx c˚q correspond to the mean and variance of the posterior of the GP. A key element about GP is the possibility to have access to a prediction and a confidence level associated to the prediction that may be used for instance in an active learning strategy with a refinement process.
p´y˚|x c˚, X M , Y M ,Θ,μ,σ¯" N´y˚|f˚,ŝ˚2¯f˚"f px c˚q "μ<code>k T x˚</code>K M M<code>σ 2 I˘´1 py M´1μ q (21) s˚2 "ŝ 2 px c˚q " k x c˚, x c˚´k T x c˚</code>K M M`σ 2 I˘´1 k x c˚</p>
<p>GP with mixed continuous, discrete and categorical variables</p>
<p>In order to adapt GP to the presence of mixed continuous, discrete and categorical variables, it is necessary to define an adapted covariance function that can deal with such a set of variables.</p>
<p>The kernel to deal with mixed continuous, discrete and categorical variables may be defined as a product of one-dimensional kernels following the RKHS formalism. The resulting mixed-variable kernel can then be defined as [45]:
kptx c , x d , x q u, tx c 1 , x d 1 , x q 1 uq " n x c ź i"1 k x c piq´x c piq , x c 1 piq¯ˆn x d ź j"1 k x d pjq´x d pjq , x d 1 pjq¯n x q ź k"1 k x q pkq´x q pkq , x q 1 pkq( 23)
with k x c piq p¨,¨q the kernel associated to the continuous variable x c piq , and k x d pjq p¨,¨q and k x q pjq p¨,¨q the kernels associated to the discrete and categorical variables. For the kernels associated to the continuous variables, any kernel discussed in the previous section may be used.</p>
<p>Different kernels have been proposed in the literature to deal with discrete and categorical variables [22,45]. Due to the absence of classical distance measures between the values of these variables (especially categorical variables), adapted kernels have to be used. In the following, the kernels are discussed for single dimensional case and the extension to multidimensional problem is done using Eq. (23). Most often, no distinction is made between discrete and categorical variables and generic kernel for these variables will be noted k z . In the following, discrete or categorical variables x d or x q are noted using the generic notation z.</p>
<p>Considering a scalar categorical (or discrete) variable z with L possible levels tz 1 , . . . , z L u. To ease the notation, as these variables are characterized by a finite number of levels, the kernel function returns a finite number of covariance values that can be organized into a LˆL matrix K defined such that:
K m,d " k z pz " z m , z 1 " z d q(24)
with pm, dq P t1, . . . , Lu 2 . In order to be a valid covariance matrix, the covariance kernel K has to be symmetric and positive semi-definite. In the following, two different kernels [22,45] adapted for discrete and categorical variables are presented.</p>
<p>Compound Symmetry kernel. The Compound Symmetry (CS) kernel [22,46,47] is also known as the Gower kernel as the distance between the possible categorical levels for a variable z, is expressed based on the Gower distance [48]. CS is characterized by a single covariance value for any-non identical pair of inputs, relying on a continuous squared exponential kernel, CS kernel is defined as:
k z pz, z 1 q " σ 2 z exp p´θd gow pz, z 1 qq (25) with d gow pz, z 1 q " " 0 if z " z 1 1 if z ‰ z 1(26)
σ 2 z and θ ě 0 are respectively the variance and hyperparameter associated to the CS kernel. In case the discrete or categorical variables z and z 1 have the same level then, the distance is null, otherwise independently of the values of the levels for the two variables, the kernel returns the same value.</p>
<p>The CS kernel is very simple to account for the effect of a given variable since it relies on a single hyperparameter. However, the covariance between any pair of non identical levels of a given variable z is the same, regardless of the level values. This assumption may be too simplistic, especially when dealing with discrete and categorical variables which present a large number of levels. To avoid such a phenomenon, alternative kernels may be considered as discussed in the following paragraph.</p>
<p>Hypersphere decomposition kernel. An alternative kernel to deal with discrete or categorical variables is based on the hypersphere decomposition [22,47,49]. The general idea is to use a mapping between each level of the variable z and a point on the surface of a L-dimensional hypersphere. This decomposition allows to ensure the symmetric and positive semi-definite nature of the corresponding covariance matrix [49,50]. In practice, a mapping is defined based on a polyspherical change of coordinates F : r´π, πs L´1 Ñ S L´1 with S L´1 the unit pL´1q-sphere such that: S L´1 " tx P R L :∥ x ∥" 1u. This corresponds to a change of coordinates between the spherical coordinates and the Cartesian coordinates. The matrix K associated to the kernel of the hypersphere decomposition is given by K " σ 2 z L T L with L the lower triangular matrix associated to the Cholesky decomposition of K. L is defined such that:
L " » - - - - 1 0¨¨¨¨¨¨0 cospθ 2,1 q sinpθ 2,1 q 0¨¨¨0 . . . . . . . . . . . . . . . cospθ L,1 q sinpθ L,1 q cospθ L,2 q¨¨¨cospθ L,L´1 q ś L´2 k"1 sinpθ L,k q ś L´1 k"1 sinpθ L,k q fi ffi ffi ffi fl (27)
The hypersphere kernel is parametrized by the θ i,j hyperparameters involved in the matrix L, corresponding to LˆpL´1q{2 parameters. Compared to the CS kernel, the hypersphere decomposition kernel is able to give a different covariance value for each pair of levels characterizing the variable z. Moreover, this covariance function can take negative values, as each covariance value is computed as the product between a number of sine and cosine functions. Therefore, it is possible to model positive and negative correlations between the levels. It is important to notice that a part of the hyperparameters characterizing this kernel influences several covariance values simultaneously (due to the repeated presence of the same θ ij for different covariance values) that can lead to difficulties to estimate the optimal values of the hyperparameters.</p>
<p>In [22], the authors proposed a formulation that unifies the definition of the CS and hypersphere decomposition kernels in the context of Gaussian kernel. This formulation is an extension of the continuous Gaussian kernel to deal with mixed continuous-categorical variables. The Gaussian hypersphere decomposition kernel consists in the composition of a Gaussian kernel with an hypersphere decomposition kernel: K " σ 2 z expp´L T Lq. This composition allows to present the CS kernel and hypersphere decompostion kernel under the same formalism. However, the Gaussian hypersphere decomposi-tion kernel is only able to represent positive correlations between two levels. An equivalence between hypersphere and Gaussian hypersphere if the correlations are positive has been presented in [22] (with a restriction for the hypersphere angles to be in r0, π{2s).</p>
<p>There exist alternative kernels to deal with discrete and categorical variables such as the latent variable kernel [51] or the coregionalization [45,52]. For more information, please refer to [45,47].</p>
<p>Bayesian Quality-Diversity algorithm 3.2.1. Overview of the proposed algorithm</p>
<p>The proposed Bayesian QD algorithm is derived from SPHEN [18] and SAIL [17] approaches but allows to account for mixed continuous, discrete and categorical variables and to handle constrained optimization problems.</p>
<p>In the proposed approach, the objective function, the feature functions and the constraint functions are replaced by Gaussian processes with adapted covariance models (either based on CS kernel or hypersphere decomposition kernel depending on the regularity of the functions to be modeled). The proposed approach (Figure 3, Algorithm 1) starts with a Design of Experiments of size M in the joint space of continuous, discrete and categorical spaces. The continuous variables x c may be sampled according to classical DoE strategies such as Latin Hypercube Sampling (LHS) [53], Sobol' sampling [54], random sampling, etc. The discrete x d and categorical x q variables are sampled randomly based on the possible values for each variable. Joint sampling in the continuous, discrete and categorical spaces may be possible with adapted space filling [55]. Based on the input DoE X M " ttx c Using these GPs, an auxiliary optimization problem is solved in order to identify the most promising candidates to be added to the DoEs in order to improve the current archive and the GPs.</p>
<p>Infill problem optimization</p>
<p>The auxiliary optimization problem consists in minimizing an infill criterion under some constraints. The infill criterion has to account for the constraint functions in order to identify interesting solutions with respect to the objective function while being feasible with respect to the constraints. Different infill criteria can be used [36]. In this paper, the selected infill criterion is the Lower Confidence Bound. Consequently, the auxiliary optimization problem is formulated as follows:
@f P F t , min x c ,x d ,x qf px c , x d , x q q´kˆŝpx c , x d , x q q (28) s.t. EVĝ i px c , x d , x q q ď t i for i " 1, . . . , n g (29) f t px c , x d , x q q Pf (30) x c lb ď x c ď x c ub (31) x d P X d , x q P X q(32)
with EVĝ i p¨q "´ĝ i p¨qˆΦ´´ĝ i p¨q sg i p¨q¯`ŝ g i p¨qˆϕ´´ĝ i p¨q sg i p¨q¯, Φp¨q and ϕp¨q the Cumulative Distribution Function and the Probability Density Function of the standard normal distribution,ĝ i p¨q the prediction of the posterior GP associated to the constraint g i p¨q andŝ g i p¨q the standard deviation associated to the posterior GP. EVĝ i p¨q corresponds to the Expected Violation (EV) [56] associated to the constraint g i p¨q and accounts for both the GP prediction and its associated uncertainty to evaluate the risk of violation of the exact constraint functions. k is a positive scalar parameter specifying the exploitation / exploration balance in the Bayesian optimization process. t i is a threshold corresponding to the maximum accepted constraint violation considering uncertainty associated to the GP of the constraint g i p¨q. In this optimization problem, the objective function, the features and the constraints are replaced by their respective GPs, reducing the computational cost associated to the infill problem optimization. To solve this auxiliary optimization problem, an adaptation from original MAP-Elites algorithm [16] is used to handle continuous, discrete and categorical variables and also to manage the presence of constraints. This algorithm is described in the following section.  </p>
<p>Adaptation of MAP-Elites algorithm for infill problem optimization</p>
<p>The adapted MAP-Elites algorithm starts with the generation of an initial population of individuals randomly distributed in the search space. For each discrete and categorical variable, each possible level has an equal probability to be chosen using a multinomial distribution. Based on this initial population, the auxiliary objective function, the auxiliary feature functions and the auxiliary constraints are evaluated on the surrogate models (Eqs.(28)- (32)). An initial archive is defined based on the responses of the functions and their association to the corresponding bins. Then, among the individuals of the population, some individuals are randomly selected, called the elites. For each elite, each coordinate of the vector rx c , x d , x q s T has a chance of being mutated (depending on a mutation probability). The continuous coordinates are mutated according to a Gaussian distribution (characterized by a variance parameter) and the discrete and categorical variables are mutated according to a multinomial distribution on the existing levels for the Algorithm 1 Bayesian QD algorithm for constrained optimization problem with mixed continuous, discrete and categorical variables 1) Initialization: M (initial DoE size), n (number of features), n g (number of constraints) 2) Initial DoE:
X M Ð ttx c 1 , x d 1 , x q 1 u, . . . , tx c M , x d M , x q M uu 3) Evaluation of the objective function: Y M Ð ty 1 " f px c 1 , x d 1 , x q 1 q, . . . , y M " f px c M , x d M , x q M qu 4) Evaluation of the feature functions: F t j M Ð tf t j px c 1 , x d 1 , x q 1 q, . . . , f t j px c M , x d M ,
x q M qu for j P t1, . . . , nu 5) Evaluation of the constraint functions: Update of the MAP-Elite archive end while 9) Use of Sobol' sequence to select p elites X p Ð ttx c1 , x d1 , x q1 , . . . , tx cp , x dp , x qp uu 10) Evaluations of the exact functions based on X p 11) Update of the DoEs
G i M " tg i px c 1 , x d 1 , x q 1 q, . . . , g i px c M , x d M , x q M qu for i P t1,X M Ð X M Y X p , Y M Ð Y M Y Y p , F t j M Ð F t j M Y F t j p for j P t1, . . . , nu and G i M Ð G i M Y G ip for i P t1, . . . , n g u 12)
Update the exact archive end while coordinate. For the discrete variables, the probability mass functions of the multinomial distribution used for the mutation can depend on the distance between the different levels for the coordinate. The elites are then evaluated on the auxiliary functions based on GPs (objective, features and constraints). The new solutions are added to the current archive based on different rules. First, only feasible solutions with respect to the auxiliary optimization problem are added in the archive (constraint dominance approach [57]). Then, if a new solution discovers an unoccupied bin, the elite is added to the archive bin. If a new solution is better in terms of objective function than the current solution in the archive bin, the elite replaces the existing solution. This process is repeated for a certain number of generations leading to an increasing number of discovered bins and better solutions in each bin. Once the maximal number of generations has been reached in the MAP-Elites process on the surrogate models, among the elites of the final archive, a certain number of elites are selected to be evaluated on the exact objective, features and constraints. As the archive may have a large number of solutions (for instance due to a large number of bins), a Sobol' sequence [54] is used to select a limited number of optimal elites that uniformly covers the feature space. These selected elites are evaluated on the exact objective, features and constraints and added to the DoE X M . The different GPs of the objective function, features and constraints are updated and a new iteration of the Bayesian QD algorithm is performed. Moreover, the exact archive is updated based on the exact objective function, features and constraints evaluations added to the DoE.</p>
<p>A new iteration of the Bayesian QD algorithm is therefore carried out. Due to the highly computational cost context, the stopping criterion is based on the maximum number of objective, feature and constraints evaluations. This number is based on the affordable computational budget defined by the user. Moreover, the algorithm may stop if no new elites in the exact archive are generated for a given number of iterations defined by the user (stagnation of the algorithm).</p>
<p>Numerical experiments</p>
<p>In order to evaluate the efficiency of the proposed Bayesian QD algorithm, three different analytical problems of increasing complexity and one engineering test problem are proposed. Three different algorithms are compared: the MAP-Elites algorithm, modified in order to deal with mixed continuous, discrete and categorical variables and the presence of constraints ; the Bayesian QD algorithm with a Gower kernel and the Bayesian QD algorithm with the hypersphere decomposition kernel. The modifications derived in the MAP-Elites are the same as those involved in the proposed Bayesian-QD algorithm to optimize the auxiliary infill problem (Section 3.2.3).</p>
<p>For each test problem, in order to account for the stochastic nature of MAP-Elites algorithm and the initial DoE for the Bayesian QD algorithm, 10 repetitions are carried out from random initializations. To be able to compare the obtained results, for each repetition, the same initial samples are considered for all the algorithms either under the form of an initial DoE (Bayesian QD algorithms) or an initial population (MAP-Elites). LHS (with a given seed associated to each repetition) is used to generate these initial samples. The number of initial samples corresponds to ten times the dimension of the QD problem [36]. All the numerical settings for the different algorithms are provided in Appendix A.1.</p>
<p>Two main indicators are used to compare the algorithms efficiency, in a context of limited number of evaluations of the exact functions. The QDscore, corresponding to the sum of the objective function values for all the illuminated niches, is used to evaluate the overall performance of the algorithm. Moreover, a second indicator corresponding the number of illuminated niches is used to evaluate the ability of the algorithms to create diversity. The three analytical problems correspond to modified versions of classical optimization problems with the Rosenbrock function [58] (Section 4.1), the Trid function [59] (Section 4.2) and the Styblinski-Tang function [60] (Section 4.3). The engineering problem consists in the aerodynamic design of an aircraft wing and is described in Section 4.5. In these problems, as the discrete and categorical variables are handled in the same way in the different algorithms, only categorical variables are involved without loss of generality.</p>
<p>Rosenbrock problem</p>
<p>The Rosenbrock problem is derived from the classical Rosenbrock optimization problem [58] which has been modified in order to incorporate mixed continuous and categorical variables, a constraint function and two features. This QD problem is in dimension four: two continuous variables (d c " 2) and two categorical variables (d q " 2). The QD problem is defined as:
@f P F t , min x c ,x q f px c , x q q(33)s.t. g 1 px c , x q q ď 0 (34) f t px c , x q q Pf (35) x c lb ď x c ď x c ub (36) x q " rx q 1 , x q 2 s T P t0, 1, 2, 3, 4, 5uˆt0, 1u(37)
The objective function is defined as:
f px c , x q q "´d c´1 ÿ i"1 a q px q qˆ<code>x c i</code>1´p x c i q 2˘2<code>b q px q qˆ</code>e q px q q´x c i<code>1˘2 f q px q q(38)
with x c " rx c 1 , x c 2 s T P r´5, 5s 2 the vector of the continuous design variables. a q , b q , e q and f q are variables whose values depend on the value taken by the categorical variables x q " rx q 1 , x q 2 s T . The two categorical variables can respectively take six and two levels such that: x q 1 P t0, 1, 2, 3, 4, 5u and x q 2 P t0, 1u. The two features are defined such that f t p¨,¨q " rf t 1 p¨,¨q, f t 2 p¨,¨qs T with:
f t 1 px c , x q q " j q px q q˚px c 1´k q px q qq rqpx q q</code>s q px q q (39) f t 2 px c , x q q " v q px q q˚px c 2´t q px q qq 2`u q px q q(40)
The correspondence between the values of the categorical variables and the values of a q , b q , e q , f q , j q , k q , r q , s q , t q , u q and v q is given in Appendix A.2.</p>
<p>The inequality constraint corresponds to :
g 1 px c , x q q "<code>px c 1´0 .5q 2</code>xc 2´5 .6˘{10. ď 0
In order to define the archive, the feature functions are discretized in a two-dimensional grid with for each feature axis:f t 1 " r´50,´40,´30, 20,´10, 0, 10, 20, 30, 40, 50s andf t 2 " r´50,´40,´30,´20,´10, 0, 10, 20, 30, 40, 50, 60, 70, 80s. Figure 4 illustrates the QD-score convergence for the MAP-Elites algorithm and the proposed Bayesian QD with the Gower and hypersphere kernels. For the ten repetitions, the curves correspond to the median whereas  the upper and lower limits of the shade correspond to the 75 th and 25 th quantiles. In order to compare with Bayesian QD, two sizes of the population are tested with MAP-Elites (population of sizes 10 and 40). A computational budget of 160 evaluations of the exact functions (objective, features and constraint) is allowed. It corresponds to 40 initial samples for the inital DoE plus 120 new added samples during BO. As it can be seen, the convergence of the Bayesian QD algorithms is better in terms of QD-score (the lower, the better) compared to MAP-Elites algorithms. Indeed, the MAP-Elites algorithm seems to have a linear convergence with the number of evaluations of the exact functions with the slope influenced by the number of individuals in the population (MAP-Elites with 10 individuals seems to converge better than MAP-Elites with 40 individuals).</p>
<p>Bayesian QD algorithms have a faster decrease in terms of convergence. Moreover, comparing both Bayesian QD algorithms, in this test case, Bayesian QD with Gower kernel seems to be more efficient than with Bayesian QD the hypersphere kernel. The ability of hypersphere kernel to give a different co-variance value for each pair of levels characterizing the categorical variables should give an advantage to the associated GPs to better model the exact functions. However, it is counterbalanced by the larger number of hyperparameters to be determined in the training of the GPs. Indeed, with the Gower kernel, 5 hyperparameters have to be optimized (two lengthscales for the continuous variables, two hyperparameters for the categorical variables and one amplitude parameter) whereas 19 hyperparameters have to be optimized for the hypersphere kernel (two lengthscales for the continuous variables, sixteen hyperparameters for the categorical variables and one amplitude parameter). Therefore, the optimization problem involved with GPs with the hypersphere kernel is more difficult to solve.</p>
<p>Eventually, in Figure 4, the repetitions with different initial populations for the MAP-Elites and different initial DoEs for the Bayesian QD algorithms illustrate the robustness of the algorithms (the dispersion represented by the shade area around the median is limited).  Regarding the number of discovered niches for the Rosenbrock problem ( Figure 5), the Bayesian QD algorithms provide better results than MAP-Elites algorithm. Indeed, QD algorithms discover almost twice as much niches compared to MAP-Elites and on this test problem. Bayesian QD with the Gower kernel discovers the largest number of niches for all the repetitions. of the exact functions is also represented as the reference map. It can be seen that with only 160 evaluations, the final QD archive provided by Bayesian QD algorithms are close to the archive obtained by MAP-Elites with 30 000 evaluations, this illustrates the efficiency of the proposed algorithm. However, the map obtained with the QD MAP-Elites with 160 evaluations is far from the one obtained with Bayesian QD algorithms both in terms of niche discovered and quality of the best individual in each discovered niche. The extension of the Bayesian QD algorithms to deal with mixed continuous, discrete and categorical variables offers the possibility to converge into different categorical optimal solutions for different niches. Indeed, for the Rosenbrock problem, in Figures 7 and 8, the optimal solutions in terms of categorical variables x q " rx q 1 , x q 2 s T are displayed for each niche for respectively the Bayesian QD Gower algorithm with 160 evaluations and for the MAP-Elites algorithm with 30 000 evaluations. It can be seen that de- pending on the niches, optimal solutions may belong to different categories. Moreover, the Bayesian QD algorithm provides optimal solutions in terms of categorical variable values (with only 160 exact function evaluations) that are similar to the ones found by MAP-Elites algorithm (with 30 000 evaluations). The ability to deal with categorical variables is interesting as this allows to converge to different categorical solutions into different niches and therefore to offer diversity with respect to the categorical variables. This is of particular interest in the field of engineering design with categorical variables representing for instance technological choices or architectural choices. This aspect is further explored in the aerospace design problem in Section 4.5.</p>
<p>Trid problem</p>
<p>The Trid problem is derived from the classical Trid optimization problem [59] (also called Neumaier number 3 function) which has been modified in order to involve mixed continuous and categorical variables, a constraint and two features. This QD problem is of dimension six: four continuous variables (d c " 4q and two categorical variables. The QD problem is defined by:
@f P F t , min x c ,x q f px c , x q q(41)s.t. g 1 px c , x q q ď 0 (42) f t px c , x q q Pf (43) x c lb ď x c ď x c ub (44) x q " rx q 1 , x q 2 s T P t0, 1, 2uˆt0, 1u(45)
The objective function is defined such that:
f px c , x q q " dc ÿ i"1 a q px q qˆpx c i´b q px q qq 2´d c ÿ i"2 c q px q qˆx c i x c i´1(46)
with x c " rx c 1 , x c 2 , x c 2 , x c 4 s T P r0, 1s 4 the vector of the continuous design variables. a q , b q , c q are variables whose values depend on the value taken by the categorical variables x q " rx q 1 , x q 2 s T . The two categorical variables can respectively take three and two levels such that: x q 1 P t0, 1, 2u and x q 2 P t0, 1u. The two feature functions are defined such that f t p¨,¨q " rf t 1 p¨,¨q, f t 2 p¨,¨qs T with:
f t 1 px c , x q q " e q px q q˚x c 3<code>p f q px q qˆx c 1´j q px q qq 2</code>k q px q qˆx c 2 (47) f t 2 px c , x q q " r q px q qˆx c 2´s q px q q`pt q px q qˆx c 4ˆx c 3´u q px q qq 2 (48)
The correspondence between the values of the categorical variables and the values of a q , b q , c q , e q , f q , j q , k q , r q , s q , t q and u q is given in Appendix A.3.</p>
<p>The inequality constraint is defined as:
g 1 px c , x q q " px c 1´0 .4q 2`1 .5ˆx c 3´1 .3 ď 0
In order to define the archive, the feature functions are discretized in a two-dimensional grid with for each feature axis:f t 1 " r´1.5,´0.5, 0.5, 1.5, 2.5, 3.5, 4.5s andf t 2 " r´2.5,´1.5,´0.5, 0.5, 1.5, 2.5s Both Figures 9 and 10 representing the convergence for the Trid problem respectively of the QD-score and the number of discover niches illustrate the better efficiency of the Bayesian QD algorithms compared to MAP-Elites. Similar trends as for the Rosenbrock problem may be described for this problem in dimension 6. The MAP-Elites algorithms seem to converge with a linear trend (determined by the population size) whereas the Bayesian QD algorithms converge faster with respect to the number of exact function evaluations. Moreover, the Bayesian QD algorithms discover a larger number of niches in the feature space.</p>
<p>Eventually, in Figure 11, the final archives obtained with 160 evaluations of the exact functions for the Bayesian QD-algorithms are very similar to the one obtained with MAP-Elites algorithm after 30 000 evaluations. However, MAP-Elites with only 160 evaluations of the exact functions is still far from the converged archive illustrating the interest of QD algorithms based on GPs with adapted covariance models to handle mixed continuous and categorical variables. </p>
<p>Styblinski-Tang problem</p>
<p>The Styblinski-Tang problem is derived from the classical Styblinski-Tang optimization problem [60] which has been modified in order to incorporate mixed continuous and categorical variables, two constraints and two features. This QD problem is in dimension nine: six continuous variables (d c " 6q and three categorical variables. The QD problem is defined as: The objective function is defined such that:
@f P F t , min x c ,x q f px c , x q q(49)s.t. g 1 px c , x q q ď 0 (50) g 2 px c , x q q ď 0 (51) f t px c , x q q Pf (52) x c lb ď x c ď x c ub (53) x q " rx q 1 , x q 2 , x q 3 s P t0, 1u 3(54)f px c , x q q " dc ÿ i"1<code>a q px q qˆpx c i q 4´b q px q qˆpx c i q 2</code>c q px q qˆx c i˘( 55) with x c " rx c 1 , x c 2 , x c 2 , x c 4 , x c 5 ,
x c 6 s T P r0, 1s 6 the vector of the continuous variables. a q , b q , c q are variables whose values depend on the value taken by the categorical variables x q " rx q 1 , x q 2 , , x q 3 s T . The three categorical variables can respectively take two levels such that: x q 1 P t0, 1u, x q 2 P t0, 1u and x q 3 P t0, 1u. The two features are defined such that f t p¨,¨q " rf t 1 p¨,¨q, f t 2 p¨,¨qs T with:
f t 1 px c , x q q " px c 3´e q px q qq 2<code>p x c 5´f q px q qq 2 (56) f t 2 px c , x q q " x c 2</code>j q px q q`px c 4´k q px q qq 2(57)
The correspondence between the values of the categorical variables and the values of a q , b q , c q , e q , f q , j q , and k q are given in Appendix A.4.</p>
<p>The two inequality constraints are defined as:
g 1 px c , x q q " x c 1<code>x c 2´1 ď 0 g 2 px c , x q q " x c 4</code>x c
6´2 ď 0 In order to define the archive, the features are discretized in a twodimensional grid with for each feature axis:f t 1 " r0, 2, 4, 6, 8, 10, 12s and f t 2 " r´5,´3,´1, 1, 3, 5s. This QD problem is of higher complexity in terms of number of continuous variables, number of categorical variables and number of constraints. The convergence curves for the QD-score ( Figure 12) and the number of discovered niches (Figure 13) provide the same tendencies as for the previous two previous analytical problems. The Bayesian QD algorithms converge faster toward a better solution compared to MAP-Elites algorithms that display For this test problem, a study of full convergence of MAP-Elites with 10 individuals is illustrated in Figure 14. It can be seen the difference with respect to the orders of magnitude in terms of the number of exact function evaluations to reach convergence with MAP-Elites (" 30000 evaluations) compared to Bayesian QD algorithms (" 220 evaluations). Moreover, through the repetitions, it can be seen that the Bayesian QD algorithms are more robust to the initialization (with respect to the initial DoE) compared to the MAP-Elites (with respect to the initial population).</p>
<p>The final archive obtained by both Bayesian QD algorithms with 220 evaluations of the exact function are identical to the final archive obtained with MAP-Elites algorithm with 30 000 evaluations to the exact function. </p>
<p>Summary of numerical experiments on analytical test cases</p>
<p>The main lessons learnt on the analytical problems are first that, in a context of computationally intensive problems, Bayesian QD algorithms converged faster than MAP-Elites approaches both in terms of QD-score and in terms of number of discovered niches. Moreover, Bayesian QD algorithms present more robust performance with respect to the initial Design of Experiments (illustrated with the problem repetitions). In addition, the choice of the kernel type of the categorical variables (between Gower and Hypersphere) is not critical with respect to the speed of convergence or the robustness to the initial DoE. However, when the number of categorical levels is large (as in the Rosenbrock problem), the hypersphere kernel might present some difficulties compared to the Gower kernel due to the number of hyperparameters to be optimized in the training of Gaussian processes. In addition, the extension of Bayesian QD algorithms to handle mixed continuous, discrete and categorical variables is interesting as it offers the possibility to have different optimal discrete/categorical values in each niche resulting in different technological or architectural choices available for the decision-makers. Eventually, independently of the problem dimension, for MAP-Elites algorithm, a smaller population size with a higher number of generations seems to be more efficient than a larger population size. Based on these conclusions, an aerospace engineering design problem is solved in the next section.</p>
<p>Three dimensional wing design problem</p>
<p>The engineering design problem consists in optimizing the aerodynamic three dimensional shape of a wing. In the design of an aircraft, the optimization of the lifting surfaces is an essential aspect as an aerodynamically efficient wing may allow to reduce the aircraft fuel consumption. In the early design phases, the designer might be interested in a diversity of efficient wing shapes in terms of aerodynamics but that could offer different behaviors in terms of aircraft aerodynamic center and flight qualities for the next design steps. Indeed, in the preliminary design phases, as the main characteristics of the aircraft are not frozen, the designer might be interested in determining a collection of efficient wings depending on the some trade-offs in terms of disciplinary requirements (for instance between structural, flying qualities or aerodynamics). Classical features such as the aspect ratio (the ratio between the wing span to its mean chord) or the taper ratio (the ratio between the root to the tip chord lengths of a wing) help the designer to learn about the overall aerodynamic characteristics of a wing. In such a context, Quality-Diversity algorithms may provide the designer with different high performance wing configurations in early design stages based on these two geometric features.</p>
<p>In the following, the proposed Bayesian QD algorithm is applied to a wing design problem with two features: the aspect ratio and the taper ratio. The airfoil is constituted of a single section parameterized by a root chord, a tip chord, a span, a dihedral angle and a sweep angle ( Figure 16). These parameters correspond to the continuous variables of the QD problem. Two categorical variables are also involved: the presence of winglets (with two possible choices: on and off, see Figure 16) and the airfoil type (with three possible airfoil profiles, NACA-0010, NACA-1210, NACA-63010, see Figure  17). The considered flight conditions are a Mach number of 0.5 and an angleof-attack of 5 degrees.  To estimate the aerodynamic performances of the wing, a Vortex Lattice Method (VLM) is used with OpenVSP and VSPAero [61]. This consists in a simplified computational fluid dynamics model used in preliminary design phases to calculate aerodynamic forces and moments acting on the wing. The method represents lifting surfaces as an infinitely thin sheet of discrete vortices. By solving the governing equations using the Biot-Savart Law and Kutta-Joukovsky theorem [62], VLM solvers can determine lift and pressure distributions, induced drag, and pitching moments endured by the wing. For the wing design problem, VLM is used to estimate the drag coefficient and the lift coefficient of a parametric wing.</p>
<p>The QD problem is formalized as follows:
@f P F t , min x c ,x q C D px c , x q q(58)s.t.´C L px c , x q q`C L T ď 0 (59) rf AR px c , x q q, f T R px c , x q qs T Pf (60) x c lb ď x c ď x c ub (61) x q P X q (62)(63)
where x c " rRc, T c, Sp, Di, Sws T with Rc the root chord, T c the tip chord, Sp the span, Di the dihedral angle and Sw the sweep angle. Table 1 gives the domain of definition of the continuous design variables. Moreover, Figure  16 presents the parameterization of the wing with the different continuous design variables. C D px c , x q q is the drag coefficient of the wing, C L px c , x q q is the lift coefficient and C L T " 0.2 is the target lift coefficient of the wing.  [20,30] Moreover, two categorical variables are considered x q " rW g, Ais T with W g the presence or not of winglet and Ai the choice of the airfoil profile tNACA-0010, NACA-1210, NACA-63010u. An example of VLM results is illustrated in Figure 18 for a wing without winglet and the airfoil NACA-63010. The results present the pressure distribution on the extrados of the wing and the associated mesh. f AR p¨,¨q and f T R p¨,¨q are the two considered feature functions corresponding respectively to the aspect ratio and the taper ratio.</p>
<p>For this design problem, the use of surrogate models is relevant as one evaluation of the exact function on a cluster of 12 cores of Skylake Intel® Xeon® Gold 6152 CPU represents about 5 min. Therefore, for this QD optimization problem in dimension 7, the number of exact evaluations with algorithms such as MAP-Elites to reach convergence (in the order of several thousand evaluations) is hardly affordable.</p>
<p>Considering the computational cost for this problem and the results obtained with the three analytical QD problems, only the MAP-Elites with a population of 10 individuals is considered in this test case. Moreover, for both Bayesian QD algorithms and for the MAP-Elites algorithm, five repetitions with different initial DoEs and populations are considered. Figures 19 and 20 present the convergence curves for the QD-score and the number of discovered niches in the wing design problem. As for the analytical test problems, for the wing design problem, the Bayesian QD algorithms converge faster to a better solution in terms of QD-score and illuminate a larger number of niches compared to the MAP-Elites algorithm. Moreover, for this engineering design problem, the choice of the covariance model to deal with the discrete and categorical variables has a limited influence, both the Gower and the hypersphere kernels provide similar results. The final archive obtained (with one of the repetitions) for the wing design problem is presented in Figure 21, for the Bayesian QD with the Gower and the hypersphere kernels and with MAP-Elites. The archives for both Bayesian QD algorithms are similar with a small advantage for the Gower kernel with lower objective values in some niches. However, as for the analytical test problems, the archive obtained by MAP-Elites does not illuminate as many niches as the Bayesian QD algorithms and the quality in each discovered niche is less optimal. Figure 22 illustrates the comparison of wing geometries extracted from the final archive obtained by one repetition with Bayesian QD with Gower kernel. Moreover, it presents the found categories x q " rW g , A i s T in each niche illustrating the convergence to different optimal solutions in terms of architecture choices (presence or absence of winglets and airfoil type). Due to the features combination (aspect ratio and taper ratio) different categorical choices are identified in the niches. The handling of mixed continuous, discrete and categorical variables offer a diversity of architecture and technol- ogy choices to the designers and the decision-makers. Moreover, the feature functions offer a diversity in terms of wing geometry. It is possible to identify thanks to the final archive the consequences of an increase of aspect ratio in terms of drag coefficient for the wing. The final archive is a valuable asset for the design to make trade-offs in early design phases and to balance the consequences of these trade-offs.  Figure 22: Comparison of wing geometries extracted from the final archive obtained by one repetition with Bayesian QD with Gower kernel and with the found categories x q " rW g , A i s P t0, 1uˆt0, 1, 2u in each niche. W g " 0 means absence of winglet and W g " 1 the presence of winglets. A i corresponds to the choice of airfoil with 0: NACA-0010, 1: NACA-63010 and 2: NACA-1210.</p>
<p>Conclusions</p>
<p>In this paper, a Bayesian Quality-Diversity (QD) optimization approach is proposed to solve constrained problem involving mixed continuous, discrete and categorical variables. The proposed algorithm extends the existing QD algorithms in order to offer the possibility to handle architectural and technological choices in engineering design problems while accounting for specification constraints. Quality-Diversity approaches allow to identify a diversity of solutions with respect to some features with a high potential with respect to the objective. The proposed Bayesian QD algorithm relies on the use of Gaussian processes (GPs) to replace the exact objective, features and constraints functions. Moreover, in order to account for the presence of mixed continuous, discrete and categorical variables, two covariance models based on the Gower distance and the hypersphere decomposition are used for the GPs. The proposed Bayesian QD algorithm has been tested on a series of analytical QD problems of increasing complexity and on an engineering problem corresponding to the design of a an aircraft wing. These experiments allowed to assess the efficiency of the Bayesian QD algorithm to determine the optimal archive compared to MAP-Elites algorithm. -Mutation : probability of 0.4 and mutation according to a Normal distribution centered on 0. and with standard deviation of 0.3 for normalized continuous variables in r0, 1s dc .</p>
<p>• Gaussian processes:</p>
<p>-Initial LHS size for Gaussian process: 10ˆpd c`dq q using pyDOE2 library [63];</p>
<p>-Gaussian process library: SMT [64];</p>
<p>-Gaussian process training algorithm: COBYLA with 20 multistarts ;</p>
<p>-Kernel type for continuous variables: squared exponential;</p>
<p>-Nugget for Gaussian process: 10´6.</p>
<p>• Bayesian Optimization:</p>
<p>-Exploration factor for infill criterion: k " 2;</p>
<p>-Expected violation threshold: t i " 0.0001 for i " 1, . . . , n g with n g the number of constraints.</p>
<p>• Benchmark:</p>
<p>-Number of repetitions of each analytical problem: 10, and 5 repetitions of the engineering wing design problem;</p>
<p>-Run on a cluster of 12 cores of Skylake Intel® Xeon® Gold 6152 CPU.</p>
<p>Appendix A.2. Rosenbrock problem</p>
<p>The correspondence between the values of the categorical variables and the values of a q , b q , e q , f q , j q , k q , r q , s q , t q , u q and v q are given in the following matrix: 
x q 1 x q 2 a q b q e q f q j q k q r q s q t q u q v q 0</p>
<p>Appendix A.3. Trid problem</p>
<p>The correspondence between the values of the categorical variables and the values of a q , b q , c q , e q , f q , j q , k q , r q , s q , t q and u q are given in the following matrix:</p>
<p>x q 1 x q 2 a q b q c q e q f q j q k q r q s q t q u q 0 0 1. The correspondence between the values of the categorical variables and the values of a q , b q , c q , e q , f q , j q , and k q are given in the following matrix: </p>
<p>Figure 1 :
1Gradient-based and population-based optimization algorithms</p>
<p>Figure 2 :
2Quality-Diversity principle for a one continuous dimensional illustration (x c P R) with two feature functions. The green niche (middle of the figure) is determined by the association of the two feature functions discretization (the yellow region of the feature 1 and the blue region of the feature 2, left of thefigure). This niche defines a region of the design space (that can be a union of disjoint regions) in which the minimum value of the objective function has to be found (right of the figure).</p>
<p>Figure 3 :
3Bayesian QD algorithm to deal with mixed continuous, discrete and categorical variables and constraints</p>
<p>. . . , n g u 6) Creation of an exact archive while (stopping criterion not reached) do 7) Build GPs for the objective function, the feature functions and the constraint functions 8) Run MAP-Elites to solve auxiliary optimization problem Eqs.(28-32) 8a) Creation of a MAP-Elite archive while (stopping criterion not reached) do 8b) Generation of the MAP-Elites population 8c) Evaluation of the auxiliary functions 8d) Random selection of elites and mutation operation 8e) Evaluation of the children on the auxiliary functions 8f)</p>
<p>Figure 4 :
4Convergence curves (QD score, the lower, the better) for the Rosenbrock problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the ten repetitions, the curves correspond to the median whereas the upper and lower limits of the shade areas correspond to the 75 th and 25 th quantiles.</p>
<p>Figure 5 :
5Number of discovered niches (the higher, the better) for the Rosenbrock problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the ten repetitions, the curves correspond to the median whereas the upper and lower limits of the shade corresponds to the 75 th and 25 th quantiles.</p>
<p>Figure 6 :Figure 7 :
67Final archive for the Rosenbrock problem obtained by Bayesian QD with the Gower kernel with 160 evaluations (top left), by Bayesian QD with the hypersphere kernel with 160 evaluations (top right), by QD MAP-Elites with 160 evaluations (bottom left) and with QD MAP-Elites with 30 000 evaluations (bottom right)InFigure 6, the final QD archive obtained (for one representative repetition) with the 160 evaluations of the exact functions (corresponding to the initial DoEs in addition with the chosen candidates during the enrichment process) are represented for the Bayesian QD algorithms (QD-Gower and QD-Hypersphere) and for the MAP-Elites. In order to compare, the final archive obtained with a MAP-Elites algorithm involving 30 000 evaluations Final archive for the Rosenbrock problem obtained by Bayesian QD with Gower kernel with 160 evaluations. The numbers correspond to value the best values of categorical variables x q " rx q 1 , x q 2 s T in each niche</p>
<p>Figure 8 :
8Final archive for the Rosenbrock problem obtained by MAP-Elites with 30 000 evaluations. The numbers correspond to value the best values of categorical variables x q " rx q 1 , x q 2 s T in each niche</p>
<p>Figure 9 :
9Convergence curves (QD-score, the lower, the better) for the Trid problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the ten repetitions, the curves correspond to the median whereas the upper and lower limits of the shade area corresponds to the 75 th and 25 th quantiles.</p>
<p>Figure 10 :
10Number of discovered niches (the higher, the better) for the Trid problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the ten repetitions, the curves correspond to the median whereas the upper and lower limits of the shade corresponds to the 75 th and 25 th quantiles.</p>
<p>Figure 11 :
11Final archive for the Trid problem obtained by Bayesian QD with the Gower kernel with 160 evaluations (top left), by Bayesian QD with the hypersphere kernel with 160 evaluations (top right), by QD MAP-Elites with 160 evaluations (bottom left) and with QD MAP-Elites with 30 000 evaluations (bottom right)</p>
<p>Figure 12 :
12Convergence curves (QD score, the lower, the better) for the Styblinski-Tang problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the ten repetitions, the curves correspond to the median whereas the upper and lower limits of the shade area corresponds to the 75 th and 25 th quantiles.</p>
<p>Figure 13 :
13Number of discovered niches (the higher, the better) for the Styblinski-Tang problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the ten repetitions, the curves correspond to the median whereas the upper and lower limits of the shade corresponds to the 75 th and 25 th quantiles. a linear convergence rate in terms of number of exact function evaluations. Both Bayesian QD algorithms converge for all the repetitions to the same solution illustrating the robustness to the initial DoE. Moreover, the number of illuminated niches corresponds (in median) to 17 for MAP-Elites with 10 individuals whereas it increases up to 24 for the Bayesian QD algorithms.</p>
<p>Figure 14 :
14Full convergence curves (with log-scale of the abscissa) for both Bayesian QD algorithms and MAP-Elites algorithm (with 10 individuals) for the QD-score (left) and the number of discovered niches (right)</p>
<p>Figure 15 :
15Final archive for the Styblinski-Tang problem obtained by Bayesian QD with Gower kernel with 220 evaluations (top left), by Bayesian QD with hypersphere kernel with 220 evaluations (top right), by QD MAP-Elites with 220 evaluations (bottom left) and with QD MAP-Elites with 30 000 evaluations (bottom right)</p>
<p>Figure 16 :
16Wing parameterization and the categorical choice corresponding to the presence or absence of winglets</p>
<p>Figure 17 : 63010 Figure 18 :
176301018Different choices of airfoil profiles: NACA-0010, NACA-1210 and NACA-Pressure distribution obtained by VLM calculation for one wing geometry, without winglet and with the airfoil NACA-63010</p>
<p>Figure 19 :
19Convergence curves (QD score, the lower, the better) for the Wing design problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the five repetitions, the curves correspond to the median whereas the upper and lower limits of the shade area corresponds to the 75 th and 25 th quantiles.</p>
<p>Figure 20 :
20Number of discovered niches (the higher, the better) for the Wing design problem with MAP-Elites and Bayesian QD algorithm with Gower and hypersphere kernels. For the five repetitions, the curves correspond to the median whereas the upper and lower limits of the shade corresponds to the 75 th and 25 th quantiles.</p>
<p>Figure 21 :
21Final archive for the Wing design problem obtained by Bayesian QD with Gower kernel (top left), by Bayesian QD with hypersphere kernel with (top right), by MAP-Elites (bottom left)</p>
<p>-
MAP-Elites population size: 10 individuals or 10ˆpd c`dq q individuals with d c the number of continuous variables and d q the number of categorical variables.</p>
<p>b q c q e q f q j q k</p>
<p>Table 1 :
1Continuous design variables and their definition domainsVariable 
Definition domain </p>
<p>Root chord Rc (m) 
[3.5, 6] 
Tip chord T c (m) 
[1, 1.5] 
Span Sp (m) 
[7, 11] 
Diehedral angle Di (deg) 
[0, 5] 
Sweep angle Sw (deg) </p>
<p>, x d 1 , x q 1 u, . . . , tx c M , x d M , x q M uu, the objective function, the features and the constraints are evaluated to get the corresponding outputs: for the objective function Y M " ty 1 " f px c1 , x d 1 , x q 1 q, . . . , y M " f px c M , x d M , x q M qu, for the features f t j p¨q, F t j M " tf t j px c 1 , x d 1 , x q 1 q, . . . , f t j px c M , x d M ,x q M qu and for the constraints (here inequality) g i p¨q,G i M " tg i px c 1 , x d 1 , x q 1 q, . . . , g i px c M , x d M ,x q M qu. Based on these DoEs, a GP with an adapted covariance model is constructed for each function to be modeled. Moreover, a QD archive is created from the input DoE and the corresponding evaluations of the exact functions. Based on the DoE, only feasible solutions with respect to the exact constraints are added in the archive. Moreover, for each discovered bin (defined with the exact features), the best solution with respect to the exact objective function is added in the niche.
AcknowledgmentsThis work is part of the PHOBOS project funded by ONERA -The French Aerospace. The authors want to thank Dr. R. Wuilbercq and M. G. Sire for fruitful discussions.
Computational approaches for aerospace design: the pursuit of excellence. A Keane, P Nair, John Wiley &amp; SonsA. Keane, P. Nair, Computational approaches for aerospace design: the pur- suit of excellence, John Wiley &amp; Sons, 2005.</p>
<p>Structural optimization in civil engineering: a literature review. L Mei, Q Wang, Buildings. 11266L. Mei, Q. Wang, Structural optimization in civil engineering: a literature review, Buildings 11 (2) (2021) 66.</p>
<p>Wind power generation and wind turbine design. W Tong, WIT pressW. Tong, Wind power generation and wind turbine design, WIT press, 2010.</p>
<p>Adaptation in natural and artificial systems, university of michigan press. J Holland, Ann Arbor. 7J. Holland, Adaptation in natural and artificial systems, university of michi- gan press, Ann Arbor 7 (1975) 390-401.</p>
<p>Particle swarm optimization. J Kennedy, R Eberhart, Proceedings of ICNN'95-international conference on neural networks. ICNN'95-international conference on neural networksIEEE4J. Kennedy, R. Eberhart, Particle swarm optimization, in: Proceedings of ICNN'95-international conference on neural networks, Vol. 4, IEEE, 1995, pp. 1942-1948.</p>
<p>Recent advances in surrogate-based optimization. A I Forrester, A J Keane, Progress in aerospace sciences. 451-3A. I. Forrester, A. J. Keane, Recent advances in surrogate-based optimization, Progress in aerospace sciences 45 (1-3) (2009) 50-79.</p>
<p>Rapid airfoil design optimization via neural networks-based parameterization and surrogate modeling. X Du, P He, J R Martins, Aerospace Science and Technology. 113106701X. Du, P. He, J. R. Martins, Rapid airfoil design optimization via neural networks-based parameterization and surrogate modeling, Aerospace Science and Technology 113 (2021) 106701.</p>
<p>Surrogates: Gaussian process modeling, design, and optimization for the applied sciences. R B Gramacy, CRC pressR. B. Gramacy, Surrogates: Gaussian process modeling, design, and opti- mization for the applied sciences, CRC press, 2020.</p>
<p>Efficient detailed design optimization of topology optimization concepts by using support vector machines and metamodels. N Strömberg, Engineering Optimization. 527N. Strömberg, Efficient detailed design optimization of topology optimization concepts by using support vector machines and metamodels, Engineering Op- timization 52 (7) (2020) 1136-1148.</p>
<p>A comparative study of infill sampling criteria for computationally expensive constrained optimization problems. K Chaiyotha, T Krityakierne, Symmetry. 12101631K. Chaiyotha, T. Krityakierne, A comparative study of infill sampling criteria for computationally expensive constrained optimization problems, Symmetry 12 (10) (2020) 1631.</p>
<p>Revisiting bayesian optimization in the light of the COCO benchmark. R Le Riche, V Picheny, Structural and Multidisciplinary Optimization. 645R. Le Riche, V. Picheny, Revisiting bayesian optimization in the light of the COCO benchmark, Structural and Multidisciplinary Optimization 64 (5) (2021) 3063-3087.</p>
<p>A review of multi-objective optimization: Methods and its applications. N Gunantara, Cogent Engineering. 511502242N. Gunantara, A review of multi-objective optimization: Methods and its applications, Cogent Engineering 5 (1) (2018) 1502242.</p>
<p>Multi-objective multidisciplinary design optimization approach for partially reusable launch vehicle design. L Brevault, M Balesdent, A , Journal of Spacecraft and Rockets. 572L. Brevault, M. Balesdent, A. Hebbal, Multi-objective multidisciplinary de- sign optimization approach for partially reusable launch vehicle design, Jour- nal of Spacecraft and Rockets 57 (2) (2020) 373-390.</p>
<p>Quality-diversity optimization: a novel branch of stochastic optimization, in: Black Box Optimization. K Chatzilygeroudis, A Cully, V Vassiliades, J.-B Mouret, Machine Learning, and No-Free Lunch Theorems. SpringerK. Chatzilygeroudis, A. Cully, V. Vassiliades, J.-B. Mouret, Quality-diversity optimization: a novel branch of stochastic optimization, in: Black Box Opti- mization, Machine Learning, and No-Free Lunch Theorems, Springer, 2021, pp. 109-135.</p>
<p>Quality diversity: A new frontier for evolutionary computation. J K Pugh, L B Soros, K O Stanley, Frontiers in Robotics and AI. 340J. K. Pugh, L. B. Soros, K. O. Stanley, Quality diversity: A new frontier for evolutionary computation, Frontiers in Robotics and AI 3 (2016) 40.</p>
<p>Illuminating search spaces by mapping elites. J.-B Mouret, J Clune, arXiv:1504.04909arXiv preprintJ.-B. Mouret, J. Clune, Illuminating search spaces by mapping elites, arXiv preprint arXiv:1504.04909 (2015).</p>
<p>Aerodynamic design exploration through surrogate-assisted illumination. A Gaier, A Asteroth, J.-B Mouret, 18th AIAA/ISSMO multidisciplinary analysis and optimization conference. 3330A. Gaier, A. Asteroth, J.-B. Mouret, Aerodynamic design exploration through surrogate-assisted illumination, in: 18th AIAA/ISSMO multidisciplinary analysis and optimization conference, 2017, p. 3330.</p>
<p>Designing air flow with surrogateassisted phenotypic niching. A Hagg, D Wilde, A Asteroth, T Bäck, International Conference on Parallel Problem Solving from Nature. SpringerA. Hagg, D. Wilde, A. Asteroth, T. Bäck, Designing air flow with surrogate- assisted phenotypic niching, in: International Conference on Parallel Problem Solving from Nature, Springer, 2020, pp. 140-153.</p>
<p>P Kent, J Branke, arXiv:2005.04320Bop-elites, a bayesian optimisation algorithm for qualitydiversity search. arXiv preprintP. Kent, J. Branke, Bop-elites, a bayesian optimisation algorithm for quality- diversity search, arXiv preprint arXiv:2005.04320 (2020).</p>
<p>P Kent, A Gaier, J.-B Mouret, J Branke, arXiv:2307.09326Bop-elites, a bayesian optimisation approach to quality diversity search with black-box descriptor functions. arXiv preprintP. Kent, A. Gaier, J.-B. Mouret, J. Branke, Bop-elites, a bayesian optimisa- tion approach to quality diversity search with black-box descriptor functions, arXiv preprint arXiv:2307.09326 (2023).</p>
<p>Efficient global optimization of constrained mixed variable problems. J Pelamatti, L Brevault, M Balesdent, E.-G Talbi, Y Guerin, Journal of Global Optimization. 73J. Pelamatti, L. Brevault, M. Balesdent, E.-G. Talbi, Y. Guerin, Efficient global optimization of constrained mixed variable problems, Journal of Global Optimization 73 (2019) 583-613.</p>
<p>A general square exponential kernel to handle mixed-categorical variables for gaussian process. P Saves, Y Diouane, N Bartoli, T Lefebvre, J Morlier, AIAA AVIATION 2022 Forum. 3870P. Saves, Y. Diouane, N. Bartoli, T. Lefebvre, J. Morlier, A general square exponential kernel to handle mixed-categorical variables for gaussian process, in: AIAA AVIATION 2022 Forum, 2022, p. 3870.</p>
<p>Introduction to optimization. P , Springer46P. Pedregal, Introduction to optimization, Vol. 46, Springer, 2004.</p>
<p>Mesh adaptive direct search algorithms for constrained optimization. C Audet, J E DennisJr, SIAM Journal on optimization. 171C. Audet, J. E. Dennis Jr, Mesh adaptive direct search algorithms for con- strained optimization, SIAM Journal on optimization 17 (1) (2006) 188-217.</p>
<p>Mesh adaptive direct search algorithms for mixed variable optimization. M A Abramson, C Audet, J W Chrissis, J G Walston, Optimization Letters. 3M. A. Abramson, C. Audet, J. W. Chrissis, J. G. Walston, Mesh adaptive direct search algorithms for mixed variable optimization, Optimization Letters 3 (2009) 35-47.</p>
<p>Branch and bound algorithms-principles and examples. J Clausen, Department of Computer Science, University of CopenhagenJ. Clausen, Branch and bound algorithms-principles and examples, Depart- ment of Computer Science, University of Copenhagen (1999) 1-30.</p>
<p>Evolutionary optimization algorithms. D Simon, John Wiley &amp; SonsD. Simon, Evolutionary optimization algorithms, John Wiley &amp; Sons, 2013.</p>
<p>Practical methods of optimization. R Fletcher, John Wiley &amp; SonsR. Fletcher, Practical methods of optimization, John Wiley &amp; Sons, 2000.</p>
<p>Quadratic programming, Numerical optimization. J Nocedal, S J Wright, J. Nocedal, S. J. Wright, Quadratic programming, Numerical optimization (2006) 448-492.</p>
<p>A review and comparison of solvers for convex minlp. J Kronqvist, D E Bernal, A Lundell, I E Grossmann, Optimization and Engineering. 20J. Kronqvist, D. E. Bernal, A. Lundell, I. E. Grossmann, A review and com- parison of solvers for convex minlp, Optimization and Engineering 20 (2019) 397-455.</p>
<p>An introduction to genetic algorithms. M Mitchell, MIT pressM. Mitchell, An introduction to genetic algorithms, MIT press, 1998.</p>
<p>Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation. N Hansen, A Ostermeier, Proceedings of IEEE international conference on evolutionary computation. IEEE international conference on evolutionary computationIEEEN. Hansen, A. Ostermeier, Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation, in: Proceedings of IEEE international conference on evolutionary computation, IEEE, 1996, pp. 312-317.</p>
<p>A hybrid differential evolution algorithm for mixed-variable optimization problems. Y Lin, Y Liu, W.-N Chen, J Zhang, Information Sciences. 466Y. Lin, Y. Liu, W.-N. Chen, J. Zhang, A hybrid differential evolution al- gorithm for mixed-variable optimization problems, Information Sciences 466 (2018) 170-188.</p>
<p>A particle swarm optimization algorithm for mixed-variable optimization problems. F Wang, H Zhang, A Zhou, Swarm and Evolutionary Computation. 60100808F. Wang, H. Zhang, A. Zhou, A particle swarm optimization algorithm for mixed-variable optimization problems, Swarm and Evolutionary Computation 60 (2021) 100808.</p>
<p>Constraint-handling techniques used with evolutionary algorithms. C A C Coello, Proceedings of the genetic and evolutionary computation conference companion. the genetic and evolutionary computation conference companionC. A. C. Coello, Constraint-handling techniques used with evolutionary algo- rithms, in: Proceedings of the genetic and evolutionary computation confer- ence companion, 2022, pp. 1310-1333.</p>
<p>Efficient global optimization of expensive black-box functions. D R Jones, M Schonlau, W J Welch, Journal of Global optimization. 13D. R. Jones, M. Schonlau, W. J. Welch, Efficient global optimization of expen- sive black-box functions, Journal of Global optimization 13 (1998) 455-492.</p>
<p>M T Emmerich, A H Deutz, A tutorial on multiobjective optimization: fundamentals and evolutionary methods. 17M. T. Emmerich, A. H. Deutz, A tutorial on multiobjective optimization: fundamentals and evolutionary methods, Natural computing 17 (2018) 585- 609.</p>
<p>Evolving a diversity of virtual creatures through novelty search and local competition. J Lehman, K O Stanley, Proceedings of the 13th annual conference on Genetic and evolutionary computation. the 13th annual conference on Genetic and evolutionary computationJ. Lehman, K. O. Stanley, Evolving a diversity of virtual creatures through novelty search and local competition, in: Proceedings of the 13th annual conference on Genetic and evolutionary computation, 2011, pp. 211-218.</p>
<p>Covariance matrix adaptation for the rapid illumination of behavior space. M C Fontaine, J Togelius, S Nikolaidis, A K Hoover, Proceedings of the 2020 genetic and evolutionary computation conference. the 2020 genetic and evolutionary computation conferenceM. C. Fontaine, J. Togelius, S. Nikolaidis, A. K. Hoover, Covariance matrix adaptation for the rapid illumination of behavior space, in: Proceedings of the 2020 genetic and evolutionary computation conference, 2020, pp. 94-102.</p>
<p>Deep surrogate assisted map-elites for automated hearthstone deckbuilding. Y Zhang, M C Fontaine, A K Hoover, S Nikolaidis, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceY. Zhang, M. C. Fontaine, A. K. Hoover, S. Nikolaidis, Deep surrogate assisted map-elites for automated hearthstone deckbuilding, in: Proceedings of the Genetic and Evolutionary Computation Conference, 2022, pp. 158-167.</p>
<p>Kriging: a method of interpolation for geographical information systems. M A Oliver, R Webster, International Journal of Geographical Information System. 43M. A. Oliver, R. Webster, Kriging: a method of interpolation for geograph- ical information systems, International Journal of Geographical Information System 4 (3) (1990) 313-332.</p>
<p>The design and analysis of computer experiments. T J Santner, B J Williams, W I Notz, B J Williams, Springer1T. J. Santner, B. J. Williams, W. I. Notz, B. J. Williams, The design and analysis of computer experiments, Vol. 1, Springer, 2003.</p>
<p>Gaussian processes in machine learning. C E Rasmussen, SpringerSummer school on machine learningC. E. Rasmussen, Gaussian processes in machine learning, in: Summer school on machine learning, Springer, 2003, pp. 63-71.</p>
<p>Kernels for vector-valued functions: A review. M A Álvarez, L Rosasco, N D Lawrence, Foundations and Trends in Machine Learning. 43M. A.Álvarez, L. Rosasco, N. D. Lawrence, Kernels for vector-valued func- tions: A review, Foundations and Trends in Machine Learning 4 (3) (2012) 195-266.</p>
<p>Mixed variable gaussian process-based surrogate modeling techniques: Application to aerospace design. J Pelamatti, L Brevault, M Balesdent, E.-G Talbi, Y Guerin, Journal of Aerospace Information Systems. 1811J. Pelamatti, L. Brevault, M. Balesdent, E.-G. Talbi, Y. Guerin, Mixed vari- able gaussian process-based surrogate modeling techniques: Application to aerospace design, Journal of Aerospace Information Systems 18 (11) (2021) 813-837.</p>
<p>Black-box optimization of mixed discrete-continuous optimization problems. M Halstrup, TU Dortmund UniversityPh.D. thesisM. Halstrup, Black-box optimization of mixed discrete-continuous optimiza- tion problems, Ph.D. thesis, TU Dortmund University (2016).</p>
<p>Overview and comparison of gaussian process-based surrogate models for mixed continuous and discrete variables: Application on aerospace design problems, High-Performance Simulation-Based Optimization. J Pelamatti, L Brevault, M Balesdent, E.-G Talbi, Y Guerin, J. Pelamatti, L. Brevault, M. Balesdent, E.-G. Talbi, Y. Guerin, Overview and comparison of gaussian process-based surrogate models for mixed con- tinuous and discrete variables: Application on aerospace design problems, High-Performance Simulation-Based Optimization (2020) 189-224.</p>
<p>A general coefficient of similarity and some of its properties. J C Gower, Biometrics. J. C. Gower, A general coefficient of similarity and some of its properties, Biometrics (1971) 857-871.</p>
<p>A simple approach to emulation for computer models with qualitative and quantitative factors. Q Zhou, P Z Qian, S Zhou, Technometrics. 533Q. Zhou, P. Z. Qian, S. Zhou, A simple approach to emulation for computer models with qualitative and quantitative factors, Technometrics 53 (3) (2011) 266-273.</p>
<p>The most general methodology to create a valid correlation matrix for risk management and option pricing purposes. R Rebonato, P , Available at SSRN 1969689. R. Rebonato, P. Jäckel, The most general methodology to create a valid correlation matrix for risk management and option pricing purposes, Available at SSRN 1969689 (2011).</p>
<p>A latent variable approach to gaussian process modeling with qualitative and quantitative factors. Y Zhang, S Tao, W Chen, D W Apley, Technometrics. 623Y. Zhang, S. Tao, W. Chen, D. W. Apley, A latent variable approach to gaussian process modeling with qualitative and quantitative factors, Techno- metrics 62 (3) (2020) 291-302.</p>
<p>Kernels for vector-valued functions: A review. M A Alvarez, L Rosasco, N D Lawrence, Foundations and Trends® in Machine Learning. 43M. A. Alvarez, L. Rosasco, N. D. Lawrence, et al., Kernels for vector-valued functions: A review, Foundations and Trends® in Machine Learning 4 (3) (2012) 195-266.</p>
<p>A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. M D Mckay, R J Beckman, W J Conover, Technometrics. 421M. D. McKay, R. J. Beckman, W. J. Conover, A comparison of three methods for selecting values of input variables in the analysis of output from a computer code, Technometrics 42 (1) (2000) 55-61.</p>
<p>On the distribution of points in a cube and the approximate evaluation of integrals. I M Sobol, Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki. 74I. M. Sobol', On the distribution of points in a cube and the approximate evaluation of integrals, Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki 7 (4) (1967) 784-802.</p>
<p>Design for computer experiments with qualitative and quantitative factors. X Deng, Y Hung, C D Lin, Statistica Sinica. X. Deng, Y. Hung, C. D. Lin, Design for computer experiments with qualita- tive and quantitative factors, Statistica Sinica (2015) 1567-1581.</p>
<p>A surrogate-model-based method for constrained optimization. C Audet, J Denni, D Moore, A Booker, P Frank, 8th symposium on multidisciplinary analysis and optimization. 4891C. Audet, J. Denni, D. Moore, A. Booker, P. Frank, A surrogate-model-based method for constrained optimization, in: 8th symposium on multidisciplinary analysis and optimization, 2000, p. 4891.</p>
<p>Constraint-handling in genetic algorithms through the use of dominance-based tournament selection. C A C Coello, E M Montes, Advanced Engineering Informatics. 163C. A. C. Coello, E. M. Montes, Constraint-handling in genetic algorithms through the use of dominance-based tournament selection, Advanced Engi- neering Informatics 16 (3) (2002) 193-203.</p>
<p>An automatic method for finding the greatest or least value of a function. H Rosenbrock, The computer journal. 33H. Rosenbrock, An automatic method for finding the greatest or least value of a function, The computer journal 3 (3) (1960) 175-184.</p>
<p>Some hard global optimization test problems. A Neumaier, A. Neumaier, Some hard global optimization test problems (1999).</p>
<p>Experiments in nonconvex optimization: stochastic approximation with function smoothing and simulated annealing. M Styblinski, T.-S Tang, Neural Networks. 34M. Styblinski, T.-S. Tang, Experiments in nonconvex optimization: stochas- tic approximation with function smoothing and simulated annealing, Neural Networks 3 (4) (1990) 467-483.</p>
<p>Advanced modeling in OpenVSP. R A Mcdonald, 16th AIAA Aviation Technology, Integration, and Operations Conference. 3282R. A. McDonald, Advanced modeling in OpenVSP, in: 16th AIAA Aviation Technology, Integration, and Operations Conference, 2016, p. 3282.</p>
<p>J Anderson, Fundamentals of Aerodynamics (SI units). McGraw hillJ. Anderson, Fundamentals of Aerodynamics (SI units), McGraw hill, 2011.</p>
<p>. R Sjögren, D Svensson, pydoe2: An experimental design package for pythonR. Sjögren, D. Svensson, pydoe2: An experimental design package for python (2018).</p>
<p>Martins, A python surrogate modeling framework with derivatives. M A Bouhlel, J T Hwang, N Bartoli, R Lafage, J Morlier, J R , Advances in Engineering Software. 135102662M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Morlier, J. R. Mar- tins, A python surrogate modeling framework with derivatives, Advances in Engineering Software 135 (2019) 102662.</p>            </div>
        </div>

    </div>
</body>
</html>
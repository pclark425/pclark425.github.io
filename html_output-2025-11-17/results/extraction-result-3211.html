<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3211 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3211</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3211</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-a81874b4a651a740fffbfc47ef96515e8c7f782f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a81874b4a651a740fffbfc47ef96515e8c7f782f" target="_blank">Latent Retrieval for Weakly Supervised Open Domain Question Answering</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is shown for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system, and outperforming BM25 by up to 19 points in exact match.</p>
                <p><strong>Paper Abstract:</strong> Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3211.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3211.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-Retrieval Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end open-domain QA system that jointly learns a dense retriever and a span-extraction reader using only question-answer string pairs; retrieval is treated as a latent variable and is bootstrapped by unsupervised pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ORQA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A two-part model composed of (1) a dense retriever that encodes questions and evidence blocks with BERT-based encoders projected to 128-d vectors and scores them by inner product, and (2) a BERT-based span reader that encodes question+block and scores start/end span endpoints via an MLP. The retriever is pre-trained with the Inverse Cloze Task and fine-tuned end-to-end (except frozen block encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory is an external text corpus (Wikipedia) split into ~13M pre-encoded evidence blocks (BERT_B outputs projected to 128-d). At inference the question is encoded (trainable), top-k blocks are retrieved by maximum inner product search over precomputed block vectors (LSH/indexing), then the reader scores spans within those blocks; retriever pre-trained via Inverse Cloze Task (ICT) and fine-tuned using marginal likelihood over top-k derivations with an early-update over top-c blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering (Natural Questions, WebQuestions, CuratedTrec, TriviaQA, SQuAD - open versions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer open-domain factoid questions by retrieving supporting evidence from a large unstructured corpus (Wikipedia) and extracting an answer span or answer string; challenges include very large search space (~13M blocks), weak supervision (only QA pairs), and spurious derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Natural Questions: 31.3% EM; WebQuestions: 38.5% EM; CuratedTrec: 36.8% EM; TriviaQA: 45.1% EM; SQuAD: 26.5% EM (exact match; values from Table 5, ORQA column).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>BM25 + BERT reader baseline: Natural Questions: 24.8% EM; WebQuestions: 20.8% EM; CuratedTrec: 27.1% EM; TriviaQA: 47.2% EM; SQuAD: 28.1% EM (exact match; values from BM25+BERT column in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a dense retriever (with ICT pre-training) and fine-tuning end-to-end yields large gains on genuine information-seeking datasets (questions where writers did not already know the answer): improvements of roughly 6–19 absolute EM points over BM25+BERT on Natural Questions, WebQuestions, and CuratedTrec; however BM25 remains stronger on datasets with high lexical overlap between question and evidence (SQuAD and TriviaQA). ICT pre-training is critical to bootstrap latent retrieval learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Capacity limits of highly compressed 128-d vectors reduce ability to represent very specific lexical concepts; spurious ambiguities due to weak supervision (answer string appearing in irrelevant contexts) require strong inductive bias from ICT; SQuAD dataset biases make learned retrieval unsuitable there; block encoder is frozen (precomputed) which limits end-to-end updates to block representations; requires large precomputation and an index (but mitigated by pre-encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3211.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3211.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Cloze Task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised pre-training objective for retrieval: treat a random sentence as a pseudo-question and its surrounding context as pseudo-evidence, and train the retriever discriminatively to select the correct context among in-batch negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inverse Cloze Task (pretraining objective for the retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not an agent per se but a pretraining procedure: sample a sentence as pseudo-query and its surrounding text as positive block; negative blocks are other in-batch blocks; retriever is trained with a softmax over batch (in-batch negatives) using the same dense inner-product scoring used at downstream retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>pretraining for retrieval-augmented external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>ICT trains the query and block encoders so that semantically related sentence/context pairs score highly by inner product; uses masking of the pseudo-query from context in 90% of examples to encourage semantic matching beyond n-gram overlap; enables zero-shot retrieval performance sufficient for subsequent latent-variable fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pretraining for open-domain retrieval (enables QA downstream)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Unsupervised objective to learn retrieval representations that map under-specified queries to supporting evidence (simulates QA retrieval without labeled QA pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation learning / retrieval pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICT pre-training provides a strong initialization: with ICT the model discards <10% of training examples (vs. much higher if randomly initialized), enables pre-encoding of evidence and dynamic top-k retrieval during fine-tuning, and is crucial for end-to-end learning; masking rate matters (90% masking optimal to balance n-gram and semantic signals).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mismatch between pseudo-queries and real questions exists but is manageable; if pseudo-query always masked, model loses n-gram overlap signal; if never masked, model memorizes and behaves like BM25; requires large-batch in-batch negatives during pretraining (they used batch size 4096).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3211.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3211.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 + BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 retrieval with BERT reader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong baseline that uses lexical BM25 retrieval (sparse retrieval) to propose top-k evidence blocks and a BERT-based reader to extract answer spans; retrieval is static and non-trainable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The probabilistic relevance framework: BM25 and beyond.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BM25 + BERT reader</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval component uses BM25 (sparse lexical matching) implemented via Lucene/Anserini to return top-k candidates; reader is a BERT-based span extractor that scores spans within retrieved blocks (final score a learned weighted sum of BM25 and reader).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external IR (sparse index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Evidence is retrieved through a BM25 index over the corpus (sparse term-based scoring). Retrieved candidates are static (not fine-tunable on downstream QA) and passed to a trained BERT reader for span scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering (same datasets as ORQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve relevant evidence passages via lexical matching and extract answers; strong for cases with high lexical overlap between question and evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Natural Questions: 24.8% EM; WebQuestions: 20.8% EM; CuratedTrec: 27.1% EM; TriviaQA: 47.2% EM; SQuAD: 28.1% EM (Table 5 BM25+BERT column).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BM25 is a powerful unsupervised retrieval method and outperforms dense 128-d retrievers on datasets where question writers included lexical hints or known evidence (TriviaQA, SQuAD); however it cannot be fine-tuned end-to-end and can set an upper bound when used as a black-box retrieval proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Static retrieval (not trainable) prevents end-to-end improvement; lexical matching fails when queries are under-specified and require semantic understanding; performance suffers on genuine information-seeking datasets compared to learned dense retrieval (ORQA).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3211.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3211.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NNLM + BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Network Language Model pooled retrieval + BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that uses pooled fixed representations from a feed-forward neural language model (NNLM) to encode evidence blocks for retrieval, combined with the BERT reader.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A neural probabilistic language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NNLM + BERT (retrieval baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Evidence blocks are encoded by pooling representations from a pre-trained feed-forward neural language model (128-d), these precomputed vectors are used for retrieval (inner-product) and the question encoder is fine-tuned; reader is BERT-based.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (dense vectors from pre-trained LM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Use precomputed dense vectors from NNLM to represent corpus blocks; retrieve by nearest neighbor / inner product and pass top-k to BERT reader.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering (same datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether off-the-shelf LM pooled sentence/block vectors are effective as retrieval encoders for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Natural Questions: 3.2% EM; WebQuestions: 9.1% EM; CuratedTrec: 6.0% EM; TriviaQA: 7.3% EM; SQuAD: 2.8% EM (Table 5 NNLM+BERT column).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pooled NNLM representations perform poorly as retrieval encoders for open-domain QA compared to BM25 and ICT-pretrained dense retrievers; they fail to capture necessary retrieval signals in compressed 128-d vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Off-the-shelf LM pooled vectors do not optimize for retrieval and thus underperform; dense 128-d compression may be insufficient to capture both lexical and semantic cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3211.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3211.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELMo + BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELMo pooled retrieval + BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline using pooled contextualized representations from ELMo (small) to encode evidence blocks for retrieval, combined with a BERT reader.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep contextualized word representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ELMo + BERT (retrieval baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Evidence blocks are encoded by pooling contextualized token representations from a pre-trained ELMo (small) model into 128-d vectors used for retrieval; the question encoder is fine-tuned; BERT serves as reader.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (dense vectors from pre-trained LM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Precompute block vectors using ELMo, retrieve nearest neighbors by inner product, feed top-k to BERT reader for span extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering (same datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Test whether ELMo pooled representations can serve as effective retrieval encoders for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Natural Questions: 3.6% EM; WebQuestions: 17.7% EM; CuratedTrec: 8.3% EM; TriviaQA: 6.0% EM; SQuAD: 1.9% EM (Table 5 ELMO+BERT column).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ELMo pooled retrieval improves over NNLM in some datasets (e.g., WebQuestions) but still falls far behind BM25 and the ICT-pretrained dense retriever (ORQA); unsupervised LM pooled vectors do not readily capture strong retrieval signals for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Similar to NNLM: not optimized for retrieval, limited representational capacity after pooling/compression to 128-d, inconsistent performance across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3211.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3211.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (Language model used as unsupervised QA example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-2 is cited as an example of unsupervised QA (generative language model that can produce answer strings without external evidence); in this paper it is only mentioned as a task analogy and not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are unsupervised multitask learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-2 (as an example of unsupervised QA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A large pre-trained generative transformer language model that can produce text outputs (including answers) without explicit retrieval of external evidence; in this paper it is referenced in Table 1 as an example of 'Unsupervised QA' where no evidence is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal model parameters (no external memory mentioned in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Not applicable in this paper; GPT-2 is only cited as an example of unsupervised QA methods that rely on the model's learned parametric knowledge rather than external retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Unsupervised QA (conceptual example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate answers without retrieving external evidence; challenges include factual accuracy and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering (unsupervised/generative)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned only as an illustrative example that some QA approaches rely solely on language models without retrieving external evidence; no experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper; generative LMs in general risk hallucination and do not provide explicit evidence grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reading wikipedia to answer open-domain questions <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>The probabilistic relevance framework: BM25 and beyond. <em>(Rating: 2)</em></li>
                <li>Language models are unsupervised multitask learners. <em>(Rating: 1)</em></li>
                <li>Improving information extraction by acquiring external evidence with reinforcement learning <em>(Rating: 2)</em></li>
                <li>End-to-end open-domain question answering with BERTserini <em>(Rating: 2)</em></li>
                <li>An efficient framework for learning sentence representations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3211",
    "paper_id": "paper-a81874b4a651a740fffbfc47ef96515e8c7f782f",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "ORQA",
            "name_full": "Open-Retrieval Question Answering",
            "brief_description": "An end-to-end open-domain QA system that jointly learns a dense retriever and a span-extraction reader using only question-answer string pairs; retrieval is treated as a latent variable and is bootstrapped by unsupervised pre-training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ORQA",
            "agent_description": "A two-part model composed of (1) a dense retriever that encodes questions and evidence blocks with BERT-based encoders projected to 128-d vectors and scores them by inner product, and (2) a BERT-based span reader that encodes question+block and scores start/end span endpoints via an MLP. The retriever is pre-trained with the Inverse Cloze Task and fine-tuned end-to-end (except frozen block encoder).",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external corpus)",
            "memory_mechanism_description": "Memory is an external text corpus (Wikipedia) split into ~13M pre-encoded evidence blocks (BERT_B outputs projected to 128-d). At inference the question is encoded (trainable), top-k blocks are retrieved by maximum inner product search over precomputed block vectors (LSH/indexing), then the reader scores spans within those blocks; retriever pre-trained via Inverse Cloze Task (ICT) and fine-tuned using marginal likelihood over top-k derivations with an early-update over top-c blocks.",
            "task_name": "Open-domain question answering (Natural Questions, WebQuestions, CuratedTrec, TriviaQA, SQuAD - open versions)",
            "task_description": "Answer open-domain factoid questions by retrieving supporting evidence from a large unstructured corpus (Wikipedia) and extracting an answer span or answer string; challenges include very large search space (~13M blocks), weak supervision (only QA pairs), and spurious derivations.",
            "task_type": "question answering",
            "performance_with_memory": "Natural Questions: 31.3% EM; WebQuestions: 38.5% EM; CuratedTrec: 36.8% EM; TriviaQA: 45.1% EM; SQuAD: 26.5% EM (exact match; values from Table 5, ORQA column).",
            "performance_without_memory": "BM25 + BERT reader baseline: Natural Questions: 24.8% EM; WebQuestions: 20.8% EM; CuratedTrec: 27.1% EM; TriviaQA: 47.2% EM; SQuAD: 28.1% EM (exact match; values from BM25+BERT column in Table 5).",
            "has_performance_comparison": true,
            "key_findings": "Learning a dense retriever (with ICT pre-training) and fine-tuning end-to-end yields large gains on genuine information-seeking datasets (questions where writers did not already know the answer): improvements of roughly 6–19 absolute EM points over BM25+BERT on Natural Questions, WebQuestions, and CuratedTrec; however BM25 remains stronger on datasets with high lexical overlap between question and evidence (SQuAD and TriviaQA). ICT pre-training is critical to bootstrap latent retrieval learning.",
            "limitations_or_challenges": "Capacity limits of highly compressed 128-d vectors reduce ability to represent very specific lexical concepts; spurious ambiguities due to weak supervision (answer string appearing in irrelevant contexts) require strong inductive bias from ICT; SQuAD dataset biases make learned retrieval unsuitable there; block encoder is frozen (precomputed) which limits end-to-end updates to block representations; requires large precomputation and an index (but mitigated by pre-encoding).",
            "uuid": "e3211.0",
            "source_info": {
                "paper_title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "ICT",
            "name_full": "Inverse Cloze Task",
            "brief_description": "An unsupervised pre-training objective for retrieval: treat a random sentence as a pseudo-question and its surrounding context as pseudo-evidence, and train the retriever discriminatively to select the correct context among in-batch negatives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Inverse Cloze Task (pretraining objective for the retriever)",
            "agent_description": "Not an agent per se but a pretraining procedure: sample a sentence as pseudo-query and its surrounding text as positive block; negative blocks are other in-batch blocks; retriever is trained with a softmax over batch (in-batch negatives) using the same dense inner-product scoring used at downstream retrieval.",
            "memory_used": true,
            "memory_type": "pretraining for retrieval-augmented external memory",
            "memory_mechanism_description": "ICT trains the query and block encoders so that semantically related sentence/context pairs score highly by inner product; uses masking of the pseudo-query from context in 90% of examples to encourage semantic matching beyond n-gram overlap; enables zero-shot retrieval performance sufficient for subsequent latent-variable fine-tuning.",
            "task_name": "Pretraining for open-domain retrieval (enables QA downstream)",
            "task_description": "Unsupervised objective to learn retrieval representations that map under-specified queries to supporting evidence (simulates QA retrieval without labeled QA pairs).",
            "task_type": "representation learning / retrieval pretraining",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "ICT pre-training provides a strong initialization: with ICT the model discards &lt;10% of training examples (vs. much higher if randomly initialized), enables pre-encoding of evidence and dynamic top-k retrieval during fine-tuning, and is crucial for end-to-end learning; masking rate matters (90% masking optimal to balance n-gram and semantic signals).",
            "limitations_or_challenges": "Mismatch between pseudo-queries and real questions exists but is manageable; if pseudo-query always masked, model loses n-gram overlap signal; if never masked, model memorizes and behaves like BM25; requires large-batch in-batch negatives during pretraining (they used batch size 4096).",
            "uuid": "e3211.1",
            "source_info": {
                "paper_title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "BM25 + BERT",
            "name_full": "BM25 retrieval with BERT reader",
            "brief_description": "A strong baseline that uses lexical BM25 retrieval (sparse retrieval) to propose top-k evidence blocks and a BERT-based reader to extract answer spans; retrieval is static and non-trainable.",
            "citation_title": "The probabilistic relevance framework: BM25 and beyond.",
            "mention_or_use": "use",
            "agent_name": "BM25 + BERT reader",
            "agent_description": "Retrieval component uses BM25 (sparse lexical matching) implemented via Lucene/Anserini to return top-k candidates; reader is a BERT-based span extractor that scores spans within retrieved blocks (final score a learned weighted sum of BM25 and reader).",
            "memory_used": true,
            "memory_type": "external IR (sparse index)",
            "memory_mechanism_description": "Evidence is retrieved through a BM25 index over the corpus (sparse term-based scoring). Retrieved candidates are static (not fine-tunable on downstream QA) and passed to a trained BERT reader for span scoring.",
            "task_name": "Open-domain question answering (same datasets as ORQA)",
            "task_description": "Retrieve relevant evidence passages via lexical matching and extract answers; strong for cases with high lexical overlap between question and evidence.",
            "task_type": "question answering",
            "performance_with_memory": "Natural Questions: 24.8% EM; WebQuestions: 20.8% EM; CuratedTrec: 27.1% EM; TriviaQA: 47.2% EM; SQuAD: 28.1% EM (Table 5 BM25+BERT column).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "BM25 is a powerful unsupervised retrieval method and outperforms dense 128-d retrievers on datasets where question writers included lexical hints or known evidence (TriviaQA, SQuAD); however it cannot be fine-tuned end-to-end and can set an upper bound when used as a black-box retrieval proposal.",
            "limitations_or_challenges": "Static retrieval (not trainable) prevents end-to-end improvement; lexical matching fails when queries are under-specified and require semantic understanding; performance suffers on genuine information-seeking datasets compared to learned dense retrieval (ORQA).",
            "uuid": "e3211.2",
            "source_info": {
                "paper_title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "NNLM + BERT",
            "name_full": "Neural Network Language Model pooled retrieval + BERT",
            "brief_description": "A baseline that uses pooled fixed representations from a feed-forward neural language model (NNLM) to encode evidence blocks for retrieval, combined with the BERT reader.",
            "citation_title": "A neural probabilistic language model.",
            "mention_or_use": "use",
            "agent_name": "NNLM + BERT (retrieval baseline)",
            "agent_description": "Evidence blocks are encoded by pooling representations from a pre-trained feed-forward neural language model (128-d), these precomputed vectors are used for retrieval (inner-product) and the question encoder is fine-tuned; reader is BERT-based.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (dense vectors from pre-trained LM)",
            "memory_mechanism_description": "Use precomputed dense vectors from NNLM to represent corpus blocks; retrieve by nearest neighbor / inner product and pass top-k to BERT reader.",
            "task_name": "Open-domain question answering (same datasets)",
            "task_description": "Evaluate whether off-the-shelf LM pooled sentence/block vectors are effective as retrieval encoders for QA.",
            "task_type": "question answering",
            "performance_with_memory": "Natural Questions: 3.2% EM; WebQuestions: 9.1% EM; CuratedTrec: 6.0% EM; TriviaQA: 7.3% EM; SQuAD: 2.8% EM (Table 5 NNLM+BERT column).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Pooled NNLM representations perform poorly as retrieval encoders for open-domain QA compared to BM25 and ICT-pretrained dense retrievers; they fail to capture necessary retrieval signals in compressed 128-d vectors.",
            "limitations_or_challenges": "Off-the-shelf LM pooled vectors do not optimize for retrieval and thus underperform; dense 128-d compression may be insufficient to capture both lexical and semantic cues.",
            "uuid": "e3211.3",
            "source_info": {
                "paper_title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "ELMo + BERT",
            "name_full": "ELMo pooled retrieval + BERT",
            "brief_description": "A baseline using pooled contextualized representations from ELMo (small) to encode evidence blocks for retrieval, combined with a BERT reader.",
            "citation_title": "Deep contextualized word representations.",
            "mention_or_use": "use",
            "agent_name": "ELMo + BERT (retrieval baseline)",
            "agent_description": "Evidence blocks are encoded by pooling contextualized token representations from a pre-trained ELMo (small) model into 128-d vectors used for retrieval; the question encoder is fine-tuned; BERT serves as reader.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (dense vectors from pre-trained LM)",
            "memory_mechanism_description": "Precompute block vectors using ELMo, retrieve nearest neighbors by inner product, feed top-k to BERT reader for span extraction.",
            "task_name": "Open-domain question answering (same datasets)",
            "task_description": "Test whether ELMo pooled representations can serve as effective retrieval encoders for QA.",
            "task_type": "question answering",
            "performance_with_memory": "Natural Questions: 3.6% EM; WebQuestions: 17.7% EM; CuratedTrec: 8.3% EM; TriviaQA: 6.0% EM; SQuAD: 1.9% EM (Table 5 ELMO+BERT column).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "ELMo pooled retrieval improves over NNLM in some datasets (e.g., WebQuestions) but still falls far behind BM25 and the ICT-pretrained dense retriever (ORQA); unsupervised LM pooled vectors do not readily capture strong retrieval signals for QA.",
            "limitations_or_challenges": "Similar to NNLM: not optimized for retrieval, limited representational capacity after pooling/compression to 128-d, inconsistent performance across datasets.",
            "uuid": "e3211.4",
            "source_info": {
                "paper_title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "GPT-2 (mentioned)",
            "name_full": "GPT-2 (Language model used as unsupervised QA example)",
            "brief_description": "GPT-2 is cited as an example of unsupervised QA (generative language model that can produce answer strings without external evidence); in this paper it is only mentioned as a task analogy and not used in experiments.",
            "citation_title": "Language models are unsupervised multitask learners.",
            "mention_or_use": "mention",
            "agent_name": "GPT-2 (as an example of unsupervised QA)",
            "agent_description": "A large pre-trained generative transformer language model that can produce text outputs (including answers) without explicit retrieval of external evidence; in this paper it is referenced in Table 1 as an example of 'Unsupervised QA' where no evidence is provided.",
            "memory_used": false,
            "memory_type": "internal model parameters (no external memory mentioned in this paper)",
            "memory_mechanism_description": "Not applicable in this paper; GPT-2 is only cited as an example of unsupervised QA methods that rely on the model's learned parametric knowledge rather than external retrieval.",
            "task_name": "Unsupervised QA (conceptual example)",
            "task_description": "Generate answers without retrieving external evidence; challenges include factual accuracy and hallucination.",
            "task_type": "question answering (unsupervised/generative)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Mentioned only as an illustrative example that some QA approaches rely solely on language models without retrieving external evidence; no experimental comparison in this paper.",
            "limitations_or_challenges": "Not discussed in this paper; generative LMs in general risk hallucination and do not provide explicit evidence grounding.",
            "uuid": "e3211.5",
            "source_info": {
                "paper_title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reading wikipedia to answer open-domain questions",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "The probabilistic relevance framework: BM25 and beyond.",
            "rating": 2
        },
        {
            "paper_title": "Language models are unsupervised multitask learners.",
            "rating": 1
        },
        {
            "paper_title": "Improving information extraction by acquiring external evidence with reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "End-to-end open-domain question answering with BERTserini",
            "rating": 2
        },
        {
            "paper_title": "An efficient framework for learning sentence representations",
            "rating": 1
        }
    ],
    "cost": 0.0146065,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Latent Retrieval for Weakly Supervised Open Domain Question Answering</h1>
<p>Kenton Lee Ming-Wei Chang Kristina Toutanova<br>Google Research<br>Seattle, WA<br>{kentonl,mingweichang,kristout}@google.com</p>
<h4>Abstract</h4>
<p>Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.</p>
<h2>1 Introduction</h2>
<p>Due to recent advances in reading comprehension systems, there has been a revival of interest in open domain question answering (QA), where the evidence must be retrieved from an open corpus, rather than being given as input. This presents a more realistic scenario for practical applications.</p>
<p>Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task. In the strongly supervised setting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi
et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence.</p>
<p>These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by definition under-specified and require more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data.</p>
<p>In this work, we introduce the first OpenRetrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by questionanswer string pairs. While recent work on improving evidence retrieval has made significant progress (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019), they still only rerank a closed evidence set. The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch. IR systems offer a reasonable but potentially suboptimal starting point.</p>
<p>The key insight of this work is that end-toend learning is possible if we pre-train the retriever with an unsupervised Inverse Cloze Task (ICT). In ICT, a sentence is treated as a pseudoquestion, and its context is treated as pseudoevidence. Given a pseudo-question, ICT requires selecting the corresponding pseudo-evidence out of the candidates in a batch. ICT pre-training provides a sufficiently strong initialization such that ORQA, a joint retriever and reader model, can be fine-tuned end-to-end by simply optimiz-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Evidence</td>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Evidence</td>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Reading Comprehension</td>
<td style="text-align: center;">given</td>
<td style="text-align: center;">span</td>
<td style="text-align: center;">given</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">SQuAD (Rajpurkar et al., 2016)</td>
</tr>
<tr>
<td style="text-align: center;">Open-domain QA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Unsupervised QA</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">GPT-2 (Radford et al., 2019)</td>
</tr>
<tr>
<td style="text-align: center;">Strongly Supervised QA</td>
<td style="text-align: center;">given</td>
<td style="text-align: center;">span</td>
<td style="text-align: center;">heuristic</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">DrQA (Chen et al., 2017)</td>
</tr>
<tr>
<td style="text-align: center;">Weakly Supervised QA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Closed Retrieval QA</td>
<td style="text-align: center;">heuristic</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">heuristic</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">TriviaQA (Joshi et al., 2017)</td>
</tr>
<tr>
<td style="text-align: center;">Open Retrieval QA</td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">learned</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">ORQA (this work)</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of assumptions made by related tasks, along with references to examples. Heuristic evidence refers to the typical strategy of considering only a closed set of evidence documents from a traditional IR system, which sets a strict upper-bound on task performance. In this work (ORQA), only question-answer string pairs are observed during training, and evidence retrieval is learned in a completely end-to-end manner.
ing the marginal log-likelihood of correct answers that were found.</p>
<p>We evaluate ORQA on open versions of five existing QA datasets. On datasets where the question writers already know the answer-SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017)-the retrieval problem resembles traditional IR, and BM25 (Robertson et al., 2009) provides state-of-the-art retrieval. On datasets where question writers do not know the answerNatural Questions (Kwiatkowski et al., 2019), WebQuestions (Berant et al., 2013), and CuratedTrec (Baudis and Sedivý, 2015)—we show that learned retrieval is crucial, providing improvements of 6 to 19 points in exact match over BM25.</p>
<h2>2 Overview</h2>
<p>In this section, we introduce notation for open domain QA that is useful for comparing prior work, baselines, and our proposed model.</p>
<h3>2.1 Task</h3>
<p>In open domain question answering, the input $q$ is a question string, and the output $a$ is an answer string. Unlike reading comprehension, the source of evidence is a modeling choice rather than a part of the task definition. We compare the assumptions made by variants of reading comprehension and question answering tasks in Table 1.</p>
<p>Evaluation is exact match with any of the reference answer strings after minor normalization such as lowercasing, following evaluation scripts from DrQA (Chen et al., 2017).</p>
<h3>2.2 Formal Definitions</h3>
<p>We introduce several general definitions of model components that subsume many retrieval-based open domain question answering systems.</p>
<p>Models are defined with respect to an unstructured text corpus that is split into $B$ blocks of evidence texts. An answer derivation is a pair $(b, s)$, where $1 \leq b \leq B$ indicates the index of an evidence block and $s$ denotes a span of text within block $b$. The start and end token indices of span $s$ are denoted by START(s) and END(s) respectively.</p>
<p>Models define a scoring function $S(b, s, q)$ indicating the goodness of an answer derivation $(b, s)$ given a question $q$. Typically, this scoring function is decomposed over a retrieval component $S_{\text {retr }}(b, q)$ and a reader component $S_{\text {read }}(b, s, q)$ :</p>
<p>$$
S(b, s, q)=S_{\text {retr }}(b, q)+S_{\text {read }}(b, s, q)
$$</p>
<p>During inference, the model outputs the answer string of the highest scoring derivation:</p>
<p>$$
a^{*}=\operatorname{TEXT}\left(\underset{b, s}{\operatorname{argmax}} S(b, s, q)\right)
$$</p>
<p>where $\operatorname{TEXT}(b, s)$ deterministically maps answer derivation $(b, s)$ to an answer string. A major challenge of any open domain question answering system is handling the scale. In our experiments on the English Wikipedia corpus, we consider over 13 million evidence blocks $b$, each with over 2000 possible answer spans $s$.</p>
<h3>2.3 Existing Pipelined Models</h3>
<p>In existing retrieval-based open domain question answering systems, a blackbox IR system first chooses a closed set of evidence candidates. For example, the score from the retriever component of DrQA (Chen et al., 2017) is defined as:</p>
<p>$$
S_{\text {retr }}(b, q)= \begin{cases}0 &amp; b \in \operatorname{TOP}(k, \operatorname{TF}-\operatorname{IDF}(q, b)) \ -\infty &amp; \text { otherwise }\end{cases}
$$</p>
<p>Most work following DrQA use the same candidates from TF-IDF and focus on reading comprehension or re-ranking. The reading component</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of ORQA. A subset of all possible answer derivations given a question <em>q</em> is shown here. Retrieval scores <em>S_retr(q, b)</em> are computed via inner products between BERT-based encoders. Top-scoring evidence blocks are jointly encoded with the question, and span representations are scored with a multi-layer perceptron (MLP) to compute <em>S_read(q, b, s)</em>. The final joint model score is <em>S_retr(q, b) + S_read(q, b, s)</em>. Unlike previous work using IR systems for candidate proposal, we learn to retrieve from all of Wikipedia directly.</p>
<p><em>S_read(b, s, q)</em> is learned from gold answer derivations, typically from the SQuAD (Rajpurkar et al., 2016) dataset, where the evidence text is given.</p>
<p>In work that is more closely related to our approach, the reader is learned entirely from weak supervision (Joshi et al., 2017; Dhingra et al., 2017; Dunn et al., 2017). Spurious ambiguities (see Table 2) are heuristically removed by the retrieval system, and the cleaned results are treated as gold derivations.</p>
<h1>3 Open-Retrieval Question Answering (ORQA)</h1>
<p>We propose an end-to-end model where the retriever and reader components are jointly learned, which we refer to as the Open-Retrieval Question Answering (ORQA) model. An important aspect of ORQA is its expressivity—it is capable of retrieving any text in an open corpus, rather than being limited to the closed set returned by a black-box IR system. An illustration of how ORQA scores answer derivations is presented in Figure 1.</p>
<p>Following recent advances in transfer learning, all scoring components are derived from BERT (Devlin et al., 2018), a bidirectional transformer that has been pre-trained on unsupervised language-modeling data. We refer the reader to the original paper for details of the architecture. In this work, the relevant abstraction can be described by the following function:</p>
<p>BERT(x1,[x2]) = {CLS: hCLS, 1: h1, 2: h2, ...}</p>
<p>The BERT function takes one or two string inputs (x1 and optionally x2) as arguments. It returns vectors corresponding to representations of the CLS pooling token or the input tokens.</p>
<p><strong>Retriever component</strong> In order for the retriever to be learnable, we define the retrieval score as the inner product of dense vector representations of the question <em>q</em> and the evidence block <em>b</em>.</p>
<p>$$
\begin{aligned}
h_q &amp;= \mathbf{W_q} \text{BERT}<em _text_retr="\text{retr">Q(q)[\text{CLS}] \
h_b &amp;= \mathbf{W_b} \text{BERT}_B(b)[\text{CLS}] \
S</em>(b, q) &amp;= h_q^\top h_b
\end{aligned}
$$}</p>
<p>where <strong>W_q</strong> and <strong>W_b</strong> are matrices that project the BERT output into 128-dimensional vectors.</p>
<p><strong>Reader component</strong> The reader is a span-based variant of the reading comprehension model proposed in Devlin et al. (2018):</p>
<p>$$
\begin{aligned}
h_{\text{start}} &amp;= \text{BERT}<em _text_end="\text{end">R(q, b)[\text{START}(s)] \
h</em>}} &amp;= \text{BERT<em _text_read="\text{read">R(q, b)[\text{END}(s)] \
S</em>])
\end{aligned}
$$}}(b, s, q) &amp;= \text{MLP}([h_{\text{start}}; h_{\text{end}</p>
<p>Following Lee et al. (2016), a span is represented by the concatenation of its end points, which is scored by a multi-layer perceptron to enable start/end interaction.</p>
<p><strong>Inference &amp; Learning Challenges</strong> The model described above is conceptually simple. However, inference and learning are challenging since (1) an</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Example</th>
<th style="text-align: left;">Supportive <br> Evidence</th>
<th style="text-align: left;">Spurious <br> Ambiguity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Q: Who is <br> credited with <br> developing the XY <br> coordinate plane? <br> A: René Descartes</td>
<td style="text-align: left;">...invention of <br> Cartesian <br> coordinates by <br> René Descartes <br> revolutionized...</td>
<td style="text-align: left;">...René <br> Descartes was <br> born in La Haye <br> en Touraine, <br> France...</td>
</tr>
<tr>
<td style="text-align: left;">Q: How many <br> districts are in the <br> state of Alabama? <br> A: seven</td>
<td style="text-align: left;">...Alabama is <br> currently divided <br> into seven <br> congressional <br> districts, each <br> represented by ...</td>
<td style="text-align: left;">...Alabama is <br> one of seven <br> states that levy a <br> tax on food at <br> the same rate as <br> other goods...</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of spurious ambiguities arising from the use of weak supervision. Good evidence retrieval is needed to generate a meaningful learning signal.
open evidence corpus presents an enormous search space (over 13 million evidence blocks), and (2) how to navigate this space is entirely latent, so standard teacher-forcing approaches do not apply. Latent-variable methods are also difficult to apply naively due to the large number of spuriously ambiguous derivations. For example, as shown in Table 2, many irrelevant passages in Wikipedia would contain the answer string "seven."</p>
<p>We address these challenges by carefully initializing the retriever with unsupervised pre-training (Section 4). The pre-trained retriever allows us to (1) pre-encode all evidence blocks from Wikipedia, enabling dynamic yet fast top-k retrieval during fine-tuning (Section 5), and (2) bias the retrieval away from spurious ambiguities and towards supportive evidence (Section 6).</p>
<h2>4 Inverse Cloze Task</h2>
<p>The goal of our proposed pre-training procedure is for the retriever to solve an unsupervised task that closely resembles evidence retrieval for QA.</p>
<p>Intuitively, useful evidence typically discusses entities, events, and relations from the question. It also contains extra information (the answer) that is not present in the question. An unsupervised analog of a question-evidence pair is a sentencecontext pair-the context of a sentence is semantically relevant and can be used to infer information missing from the sentence.</p>
<p>Following this intuition, we propose to pre-train our retrieval module with an Inverse Cloze Task (ICT). In the standard Cloze task (Taylor, 1953), the goal is to predict masked-out text based on its context. ICT instead requires predicting the inverse-given a sentence, predict its context (see
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of the Inverse Cloze Task (ICT), used for retrieval pre-training. A random sentence (pseudo-query) and its context (pseudo evidence text) are derived from the text snippet: "...Zebras have four gaits: walk, trot, canter and gallop. They are generally slower than horses, but their great stamina helps them outrun predators. When chased, a zebra will zigzag from side to side..." The objective is to select the true context among candidates in the batch.</p>
<p>Figure 2). We use a discriminative objective that is analogous to downstream retrieval:</p>
<p>$$
P_{\mathrm{ICT}}(b \mid q)=\frac{\exp \left(S_{\text {retr }}(b, q)\right)}{\sum_{b^{\prime} \in \mathrm{BATCH}} \exp \left(S_{\text {retr }}\left(b^{\prime}, q\right)\right)}
$$</p>
<p>where $q$ is a random sentence that is treated as a pseudo-question, $b$ is the text surrounding $q$, and BATCH is the set of evidence blocks in the batch that are used as sampled negatives.</p>
<p>An important aspect of ICT is that it requires learning more than word matching features, since the pseudo-question is not present in the evidence. For example, the pseudo-question in Figure 2 never explicitly mentions "Zebras", but the retriever must still be able to select the context that discusses Zebras. Being able to infer the semantics from under-specified language is what sets QA apart from traditional IR.</p>
<p>However, we also do not want to dissuade the retriever from learning to perform word matching-lexical overlap is ultimately a very useful feature for retrieval. Therefore, we only remove the sentence from its context in $90 \%$ of the examples, encouraging the model to learn both abstract representations when needed and low-level word matching features when available.</p>
<p>ICT pre-training accomplishes two main goals:</p>
<ol>
<li>Despite the mismatch between sentences dur-</li>
</ol>
<p>ing pre-training and questions during finetuning, we expect zero-shot evidence retrieval performance to be sufficient for bootstrapping the latent-variable learning.
2. There is no such mismatch between pretrained evidence blocks and downstream evidence blocks. We can expect the block encoder $\operatorname{BERT}_{B}(b)$ to work well without further training. Only the question encoder needs to be fine-tuned on downstream data.</p>
<p>As we will see in the following section, these two properties are crucial for enabling computationally feasible inference and end-to-end learning.</p>
<h2>5 Inference</h2>
<p>Since fixed block encoders already provide a useful representation for retrieval, we can precompute all block encodings in the evidence corpus. As a result, the enormous set of evidence blocks does not need to be re-encoded while finetuning, and it can be pre-compiled into an index for fast maximum inner product search using existing tools such as Locality Sensitive Hashing.</p>
<p>With the pre-compiled index, inference follows a standard beam-search procedure. We retrieve the top- $k$ evidence blocks and only compute the expensive reader scores for those $k$ blocks. While we only considering the top- $k$ evidence blocks during a single inference step, this set dynamically changes during training since the question encoder is fine-tuned according to the weakly supervised QA data, as discussed in the following section.</p>
<h2>6 Learning</h2>
<p>Learning is relatively straightforward, since ICT should provide non-trivial zero-shot retrieval. We first define a distribution over answer derivations:</p>
<p>$$
P(b, s \mid q)=\frac{\exp (S(b, s, q))}{\sum_{b^{\prime} \in \operatorname{TOP}(k)} \sum_{s^{\prime} \in b^{\prime}} \exp \left(S\left(b^{\prime}, s^{\prime}, q\right)\right)}
$$</p>
<p>where $\operatorname{TOP}(k)$ denotes the top $k$ retrieved blocks based on $S_{\text {retr }}$. We use $k=5$ in our experiments.</p>
<p>Given a gold answer string $a$, we find all (possibly spuriously) correct derivations in the beam, and optimize their marginal log-likelihood:</p>
<p>$$
L_{\text {full }}(q, a)=-\log \sum_{b \in \operatorname{TOP}(k)} \sum_{s \in b, a=\operatorname{TEXT}(s)} P^{\prime}(b, s \mid q)
$$</p>
<p>where $a=\operatorname{TEXT}(s)$ indicates whether the answer string $a$ matches exactly the span $s$.</p>
<p>To encourage more aggressive learning, we also include an early update, where we consider a larger set of $c$ evidence blocks but only update the retrieval score, which is cheap to compute:</p>
<p>$$
\begin{aligned}
P_{\text {early }}(b \mid q) &amp; =\frac{\exp \left(S_{\text {retr }}(b, q)\right)}{\sum_{b^{\prime} \in \operatorname{TOP}(c)} \exp \left(S_{\text {retr }}\left(b^{\prime}, q\right)\right)} \
L_{\text {early }}(q, a) &amp; =-\log \sum_{b \in \operatorname{TOP}(c), a \in \operatorname{TEXT}(b)} P_{\text {early }}(b \mid q)
\end{aligned}
$$</p>
<p>where $a \in \operatorname{TEXT}(b)$ indicates whether answer string $a$ appears in evidence block $b$. We use $c=5000$ in our experiments.</p>
<p>The final loss includes both updates:</p>
<p>$$
L(q, a)=L_{\text {early }}(q, a)+L_{\text {full }}(q, a)
$$</p>
<p>If no matching answers are found at all, then the example is discarded. While we would expect almost all examples to be discarded with random initialization, we discard less than $10 \%$ of examples in practice due to ICT pre-training.</p>
<p>As previously mentioned, we fine-tune all parameters except those in the evidence block encoder. Since the query encoder is trainable, the model can potentially learn to retrieve any evidence block. This expressivity is a crucial difference from blackbox IR systems, where recall can only be improved by retrieving more evidence.</p>
<h2>7 Experimental Setup</h2>
<h3>7.1 Open Domain QA Datasets</h3>
<p>We train and evaluate on data from 5 existing question answering or reading comprehension datasets. Not all of them are intended as open domain QA datasets in their original form, so we convert them to open formats, following DrQA (Chen et al., 2017). Each example in the open version of the datasets consists of a single question string and a set of reference answer strings.</p>
<p>Natural Questions contains question from aggregated queries to Google Search (Kwiatkowski et al., 2019). To gather an open version of this dataset, we only keep questions with short answers and discard the given evidence document. Answers with many tokens often resemble extractive snippets rather than canonical answers, so we discard answers with more than 5 tokens.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Dev</th>
<th style="text-align: left;">Test</th>
<th style="text-align: left;">Example Question</th>
<th style="text-align: left;">Example Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Natural Questions</td>
<td style="text-align: left;">79168</td>
<td style="text-align: left;">8757</td>
<td style="text-align: left;">3610</td>
<td style="text-align: left;">What does the zip in zip code stand for?</td>
<td style="text-align: left;">Zone Improvement Plan</td>
</tr>
<tr>
<td style="text-align: left;">WebQuestions</td>
<td style="text-align: left;">3417</td>
<td style="text-align: left;">361</td>
<td style="text-align: left;">2032</td>
<td style="text-align: left;">What airport is closer to downtown Houston?</td>
<td style="text-align: left;">William P. Hobby Airport</td>
</tr>
<tr>
<td style="text-align: left;">CuratedTrec</td>
<td style="text-align: left;">1353</td>
<td style="text-align: left;">133</td>
<td style="text-align: left;">694</td>
<td style="text-align: left;">What metal has the highest melting point?</td>
<td style="text-align: left;">Tungsten</td>
</tr>
<tr>
<td style="text-align: left;">TriviaQA</td>
<td style="text-align: left;">78785</td>
<td style="text-align: left;">8837</td>
<td style="text-align: left;">11313</td>
<td style="text-align: left;">What did L. Fran Baum, author of The Wonderful Wizard of Oz, call his home in Hollywood?</td>
<td style="text-align: left;">Ozcot</td>
</tr>
<tr>
<td style="text-align: left;">SQuAD</td>
<td style="text-align: left;">78713</td>
<td style="text-align: left;">8886</td>
<td style="text-align: left;">10570</td>
<td style="text-align: left;">Other than the Automobile Club of Southern <br> California, what other AAA Auto Club chose <br> to simplify the divide?</td>
<td style="text-align: left;">California State Automobile Association</td>
</tr>
</tbody>
</table>
<p>Table 3: Statistics and examples for the datasets that we evaluate on. There are slightly differences from the original datasets as described in Section 7.1, since not all of them were intended to be used in the open setting.</p>
<p>WebQuestions contains questions that were sampled from the Google Suggest API (Berant et al., 2013). The answers are annotated with respect to Freebase, but we only keep the string representation of the entities.</p>
<p>CuratedTrec is a corpus of question-answer pairs derived from TREC QA data curated by Baudis and Sedivý (2015). The questions come from various sources of real queries, such as MSNSearch or AskJeeves logs, where the question askers do not observe any evidence documents (Voorhees, 2001).</p>
<p>TriviaQA is a collection of trivia questionanswer pairs that were scraped from the web (Joshi et al., 2017). We use their unfiltered set and discard their distantly supervised evidence.</p>
<p>SQuAD was designed to be a reading comprehension dataset rather than an open domain QA dataset (Rajpurkar et al., 2016). Answer spans were selected from a Wikipedia paragraph, and the questions were written by annotators who were instructed to ask questions that are answered by a given answer in a given context.</p>
<p>On datasets where a development set does not exist, we randomly hold out $10 \%$ of the training data for development. On datasets where the test set is hidden, we also randomly hold out $10 \%$ of the training data for development, and use the original development set for testing (following DrQA). A summary of dataset statistics and examples are shown in Table 3.</p>
<h3>7.2 Dataset Biases</h3>
<p>Evaluating on this diverse set of question-answer pairs is crucial, because all existing datasets have inherent biases that are problematic for open domain QA systems with learned retrieval. These biases are summarized in Table 4.</p>
<p>In the Natural Questions, WebQuestions, and CuratedTrec, the question askers do not already know the answer. This accurately reflects a distribution of genuine information-seeking questions. However, annotators must separately find correct answers, which requires assistance from automatic tools and can introduce a moderate bias towards results from the tool.</p>
<p>In TriviaQA and SQuAD, automatic tools are not needed since the questions are written with known answers in mind. However, this introduces another set of biases that are arguably more problematic. Question writing is not motivated by an information need. This often results in many hints in the question that would not be present in naturally occurring questions, as shown in the examples in Table 3. This is particularly problematic for SQuAD, where the question askers are also prompted with a specific piece of evidence for the answer, leading to artificially large lexical overlap between the question and evidence.</p>
<p>Note that these are simply properties of the datasets rather than actionable criticisms-such data collection methods are necessary to scale up, and it is unclear how one could collect a truly unbiased dataset without impractical costs.</p>
<h3>7.3 Implementation Details</h3>
<p>We mainly evaluate in the setting where only question-answer string pairs are available for supervision. See Section 9 for head-to-head comparisons with the DrQA setting that uses the same evidence corpus and the same type of supervision.</p>
<p>Evidence Corpus We use the English Wikipedia snapshot from December 20, 2018 as the evidence corpus. ${ }^{1}$ The corpus is greedily</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Question <br> writer <br> knows <br> answer</th>
<th style="text-align: center;">Question <br> writer <br> knows <br> evidence</th>
<th style="text-align: center;">Tool- <br> assisted <br> answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Natural Questions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">WebQuestions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">CuratedTrec</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">TriviaQA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SQuAD</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: A breakdown of biases in existing QA datasets. These biases are associated with either the question or the answer.
split into chunks of at most 288 wordpieces based on BERT's tokenizer, while preserving sentence boundaries. This results in just over 13 million evidence blocks. The title of the document is included in the block encoder.</p>
<p>Hyperparameters In all uses of BERT (both the retriever and reader), we initialize from the uncased base model, which consists of 12 transformer layers with a hidden size of 768 .</p>
<p>As mentioned in Section 3, the retrieval representations, $h_{q}$ and $h_{b}$, have 128 dimensions. The small hidden size was chosen so that the final QA model can comfortably run on a single machine. We use the default optimizer from BERT.</p>
<p>When pre-training the retriever with ICT, we use a learning rate of $10^{-4}$ and a batch size of 4096 on Google Cloud TPUs for 100k steps. When finetuning, we use a learning rate of $10^{-5}$ and a batch size of 1 on a single machine with a 12 GB GPU. Answer spans are limited to 10 tokens. We perform 2 epochs of fine-tuning for the larger datasets (Natural Questions, TriviaQA, and SQuAD), and 20 epochs for the smaller datasets (WebQuestions and CuratedTrec).</p>
<h2>8 Main Results</h2>
<h3>8.1 Baselines</h3>
<p>We compare against other retrieval methods by using alternate retrieval scores $S_{\text {retr }}(b, q)$, but with the same reader.</p>
<p>BM25 A de-facto state-of-the-art unsupervised retrieval method is BM25 (Robertson et al., 2009). It has been shown to be robust for both traditional information retrieval tasks, and evidence retrieval for question answering (Yang et al., 2017). ${ }^{2}$ Since</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>BM25
+BERT
+BERT
+BERT
+BERT</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BM25 <br> +BERT</th>
<th style="text-align: center;">NNLM <br> +BERT</th>
<th style="text-align: center;">ELMO <br> +BERT</th>
<th style="text-align: center;">ORQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Natural Questions</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">31.3</td>
</tr>
<tr>
<td style="text-align: center;">WebQuestions</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: center;">CuratedTrec</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">36.8</td>
</tr>
<tr>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">26.5</td>
</tr>
<tr>
<td style="text-align: center;">Natural Questions</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;">WebQuestions</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: center;">CuratedTrec</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">20.2</td>
</tr>
</tbody>
</table>
<p>Table 5: Main results: End-to-end exact match for open-domain question answering from questionanswer pairs only. Datasets where question askers know the answer behave differently from datasets where they do not.</p>
<p>BM25 is not trainable, the retrieved evidence considered during fine-tuning is static. Inspired by BERTserini (Yang et al., 2019), the final score is a learned weighted sum of the BM25 and reader score. Our implementation is based on Lucene. ${ }^{3}$</p>
<p>Language Models While unsupervised neural retrieval is notoriously difficult to improve over traditional IR (Lin, 2019), we include them as baselines for comparison. We experiment with unsupervised pooled representations from neural language models (LM), which has been shown to be state-of-the-art unsupervised representations (Perone et al., 2018). We compare with two widely-used 128-dimensional representations: (1) NNLM, context-independent embeddings from a feed-forward LMs (Bengio et al., 2003), ${ }^{4}$ and (2) ELMO (small), a context-dependent bidirectional LSTM (Peters et al., 2018). ${ }^{5}$</p>
<p>As with ICT, we use the alternate encoders to pre-compute the encoded evidence blocks $h_{b}$ and to initialize the question encoding $h_{q}$, which is fine-tuned. Based on existing IR literature and the intuition that LMs do not explicitly optimize for retrieval, we do not expect these to be strong baselines, but they demonstrate the difficulty of encoding blocks of text into 128 dimensions.</p>
<h3>8.2 Results</h3>
<p>The main results are show in Table 5. The first result to note is that BM25 is a powerful retrieval system. Word matching is important, and</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Evidence <br> Retrieved</th>
<th style="text-align: center;">SQuAD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DrQA</td>
<td style="text-align: center;">5 documents</td>
<td style="text-align: center;">27.1</td>
</tr>
<tr>
<td style="text-align: left;">DrQA (DS)</td>
<td style="text-align: center;">5 documents</td>
<td style="text-align: center;">28.4</td>
</tr>
<tr>
<td style="text-align: left;">DrQA (DS + MTL)</td>
<td style="text-align: center;">5 documents</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: left;">BERTSERINI</td>
<td style="text-align: center;">5 documents</td>
<td style="text-align: center;">19.1</td>
</tr>
<tr>
<td style="text-align: left;">BERTSERINI</td>
<td style="text-align: center;">29 paragraphs</td>
<td style="text-align: center;">36.6</td>
</tr>
<tr>
<td style="text-align: left;">BERTSERINI</td>
<td style="text-align: center;">100 paragraphs</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: left;">BM25 + BERT <br> (gold deriv.)</td>
<td style="text-align: center;">5 blocks</td>
<td style="text-align: center;">34.7</td>
</tr>
</tbody>
</table>
<p>Table 6: Analysis: Results comparable to previous work in the strongly supervised setting, where models have access to gold derivations from SQuAD. Different systems segment Wikipedia differently. There are 5.1 M documents, 29.5 M paragraphs, and 12.1 M blocks in the December 12, 2016 Wikipedia snapshot.
dense vector representations derived from language models do not readily capture this.</p>
<p>We also show that on questions that were derived from real users who are seeking information (Natural Questions, WebQuestions, and CuratedTrec), our ICT pre-trained retriever outperforms BM25 by a large marge-6 to 19 points in exact match depending on the dataset.</p>
<p>However, in datasets where the question askers already know the answer, i.e. SQuAD and TriviaQA, the retrieval problem resembles traditional IR. In this setting, a highly compressed 128dimensional vector cannot match BM25's ability to precisely represent every word in the evidence.</p>
<p>The notable drop between development and test accuracy for SQuAD is a reflection of an artifact in the dataset-its 100k questions are derived from only 536 documents. Therefore, good retrieval targets are highly correlated between training examples, violating the IID assumption, and making it unsuitable for learned retrieval. We strongly suggest that those who are interested in end-to-end open-domain QA models no longer train and evaluate with SQuAD for this reason.</p>
<h2>9 Analysis</h2>
<h3>9.1 Strongly supervised comparison</h3>
<p>To verify that our BM25 baseline is indeed state of the art, we also provide direct comparisons with DrQA's setup, where systems have access to gold answer derivations from SQuAD (Rajpurkar et al., 2016). While many systems have been proposed following DrQA's original setting, we compare only to the original system and the best system that</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Analysis: Performance on our open version of the Natural Questions dev set with various masking rates for the ICT pre-training. Too much masking prevents the model from learning to exploit exact ngram overlap. Too little masking makes language understanding unnecessary.
we are aware of-BERTserini (Yang et al., 2019).
DrQA's reader is DocReader (Chen et al., 2017), and they use TF-IDF to retrieve the top $k$ documents. They also include distant supervision based on TF-IDF retrieval. BERTserini's reader is derived from base BERT (much like our reader), and they use BM25 to retrieve the top $k$ paragraphs (much like our BM25 baseline). A major difference is that BERTserini uses true paragraphs from Wikipedia rather than arbitrary blocks, resulting in more evidence blocks due to uneven lengths.</p>
<p>For fair comparison with these strongly supervised systems, we pre-train the reader on SQuAD data. ${ }^{6}$ In Table 6, our BM25 baseline, which retrieves 5 evidence blocks, greatly outperforms 5 -document BERTserini and is close to 29paragraph BERTserini.</p>
<h3>9.2 Masking Rate in the Inverse Cloze Task</h3>
<p>The pseudo-query is masked from the evidence block $90 \%$ of the time, motivated by intuition in Section 4. We empirically verify our intuitions in Figure 3 by varying the masking rate, and comparing results on our open version of the Natural Questions development set.</p>
<p>If we always mask the pseudo-query, the retriever never learns that n-gram overlap is a powerful retrieval signal, losing almost 10 points in end-to-end performance. If we never mask the pseudo-query, the problem is reduced to memorization and does not generalize well to question answering. The latter loses 6 points in end-to-end performance, which—perhaps not surprisinglyproduces near-identical results to BM25.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Example</th>
<th style="text-align: center;">ORQA</th>
<th style="text-align: center;">BM25 + BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q: what is the new <br> orleans saints symbol <br> called <br> A: fleur-de-lis</td>
<td style="text-align: center;">...The team's primary colors are old gold and <br> black; their logo is a simplified fleur-de-lis. <br> They played their home games in Tulane <br> Stadium through the 1974 NFL season....</td>
<td style="text-align: center;">...the SkyDome was owned by Sportsco at the <br> time... the sale of the New Orleans Saints with <br> team owner Tom Benson... the Saints became a <br> symbol for that community...</td>
</tr>
<tr>
<td style="text-align: center;">Q: how many senators <br> per state in the us <br> A: two</td>
<td style="text-align: center;">...powers of the Senate are established in <br> Article One of the U.S. Constitution. Each <br> U.S. state is represented by two senators...</td>
<td style="text-align: center;">...The Georgia Constitution mandates a <br> maximum of 56 senators, elected from <br> single-member districts...</td>
</tr>
<tr>
<td style="text-align: center;">Q: when was germany <br> given a permanent seat <br> on the council of the <br> league of nations <br> A: 1926</td>
<td style="text-align: center;">...Under the Weimar Republic, Germany (in <br> fact the "Deutsches Reich" or German Empire) <br> was admitted to the League of Nations through <br> a resolution passed on September 8 1926. An <br> additional 15 countries joined later...</td>
<td style="text-align: center;">...the accession of the German Democratic <br> Republic to the Federal Republic of Germany, <br> it was effective on 3 October 1990...Germany <br> has been elected as a non-permanent member <br> of the United Nations Security Council...</td>
</tr>
<tr>
<td style="text-align: center;">Q: when was diary of <br> a wimpy kid double <br> down published <br> A: November 1, 2016</td>
<td style="text-align: center;">..."Diary of a Wimpy Kid" first appeared on <br> FunBrain in 2004, where it was read 20 million <br> times. The abridged hardcover adaptation was <br> released on April 1, 2007...</td>
<td style="text-align: center;">Diary of a Wimpy Kid: Double Down is the <br> eleventh book in the "Diary of a Wimpy Kid" <br> series by Jeff Kinney... The book was <br> published on November 1, 2016...</td>
</tr>
</tbody>
</table>
<p>Table 7: Analysis: Example predictions on our open version of the Natural Questions dev set. We show the highest scoring derivation, consisting of the evidence block and the predicted answer in bold. ORQA is more robust at separating semantically distinct text that have high lexical overlap. However, the limitation of the 128-dimensional vectors is that extremely specific concepts are less precisely represented.</p>
<h3>9.3 Example Predictions</h3>
<p>For a more intuitive understanding of the improvements from ORQA, we compare its predictions with baseline predictions in Table 7. We find that ORQA is more robust at separating semantically distinct text with high lexical overlap, as shown in the first three examples. However, it is expected that there are limits to how much information can be compressed into 128-dimensional vectors. The last example shows that ORQA has trouble precisely representing extremely specific concepts that sparse representations can cleanly separate. These errors indicate that a hybrid approach would be promising future work.</p>
<h2>10 Related Work</h2>
<p>Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work.</p>
<p>Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges-(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases
are needed to find positive learning signal while avoiding spurious ambiguities.</p>
<p>While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with a coarser granularity, deep architecture, and in-batch negative sampling from Logeswaran and Lee (2018).</p>
<p>Consulting external evidence sources with latent retrieval has also been explored in information extraction (Narasimhan et al., 2016). In comparison, we are able to learn a much more expressive retriever due to the strong inductive biases from ICT pre-training.</p>
<h2>11 Conclusion</h2>
<p>We presented ORQA, the first open domain question answering system where the retriever and reader are jointly learned end-to-end using only question-answer pairs and without any IR system. This is made possible by pre-training the retriever using an Inverse Cloze Task (ICT). Experiments show that learning to retrieve is crucial when the questions reflect an information need, i.e. the question writers do not already know the answer.</p>
<h2>Acknowledgements</h2>
<p>We thank the Google AI Language Team for valuable suggestions and feedback.</p>
<h2>References</h2>
<p>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49-62.</p>
<p>Petr Baudis and Jan Sedivý. 2015. Modeling of the question answering task in the yodaqa system. In CLEF.</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137-1155.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533-1544.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1870-1879.</p>
<p>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world's response. In Proceedings of the fourteenth conference on computational natural language learning, pages 18-27. Association for Computational Linguistics.</p>
<p>Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. 2019. Multi-step retrieverreader interaction for scalable open-domain question answering. In International Conference on Learning Representations.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. 2017. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904.</p>
<p>Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q\&amp;a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179.</p>
<p>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 11561165. ACM.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1601-1611.</p>
<p>Bernhard Kratzwald and Stefan Feuerriegel. 2018. Adaptive document retrieval for deep question answering. arXiv preprint arXiv:1808.06528.</p>
<p>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1545-1556.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Rhinehart, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics.</p>
<p>Jinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung Ko, and Jaewoo Kang. 2018. Ranking paragraphs for improving answer recall in open-domain question answering. arXiv preprint arXiv:1810.00494.</p>
<p>Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur Parikh, Dipanjan Das, and Jonathan Berant. 2016. Learning recurrent span representations for extractive question answering. arXiv preprint arXiv:1611.01436.</p>
<p>Percy Liang, Michael I Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389-446.</p>
<p>Jimmy Lin. 2019. The neural hype and comparisons against weak baselines. In ACM SIGIR Forum.</p>
<p>Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893.</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>Karthik Narasimhan, Adam Yala, and Regina Barzilay. 2016. Improving information extraction by acquiring external evidence with reinforcement learning. arXiv preprint arXiv:1603.07954.</p>
<p>Christian S Perone, Roberto Silveira, and Thomas S Paula. 2018. Evaluation of sentence embeddings in downstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proc. of NAACL.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333-389.</p>
<p>Amit Singh. 2012. Entity based q\&amp;a retrieval. In Proceedings of the 2012 Joint conference on empirical methods in natural language processing and computational natural language learning, pages 12661277. Association for Computational Linguistics.</p>
<p>Wilson L Taylor. 1953. "Cloze procedure": A new tool for measuring readability. Journalism Bulletin, 30(4):415-433.</p>
<p>Ellen M Voorhees. 2001. Overview of the trec 2001 question answering track. In In Proceedings of the Tenth Text REtrieval Conference (TREC. Citeseer.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018. R 3: Reinforced ranker-reader for open-domain question answering. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of lucene for information retrieval research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1253-1256. ACM.</p>
<p>Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with bertserini. arXiv preprint arXiv:1902.01718.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We use DrQA's December 12, 2016 snapshot of Wikipedia for an apples-to-apples comparison.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://lucene.apache.org/
${ }^{4}$ https://tffub.dev/google/nnlm-en-dim128/1
${ }^{5}$ https://allennlp.org/elmc&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
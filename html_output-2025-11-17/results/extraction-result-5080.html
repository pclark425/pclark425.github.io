<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5080 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5080</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5080</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4be02694125b71876552900a53c85c47a2a83614</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4be02694125b71876552900a53c85c47a2a83614" target="_blank">CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks</a></p>
                <p><strong>Paper Venue:</strong> IEEE Robotics and Automation Letters</p>
                <p><strong>Paper TL;DR:</strong> CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks, is presented, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.</p>
                <p><strong>Paper Abstract:</strong> General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this letter, we present Composing Actions from Language and Vision (CALVIN) (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zero-shot to novel language instructions and to novel environments. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5080.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5080.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniLM (Deep self-attention distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled transformer-based language encoder that maps sentences to 384-dimensional embeddings; used in CALVIN to represent natural language instructions that condition robot policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A distilled Transformer language encoder trained via deep self-attention distillation; in CALVIN precomputed sentence embeddings of size 384 are provided. The vocabulary size reported in the paper is 30,522 tokens; embeddings are produced from a pretrained MiniLM model (no further training reported inside CALVIN).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CALVIN manipulation tasks (long-horizon spatial robot manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A suite of 34 robot manipulation subtasks (e.g., push left/right, place in drawer/slider, stack/unstack blocks, open/close drawer, move slider) and long-horizon sequences of such subtasks; tasks demand spatial perception and spatial action sequencing (object positions, relative motion, placement, rotations).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>MiniLM is used as a frozen sentence encoder: natural-language instructions are mapped to 384-d vectors (precomputed) and supplied as the language-conditioning input to a language-conditioned policy (baseline: multi-context imitation learning, MCIL). The overall approach: (1) use teleoperated play data relabeled into short goal windows, (2) pair ~1% of windows with language annotations whose embeddings come from MiniLM, (3) train a latent goal-conditioned policy (seq2seq CVAE style) conditioned on perception + the MiniLM embedding to output continuous low-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Indirect evidence only: policies conditioned on MiniLM embeddings can accomplish spatial tasks that require perceiving and manipulating objects in space (examples include placing a block inside a drawer, pushing a block 10+ cm left/right, stacking blocks). The paper contains task definitions and environment-state success checks (Fig. 6 / Fig. 9) showing that successful rollouts correspond to correct spatial changes. The paper does not present probing, ablations, or internal-embedding visualizations that directly demonstrate that MiniLM encodes spatial relations or that the language encoder performs explicit spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For the MCIL baseline conditioned on language embeddings (MiniLM) and using static-camera images, the Multi-Task Language Control (MTLC) success rate reported is 53.9% when training and testing in the same environment. The paper reports that performance remains comparable when adding gripper camera, depth, or tactile sensing for MTLC; long-horizon MTLC (LH-MTLC) success is reported as poor for this baseline but no single aggregate percentage for LH-MTLC is provided in the text (described qualitatively as 'performs poorly in the long-horizon setting'). Dataset size/context: ~24 hours of teleoperated play (~2.4M interaction steps) with only ~1% of sequences labeled with language (389 unique language instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations reported/observable in paper: (1) MiniLM embeddings are provided precomputed and used as-is (no evidence of finetuning or spatial grounding of the language encoder), so there is no demonstration that the language model itself learns spatial relations; (2) language supervision is sparse (only ~1% of data labeled), which limits how much instruction-level grounding can be learned; (3) baseline (MCIL + MiniLM) achieves only moderate short-horizon success and performs poorly on long-horizon instruction chains, indicating failures in compositional/temporal planning; (4) no ablation isolating the contribution of the language encoder itself (e.g., alternative encoders, probing) is presented; (5) no explicit mechanisms are used to model 3D spatial structure inside the language encoder—spatial reasoning is left to the multimodal policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons in the paper are primarily between sensor combinations and the MCIL baseline variants (static camera vs. gripper camera vs. depth vs. tactile): adding extra sensors did not substantially improve MTLC success beyond 53.9%. The CALVIN benchmark and baseline are compared in related work to ALFRED and to Lynch et al. (MCIL) setups; the paper states that the MCIL baseline (with language conditioning via embeddings) is not effective for long-horizon CALVIN tasks, leaving room for improved methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5080.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5080.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Composing pick-and-place by grounding language</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Composing pick-and-place tasks by grounding language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited related work that addresses grounding of language to pick-and-place behaviors (spatial grounding) for robotic manipulation; cited as prior work on grounding spatial relations for pick-and-place.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Composing pick-and-place tasks by grounding language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not detailed in the CALVIN paper; referenced as prior work on grounding language to manipulation skills (pick-and-place). See the cited paper for model specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Pick-and-place tasks / spatial grounding (robot manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Tasks that require recognizing objects and spatial relations from language (e.g., 'put X on Y', 'place inside drawer'), i.e., spatially grounded pick-and-place behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Mentioned as work that grounds language to manipulation skills; mechanism details are not provided in CALVIN (only citation).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Only referenced; CALVIN does not report details or analyses from this paper. The reference is used to motivate grounding language to spatial actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work context as prior approaches to grounding spatial relations; no direct quantitative comparison in CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5080.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5080.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial reasoning (ICRA 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial reasoning from natural language instructions for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited ICRA 2021 paper that explicitly addresses spatial reasoning from natural language for robot manipulation; referenced as related work on spatial relation grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spatial reasoning from natural language instructions for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not described in the CALVIN paper; the CALVIN authors cite this work as an example of spatial-relation grounding research. See the original ICRA paper for model/algorithmic details.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Robot manipulation tasks requiring spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Natural-language-specified manipulation tasks that require understanding spatial relations and converting them into robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Referenced as prior art on spatial reasoning from language; CALVIN does not summarize the model or method details.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Only referenced; CALVIN does not include the analyses or evidence from that work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as related work; no direct experimental comparison in CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5080.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5080.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited arXiv work that studies learning spatial structure for language-guided rearrangement; referenced by CALVIN as related research on spatial structure learning in language-conditioned manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified within the CALVIN paper. The paper is cited as related work on learning spatial structure for language-guided rearrangement.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Language-guided semantic rearrangement (spatial rearrangement tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Tasks that require reasoning about spatial layout and structure to rearrange objects according to language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Cited as prior work exploring spatial structure learning; CALVIN does not describe the model or strategies used in StructFormer.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Referenced only; no internal evidence or analysis from StructFormer is reported in CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work as relevant to spatial-language grounding; no direct comparisons reported in CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5080.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5080.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning transferable visual models from natural language supervision (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced vision-and-language model (CLIP) in related work; a contrastive vision-language pretraining approach that maps images and text into a shared embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (contrastive image-text model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not used in CALVIN experiments; referenced in the bibliography as a prominent vision-and-language model that learns joint image-text representations via contrastive pretraining. Model specifics are not described in CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Not a puzzle per se; referenced as a general vision-language model applicable to grounding tasks</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Potentially useful for grounding language to visual observations in spatial tasks, but not evaluated in CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Referenced for context; CLIP operates via contrastive learning to align image and text embeddings but CALVIN does not use it experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No evidence within CALVIN; CLIP is only cited in the references section and not applied to the CALVIN benchmark in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in references as an example of language-vision pretraining; no experimental comparison in CALVIN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spatial reasoning from natural language instructions for robot manipulation <em>(Rating: 2)</em></li>
                <li>Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects <em>(Rating: 2)</em></li>
                <li>Composing pick-and-place tasks by grounding language <em>(Rating: 2)</em></li>
                <li>Language conditioned imitation learning over unstructured data <em>(Rating: 2)</em></li>
                <li>Minilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers <em>(Rating: 2)</em></li>
                <li>Alfred: A benchmark for interpreting grounded instructions for everyday tasks <em>(Rating: 1)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5080",
    "paper_id": "paper-4be02694125b71876552900a53c85c47a2a83614",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "MiniLM",
            "name_full": "MiniLM (Deep self-attention distillation)",
            "brief_description": "A distilled transformer-based language encoder that maps sentences to 384-dimensional embeddings; used in CALVIN to represent natural language instructions that condition robot policies.",
            "citation_title": "Minilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers",
            "mention_or_use": "use",
            "model_name": "MiniLM",
            "model_description": "A distilled Transformer language encoder trained via deep self-attention distillation; in CALVIN precomputed sentence embeddings of size 384 are provided. The vocabulary size reported in the paper is 30,522 tokens; embeddings are produced from a pretrained MiniLM model (no further training reported inside CALVIN).",
            "puzzle_name": "CALVIN manipulation tasks (long-horizon spatial robot manipulation)",
            "puzzle_description": "A suite of 34 robot manipulation subtasks (e.g., push left/right, place in drawer/slider, stack/unstack blocks, open/close drawer, move slider) and long-horizon sequences of such subtasks; tasks demand spatial perception and spatial action sequencing (object positions, relative motion, placement, rotations).",
            "mechanism_or_strategy": "MiniLM is used as a frozen sentence encoder: natural-language instructions are mapped to 384-d vectors (precomputed) and supplied as the language-conditioning input to a language-conditioned policy (baseline: multi-context imitation learning, MCIL). The overall approach: (1) use teleoperated play data relabeled into short goal windows, (2) pair ~1% of windows with language annotations whose embeddings come from MiniLM, (3) train a latent goal-conditioned policy (seq2seq CVAE style) conditioned on perception + the MiniLM embedding to output continuous low-level actions.",
            "evidence_of_spatial_reasoning": "Indirect evidence only: policies conditioned on MiniLM embeddings can accomplish spatial tasks that require perceiving and manipulating objects in space (examples include placing a block inside a drawer, pushing a block 10+ cm left/right, stacking blocks). The paper contains task definitions and environment-state success checks (Fig. 6 / Fig. 9) showing that successful rollouts correspond to correct spatial changes. The paper does not present probing, ablations, or internal-embedding visualizations that directly demonstrate that MiniLM encodes spatial relations or that the language encoder performs explicit spatial reasoning.",
            "performance_metrics": "For the MCIL baseline conditioned on language embeddings (MiniLM) and using static-camera images, the Multi-Task Language Control (MTLC) success rate reported is 53.9% when training and testing in the same environment. The paper reports that performance remains comparable when adding gripper camera, depth, or tactile sensing for MTLC; long-horizon MTLC (LH-MTLC) success is reported as poor for this baseline but no single aggregate percentage for LH-MTLC is provided in the text (described qualitatively as 'performs poorly in the long-horizon setting'). Dataset size/context: ~24 hours of teleoperated play (~2.4M interaction steps) with only ~1% of sequences labeled with language (389 unique language instructions).",
            "limitations_or_failure_cases": "Limitations reported/observable in paper: (1) MiniLM embeddings are provided precomputed and used as-is (no evidence of finetuning or spatial grounding of the language encoder), so there is no demonstration that the language model itself learns spatial relations; (2) language supervision is sparse (only ~1% of data labeled), which limits how much instruction-level grounding can be learned; (3) baseline (MCIL + MiniLM) achieves only moderate short-horizon success and performs poorly on long-horizon instruction chains, indicating failures in compositional/temporal planning; (4) no ablation isolating the contribution of the language encoder itself (e.g., alternative encoders, probing) is presented; (5) no explicit mechanisms are used to model 3D spatial structure inside the language encoder—spatial reasoning is left to the multimodal policy.",
            "comparison_baseline": "Comparisons in the paper are primarily between sensor combinations and the MCIL baseline variants (static camera vs. gripper camera vs. depth vs. tactile): adding extra sensors did not substantially improve MTLC success beyond 53.9%. The CALVIN benchmark and baseline are compared in related work to ALFRED and to Lynch et al. (MCIL) setups; the paper states that the MCIL baseline (with language conditioning via embeddings) is not effective for long-horizon CALVIN tasks, leaving room for improved methods.",
            "uuid": "e5080.0",
            "source_info": {
                "paper_title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Composing pick-and-place by grounding language",
            "name_full": "Composing pick-and-place tasks by grounding language",
            "brief_description": "A cited related work that addresses grounding of language to pick-and-place behaviors (spatial grounding) for robotic manipulation; cited as prior work on grounding spatial relations for pick-and-place.",
            "citation_title": "Composing pick-and-place tasks by grounding language",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Not detailed in the CALVIN paper; referenced as prior work on grounding language to manipulation skills (pick-and-place). See the cited paper for model specifics.",
            "puzzle_name": "Pick-and-place tasks / spatial grounding (robot manipulation)",
            "puzzle_description": "Tasks that require recognizing objects and spatial relations from language (e.g., 'put X on Y', 'place inside drawer'), i.e., spatially grounded pick-and-place behaviors.",
            "mechanism_or_strategy": "Mentioned as work that grounds language to manipulation skills; mechanism details are not provided in CALVIN (only citation).",
            "evidence_of_spatial_reasoning": "Only referenced; CALVIN does not report details or analyses from this paper. The reference is used to motivate grounding language to spatial actions.",
            "performance_metrics": "",
            "limitations_or_failure_cases": "",
            "comparison_baseline": "Mentioned in related work context as prior approaches to grounding spatial relations; no direct quantitative comparison in CALVIN.",
            "uuid": "e5080.1",
            "source_info": {
                "paper_title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Spatial reasoning (ICRA 2021)",
            "name_full": "Spatial reasoning from natural language instructions for robot manipulation",
            "brief_description": "A cited ICRA 2021 paper that explicitly addresses spatial reasoning from natural language for robot manipulation; referenced as related work on spatial relation grounding.",
            "citation_title": "Spatial reasoning from natural language instructions for robot manipulation",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Not described in the CALVIN paper; the CALVIN authors cite this work as an example of spatial-relation grounding research. See the original ICRA paper for model/algorithmic details.",
            "puzzle_name": "Robot manipulation tasks requiring spatial reasoning",
            "puzzle_description": "Natural-language-specified manipulation tasks that require understanding spatial relations and converting them into robot actions.",
            "mechanism_or_strategy": "Referenced as prior art on spatial reasoning from language; CALVIN does not summarize the model or method details.",
            "evidence_of_spatial_reasoning": "Only referenced; CALVIN does not include the analyses or evidence from that work.",
            "performance_metrics": "",
            "limitations_or_failure_cases": "",
            "comparison_baseline": "Mentioned as related work; no direct experimental comparison in CALVIN.",
            "uuid": "e5080.2",
            "source_info": {
                "paper_title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "StructFormer",
            "name_full": "Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects",
            "brief_description": "A cited arXiv work that studies learning spatial structure for language-guided rearrangement; referenced by CALVIN as related research on spatial structure learning in language-conditioned manipulation.",
            "citation_title": "Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Not specified within the CALVIN paper. The paper is cited as related work on learning spatial structure for language-guided rearrangement.",
            "puzzle_name": "Language-guided semantic rearrangement (spatial rearrangement tasks)",
            "puzzle_description": "Tasks that require reasoning about spatial layout and structure to rearrange objects according to language instructions.",
            "mechanism_or_strategy": "Cited as prior work exploring spatial structure learning; CALVIN does not describe the model or strategies used in StructFormer.",
            "evidence_of_spatial_reasoning": "Referenced only; no internal evidence or analysis from StructFormer is reported in CALVIN.",
            "performance_metrics": "",
            "limitations_or_failure_cases": "",
            "comparison_baseline": "Mentioned in related work as relevant to spatial-language grounding; no direct comparisons reported in CALVIN.",
            "uuid": "e5080.3",
            "source_info": {
                "paper_title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "CLIP (ref)",
            "name_full": "Learning transferable visual models from natural language supervision (CLIP)",
            "brief_description": "Referenced vision-and-language model (CLIP) in related work; a contrastive vision-language pretraining approach that maps images and text into a shared embedding space.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "mention",
            "model_name": "CLIP (contrastive image-text model)",
            "model_description": "Not used in CALVIN experiments; referenced in the bibliography as a prominent vision-and-language model that learns joint image-text representations via contrastive pretraining. Model specifics are not described in CALVIN.",
            "puzzle_name": "Not a puzzle per se; referenced as a general vision-language model applicable to grounding tasks",
            "puzzle_description": "Potentially useful for grounding language to visual observations in spatial tasks, but not evaluated in CALVIN.",
            "mechanism_or_strategy": "Referenced for context; CLIP operates via contrastive learning to align image and text embeddings but CALVIN does not use it experimentally.",
            "evidence_of_spatial_reasoning": "No evidence within CALVIN; CLIP is only cited in the references section and not applied to the CALVIN benchmark in this paper.",
            "performance_metrics": "",
            "limitations_or_failure_cases": "",
            "comparison_baseline": "Mentioned in references as an example of language-vision pretraining; no experimental comparison in CALVIN.",
            "uuid": "e5080.4",
            "source_info": {
                "paper_title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spatial reasoning from natural language instructions for robot manipulation",
            "rating": 2
        },
        {
            "paper_title": "Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects",
            "rating": 2
        },
        {
            "paper_title": "Composing pick-and-place tasks by grounding language",
            "rating": 2
        },
        {
            "paper_title": "Language conditioned imitation learning over unstructured data",
            "rating": 2
        },
        {
            "paper_title": "Minilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers",
            "rating": 2
        },
        {
            "paper_title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks",
            "rating": 1
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 1
        }
    ],
    "cost": 0.0129365,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks</h1>
<p>Oier Mees<em>1, Lukas Hermann</em>1, Erick Rosete-Beas ${ }^{1}$, Wolfram Burgard ${ }^{2}$ http://calvin.cs.uni-freiburg.de</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: CALVIN is a benchmark to learn many long-horizon language-conditioned tasks over a range of four manipulation environments, designed to be diverse yet carry shared structure, from multimodal onboard sensor observations. In the most difficult evaluation, the methods must generalize to unseen entities by training on a large interaction corpora covering three environments and testing on an unseen scene.</p>
<h4>Abstract</h4>
<p>General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn longhorizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zeroshot to novel language instructions and to novel environments. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.</p>
<p>Manuscript received: February, 23, 2022; Accepted May, 22, 2022.
This paper was recommended for publication by Associate Editor S. Chernova and Editor D. Kulic upon evaluation of the reviewers' comments.
*Equal contribution. ${ }^{1}$ University of Freiburg, Germany. ${ }^{2}$ University of Technology Nuremberg, Germany. meeso@informatik.uni-freiburg.de Digital Object Identifier (DOI): see top of this page.
(c) 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</p>
<p>Index Terms—Data Sets for Robot Learning, Machine Learning for Robot Control, Imitation Learning, Natural Dialog for HRI</p>
<h2>I. INTRODUCTION</h2>
<p>ALONG-STANDING goal for robotics and embodied agents is to build systems that can perform tasks specified in natural language. Concepts expressed in natural language provide humans with an intuitive way to represent, summarize, and abstract diverse knowledge skills. By means of abstraction, concepts such as "open the drawer and push the middle object into the drawer" can be extended to a potentially infinite set of new and unseen entities. Additionally, humans leverage concepts to describe complex tasks as sequences of natural language instructions. This stands in contrast to current robots, which typically lack this generalization ability and learn individual tasks one at a time. Moreover, multi-task learning approaches traditionally assume that tasks are specified to the agent at test time via mechanisms such as goal images [1] and one-hot skill selectors [2], [3] that are not practical for non-expert users to instruct robots in everyday real-world settings. As robots become ubiquitous across human-centered environments the need for intuitive task specification grows: how can we scale robot learning systems to autonomously acquire general-purpose knowledge that allows them to compose long-horizon tasks by following unconstrained language instructions?</p>
<p>To address this problem we present CALVIN, a new opensource simulated benchmark that links human language to robot motor skills, behaviors, and objects in interactive visual environments. In this setting, a single agent must solve complex manipulation tasks by understanding a series of unconstrained language expressions in a row, e.g., "open the drawer ...pick up the blue block ...push the block into the drawer ...open the sliding door". Furthermore, to evaluate the agents' ability for long-horizon planning, agents in this scenario are expected to be able to perform any combination of subtasks in any order. CALVIN has been developed from the ground up to support training, prototyping, and validation of language-conditioned continuous control policies over a range of four indoor manipulation environments, visualized in Figure 1. CALVIN includes $\sim 24$ hours teleoperated unstructured play data together with 20 K language directives. Unscripted playful interactions have the advantage of being task-agnostic, diverse, and relatively cheap to obtain [1], [4]. The simulation platform supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing [5]. We believe that this flexible sensor suite will allow researchers to develop improved multimodal agents that can solve many tasks in real-world settings. This is the first public benchmark of instruction following, to our knowledge, that combines: natural language conditioning, multimodal highdimensional inputs, 7-DOF continuous control, and longhorizon robotic object manipulation. We provide an evaluation protocol with evaluation modes of varying difficulty by choosing different combinations of sensor suites and amounts of training environments. This effort joins the recent efforts to standardize robotics research for better benchmarks and more reproducible results. To open the door for future development of agents that can generalize abstract concepts to unseen entities the same way humans do, we include a challenging zeroshot evaluation by training on large play corpora covering three environments and testing on an unseen scene. The language instructions used for testing are not included in the training set and represent novel ways of describing the manipulation tasks seen during training.</p>
<p>To establish baseline performance levels, we evaluate the multi-context imitation learning (MCIL) approach that uses relabeled imitation learning to distill many reusable behaviors into a goal-directed policy [6]. This model is not effective on the complex long horizon robot manipulation tasks in CALVIN. While it achieves up to $53.9 \%$ success rate in short horizon tasks, it performs poorly in the long-horizon setting. We note that there is no constraint to use imitation learning approaches to solve CALVIN tasks, as approaches that use reinforcement learning to learn language-conditioned policies can also be applied [7].</p>
<p>In summary, CALVIN facilitates learning models that translate from language to sequences of motor skills in a realistic simulation environment. This benchmark captures many challenges present in real-world settings for relating human language to robot actions and perception for accomplishing long-horizon manipulation tasks. Models that can overcome
these challenges will begin to close the gap towards scalable, general-purpose, language-driven robotics.</p>
<h2>II. Related Work</h2>
<p>Natural language processing has recently received much attention in the field of robotics [8], following the advances made towards learning groundings between vision and language [9], [10]. Recent successes in human-robot interaction include an interactive fetching system to localize objects mentioned in referring expressions [11]-[15] or grounding not only objects, but also spatial relations to follow language expressions characterizing pick-and-place commands [16]-[18]. By contrast, CALVIN tasks require grounding language to a wide variety of general-purpose robot skills. Prior work on mapping language and vision to actions has been studied mostly in restricted environments [19], [20] and simplified actuators with discrete motion primitives [21]-[23]. A growing body of work also looks at learning language-conditioned policies for continuous visuomotor-control in 3D environments via imitation learning [6], [24], [25] or reinforcement learning [7], [26], [27]. These approaches typically require offline data sources of robotic interaction, such as teleoperation or autonomous exploration data, together with post-hoc crowd-sourced language labels. However, the lack of standardized benchmarks and algorithm implementations, makes it difficult to compare approaches and to facilitate future research.</p>
<p>The most closely related benchmark to ours is ALFRED [22], which contains language instructions for combined navigation and manipulation tasks with seven predefined action primitives. In CALVIN, rather than classifying predefined actions, the agent must learn to acquire a diverse repertoire of general-purpose skills that allows composing long-horizon tasks by following unconstrained language instructions in closed loop control. Our tabletop environments are inspired by the one shown in Lynch et al. [6] in order to have a fair comparison to their MCIL approach, which we implement to establish baseline performance levels. We note that although considered a state-of-the-art approach, no public implementation of MCIL is available. In contrast to their work, CALVIN contains more subtasks ( 34 vs 18), longer longhorizon evaluation sequences ( 5 vs 4 ), provides a range of sensors commonly utilized for visuomotor control and allows testing zero-shot generalization by leveraging a range of four manipulation environments and unseen language instructions. Finally, CALVIN goes beyond the original MCIL setup by adding a challenging visual grounding problem, where similar language instructions for differently colored blocks are given and the agent needs to identify which block is meant.</p>
<h2>III. CALVIN</h2>
<p>The aim of the CALVIN benchmark is to evaluate the learning of long-horizon language-conditioned continuous control policies. In this setting, a single agent must solve complex manipulation tasks by understanding a series of unconstrained language expressions in a row, e.g., "open the drawer...pick up the blue block...now push the block into the drawer...now open the sliding door". We note that in the benchmark we</p>
<table>
<thead>
<tr>
<th>Observation Space</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>RGB static camera</td>
<td>$200 \times 200 \times 3$</td>
<td></td>
</tr>
<tr>
<td>Depth static camera</td>
<td>$200 \times 200$</td>
<td></td>
</tr>
<tr>
<td>RGB gripper camera</td>
<td>$84 \times 84 \times 3$</td>
<td></td>
</tr>
<tr>
<td>Depth gripper camera</td>
<td>$84 \times 84$</td>
<td></td>
</tr>
<tr>
<td>Tactile image</td>
<td>$120 \times 160 \times 2$</td>
<td></td>
</tr>
<tr>
<td>Proprioceptive state</td>
<td>EE position (3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>EE orientation (3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Gripper width (1)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Joint positions (7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Gripper action (1)</td>
<td></td>
</tr>
<tr>
<td>Action Space</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Absolute cartesian pose</td>
<td>EE position (3)</td>
<td></td>
</tr>
<tr>
<td>(w.r.t. world frame)</td>
<td>EE orientation (3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Gripper action (1)</td>
<td></td>
</tr>
<tr>
<td>Relative cartesian displacement</td>
<td>EE position (3)</td>
<td></td>
</tr>
<tr>
<td>(w.r.t. gripper frame)</td>
<td>EE orientation (3)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Gripper action (1)</td>
<td></td>
</tr>
<tr>
<td>Joint action</td>
<td>Joint positions (7)</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Gripper action (1)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Fig. 2: Observation and action spaces supported by CALVIN.
only allow feasible sequences that can be achieved from a predefined initial environment state. The CALVIN benchmark consists of three key components, which are:</p>
<p>1) CALVIN Environment
2) CALVIN Dataset
3) CALVIN Challenge</p>
<h2>A. The CALVIN Environment</h2>
<p>CALVIN features four different, yet structurally related environments (A, B, C, D) so that it can be used for general playing as well as evaluating specific tasks. The environments contain a 7-DOF Franka Emika Panda robot arm with a parallel gripper and a desk with a sliding door and a drawer that can be opened and closed. On the desk, there is a button that toggles a green light and a switch to control a light bulb. Besides, there are three different colored and shaped rectangular blocks. To better evaluate the generalization capabilities of the learned language groundings, all environments have different textures and all static elements such as the sliding door, the drawer, the light button, and switch are positioned differently. The position of the desk, robot, and the static camera is the same in all environments. Due to the general difficulty of languageconditioned multi-task closed-loop control, we reduced the complexity of the objects to unicolored primitive shapes. If future advances in this field require new challenges we will reflect this by extending CALVIN to environments with more realistic and diverse objects. Physics are simulated using the PyBullet physics engine [28], which supports fast GPU rendering for large-scale parallel data collection.</p>
<p>1) Observation and Action Space: Unlike prior work which relies on RGB images from an egocentric camera to perceive its surroundings [1], [6], CALVIN offers a range of sensors that can be used to develop and prototype agents that learn task-agnostic control in the real world. Concretely, the agent
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: CALVIN supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing (bottom-left).
perceives its surroundings from RGB-D images from both a fixed and a gripper camera. It additionally has access to a vision-based tactile sensor [5] and to continuous internal proprioceptive sensors. A visualization of the supported sensor modalities is shown in Figure 3. The agent must perform closed-loop continuous control to follow unconstrained language instructions characterizing complex robot manipulation tasks, sending continuous actions to the robot at 30hz. In order to give researchers and practitioners the freedom to experiment with different action spaces, CALVIN supports absolute and relative cartesian actions, as well as actions in joint space. We encourage the community to study flexible combinations of observation and action spaces since the tasks require a varying degree of precise control vs. coarse locomotion. While the static camera and absolute cartesian actions are the natural choices for tasks that call for a complete traversal of the environment from one side to another, the gripper camera and relative actions (w.r.t to the gripper frame) allow more fine-grained control for tasks like stacking or grasping. Tactile information can become important when the task requires the robot to maintain a stable grasp on the handle while moving the sliding door to the side. See Fig. 2 for a description of the observation and action dimensionalities.
2) Tasks: We define 34 specific tasks (see Fig. 4) that can be achieved in each one of the environments The environment has the functionality to automatically detect which one of the tasks has been completed in a sequence of steps, which can serve as a sparse reward for reinforcement learning agents. The criterion for task completion is defined in terms of a change in the environment state between the initial and final step of a sequence. This also enables the automatic task detection in any variable-length sequence of offline data, since the environment can be reset to the state of each one of the recorded frames.</p>
<h2>B. The CALVIN Dataset</h2>
<p>1) Unstructured Demonstrations: Learning generally requires exposure to diverse training data. To effectively cover state space, we collect twenty-four hours of teleoperated "play"</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Natural language instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">rotate red block right</td>
<td style="text-align: center;">"rotate the red block 90 degrees to the right" "turn the red block right"</td>
</tr>
<tr>
<td style="text-align: center;">push blue block left</td>
<td style="text-align: center;">"go slide the blue block to the left" "push left the blue block"</td>
</tr>
<tr>
<td style="text-align: center;">move slider left</td>
<td style="text-align: center;">"grasp the door handle, then slide the door to the left" "slide the door to the left"</td>
</tr>
<tr>
<td style="text-align: center;">open drawer</td>
<td style="text-align: center;">"grasp the handle of the drawer and open it" "go open the drawer"</td>
</tr>
<tr>
<td style="text-align: center;">lift red block</td>
<td style="text-align: center;">"lift the red block from the table" "pick up the red block"</td>
</tr>
<tr>
<td style="text-align: center;">pick pink block from drawer</td>
<td style="text-align: center;">"pick up the pink block lying in the drawer"</td>
</tr>
<tr>
<td style="text-align: center;">place in slider</td>
<td style="text-align: center;">"put the grasped object in the slider"</td>
</tr>
<tr>
<td style="text-align: center;">stack blocks</td>
<td style="text-align: center;">"stack blocks on top of each other"</td>
</tr>
<tr>
<td style="text-align: center;">unstack blocks</td>
<td style="text-align: center;">"collapse the stacked blocks" "go to the tower of blocks and take off the top one"</td>
</tr>
<tr>
<td style="text-align: center;">turn on light bulb</td>
<td style="text-align: center;">"toggle the light switch to turn on the light bulb"</td>
</tr>
<tr>
<td style="text-align: center;">turn off green light</td>
<td style="text-align: center;">"push the button to turn off the green light"</td>
</tr>
</tbody>
</table>
<p>Fig. 4: Example crowd-sourced natural language instructions to specify manipulation tasks in CALVIN.
data in four environments with a HTC Vive VR headset, spending an approximately equal time of six hours in each environment. This corresponds to $\sim 2.4 \mathrm{M}$ interaction steps and $\sim 40 \mathrm{M}$ short-horizon windows for relabeled goal conditioned imitation learning [29], [30], each spanning 1-2 seconds. In this setting, an operator is not constrained to a set of predefined tasks, but rather engages in behavior that satisfies their own curiosity or some other intrinsic motivation. Unscripted playful interactions have the advantage of being task-agnostic, diverse, and relatively cheap to obtain [1], [4]. We asked three people to collect data, and these users were untrained and given no information about the downstream tasks. The only guideline we gave data collectors was to "explore the environment without dropping objects from the table". This includes picking up and placing objects, opening, and closing drawers, sliding doors, pushing buttons, operating switches and undirected actions. This style of data is very different from commonly used task-specific data, which only consists of expert trajectories. Playful interaction data by design is free-form, so there are no categories associated with the data. This kind of unstructured data is useful because it contains exploratory and sub-optimal behaviors that are critical to learning generalizable and robust representations, e.g., enabling retrying behavior. While expert demonstrations often only show one of the many possible ways
to solve a task, play data is richer in the sense that it covers the multimodal space of possible solutions. However, as opposed to expert demonstrations, in play data some task instances naturally occur less frequently than others, especially those that have the completion of another task as a prerequisite.
2) Language Instructions: Approaches that learn languageconditioned continuous control policies typically require posthoc crowd-sourced natural language labels aligned with its corresponding robot interaction data [6], [7]. Instead of relying entirely on crowd-sourced annotations, we collect over 400 crowd-sourced natural language instructions corresponding to over 34 tasks and label episodes procedurally using the recorded environment state of the CALVIN dataset. We note that using this labeling scheme, only sequences that display meaningful skills are labeled with language annotations. We visualize example language annotations in Fig. 4. In order to simulate a real-world scenario where it might not be possible to pair all the collected robot experience with crowdsourced language annotations, we annotate only $1 \%$ of the recorded robot interaction data with language instructions. Besides language instructions, we provide precomputed language embeddings extracted from MiniLM [31]. MiniLM distills a large Transformer based language model and is trained on generic language corpora (e.g., Wikipedia). It has a vocabulary size of 30,522 words and maps a sentence of any length into a vector of size 384 . We note that there exist many choices for encoding raw text in a semantic pre-trained vector space and encourage the community to experiment with different choices to solve for CALVIN tasks.</p>
<h2>C. The CALVIN Challenge</h2>
<p>CALVIN combines the challenging settings of open-ended robotic manipulation with open-ended human language conditioning. For example, a robot that is instructed to "place the blue block inside the drawer" must be able to relate language to its world model. Concretely, it needs to learn to identify how a blue block and a drawer look like in its multimodal perceptual observations ${ }^{1}$, and then it needs to reason over the best sequence of actions to "place inside the drawer". Ideally, a general-purpose robot should be able to perform any combination of tasks instructed with natural language in any order. Thus, to accelerate progress in language-driven robotics, we present a set of evaluation protocols of varying difficulty by choosing different combinations of sensor suites and amounts of training environments.</p>
<p>1) Training and Test Environments: CALVIN offers three combinations of training and test environments with varying difficulty:</p>
<p>Single Environment: Training in a single environment and evaluating the policy in the same environment. This corresponds to the setting of Lynch et al. [6].</p>
<p>Multi Environment: Training in all four environments and evaluating the policy in one of them. This poses an additional challenge since the policy has to generalize to multiple textures</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Long-horizon language instructions</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"turn on the led" $\rightarrow$ "open drawer" $\rightarrow$ "push the blue blue block $\rightarrow$ "pick up the blue block " $\rightarrow$ "place in slider"</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"move slider left" $\rightarrow$ "lift red block from slider" $\rightarrow$ "stack blocks" $\rightarrow$ "toggle light" $\rightarrow$ " collapse stacked blocks"</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">"open drawer" $\rightarrow$ "push block in drawer" $\rightarrow$ "pick object from drawer" $\rightarrow$ "stack blocks" $\rightarrow$ "close drawer"</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Fig. 5: Example long-horizon language tasks sequences evaluated in CALVIN. We show the abbreviated subtask names instead of the full language annotations due to space constraint.
and different locations of the sliding door, button, and switch. On the other hand, the agents can benefit from increased data.</p>
<p>Zero-Shot Multi Environment: To open the door for future development of agents that can generalize abstract concepts to unseen entities the same way humans do, we include a challenging zero-shot evaluation by training in three environments and evaluating the policy in the fourth unseen one. This is the hardest combination since the policy has never seen the test environment during training. However, all elements of the scene were present in different locations in the training environments. While highly challenging, we believe it aligns well with test-time expectations for service robots to be useful in a range of daily tasks in everyday environments. Concretely, in CALVIN agents need to generalize to a room where the environment has different textures and all static elements such as the sliding door, the drawer and the light turning button and switch are positioned differently. Thus, a language-conditioned policy should ideally be able to open a sliding door even if it is differently positioned or looks visually a bit different.
2) Evaluation Metrics: All three environment combinations are evaluated with the following metrics:</p>
<p>Multi-Task Language Control (MTLC): The simplest evaluation aims to verify how well the learned multi-task language-conditioned policy generalizes to 34 manipulation tasks, which we visualize in Fig. 6. The evaluation begins by resetting the simulator to the first state of a valid unseen demonstration, to ensure that the commanded instruction is valid. For each manipulation task 10 rollouts are performed with their corresponding different starting states. The language instructions used for testing are not included in the training set and represent novel ways of describing the manipulation tasks seen during training.</p>
<p>Long-Horizon MTLC (LH-MTLC): This evaluation aims to verify how well the learned multi-task language-conditioned policy can accomplish several language instructions in a row. This setting is very challenging as it requires agents to be able to transition between different subgoals. We treat the 34 tasks of the previous evaluation as subgoals and compute valid sequences consisting of five sequential tasks. We only allow feasible sequences that can be achieved from a predefined initial environment state. We filter the evaluation sequences for cycles, redundancies and similarities to arrive at 1000 unique instruction chains. Examples for excluded sequences are "close the drawer"..."place in drawer" (unfeasible), "move slider right"..."move slider left"..."move slider right" (cyclic) or "push blue block left"..."push red block left"(similar). We reset the robot to a neutral position after every sequence to avoid biasing the policies through the robot's initial pose. We note that this neutral initialization breaks correlation between</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Condition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rotate <br> red/blue/pink <br> block right</td>
<td style="text-align: left;">The object has to be rotated clockwise more <br> than $60^{\circ}$ around the z-axis while not being <br> rotated more than $30^{\circ}$ around the x or y -axis.</td>
</tr>
<tr>
<td style="text-align: left;">Rotate <br> red/blue/pink <br> block left</td>
<td style="text-align: left;">The object has to be rotated counterclockwise <br> more than $60^{\circ}$ around z while not being rotated <br> more than $30^{\circ}$ around the x or y-axis.</td>
</tr>
<tr>
<td style="text-align: left;">Push <br> red/blue/pink <br> block right</td>
<td style="text-align: left;">The object has to move more than 10 cm to the <br> right while having surface contact in both <br> frames.</td>
</tr>
<tr>
<td style="text-align: left;">Push <br> red/blue/pink <br> block left</td>
<td style="text-align: left;">The object has to move more than 10 cm to the <br> left while having surface contact in both <br> frames.</td>
</tr>
<tr>
<td style="text-align: left;">Move slider <br> left/right</td>
<td style="text-align: left;">The sliding door has to be pushed at least 12 <br> cm to the left/right.</td>
</tr>
<tr>
<td style="text-align: left;">Open/close <br> drawer</td>
<td style="text-align: left;">The drawer has to be pushed in/pulled out at <br> least 10 cm.</td>
</tr>
<tr>
<td style="text-align: left;">Lift <br> red/blue/pink <br> block table</td>
<td style="text-align: left;">The object has to be grasped from the table <br> surface and lifted at least 5 cm high. In the first <br> frame the gripper may not touch the object.</td>
</tr>
<tr>
<td style="text-align: left;">Lift <br> red/blue/pink <br> block slider</td>
<td style="text-align: left;">The object has to be grasped from the sliding <br> cabinet's surface and lifted at least 3 cm . In the <br> first frame the gripper may not touch the object.</td>
</tr>
<tr>
<td style="text-align: left;">Lift <br> red/blue/pink <br> block drawer</td>
<td style="text-align: left;">The object has to be grasped from the drawer's <br> surface and lifted at least 5 cm high. In the first <br> frame the gripper may not touch the object.</td>
</tr>
<tr>
<td style="text-align: left;">Place in <br> slider/drawer</td>
<td style="text-align: left;">The object has to be placed in the sliding <br> cabinet/drawer. It must be lifted by the gripper <br> in the first frame.</td>
</tr>
<tr>
<td style="text-align: left;">Push into <br> drawer</td>
<td style="text-align: left;">The object has to be pushed into the drawer. It <br> has to touch the table surface in the first frame.</td>
</tr>
<tr>
<td style="text-align: left;">Stack blocks</td>
<td style="text-align: left;">A block has to be placed on top of another <br> block. It may not be in contact with the gripper <br> in the final frame.</td>
</tr>
<tr>
<td style="text-align: left;">Unstack <br> blocks</td>
<td style="text-align: left;">A block has to be removed from the top of <br> another block. It may not be in contact with the <br> gripper in the first frame.</td>
</tr>
<tr>
<td style="text-align: left;">Turn on/off <br> light bulb</td>
<td style="text-align: left;">The switch has to be pushed up/down to turn <br> on/off the yellow light bulb.</td>
</tr>
<tr>
<td style="text-align: left;">Turn on/off <br> LED</td>
<td style="text-align: left;">The button has to be pressed to turn on/turn off <br> the green LED light.</td>
</tr>
</tbody>
</table>
<p>Fig. 6: List of all 34 tasks with their respective success criteria.
initial state and task, forcing the agent to rely entirely on language to infer and solve the task. We include different initial scene configurations in the evaluation to better evaluate generalization capabilities. We visualize the evaluated subtask distribution in Figure 7. For each subtask we condition the policy on the current language instruction and transition to the next subgoal only if the agent successfully completes the current task according to the environments state indicator.
3) Sensor Combinations: The aim of CALVIN is to develop innovative agents that learn to relate human language from onboard sensors by capturing many challenges present in realworld settings. Most autonomous robots operating in complex</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 7: Visualization of the subtask distribution across the 1000 instruction chains used for the Long Horizon MTLC evaluation. We show the percentage in which each subtask appears in the distribution.</p>
<p>environments are equipped with different sensors to perceive their surroundings. To foster development and experimentation of language-conditioned policies that perform manipulation tasks in the real-world, CALVIN supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing [5]. We therefore evaluate baseline agents for different sensors combinations.</p>
<h2>IV. BASELINE MODELS</h2>
<p>An agent trained for CALVIN needs to jointly reason over perceptual and language input and produce a sequence of low-level motor commands to interact with the environment.</p>
<h3>A. Multicontext Imitation Learning</h3>
<p>We model the interactive agent with a general-purpose goal-reaching policy based on multi-context imitation learning (MCIL) from play data [6]. To learn from unstructured "play" we assume access to an unsegmented teleoperated play dataset $\mathcal{D}$ of semantically meaningful behaviors provided by users, without a set of predefined tasks in mind. To learn control, this long temporal state-action stream $\mathcal{D} = {(x_t, a_t)}<em _text_play="\text{play">{t=0}^{\infty}$ is relabeled [30], treating each visited state in the dataset as a "reached goal state", with the preceding states and actions treated as optimal behavior for reaching that goal. Relabeling yields a dataset of $D</em> = {(\tau, x_g), }}<em _text_play="\text{play">{t=0}^{D</em>$ where each goal state $x_g$ has a trajectory demonstration $\tau = {(x_0, a_0), \ldots}$ solving for the goal. These short horizon goal image conditioned demonstrations can be fed to a simple maximum likelihood goal conditioned imitation objective:}}</p>
<p>$$
\mathcal{L}<em D__text_play="D_{\text{play" _sim="\sim" _tau_="(\tau," x_g_="x_g)">{LfP} = \mathbb{E}</em>
$$}}} \left[ \sum_{t=0}^{|\tau|} \log \pi_\theta (a_t \mid x_t, x_g) \right] \tag{1</p>
<p>to learn a goal-reaching policy $\pi_\theta (a_t \mid x_t, x_g)$. Multi-context imitation learning addresses the inherent multi-modality in free-form imitation datasets by auto-encoding contextual demonstrations through a latent "plan" space with an sequence-to-sequence conditional variational auto-encoder (seq2seq CVAE). The decoder is a policy trained to reconstruct input actions, conditioned on state $x_t$, goal $x_g$, and an inferred plan $z$ for how to get from $x_t$ to $x_g$. At test time, it takes a goal as input, and infers and follows plan $z$ in closed-loop.</p>
<p>However, when learning language-conditioned policies $\pi_\theta (a_t \mid x_t, l)$ it is not possible to relabel any visited state $x$ to a natural language goal as the goal space is no longer equivalent to the observation space. Lynch et al. [6] showed that pairing a small number of random windows with language after-the-fact instructions enables learning a single language-conditioned visuomotor policy that can perform a wide variety of robotic manipulation tasks. The key insight here is that solving a single imitation learning policy for either goal image or language goals, allows for learning control mostly from unlabeled play data and reduces the burden of language annotation to less than 1% of the total data. Concretely, given multiple contextual imitation datasets $\mathcal{D} = {D^0, D^1, \ldots, D^K}$, with a different way of describing tasks, MCIL trains a single latent goal conditioned policy $\pi_\theta (a_t \mid x_t, z)$ over all datasets simultaneously, as well as one parameterized encoder per dataset.</p>
<h3>B. Implementation Details</h3>
<p>We follow the baseline architecture implementation reported by Lynch et al. [6] unless stated otherwise. We train the agent with the Adam optimizer and a learning rate of $10^{-4}$. We set the weight controlling the influence of the KL divergence to the total loss to $\beta = 0.001$. During training, we randomly sample windows between length 16 and 32 and pad them until the max length of 32. As in the original implementation, no image data augmentations are applied and absolute cartesian actions w.r.t the world frame are used. The encoder for the gripper camera takes an image of 84 × 84 as input and consists of 3 convolutional layers with 32, 64, and 64 channels followed by a 128 unit ReLU MLP. The encoder for the visual-tactile sensor is based on a pre-trained ResNet-18 model. The feature vectors produced by the different modality encoders are concatenated. Depth images are concatenated channel-wise with the RGB images in an early-fusion fashion. In contrast to [6], the gripper fingers of the robot in the CALVIN environment cannot be controlled independently, reducing the action output of the network by one dimension. We note that the same training hyperparameters are used for all splits.</p>
<h2>V. EXPERIMENTAL RESULTS</h2>
<p>The results comparing language-conditioned policies based on multicontext imitation learning for the different evaluation modes in CALVIN are shown in Figure 8. We note that there is no constraint to use imitation learning approaches to solve CALVIN tasks, as approaches that use reinforcement learning to learn language-conditioned policies can also be applied [7]. We observe that the baseline with images of the static camera achieves a success rate of 53.9% for the MTLC evaluation setting, when training and testing the 34 manipulation tasks on the same environment. The success rate stays comparable when including a gripper camera, depth channels or tactile sensing. We hypothesize that the reason for not seeing larger improvements when adding the gripper camera is that the policy might benefit from using relative actions instead of</p>
<p>| Input | | | | | Train $\rightarrow$ Test | MTLC | LH-MTLC | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<h2>REFERENCES</h2>
<p>[1] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet, "Learning latent plans from play," in CoRL, 2019.
[2] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning," in CoRL, 2019.
[3] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman, "Scaling up multi-task robotic reinforcement learning," in CoRL, 2021.
[4] S. Young, J. Pari, P. Abbeel, and L. Pinto, "Playful interactions for representation learning," arXiv preprint arXiv:2107.09046, 2021.
[5] S. Wang, M. Lambeta, P.-W. Chou, and R. Calandra, "Tacto: A fast, flexible and open-source simulator for high-resolution vision-based tactile sensors," arXiv preprint arXiv:2012.08456, 2020.
[6] C. Lynch and P. Sermanet, "Language conditioned imitation learning over unstructured data," in RSS, 2021.
[7] S. Nair, E. Mitchell, K. Chen, B. Ichter, S. Savarese, and C. Finn, "Learning language-conditioned robot behavior from offline data and crowd-sourced annotation," in CoRL, 2021.
[8] S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek, "Robots that use language," Annual Review of Control, Robotics, and Autonomous Systems, vol. 3, pp. 25-55, 2020.
[9] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, "Referitgame: Referring to objects in photographs of natural scenes," in EMNLP, 2014.
[10] J. Lu, D. Batra, D. Parikh, and S. Lee, "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks," in NeurIPS, 2019.
[11] R. Paul, J. Arkin, N. Roy, and T. M Howard, "Efficient grounding of abstract spatial concepts for natural language interaction with robot manipulators," in RSS, 2016.
[12] M. Shridhar and D. Hsu, "Interactive visual grounding of referring expressions for human-robot interaction," in RSS, 2018.
[13] J. Hatori, Y. Kikuchi, S. Kobayashi, K. Takahashi, Y. Tsuboi, Y. Unno, W. Ko, and J. Tan, "Interactively picking real-world objects with unconstrained spoken language instructions," in ICRA, 2018.
[14] T. Nguyen, N. Gopalan, R. Patel, M. Corsaro, E. Pavlick, and S. Tellex, "Robot object retrieval with contextual natural language queries," in RSS, 2020.
[15] H. Zhang, Y. Lu, C. Yu, D. Hsu, X. La, and N. Zheng, "Invigorate: Interactive visual grounding and grasping in clutter," in RSS, 2021.
[16] O. Mees and W. Burgard, "Composing pick-and-place tasks by grounding language," in ISER, 2021.
[17] S. G. Venkatesh, A. Biswas, R. Upadrashta, V. Srinivasan, P. Talukdar, and B. Amrutur, "Spatial reasoning from natural language instructions for robot manipulation," in ICRA, 2021.
[18] W. Liu, C. Paxton, T. Hermans, and D. Fox, "Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects," arXiv preprint arXiv:2110.10189, 2021.
[19] D. Misra, J. Langford, and Y. Artzi, "Mapping instructions and visual observations to actions with reinforcement learning," in EMNLP, 2017.
[20] H. Yu, H. Zhang, and W. Xu, "Interactive grounded language acquisition and generalization in a 2d world," in $I C L R, 2018$.
[21] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments," in CVPR, 2018.
[22] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, "Alfred: A benchmark for interpreting grounded instructions for everyday tasks," in CVPR, 2020.
[23] M. Shridhar, L. Manuelli, and D. Fox, "Cliport: What and where pathways for robotic manipulation," in CoRL, 2021.
[24] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor, "Language-conditioned imitation learning for robot manipulation tasks," in NeurIPS, 2020.
[25] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, "Bc-0: Zero-shot task generalization with robotic imitation learning," in CoRL, 2021.
[26] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg, "Concept2robot: Learning manipulation concepts from instructions and human demonstrations," in RSS, 2020.
[27] V. Blukis, D. Misra, R. A. Knepper, and Y. Artzi, "Mapping navigation instructions to continuous control actions with position-visitation prediction," in CoRL, 2018.
[28] E. Coumans and Y. Bai, "Pybullet, a python module for physics simulation for games, robotics and machine learning," http://pybullet.org, 2016-2021.
[29] L. P. Kaelbling, "Learning to achieve goals," in IJCAI. Citeseer, 1993, pp. 1094-1099.
[30] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, "Hindsight experience replay," in NeurIPS, 2017.
[31] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, "Minilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers," in NeurIPS, 2020.
[32] P. de Haan, D. Jayaraman, and S. Levine, "Causal confusion in imitation learning," NeurIPS, 2019.
[33] O. Mees, A. Eitel, and W. Burgard, "Choosing smartly: Adaptive multimodal fusion for object detection in changing environments," in IROS, 2016.
[34] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg, "Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks," in ICRA, 2019.
[35] Y. Tian, D. Krishnan, and P. Isola, "Contrastive multiview coding," in $E C C V, 2020$.
[36] O. Mees, M. Merklinger, G. Kalweit, and W. Burgard, "Adversarial skill networks: Unsupervised robot skill learning from videos," in ICRA, 2020.
[37] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., "Learning transferable visual models from natural language supervision," arXiv preprint arXiv:2103.00020, 2021.
[38] T. Yu, A. Kumar, Y. Chebotar, K. Hausman, S. Levine, and C. Finn, "Conservative data sharing for multi-task offline reinforcement learning," in NeurIPS, 2021.</p>
<h1>Appendix</h1>
<h2>A. Tasks</h2>
<p>All tasks are defined in terms of change in the environment state between the first and the final frame of a sequence. In order to see if a task was solved in an arbitrary sequence of frames of the CALVIN dataset, the environment is reset to the state of the first and the last frame of that sequence. The tasks detector compares the two simulator states and checks which task conditions are fulfilled. A key advantage of this strategy is that it enables efficient evaluation of sequences for task completion independent of their length. Figure 9 shows a list of all task definitions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Condition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rotate red block right Rotate blue block right Rotate pink block right</td>
<td style="text-align: center;">The object has to be rotated clockwise more than $60^{\circ}$ around the z-axis while not being rotated for more than $30^{\circ}$ around the x or y-axis.</td>
</tr>
<tr>
<td style="text-align: center;">Rotate red block left Rotate blue block left Rotate pink block left</td>
<td style="text-align: center;">The object has to be rotated counterclockwise more than $60^{\circ}$ around the z-axis while not being rotated for more than $30^{\circ}$ around the x or y-axis.</td>
</tr>
<tr>
<td style="text-align: center;">Push red block right Push blue block right Push pink block right</td>
<td style="text-align: center;">The object has to move more than 10 cm to the right while having surface contact in both frames</td>
</tr>
<tr>
<td style="text-align: center;">Push red block left Push blue block left Push pink block left</td>
<td style="text-align: center;">The object has to move more than 10 cm to the left while having surface contact in both frames</td>
</tr>
<tr>
<td style="text-align: center;">Move slider left</td>
<td style="text-align: center;">The sliding door has to be pushed at least 12 cm to the left.</td>
</tr>
<tr>
<td style="text-align: center;">Move slider right</td>
<td style="text-align: center;">The sliding door has to be pushed at least 12 cm to the right.</td>
</tr>
<tr>
<td style="text-align: center;">Open drawer</td>
<td style="text-align: center;">The drawer has to pulled out at least 10 cm .</td>
</tr>
<tr>
<td style="text-align: center;">Close drawer</td>
<td style="text-align: center;">The drawer has to be pushed in at least 10 cm .</td>
</tr>
<tr>
<td style="text-align: center;">Lift red block table Lift blue block table Lift pink block table</td>
<td style="text-align: center;">The object has to be grasped from the table surface and lifted at least 5 cm high. In the first frame the gripper may not touch the object.</td>
</tr>
<tr>
<td style="text-align: center;">Lift red block slider Lift blue block slider Lift pink block slider</td>
<td style="text-align: center;">The object has to be grasped from the surface of the sliding cabinet and lifted at least 3 cm high. In the first frame the gripper may not touch the object.</td>
</tr>
<tr>
<td style="text-align: center;">Lift red block drawer Lift blue block drawer Lift pink block drawer</td>
<td style="text-align: center;">The object has to be grasped from the surface of the drawer and lifted at least 5 cm high. In the first frame the gripper may not touch the object.</td>
</tr>
<tr>
<td style="text-align: center;">Place in slider</td>
<td style="text-align: center;">The object has to be placed in the sliding cabinet. It must be lifted by the gripper in the first frame.</td>
</tr>
<tr>
<td style="text-align: center;">Place in drawer</td>
<td style="text-align: center;">The object has to be placed in the drawer. It must be lifted by the gripper in the first frame.</td>
</tr>
<tr>
<td style="text-align: center;">Push into drawer</td>
<td style="text-align: center;">The object has to be pushed into the drawer. It has to touch the table surface in the first frame.</td>
</tr>
<tr>
<td style="text-align: center;">Stack blocks</td>
<td style="text-align: center;">A block has to be placed on top of another block. It may not be in contact with the gripper in the final frame.</td>
</tr>
<tr>
<td style="text-align: center;">Unstack blocks</td>
<td style="text-align: center;">A block that is stacked on another block has to be removed from the top, either by grasping it or by pushing it down. It may not be in contact with the gripper in the first frame.</td>
</tr>
<tr>
<td style="text-align: center;">Turn on light bulb</td>
<td style="text-align: center;">The switch has to be pushed down to turn on the yellow light bulb.</td>
</tr>
<tr>
<td style="text-align: center;">Turn off light bulb</td>
<td style="text-align: center;">The switch has to be pushed up to turn off the yellow light bulb.</td>
</tr>
<tr>
<td style="text-align: center;">Turn on LED</td>
<td style="text-align: center;">The button has to be pressed to turn on the green LED light.</td>
</tr>
<tr>
<td style="text-align: center;">Turn off LED</td>
<td style="text-align: center;">The button has to be pressed to turn off the green LED light.</td>
</tr>
</tbody>
</table>
<p>Fig. 9: List of all 34 tasks with their respective success criteria.</p>
<h1>B. Language Annotation Generation</h1>
<p>The language annotations are extracted automatically from the recorded data with the following procedure: we randomly sample sequences with a window size of 64 frames. For each sequence the task detector checks if a task has been solved between the first and the last frame. Additionally, we check that neither that task nor any other task is solved in the first half of the sequence. The intuition behind this is that we want to include the locomotion behavior prior to the actual task. For example, before opening the drawer, the arm must navigate in the direction of the handle. This is important for learning to solve tasks with language goals from arbitrary starting positions. If a sequence qualifies for labeling, we sample a natural language instruction from a set of predefined sentences with approximately 11 synonymous instructions per task. In total, this gives 389 unique language instructions for 34 tasks. The sequence in which the task "stack blocks" is solved could for example get instructions such as "place the grasped block on top of another block" or "stack blocks on top of each other". In order to simulate a real-world scenario where it might not be possible to pair all the collected robot experience with crowd-sourced language annotations, we annotate only $1 \%$ of the recorded robot interaction data with language instructions. The CALVIN dataset conveniently includes precomputed MiniLM language embeddings for all instructions, but researchers are free to use their own language model of choice on the raw input data.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Simulator states consisting of object positions and orientations are also provided, but not used to better capture challenges of real-world settings.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3182 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3182</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3182</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-2d4dae3b4be8d69315c7f32823f338bb6e4837e8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2d4dae3b4be8d69315c7f32823f338bb6e4837e8" target="_blank">Large-Scale Retrieval for Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work pursues an alternative approach in which agents can utilise large-scale context sensitive database lookups to support their parametric computations, which allows agents to directly learn in an end-to-end manner to utilise relevant information to inform their outputs.</p>
                <p><strong>Paper Abstract:</strong> Effective decision making involves flexibly relating past experiences and relevant contextual information to a novel situation. In deep reinforcement learning (RL), the dominant paradigm is for an agent to amortise information that helps decision making into its network weights via gradient descent on training losses. Here, we pursue an alternative approach in which agents can utilise large-scale context sensitive database lookups to support their parametric computations. This allows agents to directly learn in an end-to-end manner to utilise relevant information to inform their outputs. In addition, new information can be attended to by the agent, without retraining, by simply augmenting the retrieval dataset. We study this approach for offline RL in 9x9 Go, a challenging game for which the vast combinatorial state space privileges generalisation over direct matching to past experiences. We leverage fast, approximate nearest neighbor techniques in order to retrieve relevant data from a set of tens of millions of expert demonstration states. Attending to this information provides a significant boost to prediction accuracy and game-play performance over simply using these demonstrations as training trajectories, providing a compelling demonstration of the value of large-scale retrieval in offline RL agents.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3182.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3182.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented Go agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semi-parametric retrieval-augmented model-based Go agent (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MuZero-style semi-parametric agent that augments a parametric action-conditional model with nearest-neighbor retrieval from a large external dataset of expert Go positions; retrieved neighbors are processed by a permutation-invariant neighbor tower and fused into the model embedding, and the model is used inside MCTS for acting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented MuZero-style Go agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model-based agent m_θ that encodes the current 9x9 Go observation and N retrieved neighbors x_t^i into a root embedding s_t via convolutional residual networks; it performs K-step action-conditional iterative inference s_t^{k+1}=h_θ(s_t^k,a_{t+k}) to predict policy priors and values, and uses MCTS (pUCT) with the trained model to produce an acting policy. The retrieval pipeline uses a frozen pre-trained embedding g_φ to produce d-dimensional keys/queries, SCaNN for fast approximate nearest-neighbor lookup, and a neighbor-processing network p_θ whose outputs are summed (permutation-invariant) and concatenated with the observation embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external dataset (large-scale key-value store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Each dataset entry o_i is preprocessed by a frozen embedding g_φ (PCA-compressed activations from a pre-trained MuZero-style network) to form a key k_i. At inference/training, the current observation o_t is mapped to a query q_t=g_φ(o_t); SCaNN returns the top-N approximate nearest neighbors by squared Euclidean distance. For each neighbor the model retrieves meta-data (neighbor board, next actions, outcome, final board) as x_t^i; each x_t^i is concatenated with an encoded o_t and passed through a shared residual 'neighbor tower' p_θ to produce e_i^t; the neighbor embeddings are summed (normalized) and concatenated with the encoded observation to form s_t. The retrieval dataset (~50M subsampled positions) can be augmented at evaluation time (e.g., with recent agent-opponent games) without retraining, immediately affecting policy via retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>9x9 Go (offline RL with model-based search)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Play 9x9 Go using a supervised / offline RL training dataset of expert self-play trajectories (~3.5M games, ~50M positions), learning to predict value and policy over K steps and using MCTS for planning; challenge is massive combinatorial state space and distribution shift when playing new opponents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>game playing / planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In supervised prediction, retrieval-augmented models show consistently higher test-set top-1 policy accuracy and lower value MSE compared to capacity-matched non-retrieval baselines; when used with MCTS, retrieval-augmented agents win substantially more games against the Pachi reference opponent than the non-retrieval baseline of the same parametric capacity. Increasing N (number of neighbors) and increasing retrieval dataset size improved performance; augmenting the retrieval set with recent agent-opponent games improved play without any additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Nearest-neighbor lookup is approximate and non-differentiable (keys/queries are frozen); distribution shift between training positions and positions seen in play reduces neighbor quality (mitigated by dataset augmentation); poor or random neighbors can hurt the base policy prior more than MCTS-based play; requires large retrieval datasets and incurs extra compute for neighbor processing and nearest-neighbor querying; robustness relies on regularisation techniques (neighbor randomisation/dropout) and sufficient MCTS simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Retrieval for Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3182.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3182.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Borgeaud et al. retrieval LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented language model approach (Borgeaud et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced language-model retrieval work that learns/follows a surrogate embedding procedure and retrieves from very large token stores to augment language model predictions, motivating the frozen-embedding retrieval approach used here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving language models by retrieving from trillions of tokens</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented language model (Borgeaud et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A language-modeling approach that augments transformer models with retrieval over a very large datastore of tokens; the paper is cited as an inspiration for learning an embedding function via a surrogate procedure and using a frozen embedding for scalable similarity-based retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external token datastore</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Uses a learned embedding/key-query mechanism (trained separately or via surrogate objectives) to index a very large corpus (trillions) of tokens for nearest-neighbor retrieval and conditions the LM on retrieved passages; in this paper the technique is cited as motivating the use of frozen embeddings and large-scale approximate-nearest-neighbor lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned only as prior work on retrieval for language modeling (not evaluated within this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling (mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as motivating large-scale retrieval and the use of frozen surrogate-trained embeddings to define domain-relevant similarity; inspired the decision to precompute keys and use SCaNN for fast approximate lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Retrieval for Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3182.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3182.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REALM: Retrieval-augmented language model pre-training (Guu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced retrieval-augmented LM pre-training approach that jointly trains a retrieval component and language model representations to improve language understanding and factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Realm: Retrieval-augmented language model pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REALM retrieval-augmented language model</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A language-model pre-training method that integrates a retrieval component into language model pre-training to condition predictions on retrieved documents; cited in this paper in the context of end-to-end or joint learning of query/key embeddings as a potential future direction.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external document store</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>REALM uses an end-to-end differentiable retrieval-and-read pipeline during pre-training to incorporate documents from an external corpus into model predictions; in this paper REALM is referenced as a method for learning queries/keys end-to-end, which the authors note they did not do here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned as prior work relevant to learning query/key embeddings end-to-end (not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / pre-training (mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as prior work demonstrating that end-to-end learning of retrieval components is possible and a potential future improvement over the frozen-embedding approach used in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Retrieval for Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3182.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3182.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goyal et al. RARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented reinforcement learning (Goyal et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that explores attention mechanisms over small retrieval batches of trajectories to select relevant episodic data for RL agents; cited as related work to this paper's larger-scale retrieval approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented RL (Goyal et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL approach that uses an attention mechanism to select segments or regions from available trajectories (a smaller retrieval batch) to augment decision-making; cited here as related but operating on smaller retrieval sets than the tens-of-millions scale used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic retrieval / attention over trajectory segments</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Selects and uses specific portions of stored trajectories via attention to inform the policy or value estimation; in this paper it is cited as complementary prior work that prescribes selection of where/what to use from trajectories rather than learning to process arbitrary retrieved data end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned as a related method for leveraging inter-episodic information in RL (not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reinforcement learning (mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior art using attention over stored trajectories for RL; contrasted with the present work which learns to interpret retrieved neighbors end-to-end and scales retrieval to tens of millions of items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Retrieval for Reinforcement Learning', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Realm: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented reinforcement learning <em>(Rating: 2)</em></li>
                <li>Model-free episodic control <em>(Rating: 1)</em></li>
                <li>Neural episodic control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3182",
    "paper_id": "paper-2d4dae3b4be8d69315c7f32823f338bb6e4837e8",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "Retrieval-augmented Go agent",
            "name_full": "Semi-parametric retrieval-augmented model-based Go agent (this paper)",
            "brief_description": "A MuZero-style semi-parametric agent that augments a parametric action-conditional model with nearest-neighbor retrieval from a large external dataset of expert Go positions; retrieved neighbors are processed by a permutation-invariant neighbor tower and fused into the model embedding, and the model is used inside MCTS for acting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Retrieval-augmented MuZero-style Go agent",
            "agent_description": "A model-based agent m_θ that encodes the current 9x9 Go observation and N retrieved neighbors x_t^i into a root embedding s_t via convolutional residual networks; it performs K-step action-conditional iterative inference s_t^{k+1}=h_θ(s_t^k,a_{t+k}) to predict policy priors and values, and uses MCTS (pUCT) with the trained model to produce an acting policy. The retrieval pipeline uses a frozen pre-trained embedding g_φ to produce d-dimensional keys/queries, SCaNN for fast approximate nearest-neighbor lookup, and a neighbor-processing network p_θ whose outputs are summed (permutation-invariant) and concatenated with the observation embedding.",
            "memory_used": true,
            "memory_type": "retrieval-augmented external dataset (large-scale key-value store)",
            "memory_mechanism_description": "Each dataset entry o_i is preprocessed by a frozen embedding g_φ (PCA-compressed activations from a pre-trained MuZero-style network) to form a key k_i. At inference/training, the current observation o_t is mapped to a query q_t=g_φ(o_t); SCaNN returns the top-N approximate nearest neighbors by squared Euclidean distance. For each neighbor the model retrieves meta-data (neighbor board, next actions, outcome, final board) as x_t^i; each x_t^i is concatenated with an encoded o_t and passed through a shared residual 'neighbor tower' p_θ to produce e_i^t; the neighbor embeddings are summed (normalized) and concatenated with the encoded observation to form s_t. The retrieval dataset (~50M subsampled positions) can be augmented at evaluation time (e.g., with recent agent-opponent games) without retraining, immediately affecting policy via retrieval.",
            "task_name": "9x9 Go (offline RL with model-based search)",
            "task_description": "Play 9x9 Go using a supervised / offline RL training dataset of expert self-play trajectories (~3.5M games, ~50M positions), learning to predict value and policy over K steps and using MCTS for planning; challenge is massive combinatorial state space and distribution shift when playing new opponents.",
            "task_type": "game playing / planning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "In supervised prediction, retrieval-augmented models show consistently higher test-set top-1 policy accuracy and lower value MSE compared to capacity-matched non-retrieval baselines; when used with MCTS, retrieval-augmented agents win substantially more games against the Pachi reference opponent than the non-retrieval baseline of the same parametric capacity. Increasing N (number of neighbors) and increasing retrieval dataset size improved performance; augmenting the retrieval set with recent agent-opponent games improved play without any additional training.",
            "limitations_or_challenges": "Nearest-neighbor lookup is approximate and non-differentiable (keys/queries are frozen); distribution shift between training positions and positions seen in play reduces neighbor quality (mitigated by dataset augmentation); poor or random neighbors can hurt the base policy prior more than MCTS-based play; requires large retrieval datasets and incurs extra compute for neighbor processing and nearest-neighbor querying; robustness relies on regularisation techniques (neighbor randomisation/dropout) and sufficient MCTS simulations.",
            "uuid": "e3182.0",
            "source_info": {
                "paper_title": "Large-Scale Retrieval for Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Borgeaud et al. retrieval LM",
            "name_full": "Retrieval-augmented language model approach (Borgeaud et al.)",
            "brief_description": "Referenced language-model retrieval work that learns/follows a surrogate embedding procedure and retrieves from very large token stores to augment language model predictions, motivating the frozen-embedding retrieval approach used here.",
            "citation_title": "Improving language models by retrieving from trillions of tokens",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented language model (Borgeaud et al.)",
            "agent_description": "A language-modeling approach that augments transformer models with retrieval over a very large datastore of tokens; the paper is cited as an inspiration for learning an embedding function via a surrogate procedure and using a frozen embedding for scalable similarity-based retrieval.",
            "memory_used": true,
            "memory_type": "retrieval-augmented external token datastore",
            "memory_mechanism_description": "Uses a learned embedding/key-query mechanism (trained separately or via surrogate objectives) to index a very large corpus (trillions) of tokens for nearest-neighbor retrieval and conditions the LM on retrieved passages; in this paper the technique is cited as motivating the use of frozen embeddings and large-scale approximate-nearest-neighbor lookup.",
            "task_name": "",
            "task_description": "Mentioned only as prior work on retrieval for language modeling (not evaluated within this paper).",
            "task_type": "language modeling (mention only)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Cited as motivating large-scale retrieval and the use of frozen surrogate-trained embeddings to define domain-relevant similarity; inspired the decision to precompute keys and use SCaNN for fast approximate lookup.",
            "limitations_or_challenges": null,
            "uuid": "e3182.1",
            "source_info": {
                "paper_title": "Large-Scale Retrieval for Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "REALM",
            "name_full": "REALM: Retrieval-augmented language model pre-training (Guu et al.)",
            "brief_description": "Referenced retrieval-augmented LM pre-training approach that jointly trains a retrieval component and language model representations to improve language understanding and factual recall.",
            "citation_title": "Realm: Retrieval-augmented language model pre-training",
            "mention_or_use": "mention",
            "agent_name": "REALM retrieval-augmented language model",
            "agent_description": "A language-model pre-training method that integrates a retrieval component into language model pre-training to condition predictions on retrieved documents; cited in this paper in the context of end-to-end or joint learning of query/key embeddings as a potential future direction.",
            "memory_used": true,
            "memory_type": "retrieval-augmented external document store",
            "memory_mechanism_description": "REALM uses an end-to-end differentiable retrieval-and-read pipeline during pre-training to incorporate documents from an external corpus into model predictions; in this paper REALM is referenced as a method for learning queries/keys end-to-end, which the authors note they did not do here.",
            "task_name": "",
            "task_description": "Mentioned as prior work relevant to learning query/key embeddings end-to-end (not evaluated here).",
            "task_type": "language modeling / pre-training (mention only)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Referenced as prior work demonstrating that end-to-end learning of retrieval components is possible and a potential future improvement over the frozen-embedding approach used in this study.",
            "limitations_or_challenges": null,
            "uuid": "e3182.2",
            "source_info": {
                "paper_title": "Large-Scale Retrieval for Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Goyal et al. RARL",
            "name_full": "Retrieval-augmented reinforcement learning (Goyal et al.)",
            "brief_description": "A prior work that explores attention mechanisms over small retrieval batches of trajectories to select relevant episodic data for RL agents; cited as related work to this paper's larger-scale retrieval approach.",
            "citation_title": "Retrieval-augmented reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented RL (Goyal et al.)",
            "agent_description": "An RL approach that uses an attention mechanism to select segments or regions from available trajectories (a smaller retrieval batch) to augment decision-making; cited here as related but operating on smaller retrieval sets than the tens-of-millions scale used in this paper.",
            "memory_used": true,
            "memory_type": "episodic retrieval / attention over trajectory segments",
            "memory_mechanism_description": "Selects and uses specific portions of stored trajectories via attention to inform the policy or value estimation; in this paper it is cited as complementary prior work that prescribes selection of where/what to use from trajectories rather than learning to process arbitrary retrieved data end-to-end.",
            "task_name": "",
            "task_description": "Mentioned as a related method for leveraging inter-episodic information in RL (not evaluated here).",
            "task_type": "reinforcement learning (mention only)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Cited as prior art using attention over stored trajectories for RL; contrasted with the present work which learns to interpret retrieved neighbors end-to-end and scales retrieval to tens of millions of items.",
            "limitations_or_challenges": null,
            "uuid": "e3182.3",
            "source_info": {
                "paper_title": "Large-Scale Retrieval for Reinforcement Learning",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Realm: Retrieval-augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Model-free episodic control",
            "rating": 1
        },
        {
            "paper_title": "Neural episodic control",
            "rating": 1
        }
    ],
    "cost": 0.0125245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large-Scale Retrieval for Reinforcement Learning</h1>
<p>Peter C. Humphreys, Arthur Guez*, Olivier Tieleman, Laurent Sifre, Théophane Weber, Timothy Lillicrap<br>Deepmind, London<br>{peterhumphreys, aguez, ...}@google.com</p>
<h4>Abstract</h4>
<p>Effective decision making involves flexibly relating past experiences and relevant contextual information to a novel situation. In deep reinforcement learning (RL), the dominant paradigm is for an agent to amortise information that helps decisionmaking into its network weights via gradient descent on training losses. Here, we pursue an alternative approach in which agents can utilise large-scale contextsensitive database lookups to support their parametric computations. This allows agents to directly learn in an end-to-end manner to utilise relevant information to inform their outputs. In addition, new information can be attended to by the agent, without retraining, by simply augmenting the retrieval dataset. We study this approach for offline RL in 9x9 Go, a challenging game for which the vast combinatorial state space privileges generalisation over direct matching to past experiences. We leverage fast, approximate nearest neighbor techniques in order to retrieve relevant data from a set of tens of millions of expert demonstration states. Attending to this information provides a significant boost to prediction accuracy and game-play performance over simply using these demonstrations as training trajectories, providing a compelling demonstration of the value of large-scale retrieval in offline RL agents.</p>
<h2>1 Introduction</h2>
<p>How can reinforcement learning (RL) agents leverage relevant information to inform their decisions? Deep RL agents have typically been represented as a monolithic parametric function, trained to gradually amortise useful information from experience by gradient descent. This has been effective $[25,38]$, but is a slow means of integrating experience, with no straightforward way for an agent to incorporate new information without many additional gradient updates. In addition, this requires increasingly massive models (see e.g. [31]) as environments become more complex - scaling is driven by the dual role of the parametric function, which must support both computation and memorisation. Finally, this approach has a further drawback of particular relevance in RL - the only way in which previously encountered information (that is not contained in working memory) can aid decision making in a novel situation is indirectly through weight changes mediated by network losses. There is no end-to-end means for an agent to attend to information outside of working memory to directly inform its actions. While there has been a significant amount of work focused on increasing the information available from previous experiences within an episode (e.g., recurrent networks, slot-based memory $[19,28])$, more extensive direct use of more general forms of experience or data has been limited, although some recent works have begun to explore utilising inter-episodic information from the same agent $[6,10,29,34,41]$. We seek to drastically expand the scale of information that is accessible to an agent, allowing it to attend to tens of millions of pieces of information, while learning in an end-to-end manner how to use this information for decision making. We view this as a first step towards a vision in which an agent can flexibly draw on diverse and large-scale information sources,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>including its own (inter-episodic) experiences along with experiences from humans and other agents. In addition, given that retrieved information need not match the agent's observation format, retrieval could enable agents to integrate information sources that have not typically been utilised such as videos [2], text, and third-person demonstrations.</p>
<p>We investigate a semi-parametric agent architecture for large-scale retrieval, in which fast and efficient approximate nearest neighbor matching is used to dynamically retrieve relevant information from a dataset of experience. We evaluate this approach in an offline RL setting for an environment with a combinatorial state space - the game of 9x9 Go with $\approx 10^{38}$ possible games, where generalisation from past data to novel situations is challenging. We equip our agent with a large-scale dataset of $\sim 50 \mathrm{M}$ Go board-state observations, finding that a retrieval-based approach utilising this data is able to consistently and significantly outperform a strong non-retrieval baseline.</p>
<p>Several key advantages of this retrieval approach are worth highlighting: Instead of having to amortise all relevant information into its network weights, a retrieval-augmented network can utilise more of its capacity for computation. In addition, this semi-parametric form allows us to update the information available to the agent at evaluation time without having to retrain it. Strikingly, we find that this enables improvements to agent performance without further training when games played against the evaluation opponent are added to its knowledge base.</p>
<h1>2 Methods</h1>
<p>Before introducing the details of our method, let us first present its high-level ingredients. We train a model-based agent [36] that predicts future policies and values conditioned on future actions in a given state. This semiparametric model incorporates a retrieval mechanism, which allows it to utilise information from a large-scale dataset to inform its predictions. We train the agent in a supervised offline RL setting, which allows us to directly evaluate the degree to which retrieval improves the quality of the model predictions. We subsequently evaluate the resulting model, augmented by MonteCarlo tree search, against a reference opponent. This allows us to determine how these improvements translate to an effective acting policy for out-of-training-distribution situations.</p>
<p>To effectively improve model predictions using auxiliary information, we need 1) a scalable way to select and retrieve relevant data and 2) a robust way of leveraging that data in our model. As with many successful attention mechanisms, we establish relevance for 1) through an inner product in an appropriate key-query space and select the top $N$ relevant results. Choosing the relevant key-query space is critical, since performing nearest-neighbor on the raw observations will only deliver desirable results for the simplest of domains. For example, in the game of Go, a single stone difference on the board can dramatically change the outcome and reading of the board, while seemingly different board positions might still share high-level characteristics (e.g. influence, partial local sequences, and life-and-death of groups).</p>
<p>At the scale we want to operate, learning the key \&amp; query embeddings end-to-end in order to optimize the final model predictions is challenging. Instead, as in the language modelling work by Borgeaud et al. [7], we learn an embedding function through a surrogate procedure and use the resulting frozen function to describe domain-relevant similarity. Moreover, scale also constrains the nearest-neighbors lookup to be approximate, since getting the true nearest-neighbors is too time-consuming at inference time and good approximations can be obtained.</p>
<p>To address 2) and make the best use of the relevant data, we provide the data associated with the nearest-neighbor as additional input features to the parametric part of our model, so that the network can learn to interpret it. We provide some light inductive biases in the architecture to ensure permutation invariance in the neighbors and robustness to outliers and distribution shifts (see Secs. 2.1.3 \&amp; 2.3). Letting the network interpret the nearest-neighbor data is essential in large and complex environments, such as Go, because the typically imperfect match between the current situation and retrieved neighbors will require different aspects of this retrieved data to be ignored or emphasized in a highly context specific manner. This is in contrast to episodic RL approaches like [6, 29] which prescribe how to use the retrieved data. Subsequent sections describe the model, retrieval process, and the full algorithm in more detail.</p>
<h1>2.1 Retrieval-augmented agents</h1>
<p>We consider a setting in which we wish to train a neural network model $m_{\theta}$ on a set of environment trajectories $\tau \in \mathcal{D}$ (for example, the dataset $\mathcal{D}^{\pi}$ of trajectories produced by a policy $\pi$ ). For each timestep $t$ of $\tau$, the model takes an observation $o_{t}{ }^{2}$ and must produce predictions $\hat{y}$ of targets $y_{t}$.
In our retrieval paradigm, in addition to having access to $o_{t}$, the model also has access to an auxiliary dataset of knowledge or experience $\mathcal{D}<em r="r">{r}$. The auxiliary set $\mathcal{D}</em>}$ could be the same as $\mathcal{D}$, but it can also contain more information, or in the most general case, information from a completely different distribution and in a different format. If there is overlap between $\mathcal{D<em r="r">{r}$ and $\mathcal{D}$, it is important for robustness to ensure that the model cannot retrieve the same trajectory as it is being trained on (otherwise the network will tend to overly trust information retrieved from $\mathcal{D}</em>$
We wish to use $\mathcal{D}}$ ). Common information sources in RL are trajectories from other agents or experts (offline RL) or the agent's own previous experiences (episodic memory, replay buffer). Note that, in contrast to offline RL, we do not assume that we should directly use this auxiliary data as trajectories to train on. ${ }^{3<em t="t">{r}$ to inform the model predictions $\hat{y}</em>}$, such that $\hat{y<em _theta="\theta">{t}=m</em>}\left(o_{t}, \mathcal{D<em r="r">{r}\right)$. This is challenging, as $\mathcal{D}</em>}$ is typically far too large to directly be consumed as a model input. One solution, shown in Fig. 1, is to adopt a retrieval-based model, wherein the parametric and differentiable portion of the model $m_{\theta}$ is provided with an informative subset of data $\left{x_{t}^{1}, x_{t}^{2}, \ldots, x_{t}^{N}\right}$ retrieved from $\mathcal{D<em t="t">{r}$, conditioned on $o</em>$ :</p>
<p>$$
\hat{y}<em _theta="\theta">{t}=m</em>\right)
$$}\left(o_{t}, x_{t}^{1}, x_{t}^{2}, \ldots, x_{t}^{N</p>
<p>This approach requires a number of design choices relating to the retrieval mechanism, which we explore further below.</p>
<h3>2.1.1 Scalable nearest-neighbor retrieval using SCaNN</h3>
<p>Inspired by previous work in language modelling, we chose to leverage the SCaNN architecture [11] for fast approximate nearest-neighbor retrieval. This requires each entry $o_{i}$ in the dataset $\mathcal{D}<em i="i">{r}$ to be associated with a key vector $k</em>$ are used to determine which neighbors to retrieve - reminiscent of neural attention mechanisms. This retrieval process is very efficient and can be scaled to datasets with billions of items.
We will typically want to retrieve further associated meta-data, or context, for each neighbor. For example, if a neighbor is part of a trajectory, we would like to retrieve information about action choices and their consequences. The neighbor observation $o_{i}$, together with its meta-data, forms the auxiliary input $x_{t}^{i}$ to the model.
The nearest-neighbor retrieval process is non-differentiable, which means that the query and key mappings cannot be trained end-to-end directly. Instead, in this first study, we pre-train a non-retrieval prediction network $m_{\phi}^{r}$ on our experience dataset $\mathcal{D}$ (details in Sec. A.4). We then use this network to generate an embedding corresponding to a given observation $o_{i}$ by retrieving the network activations from a specified layer of $m_{\phi}^{r}$. We use principal component analysis to compress these activations to a $d$ dimensional vector representing this observation state. The embedding and projection step together form our key ( $\&amp;$ query $^{4}$ ) network $k_{i}=g_{\phi}\left(o_{i}\right)$.} \in \mathbb{R}^{d}$, and a given observation $o_{t}$ to be mapped to a query vector $q_{t} \in \mathbb{R}^{d}$. During retrieval, the squared Euclidean distances between $q_{t}$ and the dataset keys $k_{i</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Details of the architecture used for a retrieval-augmented Go playing agent. A pre-trained network is used to generate a query $q_{t}$ corresponding to the current Go game state $o_{t}$. This query is used for fast approximate nearest-neighbor retrieval using SCaNN. Retrieved neighbors $x_{t}^{n}$ are processed using an invariant architecture, and used to inform an action-conditional recurrent forward model that outputs game outcome predictions $\hat{v}^{k}$ and distributions over next actions $\hat{\pi}^{k}$.</p>
<p>We preprocess all of the observations in $\mathcal{D}<em _phi="\phi">{r}$ using $g</em>$. However, to act during evaluation, we must do this nearest-neighbor lookup online for each new observation. In future experiments, we intend to incorporate end-to-end learning of the query vector [12] to improve agent performance. A future online RL pipeline will also require us to dynamically retrieve neighbors during training.}$ to produce corresponding keys. The resulting dataset of keys and observations is then used by ScaNN to retrieve neighbors given a query vector. For offline RL training with a fixed training dataset, we can also preprocess the nearest neighbor lookups for all training dataset observations, i.e., $q_{t}=g_{\phi}(o) \forall o_{t} \in \mathcal{D</p>
<h1>2.1.2 Policy optimization with retrieval</h1>
<p>The objective in our offline RL setting is to leverage the available data in order to optimize a policy $\pi$ for acting. Our proposed semi-parametric model (Eqn. 1) is compatible with several methods for policy optimization in this offline setting. Indeed, it can represent the actor $\pi_{\theta}\left(o_{t}, \mathcal{D}<em _theta="\theta">{r}\right)$ and/or the critic $Q</em>\right)$ in an offline actor-critic method [21], or the Q-value in a value iteration approach [32]. Here we focus on a model-based approach, inspired by MuZero [36], where the learned model is employed in a search procedure based on Monte-Carlo Tree Search (MCTS) [8]. This is motivated by the efficacy of MuZero in the offline RL setting [37, 24].}\left(o_{t}, a, \mathcal{D}_{r</p>
<p>Model-based search with retrieval Following MuZero, our prediction model $m_{\theta}$ is conditioned on the current observation $o_{t}$ but also on a sequence of future actions $\vec{a}<em t_1="t+1">{t}=a</em>}, a_{t+2}, \ldots, a_{t+K}$. The model $m_{\theta}$ is therefore redefined as $\hat{y<em _theta="\theta">{t}=m</em>}\left(o_{t}, \vec{a<em t="t">{t}, x</em>\right)$ to produce embeddings}^{1}, x_{t}^{2}, \ldots, x_{t}^{N}\right)$. The architecture for this retrieval prediction model is illustrated in Fig. 2. The model $m_{\theta}$ can be decomposed into an encoding step $s_{t}=f_{\theta}\left(o_{t}, x_{t}^{1}, x_{t}^{2}, \ldots, x_{t}^{n}\right)$, which incorporates the observation and neighbor information, followed by iterative action-conditioned inference $s_{t}^{k+1}=h_{\theta}\left(s_{t}^{k}, a_{t+k</p>
<p>$s_{t}^{k}$ corresponding to subsequent time steps (where $s_{t}^{0}=s_{t}$ ). The model output $\hat{y}<em t="t">{t}$ is composed of $K+1$ value and policy distributions for the current and next $K$ time-steps: $\left{\hat{v}</em>}^{k}, \hat{\pi<em K="K" _ldots="\ldots" k="0">{t}^{k}\right}</em>} .{ }^{5}$ These outputs are obtained from their respective embeddings $s_{t}^{k}$. The target for value predictions is the game outcome and the targets for the policy predictions are the actions taken by the expert $a_{t}, a_{t+1}, \ldots, a_{t+K}$. The sample loss $\mathcal{L}\left(\theta, \hat{y<em t="t">{t}, y</em>\right)$ is obtained by summing the $K+1$ individual loss terms for value and policy predictions (see detail in App. A.2). The training procedure for this retrieval prediction model is summarized in Alg 1.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Training semi-parametric action-conditional model
Input: Training dataset \(\mathcal{D}\) and retrieval dataset \(\mathcal{D}_{r}\)
    Initialize parameter vectors \(\theta, \phi\).
    Train key/query network \(g_{\phi}\) using \(\mathcal{D}\).
    Pre-compute \(k_{i}=g_{\phi}\left(o_{i}\right)\) for all \(o_{i} \in \mathcal{D}_{r}\).
    for each gradient step do
        Sample mini-batches from \(\mathcal{D}\) of \(\left(o_{t}, y_{t}\right)\).
        For each element \(o_{t}\) of the batch, compute \(q_{t}=g_{\phi}\left(o_{t}\right)\).
        Fetch \(N\) neighbor keys with (approx.) smallest distance \(\left\|q_{t}-k_{i}\right\|_{2}^{2}\) for all \(k_{i} \in \mathcal{D}_{r}\).
        Gather meta-data associated with these keys as \(x_{1}^{1}, \ldots x_{t}^{n}\).
        Compute model output \(\hat{y}_{t}=m_{\theta}\left(o_{t}, \vec{a}, x_{t}^{1}, \ldots, x_{t}^{N}\right)\).
        Compute and sum losses \(\mathcal{L}\left(\theta, \hat{y}_{t}, y_{t}\right)\).
        Update \(\theta\) based on \(\nabla_{\theta} \mathcal{L}\).
    end for
    output \(\theta, \phi
</code></pre></div>

<p>In order to act using the trained model, online search is used to generate an improved policy $\pi_{s}=\operatorname{Search}\left(m_{\theta}, \mathcal{D}<em _sims="{sims" _text="\text">{r}\right)$. We use MCTS with a pUCT rule for internal action selection [35, 40], carrying out $n</em>\right)$ can be sampled to select the next action.
Due to the semi-parametric nature of the model supporting the search, introducing changes to $\mathcal{D}}}$ simulations per time step. Model-based search is implemented as follows: after retrieving the observation's neighbors, the encoded state $s_{t}$ is computed. Search is carried out from $s_{t}$ by varying the input action sequence $\vec{a}$ for each simulation and collecting the model outputs to update search statistics. Details of this process can be found in [36]. At the end of the search, the resulting policy $\pi_{s}\left(a \mid o_{t<em s="s">{r}$ will have an immediate effect on the acting policy $\pi</em>$, even if the model parameters $\theta$ remain fixed. As we explore in Sec. 3.4, this provide a mechanism for fast adaption to new information or experiences.</p>
<h1>2.1.3 Using neighbor information</h1>
<p>Several choices are possible for the network $f_{\theta}$ that processes the observation and neighbors. Since this is not the main focus of this work, we chose a straightforward approach. We first compute an embedding $o_{t}^{e}$ of the observation $o_{t}$. We then compute an embedding for each neighbor $e_{i}^{t}=$ $p_{\theta}\left(o_{t}^{e}, x_{i}^{t}\right)$, using the same network $p_{\theta}$ for all neighbors. These streams are combined in a permutation invariant way through a sum, and then concatenated with $o_{t}^{e}$ to produce the final embedding $s_{t}$, :</p>
<p>$$
s_{t}=f_{\theta}\left(o_{t}, x_{1}, \ldots, x_{N}\right)=\left[o_{t}^{e}, \frac{\sum_{i=1}^{N} e_{i}^{t}}{\sqrt{N}}\right]
$$</p>
<h3>2.2 Evaluating retrieval in the domain of Go</h3>
<p>In order to test the utility of retrieval in RL, we wish to evaluate whether agents can effectively generalise between related but distinct experiences, as opposed to simply retrieving the outcome of a previous instance of an identical situation. This motivates our choice of Go as a domain. We focus on 9 x 9 Go, instead of full-scale 19 x 19 Go, as the less demanding computational costs of 9 x 9 experiments enable a more thorough analysis. Even for 9 x 9 Go, the state space of $10^{38}$ possible games is vastly larger than the number of positions that we could hope to query.
We collected a dataset of $\sim 3.5 \mathrm{M}$ expert 9x9 Go self-play games from an AlphaZero-style agent [40]. We randomly subsampled $15 \%$ of the positions from these games, leaving us with $\sim 50 \mathrm{M}$ board</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>state observations. These, along with metadata on the game outcome, final game board, and future actions (all encoded as input planes), form our retrieval dataset $\mathcal{D}<em r="r">{r}$. We chose to subsample as we hypothesised that this would reduce the chance of retrieving multiple neighbors from the same trajectory, and therefore boost the retrieved neighbor diversity. However, we did not perform ablations on this choice. A future solution would be to use a filtering mechanism to reject neighbors from the same trajectory. In this initial work, training and retrieval datasets are the same - at least during training. During training, we split $\mathcal{D}</em>$ from the half it is not contained in. This is simply to avoid retrieving the same position as the query. Other ways to obtain the same effect could be devised.}$ in two halves such that each game's observations are only in one of the datasets. We retrieve neighbors for an observation $o_{t</p>
<h1>2.3 Regularisation</h1>
<p>We wish to make our network robust to poor quality neighbors, in order to ensure that the network can perform well in settings for which there is lower overlap with the retrieval dataset than encountered in training. We therefore explore several techniques to improve network robustness to irrelevant neighbors. We randomly zero-out a subset of retrieved neighbors during training ("neighbor dropout"), and/or more adversarially, randomly replace a subset of retrieved neighbors with the neighbors of a different observation ("neighbor randomisation"). Inspired by [10], we also explore using a loss to regularise the embedding produced by the neighbor retrieval towards the embedding produced with the observation alone ("neighbor regularisation"). Further details are given in Sec. A.5.
We carried out ablations of these techniques (Appendix Fig. 7), which show that neighbor randomisation is important in some contexts for maintaining performance, but that the others do not seem to have a significant effect in our final configuration. The results reported in this study utilise all of these augmentations, as during the development process they had been found to slightly benefit performance. Interestingly, as we explore in later sections, MCTS with enough simulations compensates for the harmful effect of low-quality neighbors, and can perform effectively without these training augmentations.</p>
<h2>3 Results</h2>
<h3>3.1 Qualitative examination of retrieved neighbors</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Visualisation of $N=4$ approximate nearest-neighbors retrieved using a learned Go-specific distance function for 3 query positions (one per row). Red stone indicates the next action(s) - light red stones indicate white moves, dark red stones indicate black moves. The color of the board border indicates whether the current player to play (for each board) won the game.</p>
<p>Changing a single stone in a Go board can dramatically affect the game, and the number of Go positions is enormous. We first wanted to assess whether, despite the combinatorial aspect of the domain, meaningful nearest-neighbors could be retrieved from the dataset using our learned keys/queries. Examples of retrieved positions in Go are shown in Fig. 3, where we observed relevant matches in terms of both local and global structure. In some cases, especially in the early game, the retrieved position is an exact match even though the rest of the game (and therefore the associated nearest-neighbor meta-data) differs. Row 1 in Fig. 3 is an example of this - in this case, the retrieved data effectively provides sample rollouts from the query position akin to those derived from MCTS.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: a) Test-set top-1 policy prior $\hat{\pi}^{0}$ accuracy (top) and value $\hat{v}^{0}$ mean-squared error (MSE) (bottom) over the course of training, for the (non-retrieval) baseline and retrieval models with $N=2,10$. Results are averaged over 3 seeds (error typically too small to see). Note that the sharp transitions are due to the learning rate schedule. b) Final performance after training plotted as a function of number of neighbors $N$ retrieved - a different network is trained for each number of neighbors. c) Final performance as a function of model size relative to the size used elsewhere in this study, for networks with $N=10$ retrieved neighbors and the non-retrieval baseline. d) Final performance as a function of the evaluation retrieval dataset size, for networks with $N=10$ retrieved neighbors. The equivalent baseline performance is shown as a dashed line for reference. Note that the networks are trained with dataset fraction $=0.5$ and simply evaluated at other dataset fractions.</p>
<h1>3.2 Impact of retrieval on supervised training</h1>
<p>We next evaluated the extent to which the model can learn to exploit retrieved information for better model predictions and better decision-making. First, we evaluated the impact of retrieval on supervised learning losses. As a baseline comparison, we trained the same network architectures but setting $x_{i}=0$ for all $i$ - this has the effect of maintaining the number of parameters, flops, and useful capacity in the network, while removing access to the retrieval data. We evaluated losses for retrieval networks trained with different $N$ (Fig. 4b) and model sizes (Fig. 4c, see Sec. A. 1 for details). Across conditions, we consistently observed a significant boost in test-set accuracy for all metrics and over the course of training. This improvement to predictions is further observed across game trajectories, and is not limited, for example, to opening play positions.
One advantage of the semi-parametric approach we outlined is that we can modify the retrieval dataset and potentially see immediate effects on the prediction, without changing the parameters $\theta$. We first verified this by evaluating our model when allowed access to varying fractions of the full dataset $\mathcal{D}<em r="r">{r}$ (Fig. 4d). We observed clear gains in prediction accuracy from increasing the size of the retrieval set (only half is used at train time). A further important observation is that large-scale datasets are clearly important - the evaluation metrics drop below the baseline level for a dataset $\mathcal{D}</em>$ that is $1 \%$ the size of the full dataset.</p>
<h1>3.3 Evaluation against a reference opponent</h1>
<p>While the results observed on the offline dataset suggest a strong positive effect from learning to leverage the retrieval data, this is in the context of a fixed test data distribution that matches the retrieval data distribution. When deployed, games played against different opponents will likely diverge from that distribution. We evaluated the performance of our search policy $\pi_{s}\left(n_{\text {sims }}=200\right)$ by playing against a fixed reference opponent - the Pachi program [5], which can perform beyond strong amateur level of play in 9x9 Go given sufficient simulation budget (we evaluate against 400 k simulations). We observed a significant boost to performance for retrieval-based networks over the equivalent baseline network of the same capacity (Fig. 5). Interestingly, as shown in Appendix Fig. 8, playing using only the base policy prior $\hat{\pi}^{0}$ shows a much smaller boost over the baseline network. As we explore further below, the quality of retrieved neighbors available during play is much lower than for training - we hypothesise that the search policy is more robust to this distributional shift than $\hat{\pi}^{0}$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Win rate against a fixed reference opponent (Pachi) when playing using the MCTS policy $\pi_{s}$ for (a) retrieval networks using varying numbers of retrieved neighbors and for a baseline non-retrieval network. (b) Win rate as a function of model size relative to the size used elsewhere in this study. Retrieval leads to a clear performance boost compared to non-retrieval baselines of the same capacity.</p>
<h3>3.4 Augmenting the retrieval dataset</h3>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: (a) Distribution of similarity distances from an encoded observation to its retrieved nearest neighbors. Retrieving from the original retrieval dataset using positions from the test dataset (orange) gives neighbors with a markedly different distance distribution to positions queried during play against a Pachi opponent (blue). Augmenting the retrieval dataset with $\sim 600 \mathrm{k}$ recorded agent-Pachi game states improves the similarity distribution (green). (b) For play with the MCTS policy $\pi_{s}$, this augmentation also leads to a consistent win-rate boost (green) over using the train retrieval dataset alone (blue).</p>
<p>We observed a significant distribution shift between game positions observed in play against the Pachi opponent versus positions in the retrieval dataset (Fig. 6a), as measured by the empirical distance distribution of game observations to their approximate nearest neighbors. Changing or augmenting the retrieval dataset $\mathcal{D}_{r}$ modifies this distributional shift. A particularly interesting modification in this setting is to augment the dataset with play recorded between our agents and the Pachi opponent. As can be seen in Fig. 6a, augmenting the dataset with a set of $\sim 600 \mathrm{k}$ agent-Pachi game states increases</p>
<p>the similarity between positions observed in play and their retrieved neighbors. Strikingly, we found that integrating these games into our retrieval dataset leads to improvements in performance without further training for subsequent play against the Pachi opponent (play with the MCTS policy $\pi_{s}$ is shown in Fig. 6b, play with the policy prior $\bar{\pi}^{0}$ is shown in Appendix Fig. 9). This highlights the potential of a semi-parametric model to rapidly integrate recent experience without further training.
As a related intervention, we instead inputted randomly retrieved neighbors into the model at evaluation time (see Appendix Fig. 6). This significantly impaired performance for play using the policy prior $\bar{\pi}^{0}$, but somewhat unexpectedly, for play with the MCTS policy $\pi_{s}$, performance did not fully regress to the level of the baseline network. This suggests that our retrieval network is able to amortise some information from neighbors during training, leading to better performance even when no relevant neighbors are available.</p>
<h1>4 Related work</h1>
<p>The idea of supporting decision making by directly attending to a cache of related events has been visited many times in different contexts, under the names of case-based reasoning [30], nonparametric RL [16, 4, 1, 27], or episodic memory [20, 23]. The aim of some methods is to better support the working memory of an agent during a given episode [26, 42] or a series of successive related episodes [33]. Other methods, including ours, aim to leverage a broader class of relevant experience or persistent knowledge (including across episodes, and from other agents) to better support reasoning and planning. One differentiating factor in our work to these past approaches is that we do not prescribe how to process the information from the available data (e.g. through specifying the agent's action-value directly in terms of previously generated value estimates [6, 13, 15, 29], or a model from observed transitions [39]) but rather learn end-to-end how the data can support better predictions within the parametric model. A recent approach by Goyal et al. [10] has considered an attention mechanism to select where and what to use from available trajectories, but over a small retrieval batch of data rather than the full available experience data. Another class of method to leverage a transition dataset is to replay the data at training time in order to perform more gradient steps per experience, this is a widespread technique in modern RL algorithms [21, 22, 25, 36] but it does not benefit the agent at test time, requires additional learning steps to adapt to new data, and does not allow end-to-end learning of how to relate past experience to new situations.</p>
<h2>5 Discussion</h2>
<p>Our approach and empirical results highlight how reinforcement learning agents can benefit from direct access to a large collection of raw interaction data at inference time, through a retrieval mechanism, in addition to their already effective parametric representation. We showed this was the case even 1) when the domain is large enough to require generalisation in how to interpret past data, 2) when there is significant distribution shift when acting, and 3) at a scale where only approximate nearest-neighbors can be retrieved. We believe this already demonstrates the potential of this approach for many possible scenarios and applications.
We show that retrieval can be effectively combined with model-based search. We find that the benefits from retrieval and search are synergistic, with increasing numbers of retrieved neighbors and increasing simulations both leading to performance increases in almost all contexts we investigated. Furthermore, empirical evidence suggests that search significantly improves agent robustness to distributional shift as compared to playing with the policy prior. In Appendix Fig. 10, we compare the boost in performance as a function of parametric-model compute cost for increasing MCTS simulations versus increasing the number of retrieved neighbors processed by the model. This provides a tentative indication that retrieval is also a compute efficient means of improving performance.
A key future direction is to investigate the online learning scenario in which recent experience of the agent is rapidly made available for retrieval, hence progressively growing the retrieval dataset over the course of training. While there are additional challenges associated with the online paradigm (e.g., it may be desirable to update the queries and/or keys during training [12]), the fast adaptation effect we highlighted in this work may have even more impact there.
There are many potentially relevant sources of information beyond an agent's own experience, or that of other humans or agents. For example, it has been shown that YouTube videos are useful</p>
<p>for learning to play the Atari game Montezuma's revenge [2]. Training an embedding network on sufficiently diverse data may enable retrieval of information from a wide range of contexts [43], including third-person demonstrations, videos and perhaps even books.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>The authors received no specific funding for this work.</p>
<h2>References</h2>
<p>[1] Christopher G. Atkeson, Andrew W. Moore, and Stefan Schaal. Locally weighted learning for control. Artificial Intelligence Review, 11:75-113, 2004.
[2] Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando De Freitas. Playing hard exploration games by watching youtube. Advances in neural information processing systems, 31, 2018.
[3] Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci, Jonathan Godwin, Chris Jones, Tom Hennigan, Matteo Hessel, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Lena Martens, Vladimir Mikulik, Tamara Norman, John Quan, George Papamakarios, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Wojciech Stokowiec, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/deepmind.
[4] André MS Barreto, Doina Precup, and Joelle Pineau. Practical kernel-based reinforcement learning. The Journal of Machine Learning Research, 17(1):2372-2441, 2016.
[5] Petr Baudiš and Jean-loup Gailly. Pachi: State of the art open source Go program. In Advances in computer games, pages 24-38. Springer, 2011.
[6] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.
[7] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2021.
[8] Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 72-83. Springer, 2006.
[9] Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Advances in Neural Information Processing Systems, 32, 2019.
[10] Anirudh Goyal, Abram L Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez, Mehdi Mirza, Ksenia Konyushkova, Michal Valko, et al. Retrieval-augmented reinforcement learning. arXiv preprint arXiv:2202.08417, 2022.
[11] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887-3896. PMLR, 2020.
[12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.
[13] Steven Hansen, Alexander Pritzel, Pablo Sprechmann, André Barreto, and Charles Blundell. Fast deep reinforcement learning using online adjustments from the past. Advances in Neural Information Processing Systems, 31, 2018.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630-645. Springer, 2016.</p>
<p>[15] Mika Sarkin Jain and Jack W Lindsey. Semiparametric reinforcement learning. In ICLR, 2018.
[16] H. JoséAntonioMartín, Javier de Lope Asiaín, and Darío Maravall Gómez-Allende. The knn-td reinforcement learning algorithm. In IWINAC, 2009.
[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 1-12, 2017.
[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[19] Andrew Lampinen, Stephanie Chan, Andrea Banino, and Felix Hill. Towards mental time travel: a hierarchical memory for reinforcement learning agents. Advances in Neural Information Processing Systems, 34, 2021.
[20] Máté Lengyel and Peter Dayan. Hippocampal contributions to control: the third way. Advances in neural information processing systems, 20, 2007.
[21] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
[22] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3):293-321, 1992.
[23] Zichuan Lin, Tianqi Zhao, Guangwen Yang, and Lintao Zhang. Episodic memory deep qnetworks. arXiv preprint arXiv:1805.07603, 2018.
[24] Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii unplugged: Large scale offline reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021.
[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
[26] Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, et al. Control of memory, active perception, and action in minecraft. In International Conference on Machine Learning, pages 2790-2799. PMLR, 2016.
[27] Dirk Ormoneit and Śaunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2): $161-178,2002$.
[28] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International Conference on Machine Learning, pages 7487-7498. PMLR, 2020.
[29] Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In International Conference on Machine Learning, pages 2827-2836. PMLR, 2017.
[30] Ashwin Ram and Juan Carlos Santamaria. Continuous case-based reasoning. Artificial Intelligence, 90(1-2):25-77, 1997.
[31] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.
[32] Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. In European conference on machine learning, pages 317-328. Springer, 2005.</p>
<p>[33] Sam Ritter, Ryan Faulkner, Laurent Sartran, Adam Santoro, Matt Botvinick, and David Raposo. Rapid task-solving in novel environments. arXiv preprint arXiv:2006.03662, 2020.
[34] Samuel Ritter, Jane Wang, Zeb Kurth-Nelson, Siddhant Jayakumar, Charles Blundell, Razvan Pascanu, and Matthew Botvinick. Been there, done that: Meta-learning with episodic recall. In International conference on machine learning, pages 4354-4363. PMLR, 2018.
[35] Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence, 61(3):203-230, 2011.
[36] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.
[37] Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver. Online and offline reinforcement learning by planning with a learned model. Advances in Neural Information Processing Systems, 34, 2021.
[38] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889-1897. PMLR, 2015.
[39] Aayam Shrestha, Stefan Lee, Prasad Tadepalli, and Alan Fern. Deepaveragers: offline reinforcement learning by solving derived non-parametric mdps. arXiv preprint arXiv:2010.08891, 2020.
[40] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144, 2018.
[41] Pablo Sprechmann, Siddhant M Jayakumar, Jack W Rae, Alexander Pritzel, Adria Puigdomenech Badia, Benigno Uria, Oriol Vinyals, Demis Hassabis, Razvan Pascanu, and Charles Blundell. Memory-based parameter adaptation. arXiv preprint arXiv:1802.10542, 2018.
[42] Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka GrabskaBarwinska, Jack Rae, Piotr Mirowski, Joel Z Leibo, Adam Santoro, et al. Unsupervised predictive memory in a goal-directed agent. arXiv preprint arXiv:1803.10760, 2018.
[43] Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. Vlm: Task-agnostic video-language model pre-training for video understanding. arXiv preprint arXiv:2105.09996, 2021.</p>
<h1>A Appendix</h1>
<p>The appendix details the architecture and training mechanisms for our results, it also lists additional ablations and results.</p>
<h2>A. 1 Architecture</h2>
<p>Observations consist of 9x9 feature planes representing the current state of the board (indicating stones for the last two moves, and the player color). The networks we train are convolutional, composed of residual blocks, and maintain the 9x9 topology throughout - except for the final output layers.
As illustrated in Figure 1, the main inference network $s_{t}=f_{\theta}\left(o_{t}, x_{t}^{1}, x_{t}^{2}, \ldots, x_{t}^{n}\right)$ is composed of an encoding of the observation $o_{t}$ using $m_{\text {enc }}=2$ residual $3 \times 3$ convolutional blocks with 256 channels (each block consists of two convolutional layers with a skip connection) [14], preceded by a first convolution layer with 256 filters and followed by a convolution layer that goes back to 128 channels.
Each neighbor information $x^{i}$ is concatenated separately with the encoded observation $o^{e}$ to be processed by another residual network $p_{\theta}$ (blue layers in Fig. 1), but with $m_{\mathrm{nn}}=3$ blocks and 256 channels. The neighbor information, in addition to the feature planes corresponding to the retrieved position, also include 1-hot 9 x 9 input planes for the 10 next actions, a tiled input plane for the game outcome, and the feature planes corresponding to the final game board. The parameters for each neighbor processing stream are shared. The output from each of the neighbors $e_{i}$ are then summed (and normalized) and concatenated back with the encoded observation along the channel dimension, see Eq. 2. The concatenated tensor is processed by $m_{\text {root }}=6$ additional residual blocks with the same hyperparameters to produce the main output embedding $s_{t}=s_{t}^{0}$ (top pink layers in Fig. 1).
Each internal transition in the recurrent action-conditional model (in purple Fig. 1) takes the previous embedding $s_{t}^{k}$, concatenates a one-hot representation of the action $a_{t+k}$, and outputs the next embedding $s_{t}^{k+1}$. These internal models are composed of $m_{\text {tran }}=3$ residual blocks preceded by a convolution layer to bring the activations back to 256 layers. All internal transitions, and output networks for $k&gt;1$, share parameters. Non-linear transformations after each layer are rectified linear units (ReLU), preceded by a layer norm layer applied spatially within the residual blocks. The model outputs are obtained at each step $k$ from the embedding $s_{t}^{k}$ with a value and policy head. The value head first applies a single $1 \times 1$ convolution filter, followed by a ReLU, a flattening operation, and an MLP with a single hidden layer of 256 units to a final scalar output passed through a tanh non-linearity to obtain $\hat{v}<em t="t">{t}^{k}$. The policy head follows the same pattern, but applies two initial convolution filters, and the final output has dimension $82=9 \times 9+1$ (all possible actions including pass in 9 x 9 Go) and corresponds to the logits for $\hat{\pi}</em>$.
Together, this describes the parametric network $m_{\theta}$. The search policy $\pi_{s}=\operatorname{Search}\left(m_{\theta}, \mathcal{D}}^{k<em t="t">{r}\right)$ employs the trained network to plan and act. The main embedding $s</em>$ with different input action sequences for a given root observation. The details of the MCTS search mechanism using the trained model follows [36] closely, we refer the reader to that paper for details.
For experiments that vary the model size, model size of 1 is as described above. For model sizes $n=2 \&amp; 4$, we multiply $m_{\text {root }}, m_{\text {tran }}, m_{\text {enc }}$, and $m_{\text {nn }}$ by a factor of 2 , with the number of channels at all layers doubled for $n=4$. The model size labeled 0.5 corresponds to $m_{\text {root }}=3, m_{\text {tran }}=2, m_{\text {enc }}=1$, and $m_{\mathrm{nn}}=2$.}=f_{\theta}\left(o_{t}, x_{t}^{1}, x_{t}^{2}, \ldots, x_{t}^{n}\right)$, and the retrieval process to obtain the neighbors, is computed once per search. Multiple rollouts can be computed from $s_{t</p>
<h2>A. 2 Losses and optimization</h2>
<p>The total loss, for a single datapoint, is:</p>
<p>$$
\mathcal{L}\left(\theta, y, y^{*}\right)=\sum_{k=0}^{K} w_{k}\left(l^{p}\left(\hat{\pi}<em t_1_k="t+1+k">{t}^{k}, a</em>
$$}\right)+l^{v}\left(\hat{v}_{t}^{k}, g\right)\right)+\alpha|\theta|^{2</p>
<p>with $w_{k}=\frac{1}{K}$ for $k&gt;0$ and 1 otherwise ( $K=5$ in our experiments), $g$ the empirical return (the $-1,1$ relative game outcome for Go), and $\alpha=1 \mathrm{e}-4$ controls the weight decay. The individual loss</p>
<p>terms are $l^{v}(v, g)=\frac{1}{2}(v-g)^{2}$ for the value outputs (MSE when averaged over the batch elements), and $l^{p}(\pi, a)=-\log (\pi(a))$ for the policy outputs. In addition, an optional regularisation loss is described below in Sec. A.5.</p>
<p>We use the Adam optimizer [18] with a learning rate schedule, training for 600 k steps with a batch size of 1024. The learning rate is the initial learning rate divided by ${2,8,64,256}$ after respectively [180k, 360k, 480k, 570k} steps, with the initial learning rate of 1e-3. Training was performed on TPU [17] using JAXline and Mctx within the DeepMind JAX Ecosystem [3].</p>
<h1>A. 3 Go game configuration</h1>
<p>The komi throughout the experiments is 5.5. The Pachi program is configured with 16 threads, a max_tree_size of 2000, a resign_threshold of 0.1 , no pondering, and the Chinese ruleset. The search policy $\pi_{s}$ when testing is defined as the action with the maximum visit counts. To ensure diversity in the evaluation games against Pachi, we seed each game by letting Pachi play the first 6 moves in the opening. After an evaluation game goes past the opening moves and 50 subsequent moves, we stop the game if the value function is above 0.995 , or below 1-0.995, to avoid playing long end-games when the game is already clearly settled.</p>
<h2>A. 4 Embedding network</h2>
<p>To train the embedding $g_{\phi}$ to map observations $o_{t}$ to keys and queries, we first train a MuZero model $m_{\phi}^{e}\left(o_{t}, \vec{a}<em t="t">{t}\right)$ following the same method as in A.1-A.2. We train this on the same data $\mathcal{D}$ that we train the retrieval network on, but without any retrieval input or neighbor processing network. We also use 8 residual blocks for the main encoding stage to compute $s</em>\right)$, and 3 residual blocks per internal model transition.}=f_{\phi}\left(o_{t</p>
<p>At the end of training, we choose the output of one of the $f_{\phi}$ network layers as a (pre)-embedding $\bar{g}<em _phi="\phi">{\phi}(o)$. To reduce the dimensionality of $\bar{g}</em>}$, we project it using PCA into the first $d=512$ principal components to obtain $g_{\phi}(o)=\left(\bar{g<em _phi="\phi">{\phi}(o)-\mu\right) V \in \mathbb{R}^{d}$, where $V$ contains the first $d$ eigenvectors of the covariance matrix of the pre-embedding vectors, estimated with a subset of the data after removing their mean $\mu$.
In our experiments, we used the output of the 6th residual block of $f</em>(o)$. We have not extensively searched for the best such embedding due to the expense of testing each - further investigation of this design choice will be the subject of future research.}$ as $\bar{g}_{\phi</p>
<h2>A. 5 Regularisation ablations</h2>
<p>Here we provide more detail on techniques we investigated to improve network robustness to low quality retrieved neighbors (as introduced in Sec. 2.3).</p>
<h2>Neighbor dropout</h2>
<p>We randomly zero-out a subset of retrieved neighbors during training. For $N$ retrieved neighbors, the number of neighbors to zero out $M$ is uniformly randomly chosen from $[0, N]$. We then randomly choose $M$ out of $N$ neighbors, and mask out their contribution to the final invariant neighbor embedding (Sec. 2.1.3).</p>
<h2>Neighbor randomisation</h2>
<p>We randomly replace a subset of retrieved neighbors with the neighbors of a different observation within the mini-batch. This is implemented by randomly choosing $M$ neighbors as described for dropout above, and then replacing each by a randomly chosen member of the mini-batch (which may with a small chance be the correct neighbor).</p>
<h2>Neighbor regularisation</h2>
<p>Inspired by [10], we also explore using a loss to regularise the embedding produced by the neighbor retrieval towards the embedding produced with the observation alone. In this case, we apply a convolutional layer to the embedding produced from the game state observation, and use this to</p>
<p>predict the output of the neighbor processing tower. We apply a mean-squared-error loss to encourage the base embedding to be predictive of the neighbor output, and similarly apply an equivalent loss to the neighbor output.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Ablations of different regularisation techniques explored in this study, as measured by evaluating the win rate against the Pachi reference opponent. Win rates are shown for (a) the MCTS policy $\pi_{s}$ and (b) the policy prior $\hat{\pi}^{0}$. The most critical regularisation is neighbor randomisation. Without this randomisation, the retrieval network prior $\hat{\pi}^{0}$ performs significantly worse than the baseline non-retrieval network. The other regularisation methods do not have a significant effect on performance.</p>
<h1>A. 6 Additional results</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Equivalent figure to Fig. 5, but for playing with the policy prior $\hat{\pi}^{0}$, as opposed to with the MCTS policy $\pi_{s}$. Win rate against a fixed reference opponent (Pachi) for (a) retrieval networks using varying numbers of retrieved neighbors and a baseline non-retrieval network. (b) Win rate as a function of model size relative to the size used elsewhere in this study.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: (a) Extended version of Fig. 6b: Changes to win rate for the MCTS policy $\pi_{s}$ against the Pachi opponent without further training by adjusting the retrieval dataset. Augmenting the dataset with agent-Pachi games (green) leads to a consistent boost in performance over using the train retrieval dataset by itself (blue), and over retrieving from only agent-Pachi games (red). Using randomly chosen neighbors (dashed, yellow) hurts performance, but does not regress to the level of the non-retrieval network of the same capacity (purple). (b) Playing with the policy prior $\hat{\pi}^{\mathrm{S}}$, as opposed to with the MCTS policy $\pi_{s}$. In this setting, the retrieval network performs at only a slightly higher level than the baseline, suggesting that prior play is less robust to the distributional shift from training to playing against Pachi than for MCTS. Nonetheless, as for MCTS, augmenting the dataset with games recorded against Pachi leads to a boost in performance. Using randomly chosen neighbors brings performance below the baseline, showing that our networks are not fully robust to invalid neighbors.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Change in win rate as a function of additional parametric-model computational cost for processing increasing numbers of retrieved neighbors versus increasing the number of MCTS simulations. The comparison is made relative to a baseline non-retrieval network using 50 MCTS simulations. In our configuration, each MCTS simulation uses the same amount of compute as processing one neighbor. Note that this comparison does not account for the computational cost of querying for and retrieving nearest neighbors.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ In cases where non-terminal rewards exist, we also output reward estimates for each model transition.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
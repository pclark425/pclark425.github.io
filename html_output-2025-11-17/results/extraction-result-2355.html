<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2355 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2355</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2355</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-442e10a3c6640ded9408622005e3c2a8906ce4c2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/442e10a3c6640ded9408622005e3c2a8906ce4c2" target="_blank">A Unified Approach to Interpreting Model Predictions</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations), which unifies six existing methods and presents new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.</p>
                <p><strong>Paper Abstract:</strong> Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2355.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2355.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHapley Additive exPlanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified framework that defines feature attributions as Shapley values of a conditional expectation function, producing additive explanations that satisfy local accuracy, missingness, and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Model interpretability / explanation of ML predictions (applied to image classification, text, and tabular models)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide consistent, locally accurate feature attributions for individual model predictions from complex models (ensembles, deep networks) so users can understand and trust predictions and gain insight into modeled processes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies by application; paper demonstrates on standard benchmark MNIST (abundant labeled images) and on datasets used for sampling-based Shapley estimates (training data used for conditional expectations); conditional expectation estimation requires access to representative training data; no single scarcity claim.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular data, bag-of-words text, images (e.g., MNIST), and high-dimensional/sparse feature vectors (decision-tree examples).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: models are nonlinear and high-dimensional (deep nets, ensembles); exact Shapley computation combinatorial (2^M subsets) creating exponential computational complexity; requires handling feature dependence / conditional expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Active and maturing subfield within ML; builds on established game-theory Shapley values and existing interpretability methods (LIME, DeepLIFT, Shapley sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — interpretability is required for user trust, debugging, and deriving insight; SHAP enforces interpretability properties as core requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>SHAP (Shapley value-based additive feature attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Defines an explanation model g(z') = phi0 + sum_i phi_i z'_i where phi_i are Shapley values computed on f_x(z') = E[f(z) | z_S]; uses conditional expectations to approximate effect of missing features and enforces three axioms (local accuracy, missingness, consistency). Practical computation is via exact Shapley formula when possible or approximations (Kernel SHAP, Shapley sampling, model-specific approximations).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / model explanation (game-theory based)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable broadly to any predictive model and input type; limitations include computational cost for many features and the need to approximate conditional expectations (may assume feature independence or use training-data sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Exact Shapley values are combinatorial; paper reports use of 50k model evaluations in MNIST comparisons for sampling-based methods; regression-based Kernel SHAP shows improved sample efficiency vs. Shapley sampling in experiments (figures) but no single overall accuracy number is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>SHAP explanations are theoretically unique under stated properties and are shown to be more consistent with human intuition than alternatives (LIME, DeepLIFT); Kernel SHAP improves sample efficiency vs. sampling baselines; model-specific approximations (Deep SHAP, Linear SHAP, Max SHAP) make SHAP practical for specific model families.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — provides a principled, unified explanation framework that can increase trust, improve debugging of complex models, and be adapted for many model types; potential to standardize explanation methods across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to LIME, DeepLIFT, and Shapley sampling in experiments: SHAP (via Kernel/Deep approximations) gave explanations that matched human judgments better and required fewer model evaluations than sampling-based Shapley; LIME and original DeepLIFT sometimes violated desirable properties and produced less intuitive attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Theoretical uniqueness from Shapley axioms (local accuracy, missingness, consistency), using conditional expectation mapping for missing features, regression-based Kernel SHAP for sample efficiency, and compositional approximations (Deep SHAP) that exploit model structure.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Formulating local feature attributions as Shapley values of conditional expectations yields a unique, axiomatic explanation and, with tailored approximations (Kernel/Deep), makes theoretically-grounded explanations computationally practical and human-aligned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2355.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2355.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kernel SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kernel SHAP (regression-based, model-agnostic SHAP approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic approximation to SHAP that frames Shapley value computation as a weighted linear regression problem with a specific 'Shapley kernel' weighting, improving sample efficiency over direct Shapley sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Model interpretability / explaining individual predictions for arbitrary ML models (applied to decision trees and MNIST in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Efficiently estimate SHAP values (feature attributions) for arbitrary models where exact Shapley computation is infeasible due to combinatorial subset space.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires access to model evaluation (predict function) and representative samples or the training data to approximate conditional expectations; in experiments used both small and large sample budgets (e.g., 50k evaluations in MNIST comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Works on any input type (binary simplified inputs mapping from original images, text, or structured features); examples include images and tabular/sparse features.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Addresses exponential subset complexity (2^M) by using weighted regression; computational complexity dominated by number of sampled simplified inputs and regression solve (the paper notes complexity O(2^M + M^3) for a full regression formulation, practical runtime reduced via sampling and regularization).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>A practical refinement within the interpretability domain; builds on LIME-style local regression and classical Shapley theory to produce a principled kernel.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — preserves SHAP's interpretability axioms, ensuring explanations are meaningful to users and comparable across models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Kernel SHAP (weighted linear regression approximation of Shapley values)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Chooses loss L as squared error, analytic constraints for phi0 and sum_i phi_i, and a Shapley-specific weighting kernel pi_{x'} that makes the regression solution equal the Shapley values; uses sampling of simplified inputs and solves a weighted linear regression (optionally with regularization) to estimate all phi_i jointly, improving sample efficiency relative to permutation sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / model-agnostic explanation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Widely applicable to any model accessible via predictions; more sample-efficient than naive Shapley sampling but still requires many model evaluations for large M; assumptions about conditional expectation estimation (feature independence or training-data sampling) affect accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Experimentally more sample-efficient than Shapley sampling in decision-tree examples (visualized in paper); specific MNIST runs used 50k samples for both SHAP and LIME for visualization; regression solve complexity includes O(M^3) term for dense solves.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Produces SHAP-consistent attributions and converges faster (fewer model evaluations) than sampling-only approaches; outperforms LIME when LIME's heuristics violate SHAP axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables practical application of SHAP to real-world models without retraining or enumerating subsets; makes axiomatic explanations feasible for practitioners.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against Shapley sampling (sampling-based permutation Shapley) and LIME: Kernel SHAP obtains similar or better approximation accuracy with fewer model evaluations; LIME's heuristic kernel sometimes produces inconsistent attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Deriving the exact kernel that recovers Shapley values in linear local models, joint estimation of all attributions via regression (improves sample efficiency), and the ability to incorporate regularization for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Casting Shapley value estimation as a constrained weighted linear regression with a unique Shapley kernel yields a model-agnostic estimator that is substantially more sample-efficient and consistent than previous local-regression heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2355.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2355.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep SHAP (compositional SHAP approximation for deep networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-specific compositional approximation that combines SHAP values for network components using DeepLIFT-style backpropagation of multipliers to efficiently approximate SHAP for deep neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Interpretability of deep neural networks (image classification example: MNIST convolutional network)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide fast, compositional approximations of SHAP values for deep networks by leveraging analytic solutions for components and combining them via backpropagation-like rules.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires access to model internals (activations) and representative background/reference inputs (e.g., expected input values); the MNIST example used a pre-trained model and normalized inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Deep network inputs such as images (convolutional layers), and intermediate layer activations (compositional structure).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High model complexity due to deep, nonlinear compositions; exact SHAP remains combinatorial, but Deep SHAP reduces complexity by exploiting component-wise analytical SHAP and multiplicative composition rules.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Builds on existing deep-network attribution methods (DeepLIFT, layer-wise relevance) and is an emerging practical technique within interpretability research.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — deep models require interpretable attributions for users; Deep SHAP aims to preserve SHAP axioms while using component-wise linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep SHAP (DeepLIFT + Shapley values compositional approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Interprets DeepLIFT reference-based multipliers in terms of SHAP values and recursively composes component-level SHAP solutions: compute phi for simple components (linear, max, single-input activations) analytically, compute multipliers m = phi / (x - E[x]) and propagate these multipliers through the network to estimate phi for earlier layers, approximating conditional expectations by using reference/background values.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / model-specific compositional approximation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for deep networks when internal structure is accessible; substantially faster than model-agnostic approaches and better aligned with SHAP axioms than heuristic linearizations (original DeepLIFT).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Paper provides qualitative and experimental evidence (MNIST) showing improved visual explanations and greater change in log odds when masking pixels according to attributions; no single scalar performance metric provided, but computational gains are implied versus full model-agnostic SHAP.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Produces attributions closer to SHAP than original DeepLIFT and better handles max pooling; improves interpretability in convolutional network examples and aligns better with human expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for practitioners using deep models: makes theoretically-grounded SHAP explanations tractable for deep architectures and improves attribution quality versus heuristic component linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against original DeepLIFT and Kernel SHAP on MNIST: Deep SHAP (and updated DeepLIFT) better approximate SHAP values and give more intuitive saliency maps than the original DeepLIFT heuristic linearizations; Kernel SHAP is model-agnostic but slower.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Leveraging analytic SHAP for simple components, using multiplier composition rules to avoid combinatorial enumeration, and grounding approximations in SHAP axioms rather than heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Exploiting the compositional structure of deep nets and analytically solvable component SHAP values yields fast, principled approximations (Deep SHAP) that better match SHAP axioms and human intuition than prior heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2355.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2355.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Interpretable Model-agnostic Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic local explanation method that fits a simple (typically linear) interpretable model locally around an instance to explain predictions; uses simplified interpretable inputs and a locality-weighting kernel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Why should i trust you?: Explaining the predictions of any classifier</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Model interpretability / local explanations for arbitrary predictive models (applied to text, images, and tabular models)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Explain an individual prediction by learning a locally-faithful interpretable model (e.g., linear) using samples in a simplified input space near the instance of interest.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires access to the model's prediction function and ability to sample or perturb inputs; uses training data or input perturbations to create local samples; works with abundant or moderate data for sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Bag-of-words text, superpixel-based images, and tabular features via simplified binary presence vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handles complex global models by focusing locally; local linear approximation may fail when local nonlinearities or feature dependencies are strong.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Widely-cited and used in interpretability research and practice; established baseline for local explanation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-high — provides interpretable local models but may not satisfy game-theoretic properties; useful for user trust but can produce unintuitive attributions under some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>LIME (locally-weighted interpretable surrogate models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Defines a simplified binary input mapping h_x, samples perturbed simplified inputs, weights them by a locality kernel pi_x', and fits a penalized linear model g by minimizing a loss L(f, g, pi_x') + Omega(g); kernel and regularization choices are heuristic in the original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / model-agnostic local surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Widely applicable to many model types and input modalities; limitations arise from heuristic kernel choices that can violate SHAP-like axioms (local accuracy, consistency) and from sensitivity to sampling and simplification mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Paper reports LIME runs (50k samples in MNIST comparison) for visualization; relative sample-efficiency shown worse than Kernel SHAP in decision-tree experiments (plots in paper) but exact numbers depend on experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides intuitive local explanations in many settings but can produce attributions that conflict with axiomatic desiderata and human judgments in tested examples; sensitive to kernel and interpretable representation design.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High as a general, easy-to-use interpretability tool adopted widely in practice; its limitations motivate development of SHAP and kernel improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared to SHAP and Shapley sampling in paper: LIME's heuristic kernel can produce inconsistent attributions and is less sample-efficient/less consistent with human intuition than Kernel SHAP which uses Shapley kernel.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simplicity, model-agnosticism, and flexible interpretable representations; however, heuristic kernel and loss choices limit theoretical guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Local surrogate regression is a practical way to explain predictions, but choosing the locality kernel and representation matter greatly for fidelity and consistency — principled kernels (SHAP) improve upon LIME's heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2355.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2355.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLIFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Learning Important FeaTures (DeepLIFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive backpropagation-style attribution method for deep networks that attributes to each input the difference-from-reference contribution to the output using summation-to-delta rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Not Just a Black Box: Learning Important Features Through Propagating Activation Differences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Interpretability of deep neural networks (applied to convolutional networks and other architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Compute per-input attributions in deep networks by propagating activation differences relative to a chosen reference input, ensuring attributions sum to output delta.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires access to model internals and a chosen reference/background input (often derived from dataset means); uses available training data to set references or expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Deep network activations for image inputs (e.g., MNIST) and other neural architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Deep network nonlinearity and interactions create challenges for attribution; DeepLIFT uses heuristics for linearizing nonlinear components and a reference baseline to make attributions tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established method in deep-network interpretability literature; widely used and extended (paper references adaptations to better match SHAP).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — provides interpretable additive attributions that inform about internal network behavior; choice of reference affects interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DeepLIFT (activation-difference propagation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Defines contribution C_{Δx_i Δy} for each input relative to a reference input r with Δ notation; enforces summation-to-delta property sum_i C_{Δx_i Δo} = Δo; uses backpropagation-like rules to distribute output delta through network components via heuristically-chosen composition rules.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / model-specific attribution</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to networks where internals are accessible; approximates SHAP under assumptions (feature independence and linearity) and can be adapted (Deep SHAP) to better align with SHAP axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Paper shows qualitative improvements when DeepLIFT is adjusted to approximate SHAP (visual saliency and change-in-log-odds measurements in MNIST), but no single quantitative metric summarized.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Original DeepLIFT offers fast attributions but can mis-handle max pooling and other functions; adapting DeepLIFT to approximate SHAP (Deep SHAP) yields attributions more consistent with axiomatic expectations and human intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for practitioners needing fast attributions for deep models; connection to SHAP increases its theoretical grounding and improves trustworthiness of attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to SHAP and updated DeepLIFT versions: original DeepLIFT may diverge from SHAP values and human judgments, while Deep SHAP provides improved alignment; Kernel SHAP is model-agnostic but slower.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Efficient propagation rules, accessibility of network internals, and ability to incorporate reference inputs; further improvement achieved by grounding rules in SHAP values.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>A backpropagation-style attribution can be fast and practical for deep nets, but grounding component linearizations in Shapley-value reasoning (Deep SHAP) improves theoretical consistency and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2355.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2355.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shapley sampling / regression / QII</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classical Shapley-based methods (Shapley regression values, Shapley sampling values, Quantitative Input Influence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Game-theoretic Shapley-value approaches for feature attribution: exact Shapley regression retrains on all subsets, Shapley sampling approximates via permutation sampling, and Quantitative Input Influence proposes sampling approximations and a broader framework for influence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Model interpretability / feature importance estimation for predictive models (regression/classification)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Compute feature importances representing the marginal contribution of each feature to a model's prediction, accounting for feature dependencies and interactions through averaging over feature subsets or orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires access to training data for sampling-based approximations; Shapley regression requires retraining models on all subsets (data- and compute-intensive), so availability of data and computation is central.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Applies to tabular, structured inputs and any model that can be retrained or probed via sampling; sampling methods approximate effect of removing variables by integrating over training data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Computationally expensive: exact Shapley regression requires retraining on up to 2^M subsets; Shapley sampling reduces cost via Monte Carlo permutation sampling but still can require many evaluations to converge.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Foundational (Shapley values are from cooperative game theory, 1953) with various applied adaptations in ML; several methods existed prior to SHAP unification.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for interpretability: Shapley values provide axiomatic attributions that are interpretable and fair under the axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Shapley-based attribution methods (regression, sampling, Quantitative Input Influence)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Shapley regression computes phi_i by retraining models on feature subsets and averaging marginal contributions weighted by factorial terms; Shapley sampling approximates the permutation-based Shapley formula via Monte Carlo sampling and integrates missing features via training-data sampling; Quantitative Input Influence provides a similar sampling-based approach within a broader influence framework.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / game-theoretic attribution</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Theoretically applicable to any model, but exact methods are often infeasible due to retraining costs; sampling approximations make them usable but potentially expensive for high-dimensional inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Exact methods are combinatorial (up to 2^M retrainings); sampling approximations converge with number of permutations sampled (no single rate given); paper shows Kernel SHAP is more sample-efficient than Shapley sampling in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provide principled, axiomatic attributions but practical approximations are needed; sampling approximations capture dependencies when sampling conditional distributions from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High theoretical impact as the basis for axiomatic attribution; motivates practical approximations (SHAP, Kernel SHAP) that retain theoretical properties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Shapley sampling/regression are the theoretical baseline compared against Kernel SHAP and LIME; sampling is accurate but less sample-efficient than Kernel SHAP; retraining-based Shapley regression is costly but exact under retraining assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong game-theory foundations and averaging over all feature orderings produce fair, consistent attributions; main barrier is computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Shapley-value formulations give axiomatic attributions but require practical approximations (sampling, regression kernels, model-specific compositional rules) to be usable on complex, high-dimensional models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2355.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2355.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear SHAP (analytic SHAP for linear models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analytic approximation of SHAP values for linear models under feature-independence, where feature attributions reduce to weight times deviation from expected feature value.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Interpretable explanations for linear predictive models (tabular data)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide exact or approximate SHAP feature attributions for linear models in a computationally trivial way.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires access to model coefficients and estimates of feature expectations (E[x_j]) from training data; typically readily available.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular data with numeric features for linear models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Low complexity for linear models: phi_i = w_j (x_j - E[x_j]) under independence assumption; no combinatorial enumeration required.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Derivation is straightforward and builds on prior observations; provides efficient closed-form attributions for an important class of models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-high — linear models are already interpretable, and Linear SHAP gives principled attributions consistent with SHAP axioms under assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Linear SHAP (analytic SHAP for linear models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>For f(x)=sum_j w_j x_j + b and assuming feature independence, set phi0 = b and phi_i = w_j (x_j - E[x_j]), deriving SHAP-consistent attributions directly from model weights and feature expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / analytic model-specific</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable and exact for linear models under independence; limited to models where linear form and expectation estimates hold.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Closed-form computation (constant time per feature) — negligible computation compared to sampling-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides interpretable, SHAP-consistent attributions for linear models; coincides with other known linear attribution formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables immediate, principled attributions for linear models and serves as a building block or baseline for more complex models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Simpler and exact (under assumptions) compared to sampling or kernel-based approximations; reduces to intuitive weight-times-deviation attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simplicity of linear models and closed-form expectations; leverages SHAP theory to produce exact attributions under mild assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>For linear models with independent features, SHAP attributions have an immediate closed-form: coefficient times deviation from expected feature value, making SHAP trivial and exact in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2355.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2355.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Max SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Max SHAP (efficient Shapley for max function)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized algorithm to compute Shapley attributions for the max function in O(M^2) time by using a permutation formulation and probabilities of each input being maximal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Feature attribution for compositional functions (specifically max-pooling / max operations within models)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Efficiently compute Shapley values for a max operator over M inputs where naive enumeration is exponential.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>No special data required beyond the input values to the max operator and knowledge of distributions if sampling conditional expectations; algorithmic and analytic rather than data-hungry.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Vector input to a max function; can arise in neural networks (max-pooling) or decision logic.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Reduces complexity from O(M 2^M) to O(M^2) by analytically computing probabilities that each input attains the maximum in permutations and aggregating contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>A targeted algorithmic contribution for a common component (max) in compositional models; extends applicability of SHAP to max operations efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — needed where exact attribution for max-like operations is important (e.g., assigning credit to the argmax contributor).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Max SHAP (analytic/combinatorial SHAP for max functions)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Uses permutation-based Shapley formulation to compute the probability that each input will be the maximum when inputs are added in random order and aggregates marginal contributions to compute phi_i in O(M^2) time.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Post-hoc interpretability / model-specific analytic technique</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to models/components that use max operations; particularly useful inside compositional approximations like Deep SHAP to handle max pooling efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Computational complexity O(M^2) vs naive O(M 2^M); no other numeric performance metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enables accurate SHAP approximations for max functions and improves attributions in compositional approximations compared to heuristic handling.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Improves tractability and fidelity of SHAP-based explanations in models with max operations, enabling more accurate compositional approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More efficient and principled than heuristic linearizations used in some DeepLIFT treatments; complements Deep SHAP by providing exact component SHAP for max operations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Exploiting combinatorial structure of the max operator and permutation-based Shapley formulation to derive analytic probability computations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Special-casing frequently used compositional components (like max) with analytic Shapley solutions yields large computational savings and improves attribution fidelity within compositional approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Unified Approach to Interpreting Model Predictions', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Why should i trust you?: Explaining the predictions of any classifier <em>(Rating: 2)</em></li>
                <li>Not Just a Black Box: Learning Important Features Through Propagating Activation Differences <em>(Rating: 2)</em></li>
                <li>Explaining prediction models and individual predictions with feature contributions <em>(Rating: 2)</em></li>
                <li>Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems <em>(Rating: 2)</em></li>
                <li>On pixel-wise explanations for non-linear classifier decisions by layerwise relevance propagation <em>(Rating: 1)</em></li>
                <li>A value for n-person games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2355",
    "paper_id": "paper-442e10a3c6640ded9408622005e3c2a8906ce4c2",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "SHAP",
            "name_full": "SHapley Additive exPlanations",
            "brief_description": "A unified framework that defines feature attributions as Shapley values of a conditional expectation function, producing additive explanations that satisfy local accuracy, missingness, and consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Model interpretability / explanation of ML predictions (applied to image classification, text, and tabular models)",
            "problem_description": "Provide consistent, locally accurate feature attributions for individual model predictions from complex models (ensembles, deep networks) so users can understand and trust predictions and gain insight into modeled processes.",
            "data_availability": "Varies by application; paper demonstrates on standard benchmark MNIST (abundant labeled images) and on datasets used for sampling-based Shapley estimates (training data used for conditional expectations); conditional expectation estimation requires access to representative training data; no single scarcity claim.",
            "data_structure": "Structured tabular data, bag-of-words text, images (e.g., MNIST), and high-dimensional/sparse feature vectors (decision-tree examples).",
            "problem_complexity": "High: models are nonlinear and high-dimensional (deep nets, ensembles); exact Shapley computation combinatorial (2^M subsets) creating exponential computational complexity; requires handling feature dependence / conditional expectations.",
            "domain_maturity": "Active and maturing subfield within ML; builds on established game-theory Shapley values and existing interpretability methods (LIME, DeepLIFT, Shapley sampling).",
            "mechanistic_understanding_requirements": "High — interpretability is required for user trust, debugging, and deriving insight; SHAP enforces interpretability properties as core requirements.",
            "ai_methodology_name": "SHAP (Shapley value-based additive feature attribution)",
            "ai_methodology_description": "Defines an explanation model g(z') = phi0 + sum_i phi_i z'_i where phi_i are Shapley values computed on f_x(z') = E[f(z) | z_S]; uses conditional expectations to approximate effect of missing features and enforces three axioms (local accuracy, missingness, consistency). Practical computation is via exact Shapley formula when possible or approximations (Kernel SHAP, Shapley sampling, model-specific approximations).",
            "ai_methodology_category": "Post-hoc interpretability / model explanation (game-theory based)",
            "applicability": "Applicable broadly to any predictive model and input type; limitations include computational cost for many features and the need to approximate conditional expectations (may assume feature independence or use training-data sampling).",
            "effectiveness_quantitative": "Exact Shapley values are combinatorial; paper reports use of 50k model evaluations in MNIST comparisons for sampling-based methods; regression-based Kernel SHAP shows improved sample efficiency vs. Shapley sampling in experiments (figures) but no single overall accuracy number is provided.",
            "effectiveness_qualitative": "SHAP explanations are theoretically unique under stated properties and are shown to be more consistent with human intuition than alternatives (LIME, DeepLIFT); Kernel SHAP improves sample efficiency vs. sampling baselines; model-specific approximations (Deep SHAP, Linear SHAP, Max SHAP) make SHAP practical for specific model families.",
            "impact_potential": "High — provides a principled, unified explanation framework that can increase trust, improve debugging of complex models, and be adapted for many model types; potential to standardize explanation methods across domains.",
            "comparison_to_alternatives": "Compared directly to LIME, DeepLIFT, and Shapley sampling in experiments: SHAP (via Kernel/Deep approximations) gave explanations that matched human judgments better and required fewer model evaluations than sampling-based Shapley; LIME and original DeepLIFT sometimes violated desirable properties and produced less intuitive attributions.",
            "success_factors": "Theoretical uniqueness from Shapley axioms (local accuracy, missingness, consistency), using conditional expectation mapping for missing features, regression-based Kernel SHAP for sample efficiency, and compositional approximations (Deep SHAP) that exploit model structure.",
            "key_insight": "Formulating local feature attributions as Shapley values of conditional expectations yields a unique, axiomatic explanation and, with tailored approximations (Kernel/Deep), makes theoretically-grounded explanations computationally practical and human-aligned.",
            "uuid": "e2355.0",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Kernel SHAP",
            "name_full": "Kernel SHAP (regression-based, model-agnostic SHAP approximation)",
            "brief_description": "A model-agnostic approximation to SHAP that frames Shapley value computation as a weighted linear regression problem with a specific 'Shapley kernel' weighting, improving sample efficiency over direct Shapley sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Model interpretability / explaining individual predictions for arbitrary ML models (applied to decision trees and MNIST in paper)",
            "problem_description": "Efficiently estimate SHAP values (feature attributions) for arbitrary models where exact Shapley computation is infeasible due to combinatorial subset space.",
            "data_availability": "Requires access to model evaluation (predict function) and representative samples or the training data to approximate conditional expectations; in experiments used both small and large sample budgets (e.g., 50k evaluations in MNIST comparisons).",
            "data_structure": "Works on any input type (binary simplified inputs mapping from original images, text, or structured features); examples include images and tabular/sparse features.",
            "problem_complexity": "Addresses exponential subset complexity (2^M) by using weighted regression; computational complexity dominated by number of sampled simplified inputs and regression solve (the paper notes complexity O(2^M + M^3) for a full regression formulation, practical runtime reduced via sampling and regularization).",
            "domain_maturity": "A practical refinement within the interpretability domain; builds on LIME-style local regression and classical Shapley theory to produce a principled kernel.",
            "mechanistic_understanding_requirements": "High — preserves SHAP's interpretability axioms, ensuring explanations are meaningful to users and comparable across models.",
            "ai_methodology_name": "Kernel SHAP (weighted linear regression approximation of Shapley values)",
            "ai_methodology_description": "Chooses loss L as squared error, analytic constraints for phi0 and sum_i phi_i, and a Shapley-specific weighting kernel pi_{x'} that makes the regression solution equal the Shapley values; uses sampling of simplified inputs and solves a weighted linear regression (optionally with regularization) to estimate all phi_i jointly, improving sample efficiency relative to permutation sampling.",
            "ai_methodology_category": "Post-hoc interpretability / model-agnostic explanation",
            "applicability": "Widely applicable to any model accessible via predictions; more sample-efficient than naive Shapley sampling but still requires many model evaluations for large M; assumptions about conditional expectation estimation (feature independence or training-data sampling) affect accuracy.",
            "effectiveness_quantitative": "Experimentally more sample-efficient than Shapley sampling in decision-tree examples (visualized in paper); specific MNIST runs used 50k samples for both SHAP and LIME for visualization; regression solve complexity includes O(M^3) term for dense solves.",
            "effectiveness_qualitative": "Produces SHAP-consistent attributions and converges faster (fewer model evaluations) than sampling-only approaches; outperforms LIME when LIME's heuristics violate SHAP axioms.",
            "impact_potential": "Enables practical application of SHAP to real-world models without retraining or enumerating subsets; makes axiomatic explanations feasible for practitioners.",
            "comparison_to_alternatives": "Compared against Shapley sampling (sampling-based permutation Shapley) and LIME: Kernel SHAP obtains similar or better approximation accuracy with fewer model evaluations; LIME's heuristic kernel sometimes produces inconsistent attributions.",
            "success_factors": "Deriving the exact kernel that recovers Shapley values in linear local models, joint estimation of all attributions via regression (improves sample efficiency), and the ability to incorporate regularization for stability.",
            "key_insight": "Casting Shapley value estimation as a constrained weighted linear regression with a unique Shapley kernel yields a model-agnostic estimator that is substantially more sample-efficient and consistent than previous local-regression heuristics.",
            "uuid": "e2355.1",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Deep SHAP",
            "name_full": "Deep SHAP (compositional SHAP approximation for deep networks)",
            "brief_description": "A model-specific compositional approximation that combines SHAP values for network components using DeepLIFT-style backpropagation of multipliers to efficiently approximate SHAP for deep neural networks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Interpretability of deep neural networks (image classification example: MNIST convolutional network)",
            "problem_description": "Provide fast, compositional approximations of SHAP values for deep networks by leveraging analytic solutions for components and combining them via backpropagation-like rules.",
            "data_availability": "Requires access to model internals (activations) and representative background/reference inputs (e.g., expected input values); the MNIST example used a pre-trained model and normalized inputs.",
            "data_structure": "Deep network inputs such as images (convolutional layers), and intermediate layer activations (compositional structure).",
            "problem_complexity": "High model complexity due to deep, nonlinear compositions; exact SHAP remains combinatorial, but Deep SHAP reduces complexity by exploiting component-wise analytical SHAP and multiplicative composition rules.",
            "domain_maturity": "Builds on existing deep-network attribution methods (DeepLIFT, layer-wise relevance) and is an emerging practical technique within interpretability research.",
            "mechanistic_understanding_requirements": "High — deep models require interpretable attributions for users; Deep SHAP aims to preserve SHAP axioms while using component-wise linearizations.",
            "ai_methodology_name": "Deep SHAP (DeepLIFT + Shapley values compositional approximation)",
            "ai_methodology_description": "Interprets DeepLIFT reference-based multipliers in terms of SHAP values and recursively composes component-level SHAP solutions: compute phi for simple components (linear, max, single-input activations) analytically, compute multipliers m = phi / (x - E[x]) and propagate these multipliers through the network to estimate phi for earlier layers, approximating conditional expectations by using reference/background values.",
            "ai_methodology_category": "Post-hoc interpretability / model-specific compositional approximation",
            "applicability": "Appropriate for deep networks when internal structure is accessible; substantially faster than model-agnostic approaches and better aligned with SHAP axioms than heuristic linearizations (original DeepLIFT).",
            "effectiveness_quantitative": "Paper provides qualitative and experimental evidence (MNIST) showing improved visual explanations and greater change in log odds when masking pixels according to attributions; no single scalar performance metric provided, but computational gains are implied versus full model-agnostic SHAP.",
            "effectiveness_qualitative": "Produces attributions closer to SHAP than original DeepLIFT and better handles max pooling; improves interpretability in convolutional network examples and aligns better with human expectations.",
            "impact_potential": "High for practitioners using deep models: makes theoretically-grounded SHAP explanations tractable for deep architectures and improves attribution quality versus heuristic component linearizations.",
            "comparison_to_alternatives": "Compared against original DeepLIFT and Kernel SHAP on MNIST: Deep SHAP (and updated DeepLIFT) better approximate SHAP values and give more intuitive saliency maps than the original DeepLIFT heuristic linearizations; Kernel SHAP is model-agnostic but slower.",
            "success_factors": "Leveraging analytic SHAP for simple components, using multiplier composition rules to avoid combinatorial enumeration, and grounding approximations in SHAP axioms rather than heuristics.",
            "key_insight": "Exploiting the compositional structure of deep nets and analytically solvable component SHAP values yields fast, principled approximations (Deep SHAP) that better match SHAP axioms and human intuition than prior heuristics.",
            "uuid": "e2355.2",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "LIME",
            "name_full": "Local Interpretable Model-agnostic Explanations",
            "brief_description": "A model-agnostic local explanation method that fits a simple (typically linear) interpretable model locally around an instance to explain predictions; uses simplified interpretable inputs and a locality-weighting kernel.",
            "citation_title": "Why should i trust you?: Explaining the predictions of any classifier",
            "mention_or_use": "use",
            "scientific_problem_domain": "Model interpretability / local explanations for arbitrary predictive models (applied to text, images, and tabular models)",
            "problem_description": "Explain an individual prediction by learning a locally-faithful interpretable model (e.g., linear) using samples in a simplified input space near the instance of interest.",
            "data_availability": "Requires access to the model's prediction function and ability to sample or perturb inputs; uses training data or input perturbations to create local samples; works with abundant or moderate data for sampling.",
            "data_structure": "Bag-of-words text, superpixel-based images, and tabular features via simplified binary presence vectors.",
            "problem_complexity": "Handles complex global models by focusing locally; local linear approximation may fail when local nonlinearities or feature dependencies are strong.",
            "domain_maturity": "Widely-cited and used in interpretability research and practice; established baseline for local explanation methods.",
            "mechanistic_understanding_requirements": "Medium-high — provides interpretable local models but may not satisfy game-theoretic properties; useful for user trust but can produce unintuitive attributions under some conditions.",
            "ai_methodology_name": "LIME (locally-weighted interpretable surrogate models)",
            "ai_methodology_description": "Defines a simplified binary input mapping h_x, samples perturbed simplified inputs, weights them by a locality kernel pi_x', and fits a penalized linear model g by minimizing a loss L(f, g, pi_x') + Omega(g); kernel and regularization choices are heuristic in the original formulation.",
            "ai_methodology_category": "Post-hoc interpretability / model-agnostic local surrogate",
            "applicability": "Widely applicable to many model types and input modalities; limitations arise from heuristic kernel choices that can violate SHAP-like axioms (local accuracy, consistency) and from sensitivity to sampling and simplification mappings.",
            "effectiveness_quantitative": "Paper reports LIME runs (50k samples in MNIST comparison) for visualization; relative sample-efficiency shown worse than Kernel SHAP in decision-tree experiments (plots in paper) but exact numbers depend on experimental setup.",
            "effectiveness_qualitative": "Provides intuitive local explanations in many settings but can produce attributions that conflict with axiomatic desiderata and human judgments in tested examples; sensitive to kernel and interpretable representation design.",
            "impact_potential": "High as a general, easy-to-use interpretability tool adopted widely in practice; its limitations motivate development of SHAP and kernel improvements.",
            "comparison_to_alternatives": "Directly compared to SHAP and Shapley sampling in paper: LIME's heuristic kernel can produce inconsistent attributions and is less sample-efficient/less consistent with human intuition than Kernel SHAP which uses Shapley kernel.",
            "success_factors": "Simplicity, model-agnosticism, and flexible interpretable representations; however, heuristic kernel and loss choices limit theoretical guarantees.",
            "key_insight": "Local surrogate regression is a practical way to explain predictions, but choosing the locality kernel and representation matter greatly for fidelity and consistency — principled kernels (SHAP) improve upon LIME's heuristics.",
            "uuid": "e2355.3",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "DeepLIFT",
            "name_full": "Deep Learning Important FeaTures (DeepLIFT)",
            "brief_description": "A recursive backpropagation-style attribution method for deep networks that attributes to each input the difference-from-reference contribution to the output using summation-to-delta rules.",
            "citation_title": "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences",
            "mention_or_use": "use",
            "scientific_problem_domain": "Interpretability of deep neural networks (applied to convolutional networks and other architectures)",
            "problem_description": "Compute per-input attributions in deep networks by propagating activation differences relative to a chosen reference input, ensuring attributions sum to output delta.",
            "data_availability": "Requires access to model internals and a chosen reference/background input (often derived from dataset means); uses available training data to set references or expectations.",
            "data_structure": "Deep network activations for image inputs (e.g., MNIST) and other neural architectures.",
            "problem_complexity": "Deep network nonlinearity and interactions create challenges for attribution; DeepLIFT uses heuristics for linearizing nonlinear components and a reference baseline to make attributions tractable.",
            "domain_maturity": "Established method in deep-network interpretability literature; widely used and extended (paper references adaptations to better match SHAP).",
            "mechanistic_understanding_requirements": "High — provides interpretable additive attributions that inform about internal network behavior; choice of reference affects interpretation.",
            "ai_methodology_name": "DeepLIFT (activation-difference propagation)",
            "ai_methodology_description": "Defines contribution C_{Δx_i Δy} for each input relative to a reference input r with Δ notation; enforces summation-to-delta property sum_i C_{Δx_i Δo} = Δo; uses backpropagation-like rules to distribute output delta through network components via heuristically-chosen composition rules.",
            "ai_methodology_category": "Post-hoc interpretability / model-specific attribution",
            "applicability": "Applicable to networks where internals are accessible; approximates SHAP under assumptions (feature independence and linearity) and can be adapted (Deep SHAP) to better align with SHAP axioms.",
            "effectiveness_quantitative": "Paper shows qualitative improvements when DeepLIFT is adjusted to approximate SHAP (visual saliency and change-in-log-odds measurements in MNIST), but no single quantitative metric summarized.",
            "effectiveness_qualitative": "Original DeepLIFT offers fast attributions but can mis-handle max pooling and other functions; adapting DeepLIFT to approximate SHAP (Deep SHAP) yields attributions more consistent with axiomatic expectations and human intuition.",
            "impact_potential": "High for practitioners needing fast attributions for deep models; connection to SHAP increases its theoretical grounding and improves trustworthiness of attributions.",
            "comparison_to_alternatives": "Compared to SHAP and updated DeepLIFT versions: original DeepLIFT may diverge from SHAP values and human judgments, while Deep SHAP provides improved alignment; Kernel SHAP is model-agnostic but slower.",
            "success_factors": "Efficient propagation rules, accessibility of network internals, and ability to incorporate reference inputs; further improvement achieved by grounding rules in SHAP values.",
            "key_insight": "A backpropagation-style attribution can be fast and practical for deep nets, but grounding component linearizations in Shapley-value reasoning (Deep SHAP) improves theoretical consistency and interpretability.",
            "uuid": "e2355.4",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Shapley sampling / regression / QII",
            "name_full": "Classical Shapley-based methods (Shapley regression values, Shapley sampling values, Quantitative Input Influence)",
            "brief_description": "Game-theoretic Shapley-value approaches for feature attribution: exact Shapley regression retrains on all subsets, Shapley sampling approximates via permutation sampling, and Quantitative Input Influence proposes sampling approximations and a broader framework for influence.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Model interpretability / feature importance estimation for predictive models (regression/classification)",
            "problem_description": "Compute feature importances representing the marginal contribution of each feature to a model's prediction, accounting for feature dependencies and interactions through averaging over feature subsets or orderings.",
            "data_availability": "Requires access to training data for sampling-based approximations; Shapley regression requires retraining models on all subsets (data- and compute-intensive), so availability of data and computation is central.",
            "data_structure": "Applies to tabular, structured inputs and any model that can be retrained or probed via sampling; sampling methods approximate effect of removing variables by integrating over training data.",
            "problem_complexity": "Computationally expensive: exact Shapley regression requires retraining on up to 2^M subsets; Shapley sampling reduces cost via Monte Carlo permutation sampling but still can require many evaluations to converge.",
            "domain_maturity": "Foundational (Shapley values are from cooperative game theory, 1953) with various applied adaptations in ML; several methods existed prior to SHAP unification.",
            "mechanistic_understanding_requirements": "High for interpretability: Shapley values provide axiomatic attributions that are interpretable and fair under the axioms.",
            "ai_methodology_name": "Shapley-based attribution methods (regression, sampling, Quantitative Input Influence)",
            "ai_methodology_description": "Shapley regression computes phi_i by retraining models on feature subsets and averaging marginal contributions weighted by factorial terms; Shapley sampling approximates the permutation-based Shapley formula via Monte Carlo sampling and integrates missing features via training-data sampling; Quantitative Input Influence provides a similar sampling-based approach within a broader influence framework.",
            "ai_methodology_category": "Post-hoc interpretability / game-theoretic attribution",
            "applicability": "Theoretically applicable to any model, but exact methods are often infeasible due to retraining costs; sampling approximations make them usable but potentially expensive for high-dimensional inputs.",
            "effectiveness_quantitative": "Exact methods are combinatorial (up to 2^M retrainings); sampling approximations converge with number of permutations sampled (no single rate given); paper shows Kernel SHAP is more sample-efficient than Shapley sampling in experiments.",
            "effectiveness_qualitative": "Provide principled, axiomatic attributions but practical approximations are needed; sampling approximations capture dependencies when sampling conditional distributions from training data.",
            "impact_potential": "High theoretical impact as the basis for axiomatic attribution; motivates practical approximations (SHAP, Kernel SHAP) that retain theoretical properties.",
            "comparison_to_alternatives": "Shapley sampling/regression are the theoretical baseline compared against Kernel SHAP and LIME; sampling is accurate but less sample-efficient than Kernel SHAP; retraining-based Shapley regression is costly but exact under retraining assumptions.",
            "success_factors": "Strong game-theory foundations and averaging over all feature orderings produce fair, consistent attributions; main barrier is computational cost.",
            "key_insight": "Shapley-value formulations give axiomatic attributions but require practical approximations (sampling, regression kernels, model-specific compositional rules) to be usable on complex, high-dimensional models.",
            "uuid": "e2355.5",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Linear SHAP",
            "name_full": "Linear SHAP (analytic SHAP for linear models)",
            "brief_description": "An analytic approximation of SHAP values for linear models under feature-independence, where feature attributions reduce to weight times deviation from expected feature value.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Interpretable explanations for linear predictive models (tabular data)",
            "problem_description": "Provide exact or approximate SHAP feature attributions for linear models in a computationally trivial way.",
            "data_availability": "Requires access to model coefficients and estimates of feature expectations (E[x_j]) from training data; typically readily available.",
            "data_structure": "Structured tabular data with numeric features for linear models.",
            "problem_complexity": "Low complexity for linear models: phi_i = w_j (x_j - E[x_j]) under independence assumption; no combinatorial enumeration required.",
            "domain_maturity": "Derivation is straightforward and builds on prior observations; provides efficient closed-form attributions for an important class of models.",
            "mechanistic_understanding_requirements": "Medium-high — linear models are already interpretable, and Linear SHAP gives principled attributions consistent with SHAP axioms under assumptions.",
            "ai_methodology_name": "Linear SHAP (analytic SHAP for linear models)",
            "ai_methodology_description": "For f(x)=sum_j w_j x_j + b and assuming feature independence, set phi0 = b and phi_i = w_j (x_j - E[x_j]), deriving SHAP-consistent attributions directly from model weights and feature expectations.",
            "ai_methodology_category": "Post-hoc interpretability / analytic model-specific",
            "applicability": "Highly applicable and exact for linear models under independence; limited to models where linear form and expectation estimates hold.",
            "effectiveness_quantitative": "Closed-form computation (constant time per feature) — negligible computation compared to sampling-based methods.",
            "effectiveness_qualitative": "Provides interpretable, SHAP-consistent attributions for linear models; coincides with other known linear attribution formulas.",
            "impact_potential": "Enables immediate, principled attributions for linear models and serves as a building block or baseline for more complex models.",
            "comparison_to_alternatives": "Simpler and exact (under assumptions) compared to sampling or kernel-based approximations; reduces to intuitive weight-times-deviation attribution.",
            "success_factors": "Simplicity of linear models and closed-form expectations; leverages SHAP theory to produce exact attributions under mild assumptions.",
            "key_insight": "For linear models with independent features, SHAP attributions have an immediate closed-form: coefficient times deviation from expected feature value, making SHAP trivial and exact in this domain.",
            "uuid": "e2355.6",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Max SHAP",
            "name_full": "Max SHAP (efficient Shapley for max function)",
            "brief_description": "A specialized algorithm to compute Shapley attributions for the max function in O(M^2) time by using a permutation formulation and probabilities of each input being maximal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Feature attribution for compositional functions (specifically max-pooling / max operations within models)",
            "problem_description": "Efficiently compute Shapley values for a max operator over M inputs where naive enumeration is exponential.",
            "data_availability": "No special data required beyond the input values to the max operator and knowledge of distributions if sampling conditional expectations; algorithmic and analytic rather than data-hungry.",
            "data_structure": "Vector input to a max function; can arise in neural networks (max-pooling) or decision logic.",
            "problem_complexity": "Reduces complexity from O(M 2^M) to O(M^2) by analytically computing probabilities that each input attains the maximum in permutations and aggregating contributions.",
            "domain_maturity": "A targeted algorithmic contribution for a common component (max) in compositional models; extends applicability of SHAP to max operations efficiently.",
            "mechanistic_understanding_requirements": "Medium — needed where exact attribution for max-like operations is important (e.g., assigning credit to the argmax contributor).",
            "ai_methodology_name": "Max SHAP (analytic/combinatorial SHAP for max functions)",
            "ai_methodology_description": "Uses permutation-based Shapley formulation to compute the probability that each input will be the maximum when inputs are added in random order and aggregates marginal contributions to compute phi_i in O(M^2) time.",
            "ai_methodology_category": "Post-hoc interpretability / model-specific analytic technique",
            "applicability": "Applicable to models/components that use max operations; particularly useful inside compositional approximations like Deep SHAP to handle max pooling efficiently.",
            "effectiveness_quantitative": "Computational complexity O(M^2) vs naive O(M 2^M); no other numeric performance metrics reported.",
            "effectiveness_qualitative": "Enables accurate SHAP approximations for max functions and improves attributions in compositional approximations compared to heuristic handling.",
            "impact_potential": "Improves tractability and fidelity of SHAP-based explanations in models with max operations, enabling more accurate compositional approximations.",
            "comparison_to_alternatives": "More efficient and principled than heuristic linearizations used in some DeepLIFT treatments; complements Deep SHAP by providing exact component SHAP for max operations.",
            "success_factors": "Exploiting combinatorial structure of the max operator and permutation-based Shapley formulation to derive analytic probability computations.",
            "key_insight": "Special-casing frequently used compositional components (like max) with analytic Shapley solutions yields large computational savings and improves attribution fidelity within compositional approximations.",
            "uuid": "e2355.7",
            "source_info": {
                "paper_title": "A Unified Approach to Interpreting Model Predictions",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Why should i trust you?: Explaining the predictions of any classifier",
            "rating": 2
        },
        {
            "paper_title": "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences",
            "rating": 2
        },
        {
            "paper_title": "Explaining prediction models and individual predictions with feature contributions",
            "rating": 2
        },
        {
            "paper_title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
            "rating": 2
        },
        {
            "paper_title": "On pixel-wise explanations for non-linear classifier decisions by layerwise relevance propagation",
            "rating": 1
        },
        {
            "paper_title": "A value for n-person games",
            "rating": 1
        }
    ],
    "cost": 0.019011249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Unified Approach to Interpreting Model Predictions</h1>
<p>Scott M. Lundberg<br>Paul G. Allen School of Computer Science<br>University of Washington<br>Seattle, WA 98105<br>slund1@cs.washington.edu</p>
<p>Su-In Lee<br>Paul G. Allen School of Computer Science<br>Department of Genome Sciences<br>University of Washington<br>Seattle, WA 98105<br>suinlee@cs.washington.edu</p>
<h4>Abstract</h4>
<p>Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.</p>
<h2>1 Introduction</h2>
<p>The ability to correctly interpret a prediction model's output is extremely important. It engenders appropriate user trust, provides insight into how a model may be improved, and supports understanding of the process being modeled. In some applications, simple models (e.g., linear models) are often preferred for their ease of interpretation, even if they may be less accurate than complex ones. However, the growing availability of big data has increased the benefits of using complex models, so bringing to the forefront the trade-off between accuracy and interpretability of a model's output. A wide variety of different methods have been recently proposed to address this issue [5, 8, 9, 3, 4, 1]. But an understanding of how these methods relate and when one method is preferable to another is still lacking.</p>
<p>Here, we present a novel unified approach to interpreting model predictions. ${ }^{1}$ Our approach leads to three potentially surprising results that bring clarity to the growing space of methods:</p>
<ol>
<li>
<p>We introduce the perspective of viewing any explanation of a model's prediction as a model itself, which we term the explanation model. This lets us define the class of additive feature attribution methods (Section 2), which unifies six current methods.
<sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
</li>
<li>
<p>We then show that game theory results guaranteeing a unique solution apply to the entire class of additive feature attribution methods (Section 3) and propose SHAP values as a unified measure of feature importance that various methods approximate (Section 4).</p>
</li>
<li>We propose new SHAP value estimation methods and demonstrate that they are better aligned with human intuition as measured by user studies and more effectually discriminate among model output classes than several existing methods (Section 5).</li>
</ol>
<h1>2 Additive Feature Attribution Methods</h1>
<p>The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. Instead, we must use a simpler explanation model, which we define as any interpretable approximation of the original model. We show below that six current explanation methods from the literature all use the same explanation model. This previously unappreciated unity has interesting implications, which we describe in later sections.</p>
<p>Let $f$ be the original prediction model to be explained and $g$ the explanation model. Here, we focus on local methods designed to explain a prediction $f(x)$ based on a single input $x$, as proposed in LIME [5]. Explanation models often use simplified inputs $x^{\prime}$ that map to the original inputs through a mapping function $x=h_{x}\left(x^{\prime}\right)$. Local methods try to ensure $g\left(z^{\prime}\right) \approx f\left(h_{x}\left(z^{\prime}\right)\right)$ whenever $z^{\prime} \approx x^{\prime}$. (Note that $h_{x}\left(x^{\prime}\right)=x$ even though $x^{\prime}$ may contain less information than $x$ because $h_{x}$ is specific to the current input $x$.)</p>
<p>Definition 1 Additive feature attribution methods have an explanation model that is a linear function of binary variables:</p>
<p>$$
g\left(z^{\prime}\right)=\phi_{0}+\sum_{i=1}^{M} \phi_{i} z_{i}^{\prime}
$$</p>
<p>where $z^{\prime} \in{0,1}^{M}, M$ is the number of simplified input features, and $\phi_{i} \in \mathbb{R}$.
Methods with explanation models matching Definition 1 attribute an effect $\phi_{i}$ to each feature, and summing the effects of all feature attributions approximates the output $f(x)$ of the original model. Many current methods match Definition 1, several of which are discussed below.</p>
<h3>2.1 LIME</h3>
<p>The LIME method interprets individual model predictions based on locally approximating the model around a given prediction [5]. The local linear explanation model that LIME uses adheres to Equation 1 exactly and is thus an additive feature attribution method. LIME refers to simplified inputs $x^{\prime}$ as "interpretable inputs," and the mapping $x=h_{x}\left(x^{\prime}\right)$ converts a binary vector of interpretable inputs into the original input space. Different types of $h_{x}$ mappings are used for different input spaces. For bag of words text features, $h_{x}$ converts a vector of 1 's or 0 's (present or not) into the original word count if the simplified input is one, or zero if the simplified input is zero. For images, $h_{x}$ treats the image as a set of super pixels; it then maps 1 to leaving the super pixel as its original value and 0 to replacing the super pixel with an average of neighboring pixels (this is meant to represent being missing).
To find $\phi$, LIME minimizes the following objective function:</p>
<p>$$
\xi=\underset{g \in \mathcal{G}}{\arg \min } L\left(f, g, \pi_{x^{\prime}}\right)+\Omega(g)
$$</p>
<p>Faithfulness of the explanation model $g\left(z^{\prime}\right)$ to the original model $f\left(h_{x}\left(z^{\prime}\right)\right)$ is enforced through the loss $L$ over a set of samples in the simplified input space weighted by the local kernel $\pi_{x^{\prime}} . \Omega$ penalizes the complexity of $g$. Since in LIME $g$ follows Equation 1 and $L$ is a squared loss, Equation 2 can be solved using penalized linear regression.</p>
<h1>2.2 DeepLIFT</h1>
<p>DeepLIFT was recently proposed as a recursive prediction explanation method for deep learning [8, 7]. It attributes to each input $x_{i}$ a value $C_{\Delta x_{i} \Delta y}$ that represents the effect of that input being set to a reference value as opposed to its original value. This means that for DeepLIFT, the mapping $x=h_{x}\left(x^{\prime}\right)$ converts binary values into the original inputs, where 1 indicates that an input takes its original value, and 0 indicates that it takes the reference value. The reference value, though chosen by the user, represents a typical uninformative background value for the feature.</p>
<p>DeepLIFT uses a "summation-to-delta" property that states:</p>
<p>$$
\sum_{i=1}^{n} C_{\Delta x_{i} \Delta o}=\Delta o
$$</p>
<p>where $o=f(x)$ is the model output, $\Delta o=f(x)-f(r), \Delta x_{i}=x_{i}-r_{i}$, and $r$ is the reference input. If we let $\phi_{i}=C_{\Delta x_{i} \Delta o}$ and $\phi_{0}=f(r)$, then DeepLIFT's explanation model matches Equation 1 and is thus another additive feature attribution method.</p>
<h3>2.3 Layer-Wise Relevance Propagation</h3>
<p>The layer-wise relevance propagation method interprets the predictions of deep networks [1]. As noted by Shrikumar et al., this menthod is equivalent to DeepLIFT with the reference activations of all neurons fixed to zero. Thus, $x=h_{x}\left(x^{\prime}\right)$ converts binary values into the original input space, where 1 means that an input takes its original value, and 0 means an input takes the 0 value. Layer-wise relevance propagation's explanation model, like DeepLIFT's, matches Equation 1.</p>
<h3>2.4 Classic Shapley Value Estimation</h3>
<p>Three previous methods use classic equations from cooperative game theory to compute explanations of model predictions: Shapley regression values [4], Shapley sampling values [9], and Quantitative Input Influence [3].
Shapley regression values are feature importances for linear models in the presence of multicollinearity. This method requires retraining the model on all feature subsets $S \subseteq F$, where $F$ is the set of all features. It assigns an importance value to each feature that represents the effect on the model prediction of including that feature. To compute this effect, a model $f_{S \cup{i}}$ is trained with that feature present, and another model $f_{S}$ is trained with the feature withheld. Then, predictions from the two models are compared on the current input $f_{S \cup{i}}\left(x_{S \cup{i}}\right)-f_{S}\left(x_{S}\right)$, where $x_{S}$ represents the values of the input features in the set $S$. Since the effect of withholding a feature depends on other features in the model, the preceding differences are computed for all possible subsets $S \subseteq F \backslash{i}$. The Shapley values are then computed and used as feature attributions. They are a weighted average of all possible differences:</p>
<p>$$
\phi_{i}=\sum_{S \subseteq F \backslash{i}} \frac{|S|!(|F|-|S|-1)!}{|F|!}\left[f_{S \cup{i}}\left(x_{S \cup{i}}\right)-f_{S}\left(x_{S}\right)\right]
$$</p>
<p>For Shapley regression values, $h_{x}$ maps 1 or 0 to the original input space, where 1 indicates the input is included in the model, and 0 indicates exclusion from the model. If we let $\phi_{0}=f_{\varnothing}(\varnothing)$, then the Shapley regression values match Equation 1 and are hence an additive feature attribution method.
Shapley sampling values are meant to explain any model by: (1) applying sampling approximations to Equation 4, and (2) approximating the effect of removing a variable from the model by integrating over samples from the training dataset. This eliminates the need to retrain the model and allows fewer than $2^{|F|}$ differences to be computed. Since the explanation model form of Shapley sampling values is the same as that for Shapley regression values, it is also an additive feature attribution method.
Quantitative input influence is a broader framework that addresses more than feature attributions. However, as part of its method it independently proposes a sampling approximation to Shapley values that is nearly identical to Shapley sampling values. It is thus another additive feature attribution method.</p>
<h1>3 Simple Properties Uniquely Determine Additive Feature Attributions</h1>
<p>A surprising attribute of the class of additive feature attribution methods is the presence of a single unique solution in this class with three desirable properties (described below). While these properties are familiar to the classical Shapley value estimation methods, they were previously unknown for other additive feature attribution methods.</p>
<p>The first desirable property is local accuracy. When approximating the original model $f$ for a specific input $x$, local accuracy requires the explanation model to at least match the output of $f$ for the simplified input $x^{\prime}$ (which corresponds to the original input $x$ ).</p>
<h2>Property 1 (Local accuracy)</h2>
<p>$$
f(x)=g\left(x^{\prime}\right)=\phi_{0}+\sum_{i=1}^{M} \phi_{i} x_{i}^{\prime}
$$</p>
<p>The explanation model $g\left(x^{\prime}\right)$ matches the original model $f(x)$ when $x=h_{x}\left(x^{\prime}\right)$.
The second property is missingness. If the simplified inputs represent feature presence, then missingness requires features missing in the original input to have no impact. All of the methods described in Section 2 obey the missingness property.</p>
<h2>Property 2 (Missingness)</h2>
<p>$$
x_{i}^{\prime}=0 \Longrightarrow \phi_{i}=0
$$</p>
<p>Missingness constrains features where $x_{i}^{\prime}=0$ to have no attributed impact.
The third property is consistency. Consistency states that if a model changes so that some simplified input's contribution increases or stays the same regardless of the other inputs, that input's attribution should not decrease.</p>
<p>Property 3 (Consistency) Let $f_{x}\left(z^{\prime}\right)=f\left(h_{x}\left(z^{\prime}\right)\right)$ and $z^{\prime} \backslash i$ denote setting $z_{i}^{\prime}=0$. For any two models $f$ and $f^{\prime}$, if</p>
<p>$$
f_{x}^{\prime}\left(z^{\prime}\right)-f_{x}^{\prime}\left(z^{\prime} \backslash i\right) \geq f_{x}\left(z^{\prime}\right)-f_{x}\left(z^{\prime} \backslash i\right)
$$</p>
<p>for all inputs $z^{\prime} \in{0,1}^{M}$, then $\phi_{i}\left(f^{\prime}, x\right) \geq \phi_{i}(f, x)$.
Theorem 1 Only one possible explanation model g follows Definition 1 and satisfies Properties 1, 2, and 3:</p>
<p>$$
\phi_{i}(f, x)=\sum_{z^{\prime} \subseteq x^{\prime}} \frac{\left|z^{\prime}\right|\left(\left(M-\left|z^{\prime}\right|-1\right)\right|}{M!}\left[f_{x}\left(z^{\prime}\right)-f_{x}\left(z^{\prime} \backslash i\right)\right]
$$</p>
<p>where $\left|z^{\prime}\right|$ is the number of non-zero entries in $z^{\prime}$, and $z^{\prime} \subseteq x^{\prime}$ represents all $z^{\prime}$ vectors where the non-zero entries are a subset of the non-zero entries in $x^{\prime}$.</p>
<p>Theorem 1 follows from combined cooperative game theory results, where the values $\phi_{i}$ are known as Shapley values [6]. Young (1985) demonstrated that Shapley values are the only set of values that satisfy three axioms similar to Property 1, Property 3, and a final property that we show to be redundant in this setting (see Supplementary Material). Property 2 is required to adapt the Shapley proofs to the class of additive feature attribution methods.
Under Properties 1-3, for a given simplified input mapping $h_{x}$, Theorem 1 shows that there is only one possible additive feature attribution method. This result implies that methods not based on Shapley values violate local accuracy and/or consistency (methods in Section 2 already respect missingness). The following section proposes a unified approach that improves previous methods, preventing them from unintentionally violating Properties 1 and 3.</p>
<h2>4 SHAP (SHapley Additive exPlanation) Values</h2>
<p>We propose SHAP values as a unified measure of feature importance. These are the Shapley values of a conditional expectation function of the original model; thus, they are the solution to Equation</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SHAP (SHapley Additive exPlanation) values attribute to each feature the change in the expected model prediction when conditioning on that feature. They explain how to get from the base value $E[f(z)]$ that would be predicted if we did not know any features to the current output $f(x)$. This diagram shows a single ordering. When the model is non-linear or the input features are not independent, however, the order in which features are added to the expectation matters, and the SHAP values arise from averaging the $\phi_{i}$ values across all possible orderings.</p>
<p>8, where $f_{x}\left(z^{\prime}\right)=f\left(h_{x}\left(z^{\prime}\right)\right)=E\left[f(z) \mid z_{S}\right]$, and $S$ is the set of non-zero indexes in $z^{\prime}$ (Figure 1). Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that adheres to Properties 1-3 and uses conditional expectations to define simplified inputs. Implicit in this definition of SHAP values is a simplified input mapping, $h_{x}\left(z^{\prime}\right)=z_{S}$, where $z_{S}$ has missing values for features not in the set $S$. Since most models cannot handle arbitrary patterns of missing input values, we approximate $f\left(z_{S}\right)$ with $E\left[f(z) \mid z_{S}\right]$. This definition of SHAP values is designed to closely align with the Shapley regression, Shapley sampling, and quantitative input influence feature attributions, while also allowing for connections with LIME, DeepLIFT, and layer-wise relevance propagation.</p>
<p>The exact computation of SHAP values is challenging. However, by combining insights from current additive feature attribution methods, we can approximate them. We describe two model-agnostic approximation methods, one that is already known (Shapley sampling values) and another that is novel (Kernel SHAP). We also describe four model-type-specific approximation methods, two of which are novel (Max SHAP, Deep SHAP). When using these methods, feature independence and model linearity are two optional assumptions simplifying the computation of the expected values (note that $\bar{S}$ is the set of features not in $S$ ):</p>
<p>$$
\begin{aligned}
f\left(h_{x}\left(z^{\prime}\right)\right) &amp; =E\left[f(z) \mid z_{S}\right] \
&amp; =E_{z_{\bar{S}} \mid z_{S}}[f(z)] \
&amp; \approx E_{z_{\bar{S}}}[f(z)] \
&amp; \approx f\left(\left[z_{S}, E\left[z_{\bar{S}}\right]\right]\right) .
\end{aligned}
$$</p>
<p>SHAP explanation model simplified input mapping expectation over $z_{\bar{S}} \mid z_{S}$ assume feature independence (as in $[9,5,7,3]$ ) assume model linearity</p>
<h1>4.1 Model-Agnostic Approximations</h1>
<p>If we assume feature independence when approximating conditional expectations (Equation 11), as in $[9,5,7,3]$, then SHAP values can be estimated directly using the Shapley sampling values method [9] or equivalently the Quantitative Input Influence method [3]. These methods use a sampling approximation of a permutation version of the classic Shapley value equations (Equation 8). Separate sampling estimates are performed for each feature attribution. While reasonable to compute for a small number of inputs, the Kernel SHAP method described next requires fewer evaluations of the original model to obtain similar approximation accuracy (Section 5).</p>
<h2>Kernel SHAP (Linear LIME + Shapley values)</h2>
<p>Linear LIME uses a linear explanation model to locally approximate $f$, where local is measured in the simplified binary input space. At first glance, the regression formulation of LIME in Equation 2 seems very different from the classical Shapley value formulation of Equation 8. However, since linear LIME is an additive feature attribution method, we know the Shapley values are the only possible solution to Equation 2 that satisfies Properties 1-3 - local accuracy, missingness and consistency. A natural question to pose is whether the solution to Equation 2 recovers these values. The answer depends on the choice of loss function $L$, weighting kernel $\pi_{x^{\prime}}$ and regularization term $\Omega$. The LIME choices for these parameters are made heuristically; using these choices, Equation 2 does not recover the Shapley values. One consequence is that local accuracy and/or consistency are violated, which in turn leads to unintuitive behavior in certain circumstances (see Section 5).</p>
<p>Below we show how to avoid heuristically choosing the parameters in Equation 2 and how to find the loss function $L$, weighting kernel $\pi_{x^{\prime}}$, and regularization term $\Omega$ that recover the Shapley values.</p>
<p>Theorem 2 (Shapley kernel) Under Definition 1, the specific forms of $\pi_{x^{\prime}}, L$, and $\Omega$ that make solutions of Equation 2 consistent with Properties 1 through 3 are:</p>
<p>$$
\begin{aligned}
\Omega(g) &amp; =0 \
\pi_{x^{\prime}}\left(z^{\prime}\right) &amp; =\frac{(M-1)}{(M \text { choose }\left|z^{\prime}\right|)\left|z^{\prime}\right|\left(M-\left|z^{\prime}\right|\right)} \
L\left(f, g, \pi_{x^{\prime}}\right) &amp; =\sum_{z^{\prime} \in Z}\left[f\left(h_{x}^{-1}\left(z^{\prime}\right)\right)-g\left(z^{\prime}\right)\right]^{2} \pi_{x^{\prime}}\left(z^{\prime}\right)
\end{aligned}
$$</p>
<p>where $\left|z^{\prime}\right|$ is the number of non-zero elements in $z^{\prime}$.
The proof of Theorem 2 is shown in the Supplementary Material.
It is important to note that $\pi_{x^{\prime}}\left(z^{\prime}\right)=\infty$ when $\left|z^{\prime}\right| \in{0, M}$, which enforces $\phi_{0}=f_{x}(\varnothing)$ and $f(x)=$ $\sum_{i=0}^{M} \phi_{i}$. In practice, these infinite weights can be avoided during optimization by analytically eliminating two variables using these constraints.
Since $g\left(z^{\prime}\right)$ in Theorem 2 is assumed to follow a linear form, and $L$ is a squared loss, Equation 2 can still be solved using linear regression. As a consequence, the Shapley values from game theory can be computed using weighted linear regression. ${ }^{2}$ Since LIME uses a simplified input mapping that is equivalent to the approximation of the SHAP mapping given in Equation 12, this enables regression-based, model-agnostic estimation of SHAP values. Jointly estimating all SHAP values using regression provides better sample efficiency than the direct use of classical Shapley equations (see Section 5).
The intuitive connection between linear regression and Shapley values is that Equation 8 is a difference of means. Since the mean is also the best least squares point estimate for a set of data points, it is natural to search for a weighting kernel that causes linear least squares regression to recapitulate the Shapley values. This leads to a kernel that distinctly differs from previous heuristically chosen kernels (Figure 2A).</p>
<h1>4.2 Model-Specific Approximations</h1>
<p>While Kernel SHAP improves the sample efficiency of model-agnostic estimations of SHAP values, by restricting our attention to specific model types, we can develop faster model-specific approximation methods.</p>
<h2>Linear SHAP</h2>
<p>For linear models, if we assume input feature independence (Equation 11), SHAP values can be approximated directly from the model's weight coefficients.</p>
<p>Corollary 1 (Linear SHAP) Given a linear model $f(x)=\sum_{j=1}^{M} w_{j} x_{j}+b: \phi_{0}(f, x)=b$ and</p>
<p>$$
\phi_{i}(f, x)=w_{j}\left(x_{j}-E\left[x_{j}\right]\right)
$$</p>
<p>This follows from Theorem 2 and Equation 11, and it has been previously noted by Štrumbelj and Kononenko [9].</p>
<h2>Low-Order SHAP</h2>
<p>Since linear regression using Theorem 2 has complexity $O\left(2^{M}+M^{3}\right)$, it is efficient for small values of $M$ if we choose an approximation of the conditional expectations (Equation 11 or 12).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (A) The Shapley kernel weighting is symmetric when all possible $z^{\prime}$ vectors are ordered by cardinality there are $2^{15}$ vectors in this example. This is distinctly different from previous heuristically chosen kernels. (B) Compositional models such as deep neural networks are comprised of many simple components. Given analytic solutions for the Shapley values of the components, fast approximations for the full model can be made using DeepLIFT's style of back-propagation.</p>
<h1>Max SHAP</h1>
<p>Using a permutation formulation of Shapley values, we can calculate the probability that each input will increase the maximum value over every other input. Doing this on a sorted order of input values lets us compute the Shapley values of a max function with $M$ inputs in $O\left(M^{2}\right)$ time instead of $O\left(M 2^{M}\right)$. See Supplementary Material for the full algorithm.</p>
<h2>Deep SHAP (DeepLIFT + Shapley values)</h2>
<p>While Kernel SHAP can be used on any model, including deep models, it is natural to ask whether there is a way to leverage extra knowledge about the compositional nature of deep networks to improve computational performance. We find an answer to this question through a previously unappreciated connection between Shapley values and DeepLIFT [8]. If we interpret the reference value in Equation 3 as representing $E[x]$ in Equation 12, then DeepLIFT approximates SHAP values assuming that the input features are independent of one another and the deep model is linear. DeepLIFT uses a linear composition rule, which is equivalent to linearizing the non-linear components of a neural network. Its back-propagation rules defining how each component is linearized are intuitive but were heuristically chosen. Since DeepLIFT is an additive feature attribution method that satisfies local accuracy and missingness, we know that Shapley values represent the only attribution values that satisfy consistency. This motivates our adapting DeepLIFT to become a compositional approximation of SHAP values, leading to Deep SHAP.
Deep SHAP combines SHAP values computed for smaller components of the network into SHAP values for the whole network. It does so by recursively passing DeepLIFT's multipliers, now defined in terms of SHAP values, backwards through the network as in Figure 2B:</p>
<p>$$
\begin{aligned}
m_{x_{j} f_{3}} &amp; =\frac{\phi_{i}\left(f_{3}, x\right)}{x_{j}-E\left[x_{j}\right]} \
\forall_{j \in{1,2}} \quad m_{y_{i} f_{j}} &amp; =\frac{\phi_{i}\left(f_{j}, y\right)}{y_{i}-E\left[y_{i}\right]} \
m_{y_{i} f_{3}} &amp; =\sum_{j=1}^{2} m_{y_{i} f_{j}} m_{x_{j} f_{3}} \
\phi_{i}\left(f_{3}, y\right) &amp; \approx m_{y_{i} f_{3}}\left(y_{i}-E\left[y_{i}\right]\right)
\end{aligned}
$$</p>
<h2>chain rule</h2>
<p>linear approximation</p>
<p>Since the SHAP values for the simple network components can be efficiently solved analytically if they are linear, max pooling, or an activation function with just one input, this composition rule enables a fast approximation of values for the whole model. Deep SHAP avoids the need to heuristically choose ways to linearize components. Instead, it derives an effective linearization from the SHAP values computed for each component. The max function offers one example where this leads to improved attributions (see Section 5).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison of three additive feature attribution methods: Kernel SHAP (using a debiased lasso), Shapley sampling values, and LIME (using the open source implementation). Feature importance estimates are shown for one feature in two models as the number of evaluations of the original model function increases. The 10th and 90th percentiles are shown for 200 replicate estimates at each sample size. (A) A decision tree model using all 10 input features is explained for a single input. (B) A decision tree using only 3 of 100 input features is explained for a single input.</p>
<h1>5 Computational and User Study Experiments</h1>
<p>We evaluated the benefits of SHAP values using the Kernel SHAP and Deep SHAP approximation methods. First, we compared the computational efficiency and accuracy of Kernel SHAP vs. LIME and Shapley sampling values. Second, we designed user studies to compare SHAP values with alternative feature importance allocations represented by DeepLIFT and LIME. As might be expected, SHAP values prove more consistent with human intuition than other methods that fail to meet Properties 1-3 (Section 2). Finally, we use MNIST digit image classification to compare SHAP with DeepLIFT and LIME.</p>
<h3>5.1 Computational Efficiency</h3>
<p>Theorem 2 connects Shapley values from game theory with weighted linear regression. Kernal SHAP uses this connection to compute feature importance. This leads to more accurate estimates with fewer evaluations of the original model than previous sampling-based estimates of Equation 8, particularly when regularization is added to the linear model (Figure 3). Comparing Shapley sampling, SHAP, and LIME on both dense and sparse decision tree models illustrates both the improved sample efficiency of Kernel SHAP and that values from LIME can differ significantly from SHAP values that satisfy local accuracy and consistency.</p>
<h3>5.2 Consistency with Human Intuition</h3>
<p>Theorem 1 provides a strong incentive for all additive feature attribution methods to use SHAP values. Both LIME and DeepLIFT, as originally demonstrated, compute different feature importance values. To validate the importance of Theorem 1, we compared explanations from LIME, DeepLIFT, and SHAP with user explanations of simple models (using Amazon Mechanical Turk). Our testing assumes that good model explanations should be consistent with explanations from humans who understand that model.</p>
<p>We compared LIME, DeepLIFT, and SHAP with human explanations for two settings. The first setting used a sickness score that was higher when only one of two symptoms was present (Figure 4A). The second used a max allocation problem to which DeepLIFT can be applied. Participants were told a short story about how three men made money based on the maximum score any of them achieved (Figure 4B). In both cases, participants were asked to assign credit for the output (the sickness score or money won) among the inputs (i.e., symptoms or players). We found a much stronger agreement between human explanations and SHAP than with other methods. SHAP's improved performance for max functions addresses the open problem of max pooling functions in DeepLIFT [7].</p>
<h3>5.3 Explaining Class Differences</h3>
<p>As discussed in Section 4.2, DeepLIFT's compositional approach suggests a compositional approximation of SHAP values (Deep SHAP). These insights, in turn, improve DeepLIFT, and a new version</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Human feature impact estimates are shown as the most common explanation given among 30 (A) and 52 (B) random individuals, respectively. (A) Feature attributions for a model output value (sickness score) of 2. The model output is 2 when fever and cough are both present, 5 when only one of fever or cough is present, and 0 otherwise. (B) Attributions of profit among three men, given according to the maximum number of questions any man got right. The first man got 5 questions right, the second 4 questions, and the third got none right, so the profit is $5.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Explaining the output of a convolutional network trained on the MNIST digit dataset. Orig. DeepLIFT has no explicit Shapley approximations, while New DeepLIFT seeks to better approximate Shapley values. (A) Red areas increase the probability of that class, and blue areas decrease the probability. Masked removes pixels in order to go from 8 to 3. (B) The change in log odds when masking over 20 random images supports the use of better estimates of SHAP values.</p>
<p>includes updates to better match Shapley values [7]. Figure 5 extends DeepLIFT's convolutional network example to highlight the increased performance of estimates that are closer to SHAP values. The pre-trained model and Figure 5 example are the same as those used in [7], with inputs normalized between 0 and 1. Two convolution layers and 2 dense layers are followed by a 10-way softmax output layer. Both DeepLIFT versions explain a normalized version of the linear layer, while SHAP (computed using Kernel SHAP) and LIME explain the model's output. SHAP and LIME were both run with 50k samples (Supplementary Figure 1); to improve performance, LIME was modified to use single pixel segmentation over the digit pixels. To match [7], we masked 20% of the pixels chosen to switch the predicted class from 8 to 3 according to the feature attribution given by each method.</p>
<h2>6 Conclusion</h2>
<p>The growing tension between the accuracy and interpretability of model predictions has motivated the development of methods that help users interpret predictions. The SHAP framework identifies the class of additive feature importance methods (which includes six previous methods) and shows there is a unique solution in this class that adheres to desirable properties. The thread of unity that SHAP weaves through the literature is an encouraging sign that common principles about model interpretation can inform the development of future methods.</p>
<p>We presented several different estimation methods for SHAP values, along with proofs and experiments showing that these values are desirable. Promising next steps involve developing faster model-type-specific estimation methods that make fewer assumptions, integrating work on estimating interaction effects from game theory, and defining new explanation model classes.</p>
<h1>Acknowledgements</h1>
<p>This work was supported by a National Science Foundation (NSF) DBI-135589, NSF CAREER DBI-155230, American Cancer Society 127332-RSG-15-097-01-TBG, National Institute of Health (NIH) AG049196, and NSF Graduate Research Fellowship. We would like to thank Marco Ribeiro, Erik Štrumbelj, Avanti Shrikumar, Yair Zick, the Lee Lab, and the NIPS reviewers for feedback that has significantly improved this work.</p>
<h2>References</h2>
<p>[1] Sebastian Bach et al. "On pixel-wise explanations for non-linear classifier decisions by layerwise relevance propagation". In: PloS One 10.7 (2015), e0130140.
[2] A Charnes et al. "Extremal principle solutions of games in characteristic function form: core, Chebychev and Shapley value generalizations". In: Econometrics of Planning and Efficiency 11 (1988), pp. 123-133.
[3] Anupam Datta, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems". In: Security and Privacy (SP), 2016 IEEE Symposium on. IEEE. 2016, pp. 598-617.
[4] Stan Lipovetsky and Michael Conklin. "Analysis of regression in game theory approach". In: Applied Stochastic Models in Business and Industry 17.4 (2001), pp. 319-330.
[5] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier". In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM. 2016, pp. 1135-1144.
[6] Lloyd S Shapley. "A value for n-person games". In: Contributions to the Theory of Games 2.28 (1953), pp. 307-317.
[7] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. "Learning Important Features Through Propagating Activation Differences". In: arXiv preprint arXiv:1704.02685 (2017).
[8] Avanti Shrikumar et al. "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences". In: arXiv preprint arXiv:1605.01713 (2016).
[9] Erik Štrumbelj and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions". In: Knowledge and information systems 41.3 (2014), pp. 647-665.
[10] H Peyton Young. "Monotonic solutions of cooperative games". In: International Journal of Game Theory 14.2 (1985), pp. 65-72.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ During the preparation of this manuscript we discovered this parallels an equivalent constrained quadratic minimization formulation of Shapley values proposed in econometrics [2].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>